{"home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpg.SGD_M.__init__": [[40, 54], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "0.01", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "nesterov", "=", "nesterov", ")", "\n", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "", "super", "(", "SGD_M", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpg.SGD_M.__setstate__": [[55, 59], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "SGD_M", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'nesterov'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpg.SGD_M.step": [[60, 89], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "p.add_", "grads[].view"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ",", "grads", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "", "", "grad_iter", "=", "0", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "if", "grads", "is", "not", "None", ":", "\n", "                \t", "d_p", "=", "(", "grads", "[", "grad_iter", "]", ")", ".", "view", "(", "p", ".", "shape", ")", "\n", "grad_iter", "+=", "1", "\n", "", "else", ":", "\n", "                \t", "d_p", "=", "p", ".", "grad", "\n", "", "p", ".", "add_", "(", "d_p", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpg.main": [[90, 352], ["envs.make_particleworld.make_env", "print", "print", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "update_functions.load_pi", "update_functions.take_param_consensus", "zip", "envs.make_particleworld.make_env", "range", "matplotlib.figure", "matplotlib.plot", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.close", "numpy.array", "numpy.array", "numpy.array", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "numpy.save", "os.path.join", "os.path.isdir", "os.makedirs", "update_functions.take_param_consensus.append", "optimizers.append", "copy.deepcopy", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "numpy.asarray", "numpy.mean", "minibatch_grads.tolist.tolist", "copy.deepcopy", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "zip", "update_functions.update_weights", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "state_list.append", "copy.deepcopy", "copy.deepcopy", "range", "update_functions.compute_IS_weight", "print", "np.array.append", "np.array.append", "np.array.append", "range", "zip", "zip", "update_functions.take_param_consensus", "zip", "copy.deepcopy", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "os.path.join", "os.path.join", "numpy.concatenate().ravel", "model.Policy().to", "train_particleworld_mdpg.SGD_M", "range", "zip", "minibatch_grads.tolist.append", "copy.deepcopy.append", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "range", "numpy.sum", "update_functions.compute_grads", "copy.deepcopy.append", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "action_list.append", "state_list.append", "range", "sum", "old_agent_optimizers.append", "update_functions.compute_grad_traj_prev_weights", "list_grad_traj_prev_weights.append", "update_functions.compute_u", "u_k_list.append", "update_functions.update_weights", "R_hist_plot.append", "print", "str", "str", "str", "str", "os.path.join", "str", "agents[].parameters", "numpy.concatenate().ravel", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "range", "numpy.sum", "update_functions.compute_grads", "single_traj_grads.append", "optimizer.zero_grad", "temp.append", "numpy.concatenate().ravel", "model.select_action", "actions.append", "len", "agents[].rewards.append", "print", "numpy.concatenate().ravel", "model.select_action", "actions.append", "torch.as_tensor", "torch.as_tensor", "len", "agents[].rewards.append", "print", "R_hist.append", "train_particleworld_mdpg.SGD_M", "numpy.sum", "len", "str", "str", "str", "str", "str", "str", "str", "numpy.concatenate", "model.Policy", "model.select_action", "actions.append", "len", "agents[].rewards.append", "print", "torch.FloatTensor", "torch.FloatTensor", "numpy.concatenate().ravel", "numpy.concatenate().ravel", "phi[].parameters", "str", "str", "str", "str", "str", "numpy.concatenate", "numpy.concatenate().ravel", "numpy.concatenate", "numpy.concatenate", "str", "len", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_IS_weight", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grad_traj_prev_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_u", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action"], ["", "", "def", "main", "(", ")", ":", "\n", "# initialize env", "\n", "\t", "num_agents", "=", "args", ".", "num_agents", "\n", "dimension", "=", "args", ".", "dim", "\n", "if", "args", ".", "minibatch_init", "==", "False", ":", "\n", "\t\t", "fpath", "=", "os", ".", "path", ".", "join", "(", "'mdpg_results_min_global_'", "+", "str", "(", "args", ".", "min_isw", ")", "+", "'isw'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "'beta='", "+", "str", "(", "args", ".", "beta", ")", "+", "'_'", "+", "args", ".", "topology", ")", "\n", "", "elif", "args", ".", "minibatch_init", "==", "True", ":", "\n", "\t\t", "fpath", "=", "os", ".", "path", ".", "join", "(", "'mdpg_results_min_global_'", "+", "str", "(", "args", ".", "min_isw", ")", "+", "'isw'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "'beta='", "+", "str", "(", "args", ".", "beta", ")", "+", "'_'", "+", "args", ".", "topology", "+", "'MI'", "+", "str", "(", "args", ".", "minibatch_size", ")", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "\t\t", "os", ".", "makedirs", "(", "fpath", ")", "\n", "\n", "", "if", "args", ".", "gpu", ":", "\n", "\t\t", "device", "=", "'cuda:0'", "\n", "", "else", ":", "\n", "\t\t", "device", "=", "'cpu'", "\n", "\n", "", "sample_env", "=", "make_particleworld", ".", "make_env", "(", "'simple_spread'", ",", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "dimension", ")", "\n", "sample_env", ".", "discrete_action_input", "=", "True", "#set action space to take in discrete numbers 0,1,2,3", "\n", "print", "(", "'observation Space:'", ",", "sample_env", ".", "observation_space", ")", "\n", "print", "(", "'Action Space:'", ",", "sample_env", ".", "action_space", ")", "\n", "print", "(", "'Number of agents:'", ",", "sample_env", ".", "n", ")", "\n", "sample_obs", "=", "sample_env", ".", "reset", "(", ")", "\n", "sample_obs", "=", "np", ".", "concatenate", "(", "sample_obs", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "agents", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "agents", ".", "append", "(", "model", ".", "Policy", "(", "state_dim", "=", "len", "(", "sample_obs", ")", ",", "action_dim", "=", "4", ")", ".", "to", "(", "device", ")", ")", "\n", "optimizers", ".", "append", "(", "SGD_M", "(", "agents", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "# load connectivity matrix", "\n", "", "pi", "=", "load_pi", "(", "num_agents", "=", "args", ".", "num_agents", ",", "topology", "=", "args", ".", "topology", ")", "\n", "if", "args", ".", "minibatch_init", ":", "\n", "# if using Minibatch - Initialization", "\n", "# For i in range B trajectories:", "\n", "#    Sample traj", "\n", "#    Compute grads", "\n", "#    Store grads", "\n", "#    Average grads", "\n", "\t\t", "old_agents", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "print", "(", "'Sampling initial minibatch of trajectory'", ")", "\n", "state", "=", "sample_env", ".", "reset", "(", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "R", "=", "0", "\n", "minibatch_grads", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "minibatch_size", ")", ":", "\n", "\t\t\t", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "", "state", ",", "rewards", ",", "done_n", ",", "_", "=", "sample_env", ".", "step", "(", "actions", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t\t\t", "agents", "[", "j", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "j", "]", ")", "\n", "\n", "", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t\t", "print", "(", "'Batch Initial Trajectory '", "+", "str", "(", "i", ")", "+", "': Reward'", ",", "R", ",", "'Done'", ",", "done", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "", "", "single_traj_grads", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t\t\t", "grads", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ",", "minibatch_init", "=", "True", ")", "\n", "single_traj_grads", ".", "append", "(", "grads", ")", "#list of num_agent x list grads of every layer", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "del", "policy", ".", "rewards", "[", ":", "]", "\n", "del", "policy", ".", "saved_log_probs", "[", ":", "]", "\n", "\n", "", "minibatch_grads", ".", "append", "(", "single_traj_grads", ")", "#list of minibatch x num_agent x list of grads of every layer", "\n", "\n", "# need grads to be shape num_agent x list of grads of every layer", "\n", "", "minibatch_grads", "=", "np", ".", "asarray", "(", "minibatch_grads", ")", "\n", "minibatch_grads", "=", "np", ".", "mean", "(", "minibatch_grads", ",", "0", ")", "#average across batch", "\n", "minibatch_grads", "=", "minibatch_grads", ".", "tolist", "(", ")", "\n", "# initializating with consensus of weights and grads", "\n", "prev_u_list", "=", "[", "]", "\n", "for", "avg_grads", "in", "minibatch_grads", ":", "\n", "\t\t\t", "temp", "=", "[", "]", "\n", "for", "layer", "in", "avg_grads", ":", "\n", "\t\t\t\t", "temp", ".", "append", "(", "torch", ".", "FloatTensor", "(", "layer", ")", ")", "\n", "", "prev_u_list", ".", "append", "(", "temp", ")", "\n", "", "", "else", ":", "\n", "#initialization", "\n", "\t\t", "old_agents", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "print", "(", "'Sampling initial trajectory'", ")", "\n", "state", "=", "sample_env", ".", "reset", "(", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "R", "=", "0", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "", "state", ",", "rewards", ",", "done_n", ",", "_", "=", "sample_env", ".", "step", "(", "actions", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t\t", "agents", "[", "j", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "j", "]", ")", "\n", "\n", "", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t", "print", "(", "'Initial Trajectory: Reward'", ",", "R", ",", "'Done'", ",", "done", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# initializating with consensus of weights and grads", "\n", "", "", "prev_u_list", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t\t", "grads", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ",", "minibatch_init", "=", "False", ")", "\n", "prev_u_list", ".", "append", "(", "grads", ")", "\n", "\n", "#agents = global_average(agents, num_agents)", "\n", "", "", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "\n", "for", "policy", ",", "optimizer", ",", "u_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "prev_u_list", ")", ":", "\n", "\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "u_k", ")", "\n", "\n", "# RL setup", "\n", "", "done", "=", "False", "\n", "R", "=", "0", "\n", "R_hist", "=", "[", "]", "\n", "R_hist_plot", "=", "[", "]", "\n", "isw_plot", "=", "[", "]", "\n", "num_plot", "=", "[", "]", "\n", "denom_plot", "=", "[", "]", "\n", "\n", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "env", "=", "make_particleworld", ".", "make_env", "(", "'simple_spread'", ",", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "dimension", ")", "\n", "env", ".", "discrete_action_input", "=", "True", "#set action space to take in discrete numbers 0,1,2,3", "\n", "\n", "for", "episode", "in", "range", "(", "args", ".", "num_episodes", ")", ":", "\n", "\t\t", "state", "=", "env", ".", "reset", "(", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "\n", "# phi is now old agent", "\n", "phi", "=", "copy", ".", "deepcopy", "(", "old_agents", ")", "\n", "# old_agent is now updated agent", "\n", "old_agent", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "\n", "# sample one trajectory", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "", "state", ",", "rewards", ",", "done_n", ",", "_", "=", "env", ".", "step", "(", "actions", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "action_list", ".", "append", "(", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "#print('r:', rewards[i])", "\n", "\t\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "R", "+=", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "#print(f'Eps: {episode} Done: {done_n} Reset:{reset} State:{state} reward:{rewards}')", "\n", "\t\t\t\t", "print", "(", "f'Eps: {episode} Done: {done_n} Reset:{reset} reward:{rewards}'", ")", "\n", "R_hist", ".", "append", "(", "R", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# compute ISW using latest traj with current agent and old agents", "\n", "", "", "isw_list", ",", "num", ",", "denom", "=", "compute_IS_weight", "(", "action_list", ",", "state_list", ",", "agents", ",", "phi", ",", "args", ".", "min_isw", ")", "\n", "print", "(", "isw_list", ")", "\n", "isw_plot", ".", "append", "(", "isw_list", ")", "\n", "num_plot", ".", "append", "(", "num", ")", "\n", "denom_plot", ".", "append", "(", "denom", ")", "\n", "\n", "# compute gradient of current trajectory using old agents. This requires old agents with gradients", "\n", "old_agent_optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t\t", "old_agent_optimizers", ".", "append", "(", "SGD_M", "(", "phi", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "", "list_grad_traj_prev_weights", "=", "[", "]", "\n", "for", "policy", ",", "old_policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "phi", ",", "old_agent_optimizers", ")", ":", "\n", "\t\t\t", "prev_g", "=", "compute_grad_traj_prev_weights", "(", "args", ",", "state_list", ",", "action_list", ",", "policy", ",", "old_policy", ",", "optimizer", ")", "\n", "list_grad_traj_prev_weights", ".", "append", "(", "prev_g", ")", "\n", "\n", "", "u_k_list", "=", "[", "]", "\n", "# compute u_k", "\n", "for", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", "in", "zip", "(", "agents", ",", "optimizers", ",", "prev_u_list", ",", "isw_list", ",", "list_grad_traj_prev_weights", ")", ":", "\n", "\t\t\t", "u_k", "=", "compute_u", "(", "args", ",", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", ",", "args", ".", "beta", ")", "\n", "u_k_list", ".", "append", "(", "u_k", ")", "\n", "\n", "# take consensus of parameters ", "\n", "# agents = global_average(agents, num_agents)", "\n", "", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "\n", "\n", "# update_weights with local grad surrogate, u_k", "\n", "for", "policy", ",", "optimizer", ",", "u_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "u_k_list", ")", ":", "\n", "\t\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "u_k", ")", "\n", "\n", "", "prev_u_list", "=", "copy", ".", "deepcopy", "(", "u_k_list", ")", "\n", "\n", "#update old_agents to current agent", "\n", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "if", "episode", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "\t\t\t", "avg_reward", "=", "np", ".", "sum", "(", "R_hist", ")", "/", "len", "(", "R_hist", ")", "\n", "R_hist_plot", ".", "append", "(", "avg_reward", ")", "\n", "R_hist", "=", "[", "]", "\n", "print", "(", "f'Episode:{episode} Average reward:{avg_reward:.2f}'", ")", "\n", "\n", "", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "R_hist_plot", ")", "\n", "plt", ".", "ylabel", "(", "'Reward'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_R.jpg'", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "isw_plot", "=", "np", ".", "array", "(", "isw_plot", ")", "\n", "num_plot", "=", "np", ".", "array", "(", "num_plot", ")", "\n", "denom_plot", "=", "np", ".", "array", "(", "denom_plot", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "isw_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "num_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Numerator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_num.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "denom_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Denominator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_denom.jpg'", ")", ")", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpg.SGD_M.__init__": [[38, 52], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "0.01", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "nesterov", "=", "nesterov", ")", "\n", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "", "super", "(", "SGD_M", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpg.SGD_M.__setstate__": [[53, 57], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "SGD_M", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'nesterov'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpg.SGD_M.step": [[58, 87], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "p.add_", "grads[].view"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ",", "grads", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "", "", "grad_iter", "=", "0", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "if", "grads", "is", "not", "None", ":", "\n", "                \t", "d_p", "=", "(", "grads", "[", "grad_iter", "]", ")", ".", "view", "(", "p", ".", "shape", ")", "\n", "grad_iter", "+=", "1", "\n", "", "else", ":", "\n", "                \t", "d_p", "=", "p", ".", "grad", "\n", "", "p", ".", "add_", "(", "d_p", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpg.main": [[88, 281], ["os.path.join", "envs.lineworld.LineWorld", "range", "update_functions.load_pi", "copy.deepcopy", "print", "lineworld.LineWorld.reset", "torch.FloatTensor().to", "torch.FloatTensor().to", "range", "zip", "update_functions.take_param_consensus", "zip", "range", "matplotlib.figure", "matplotlib.plot", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.close", "numpy.array", "numpy.array", "numpy.array", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "numpy.save", "os.path.isdir", "os.makedirs", "update_functions.take_param_consensus.append", "optimizers.append", "torch.as_tensor", "torch.as_tensor", "lineworld.LineWorld.step", "range", "torch.FloatTensor().to", "torch.FloatTensor().to", "numpy.sum", "update_functions.compute_grads", "copy.deepcopy.append", "update_functions.update_weights", "lineworld.LineWorld.reset", "state_list.append", "torch.FloatTensor().to", "torch.FloatTensor().to", "copy.deepcopy", "copy.deepcopy", "range", "update_functions.compute_IS_weight", "print", "np.array.append", "np.array.append", "np.array.append", "range", "zip", "zip", "update_functions.take_param_consensus", "zip", "copy.deepcopy", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "os.path.join", "str", "str", "model.Policy().to", "train_lineworld_mdpg.SGD_M", "torch.FloatTensor", "torch.FloatTensor", "model.select_action", "torch.as_tensor.append", "len", "agents[].rewards.append", "print", "torch.as_tensor", "torch.as_tensor", "lineworld.LineWorld.step", "action_list.append", "state_list.append", "range", "torch.FloatTensor().to", "torch.FloatTensor().to", "numpy.sum", "old_agent_optimizers.append", "update_functions.compute_grad_traj_prev_weights", "list_grad_traj_prev_weights.append", "update_functions.compute_u", "u_k_list.append", "update_functions.update_weights", "R_hist_plot.append", "print", "str", "str", "str", "str", "os.path.join", "str", "agents[].parameters", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "model.select_action", "torch.as_tensor.append", "len", "agents[].rewards.append", "print", "R_hist.append", "train_lineworld_mdpg.SGD_M", "numpy.sum", "len", "str", "str", "str", "str", "model.Policy", "torch.FloatTensor", "torch.FloatTensor", "phi[].parameters", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_IS_weight", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grad_traj_prev_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_u", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action"], ["", "", "def", "main", "(", ")", ":", "\n", "# initialize env", "\n", "\t", "num_agents", "=", "args", ".", "num_agents", "\n", "dimension", "=", "args", ".", "dim", "\n", "fpath", "=", "os", ".", "path", ".", "join", "(", "'mdpg_results_min_'", "+", "str", "(", "args", ".", "min_isw", ")", "+", "'isw'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "'beta='", "+", "str", "(", "args", ".", "beta", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "\t\t", "os", ".", "makedirs", "(", "fpath", ")", "\n", "\n", "", "env", "=", "lineworld", ".", "LineWorld", "(", "dimension", "=", "dimension", ",", "seed", "=", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "gpu", ":", "\n", "\t\t", "device", "=", "'cuda:0'", "\n", "", "else", ":", "\n", "\t\t", "device", "=", "'cpu'", "\n", "\n", "", "agents", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "agents", ".", "append", "(", "model", ".", "Policy", "(", "state_dim", "=", "dimension", ",", "action_dim", "=", "3", ")", ".", "to", "(", "device", ")", ")", "\n", "optimizers", ".", "append", "(", "SGD_M", "(", "agents", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "# load connectivity matrix", "\n", "", "pi", "=", "load_pi", "(", "num_agents", "=", "args", ".", "num_agents", ",", "topology", "=", "args", ".", "topology", ")", "\n", "#initialization", "\n", "old_agents", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "print", "(", "'Sampling initial trajectory'", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "R", "=", "0", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "", "actions", "=", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", "\n", "\n", "state", ",", "rewards", ",", "done", "=", "env", ".", "step", "(", "actions", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t", "print", "(", "'Initial Trajectory: Reward'", ",", "R", ",", "'Done'", ",", "done", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# initializating without consensus of u", "\n", "", "", "prev_u_list", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t", "grads", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ")", "\n", "prev_u_list", ".", "append", "(", "grads", ")", "\n", "\n", "", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "#agents = global_average(agents, num_agents)", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t", "update_weights", "(", "policy", ",", "optimizer", ")", "\n", "\n", "# RL setup", "\n", "", "done", "=", "False", "\n", "R", "=", "0", "\n", "R_hist", "=", "[", "]", "\n", "R_hist_plot", "=", "[", "]", "\n", "isw_plot", "=", "[", "]", "\n", "num_plot", "=", "[", "]", "\n", "denom_plot", "=", "[", "]", "\n", "\n", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "for", "episode", "in", "range", "(", "args", ".", "num_episodes", ")", ":", "\n", "\t\t", "state", "=", "env", ".", "reset", "(", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "\n", "# phi is now old agent", "\n", "phi", "=", "copy", ".", "deepcopy", "(", "old_agents", ")", "\n", "# old_agent is now updated agent", "\n", "old_agent", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "\n", "if", "episode", "==", "args", ".", "num_episodes", "-", "1", ":", "\n", "\t\t\t", "path", "=", "[", "state", "]", "\n", "\n", "# sample one trajectory", "\n", "", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "", "actions", "=", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", "\n", "\n", "#step through enviroment with set of actions. rewards is list of reward", "\n", "state", ",", "rewards", ",", "done", "=", "env", ".", "step", "(", "actions", ")", "\n", "\n", "action_list", ".", "append", "(", "actions", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t", "print", "(", "f'Eps: {episode} Done: {done} Reset:{reset} reward:{rewards}'", ")", "\n", "R_hist", ".", "append", "(", "R", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# compute ISW using latest traj with current agent and old agents", "\n", "", "", "isw_list", ",", "num", ",", "denom", "=", "compute_IS_weight", "(", "action_list", ",", "state_list", ",", "agents", ",", "phi", ",", "args", ".", "min_isw", ")", "\n", "print", "(", "isw_list", ")", "\n", "isw_plot", ".", "append", "(", "isw_list", ")", "\n", "num_plot", ".", "append", "(", "num", ")", "\n", "denom_plot", ".", "append", "(", "denom", ")", "\n", "\n", "# compute gradient of current trajectory using old agents. This requires old agents with gradients", "\n", "old_agent_optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t\t", "old_agent_optimizers", ".", "append", "(", "SGD_M", "(", "phi", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "", "list_grad_traj_prev_weights", "=", "[", "]", "\n", "for", "policy", ",", "old_policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "phi", ",", "old_agent_optimizers", ")", ":", "\n", "\t\t\t", "prev_g", "=", "compute_grad_traj_prev_weights", "(", "args", ",", "state_list", ",", "action_list", ",", "policy", ",", "old_policy", ",", "optimizer", ")", "\n", "list_grad_traj_prev_weights", ".", "append", "(", "prev_g", ")", "\n", "\n", "", "u_k_list", "=", "[", "]", "\n", "# compute gradient surrogate", "\n", "for", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", "in", "zip", "(", "agents", ",", "optimizers", ",", "prev_u_list", ",", "isw_list", ",", "list_grad_traj_prev_weights", ")", ":", "\n", "\t\t\t", "u_k", "=", "compute_u", "(", "args", ",", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", ",", "args", ".", "beta", ")", "\n", "u_k_list", ".", "append", "(", "u_k", ")", "\n", "\n", "# take consensus of parameters", "\n", "", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "#agents = global_average(agents, num_agents)", "\n", "\n", "# update_weights with local grad surrogate, u_k", "\n", "for", "policy", ",", "optimizer", ",", "u_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "u_k_list", ")", ":", "\n", "\t\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "u_k", ")", "\n", "\n", "", "prev_u_list", "=", "copy", ".", "deepcopy", "(", "u_k_list", ")", "\n", "\n", "#update old_agents to current agent", "\n", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "if", "episode", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "\t\t\t", "avg_reward", "=", "np", ".", "sum", "(", "R_hist", ")", "/", "len", "(", "R_hist", ")", "\n", "R_hist_plot", ".", "append", "(", "avg_reward", ")", "\n", "R_hist", "=", "[", "]", "\n", "print", "(", "f'Episode:{episode} Average reward:{avg_reward:.2f}'", ")", "\n", "\n", "", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "R_hist_plot", ")", "\n", "plt", ".", "ylabel", "(", "'Reward'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_R.jpg'", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "isw_plot", "=", "np", ".", "array", "(", "isw_plot", ")", "\n", "num_plot", "=", "np", ".", "array", "(", "num_plot", ")", "\n", "denom_plot", "=", "np", ".", "array", "(", "denom_plot", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "isw_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "num_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Numerator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_num.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "denom_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Denominator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_denom.jpg'", ")", ")", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpgt.SGD_GT.__init__": [[40, 54], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["\t", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "0.01", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ")", ":", "\n", "\t\t", "if", "lr", "<", "0.0", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "nesterov", "=", "nesterov", ")", "\n", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "", "super", "(", "SGD_GT", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpgt.SGD_GT.__setstate__": [[55, 59], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "\t\t", "super", "(", "SGD_GT", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\t\t\t", "group", ".", "setdefault", "(", "'nesterov'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpgt.SGD_GT.step": [[60, 89], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "p.add_", "grads[].view"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ",", "grads", "=", "None", ")", ":", "\n", "\t\t", "\"\"\"Performs a single optimization step.\n\n\t\tArguments:\n\t\t\tclosure (callable, optional): A closure that reevaluates the model\n\t\t\t\tand returns the loss.\n\t\t\"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "\t\t\t", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "\t\t\t\t", "loss", "=", "closure", "(", ")", "\n", "", "", "grad_iter", "=", "0", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\t\t\t", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "\t\t\t\t", "if", "p", ".", "grad", "is", "None", ":", "\n", "\t\t\t\t\t", "continue", "\n", "", "if", "grads", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "d_p", "=", "(", "grads", "[", "grad_iter", "]", ")", ".", "view", "(", "p", ".", "shape", ")", "\n", "grad_iter", "+=", "1", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "d_p", "=", "p", ".", "grad", "\n", "", "p", ".", "add_", "(", "d_p", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_mdpgt.main": [[90, 421], ["envs.make_particleworld.make_env", "print", "print", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "update_functions.load_pi", "copy.deepcopy", "update_functions.take_grad_consensus", "update_functions.take_param_consensus", "zip", "envs.make_particleworld.make_env", "range", "matplotlib.figure", "matplotlib.plot", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.close", "numpy.array", "numpy.array", "numpy.array", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "numpy.save", "os.path.join", "os.path.isdir", "os.makedirs", "update_functions.take_param_consensus.append", "optimizers.append", "copy.deepcopy", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "numpy.asarray", "numpy.mean", "minibatch_grads.tolist.tolist", "copy.deepcopy", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "zip", "update_functions.update_weights", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "state_list.append", "copy.deepcopy", "copy.deepcopy", "range", "update_functions.compute_IS_weight", "print", "np.array.append", "np.array.append", "np.array.append", "range", "zip", "zip", "update_functions.take_grad_consensus", "zip", "copy.deepcopy", "copy.deepcopy", "update_functions.take_param_consensus", "update_functions.take_grad_consensus", "zip", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "os.path.join", "os.path.join", "numpy.concatenate().ravel", "model.Policy().to", "train_particleworld_mdpgt.SGD_GT", "range", "zip", "minibatch_grads.tolist.append", "copy.deepcopy.append", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "range", "numpy.sum", "update_functions.compute_grads", "copy.deepcopy.append", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "action_list.append", "state_list.append", "range", "sum", "old_agent_optimizers.append", "update_functions.compute_grad_traj_prev_weights", "list_grad_traj_prev_weights.append", "update_functions.compute_u", "u_k_list.append", "update_functions.update_v", "next_v_k_list.append", "update_functions.update_weights", "R_hist_plot.append", "print", "numpy.save", "str", "str", "str", "str", "os.path.join", "str", "agents[].parameters", "numpy.concatenate().ravel", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "range", "numpy.sum", "update_functions.compute_grads", "single_traj_grads.append", "optimizer.zero_grad", "temp.append", "numpy.concatenate().ravel", "model.select_action", "actions.append", "len", "agents[].rewards.append", "print", "numpy.concatenate().ravel", "model.select_action", "actions.append", "torch.as_tensor", "torch.as_tensor", "len", "agents[].rewards.append", "print", "R_hist.append", "train_particleworld_mdpgt.SGD_GT", "numpy.sum", "len", "os.path.join", "str", "str", "str", "str", "str", "str", "str", "numpy.concatenate", "model.Policy", "model.select_action", "actions.append", "len", "agents[].rewards.append", "print", "torch.FloatTensor", "torch.FloatTensor", "numpy.concatenate().ravel", "numpy.concatenate().ravel", "phi[].parameters", "os.path.join", "str", "str", "str", "str", "str", "numpy.concatenate", "numpy.concatenate().ravel", "numpy.concatenate", "numpy.concatenate", "str", "len", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_IS_weight", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grad_traj_prev_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_u", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_v", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action"], ["", "", "def", "main", "(", ")", ":", "\n", "# initialize env", "\n", "\t", "num_agents", "=", "args", ".", "num_agents", "\n", "dimension", "=", "args", ".", "dim", "\n", "if", "args", ".", "minibatch_init", "==", "False", ":", "\n", "\t\t", "fpath", "=", "os", ".", "path", ".", "join", "(", "'mdpgt_results_min_global_'", "+", "str", "(", "args", ".", "min_isw", ")", "+", "'isw'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "'beta='", "+", "str", "(", "args", ".", "beta", ")", "+", "'_'", "+", "args", ".", "topology", ")", "\n", "", "elif", "args", ".", "minibatch_init", "==", "True", ":", "\n", "\t\t", "fpath", "=", "os", ".", "path", ".", "join", "(", "'mdpgt_results_min_global_'", "+", "str", "(", "args", ".", "min_isw", ")", "+", "'isw'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "'beta='", "+", "str", "(", "args", ".", "beta", ")", "+", "'_'", "+", "args", ".", "topology", "+", "'MI'", "+", "str", "(", "args", ".", "minibatch_size", ")", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "\t\t", "os", ".", "makedirs", "(", "fpath", ")", "\n", "\n", "", "if", "args", ".", "gpu", ":", "\n", "\t\t", "device", "=", "'cuda:0'", "\n", "", "else", ":", "\n", "\t\t", "device", "=", "'cpu'", "\n", "\n", "", "sample_env", "=", "make_particleworld", ".", "make_env", "(", "'simple_spread'", ",", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "dimension", ")", "\n", "sample_env", ".", "discrete_action_input", "=", "True", "# set action space to take in discrete numbers 0,1,2,3", "\n", "print", "(", "'observation Space:'", ",", "sample_env", ".", "observation_space", ")", "\n", "print", "(", "'Action Space:'", ",", "sample_env", ".", "action_space", ")", "\n", "print", "(", "'Number of agents:'", ",", "sample_env", ".", "n", ")", "\n", "sample_obs", "=", "sample_env", ".", "reset", "(", ")", "\n", "sample_obs", "=", "np", ".", "concatenate", "(", "sample_obs", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "agents", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "agents", ".", "append", "(", "model", ".", "Policy", "(", "state_dim", "=", "len", "(", "sample_obs", ")", ",", "action_dim", "=", "4", ")", ".", "to", "(", "device", ")", ")", "\n", "optimizers", ".", "append", "(", "SGD_GT", "(", "agents", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "# load connectivity matrix", "\n", "", "pi", "=", "load_pi", "(", "num_agents", "=", "args", ".", "num_agents", ",", "topology", "=", "args", ".", "topology", ")", "\n", "if", "args", ".", "minibatch_init", ":", "\n", "# if using Minibatch - Initialization", "\n", "# For i in range B trajectories:", "\n", "#    Sample traj", "\n", "#    Compute grads", "\n", "#    Store grads", "\n", "#    Average grads", "\n", "\t\t", "old_agents", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "print", "(", "'Sampling initial minibatch of trajectory'", ")", "\n", "state", "=", "sample_env", ".", "reset", "(", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "R", "=", "0", "\n", "minibatch_grads", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "minibatch_size", ")", ":", "\n", "\t\t\t", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "", "state", ",", "rewards", ",", "done_n", ",", "_", "=", "sample_env", ".", "step", "(", "actions", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t\t\t", "agents", "[", "j", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "j", "]", ")", "\n", "\n", "", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t\t", "print", "(", "'Batch Initial Trajectory '", "+", "str", "(", "i", ")", "+", "': Reward'", ",", "R", ",", "'Done'", ",", "done", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "", "", "single_traj_grads", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t\t\t", "grads", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ",", "minibatch_init", "=", "True", ")", "\n", "single_traj_grads", ".", "append", "(", "grads", ")", "#list of num_agent x list grads of every layer", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "del", "policy", ".", "rewards", "[", ":", "]", "\n", "del", "policy", ".", "saved_log_probs", "[", ":", "]", "\n", "\n", "", "minibatch_grads", ".", "append", "(", "single_traj_grads", ")", "#list of minibatch x num_agent x list of grads of every layer", "\n", "# need grads to be shape num_agent x list of grads of every layer", "\n", "", "minibatch_grads", "=", "np", ".", "asarray", "(", "minibatch_grads", ")", "\n", "minibatch_grads", "=", "np", ".", "mean", "(", "minibatch_grads", ",", "0", ")", "#average across batch", "\n", "minibatch_grads", "=", "minibatch_grads", ".", "tolist", "(", ")", "\n", "# initializating with consensus of weights and grads", "\n", "prev_u_list", "=", "[", "]", "\n", "for", "avg_grads", "in", "minibatch_grads", ":", "\n", "\t\t\t", "temp", "=", "[", "]", "\n", "for", "layer", "in", "avg_grads", ":", "\n", "\t\t\t\t", "temp", ".", "append", "(", "torch", ".", "FloatTensor", "(", "layer", ")", ")", "\n", "", "prev_u_list", ".", "append", "(", "temp", ")", "\n", "", "", "else", ":", "\n", "#initialization", "\n", "\t\t", "old_agents", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "print", "(", "'Sampling initial trajectory'", ")", "\n", "state", "=", "sample_env", ".", "reset", "(", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "R", "=", "0", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "", "state", ",", "rewards", ",", "done_n", ",", "_", "=", "sample_env", ".", "step", "(", "actions", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t\t", "agents", "[", "j", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "j", "]", ")", "\n", "\n", "", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t", "print", "(", "'Initial Trajectory: Reward'", ",", "R", ",", "'Done'", ",", "done", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# initializating with consensus of weights and grads", "\n", "", "", "prev_u_list", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t\t", "grads", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ",", "minibatch_init", "=", "False", ")", "\n", "prev_u_list", ".", "append", "(", "grads", ")", "\n", "\n", "", "", "v_k_list", "=", "copy", ".", "deepcopy", "(", "prev_u_list", ")", "\n", "\n", "#consensus_v_k_list = take_consensus(v_k_list, args.num_agents)", "\n", "consensus_v_k_list", "=", "take_grad_consensus", "(", "v_k_list", ",", "pi", ")", "\n", "\n", "# print(pi)", "\n", "# ##### for debugging new and old method", "\n", "# v_new = take_grad_consensus(v_k_list, pi)", "\n", "# v_old = take_consensus(v_k_list, args.num_agents)", "\n", "# for i in range(5):", "\n", "# \tfor j in range(6):", "\n", "# \t\tprint('Agent' + str(i) + 'Layer' + str(j) + ' ' + str(torch.nn.functional.mse_loss(v_new[i][j], v_old[i][j])))", "\n", "# \t\tif torch.nn.functional.mse_loss(v_new[i][j], v_old[i][j]) > 1e-6:", "\n", "# \t\t\tst", "\n", "# a_new = take_param_consensus(copy.deepcopy(old_agents), pi)", "\n", "# a_old = global_average(copy.deepcopy(old_agents), args.num_agents)", "\n", "# for i in range(5):", "\n", "# \tprint('Dense1 weight ', torch.nn.functional.mse_loss(a_new[0].dense1.weight.data, a_old[0].dense1.weight.data))", "\n", "# \tprint('Dense3 weight ', torch.nn.functional.mse_loss(a_new[0].dense3.weight.data, a_old[0].dense3.weight.data))", "\n", "# \tprint('Dense2 bias ', torch.nn.functional.mse_loss(a_new[0].dense2.bias.data, a_old[0].dense2.bias.data))", "\n", "# \tprint('Dense3 bias ', torch.nn.functional.mse_loss(a_new[0].dense3.bias.data, a_old[0].dense3.bias.data))", "\n", "# st", "\n", "# ######", "\n", "\n", "#agents = global_average(agents, args.num_agents)", "\n", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "\n", "for", "policy", ",", "optimizer", ",", "v_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "consensus_v_k_list", ")", ":", "\n", "\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "v_k", ")", "\n", "\n", "# RL setup", "\n", "", "done", "=", "False", "\n", "R", "=", "0", "\n", "R_hist", "=", "[", "]", "\n", "R_hist_plot", "=", "[", "]", "\n", "isw_plot", "=", "[", "]", "\n", "num_plot", "=", "[", "]", "\n", "denom_plot", "=", "[", "]", "\n", "\n", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "env", "=", "make_particleworld", ".", "make_env", "(", "'simple_spread'", ",", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "dimension", ")", "\n", "env", ".", "discrete_action_input", "=", "True", "#set action space to take in discrete numbers 0,1,2,3", "\n", "\n", "for", "episode", "in", "range", "(", "args", ".", "num_episodes", ")", ":", "\n", "\t\t", "state", "=", "env", ".", "reset", "(", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "\n", "# phi is now old agent", "\n", "phi", "=", "copy", ".", "deepcopy", "(", "old_agents", ")", "\n", "# old_agent is now updated agent", "\n", "old_agent", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "\n", "# sample one trajectory", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "", "state", ",", "rewards", ",", "done_n", ",", "_", "=", "env", ".", "step", "(", "actions", ")", "\n", "state", "=", "np", ".", "concatenate", "(", "state", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "action_list", ".", "append", "(", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "#print('r:', rewards[i])", "\n", "\t\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "R", "+=", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "#print(f'Eps: {episode} Done: {done_n} Reset:{reset} State:{state} reward:{rewards}')", "\n", "\t\t\t\t", "print", "(", "f'Eps: {episode} Done: {done_n} Reset:{reset} reward:{rewards}'", ")", "\n", "R_hist", ".", "append", "(", "R", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# compute ISW using latest traj with current agent and old agents", "\n", "", "", "isw_list", ",", "num", ",", "denom", "=", "compute_IS_weight", "(", "action_list", ",", "state_list", ",", "agents", ",", "phi", ",", "args", ".", "min_isw", ")", "\n", "print", "(", "isw_list", ")", "\n", "isw_plot", ".", "append", "(", "isw_list", ")", "\n", "num_plot", ".", "append", "(", "num", ")", "\n", "denom_plot", ".", "append", "(", "denom", ")", "\n", "\n", "# compute gradient of current trajectory using old agents. This requires old agents with gradients", "\n", "old_agent_optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t\t", "old_agent_optimizers", ".", "append", "(", "SGD_GT", "(", "phi", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "", "list_grad_traj_prev_weights", "=", "[", "]", "\n", "for", "policy", ",", "old_policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "phi", ",", "old_agent_optimizers", ")", ":", "\n", "\t\t\t", "prev_g", "=", "compute_grad_traj_prev_weights", "(", "args", ",", "state_list", ",", "action_list", ",", "policy", ",", "old_policy", ",", "optimizer", ")", "\n", "list_grad_traj_prev_weights", ".", "append", "(", "prev_g", ")", "\n", "\n", "# compute u_k", "\n", "", "u_k_list", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", "in", "zip", "(", "agents", ",", "optimizers", ",", "prev_u_list", ",", "isw_list", ",", "list_grad_traj_prev_weights", ")", ":", "\n", "\t\t\t", "u_k", "=", "compute_u", "(", "args", ",", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", ",", "args", ".", "beta", ")", "\n", "u_k_list", ".", "append", "(", "u_k", ")", "\n", "\n", "## take consensus of v_k first", "\n", "", "v_k_list", "=", "take_grad_consensus", "(", "v_k_list", ",", "pi", ")", "\n", "#v_k_list = take_consensus(v_k_list, args.num_agents)", "\n", "\n", "# # ##### for debugging new and old method", "\n", "# print('--------------------')", "\n", "# print('debug 2')", "\n", "# v_new = take_grad_consensus(v_k_list, pi)", "\n", "# v_old = take_consensus(v_k_list, args.num_agents)", "\n", "# for i in range(5):", "\n", "# \tfor j in range(6):", "\n", "# \t\tprint('Agent' + str(i) + 'Layer' + str(j) + ' ' + str(sum(v_new[i][j] - v_old[i][j])))", "\n", "# \t\tif torch.abs(sum(v_new[i][j] - v_old[i][j])) > 1e-5:", "\n", "# \t\t\tst", "\n", "# # ######", "\n", "\n", "## update v_k+1", "\n", "next_v_k_list", "=", "[", "]", "\n", "for", "v_k", ",", "u_k", ",", "prev_u_k", "in", "zip", "(", "v_k_list", ",", "u_k_list", ",", "prev_u_list", ")", ":", "\n", "\t\t\t", "v_k_new", "=", "update_v", "(", "v_k", ",", "u_k", ",", "prev_u_k", ")", "\n", "next_v_k_list", ".", "append", "(", "v_k_new", ")", "\n", "\n", "", "v_k_list", "=", "copy", ".", "deepcopy", "(", "next_v_k_list", ")", "\n", "prev_u_list", "=", "copy", ".", "deepcopy", "(", "u_k_list", ")", "\n", "\n", "# take consensus of parameters and v_k+1", "\n", "# agents = global_average(agents, args.num_agents)", "\n", "# consensus_next_v_k_list = take_consensus(next_v_k_list, args.num_agents)", "\n", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "consensus_next_v_k_list", "=", "take_grad_consensus", "(", "next_v_k_list", ",", "pi", ")", "\n", "\n", "# # ##### for debugging new and old method", "\n", "# print('--------------------')", "\n", "# print('debug 3')", "\n", "# v_new = take_grad_consensus(v_k_list, pi)", "\n", "# v_old = take_consensus(v_k_list, args.num_agents)", "\n", "# for i in range(5):", "\n", "# \tfor j in range(6):", "\n", "# \t\tprint('Agent' + str(i) + 'Layer' + str(j) + ' ' + str(sum(v_new[i][j] - v_old[i][j])))", "\n", "# \t\tif torch.abs(sum(v_new[i][j] - v_old[i][j])) > 1e-5:", "\n", "# \t\t\tst", "\n", "# a_new = take_param_consensus(copy.deepcopy(old_agents), pi)", "\n", "# a_old = global_average(copy.deepcopy(old_agents), args.num_agents)", "\n", "# for i in range(5):", "\n", "# \tprint('Dense1 weight ', sum(sum(a_new[0].dense1.weight.data - a_old[0].dense1.weight.data)))", "\n", "# \tprint('Dense3 weight ', sum(sum(a_new[0].dense3.weight.data - a_old[0].dense3.weight.data)))", "\n", "# \tprint('Dense2 bias ',sum(a_new[0].dense2.bias.data - a_old[0].dense2.bias.data))", "\n", "# \tprint('Dense3 bias ',sum(a_new[0].dense3.bias.data - a_old[0].dense3.bias.data))", "\n", "# # ######", "\n", "\n", "# update_weights with v_k+1", "\n", "for", "policy", ",", "optimizer", ",", "v_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "consensus_next_v_k_list", ")", ":", "\n", "\t\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "v_k", ")", "\n", "\n", "#update old_agents to current agent", "\n", "", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "if", "episode", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "\t\t\t", "avg_reward", "=", "np", ".", "sum", "(", "R_hist", ")", "/", "len", "(", "R_hist", ")", "\n", "R_hist_plot", ".", "append", "(", "avg_reward", ")", "\n", "R_hist", "=", "[", "]", "\n", "print", "(", "f'Episode:{episode} Average reward:{avg_reward:.2f}'", ")", "\n", "\n", "", "if", "episode", "==", "50000", ":", "\n", "\t\t\t", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'_50k.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n", "", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "R_hist_plot", ")", "\n", "plt", ".", "ylabel", "(", "'Reward'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_R.jpg'", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "isw_plot", "=", "np", ".", "array", "(", "isw_plot", ")", "\n", "num_plot", "=", "np", ".", "array", "(", "num_plot", ")", "\n", "denom_plot", "=", "np", ".", "array", "(", "denom_plot", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "isw_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "num_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Numerator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_num.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "denom_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Denominator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_denom.jpg'", ")", ")", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_particleworld_dpg.main": [[35, 124], ["os.path.join", "envs.make_particleworld.make_env", "print", "print", "print", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "update_functions.load_pi", "envs.make_particleworld.make_env", "range", "matplotlib.figure", "matplotlib.plot", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.close", "numpy.save", "os.path.isdir", "os.makedirs", "update_functions.take_param_consensus.append", "optimizers.append", "make_particleworld.make_env.reset", "numpy.concatenate().ravel().tolist", "range", "zip", "update_functions.take_param_consensus", "zip", "os.path.join", "os.path.join", "str", "str", "numpy.concatenate().ravel", "model.Policy().to", "torch.SGD", "make_particleworld.make_env.step", "numpy.concatenate().ravel().tolist", "all", "range", "sum", "update_functions.compute_grads", "update_functions.update_weights", "R_hist_plot.append", "print", "str", "os.path.join", "agents[].parameters", "numpy.concatenate().ravel", "model.select_action", "actions.append", "len", "agents[].rewards.append", "print", "R_hist.append", "numpy.sum", "len", "str", "numpy.concatenate", "model.Policy", "numpy.concatenate().ravel", "numpy.concatenate", "str", "len", "numpy.concatenate", "str"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action"], ["def", "main", "(", ")", ":", "\n", "    ", "num_agents", "=", "args", ".", "num_agents", "\n", "dimension", "=", "args", ".", "dim", "\n", "fpath", "=", "os", ".", "path", ".", "join", "(", "'dpg_results'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "str", "(", "args", ".", "topology", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "fpath", ")", "\n", "\n", "# initliaze multiple agents and optimizer", "\n", "", "if", "args", ".", "gpu", ":", "\n", "        ", "device", "=", "'cuda:0'", "\n", "", "else", ":", "\n", "        ", "device", "=", "'cpu'", "\n", "\n", "", "sample_env", "=", "make_particleworld", ".", "make_env", "(", "'simple_spread'", ",", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "dimension", ")", "\n", "sample_env", ".", "discrete_action_input", "=", "True", "#set action space to take in discrete numbers 0,1,2,3", "\n", "print", "(", "'observation Space:'", ",", "sample_env", ".", "observation_space", ")", "\n", "print", "(", "'Action Space:'", ",", "sample_env", ".", "action_space", ")", "\n", "print", "(", "'Number of agents:'", ",", "sample_env", ".", "n", ")", "\n", "sample_obs", "=", "sample_env", ".", "reset", "(", ")", "\n", "sample_obs", "=", "np", ".", "concatenate", "(", "sample_obs", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "agents", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "        ", "agents", ".", "append", "(", "model", ".", "Policy", "(", "state_dim", "=", "len", "(", "sample_obs", ")", ",", "action_dim", "=", "4", ")", ".", "to", "(", "device", ")", ")", "\n", "optimizers", ".", "append", "(", "optim", ".", "SGD", "(", "agents", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "0.0", ")", ")", "\n", "\n", "", "pi", "=", "load_pi", "(", "num_agents", "=", "args", ".", "num_agents", ",", "topology", "=", "args", ".", "topology", ")", "\n", "# RL setup", "\n", "num_episodes", "=", "args", ".", "num_episodes", "\n", "max_eps_len", "=", "args", ".", "max_eps_len", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "R_hist", "=", "[", "]", "\n", "R_hist_plot", "=", "[", "]", "\n", "\n", "env", "=", "make_particleworld", ".", "make_env", "(", "'simple_spread'", ",", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "dimension", ")", "\n", "env", ".", "discrete_action_input", "=", "True", "#set action space to take in discrete numbers 0,1,2,3", "\n", "\n", "for", "episode", "in", "range", "(", "num_episodes", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "obs", "=", "np", ".", "concatenate", "(", "obs", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "for", "t", "in", "range", "(", "1", ",", "max_eps_len", ")", ":", "\n", "            ", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "                ", "action", "=", "model", ".", "select_action", "(", "obs", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "#print('step:', t)", "\n", "", "obs", ",", "rewards", ",", "done_n", ",", "_", "=", "env", ".", "step", "(", "actions", ")", "\n", "obs", "=", "np", ".", "concatenate", "(", "obs", ")", ".", "ravel", "(", ")", ".", "tolist", "(", ")", "\n", "done", "=", "all", "(", "item", "==", "True", "for", "item", "in", "done_n", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "#print('r:', rewards[i])", "\n", "                ", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "R", "+=", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "#print(f'Eps: {episode} Done: {done_n} Reset:{reset} State:{obs} reward:{rewards}')", "\n", "                ", "print", "(", "f'Eps: {episode} Done: {done_n} Reset:{reset} reward:{rewards}'", ")", "\n", "R_hist", ".", "append", "(", "R", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "", "", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "            ", "_", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ")", "\n", "\n", "#agents = global_average(agents, num_agents)", "\n", "", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "            ", "update_weights", "(", "policy", ",", "optimizer", ")", "\n", "\n", "", "if", "episode", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "            ", "avg_reward", "=", "np", ".", "sum", "(", "R_hist", ")", "/", "len", "(", "R_hist", ")", "\n", "R_hist_plot", ".", "append", "(", "avg_reward", ")", "\n", "R_hist", "=", "[", "]", "\n", "print", "(", "f'Episode:{episode} Average reward:{avg_reward:.2f}'", ")", "\n", "\n", "", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "R_hist_plot", ")", "\n", "plt", ".", "ylabel", "(", "'Reward'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_R.jpg'", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_dpg.main": [[35, 115], ["os.path.join", "envs.lineworld.LineWorld", "range", "update_functions.load_pi", "range", "matplotlib.figure", "matplotlib.plot", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.close", "numpy.save", "os.path.isdir", "os.makedirs", "update_functions.take_param_consensus.append", "optimizers.append", "lineworld.LineWorld.reset", "torch.FloatTensor().to", "torch.FloatTensor().to", "range", "zip", "update_functions.take_param_consensus", "zip", "os.path.join", "os.path.join", "str", "model.Policy().to", "torch.SGD", "torch.as_tensor", "torch.as_tensor", "lineworld.LineWorld.step", "range", "torch.FloatTensor().to", "torch.FloatTensor().to", "numpy.sum", "update_functions.compute_grads", "update_functions.update_weights", "R_hist_plot.append", "print", "str", "os.path.join", "agents[].parameters", "torch.FloatTensor", "torch.FloatTensor", "model.select_action", "torch.as_tensor.append", "len", "agents[].rewards.append", "print", "R_hist.append", "numpy.sum", "len", "str", "model.Policy", "torch.FloatTensor", "torch.FloatTensor", "str", "str"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action"], ["def", "main", "(", ")", ":", "\n", "# initialize env", "\n", "\t", "num_agents", "=", "args", ".", "num_agents", "\n", "dimension", "=", "args", ".", "dim", "\n", "fpath", "=", "os", ".", "path", ".", "join", "(", "'dpg_results'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "\t\t", "os", ".", "makedirs", "(", "fpath", ")", "\n", "\n", "", "env", "=", "lineworld", ".", "LineWorld", "(", "dimension", "=", "dimension", ",", "seed", "=", "args", ".", "seed", ")", "\n", "\n", "# initliaze multiple agents and optimizer", "\n", "if", "args", ".", "gpu", ":", "\n", "\t\t", "device", "=", "'cuda:0'", "\n", "", "else", ":", "\n", "\t\t", "device", "=", "'cpu'", "\n", "\n", "", "agents", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "agents", ".", "append", "(", "model", ".", "Policy", "(", "state_dim", "=", "dimension", ",", "action_dim", "=", "3", ")", ".", "to", "(", "device", ")", ")", "\n", "optimizers", ".", "append", "(", "optim", ".", "SGD", "(", "agents", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "", "pi", "=", "load_pi", "(", "num_agents", "=", "args", ".", "num_agents", ",", "topology", "=", "args", ".", "topology", ")", "\n", "# RL setup", "\n", "num_episodes", "=", "args", ".", "num_episodes", "\n", "done", "=", "False", "\n", "max_eps_len", "=", "args", ".", "max_eps_len", "\n", "R", "=", "0", "\n", "R_hist", "=", "[", "]", "\n", "R_hist_plot", "=", "[", "]", "\n", "\n", "for", "episode", "in", "range", "(", "num_episodes", ")", ":", "\n", "\t\t", "state", "=", "env", ".", "reset", "(", ")", "\n", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "for", "t", "in", "range", "(", "1", ",", "max_eps_len", ")", ":", "# Don't infinite loop while learning", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "", "actions", "=", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", "\n", "\n", "#step through enviroment with set of actions. rewards is list of reward", "\n", "state", ",", "rewards", ",", "done", "=", "env", ".", "step", "(", "actions", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "#print('r:', rewards[i])", "\n", "\t\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t", "print", "(", "f'Done: {done} Reset:{reset} State:{state} reward:{rewards}'", ")", "\n", "R_hist", ".", "append", "(", "R", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "", "", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t\t", "_", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ")", "\n", "\n", "#agents = global_average(agents, num_agents)", "\n", "", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t\t", "update_weights", "(", "policy", ",", "optimizer", ")", "\n", "\n", "", "if", "episode", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "\t\t\t", "avg_reward", "=", "np", ".", "sum", "(", "R_hist", ")", "/", "len", "(", "R_hist", ")", "\n", "R_hist_plot", ".", "append", "(", "avg_reward", ")", "\n", "R_hist", "=", "[", "]", "\n", "print", "(", "f'Episode:{episode} Average reward:{avg_reward:.2f}'", ")", "\n", "\n", "", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "R_hist_plot", ")", "\n", "plt", ".", "ylabel", "(", "'Reward'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_R.jpg'", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.Policy.__init__": [[6, 13], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ")", ":", "\n", "        ", "super", "(", "Policy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense1", "=", "nn", ".", "Linear", "(", "state_dim", ",", "128", ")", "\n", "self", ".", "dense2", "=", "nn", ".", "Linear", "(", "128", ",", "64", ")", "\n", "self", ".", "dense3", "=", "nn", ".", "Linear", "(", "64", ",", "action_dim", ")", "\n", "self", ".", "saved_log_probs", "=", "[", "]", "\n", "self", ".", "rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.Policy.forward": [[14, 20], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "model.Policy.dense3", "torch.distributions.Categorical", "torch.distributions.Categorical", "model.Policy.dense1", "model.Policy.dense2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x1", "=", "torch", ".", "tanh", "(", "self", ".", "dense1", "(", "x", ")", ")", "\n", "x2", "=", "torch", ".", "tanh", "(", "self", ".", "dense2", "(", "x1", ")", ")", "\n", "x3", "=", "self", ".", "dense3", "(", "x2", ")", "\n", "dist", "=", "Categorical", "(", "logits", "=", "x3", ")", "\n", "return", "dist", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action": [[21, 30], ["policy", "policy.sample", "policy.saved_log_probs.append", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "policy.log_prob", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "numpy.array"], "function", ["None"], ["", "", "def", "select_action", "(", "state", ",", "policy", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "state", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "state", ")", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "except", ":", "\n", "        ", "pass", "\n", "", "dist", "=", "policy", "(", "state", ")", "\n", "action", "=", "dist", ".", "sample", "(", ")", "\n", "policy", ".", "saved_log_probs", ".", "append", "(", "dist", ".", "log_prob", "(", "action", ")", ")", "\n", "return", "action", "", "", ""]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi": [[6, 18], ["open", "json.load"], "function", ["None"], ["def", "load_pi", "(", "num_agents", ",", "topology", ")", ":", "\n", "\t", "wsize", "=", "num_agents", "\n", "if", "topology", "==", "'dense'", ":", "\n", "\t\t", "topo", "=", "1", "\n", "", "elif", "topology", "==", "'ring'", ":", "\n", "\t\t", "topo", "=", "2", "\n", "", "elif", "topology", "==", "'bipartite'", ":", "\n", "\t\t", "topo", "=", "3", "\n", "\n", "", "with", "open", "(", "'generate_topology/connectivity/%s_%s.json'", "%", "(", "wsize", ",", "topo", ")", ",", "'r'", ")", "as", "f", ":", "\n", "\t\t", "cdict", "=", "json", ".", "load", "(", "f", ")", "# connectivity dict.", "\n", "", "return", "cdict", "[", "'pi'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus": [[22, 63], ["enumerate", "layer_1_w.append", "layer_1_b.append", "layer_2_w.append", "layer_2_b.append", "layer_3_w.append", "layer_3_b.append", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.stack", "torch.tensor().unsqueeze().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "tuple", "tuple", "tuple", "tuple", "tuple", "tuple", "torch.tensor().unsqueeze", "torch.tensor", "torch.tensor().unsqueeze", "torch.tensor", "torch.tensor().unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "take_param_consensus", "(", "agents", ",", "pi", ")", ":", "\n", "\t", "layer_1_w", "=", "[", "]", "\n", "layer_1_b", "=", "[", "]", "\n", "\n", "layer_2_w", "=", "[", "]", "\n", "layer_2_b", "=", "[", "]", "\n", "\n", "layer_3_w", "=", "[", "]", "\n", "layer_3_b", "=", "[", "]", "\n", "\n", "for", "agent", "in", "agents", ":", "\n", "\t\t", "layer_1_w", ".", "append", "(", "agent", ".", "dense1", ".", "weight", ".", "data", ")", "\n", "layer_1_b", ".", "append", "(", "agent", ".", "dense1", ".", "bias", ".", "data", ")", "\n", "\n", "layer_2_w", ".", "append", "(", "agent", ".", "dense2", ".", "weight", ".", "data", ")", "\n", "layer_2_b", ".", "append", "(", "agent", ".", "dense2", ".", "bias", ".", "data", ")", "\n", "\n", "layer_3_w", ".", "append", "(", "agent", ".", "dense3", ".", "weight", ".", "data", ")", "\n", "layer_3_b", ".", "append", "(", "agent", ".", "dense3", ".", "bias", ".", "data", ")", "\n", "\n", "# layer_1_w = torch.sum(torch.stack(tuple(layer_1_w)),0)", "\n", "# layer_1_b = torch.sum(torch.stack(tuple(layer_1_b)),0)", "\n", "\n", "# layer_2_w = torch.sum(torch.stack(tuple(layer_2_w)),0)", "\n", "# layer_2_b = torch.sum(torch.stack(tuple(layer_2_b)),0)", "\n", "\n", "# layer_3_w = torch.sum(torch.stack(tuple(layer_3_w)),0)", "\n", "# layer_3_b = torch.sum(torch.stack(tuple(layer_3_b)),0)", "\n", "\n", "\n", "", "for", "agent_idx", ",", "agent", "in", "enumerate", "(", "agents", ")", ":", "\n", "\t\t", "agent", ".", "dense1", ".", "weight", ".", "data", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "layer_1_w", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "agent_idx", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "agent", ".", "dense1", ".", "bias", ".", "data", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "layer_1_b", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "agent_idx", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "\n", "agent", ".", "dense2", ".", "weight", ".", "data", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "layer_2_w", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "agent_idx", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "agent", ".", "dense2", ".", "bias", ".", "data", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "layer_2_b", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "agent_idx", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "\n", "agent", ".", "dense3", ".", "weight", ".", "data", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "layer_3_w", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "agent_idx", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "agent", ".", "dense3", ".", "bias", ".", "data", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "layer_3_b", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "agent_idx", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "\n", "", "return", "agents", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.global_average": [[118, 157], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.append", "torch.sum.append", "torch.sum.append", "torch.sum.append", "torch.sum.append", "torch.sum.append", "torch.sum.clone", "torch.sum.clone", "torch.sum.clone", "torch.sum.clone", "torch.sum.clone", "torch.sum.clone", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "def", "global_average", "(", "agents", ",", "num_agents", ")", ":", "\n", "\t", "layer_1_w", "=", "[", "]", "\n", "layer_1_b", "=", "[", "]", "\n", "\n", "layer_2_w", "=", "[", "]", "\n", "layer_2_b", "=", "[", "]", "\n", "\n", "layer_3_w", "=", "[", "]", "\n", "layer_3_b", "=", "[", "]", "\n", "\n", "for", "agent", "in", "agents", ":", "\n", "\t\t", "layer_1_w", ".", "append", "(", "agent", ".", "dense1", ".", "weight", ".", "data", ")", "\n", "layer_1_b", ".", "append", "(", "agent", ".", "dense1", ".", "bias", ".", "data", ")", "\n", "\n", "layer_2_w", ".", "append", "(", "agent", ".", "dense2", ".", "weight", ".", "data", ")", "\n", "layer_2_b", ".", "append", "(", "agent", ".", "dense2", ".", "bias", ".", "data", ")", "\n", "\n", "layer_3_w", ".", "append", "(", "agent", ".", "dense3", ".", "weight", ".", "data", ")", "\n", "layer_3_b", ".", "append", "(", "agent", ".", "dense3", ".", "bias", ".", "data", ")", "\n", "\n", "", "layer_1_w", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "layer_1_w", ")", "/", "num_agents", ",", "0", ")", "\n", "layer_1_b", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "layer_1_b", ")", "/", "num_agents", ",", "0", ")", "\n", "\n", "layer_2_w", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "layer_2_w", ")", "/", "num_agents", ",", "0", ")", "\n", "layer_2_b", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "layer_2_b", ")", "/", "num_agents", ",", "0", ")", "\n", "\n", "layer_3_w", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "layer_3_w", ")", "/", "num_agents", ",", "0", ")", "\n", "layer_3_b", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "layer_3_b", ")", "/", "num_agents", ",", "0", ")", "\n", "\n", "for", "agent", "in", "agents", ":", "\n", "\t\t", "agent", ".", "dense1", ".", "weight", ".", "data", "=", "layer_1_w", ".", "clone", "(", ")", "\n", "agent", ".", "dense1", ".", "bias", ".", "data", "=", "layer_1_b", ".", "clone", "(", ")", "\n", "\n", "agent", ".", "dense2", ".", "weight", ".", "data", "=", "layer_2_w", ".", "clone", "(", ")", "\n", "agent", ".", "dense2", ".", "bias", ".", "data", "=", "layer_2_b", ".", "clone", "(", ")", "\n", "\n", "agent", ".", "dense3", ".", "weight", ".", "data", "=", "layer_3_w", ".", "clone", "(", ")", "\n", "agent", ".", "dense3", ".", "bias", ".", "data", "=", "layer_3_b", ".", "clone", "(", ")", "\n", "", "return", "agents", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_IS_weight": [[158, 195], ["range", "len", "range", "numpy.prod", "range", "numpy.prod", "weight_list.append", "num_list.append", "denom_list.append", "len", "numpy.exp", "prob_curr_traj.append", "len", "old_dist.log_prob", "numpy.exp", "prob_old_traj.append", "numpy.max", "old_dist.log_prob.detach().numpy", "torch.FloatTensor", "old_dist.log_prob.detach().numpy", "old_dist.log_prob.detach", "old_dist.log_prob.detach"], "function", ["None"], ["", "def", "compute_IS_weight", "(", "action_list", ",", "state_list", ",", "cur_policy", ",", "old_policy", ",", "min_isw", ")", ":", "\n", "\t", "num_list", "=", "[", "]", "\n", "denom_list", "=", "[", "]", "\n", "weight_list", "=", "[", "]", "\n", "# for each policy", "\n", "for", "i", "in", "range", "(", "len", "(", "old_policy", ")", ")", ":", "\n", "\t\t", "prob_curr_traj", "=", "[", "]", "# list of probability of every action taken under curreny policy", "\n", "# for each step taken", "\n", "for", "j", "in", "range", "(", "len", "(", "action_list", ")", ")", ":", "\n", "# use save log probability attached to agent", "\n", "\t\t\t", "log_prob", "=", "cur_policy", "[", "i", "]", ".", "saved_log_probs", "[", "j", "]", "[", "0", "]", "\n", "prob", "=", "np", ".", "exp", "(", "log_prob", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "prob_curr_traj", ".", "append", "(", "prob", ")", "\n", "\n", "# multiply along all", "\n", "", "prob_tau", "=", "np", ".", "prod", "(", "prob_curr_traj", ")", "\n", "\n", "prob_old_traj", "=", "[", "]", "\n", "# for each step taken", "\n", "for", "j", "in", "range", "(", "len", "(", "action_list", ")", ")", ":", "\n", "# obtain distribution for given state", "\n", "\t\t\t", "old_dist", "=", "old_policy", "[", "i", "]", "(", "torch", ".", "FloatTensor", "(", "state_list", "[", "j", "]", ")", ")", "\n", "# compute log prob of action", "\n", "# action_list is in the shape of (episode_len, 1, num_agents)", "\n", "log_prob", "=", "old_dist", ".", "log_prob", "(", "action_list", "[", "j", "]", "[", "0", "]", "[", "i", "]", ")", "\n", "prob", "=", "np", ".", "exp", "(", "log_prob", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "prob_old_traj", ".", "append", "(", "prob", ")", "\n", "\n", "# multiply along all", "\n", "", "prob_old_tau", "=", "np", ".", "prod", "(", "prob_old_traj", ")", "\n", "\n", "weight", "=", "prob_old_tau", "/", "(", "prob_tau", "+", "1e-8", ")", "\n", "weight_list", ".", "append", "(", "np", ".", "max", "(", "(", "min_isw", ",", "weight", ")", ")", ")", "\n", "num_list", ".", "append", "(", "prob_old_tau", ")", "\n", "denom_list", ".", "append", "(", "prob_tau", ")", "\n", "\n", "", "return", "weight_list", ",", "num_list", ",", "denom_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grad_traj_prev_weights": [[196, 225], ["range", "numpy.finfo().eps.item", "torch.tensor", "zip", "optimizer.zero_grad", "torch.stack().sum", "torch.stack().sum.backward", "len", "old_policy", "old_policy.log_prob", "old_policy_log_probs.append", "torch.tensor.insert", "torch.stack().sum.append", "torch.FloatTensor", "torch.tensor.mean", "torch.tensor.std", "torch.stack", "p.grad.detach().clone().flatten", "numpy.finfo", "p.grad.detach().clone", "p.grad.detach"], "function", ["None"], ["", "def", "compute_grad_traj_prev_weights", "(", "args", ",", "state_list", ",", "action_list", ",", "policy", ",", "old_policy", ",", "optimizer", ")", ":", "\n", "\n", "\t", "old_policy_log_probs", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "action_list", ")", ")", ":", "\n", "\t\t", "dist", "=", "old_policy", "(", "torch", ".", "FloatTensor", "(", "state_list", "[", "i", "]", ")", ")", "\n", "log_prob", "=", "dist", ".", "log_prob", "(", "action_list", "[", "i", "]", ")", "\n", "old_policy_log_probs", ".", "append", "(", "log_prob", ")", "\n", "\n", "", "eps", "=", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ".", "item", "(", ")", "\n", "R", "=", "0", "\n", "policy_loss", "=", "[", "]", "\n", "returns", "=", "[", "]", "\n", "for", "r", "in", "policy", ".", "rewards", "[", ":", ":", "-", "1", "]", ":", "\n", "\t\t", "R", "=", "r", "+", "args", ".", "gamma", "*", "R", "\n", "returns", ".", "insert", "(", "0", ",", "R", ")", "\n", "", "returns", "=", "torch", ".", "tensor", "(", "returns", ")", "\n", "returns", "=", "(", "returns", "-", "returns", ".", "mean", "(", ")", ")", "/", "(", "returns", ".", "std", "(", ")", "+", "eps", ")", "\n", "for", "old_log_prob", ",", "R", "in", "zip", "(", "old_policy_log_probs", ",", "returns", ")", ":", "\n", "\t\t", "policy_loss", ".", "append", "(", "-", "old_log_prob", "*", "R", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "policy_loss", "=", "torch", ".", "stack", "(", "policy_loss", ")", ".", "sum", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "# list of tensors gradients, each tensor has shape", "\n", "grad", "=", "[", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "if", "(", "p", ".", "requires_grad", "is", "True", "and", "p", ".", "grad", "is", "not", "None", ")", "\n", "else", "None", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "'params'", "]", "]", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads": [[226, 251], ["numpy.finfo().eps.item", "torch.tensor", "zip", "optimizer.zero_grad", "torch.stack().sum", "torch.stack().sum.backward", "torch.tensor.insert", "torch.stack().sum.append", "torch.tensor.mean", "torch.tensor.std", "torch.stack", "numpy.finfo", "numpy.array", "p.grad.detach().clone().flatten", "p.grad.detach().clone().flatten", "p.grad.detach().clone", "p.grad.detach().clone", "p.grad.detach", "p.grad.detach"], "function", ["None"], ["", "def", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ",", "minibatch_init", "=", "False", ")", ":", "\n", "\t", "eps", "=", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ".", "item", "(", ")", "\n", "R", "=", "0", "\n", "policy_loss", "=", "[", "]", "\n", "returns", "=", "[", "]", "\n", "for", "r", "in", "policy", ".", "rewards", "[", ":", ":", "-", "1", "]", ":", "\n", "\t\t", "R", "=", "r", "+", "args", ".", "gamma", "*", "R", "\n", "returns", ".", "insert", "(", "0", ",", "R", ")", "\n", "\n", "", "returns", "=", "torch", ".", "tensor", "(", "returns", ")", "\n", "returns", "=", "(", "returns", "-", "returns", ".", "mean", "(", ")", ")", "/", "(", "returns", ".", "std", "(", ")", "+", "eps", ")", "\n", "for", "log_prob", ",", "R", "in", "zip", "(", "policy", ".", "saved_log_probs", ",", "returns", ")", ":", "\n", "\t\t", "policy_loss", ".", "append", "(", "-", "log_prob", "*", "R", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "policy_loss", "=", "torch", ".", "stack", "(", "policy_loss", ")", ".", "sum", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "\n", "# list of tensors gradients, each tensor has shape", "\n", "if", "minibatch_init", "==", "True", ":", "\n", "\t\t", "grad", "=", "[", "np", ".", "array", "(", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", ")", "if", "(", "p", ".", "requires_grad", "is", "True", "and", "p", ".", "grad", "is", "not", "None", ")", "\n", "else", "None", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "'params'", "]", "]", "\n", "", "elif", "minibatch_init", "==", "False", ":", "\n", "\t\t", "grad", "=", "[", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "if", "(", "p", ".", "requires_grad", "is", "True", "and", "p", ".", "grad", "is", "not", "None", ")", "\n", "else", "None", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "'params'", "]", "]", "\n", "", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights": [[252, 259], ["optimizer.step", "optimizer.step"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step"], ["", "def", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "None", ")", ":", "\n", "\t", "if", "grads", "is", "not", "None", ":", "\n", "\t\t", "optimizer", ".", "step", "(", "grads", "=", "grads", ")", "\n", "", "else", ":", "\n", "\t\t", "optimizer", ".", "step", "(", ")", "\n", "", "del", "policy", ".", "rewards", "[", ":", "]", "\n", "del", "policy", ".", "saved_log_probs", "[", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_u": [[261, 291], ["numpy.finfo().eps.item", "torch.tensor", "zip", "optimizer.zero_grad", "torch.stack().sum", "torch.stack().sum.backward", "range", "torch.tensor.insert", "torch.stack().sum.append", "len", "len", "torch.tensor.mean", "torch.tensor.std", "torch.stack", "p.grad.detach().clone().flatten", "numpy.finfo", "p.grad.detach().clone", "p.grad.detach"], "function", ["None"], ["", "def", "compute_u", "(", "args", ",", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", ",", "beta", ")", ":", "\n", "\n", "\t", "eps", "=", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ".", "item", "(", ")", "\n", "R", "=", "0", "\n", "policy_loss", "=", "[", "]", "\n", "returns", "=", "[", "]", "\n", "for", "r", "in", "policy", ".", "rewards", "[", ":", ":", "-", "1", "]", ":", "\n", "\t\t", "R", "=", "r", "+", "args", ".", "gamma", "*", "R", "\n", "returns", ".", "insert", "(", "0", ",", "R", ")", "\n", "\n", "", "returns", "=", "torch", ".", "tensor", "(", "returns", ")", "\n", "returns", "=", "(", "returns", "-", "returns", ".", "mean", "(", ")", ")", "/", "(", "returns", ".", "std", "(", ")", "+", "eps", ")", "\n", "for", "log_prob", ",", "R", "in", "zip", "(", "policy", ".", "saved_log_probs", ",", "returns", ")", ":", "\n", "\t\t", "policy_loss", ".", "append", "(", "-", "log_prob", "*", "R", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "policy_loss", "=", "torch", ".", "stack", "(", "policy_loss", ")", ".", "sum", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "\n", "# list of tensors gradients, each tensor has shape", "\n", "grad", "=", "[", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "if", "(", "p", ".", "requires_grad", "is", "True", "and", "p", ".", "grad", "is", "not", "None", ")", "\n", "else", "None", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "'params'", "]", "]", "\n", "\n", "# grad, prev_u and prev_g are all flatten grads", "\n", "assert", "grad", "[", "0", "]", ".", "shape", "==", "prev_u", "[", "0", "]", ".", "shape", "==", "prev_g", "[", "0", "]", ".", "shape", "\n", "# list of flatten surrogate grads [128, 64, 4096, ...]", "\n", "grad_surrogate", "=", "[", "1", "]", "*", "len", "(", "grad", ")", "\n", "#u = beta*grad + (1-beta)*(prev_u + grad - isw*prev_g)", "\n", "for", "i", "in", "range", "(", "len", "(", "grad_surrogate", ")", ")", ":", "\n", "\t\t", "grad_surrogate", "[", "i", "]", "=", "beta", "*", "grad", "[", "i", "]", "+", "(", "1", "-", "beta", ")", "*", "(", "prev_u", "[", "i", "]", "+", "grad", "[", "i", "]", "-", "isw", "*", "prev_g", "[", "i", "]", ")", "\n", "", "return", "grad_surrogate", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_v": [[295, 302], ["range", "len", "len"], "function", ["None"], ["", "def", "update_v", "(", "v_k", ",", "u_k", ",", "prev_u_k", ")", ":", "\n", "\t", "assert", "v_k", "[", "0", "]", ".", "shape", "==", "u_k", "[", "0", "]", ".", "shape", "==", "prev_u_k", "[", "0", "]", ".", "shape", "\n", "next_v_k", "=", "[", "1", "]", "*", "len", "(", "v_k", ")", "\n", "#next_v_k = v_k + u_k - prev_u_k", "\n", "for", "i", "in", "range", "(", "len", "(", "v_k", ")", ")", ":", "\n", "\t\t", "next_v_k", "[", "i", "]", "=", "v_k", "[", "i", "]", "+", "u_k", "[", "i", "]", "-", "prev_u_k", "[", "i", "]", "\n", "", "return", "next_v_k", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus": [[303, 333], ["range", "range", "len", "grads_0.append", "grads_1.append", "grads_2.append", "grads_3.append", "grads_4.append", "grads_5.append", "len", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "consensus_v_k.append", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "torch.stack", "torch.tensor().unsqueeze", "tuple", "tuple", "tuple", "tuple", "tuple", "tuple", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "take_grad_consensus", "(", "v_k_list", ",", "pi", ")", ":", "\n", "\n", "# list of n agents, each agent a list of 6 layers", "\n", "\t", "grads_0", "=", "[", "]", "\n", "grads_1", "=", "[", "]", "\n", "grads_2", "=", "[", "]", "\n", "grads_3", "=", "[", "]", "\n", "grads_4", "=", "[", "]", "\n", "grads_5", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "v_k_list", ")", ")", ":", "\n", "\t\t", "grads_0", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "0", "]", ")", "\n", "grads_1", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "1", "]", ")", "\n", "grads_2", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "2", "]", ")", "\n", "grads_3", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "3", "]", ")", "\n", "grads_4", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "4", "]", ")", "\n", "grads_5", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "5", "]", ")", "\n", "\n", "", "consensus_v_k", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "v_k_list", ")", ")", ":", "\n", "\t\t", "grads_0_j", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "grads_0", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "j", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_1_j", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "grads_1", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "j", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_2_j", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "grads_2", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "j", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_3_j", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "grads_3", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "j", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_4_j", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "grads_4", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "j", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_5_j", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "tuple", "(", "grads_5", ")", ")", "*", "torch", ".", "tensor", "(", "pi", "[", "j", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "v_k_list", "=", "[", "grads_0_j", ",", "grads_1_j", ",", "grads_2_j", ",", "grads_3_j", ",", "grads_4_j", ",", "grads_5_j", "]", "\n", "consensus_v_k", ".", "append", "(", "v_k_list", ")", "\n", "\n", "", "return", "consensus_v_k", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_consensus": [[380, 408], ["range", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "torch.sum().clone", "len", "torch.sum().clone.append", "torch.sum().clone.append", "torch.sum().clone.append", "torch.sum().clone.append", "torch.sum().clone.append", "torch.sum().clone.append", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.stack", "len", "torch.stack", "len", "torch.stack", "len", "torch.stack", "len", "torch.stack", "len", "torch.stack", "len"], "function", ["None"], ["", "def", "take_consensus", "(", "v_k_list", ",", "num_agents", ")", ":", "\n", "# list of n agents, each agent a list of 6 layers", "\n", "\t", "grads_0", "=", "[", "]", "\n", "grads_1", "=", "[", "]", "\n", "grads_2", "=", "[", "]", "\n", "grads_3", "=", "[", "]", "\n", "grads_4", "=", "[", "]", "\n", "grads_5", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "v_k_list", ")", ")", ":", "\n", "\t\t", "grads_0", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "0", "]", ")", "\n", "grads_1", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "1", "]", ")", "\n", "grads_2", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "2", "]", ")", "\n", "grads_3", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "3", "]", ")", "\n", "grads_4", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "4", "]", ")", "\n", "grads_5", ".", "append", "(", "v_k_list", "[", "i", "]", "[", "5", "]", ")", "\n", "\n", "", "grads_0", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "grads_0", ")", "/", "len", "(", "grads_0", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_1", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "grads_1", ")", "/", "len", "(", "grads_1", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_2", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "grads_2", ")", "/", "len", "(", "grads_2", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_3", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "grads_3", ")", "/", "len", "(", "grads_3", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_4", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "grads_4", ")", "/", "len", "(", "grads_4", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "grads_5", "=", "torch", ".", "sum", "(", "torch", ".", "stack", "(", "grads_5", ")", "/", "len", "(", "grads_5", ")", ",", "0", ")", ".", "clone", "(", ")", "\n", "\n", "v_k_list", "=", "[", "grads_0", ",", "grads_1", ",", "grads_2", ",", "grads_3", ",", "grads_4", ",", "grads_5", "]", "\n", "\n", "consensus_v_k", "=", "[", "v_k_list", "]", "*", "num_agents", "\n", "return", "consensus_v_k", "", "", ""]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.__init__": [[42, 56], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["\t", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "0.01", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ")", ":", "\n", "\t\t", "if", "lr", "<", "0.0", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "nesterov", "=", "nesterov", ")", "\n", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "", "super", "(", "SGD_GT", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.__setstate__": [[57, 61], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "\t\t", "super", "(", "SGD_GT", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\t\t\t", "group", ".", "setdefault", "(", "'nesterov'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.SGD_GT.step": [[62, 91], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "p.add_", "grads[].view"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ",", "grads", "=", "None", ")", ":", "\n", "\t\t", "\"\"\"Performs a single optimization step.\n\n\t\tArguments:\n\t\t\tclosure (callable, optional): A closure that reevaluates the model\n\t\t\t\tand returns the loss.\n\t\t\"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "\t\t\t", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "\t\t\t\t", "loss", "=", "closure", "(", ")", "\n", "", "", "grad_iter", "=", "0", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\t\t\t", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "\t\t\t\t", "if", "p", ".", "grad", "is", "None", ":", "\n", "\t\t\t\t\t", "continue", "\n", "", "if", "grads", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "d_p", "=", "(", "grads", "[", "grad_iter", "]", ")", ".", "view", "(", "p", ".", "shape", ")", "\n", "grad_iter", "+=", "1", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "d_p", "=", "p", ".", "grad", "\n", "", "p", ".", "add_", "(", "d_p", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.None.train_lineworld_mdpgt.main": [[92, 301], ["os.path.join", "envs.lineworld.LineWorld", "range", "update_functions.load_pi", "copy.deepcopy", "print", "lineworld.LineWorld.reset", "range", "zip", "copy.deepcopy", "update_functions.take_grad_consensus", "update_functions.take_param_consensus", "zip", "range", "matplotlib.figure", "matplotlib.plot", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.close", "numpy.array", "numpy.array", "numpy.array", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "matplotlib.figure", "range", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.savefig", "numpy.save", "os.path.isdir", "os.makedirs", "update_functions.take_param_consensus.append", "optimizers.append", "torch.as_tensor", "torch.as_tensor", "lineworld.LineWorld.step", "range", "torch.FloatTensor().to", "torch.FloatTensor().to", "numpy.sum", "update_functions.compute_grads", "copy.deepcopy.append", "update_functions.update_weights", "lineworld.LineWorld.reset", "state_list.append", "torch.FloatTensor().to", "torch.FloatTensor().to", "copy.deepcopy", "copy.deepcopy", "range", "update_functions.compute_IS_weight", "print", "np.array.append", "np.array.append", "np.array.append", "range", "zip", "zip", "update_functions.take_grad_consensus", "zip", "copy.deepcopy", "copy.deepcopy", "update_functions.take_param_consensus", "update_functions.take_grad_consensus", "zip", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "matplotlib.plot", "os.path.join", "os.path.join", "str", "str", "model.Policy().to", "train_lineworld_mdpgt.SGD_GT", "model.select_action", "torch.as_tensor.append", "len", "agents[].rewards.append", "print", "torch.as_tensor", "torch.as_tensor", "lineworld.LineWorld.step", "action_list.append", "state_list.append", "range", "torch.FloatTensor().to", "torch.FloatTensor().to", "numpy.sum", "old_agent_optimizers.append", "update_functions.compute_grad_traj_prev_weights", "list_grad_traj_prev_weights.append", "update_functions.compute_u", "u_k_list.append", "update_functions.update_v", "next_v_k_list.append", "update_functions.update_weights", "R_hist_plot.append", "print", "str", "str", "str", "str", "os.path.join", "str", "agents[].parameters", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "model.select_action", "torch.as_tensor.append", "len", "agents[].rewards.append", "print", "R_hist.append", "train_lineworld_mdpgt.SGD_GT", "numpy.sum", "len", "str", "str", "str", "str", "model.Policy", "torch.FloatTensor", "torch.FloatTensor", "phi[].parameters", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.load_pi", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grads", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_IS_weight", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_param_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.take_grad_consensus", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_grad_traj_prev_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.compute_u", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_v", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.update_functions.update_weights", "home.repos.pwc.inspect_result.xylee95_md-pgt.None.model.select_action"], ["", "", "def", "main", "(", ")", ":", "\n", "# initialize env", "\n", "\t", "num_agents", "=", "args", ".", "num_agents", "\n", "dimension", "=", "args", ".", "dim", "\n", "fpath", "=", "os", ".", "path", ".", "join", "(", "'mdpgt_results_min_'", "+", "str", "(", "args", ".", "min_isw", ")", "+", "'isw'", ",", "args", ".", "env", ",", "str", "(", "dimension", ")", "+", "'D'", ",", "args", ".", "opt", "+", "'beta='", "+", "str", "(", "args", ".", "beta", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "\t\t", "os", ".", "makedirs", "(", "fpath", ")", "\n", "\n", "", "env", "=", "lineworld", ".", "LineWorld", "(", "dimension", "=", "dimension", ",", "seed", "=", "args", ".", "seed", ")", "\n", "\n", "# initliaze multiple agents and optimizer", "\n", "if", "args", ".", "gpu", ":", "\n", "\t\t", "device", "=", "'cuda:0'", "\n", "", "else", ":", "\n", "\t\t", "device", "=", "'cpu'", "\n", "\n", "", "agents", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "agents", ".", "append", "(", "model", ".", "Policy", "(", "state_dim", "=", "dimension", ",", "action_dim", "=", "3", ")", ".", "to", "(", "device", ")", ")", "\n", "optimizers", ".", "append", "(", "SGD_GT", "(", "agents", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "\n", "# load connectivity matrix", "\n", "", "pi", "=", "load_pi", "(", "num_agents", "=", "args", ".", "num_agents", ",", "topology", "=", "args", ".", "topology", ")", "\n", "#initialization", "\n", "old_agents", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "print", "(", "'Sampling initial trajectory'", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "R", "=", "0", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "", "actions", "=", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", "\n", "\n", "state", ",", "rewards", ",", "done", "=", "env", ".", "step", "(", "actions", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t", "print", "(", "'Initial Trajectory: Reward'", ",", "R", ",", "'Done'", ",", "done", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# initializating with consensus of weights and grads", "\n", "", "", "prev_u_list", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "optimizers", ")", ":", "\n", "\t\t", "grads", "=", "compute_grads", "(", "args", ",", "policy", ",", "optimizer", ")", "\n", "prev_u_list", ".", "append", "(", "grads", ")", "\n", "\n", "", "v_k_list", "=", "copy", ".", "deepcopy", "(", "prev_u_list", ")", "\n", "\n", "#consensus_v_k_list = take_consensus(v_k_list, num_agents)", "\n", "consensus_v_k_list", "=", "take_grad_consensus", "(", "v_k_list", ",", "pi", ")", "\n", "#agents = global_average(agents, num_agents)", "\n", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "\n", "for", "policy", ",", "optimizer", ",", "v_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "consensus_v_k_list", ")", ":", "\n", "\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "v_k", ")", "\n", "\n", "# RL setup", "\n", "", "done", "=", "False", "\n", "R", "=", "0", "\n", "R_hist", "=", "[", "]", "\n", "R_hist_plot", "=", "[", "]", "\n", "isw_plot", "=", "[", "]", "\n", "num_plot", "=", "[", "]", "\n", "denom_plot", "=", "[", "]", "\n", "\n", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "for", "episode", "in", "range", "(", "args", ".", "num_episodes", ")", ":", "\n", "\t\t", "state", "=", "env", ".", "reset", "(", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "\n", "# phi is now old agent", "\n", "phi", "=", "copy", ".", "deepcopy", "(", "old_agents", ")", "\n", "# old_agent is now updated agent", "\n", "old_agent", "=", "copy", ".", "deepcopy", "(", "agents", ")", "\n", "\n", "# sample one trajectory", "\n", "for", "t", "in", "range", "(", "1", ",", "args", ".", "max_eps_len", ")", ":", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "policy", "in", "agents", ":", "\n", "\t\t\t\t", "action", "=", "model", ".", "select_action", "(", "state", ",", "policy", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "", "actions", "=", "torch", ".", "as_tensor", "(", "[", "actions", "]", ")", "\n", "\n", "#step through enviroment with set of actions. rewards is list of reward", "\n", "state", ",", "rewards", ",", "done", "=", "env", ".", "step", "(", "actions", ")", "\n", "#print('State:', state, 'Action:', actions, 'Rewards:', rewards)", "\n", "action_list", ".", "append", "(", "actions", ")", "\n", "state_list", ".", "append", "(", "state", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "agents", ")", ")", ":", "\n", "\t\t\t\t", "agents", "[", "i", "]", ".", "rewards", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "\n", "", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "device", ")", "\n", "R", "+=", "np", ".", "sum", "(", "rewards", ")", "\n", "reset", "=", "t", "==", "args", ".", "max_eps_len", "-", "1", "\n", "if", "done", "or", "reset", ":", "\n", "\t\t\t\t", "print", "(", "f'Eps: {episode} Done: {done} Reset:{reset} reward:{rewards}'", ")", "\n", "R_hist", ".", "append", "(", "R", ")", "\n", "R", "=", "0", "\n", "break", "\n", "\n", "# compute ISW using latest traj with current agent and old agents", "\n", "", "", "isw_list", ",", "num", ",", "denom", "=", "compute_IS_weight", "(", "action_list", ",", "state_list", ",", "agents", ",", "phi", ",", "args", ".", "min_isw", ")", "\n", "print", "(", "isw_list", ")", "\n", "isw_plot", ".", "append", "(", "isw_list", ")", "\n", "num_plot", ".", "append", "(", "num", ")", "\n", "denom_plot", ".", "append", "(", "denom", ")", "\n", "\n", "# compute gradient of current trajectory using old agents. This requires old agents with gradients", "\n", "old_agent_optimizers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t\t", "old_agent_optimizers", ".", "append", "(", "SGD_GT", "(", "phi", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ",", "momentum", "=", "args", ".", "momentum", ")", ")", "\n", "\n", "", "list_grad_traj_prev_weights", "=", "[", "]", "\n", "for", "policy", ",", "old_policy", ",", "optimizer", "in", "zip", "(", "agents", ",", "phi", ",", "old_agent_optimizers", ")", ":", "\n", "\t\t\t", "prev_g", "=", "compute_grad_traj_prev_weights", "(", "args", ",", "state_list", ",", "action_list", ",", "policy", ",", "old_policy", ",", "optimizer", ")", "\n", "list_grad_traj_prev_weights", ".", "append", "(", "prev_g", ")", "\n", "\n", "# compute u_k", "\n", "", "u_k_list", "=", "[", "]", "\n", "for", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", "in", "zip", "(", "agents", ",", "optimizers", ",", "prev_u_list", ",", "isw_list", ",", "list_grad_traj_prev_weights", ")", ":", "\n", "\t\t\t", "u_k", "=", "compute_u", "(", "args", ",", "policy", ",", "optimizer", ",", "prev_u", ",", "isw", ",", "prev_g", ",", "args", ".", "beta", ")", "\n", "u_k_list", ".", "append", "(", "u_k", ")", "\n", "\n", "## take consensus of v_k first", "\n", "", "v_k_list", "=", "take_grad_consensus", "(", "v_k_list", ",", "pi", ")", "\n", "#v_k_list = take_consensus(v_k_list, num_agents)", "\n", "\n", "## update v_k+1", "\n", "next_v_k_list", "=", "[", "]", "\n", "for", "v_k", ",", "u_k", ",", "prev_u_k", "in", "zip", "(", "v_k_list", ",", "u_k_list", ",", "prev_u_list", ")", ":", "\n", "\t\t\t", "v_k_new", "=", "update_v", "(", "v_k", ",", "u_k", ",", "prev_u_k", ")", "\n", "next_v_k_list", ".", "append", "(", "v_k_new", ")", "\n", "\n", "", "v_k_list", "=", "copy", ".", "deepcopy", "(", "next_v_k_list", ")", "\n", "prev_u_list", "=", "copy", ".", "deepcopy", "(", "u_k_list", ")", "\n", "\n", "# take consensus of parameters and v_k+1", "\n", "# agents = global_average(agents, num_agents)", "\n", "# consensus_next_v_k_list = take_consensus(next_v_k_list, num_agents)", "\n", "agents", "=", "take_param_consensus", "(", "agents", ",", "pi", ")", "\n", "consensus_next_v_k_list", "=", "take_grad_consensus", "(", "next_v_k_list", ",", "pi", ")", "\n", "\n", "# update_weights with grad surrogate", "\n", "for", "policy", ",", "optimizer", ",", "v_k", "in", "zip", "(", "agents", ",", "optimizers", ",", "consensus_next_v_k_list", ")", ":", "\n", "\t\t\t", "update_weights", "(", "policy", ",", "optimizer", ",", "grads", "=", "v_k", ")", "\n", "\n", "#update old_agents to current agent", "\n", "", "action_list", "=", "[", "]", "\n", "state_list", "=", "[", "]", "\n", "\n", "if", "episode", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "\t\t\t", "avg_reward", "=", "np", ".", "sum", "(", "R_hist", ")", "/", "len", "(", "R_hist", ")", "\n", "R_hist_plot", ".", "append", "(", "avg_reward", ")", "\n", "R_hist", "=", "[", "]", "\n", "print", "(", "f'Episode:{episode} Average reward:{avg_reward:.2f}'", ")", "\n", "\n", "", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "R_hist_plot", ")", "\n", "plt", ".", "ylabel", "(", "'Reward'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_R.jpg'", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "isw_plot", "=", "np", ".", "array", "(", "isw_plot", ")", "\n", "num_plot", "=", "np", ".", "array", "(", "num_plot", ")", "\n", "denom_plot", "=", "np", ".", "array", "(", "denom_plot", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "isw_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "num_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Numerator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_num.jpg'", ")", ")", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "num_agents", ")", ":", "\n", "\t\t", "plt", ".", "plot", "(", "denom_plot", "[", ":", ",", "i", "]", ",", "label", "=", "'Agent'", "+", "str", "(", "i", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "ylabel", "(", "'Importance Sampling Weight (Denominator)'", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "title", "(", "str", "(", "dimension", ")", "+", "'-d '", "+", "' '", "+", "args", ".", "env", "+", "args", ".", "opt", "+", "' '", "+", "str", "(", "args", ".", "seed", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "str", "(", "args", ".", "seed", ")", "+", "'_ISW_denom.jpg'", ")", ")", "\n", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "fpath", ",", "'R_array_'", "+", "str", "(", "args", ".", "seed", ")", "+", "'.npy'", ")", ")", ",", "R_hist_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.generate_topology.genconnectivity.FullyConnectedTopology": [[21, 28], ["range", "connectivity.append", "numpy.array", "range"], "function", ["None"], ["def", "FullyConnectedTopology", "(", "num_agents", ")", ":", "\n", "    ", "connectivity", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_agents", ")", ":", "\n", "        ", "neighbors", "=", "[", "1.0", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "connectivity", ".", "append", "(", "neighbors", ")", "\n", "", "pi", "=", "np", ".", "array", "(", "connectivity", ")", "/", "num_agents", "\n", "return", "connectivity", ",", "pi", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.generate_topology.genconnectivity.RingTopology": [[30, 43], ["range", "connectivity.append", "numpy.array", "range"], "function", ["None"], ["", "def", "RingTopology", "(", "num_agents", ")", ":", "\n", "    ", "connectivity", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_agents", ")", ":", "\n", "        ", "neighbors", "=", "[", "0.0", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "neighbors", "[", "j", "]", "=", "1.0", "\n", "neighbors", "[", "j", "-", "1", "]", "=", "1.0", "\n", "if", "j", "is", "num_agents", "-", "1", ":", "\n", "            ", "neighbors", "[", "0", "]", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "neighbors", "[", "j", "+", "1", "]", "=", "1.0", "\n", "", "connectivity", ".", "append", "(", "neighbors", ")", "\n", "", "pi", "=", "np", ".", "array", "(", "connectivity", ")", "/", "3", "# Since only 3 agents are active at every situation", "\n", "return", "connectivity", ",", "pi", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.generate_topology.genconnectivity.BiparTopology": [[45, 88], ["range", "connectivity.append", "pi.append", "connectivity.append", "pi.append", "math.floor", "connectivity.append", "pi.append", "numpy.array", "range", "range", "range", "range", "range", "range"], "function", ["None"], ["", "def", "BiparTopology", "(", "num_agents", ")", ":", "\n", "    ", "factor", "=", "10", "**", "2", "\n", "a", "=", "math", ".", "floor", "(", "(", "1", "/", "(", "num_agents", "-", "1", ")", ")", "*", "factor", ")", "/", "factor", "# round DOWN the 1/(num_agents-1) to 2 decimals", "\n", "b", "=", "1", "-", "(", "num_agents", "-", "2", ")", "*", "a", "\n", "c", "=", "1", "-", "2", "*", "a", "\n", "\n", "connectivity", "=", "[", "]", "\n", "pi", "=", "[", "]", "\n", "# First (num_agents-2) rows:", "\n", "for", "j", "in", "range", "(", "num_agents", "-", "2", ")", ":", "\n", "        ", "neighbors", "=", "[", "0.0", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "neighbors", "[", "j", "]", "=", "1.0", "\n", "neighbors", "[", "-", "1", "]", "=", "1.0", "\n", "neighbors", "[", "-", "2", "]", "=", "1.0", "\n", "connectivity", ".", "append", "(", "neighbors", ")", "\n", "\n", "pi_", "=", "[", "0.0", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "pi_", "[", "j", "]", "=", "c", "\n", "pi_", "[", "-", "1", "]", "=", "a", "\n", "pi_", "[", "-", "2", "]", "=", "a", "\n", "pi", ".", "append", "(", "pi_", ")", "\n", "\n", "# Second last row ", "\n", "", "neighbors", "=", "[", "1.0", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "neighbors", "[", "-", "1", "]", "=", "0.0", "\n", "connectivity", ".", "append", "(", "neighbors", ")", "\n", "\n", "pi_", "=", "[", "a", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "pi_", "[", "-", "1", "]", "=", "0.0", "\n", "pi_", "[", "-", "2", "]", "=", "b", "\n", "pi", ".", "append", "(", "pi_", ")", "\n", "\n", "# Last row ", "\n", "neighbors", "=", "[", "1.0", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "neighbors", "[", "-", "2", "]", "=", "0.0", "\n", "connectivity", ".", "append", "(", "neighbors", ")", "\n", "\n", "pi_", "=", "[", "a", "for", "_", "in", "range", "(", "num_agents", ")", "]", "\n", "pi_", "[", "-", "1", "]", "=", "b", "\n", "pi_", "[", "-", "2", "]", "=", "0.0", "\n", "pi", ".", "append", "(", "pi_", ")", "\n", "\n", "return", "connectivity", ",", "np", ".", "array", "(", "pi", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.generate_topology.genconnectivity.main": [[105, 132], ["agent_num_list.append", "os.path.exists", "os.makedirs", "enumerate", "range", "topology", "int", "pi.tolist", "open", "json.dump"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "    ", "'''\n    New connectivity folder contains json files named with: {num_agents}_{graph_type}.json where\n    graph_type 1 = FC\n    graph_type 2 = Ring\n    graph_type 3 = Bipar (Not implemented yet)\n    '''", "\n", "\n", "connectivity_folder", "=", "'connectivity'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "connectivity_folder", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "connectivity_folder", ")", "\n", "\n", "", "Topologies", "=", "[", "FullyConnectedTopology", ",", "RingTopology", ",", "BiparTopology", "]", "# Add BiparTopology when defined", "\n", "Topologies_name", "=", "[", "'FC'", ",", "'Ring'", ",", "'Bipar'", "]", "# Add BiparTopology when defined", "\n", "agent_num_list", "=", "[", "i", "for", "i", "in", "range", "(", "2", ",", "41", ",", "1", ")", "]", "\n", "agent_num_list", ".", "append", "(", "5", ")", "\n", "for", "num_agents", "in", "agent_num_list", ":", "\n", "        ", "for", "eid", ",", "topology", "in", "enumerate", "(", "Topologies", ")", ":", "\n", "            ", "connectivity", ",", "pi", "=", "topology", "(", "num_agents", ")", "\n", "cdict", "=", "{", "}", "\n", "cdict", "[", "'graph_type'", "]", "=", "Topologies_name", "[", "eid", "]", "\n", "cdict", "[", "'experiment id'", "]", "=", "eid", "+", "1", "\n", "cdict", "[", "'num_agents'", "]", "=", "int", "(", "num_agents", ")", "\n", "cdict", "[", "'connectivity'", "]", "=", "connectivity", "\n", "cdict", "[", "'pi'", "]", "=", "pi", ".", "tolist", "(", ")", "\n", "with", "open", "(", "'%s/%s_%s.json'", "%", "(", "connectivity_folder", ",", "num_agents", ",", "eid", "+", "1", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "cdict", ",", "f", ",", "sort_keys", "=", "False", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic.__init__": [[9, 26], ["numpy.random.seed", "gym.spaces.Discrete", "gym.spaces.Box", "numpy.arange", "quadratic.Quadratic.reset", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset"], ["def", "__init__", "(", "self", ",", "dimension", "=", "2", ",", "seed", "=", "0", ")", ":", "\n", "        ", "self", ".", "seed", "=", "seed", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "max_bound", "=", "5", "\n", "self", ".", "min_bound", "=", "-", "5", "\n", "self", ".", "y", "=", "1", "#dummy variable", "\n", "self", ".", "prev_y", "=", "1", "#dummy", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "dimension", ")", "#3 action per agent (up, down, stay) ^ num agents", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "self", ".", "min_bound", ",", "high", "=", "self", ".", "max_bound", ",", "shape", "=", "(", "self", ".", "dimension", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "step_size", "=", "0.5", "\n", "self", ".", "bounds", "=", "np", ".", "arange", "(", "self", ".", "min_bound", ",", "self", ".", "max_bound", ",", "self", ".", "step_size", ")", "\n", "self", ".", "reset", "(", ")", "\n", "# only used for contour plotting purposes for 2D", "\n", "self", ".", "minima", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic.step": [[27, 49], ["action[].detach().numpy", "numpy.clip", "quadratic.Quadratic.eval_func", "quadratic.Quadratic.get_reward", "numpy.less_equal().all", "delta.append", "action[].detach", "numpy.less_equal", "numpy.absolute", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.eval_func", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "#action is vector of delta", "\n", "        ", "action", "=", "action", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "delta", "=", "[", "]", "\n", "for", "item", "in", "action", ":", "\n", "            ", "if", "item", "==", "0", ":", "\n", "                ", "change", "=", "self", ".", "step_size", "\n", "", "elif", "item", "==", "1", ":", "\n", "                ", "change", "=", "-", "1", "*", "self", ".", "step_size", "\n", "", "elif", "item", "==", "2", ":", "\n", "                ", "change", "=", "0", "\n", "", "delta", ".", "append", "(", "change", ")", "\n", "", "self", ".", "state", "=", "self", ".", "state", "+", "delta", "\n", "self", ".", "state", "=", "np", ".", "clip", "(", "self", ".", "state", ",", "self", ".", "min_bound", ",", "self", ".", "max_bound", ")", "\n", "self", ".", "prev_y", "=", "self", ".", "y", "\n", "self", ".", "y", "=", "self", ".", "eval_func", "(", "action", ")", "\n", "self", ".", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "# optima at x = -y -z -a -b etc...", "\n", "if", "np", ".", "less_equal", "(", "(", "np", ".", "absolute", "(", "self", ".", "state", ")", ")", ",", "np", ".", "ones", "(", "self", ".", "dimension", ")", "*", "1e-5", ")", ".", "all", "(", ")", ":", "\n", "            ", "self", ".", "done", "=", "True", "\n", "self", ".", "reward", "=", "1", "\n", "", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", ",", "self", ".", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic.reset": [[50, 61], ["numpy.random.choice", "print", "numpy.array", "numpy.linalg.norm", "numpy.random.choice"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "#self.state = np.floor(np.random.uniform(low=self.min_bound, high=self.max_bound, size=(self.dimension,)))", "\n", "        ", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "while", "np", ".", "linalg", ".", "norm", "(", "self", ".", "state", ")", "<", "5", "*", "self", ".", "step_size", ":", "#np.linalg.norm(self.max_bound):", "\n", "#self.state = np.floor(np.random.uniform(low=self.min_bound, high=self.max_bound, size=(self.dimension,)))", "\n", "#self.state = np.clip(self.state, self.min_bound, self.max_bound)", "\n", "            ", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "\n", "", "print", "(", "'Reset state:'", ",", "self", ".", "state", ")", "\n", "self", ".", "done", "=", "False", "\n", "return", "np", ".", "array", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic.get_reward": [[63, 76], ["None"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# Reward compute based on y: we want previous y to be bigger than current y", "\n", "# Might also want to consider based on distance of x from optimal", "\n", "\n", "        ", "reward", "=", "-", "0.1", "*", "self", ".", "y", "+", "(", "self", ".", "prev_y", "-", "self", ".", "y", ")", "-", "0.1", "\n", "#reward = -0.1*self.y + 0.1*(self.prev_y - self.y)", "\n", "\n", "#r3", "\n", "##reward = -0.1*self.y ", "\n", "\n", "#r4", "\n", "#reward = -0.1*np.linalg.norm(self.state)", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic.eval_func": [[77, 82], ["len"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "# optimum at x = y = 0", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "y", "=", "self", ".", "state", "[", "0", "]", "**", "2", "+", "2", "*", "self", ".", "state", "[", "0", "]", "*", "self", ".", "state", "[", "1", "]", "+", "self", ".", "state", "[", "1", "]", "**", "2", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic.plot_eval_func": [[83, 86], ["None"], "methods", ["None"], ["", "def", "plot_eval_func", "(", "self", ",", "state", ")", ":", "\n", "        ", "x", ",", "y", "=", "state", "\n", "return", "x", "**", "2", "+", "2", "*", "x", "*", "y", "+", "y", "**", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic3D.__init__": [[89, 91], ["quadratic.Quadratic.__init__"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["def", "__init__", "(", "self", ",", "dimension", "=", "3", ",", "seed", "=", "0", ")", ":", "\n", "        ", "super", "(", "Quadratic3D", ",", "self", ")", ".", "__init__", "(", "dimension", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic3D.eval_func": [[92, 98], ["len"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "# optimum at x = -y-z", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "y", "=", "self", ".", "state", "[", "0", "]", "**", "2", "+", "self", ".", "state", "[", "1", "]", "**", "2", "+", "self", ".", "state", "[", "2", "]", "**", "2", "+", "2", "*", "self", ".", "state", "[", "0", "]", "*", "self", ".", "state", "[", "1", "]", "+", "2", "*", "self", ".", "state", "[", "0", "]", "*", "self", ".", "state", "[", "2", "]", "+", "2", "*", "self", ".", "state", "[", "1", "]", "*", "self", ".", "state", "[", "2", "]", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic3D.plot_eval_func": [[99, 102], ["None"], "methods", ["None"], ["", "def", "plot_eval_func", "(", "self", ",", "state", ")", ":", "\n", "        ", "x", ",", "y", ",", "z", "=", "state", "\n", "return", "x", "**", "2", "+", "y", "**", "2", "+", "z", "**", "2", "+", "2", "*", "x", "*", "y", "+", "2", "*", "x", "*", "z", "+", "2", "*", "y", "*", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic5D.__init__": [[105, 107], ["quadratic.Quadratic.__init__"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["def", "__init__", "(", "self", ",", "dimension", "=", "5", ",", "seed", "=", "0", ")", ":", "\n", "        ", "super", "(", "Quadratic5D", ",", "self", ")", ".", "__init__", "(", "dimension", "=", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic5D.eval_func": [[108, 115], ["numpy.ones", "numpy.matmul", "len", "numpy.matmul", "numpy.transpose"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "# optimum at x = -y-z", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "\n", "A", "=", "np", ".", "ones", "(", "(", "5", ",", "5", ")", ")", "\n", "y", "=", "np", ".", "matmul", "(", "np", ".", "matmul", "(", "self", ".", "state", ",", "A", ")", ",", "np", ".", "transpose", "(", "self", ".", "state", ")", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic10D.__init__": [[118, 120], ["quadratic.Quadratic.__init__"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__"], ["def", "__init__", "(", "self", ",", "dimension", "=", "10", ",", "seed", "=", "0", ")", ":", "\n", "        ", "super", "(", "Quadratic10D", ",", "self", ")", ".", "__init__", "(", "dimension", "=", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.quadratic.Quadratic10D.eval_func": [[121, 128], ["numpy.ones", "numpy.matmul", "len", "numpy.matmul", "numpy.transpose"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "# optimum at x = -y-z", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "\n", "A", "=", "np", ".", "ones", "(", "(", "10", ",", "10", ")", ")", "\n", "y", "=", "np", ".", "matmul", "(", "np", ".", "matmul", "(", "self", ".", "state", ",", "A", ")", ",", "np", ".", "transpose", "(", "self", ".", "state", ")", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.griewangk.Griewangk.__init__": [[9, 28], ["numpy.random.seed", "gym.spaces.Discrete", "gym.spaces.Box", "numpy.arange", "griewangk.Griewangk.reset", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset"], ["def", "__init__", "(", "self", ",", "dimension", "=", "2", ",", "seed", "=", "0", ")", ":", "\n", "#dimension of benchmark Griewangk function", "\n", "#The function is usually evaluated on the hypercube xi \u2208 [-600, 600], for all i = 1, \u2026, d. ", "\n", "        ", "self", ".", "seed", "=", "seed", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "max_bound", "=", "5", "\n", "self", ".", "min_bound", "=", "-", "5", "\n", "self", ".", "y", "=", "1", "#dummy variable", "\n", "self", ".", "prev_y", "=", "1", "#dummy variable", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "dimension", ")", "#3 action per agent (up, down, stay) ^ num agents", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "self", ".", "min_bound", ",", "high", "=", "self", ".", "max_bound", ",", "shape", "=", "(", "self", ".", "dimension", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "step_size", "=", "0.5", "\n", "self", ".", "bounds", "=", "np", ".", "arange", "(", "self", ".", "min_bound", ",", "self", ".", "max_bound", ",", "self", ".", "step_size", ")", "\n", "self", ".", "reset", "(", ")", "\n", "# only used for contour plotting purposes for 2D", "\n", "self", ".", "minima", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.griewangk.Griewangk.step": [[29, 50], ["action[].detach().numpy", "numpy.clip", "griewangk.Griewangk.eval_func", "griewangk.Griewangk.get_reward", "numpy.less_equal().all", "delta.append", "action[].detach", "numpy.less_equal", "numpy.absolute", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.eval_func", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "#action is vector of delta", "\n", "        ", "action", "=", "action", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "delta", "=", "[", "]", "\n", "for", "item", "in", "action", ":", "\n", "            ", "if", "item", "==", "0", ":", "\n", "                ", "change", "=", "self", ".", "step_size", "\n", "", "elif", "item", "==", "1", ":", "\n", "                ", "change", "=", "-", "1", "*", "self", ".", "step_size", "\n", "", "elif", "item", "==", "2", ":", "\n", "                ", "change", "=", "0.00", "\n", "", "delta", ".", "append", "(", "change", ")", "\n", "", "self", ".", "state", "=", "self", ".", "state", "+", "delta", "\n", "self", ".", "state", "=", "np", ".", "clip", "(", "self", ".", "state", ",", "self", ".", "min_bound", ",", "self", ".", "max_bound", ")", "\n", "self", ".", "prev_y", "=", "self", ".", "y", "\n", "self", ".", "y", "=", "self", ".", "eval_func", "(", "action", ")", "\n", "self", ".", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "if", "np", ".", "less_equal", "(", "(", "np", ".", "absolute", "(", "self", ".", "state", ")", ")", ",", "np", ".", "ones", "(", "self", ".", "dimension", ")", "*", "1e-3", ")", ".", "all", "(", ")", ":", "\n", "            ", "self", ".", "done", "=", "True", "\n", "self", ".", "reward", "=", "1", "\n", "", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", ",", "self", ".", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.griewangk.Griewangk.reset": [[51, 59], ["numpy.random.choice", "print", "numpy.array", "numpy.linalg.norm", "numpy.random.choice"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "while", "np", ".", "linalg", ".", "norm", "(", "self", ".", "state", ")", "<", "5", "*", "self", ".", "step_size", ":", "\n", "            ", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "\n", "", "print", "(", "'Reset state:'", ",", "self", ".", "state", ")", "\n", "self", ".", "done", "=", "False", "\n", "return", "np", ".", "array", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.griewangk.Griewangk.get_reward": [[60, 73], ["None"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# Reward compute based on y: we want previous y to be bigger than current y", "\n", "# Might also want to consider based on distance of x from optimal", "\n", "\n", "#reward = -0.1*self.y + (self.prev_y - self.y)", "\n", "#reward = -0.1*self.y + 0.1*(self.prev_y - self.y)", "\n", "\n", "#r3", "\n", "        ", "reward", "=", "-", "0.1", "*", "self", ".", "y", "-", "0.1", "\n", "\n", "#r4", "\n", "#reward = -0.1*np.linalg.norm(self.state)", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.griewangk.Griewangk.eval_func": [[74, 86], ["range", "len", "numpy.cos", "numpy.sqrt"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "# https://www.sfu.ca/~ssurjano/Griewangk.html", "\n", "term1", "=", "0", "\n", "term2", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "dimension", ")", ":", "\n", "            ", "term1", "=", "term1", "+", "(", "self", ".", "state", "[", "i", "]", "**", "2", ")", "/", "4000", "\n", "term2", "=", "term2", "*", "np", ".", "cos", "(", "self", ".", "state", "[", "i", "]", "/", "np", ".", "sqrt", "(", "i", "+", "1", ")", ")", "\n", "\n", "", "y", "=", "term1", "-", "term2", "+", "1", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.griewangk.Griewangk.plot_eval_func": [[87, 91], ["len", "numpy.cos", "numpy.cos", "numpy.sqrt"], "methods", ["None"], ["", "def", "plot_eval_func", "(", "self", ",", "state", ")", ":", "\n", "        ", "assert", "len", "(", "state", ")", "==", "2", ",", "\"action dimension surpasses 3D for visualization purposes\"", "\n", "x", ",", "y", "=", "state", "\n", "return", "(", "(", "x", "**", "2", "+", "y", "**", "2", ")", "/", "4000", ")", "-", "np", ".", "cos", "(", "x", ")", "*", "np", ".", "cos", "(", "y", "/", "np", ".", "sqrt", "(", "2", ")", ")", "+", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.lineworld.LineWorld.__init__": [[9, 22], ["numpy.random.seed", "gym.spaces.Discrete", "gym.spaces.Box", "numpy.arange", "lineworld.LineWorld.reset"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset"], ["def", "__init__", "(", "self", ",", "dimension", "=", "2", ",", "seed", "=", "0", ")", ":", "\n", "\t\t", "self", ".", "seed", "=", "seed", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "max_bound", "=", "1", "\n", "self", ".", "min_bound", "=", "-", "1", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "dimension", ")", "#3 action per agent (up, down, stay) ^ num agents", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "self", ".", "min_bound", ",", "high", "=", "self", ".", "max_bound", ",", "shape", "=", "(", "self", ".", "dimension", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "step_size", "=", "0.1", "\n", "self", ".", "bounds", "=", "np", ".", "arange", "(", "self", ".", "min_bound", ",", "self", ".", "max_bound", ",", "self", ".", "step_size", ")", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.lineworld.LineWorld.step": [[23, 42], ["action[].detach().numpy", "numpy.clip", "lineworld.LineWorld.get_reward", "numpy.less_equal().all", "delta.append", "action[].detach", "numpy.less_equal", "numpy.absolute", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "#action is vector of decision (0: up, 1: down, 2: stay)", "\n", "\t\t", "action", "=", "action", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "delta", "=", "[", "]", "\n", "for", "item", "in", "action", ":", "\n", "\t\t\t", "if", "item", "==", "0", ":", "\n", "\t\t\t\t", "change", "=", "self", ".", "step_size", "\n", "", "elif", "item", "==", "1", ":", "\n", "\t\t\t\t", "change", "=", "-", "1", "*", "self", ".", "step_size", "\n", "", "elif", "item", "==", "2", ":", "\n", "\t\t\t\t", "change", "=", "0.00", "\n", "", "delta", ".", "append", "(", "change", ")", "\n", "", "self", ".", "state", "=", "self", ".", "state", "+", "delta", "\n", "self", ".", "state", "=", "np", ".", "clip", "(", "self", ".", "state", ",", "self", ".", "min_bound", ",", "self", ".", "max_bound", ")", "\n", "self", ".", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "if", "np", ".", "less_equal", "(", "(", "np", ".", "absolute", "(", "self", ".", "state", ")", ")", ",", "np", ".", "ones", "(", "self", ".", "dimension", ")", "*", "1e-3", ")", ".", "all", "(", ")", ":", "\n", "\t\t\t", "self", ".", "done", "=", "True", "\n", "self", ".", "reward", "=", "[", "1", "]", "*", "self", ".", "dimension", "\n", "", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.lineworld.LineWorld.reset": [[43, 50], ["numpy.random.choice", "numpy.array", "numpy.linalg.norm", "numpy.random.choice"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "while", "np", ".", "linalg", ".", "norm", "(", "self", ".", "state", ")", "<", "5", "*", "self", ".", "step_size", ":", "\n", "\t\t\t", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "#print('Reset state:', self.state)", "\n", "", "self", ".", "done", "=", "False", "\n", "return", "np", ".", "array", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.lineworld.LineWorld.get_reward": [[51, 54], ["numpy.abs"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "\t\t", "reward", "=", "-", "0.1", "*", "np", ".", "abs", "(", "self", ".", "state", ")", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.sphere.Sphere.__init__": [[9, 26], ["numpy.random.seed", "gym.spaces.Discrete", "gym.spaces.Box", "numpy.arange", "sphere.Sphere.reset", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset"], ["def", "__init__", "(", "self", ",", "dimension", "=", "2", ",", "seed", "=", "0", ")", ":", "\n", "\t\t", "self", ".", "seed", "=", "seed", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "max_bound", "=", "1", "\n", "self", ".", "min_bound", "=", "-", "1", "\n", "self", ".", "y", "=", "1", "#dummy variable", "\n", "self", ".", "prev_y", "=", "1", "#dummy", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "dimension", ")", "#3 action per agent (up, down, stay) ^ num agents", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "self", ".", "min_bound", ",", "high", "=", "self", ".", "max_bound", ",", "shape", "=", "(", "self", ".", "dimension", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "step_size", "=", "0.1", "\n", "self", ".", "bounds", "=", "np", ".", "arange", "(", "self", ".", "min_bound", ",", "self", ".", "max_bound", ",", "self", ".", "step_size", ")", "\n", "self", ".", "reset", "(", ")", "\n", "# only used for contour plotting purposes for 2D", "\n", "self", ".", "minima", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.sphere.Sphere.step": [[27, 48], ["action[].detach().numpy", "numpy.clip", "sphere.Sphere.eval_func", "sphere.Sphere.get_reward", "numpy.less_equal().all", "delta.append", "action[].detach", "numpy.less_equal", "numpy.absolute", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.eval_func", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "#action is vector of decision (0: up, 1: down, 2: stay)", "\n", "\t\t", "action", "=", "action", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "delta", "=", "[", "]", "\n", "for", "item", "in", "action", ":", "\n", "\t\t\t", "if", "item", "==", "0", ":", "\n", "\t\t\t\t", "change", "=", "self", ".", "step_size", "\n", "", "elif", "item", "==", "1", ":", "\n", "\t\t\t\t", "change", "=", "-", "1", "*", "self", ".", "step_size", "\n", "", "elif", "item", "==", "2", ":", "\n", "\t\t\t\t", "change", "=", "0.00", "\n", "", "delta", ".", "append", "(", "change", ")", "\n", "", "self", ".", "state", "=", "self", ".", "state", "+", "delta", "\n", "self", ".", "state", "=", "np", ".", "clip", "(", "self", ".", "state", ",", "self", ".", "min_bound", ",", "self", ".", "max_bound", ")", "\n", "self", ".", "prev_y", "=", "self", ".", "y", "\n", "self", ".", "y", "=", "self", ".", "eval_func", "(", "action", ")", "\n", "self", ".", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "if", "np", ".", "less_equal", "(", "(", "np", ".", "absolute", "(", "self", ".", "state", ")", ")", ",", "np", ".", "ones", "(", "self", ".", "dimension", ")", "*", "1e-3", ")", ".", "all", "(", ")", ":", "\n", "\t\t\t", "self", ".", "done", "=", "True", "\n", "self", ".", "reward", "=", "1", "\n", "", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", ",", "self", ".", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.sphere.Sphere.reset": [[49, 57], ["numpy.random.choice", "print", "numpy.array", "numpy.linalg.norm", "numpy.random.choice"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "while", "np", ".", "linalg", ".", "norm", "(", "self", ".", "state", ")", "<", "5", "*", "self", ".", "step_size", ":", "\n", "\t\t\t", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "\n", "", "print", "(", "'Reset state:'", ",", "self", ".", "state", ")", "\n", "self", ".", "done", "=", "False", "\n", "return", "np", ".", "array", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.sphere.Sphere.get_reward": [[58, 75], ["None"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# Reward compute based on y: we want previous y to be bigger than current y", "\n", "# Might also want to consider based on distance of x from optimal", "\n", "\n", "#r1", "\n", "\t\t", "reward", "=", "-", "0.1", "*", "self", ".", "y", "+", "(", "self", ".", "prev_y", "-", "self", ".", "y", ")", "-", "0.1", "\n", "#reward = -0.1*self.y + 0.1*(self.prev_y - self.y)", "\n", "\n", "#r3", "\n", "#reward = -0.1*self.y ", "\n", "\n", "#r4", "\n", "#reward = -1*np.linalg.norm(self.state)", "\n", "\n", "#r5", "\n", "#reward = -0.1", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.sphere.Sphere.eval_func": [[76, 83], ["range", "len"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "# optimum at x = y = 0", "\n", "\t\t", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "y", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "dimension", ")", ":", "\n", "\t\t\t", "y", "+=", "self", ".", "state", "[", "i", "]", "**", "2", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.sphere.Sphere.plot_eval_func": [[84, 88], ["len"], "methods", ["None"], ["", "def", "plot_eval_func", "(", "self", ",", "state", ")", ":", "\n", "\t\t", "assert", "len", "(", "state", ")", "==", "2", ",", "\"action dimension surpasses 3D for visualization purposes\"", "\n", "x", ",", "y", "=", "state", "\n", "return", "x", "**", "2", "+", "y", "**", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.make_particleworld.make_env": [[4, 27], ["multiagent.load().Scenario", "scenarios.load().Scenario.make_world", "multiagent.environment.MultiAgentEnv", "multiagent.load"], "function", ["None"], ["def", "make_env", "(", "scenario_name", ",", "num_agents", "=", "2", ",", "num_landmarks", "=", "2", ")", ":", "\n", "    ", "'''\n    Creates a MultiAgentEnv object as env. This can be used similar to a gym\n    environment by calling env.reset() and env.step().\n    Use env.render() to view the environment on the screen.\n    Input:\n        scenario_name   :   name of the scenario from ./scenarios/ to be Returns\n                            (without the .py extension)\n    Some useful env properties (see environment.py):\n        .observation_space  :   Returns the observation space for each agent\n        .action_space       :   Returns the action space for each agent\n        .n                  :   Returns the number of Agents\n    '''", "\n", "from", "multiagent", ".", "environment", "import", "MultiAgentEnv", "\n", "import", "multiagent", ".", "scenarios", "as", "scenarios", "\n", "\n", "# load scenario from script", "\n", "scenario", "=", "scenarios", ".", "load", "(", "scenario_name", "+", "\".py\"", ")", ".", "Scenario", "(", ")", "\n", "# create world", "\n", "world", "=", "scenario", ".", "make_world", "(", "num_agents", "=", "num_agents", ",", "num_landmarks", "=", "num_landmarks", ")", "\n", "# create multiagent environment", "\n", "env", "=", "MultiAgentEnv", "(", "world", ",", "scenario", ".", "reset_world", ",", "scenario", ".", "reward", ",", "scenario", ".", "observation", ")", "\n", "return", "env", "\n", "", ""]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.agent.__init__": [[13, 18], ["numpy.random.randint", "numpy.random.randint"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "row", ",", "col", ")", ":", "\n", "        ", "self", ".", "row", "=", "row", "\n", "self", ".", "col", "=", "col", "\n", "self", ".", "pos", "=", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "row", ",", "self", ".", "col", ")", ")", "\n", "self", ".", "goal", "=", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "row", ",", "self", ".", "col", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.agent.reset": [[19, 22], ["numpy.random.randint", "numpy.random.randint"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "pos", "=", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "row", ",", "self", ".", "col", ")", ")", "\n", "self", ".", "goal", "=", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "row", ",", "self", ".", "col", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.__init__": [[25, 38], ["numpy.random.seed", "gridworld.grid.env_params", "len", "gridworld.agent", "range"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.env_params"], ["    ", "def", "__init__", "(", "self", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "agents", ":", "int", ",", "seed", ":", "int", ")", ":", "\n", "        ", "assert", "len", "(", "size", ")", "==", "2", ",", "\"grid size must be 2D\"", "\n", "self", ".", "grid_row", "=", "size", "[", "0", "]", "\n", "self", ".", "grid_col", "=", "size", "[", "1", "]", "\n", "self", ".", "agents", "=", "agents", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "self", ".", "list_of_agents", "=", "[", "agent", "(", "self", ".", "grid_row", ",", "self", ".", "grid_col", ")", "for", "i", "in", "range", "(", "self", ".", "agents", ")", "]", "\n", "\n", "# Setup action mapper", "\n", "self", ".", "action_mapper", "=", "{", "0", ":", "'up'", ",", "1", ":", "'down'", ",", "2", ":", "'left'", ",", "3", ":", "'right'", ",", "4", ":", "'no-op'", "}", "\n", "self", ".", "store_position", "=", "{", "}", "\n", "self", ".", "env_params", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.env_params": [[39, 53], ["numpy.zeros", "range", "gridworld.grid.update_state", "agent_n.reset", "gridworld.grid.check_spawn", "tuple", "tuple"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.update_state", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.check_spawn"], ["", "def", "env_params", "(", "self", ")", ":", "\n", "# reset gridworld state", "\n", "        ", "self", ".", "grid", "=", "np", ".", "zeros", "(", "[", "self", ".", "grid_row", ",", "self", ".", "grid_col", "]", ")", "\n", "\n", "# reset individual goal pos-goal pairs and register in grid", "\n", "for", "i", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "            ", "agent_n", "=", "self", ".", "list_of_agents", "[", "i", "]", "\n", "agent_n", ".", "reset", "(", ")", "\n", "self", ".", "list_of_agents", "[", "i", "]", "=", "agent_n", "\n", "self", ".", "grid", "[", "tuple", "(", "agent_n", ".", "pos", ")", "]", "=", "AGENT", "\n", "self", ".", "grid", "[", "tuple", "(", "agent_n", ".", "goal", ")", "]", "=", "GOAL", "\n", "self", ".", "check_spawn", "(", "i", ")", "\n", "\n", "", "self", ".", "update_state", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.update_state": [[54, 70], ["range", "list", "list.remove", "temp_state.append", "temp_state.append", "gridworld.grid.state.append", "range", "temp_state.append", "temp_state.append", "numpy.array().reshape", "numpy.array"], "methods", ["None"], ["", "def", "update_state", "(", "self", ")", ":", "\n", "# compile per-agent states", "\n", "        ", "self", ".", "state", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "# per-agent state is [pos, goal, other-pos, other-goal]", "\n", "            ", "temp_state", "=", "[", "]", "\n", "mod_list", "=", "list", "(", "range", "(", "self", ".", "agents", ")", ")", "\n", "mod_list", ".", "remove", "(", "i", ")", "\n", "temp_state", ".", "append", "(", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", ")", "\n", "temp_state", ".", "append", "(", "self", ".", "list_of_agents", "[", "i", "]", ".", "goal", ")", "\n", "# temp_state.append(self.list_of_agents[i].goal - self.list_of_agents[i].pos)", "\n", "for", "others", "in", "mod_list", ":", "\n", "                ", "temp_state", ".", "append", "(", "self", ".", "list_of_agents", "[", "others", "]", ".", "pos", ")", "\n", "", "for", "others", "in", "mod_list", ":", "\n", "                ", "temp_state", ".", "append", "(", "self", ".", "list_of_agents", "[", "others", "]", ".", "goal", ")", "\n", "", "self", ".", "state", ".", "append", "(", "np", ".", "array", "(", "temp_state", ")", ".", "reshape", "(", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.check_spawn": [[72, 96], ["numpy.array_equal", "tuple", "numpy.array_equal", "numpy.random.randint", "tuple", "numpy.array_equal", "numpy.random.randint", "tuple", "numpy.array_equal", "numpy.random.randint", "tuple", "numpy.array_equal", "numpy.random.randint", "tuple", "numpy.random.randint"], "methods", ["None"], ["", "", "def", "check_spawn", "(", "self", ",", "idx", ")", ":", "\n", "        ", "condition", "=", "1", "\n", "while", "condition", ":", "\n", "            ", "up", "=", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "0", "]", "-", "1", ",", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "1", "]", ")", "\n", "down", "=", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "0", "]", "+", "1", ",", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "1", "]", ")", "\n", "left", "=", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "0", "]", ",", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "1", "]", "-", "1", ")", "\n", "right", "=", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "0", "]", ",", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", "[", "1", "]", "+", "1", ")", "\n", "if", "np", ".", "array_equal", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", ",", "self", ".", "list_of_agents", "[", "idx", "]", ".", "goal", ")", ":", "\n", "                ", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "grid_row", ",", "self", ".", "grid_col", ")", ")", ")", "\n", "continue", "\n", "", "elif", "np", ".", "array_equal", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", ",", "up", ")", ":", "\n", "                ", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "grid_row", ",", "self", ".", "grid_col", ")", ")", ")", "\n", "continue", "\n", "", "elif", "np", ".", "array_equal", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", ",", "down", ")", ":", "\n", "                ", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "grid_row", ",", "self", ".", "grid_col", ")", ")", ")", "\n", "continue", "\n", "", "elif", "np", ".", "array_equal", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", ",", "left", ")", ":", "\n", "                ", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "grid_row", ",", "self", ".", "grid_col", ")", ")", ")", "\n", "continue", "\n", "", "elif", "np", ".", "array_equal", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", ",", "right", ")", ":", "\n", "                ", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "(", "self", ".", "grid_row", ",", "self", ".", "grid_col", ")", ")", ")", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "condition", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.reset": [[97, 100], ["gridworld.grid.env_params"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.env_params"], ["", "", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "env_params", "(", ")", "\n", "return", "self", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.getreward": [[101, 133], ["numpy.full", "range", "numpy.sqrt", "numpy.array", "numpy.array", "gridworld.grid.getreward.l2_norm"], "methods", ["None"], ["", "def", "getreward", "(", "self", ")", ":", "\n", "        ", "reward", "=", "np", ".", "full", "(", "self", ".", "agents", ",", "0.", ")", "\n", "done_status", "=", "[", "]", "\n", "\n", "# Sparse reward", "\n", "# for agent in range(self.agents):", "\n", "#     if np.array_equal(self.list_of_agents[agent].goal, self.list_of_agents[agent].pos):", "\n", "#         reward[agent] += 1", "\n", "#     else:", "\n", "#         reward[agent] += 0", "\n", "\n", "# for agent in range(self.agents):", "\n", "#     if np.array_equal(self.list_of_agents[agent].goal, self.list_of_agents[agent].pos):", "\n", "#         done_status.append(1)", "\n", "#     else:", "\n", "#         done_status.append(0)", "\n", "\n", "# if np.sum(done_status) == self.agents:", "\n", "#     for agent in range(self.agents):", "\n", "#         reward[agent] += 100*self.agents", "\n", "\n", "# Distance reward", "\n", "def", "l2_norm", "(", "x", ",", "y", ")", ":", "\n", "            ", "return", "np", ".", "sqrt", "(", "(", "x", "[", "0", "]", "-", "y", "[", "0", "]", ")", "**", "2", "+", "(", "x", "[", "1", "]", "-", "y", "[", "1", "]", ")", "**", "2", ")", "\n", "\n", "", "for", "agent", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "            ", "pos", "=", "np", ".", "array", "(", "self", ".", "list_of_agents", "[", "agent", "]", ".", "pos", ")", "\n", "goal", "=", "np", ".", "array", "(", "self", ".", "list_of_agents", "[", "agent", "]", ".", "goal", ")", "\n", "\n", "reward", "[", "agent", "]", "-=", "l2_norm", "(", "goal", ",", "pos", ")", "\n", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.getdone": [[134, 148], ["range", "numpy.array_equal", "numpy.sum", "done.append", "done.append"], "methods", ["None"], ["", "def", "getdone", "(", "self", ")", ":", "\n", "        ", "done", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "            ", "if", "np", ".", "array_equal", "(", "self", ".", "list_of_agents", "[", "i", "]", ".", "goal", ",", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", ")", ":", "\n", "                ", "done", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "done", ".", "append", "(", "0", ")", "\n", "\n", "", "", "if", "np", ".", "sum", "(", "done", ")", "==", "self", ".", "agents", ":", "\n", "            ", "done", "=", "1", "\n", "", "else", ":", "\n", "            ", "done", "=", "0", "\n", "\n", "", "return", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.step": [[149, 203], ["range", "range", "gridworld.grid.getreward", "gridworld.grid.getdone", "gridworld.grid.update_state", "len", "tuple", "tuple", "tuple", "raw_action[].numpy().item", "numpy.array", "tuple", "numpy.array", "tuple", "raw_action[].numpy", "numpy.array", "tuple", "tuple", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.getreward", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.getdone", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.update_state"], ["", "def", "step", "(", "self", ",", "raw_action", ")", ":", "\n", "        ", "raw_action", "=", "raw_action", "[", "0", "]", "\n", "assert", "len", "(", "raw_action", ")", "==", "self", ".", "agents", "\n", "# self.state = self.state.reshape(self.agents, -1)", "\n", "\n", "\"\"\"\n        Reference\n        0 | 1 | 2\n        --\n        1\n        --\n        2\n        --\n\n        (row, col)\n\n        ------------\n        Actions\n        UP    -> row -1\n        DOWN  -> row +1\n        LEFT  -> col -1\n        RIGHT -> col +1\n        STAY  -> no-op\n        ------------\n        \"\"\"", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "            ", "self", ".", "grid", "[", "tuple", "(", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", ")", "]", "=", "EMPTY", "\n", "self", ".", "grid", "[", "tuple", "(", "self", ".", "list_of_agents", "[", "i", "]", ".", "goal", ")", "]", "=", "GOAL", "\n", "action", "=", "self", ".", "action_mapper", "[", "raw_action", "[", "i", "]", ".", "numpy", "(", ")", ".", "item", "(", ")", "]", "\n", "\n", "if", "action", "==", "'up'", ":", "\n", "                ", "nextstep", "=", "tuple", "(", "np", ".", "array", "(", "[", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "0", "]", "-", "1", ",", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "1", "]", "]", ")", ")", "\n", "", "elif", "action", "==", "'down'", ":", "\n", "                ", "nextstep", "=", "tuple", "(", "np", ".", "array", "(", "[", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "0", "]", "+", "1", ",", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "1", "]", "]", ")", ")", "\n", "", "elif", "action", "==", "'left'", ":", "\n", "                ", "nextstep", "=", "tuple", "(", "np", ".", "array", "(", "[", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "0", "]", ",", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "1", "]", "-", "1", "]", ")", ")", "\n", "", "elif", "action", "==", "'right'", ":", "\n", "                ", "nextstep", "=", "tuple", "(", "np", ".", "array", "(", "[", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "0", "]", ",", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "1", "]", "+", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "nextstep", "=", "tuple", "(", "np", ".", "array", "(", "[", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "0", "]", ",", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "[", "1", "]", "]", ")", ")", "\n", "\n", "", "if", "nextstep", "[", "0", "]", ">=", "0", "and", "nextstep", "[", "0", "]", "<=", "self", ".", "grid_row", "-", "1", ":", "\n", "                ", "if", "nextstep", "[", "1", "]", ">=", "0", "and", "nextstep", "[", "1", "]", "<=", "self", ".", "grid_col", "-", "1", ":", "\n", "                    ", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "=", "nextstep", "\n", "\n", "", "", "", "for", "i", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "            ", "self", ".", "grid", "[", "self", ".", "list_of_agents", "[", "i", "]", ".", "pos", "]", "=", "AGENT", "\n", "\n", "", "reward", "=", "self", ".", "getreward", "(", ")", "\n", "done", "=", "self", ".", "getdone", "(", ")", "\n", "self", ".", "update_state", "(", ")", "\n", "\n", "return", "self", ".", "state", ",", "reward", ",", "done", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.gridworld.grid.render": [[204, 234], ["range", "print", "range", "print", "range", "print", "range", "numpy.array_equal", "tuple", "numpy.array", "print", "print", "print"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "grid_row", ")", ":", "\n", "            ", "rowstrings", "=", "'--'", "\n", "for", "rs", "in", "range", "(", "self", ".", "grid_col", ")", ":", "\n", "                ", "rowstrings", "+=", "'-'", "*", "4", "\n", "", "print", "(", "rowstrings", ")", "\n", "out", "=", "'| '", "\n", "for", "j", "in", "range", "(", "self", ".", "grid_col", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "i", ",", "j", "]", "==", "AGENT", ":", "\n", "                    ", "token", "=", "'('", "\n", "for", "idx", "in", "range", "(", "self", ".", "agents", ")", ":", "\n", "                        ", "if", "np", ".", "array_equal", "(", "tuple", "(", "self", ".", "list_of_agents", "[", "idx", "]", ".", "pos", ")", ",", "np", ".", "array", "(", "[", "i", ",", "j", "]", ")", ")", ":", "\n", "                            ", "token", "+=", "f'{idx}'", "\n", "", "", "token", "+=", "')'", "\n", "", "elif", "self", ".", "grid", "[", "i", ",", "j", "]", "==", "TRAP", ":", "\n", "                    ", "token", "=", "'T'", "\n", "", "elif", "self", ".", "grid", "[", "i", ",", "j", "]", "==", "EMPTY", ":", "\n", "                    ", "token", "=", "'0'", "\n", "", "elif", "self", ".", "grid", "[", "i", ",", "j", "]", "==", "GOAL", ":", "\n", "                    ", "token", "=", "'G'", "\n", "", "elif", "self", ".", "grid", "[", "i", ",", "j", "]", "==", "WALL", ":", "\n", "                    ", "token", "=", "'^'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "f\"Invalid value inside grid: {self.grid[i, j]}\"", ")", "\n", "print", "(", "self", ".", "grid", "[", "i", ",", "j", "]", "==", "AGENT", ")", "\n", "print", "(", "self", ".", "grid", ")", "\n", "", "out", "+=", "token", "+", "' | '", "\n", "", "print", "(", "out", ")", "\n", "", "print", "(", "rowstrings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.styblinski_tang.Styblinski_Tang.__init__": [[9, 26], ["numpy.random.seed", "gym.spaces.Discrete", "gym.spaces.Box", "styblinski_tang.Styblinski_Tang.reset", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset"], ["def", "__init__", "(", "self", ",", "dimension", "=", "2", ",", "seed", "=", "0", ")", ":", "\n", "#dimension of benchmark Rastrigin function", "\n", "#The function is usually evaluated on the hypercube xi \u2208 [-5.12, 5.12], for all i = 1, \u2026, d. ", "\n", "        ", "self", ".", "seed", "=", "seed", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "max_bound", "=", "5", "\n", "self", ".", "min_bound", "=", "-", "5", "\n", "self", ".", "y", "=", "1", "#dummy variable", "\n", "self", ".", "prev_y", "=", "1", "#dummy variable", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "dimension", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "self", ".", "min_action", ",", "high", "=", "self", ".", "max_action", ",", "shape", "=", "(", "self", ".", "dimension", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "reset", "(", ")", "\n", "# only used for contour plotting purposes for 2D", "\n", "self", ".", "minima", "=", "np", ".", "array", "(", "[", "-", "2.903534", ",", "-", "2.903534", "]", ")", "*", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.styblinski_tang.Styblinski_Tang.step": [[27, 48], ["action[].detach().numpy", "numpy.clip", "styblinski_tang.Styblinski_Tang.eval_func", "styblinski_tang.Styblinski_Tang.get_reward", "delta.append", "action[].detach"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.eval_func", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "#action is vector of delta", "\n", "        ", "action", "=", "action", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "delta", "=", "[", "]", "\n", "for", "item", "in", "action", ":", "\n", "            ", "if", "item", "==", "0", ":", "\n", "                ", "change", "=", "0.05", "\n", "", "elif", "item", "==", "1", ":", "\n", "                ", "change", "=", "-", "0.05", "\n", "", "elif", "item", "==", "2", ":", "\n", "                ", "change", "=", "0.00", "\n", "", "delta", ".", "append", "(", "change", ")", "\n", "", "self", ".", "state", "=", "self", ".", "state", "+", "delta", "\n", "self", ".", "state", "=", "np", ".", "clip", "(", "self", ".", "state", ",", "self", ".", "min_bound", ",", "self", ".", "max_bound", ")", "\n", "self", ".", "prev_y", "=", "self", ".", "y", "\n", "self", ".", "y", "=", "self", ".", "eval_func", "(", "action", ")", "\n", "self", ".", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "# if np.less_equal((np.absolute(self.state - np.ones(self.dimension)*(-2.903534))), np.ones(self.dimension)*(1e-3)).all(): ", "\n", "#     self.done = True", "\n", "#     self.reward = 10", "\n", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", ",", "self", ".", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.styblinski_tang.Styblinski_Tang.reset": [[49, 53], ["numpy.random.uniform", "numpy.array"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "5", ",", "high", "=", "5", ",", "size", "=", "(", "self", ".", "dimension", ",", ")", ")", "\n", "self", ".", "done", "=", "False", "\n", "return", "np", ".", "array", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.styblinski_tang.Styblinski_Tang.get_reward": [[54, 67], ["None"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# Reward compute based on y: we want previous y to be bigger than current y", "\n", "# Might also want to consider based on distance of x from optimal", "\n", "\n", "        ", "reward", "=", "-", "0.1", "*", "self", ".", "y", "+", "(", "self", ".", "prev_y", "-", "self", ".", "y", ")", "\n", "#reward = -0.1*self.y + 0.1*(self.prev_y - self.y)", "\n", "\n", "#r3", "\n", "#reward = -0.1*self.y ", "\n", "\n", "#r4", "\n", "#reward = -0.1*np.linalg.norm(self.state)", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.styblinski_tang.Styblinski_Tang.eval_func": [[68, 76], ["range", "len"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "#https://www.sfu.ca/~ssurjano/stybtang.html", "\n", "summation", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "dimension", ")", ":", "\n", "            ", "summation", "=", "summation", "+", "self", ".", "state", "[", "i", "]", "**", "4", "-", "16", "*", "self", ".", "state", "[", "i", "]", "**", "2", "+", "5", "*", "self", ".", "state", "[", "i", "]", "\n", "", "y", "=", "0.5", "*", "summation", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.styblinski_tang.Styblinski_Tang.plot_eval_func": [[77, 81], ["len"], "methods", ["None"], ["", "def", "plot_eval_func", "(", "self", ",", "state", ")", ":", "\n", "        ", "assert", "len", "(", "state", ")", "==", "2", ",", "\"state dimension surpasses 3D for visualization purposes\"", "\n", "x", ",", "y", "=", "state", "\n", "return", "0.5", "*", "(", "x", "**", "4", "-", "16", "*", "(", "x", "**", "2", ")", "+", "5", "*", "x", "+", "y", "**", "4", "-", "16", "*", "(", "y", "**", "2", ")", "+", "5", "*", "y", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.__init__": [[9, 28], ["numpy.random.seed", "gym.spaces.Discrete", "gym.spaces.Box", "numpy.arange", "rastrigin.Rastrigin.reset", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset"], ["def", "__init__", "(", "self", ",", "dimension", "=", "2", ",", "seed", "=", "0", ")", ":", "\n", "#dimension of benchmark Rastrigin function", "\n", "#The function is usually evaluated on the hypercube xi \u2208 [-5.12, 5.12], for all i = 1, \u2026, d. ", "\n", "        ", "self", ".", "seed", "=", "seed", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "max_bound", "=", "5", "\n", "self", ".", "min_bound", "=", "-", "5", "\n", "self", ".", "y", "=", "1", "#dummy variable", "\n", "self", ".", "prev_y", "=", "1", "#dummy", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "dimension", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "self", ".", "min_bound", ",", "high", "=", "self", ".", "max_bound", ",", "shape", "=", "(", "self", ".", "dimension", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "step_size", "=", "0.5", "\n", "self", ".", "bounds", "=", "np", ".", "arange", "(", "self", ".", "min_bound", ",", "self", ".", "max_bound", ",", "self", ".", "step_size", ")", "\n", "self", ".", "reset", "(", ")", "\n", "# only used for contour plotting purposes for 2D", "\n", "self", ".", "minima", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.step": [[29, 50], ["action[].detach().numpy", "numpy.clip", "rastrigin.Rastrigin.eval_func", "rastrigin.Rastrigin.get_reward", "numpy.less_equal().all", "delta.append", "action[].detach", "numpy.less_equal", "numpy.absolute", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.eval_func", "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "#action is vector of delta", "\n", "        ", "action", "=", "action", "[", "0", "]", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "delta", "=", "[", "]", "\n", "for", "item", "in", "action", ":", "\n", "            ", "if", "item", "==", "0", ":", "\n", "                ", "change", "=", "self", ".", "step_size", "\n", "", "elif", "item", "==", "1", ":", "\n", "                ", "change", "=", "-", "1", "*", "self", ".", "step_size", "\n", "", "elif", "item", "==", "2", ":", "\n", "                ", "change", "=", "0.00", "\n", "", "delta", ".", "append", "(", "change", ")", "\n", "", "self", ".", "state", "=", "self", ".", "state", "+", "delta", "\n", "self", ".", "state", "=", "np", ".", "clip", "(", "self", ".", "state", ",", "self", ".", "min_bound", ",", "self", ".", "max_bound", ")", "\n", "self", ".", "prev_y", "=", "self", ".", "y", "\n", "self", ".", "y", "=", "self", ".", "eval_func", "(", "action", ")", "\n", "self", ".", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "if", "np", ".", "less_equal", "(", "(", "np", ".", "absolute", "(", "self", ".", "state", ")", ")", ",", "np", ".", "ones", "(", "self", ".", "dimension", ")", "*", "1e-3", ")", ".", "all", "(", ")", ":", "\n", "            ", "self", ".", "done", "=", "True", "\n", "self", ".", "reward", "=", "1", "\n", "", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", ",", "self", ".", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.reset": [[51, 59], ["numpy.random.choice", "print", "numpy.array", "numpy.linalg.norm", "numpy.random.choice"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "while", "np", ".", "linalg", ".", "norm", "(", "self", ".", "state", ")", "<", "5", "*", "self", ".", "step_size", ":", "\n", "            ", "self", ".", "state", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "bounds", ",", "self", ".", "dimension", ",", "replace", "=", "True", ")", "\n", "\n", "", "print", "(", "'Reset state:'", ",", "self", ".", "state", ")", "\n", "self", ".", "done", "=", "False", "\n", "return", "np", ".", "array", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.get_reward": [[61, 74], ["None"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# Reward compute based on y: we want previous y to be bigger than current y", "\n", "# Might also want to consider based on distance of x from optimal", "\n", "\n", "#reward = -0.1*self.y + (self.prev_y - self.y) - 0.1", "\n", "#reward = -0.1*self.y + 0.1*(self.prev_y - self.y)", "\n", "\n", "#r3", "\n", "        ", "reward", "=", "-", "0.1", "*", "self", ".", "y", "-", "0.1", "\n", "\n", "#r4", "\n", "#reward = -0.1*np.linalg.norm(self.state)", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.eval_func": [[75, 84], ["range", "len", "numpy.cos"], "methods", ["None"], ["", "def", "eval_func", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "len", "(", "action", ")", "==", "self", ".", "dimension", "\n", "# https://www.sfu.ca/~ssurjano/rastr.html", "\n", "sum_term", "=", "0", "\n", "A", "=", "10", "\n", "for", "i", "in", "range", "(", "self", ".", "dimension", ")", ":", "\n", "            ", "sum_term", "=", "sum_term", "+", "(", "self", ".", "state", "[", "i", "]", "**", "2", "-", "10", "*", "np", ".", "cos", "(", "2", "*", "np", ".", "pi", "*", "self", ".", "state", "[", "i", "]", ")", ")", "\n", "", "y", "=", "A", "*", "self", ".", "dimension", "+", "sum_term", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_md-pgt.envs.rastrigin.Rastrigin.plot_eval_func": [[85, 89], ["len", "numpy.cos", "numpy.cos"], "methods", ["None"], ["", "def", "plot_eval_func", "(", "self", ",", "state", ")", ":", "\n", "        ", "assert", "len", "(", "state", ")", "==", "2", ",", "\"action dimension surpasses 3D for visualization purposes\"", "\n", "x", ",", "y", "=", "state", "\n", "return", "10", "*", "2", "+", "(", "x", "**", "2", "-", "10", "*", "np", ".", "cos", "(", "2", "*", "np", ".", "pi", "*", "x", ")", ")", "+", "(", "y", "**", "2", "-", "10", "*", "np", ".", "cos", "(", "2", "*", "np", ".", "pi", "*", "y", ")", ")", "", "", "", ""]]}