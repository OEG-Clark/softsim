{"home.repos.pwc.inspect_result.kandorm_CLINE.src.text.TextConfig.pa_read_options": [[29, 42], ["pyarrow.ReadOptions"], "methods", ["None"], ["@", "property", "\n", "def", "pa_read_options", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "read_options", "is", "not", "None", ":", "\n", "            ", "read_options", "=", "self", ".", "read_options", "\n", "", "else", ":", "\n", "            ", "read_options", "=", "pac", ".", "ReadOptions", "(", "column_names", "=", "[", "\"text\"", "]", ")", "\n", "", "if", "self", ".", "encoding", "is", "not", "None", ":", "\n", "            ", "read_options", ".", "encoding", "=", "self", ".", "encoding", "\n", "", "if", "self", ".", "block_size", "is", "not", "None", ":", "\n", "            ", "read_options", ".", "block_size", "=", "self", ".", "block_size", "\n", "", "if", "self", ".", "use_threads", "is", "not", "None", ":", "\n", "            ", "read_options", ".", "use_threads", "=", "self", ".", "use_threads", "\n", "", "return", "read_options", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.text.TextConfig.pa_parse_options": [[43, 57], ["pyarrow.ParseOptions"], "methods", ["None"], ["", "@", "property", "\n", "def", "pa_parse_options", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "parse_options", "is", "not", "None", ":", "\n", "            ", "parse_options", "=", "self", ".", "parse_options", "\n", "", "else", ":", "\n", "            ", "parse_options", "=", "pac", ".", "ParseOptions", "(", "\n", "delimiter", "=", "\"\\r\"", ",", "\n", "quote_char", "=", "False", ",", "\n", "double_quote", "=", "False", ",", "\n", "escape_char", "=", "False", ",", "\n", "newlines_in_values", "=", "False", ",", "\n", "ignore_empty_lines", "=", "False", ",", "\n", ")", "\n", "", "return", "parse_options", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.text.TextConfig.pa_convert_options": [[58, 67], ["pyarrow.ConvertOptions"], "methods", ["None"], ["", "@", "property", "\n", "def", "pa_convert_options", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "convert_options", "is", "not", "None", ":", "\n", "            ", "convert_options", "=", "self", ".", "convert_options", "\n", "", "else", ":", "\n", "            ", "convert_options", "=", "pac", ".", "ConvertOptions", "(", "\n", "column_types", "=", "FEATURES", ".", "type", ",", "\n", ")", "\n", "", "return", "convert_options", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.text.Text._info": [[72, 74], ["datasets.DatasetInfo"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "return", "datasets", ".", "DatasetInfo", "(", "features", "=", "FEATURES", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.text.Text._split_generators": [[75, 96], ["dl_manager.download_and_extract", "isinstance", "ValueError", "isinstance", "datasets.SplitGenerator", "isinstance", "splits.append", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"The `datafiles` kwarg in load_dataset() can be a str, List[str], Dict[str,str], or Dict[str,List[str]].\n        If str or List[str], then the dataset returns only the 'train' split.\n        If dict, then keys should be from the `datasets.Split` enum.\n        \"\"\"", "\n", "if", "not", "self", ".", "config", ".", "data_files", ":", "\n", "            ", "raise", "ValueError", "(", "f\"At least one data file must be specified, but got data_files={self.config.data_files}\"", ")", "\n", "", "data_files", "=", "dl_manager", ".", "download_and_extract", "(", "self", ".", "config", ".", "data_files", ")", "\n", "if", "isinstance", "(", "data_files", ",", "(", "str", ",", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "files", "=", "data_files", "\n", "if", "isinstance", "(", "files", ",", "str", ")", ":", "\n", "                ", "files", "=", "[", "files", "]", "\n", "", "return", "[", "datasets", ".", "SplitGenerator", "(", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "gen_kwargs", "=", "{", "\"files\"", ":", "files", "}", ")", "]", "\n", "", "splits", "=", "[", "]", "\n", "for", "split_name", "in", "[", "datasets", ".", "Split", ".", "TRAIN", ",", "datasets", ".", "Split", ".", "VALIDATION", ",", "datasets", ".", "Split", ".", "TEST", "]", ":", "\n", "            ", "if", "split_name", "in", "data_files", ":", "\n", "                ", "files", "=", "data_files", "[", "split_name", "]", "\n", "if", "isinstance", "(", "files", ",", "str", ")", ":", "\n", "                    ", "files", "=", "[", "files", "]", "\n", "", "splits", ".", "append", "(", "datasets", ".", "SplitGenerator", "(", "name", "=", "split_name", ",", "gen_kwargs", "=", "{", "\"files\"", ":", "files", "}", ")", ")", "\n", "", "", "return", "splits", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.text.Text._generate_tables": [[97, 109], ["enumerate", "pyarrow.read_csv"], "methods", ["None"], ["", "def", "_generate_tables", "(", "self", ",", "files", ")", ":", "\n", "        ", "for", "i", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "            ", "pa_table", "=", "pac", ".", "read_csv", "(", "\n", "file", ",", "\n", "read_options", "=", "self", ".", "config", ".", "pa_read_options", ",", "\n", "parse_options", "=", "self", ".", "config", ".", "pa_parse_options", ",", "\n", "convert_options", "=", "self", ".", "config", ".", "convert_options", ",", "\n", ")", "\n", "# Uncomment for debugging (will print the Arrow table size and elements)", "\n", "# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")", "\n", "# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))", "\n", "yield", "i", ",", "pa_table", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_synonym": [[15, 36], ["set", "nltk.corpus.wordnet.synsets", "list", "list", "synset.lemma_names", "eval", "word.replace.replace", "set.add", "word.replace.lower", "text.lower", "word.replace.lower", "lemma.lower"], "function", ["None"], ["def", "get_synonym", "(", "token", ")", ":", "\n", "    ", "lemma", "=", "token", ".", "lemma_", "\n", "text", "=", "token", ".", "text", "\n", "tag", "=", "token", ".", "tag_", "\n", "pos", "=", "token", ".", "pos_", "\n", "word_synset", "=", "set", "(", ")", "\n", "if", "pos", "not", "in", "REPLACE_POS", ":", "\n", "        ", "return", "list", "(", "word_synset", ")", "\n", "\n", "", "synsets", "=", "wn", ".", "synsets", "(", "text", ",", "pos", "=", "eval", "(", "\"wn.\"", "+", "pos", ")", ")", "\n", "for", "synset", "in", "synsets", ":", "\n", "        ", "words", "=", "synset", ".", "lemma_names", "(", ")", "\n", "for", "word", "in", "words", ":", "\n", "#word = wnl.lemmatize(word, pos=eval(\"wn.\"+pos))", "\n", "            ", "if", "word", ".", "lower", "(", ")", "!=", "text", ".", "lower", "(", ")", "and", "word", ".", "lower", "(", ")", "!=", "lemma", ".", "lower", "(", ")", ":", "\n", "# inflt = getInflection(word, tag=tag)", "\n", "# word = inflt[0] if len(inflt) else word", "\n", "                ", "word", "=", "word", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "word_synset", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "return", "list", "(", "word_synset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_hypernyms": [[38, 60], ["set", "nltk.corpus.wordnet.synsets", "list", "list", "synset.hypernyms", "eval", "hyperset.lemma_names", "word.replace.replace", "set.add", "word.replace.lower", "text.lower", "word.replace.lower", "lemma.lower"], "function", ["None"], ["", "def", "get_hypernyms", "(", "token", ")", ":", "\n", "    ", "lemma", "=", "token", ".", "lemma_", "\n", "text", "=", "token", ".", "text", "\n", "tag", "=", "token", ".", "tag_", "\n", "pos", "=", "token", ".", "pos_", "\n", "word_hypernyms", "=", "set", "(", ")", "\n", "if", "pos", "not", "in", "REPLACE_POS", ":", "\n", "        ", "return", "list", "(", "word_hypernyms", ")", "\n", "\n", "", "synsets", "=", "wn", ".", "synsets", "(", "text", ",", "pos", "=", "eval", "(", "\"wn.\"", "+", "pos", ")", ")", "\n", "for", "synset", "in", "synsets", ":", "\n", "        ", "for", "hyperset", "in", "synset", ".", "hypernyms", "(", ")", ":", "\n", "            ", "words", "=", "hyperset", ".", "lemma_names", "(", ")", "\n", "for", "word", "in", "words", ":", "\n", "#word = wnl.lemmatize(word, pos=eval(\"wn.\"+pos))", "\n", "                ", "if", "word", ".", "lower", "(", ")", "!=", "text", ".", "lower", "(", ")", "and", "word", ".", "lower", "(", ")", "!=", "lemma", ".", "lower", "(", ")", ":", "\n", "# inflt = getInflection(word, tag=tag)", "\n", "# word = inflt[0] if len(inflt) else word", "\n", "                    ", "word", "=", "word", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "word_hypernyms", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "", "return", "list", "(", "word_hypernyms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_antonym": [[62, 84], ["set", "nltk.corpus.wordnet.synsets", "list", "list", "synset.lemmas", "eval", "synlemma.antonyms", "antonym.name", "word.replace.replace", "set.add", "word.replace.lower", "text.lower", "word.replace.lower", "lemma.lower"], "function", ["None"], ["", "def", "get_antonym", "(", "token", ")", ":", "\n", "    ", "lemma", "=", "token", ".", "lemma_", "\n", "text", "=", "token", ".", "text", "\n", "tag", "=", "token", ".", "tag_", "\n", "pos", "=", "token", ".", "pos_", "\n", "word_antonym", "=", "set", "(", ")", "\n", "if", "pos", "not", "in", "REPLACE_POS", ":", "\n", "        ", "return", "list", "(", "word_antonym", ")", "\n", "\n", "", "synsets", "=", "wn", ".", "synsets", "(", "text", ",", "pos", "=", "eval", "(", "\"wn.\"", "+", "pos", ")", ")", "\n", "for", "synset", "in", "synsets", ":", "\n", "        ", "for", "synlemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "            ", "for", "antonym", "in", "synlemma", ".", "antonyms", "(", ")", ":", "\n", "                ", "word", "=", "antonym", ".", "name", "(", ")", "\n", "#word = wnl.lemmatize(word, pos=eval(\"wn.\"+pos))", "\n", "if", "word", ".", "lower", "(", ")", "!=", "text", ".", "lower", "(", ")", "and", "word", ".", "lower", "(", ")", "!=", "lemma", ".", "lower", "(", ")", ":", "\n", "# inflt = getInflection(word, tag=tag)", "\n", "# word = inflt[0] if len(inflt) else word", "\n", "                    ", "word", "=", "word", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "word_antonym", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "", "return", "list", "(", "word_antonym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_lemminflect": [[86, 104], ["set", "list", "list", "lemminflect.getInflection", "word.lower", "text.lower", "set.add"], "function", ["None"], ["", "def", "get_lemminflect", "(", "token", ")", ":", "\n", "    ", "text", "=", "token", ".", "text", "\n", "lemma", "=", "token", ".", "lemma_", "\n", "tag", "=", "token", ".", "tag_", "\n", "pos", "=", "token", ".", "pos_", "\n", "word_lemminflect", "=", "set", "(", ")", "\n", "if", "pos", "not", "in", "REPLACE_POS", ":", "\n", "        ", "return", "list", "(", "word_lemminflect", ")", "\n", "\n", "", "tags", "=", "POS_TO_TAGS", "[", "pos", "]", "\n", "for", "tg", "in", "tags", ":", "\n", "        ", "if", "tg", "==", "tag", ":", "continue", "\n", "inflects", "=", "getInflection", "(", "lemma", ",", "tag", "=", "tg", ")", "\n", "for", "word", "in", "inflects", ":", "\n", "            ", "if", "word", ".", "lower", "(", ")", "!=", "text", ".", "lower", "(", ")", ":", "\n", "                ", "word_lemminflect", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "return", "list", "(", "word_lemminflect", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.run_glue.main": [[70, 245], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.warning", "logger.info", "transformers.set_seed", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForSequenceClassification.from_pretrained", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.exists", "os.listdir", "ValueError", "bool", "transformers.GlueDataset", "transformers.GlueDataset", "transformers.GlueDataset", "transformers.Trainer.train", "transformers.Trainer.save_model", "transformers.Trainer.is_world_master", "logger.info", "logging.info", "len", "ValueError", "bool", "transformers.glue_compute_metrics", "run_glue.main.build_compute_metrics_fn"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "training_args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "training_args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "training_args", ".", "local_rank", ",", "\n", "training_args", ".", "device", ",", "\n", "training_args", ".", "n_gpu", ",", "\n", "bool", "(", "training_args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "training_args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "try", ":", "\n", "        ", "num_labels", "=", "glue_tasks_num_labels", "[", "data_args", ".", "task_name", "]", "\n", "output_mode", "=", "glue_output_modes", "[", "data_args", ".", "task_name", "]", "\n", "", "except", "KeyError", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "data_args", ".", "task_name", ")", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "\n", "", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", "if", "model_args", ".", "config_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "finetuning_task", "=", "data_args", ".", "task_name", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "if", "model_args", ".", "tokenizer_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "model", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "\n", "# Get datasets", "\n", "train_dataset", "=", "(", "\n", "GlueDataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "if", "training_args", ".", "do_train", "else", "None", "\n", ")", "\n", "eval_dataset", "=", "(", "\n", "GlueDataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "mode", "=", "\"dev\"", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "if", "training_args", ".", "do_eval", "\n", "else", "None", "\n", ")", "\n", "test_dataset", "=", "(", "\n", "GlueDataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "mode", "=", "\"test\"", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "if", "training_args", ".", "do_predict", "\n", "else", "None", "\n", ")", "\n", "\n", "def", "build_compute_metrics_fn", "(", "task_name", ":", "str", ")", "->", "Callable", "[", "[", "EvalPrediction", "]", ",", "Dict", "]", ":", "\n", "        ", "def", "compute_metrics_fn", "(", "p", ":", "EvalPrediction", ")", ":", "\n", "            ", "preds", "=", "p", ".", "predictions", "[", "0", "]", "if", "isinstance", "(", "p", ".", "predictions", ",", "tuple", ")", "else", "p", ".", "predictions", "\n", "if", "output_mode", "==", "\"classification\"", ":", "\n", "                ", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "# regression", "\n", "                ", "preds", "=", "np", ".", "squeeze", "(", "preds", ")", "\n", "", "return", "glue_compute_metrics", "(", "task_name", ",", "preds", ",", "p", ".", "label_ids", ")", "\n", "\n", "", "return", "compute_metrics_fn", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "eval_dataset", ",", "\n", "compute_metrics", "=", "build_compute_metrics_fn", "(", "data_args", ".", "task_name", ")", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "trainer", ".", "train", "(", "\n", "model_path", "=", "model_args", ".", "model_name_or_path", "if", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", "else", "None", "\n", ")", "\n", "trainer", ".", "save_model", "(", ")", "\n", "# For convenience, we also re-save the tokenizer to the same directory,", "\n", "# so that you can share your model easily on huggingface.co/models =)", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "tokenizer", ".", "save_pretrained", "(", "training_args", ".", "output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "", "eval_results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "# Loop to handle MNLI double evaluation (matched, mis-matched)", "\n", "eval_datasets", "=", "[", "eval_dataset", "]", "\n", "if", "data_args", ".", "task_name", "==", "\"mnli\"", ":", "\n", "            ", "mnli_mm_data_args", "=", "dataclasses", ".", "replace", "(", "data_args", ",", "task_name", "=", "\"mnli-mm\"", ")", "\n", "eval_datasets", ".", "append", "(", "\n", "GlueDataset", "(", "mnli_mm_data_args", ",", "tokenizer", "=", "tokenizer", ",", "mode", "=", "\"dev\"", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", ")", "\n", "\n", "", "for", "eval_dataset", "in", "eval_datasets", ":", "\n", "            ", "trainer", ".", "compute_metrics", "=", "build_compute_metrics_fn", "(", "eval_dataset", ".", "args", ".", "task_name", ")", "\n", "eval_result", "=", "trainer", ".", "evaluate", "(", "eval_dataset", "=", "eval_dataset", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "\n", "training_args", ".", "output_dir", ",", "f\"eval_results_{eval_dataset.args.task_name}.txt\"", "\n", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "                ", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                    ", "logger", ".", "info", "(", "\"***** Eval results {} *****\"", ".", "format", "(", "eval_dataset", ".", "args", ".", "task_name", ")", ")", "\n", "for", "key", ",", "value", "in", "eval_result", ".", "items", "(", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "value", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "value", ")", ")", "\n", "\n", "", "", "", "eval_results", ".", "update", "(", "eval_result", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "logging", ".", "info", "(", "\"*** Test ***\"", ")", "\n", "test_datasets", "=", "[", "test_dataset", "]", "\n", "if", "data_args", ".", "task_name", "==", "\"mnli\"", ":", "\n", "            ", "mnli_mm_data_args", "=", "dataclasses", ".", "replace", "(", "data_args", ",", "task_name", "=", "\"mnli-mm\"", ")", "\n", "test_datasets", ".", "append", "(", "\n", "GlueDataset", "(", "mnli_mm_data_args", ",", "tokenizer", "=", "tokenizer", ",", "mode", "=", "\"test\"", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", ")", "\n", "\n", "", "for", "test_dataset", "in", "test_datasets", ":", "\n", "            ", "predictions", "=", "trainer", ".", "predict", "(", "test_dataset", "=", "test_dataset", ")", ".", "predictions", "\n", "if", "output_mode", "==", "\"classification\"", ":", "\n", "                ", "predictions", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "1", ")", "\n", "\n", "", "output_test_file", "=", "os", ".", "path", ".", "join", "(", "\n", "training_args", ".", "output_dir", ",", "f\"test_results_{test_dataset.args.task_name}.txt\"", "\n", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "                ", "with", "open", "(", "output_test_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                    ", "logger", ".", "info", "(", "\"***** Test results {} *****\"", ".", "format", "(", "test_dataset", ".", "args", ".", "task_name", ")", ")", "\n", "writer", ".", "write", "(", "\"index\\tprediction\\n\"", ")", "\n", "for", "index", ",", "item", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                        ", "if", "output_mode", "==", "\"regression\"", ":", "\n", "                            ", "writer", ".", "write", "(", "\"%d\\t%3.3f\\n\"", "%", "(", "index", ",", "item", ")", ")", "\n", "", "else", ":", "\n", "                            ", "item", "=", "test_dataset", ".", "get_labels", "(", ")", "[", "item", "]", "\n", "writer", ".", "write", "(", "\"%d\\t%s\\n\"", "%", "(", "index", ",", "item", ")", ")", "\n", "", "", "", "", "", "", "return", "eval_results", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.run_glue._mp_fn": [[247, 250], ["run_glue.main"], "function", ["home.repos.pwc.inspect_result.kandorm_CLINE.src.run.main"], ["", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.run.main": [[50, 234], ["transformers.HfArgumentParser", "transformers.HfArgumentParser.parse_args_into_dataclasses", "logging.basicConfig", "logger.warning", "logger.info", "transformers.set_seed", "AutoModelForPreTraining.from_config.resize_token_embeddings", "transformers.Trainer", "ValueError", "os.path.exists", "os.listdir", "bool", "AutoConfig.from_pretrained", "AutoTokenizer.from_pretrained", "AutoModelForPreTraining.from_pretrained", "logger.info", "len", "ValueError", "min", "dataloader.get_dataset", "dataloader.get_dataset", "lecbert.DataCollatorForLEC", "transformers.Trainer.train", "transformers.Trainer.save_model", "transformers.Trainer.is_world_process_zero", "logger.info", "transformers.Trainer.evaluate", "math.exp", "os.path.join", "transformers.Trainer.is_world_master", "results.update", "os.listdir", "os.listdir.sort", "os.path.join", "logger.info", "AutoConfig.from_pretrained", "logger.warning", "AutoTokenizer.from_pretrained", "ValueError", "AutoModelForPreTraining", "AutoModelForPreTraining.from_config", "transformers.DataCollatorForPermutationLanguageModeling", "transformers.DataCollatorForLanguageModeling", "AutoTokenizer.from_pretrained.save_pretrained", "bool", "os.path.isdir", "open", "logger.info", "sorted", "result.keys", "logger.info", "writer.write", "int", "str", "x.split", "str"], "function", ["home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_dataset", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_dataset"], ["def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "if", "data_args", ".", "eval_data_file", "is", "None", "and", "training_args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"", "\n", "\"or remove the --do_eval argument.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "training_args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "\n", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "training_args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", ")", ":", "\n", "        ", "if", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "            ", "ckt", "=", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", "\n", "ckt", ".", "sort", "(", "key", "=", "lambda", "x", ":", "int", "(", "x", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", ")", "\n", "model_args", ".", "model_name_or_path", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "ckt", "[", "-", "1", "]", ")", "\n", "logger", ".", "info", "(", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. Training from checkout %s.\"", ",", "model_args", ".", "model_name_or_path", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "training_args", ".", "local_rank", ",", "\n", "training_args", ".", "device", ",", "\n", "training_args", ".", "n_gpu", ",", "\n", "bool", "(", "training_args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "training_args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "\n", "if", "model_args", ".", "model_type", "in", "[", "\"lecbert\"", "]", ":", "\n", "        ", "from", "lecbert", "import", "LecbertConfig", "as", "AutoConfig", "\n", "from", "lecbert", "import", "LecbertTokenizer", "as", "AutoTokenizer", "\n", "from", "lecbert", "import", "LecbertForPreTraining", "as", "AutoModelForPreTraining", "\n", "", "else", ":", "\n", "        ", "from", "transformers", "import", "AutoConfig", ",", "AutoTokenizer", ",", "AutoModelForPreTraining", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "elif", "model_args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "config_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "elif", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "config", "=", "config", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"", "\n", "\"and load it from here, using --tokenizer_name\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelForPreTraining", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "if", "model_args", ".", "model_type", "==", "\"lecbert\"", ":", "\n", "            ", "model", "=", "AutoModelForPreTraining", "(", "config", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "AutoModelForPreTraining", ".", "from_config", "(", "config", ")", "\n", "\n", "", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "if", "config", ".", "model_type", "in", "[", "\"bert\"", ",", "\"roberta\"", ",", "\"distilbert\"", ",", "\"camembert\"", "]", "and", "not", "data_args", ".", "mlm", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the\"", "\n", "\"--mlm flag (masked language modeling).\"", "\n", ")", "\n", "\n", "", "if", "data_args", ".", "block_size", "<=", "0", ":", "\n", "        ", "data_args", ".", "block_size", "=", "tokenizer", ".", "model_max_length", "\n", "# Our input block size will be the max possible for the model", "\n", "", "else", ":", "\n", "        ", "data_args", ".", "block_size", "=", "min", "(", "data_args", ".", "block_size", ",", "tokenizer", ".", "model_max_length", ")", "\n", "\n", "# Get datasets", "\n", "\n", "", "train_dataset", "=", "(", "\n", "get_dataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "if", "training_args", ".", "do_train", "\n", "else", "None", "\n", ")", "\n", "\n", "eval_dataset", "=", "(", "\n", "get_dataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "evaluate", "=", "True", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "if", "training_args", ".", "do_eval", "\n", "else", "None", "\n", ")", "\n", "\n", "if", "model_args", ".", "model_type", "==", "\"lecbert\"", ":", "\n", "        ", "data_collator", "=", "DataCollatorForLEC", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "mlm", "=", "data_args", ".", "mlm", ",", "\n", "mlm_probability", "=", "data_args", ".", "mlm_probability", ",", "\n", "block_size", "=", "data_args", ".", "block_size", "\n", ")", "\n", "", "elif", "model_args", ".", "model_type", "==", "\"xlnet\"", ":", "\n", "        ", "data_collator", "=", "DataCollatorForPermutationLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "plm_probability", "=", "data_args", ".", "plm_probability", ",", "\n", "max_span_length", "=", "data_args", ".", "max_span_length", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "mlm", "=", "data_args", ".", "mlm", ",", "\n", "mlm_probability", "=", "data_args", ".", "mlm_probability", "\n", ")", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "eval_dataset", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "model_path", "=", "(", "\n", "model_args", ".", "model_name_or_path", "\n", "if", "model_args", ".", "model_name_or_path", "is", "not", "None", "and", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", "\n", "else", "None", "\n", ")", "\n", "trainer", ".", "train", "(", "model_path", "=", "model_path", ")", "\n", "trainer", ".", "save_model", "(", "training_args", ".", "output_dir", ")", "\n", "# For convenience, we also re-save the tokenizer to the same directory,", "\n", "# so that you can share your model easily on huggingface.co/models =)", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "tokenizer", ".", "save_pretrained", "(", "training_args", ".", "output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "", "results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "eval_output", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "perplexity", "=", "math", ".", "exp", "(", "eval_output", "[", "\"eval_loss\"", "]", ")", "\n", "result", "=", "{", "\"perplexity\"", ":", "perplexity", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"eval_results_lm.txt\"", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "results", ".", "update", "(", "result", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_replace_label": [[158, 196], ["enumerate", "len", "len", "len", "len", "len", "len", "orig_sent[].encode", "len", "len", "len"], "function", ["None"], ["", "def", "get_replace_label", "(", "args", ",", "word_list", ",", "repl_intv", ",", "orig_sent", ")", ":", "\n", "    ", "label", "=", "[", "REPLACE_NONE", "]", "*", "len", "(", "word_list", ")", "\n", "if", "not", "repl_intv", ":", "\n", "        ", "return", "label", "\n", "", "byte_index", "=", "0", "# point to the start of the next token in the byte type sentence", "\n", "orig_index", "=", "0", "# point to the start of the next token in the utf-8 type sentence", "\n", "cur_range", "=", "0", "\n", "cur_start", ",", "cur_end", ",", "cur_label", "=", "repl_intv", "[", "cur_range", "]", "# raplacement range is of increasing ordered (include spaces in text)", "\n", "for", "index", ",", "word", "in", "enumerate", "(", "word_list", ")", ":", "\n", "        ", "if", "byte_index", ">=", "cur_start", "and", "byte_index", "<=", "cur_end", ":", "# word piece is in replacement range", "\n", "            ", "label", "[", "index", "]", "=", "cur_label", "\n", "\n", "", "if", "args", ".", "preprocess_model_type", "in", "[", "'roberta'", "]", ":", "\n", "            ", "byte_offset", "=", "len", "(", "word", ")", "# bytelevel contains spaces in the token", "\n", "", "elif", "args", ".", "preprocess_model_type", "in", "[", "'bert'", ",", "'electra'", "]", ":", "\n", "            ", "if", "word", "[", ":", "2", "]", "==", "'##'", ":", "\n", "                ", "orig_offset", "=", "len", "(", "word", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "if", "index", "==", "0", "or", "orig_sent", "[", "orig_index", "]", "!=", "\" \"", ":", "\n", "                    ", "orig_offset", "=", "len", "(", "word", ")", "\n", "", "else", ":", "\n", "                    ", "orig_offset", "=", "len", "(", "word", ")", "+", "1", "\n", "", "", "byte_offset", "=", "len", "(", "orig_sent", "[", "orig_index", ":", "orig_index", "+", "orig_offset", "]", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "orig_index", "+=", "orig_offset", "\n", "", "else", ":", "\n", "            ", "byte_offset", "=", "len", "(", "word", ")", "\n", "\n", "", "byte_index", "+=", "byte_offset", "# bytelevel contains spaces in the token", "\n", "if", "byte_index", ">", "cur_end", ":", "# update replacement range", "\n", "            ", "if", "cur_range", "!=", "len", "(", "repl_intv", ")", "-", "1", ":", "# not the last range", "\n", "                ", "cur_range", "+=", "1", "\n", "cur_start", ",", "cur_end", ",", "cur_label", "=", "repl_intv", "[", "cur_range", "]", "\n", "", "else", ":", "# no new range", "\n", "                ", "break", "\n", "\n", "", "", "", "assert", "cur_range", "==", "len", "(", "repl_intv", ")", "-", "1", "\n", "\n", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_dataset": [[198, 369], ["os.path.isdir", "datasets.load_dataset", "datasets.Dataset.load_from_disk", "os.listdir", "tokenizer.convert_tokens_to_ids", "range", "tokenizer.tokenize", "range", "spacy_nlp.pipe", "len", "range", "dataset.map.map", "dataset.map.set_format", "os.path.join", "tokenizer.num_special_tokens_to_add", "tokenizer.tokenize", "outputs.append", "tokenizer.num_special_tokens_to_add", "inputs.append", "original_sent.append", "ori_syn_intv.append", "ori_ant_intv.append", "synonym_sent.append", "synonym_intv.append", "antonym_sent.append", "antonym_intv.append", "tokenizer.tokenize", "tokenizer.tokenize", "tokenizer.tokenize", "dataloader.get_replace_label", "dataloader.get_replace_label", "dataloader.get_replace_label", "dataloader.get_replace_label", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "input_ids.append", "ori_syn_label.append", "ori_ant_label.append", "synonym_ids.append", "synonym_label.append", "antonym_ids.append", "antonym_label.append", "dataset.map.map", "dataset.map.set_format", "dataset.map.map", "dataset.map.set_format", "tokenizer.build_inputs_with_special_tokens", "tokenizer.convert_tokens_to_string", "tokenizer", "os.path.exists", "datasets.Dataset.load_from_disk", "dataset.map.map", "dataset.map.set_format", "dataset.map.save_to_disk", "len", "len", "get_replace_label.count", "get_replace_label.count", "get_replace_label.count", "get_replace_label.count", "get_replace_label.count", "get_replace_label.count", "get_replace_label.count", "get_replace_label.count"], "function", ["home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_replace_label", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_replace_label", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_replace_label", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.get_replace_label", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.build_inputs_with_special_tokens"], ["", "def", "get_dataset", "(", "\n", "args", ":", "DataTrainingArguments", ",", "\n", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "evaluate", ":", "bool", "=", "False", ",", "\n", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "spacy_nlp", "=", "None", "\n", ")", ":", "\n", "    ", "file_path", "=", "args", ".", "eval_data_file", "if", "evaluate", "else", "args", ".", "train_data_file", "\n", "if", "args", ".", "load_from_disk", ":", "\n", "        ", "return", "Dataset", ".", "load_from_disk", "(", "file_path", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "file_path", ")", ":", "\n", "        ", "file_names", "=", "os", ".", "listdir", "(", "file_path", ")", "\n", "file_path", "=", "[", "os", ".", "path", ".", "join", "(", "file_path", ",", "fn", ")", "for", "fn", "in", "file_names", "]", "\n", "\n", "", "dataset", "=", "load_dataset", "(", "\"src/text.py\"", ",", "data_files", "=", "file_path", ",", "split", "=", "\"train\"", ",", "cache_dir", "=", "cache_dir", ",", "ignore_verifications", "=", "True", ")", "\n", "\n", "\n", "def", "lines_to_block", "(", "examples", ")", ":", "\n", "        ", "outputs", "=", "[", "]", "\n", "block_size", "=", "args", ".", "block_size", "-", "tokenizer", ".", "num_special_tokens_to_add", "(", "pair", "=", "False", ")", "\n", "lines", "=", "examples", "[", "'text'", "]", "\n", "text", "=", "\"\\n\"", ".", "join", "(", "lines", ")", "\n", "tokenized_text", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tokenized_text", ")", "-", "block_size", "+", "1", ",", "block_size", ")", ":", "# Truncate in block of block_size", "\n", "            ", "outputs", ".", "append", "(", "\n", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "tokenized_text", "[", "i", ":", "i", "+", "block_size", "]", ")", "\n", ")", "\n", "# Note that we are losing the last truncated example here for the sake of simplicity (no padding)", "\n", "# If your dataset is small, first you should loook for a bigger one :-) and second you", "\n", "# can change this behavior by adding (model specific) padding.", "\n", "", "return", "{", "'input_ids'", ":", "outputs", "}", "\n", "\n", "\n", "", "def", "word_replace", "(", "examples", ")", ":", "\n", "        ", "inputs", "=", "[", "]", "\n", "block_size", "=", "args", ".", "block_size", "-", "tokenizer", ".", "num_special_tokens_to_add", "(", "pair", "=", "False", ")", "\n", "lines", "=", "examples", "[", "'text'", "]", "\n", "text", "=", "\"\\n\"", ".", "join", "(", "lines", ")", "\n", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tokenized_text", ")", "-", "block_size", "+", "1", ",", "block_size", ")", ":", "# Truncate in block of block_size", "\n", "            ", "inputs", ".", "append", "(", "tokenizer", ".", "convert_tokens_to_string", "(", "tokenized_text", "[", "i", ":", "i", "+", "block_size", "]", ")", ")", "\n", "\n", "# inputs = examples['text']", "\n", "\n", "", "original_sent", "=", "[", "]", "\n", "ori_syn_intv", "=", "[", "]", "\n", "ori_ant_intv", "=", "[", "]", "\n", "synonym_sent", "=", "[", "]", "\n", "synonym_intv", "=", "[", "]", "\n", "antonym_sent", "=", "[", "]", "\n", "antonym_intv", "=", "[", "]", "\n", "\n", "docs", "=", "spacy_nlp", ".", "pipe", "(", "inputs", ",", "n_process", "=", "1", ",", "batch_size", "=", "100", ",", "disable", "=", "[", "'parser'", ",", "'ner'", "]", ")", "\n", "for", "doc", "in", "docs", ":", "\n", "            ", "ori_sent", "=", "\" \"", ".", "join", "(", "[", "t", ".", "text", "for", "t", "in", "doc", "]", ")", "\n", "syn_sent", "=", "\" \"", ".", "join", "(", "doc", ".", "_", ".", "_synonym_sent", ")", "\n", "ant_sent", "=", "\" \"", ".", "join", "(", "doc", ".", "_", ".", "_antonym_sent", ")", "\n", "\n", "syn_intv", "=", "doc", ".", "_", ".", "_synonym_intv", "\n", "ant_intv", "=", "doc", ".", "_", ".", "_antonym_intv", "\n", "os_intv", "=", "doc", ".", "_", ".", "_ori_syn_intv", "\n", "oa_intv", "=", "doc", ".", "_", ".", "_ori_ant_intv", "\n", "\n", "original_sent", ".", "append", "(", "ori_sent", ")", "\n", "ori_syn_intv", ".", "append", "(", "os_intv", ")", "\n", "ori_ant_intv", ".", "append", "(", "oa_intv", ")", "\n", "synonym_sent", ".", "append", "(", "syn_sent", ")", "\n", "synonym_intv", ".", "append", "(", "syn_intv", ")", "\n", "antonym_sent", ".", "append", "(", "ant_sent", ")", "\n", "antonym_intv", ".", "append", "(", "ant_intv", ")", "\n", "\n", "", "return", "{", "'original_sent'", ":", "original_sent", ",", "\n", "'ori_syn_intv'", ":", "ori_syn_intv", ",", "\n", "'ori_ant_intv'", ":", "ori_ant_intv", ",", "\n", "'synonym_sent'", ":", "synonym_sent", ",", "\n", "'synonym_intv'", ":", "synonym_intv", ",", "\n", "'antonym_sent'", ":", "antonym_sent", ",", "\n", "'antonym_intv'", ":", "antonym_intv", "}", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "examples", ")", ":", "\n", "        ", "input_ids", "=", "[", "]", "\n", "ori_syn_label", "=", "[", "]", "\n", "ori_ant_label", "=", "[", "]", "\n", "synonym_ids", "=", "[", "]", "\n", "synonym_label", "=", "[", "]", "\n", "antonym_ids", "=", "[", "]", "\n", "antonym_label", "=", "[", "]", "\n", "\n", "exp_nums", "=", "len", "(", "examples", "[", "'original_sent'", "]", ")", "\n", "for", "i", "in", "range", "(", "exp_nums", ")", ":", "\n", "            ", "ori_sent", "=", "tokenizer", ".", "tokenize", "(", "examples", "[", "'original_sent'", "]", "[", "i", "]", ")", "\n", "syn_sent", "=", "tokenizer", ".", "tokenize", "(", "examples", "[", "'synonym_sent'", "]", "[", "i", "]", ")", "\n", "ant_sent", "=", "tokenizer", ".", "tokenize", "(", "examples", "[", "'antonym_sent'", "]", "[", "i", "]", ")", "\n", "\n", "syn_labl", "=", "get_replace_label", "(", "args", ",", "syn_sent", ",", "examples", "[", "'synonym_intv'", "]", "[", "i", "]", ",", "examples", "[", "'synonym_sent'", "]", "[", "i", "]", ")", "\n", "ori_syn_labl", "=", "get_replace_label", "(", "args", ",", "ori_sent", ",", "examples", "[", "'ori_syn_intv'", "]", "[", "i", "]", ",", "examples", "[", "'original_sent'", "]", "[", "i", "]", ")", "\n", "ant_labl", "=", "get_replace_label", "(", "args", ",", "ant_sent", ",", "examples", "[", "'antonym_intv'", "]", "[", "i", "]", ",", "examples", "[", "'antonym_sent'", "]", "[", "i", "]", ")", "\n", "ori_ant_labl", "=", "get_replace_label", "(", "args", ",", "ori_sent", ",", "examples", "[", "'ori_ant_intv'", "]", "[", "i", "]", ",", "examples", "[", "'original_sent'", "]", "[", "i", "]", ")", "\n", "\n", "assert", "syn_labl", ".", "count", "(", "-", "100", ")", "==", "ori_syn_labl", ".", "count", "(", "-", "100", ")", "and", "syn_labl", ".", "count", "(", "0", ")", "==", "ori_syn_labl", ".", "count", "(", "0", ")", "\n", "assert", "ant_labl", ".", "count", "(", "-", "100", ")", "==", "ori_ant_labl", ".", "count", "(", "-", "100", ")", "and", "ant_labl", ".", "count", "(", "0", ")", "==", "ori_ant_labl", ".", "count", "(", "0", ")", "\n", "\n", "ori_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "ori_sent", ")", "\n", "syn_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "syn_sent", ")", "\n", "ant_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "ant_sent", ")", "\n", "\n", "input_ids", ".", "append", "(", "ori_ids", ")", "\n", "ori_syn_label", ".", "append", "(", "ori_syn_labl", ")", "\n", "ori_ant_label", ".", "append", "(", "ori_ant_labl", ")", "\n", "synonym_ids", ".", "append", "(", "syn_ids", ")", "\n", "synonym_label", ".", "append", "(", "syn_labl", ")", "\n", "antonym_ids", ".", "append", "(", "ant_ids", ")", "\n", "antonym_label", ".", "append", "(", "ant_labl", ")", "\n", "\n", "", "return", "{", "'input_ids'", ":", "input_ids", ",", "\n", "'ori_syn_label'", ":", "ori_syn_label", ",", "\n", "'ori_ant_label'", ":", "ori_ant_label", ",", "\n", "'synonym_ids'", ":", "synonym_ids", ",", "\n", "'synonym_label'", ":", "synonym_label", ",", "\n", "'antonym_ids'", ":", "antonym_ids", ",", "\n", "'antonym_label'", ":", "antonym_label", "}", "\n", "\n", "\n", "", "if", "args", ".", "line_by_line", ":", "\n", "        ", "dataset", "=", "dataset", ".", "map", "(", "lambda", "ex", ":", "tokenizer", "(", "ex", "[", "\"text\"", "]", ",", "add_special_tokens", "=", "True", ",", "\n", "truncation", "=", "True", ",", "max_length", "=", "args", ".", "block_size", ")", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "writer_batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "remove_columns", "=", "dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "True", ",", "\n", "cache_file_name", "=", "args", ".", "preprocess_cache_file", ")", "\n", "dataset", ".", "set_format", "(", "type", "=", "None", ",", "columns", "=", "[", "'input_ids'", "]", ")", "\n", "\n", "", "elif", "args", ".", "word_replace", ":", "\n", "        ", "if", "args", ".", "word_replace_file", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "word_replace_file", ")", ":", "\n", "            ", "dataset", "=", "Dataset", ".", "load_from_disk", "(", "args", ".", "word_replace_file", ")", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "dataset", ".", "map", "(", "word_replace", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "writer_batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "remove_columns", "=", "dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "True", ",", "\n", "cache_file_name", "=", "args", ".", "preprocess_cache_file", ")", "\n", "dataset", ".", "set_format", "(", "type", "=", "None", ",", "columns", "=", "[", "'original_sent'", ",", "'ori_syn_intv'", ",", "'ori_ant_intv'", ",", "\n", "'synonym_sent'", ",", "'synonym_intv'", ",", "'antonym_sent'", ",", "'antonym_intv'", "]", ")", "\n", "dataset", ".", "save_to_disk", "(", "args", ".", "word_replace_file", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "map", "(", "convert_tokens_to_ids", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "writer_batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "remove_columns", "=", "dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ")", "\n", "\n", "dataset", ".", "set_format", "(", "type", "=", "None", ",", "columns", "=", "[", "'input_ids'", ",", "'ori_syn_label'", ",", "'ori_ant_label'", ",", "\n", "'synonym_ids'", ",", "'synonym_label'", ",", "'antonym_ids'", ",", "'antonym_label'", "]", ")", "\n", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "dataset", ".", "map", "(", "lines_to_block", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "writer_batch_size", "=", "args", ".", "preprocess_batch_size", ",", "\n", "remove_columns", "=", "dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "True", ",", "\n", "cache_file_name", "=", "args", ".", "preprocess_cache_file", ")", "\n", "dataset", ".", "set_format", "(", "type", "=", "None", ",", "columns", "=", "[", "'input_ids'", "]", ")", "\n", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.search_replacement": [[371, 406], ["wordnet.get_antonym", "sr_rep.append", "len", "random.choice", "rw.random_word.lower", "token.text.lower", "random.choice", "rw.random_word", "wordnet.get_synonym", "random.choice", "wordnet.get_hypernyms", "random.choice", "wordnet.get_lemminflect", "random.choice"], "function", ["home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_antonym", "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_synonym", "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_hypernyms", "home.repos.pwc.inspect_result.kandorm_CLINE.src.wordnet.get_lemminflect"], ["", "def", "search_replacement", "(", "doc", ",", "candidate_index", ",", "replace_type", ",", "max_num", ",", "pos_to_words", "=", "None", ")", ":", "\n", "    ", "sr_rep", "=", "[", "]", "\n", "if", "max_num", "<", "1", ":", "\n", "        ", "return", "sr_rep", "\n", "\n", "", "for", "r_idx", "in", "candidate_index", ":", "\n", "        ", "token", "=", "doc", "[", "r_idx", "]", "\n", "rep", "=", "None", "\n", "if", "replace_type", "==", "REPLACE_ANTONYM", ":", "\n", "            ", "reps", "=", "get_antonym", "(", "token", ")", "\n", "rep", "=", "random", ".", "choice", "(", "reps", ")", "if", "reps", "else", "None", "\n", "", "elif", "replace_type", "==", "REPLACE_ADJACENCY", ":", "\n", "            ", "reps", "=", "pos_to_words", "[", "token", ".", "pos_", "]", "\n", "rep", "=", "random", ".", "choice", "(", "reps", ")", "if", "reps", "else", "None", "\n", "", "elif", "replace_type", "==", "REPLACE_RANDOM", ":", "\n", "            ", "rep", "=", "rw", ".", "random_word", "(", ")", "\n", "", "elif", "replace_type", "==", "REPLACE_SYNONYM", ":", "\n", "            ", "reps", "=", "get_synonym", "(", "token", ")", "\n", "rep", "=", "random", ".", "choice", "(", "reps", ")", "if", "reps", "else", "None", "\n", "", "elif", "replace_type", "==", "REPLACE_HYPERNYMS", ":", "\n", "            ", "reps", "=", "get_hypernyms", "(", "token", ")", "\n", "rep", "=", "random", ".", "choice", "(", "reps", ")", "if", "reps", "else", "None", "\n", "", "elif", "replace_type", "==", "REPLACE_LEMMINFLECT", ":", "\n", "            ", "reps", "=", "get_lemminflect", "(", "token", ")", "\n", "rep", "=", "random", ".", "choice", "(", "reps", ")", "if", "reps", "else", "None", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "if", "rep", "and", "rep", ".", "lower", "(", ")", "!=", "token", ".", "text", ".", "lower", "(", ")", ":", "\n", "            ", "sr_rep", ".", "append", "(", "(", "r_idx", ",", "rep", ",", "replace_type", ")", ")", "\n", "\n", "", "if", "len", "(", "sr_rep", ")", ">=", "max_num", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "sr_rep", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.replace_word": [[408, 508], ["len", "int", "enumerate", "min", "random.random", "random.random", "random.shuffle", "random.shuffle", "sorted", "enumerate", "len", "dataloader.search_replacement", "dataloader.search_replacement", "dataloader.search_replacement", "dataloader.search_replacement", "dataloader.search_replacement", "sorted.pop", "synonym_sent.append", "antonym_sent.append", "rep_index.append", "synonym_intv.append", "ori_syn_intv.append", "sorted.pop", "synonym_intv.append", "ori_syn_intv.append", "antonym_intv.append", "ori_ant_intv.append", "len", "len", "len", "antonym_intv.append", "ori_ant_intv.append", "ori.encode", "syn.encode", "ant.encode", "len", "len", "len", "len", "len", "len", "syn.encode", "ori.encode", "len", "len", "syn.encode", "ori.encode", "ant.encode", "ori.encode", "ant.encode", "ori.encode"], "function", ["home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.search_replacement", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.search_replacement", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.search_replacement", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.search_replacement", "home.repos.pwc.inspect_result.kandorm_CLINE.src.dataloader.search_replacement"], ["", "def", "replace_word", "(", "doc", ")", ":", "\n", "    ", "synonym_sent", "=", "[", "]", "\n", "synonym_intv", "=", "[", "]", "\n", "ori_syn_intv", "=", "[", "]", "\n", "antonym_sent", "=", "[", "]", "\n", "antonym_intv", "=", "[", "]", "\n", "ori_ant_intv", "=", "[", "]", "\n", "\n", "length", "=", "len", "(", "doc", ")", "\n", "rep_num", "=", "int", "(", "length", "*", "REPLACE_RATIO", ")", "\n", "\n", "rep_index", "=", "[", "]", "\n", "# pos_word = {p:[] for p in REPLACE_POS}", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "doc", ")", ":", "\n", "        ", "if", "token", ".", "pos_", "in", "REPLACE_POS", ":", "\n", "            ", "rep_index", ".", "append", "(", "index", ")", "\n", "# pos_word[token.pos_].append(token.text)", "\n", "\n", "", "", "rep_num", "=", "min", "(", "rep_num", ",", "len", "(", "rep_index", ")", ")", "\n", "\n", "syn_rand", "=", "random", ".", "random", "(", ")", "\n", "ant_rand", "=", "random", ".", "random", "(", ")", "\n", "\n", "syn_index", "=", "rep_index", "[", ":", "]", "\n", "random", ".", "shuffle", "(", "syn_index", ")", "\n", "ant_index", "=", "rep_index", "[", ":", "]", "\n", "random", ".", "shuffle", "(", "ant_index", ")", "\n", "\n", "syn_replace", "=", "[", "]", "\n", "ant_replace", "=", "[", "]", "# [(rep_idx, rep_word, rep_type)]", "\n", "\n", "############### Antonym Replacement ####################", "\n", "if", "ant_rand", "<", "ANTONYM_RATIO", ":", "\n", "        ", "ant_replace", "=", "search_replacement", "(", "doc", ",", "candidate_index", "=", "ant_index", ",", "replace_type", "=", "REPLACE_ANTONYM", ",", "max_num", "=", "rep_num", ")", "\n", "\n", "# if not ant_replace and ant_rand < ANTONYM_RATIO + ADJACENCY_RATIO:", "\n", "#     ant_replace = search_replacement(doc, candidate_index=ant_index, replace_type=REPLACE_ADJACENCY, max_num=rep_num, pos_to_words=pos_word)", "\n", "\n", "", "if", "not", "ant_replace", ":", "\n", "        ", "ant_replace", "=", "search_replacement", "(", "doc", ",", "candidate_index", "=", "ant_index", ",", "replace_type", "=", "REPLACE_RANDOM", ",", "max_num", "=", "rep_num", ")", "\n", "\n", "############### Synonym Replacement ####################", "\n", "", "if", "syn_rand", "<", "HYPERNYMS_RATIO", ":", "\n", "        ", "syn_replace", "=", "search_replacement", "(", "doc", ",", "candidate_index", "=", "syn_index", ",", "replace_type", "=", "REPLACE_HYPERNYMS", ",", "max_num", "=", "rep_num", ")", "\n", "\n", "", "if", "not", "syn_replace", "and", "syn_rand", "<", "HYPERNYMS_RATIO", "+", "SYNONYM_RATIO", ":", "\n", "        ", "syn_replace", "=", "search_replacement", "(", "doc", ",", "candidate_index", "=", "syn_index", ",", "replace_type", "=", "REPLACE_SYNONYM", ",", "max_num", "=", "rep_num", ")", "\n", "\n", "", "if", "not", "syn_replace", ":", "\n", "        ", "syn_replace", "=", "search_replacement", "(", "doc", ",", "candidate_index", "=", "syn_index", ",", "replace_type", "=", "REPLACE_LEMMINFLECT", ",", "max_num", "=", "rep_num", ")", "\n", "\n", "############### Original Replacement ####################", "\n", "\n", "", "all_replace", "=", "ant_replace", "+", "syn_replace", "\n", "all_replace", "=", "sorted", "(", "all_replace", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "ori_len", "=", "-", "1", "# point to the space before next token", "\n", "syn_len", "=", "-", "1", "\n", "ant_len", "=", "-", "1", "\n", "rep_idx", ",", "rep_word", ",", "rep_type", "=", "all_replace", ".", "pop", "(", ")", "if", "all_replace", "else", "(", "None", ",", "None", ",", "None", ")", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "doc", ")", ":", "\n", "        ", "ori", "=", "syn", "=", "ant", "=", "token", ".", "text", "\n", "\n", "while", "index", "==", "rep_idx", ":", "\n", "            ", "if", "rep_type", "in", "[", "REPLACE_SYNONYM", ",", "REPLACE_HYPERNYMS", ",", "REPLACE_LEMMINFLECT", "]", ":", "\n", "                ", "syn", "=", "rep_word", "\n", "synonym_intv", ".", "append", "(", "(", "syn_len", ",", "syn_len", "+", "len", "(", "syn", ".", "encode", "(", "'utf-8'", ")", ")", ",", "rep_type", ")", ")", "# fix length mismatch, mx.encode for bytelevelbpe", "\n", "ori_syn_intv", ".", "append", "(", "(", "ori_len", ",", "ori_len", "+", "len", "(", "ori", ".", "encode", "(", "'utf-8'", ")", ")", ",", "rep_type", ")", ")", "\n", "", "elif", "rep_type", "in", "[", "REPLACE_ANTONYM", ",", "REPLACE_RANDOM", "]", ":", "\n", "                ", "ant", "=", "rep_word", "\n", "antonym_intv", ".", "append", "(", "(", "ant_len", ",", "ant_len", "+", "len", "(", "ant", ".", "encode", "(", "'utf-8'", ")", ")", ",", "rep_type", ")", ")", "\n", "ori_ant_intv", ".", "append", "(", "(", "ori_len", ",", "ori_len", "+", "len", "(", "ori", ".", "encode", "(", "'utf-8'", ")", ")", ",", "rep_type", ")", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "rep_idx", ",", "rep_word", ",", "rep_type", "=", "all_replace", ".", "pop", "(", ")", "if", "all_replace", "else", "(", "None", ",", "None", ",", "None", ")", "\n", "\n", "", "if", "index", "in", "rep_index", ":", "\n", "            ", "if", "ori", "==", "syn", ":", "\n", "                ", "synonym_intv", ".", "append", "(", "(", "syn_len", ",", "syn_len", "+", "len", "(", "syn", ".", "encode", "(", "'utf-8'", ")", ")", ",", "REPLACE_ORIGINAL", ")", ")", "\n", "ori_syn_intv", ".", "append", "(", "(", "ori_len", ",", "ori_len", "+", "len", "(", "ori", ".", "encode", "(", "'utf-8'", ")", ")", ",", "REPLACE_ORIGINAL", ")", ")", "\n", "", "if", "ori", "==", "ant", ":", "\n", "                ", "antonym_intv", ".", "append", "(", "(", "ant_len", ",", "ant_len", "+", "len", "(", "ant", ".", "encode", "(", "'utf-8'", ")", ")", ",", "REPLACE_ORIGINAL", ")", ")", "\n", "ori_ant_intv", ".", "append", "(", "(", "ori_len", ",", "ori_len", "+", "len", "(", "ori", ".", "encode", "(", "'utf-8'", ")", ")", ",", "REPLACE_ORIGINAL", ")", ")", "\n", "\n", "", "", "ori_len", "=", "ori_len", "+", "len", "(", "ori", ".", "encode", "(", "'utf-8'", ")", ")", "+", "1", "\n", "syn_len", "=", "syn_len", "+", "len", "(", "syn", ".", "encode", "(", "'utf-8'", ")", ")", "+", "1", "# +1 to point the space before next token", "\n", "ant_len", "=", "ant_len", "+", "len", "(", "ant", ".", "encode", "(", "'utf-8'", ")", ")", "+", "1", "\n", "\n", "synonym_sent", ".", "append", "(", "syn", ")", "\n", "antonym_sent", ".", "append", "(", "ant", ")", "\n", "\n", "", "doc", ".", "_", ".", "_synonym_sent", "=", "synonym_sent", "\n", "doc", ".", "_", ".", "_synonym_intv", "=", "synonym_intv", "\n", "doc", ".", "_", ".", "_ori_syn_intv", "=", "ori_syn_intv", "\n", "doc", ".", "_", ".", "_antonym_sent", "=", "antonym_sent", "\n", "doc", ".", "_", ".", "_antonym_intv", "=", "antonym_intv", "\n", "doc", ".", "_", ".", "_ori_ant_intv", "=", "ori_ant_intv", "\n", "\n", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertLMHead.__init__": [[19, 29], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.MyProcess.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "vocab_size", ")", ")", "\n", "\n", "# Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`", "\n", "self", ".", "decoder", ".", "bias", "=", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertLMHead.forward": [[30, 39], ["modeling.LecbertLMHead.dense", "transformers.activations.gelu", "modeling.LecbertLMHead.layer_norm", "modeling.LecbertLMHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "gelu", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# project back to size of vocabulary with bias", "\n", "x", "=", "self", ".", "decoder", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertTECHead.__init__": [[43, 52], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.MyProcess.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_token_error", ",", "bias", "=", "False", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "num_token_error", ")", ")", "\n", "\n", "self", ".", "decoder", ".", "bias", "=", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertTECHead.forward": [[53, 62], ["modeling.LecbertTECHead.dense", "transformers.activations.gelu", "modeling.LecbertTECHead.layer_norm", "modeling.LecbertTECHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "gelu", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# project back to size of labels", "\n", "x", "=", "self", ".", "decoder", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertForPreTraining.__init__": [[67, 76], ["transformers.modeling_roberta.RobertaPreTrainedModel.__init__", "transformers.modeling_roberta.RobertaModel", "modeling.LecbertLMHead", "modeling.LecbertTECHead", "torch.Parameter", "torch.Parameter", "modeling.LecbertForPreTraining.init_weights", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.MyProcess.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "mlm_head", "=", "LecbertLMHead", "(", "config", ")", "\n", "self", ".", "tokn_classifier", "=", "LecbertTECHead", "(", "config", ")", "\n", "self", ".", "log_vars", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "3", ")", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertForPreTraining.get_output_embeddings": [[77, 79], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "mlm_head", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.modeling.LecbertForPreTraining.forward": [[80, 179], ["modeling.LecbertForPreTraining.roberta", "modeling.LecbertForPreTraining.mlm_head", "modeling.LecbertForPreTraining.tokn_classifier", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "modeling.LecbertForPretrainingOutput", "warnings.warn", "kwargs.pop", "input_ids.size", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.BCELoss", "torch.nn.BCELoss", "torch.nn.BCELoss.", "torch.nn.BCELoss.", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.LecbertForPreTraining.view", "mlm_labels.view", "modeling.LecbertForPreTraining.view", "tec_labels.view", "torch.cat.view", "torch.cat.view", "torch.cat().to.view", "torch.cat().to.view", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "kwargs.keys", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "antonym_ids", "=", "None", ",", "\n", "antonym_label", "=", "None", ",", "\n", "synonym_ids", "=", "None", ",", "\n", "synonym_label", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        replace_label (``torch.LongTensor`` of shape ``(batch_size,sequence_length)``, `optional`):\n            Labels for computing the token replace type prediction (classification) loss.\n            Indices should be in ``[0, 1, 2, 3, 4, 5, 6]``:\n            - 0 indicates the token is the original token,\n            - 1 indicates the token is replaced with the lemminflect token,\n            - 2 indicates the token is replaced with the synonym,\n            - 3 indicates the token is replaced with the hypernyms,\n            - 4 indicates the token is replaced with the adjacency,\n            - 5 indicates the token is replaced with the antonym,\n            - 6 indicates the token is replaced with the random word.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n        Returns:\n        \"\"\"", "\n", "if", "\"masked_lm_labels\"", "in", "kwargs", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"The `masked_lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\"", ",", "\n", "FutureWarning", ",", "\n", ")", "\n", "labels", "=", "kwargs", ".", "pop", "(", "\"masked_lm_labels\"", ")", "\n", "", "assert", "kwargs", "==", "{", "}", ",", "f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "# Masked Language Model", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", ",", "pooled_output", "=", "outputs", "[", ":", "2", "]", "\n", "\n", "batch_size", "=", "input_ids", ".", "size", "(", "0", ")", "//", "3", "\n", "ori_seq", ",", "syn_ant_seq", "=", "sequence_output", "[", ":", "batch_size", "]", ",", "sequence_output", "[", "batch_size", ":", "]", "\n", "mlm_labels", ",", "tec_labels", "=", "labels", "[", ":", "batch_size", "]", ",", "labels", "[", "batch_size", ":", "]", "\n", "mlm_scores", "=", "self", ".", "mlm_head", "(", "ori_seq", ")", "\n", "tec_scores", "=", "self", ".", "tokn_classifier", "(", "syn_ant_seq", ")", "\n", "\n", "ori_sen", ",", "syn_sen", ",", "ant_sen", "=", "pooled_output", "[", ":", "batch_size", "]", ",", "pooled_output", "[", "batch_size", ":", "batch_size", "*", "2", "]", ",", "pooled_output", "[", "batch_size", "*", "2", ":", "]", "\n", "ori_syn_rel", "=", "torch", ".", "sigmoid", "(", "torch", ".", "mean", "(", "ori_sen", "*", "syn_sen", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "ori_ant_rel", "=", "torch", ".", "sigmoid", "(", "torch", ".", "mean", "(", "ori_sen", "*", "ant_sen", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "sec_scores", "=", "torch", ".", "cat", "(", "(", "ori_syn_rel", ",", "ori_ant_rel", ")", ",", "dim", "=", "0", ")", "\n", "sec_labels", "=", "torch", ".", "cat", "(", "(", "torch", ".", "ones", "(", "batch_size", ")", ",", "torch", ".", "zeros", "(", "batch_size", ")", ")", ",", "dim", "=", "0", ")", ".", "to", "(", "labels", ".", "device", ")", "\n", "\n", "total_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_tok", "=", "CrossEntropyLoss", "(", ")", "\n", "mlm_loss", "=", "loss_tok", "(", "mlm_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "mlm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "tec_loss", "=", "loss_tok", "(", "tec_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "num_token_error", ")", ",", "tec_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "loss_sen", "=", "BCELoss", "(", ")", "\n", "sec_loss", "=", "loss_sen", "(", "sec_scores", ".", "view", "(", "-", "1", ")", ",", "sec_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "# total_loss = mlm_loss + tec_loss + sec_loss", "\n", "total_loss", "=", "torch", ".", "exp", "(", "-", "self", ".", "log_vars", "[", "0", "]", ")", "*", "mlm_loss", "+", "torch", ".", "clamp", "(", "self", ".", "log_vars", "[", "0", "]", ",", "min", "=", "0", ")", "+", "torch", ".", "exp", "(", "-", "self", ".", "log_vars", "[", "1", "]", ")", "*", "tec_loss", "+", "torch", ".", "clamp", "(", "self", ".", "log_vars", "[", "1", "]", ",", "min", "=", "0", ")", "+", "torch", ".", "exp", "(", "-", "self", ".", "log_vars", "[", "2", "]", ")", "*", "sec_loss", "+", "torch", ".", "clamp", "(", "self", ".", "log_vars", "[", "2", "]", ",", "min", "=", "0", ")", "\n", "\n", "#print(mlm_loss.item(), tec_loss.item(), sec_loss.item())", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "mlm_scores", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "total_loss", ",", ")", "+", "output", ")", "if", "total_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "LecbertForPretrainingOutput", "(", "\n", "loss", "=", "total_loss", ",", "\n", "prediction_logits", "=", "mlm_scores", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.build_inputs_with_special_tokens": [[8, 32], ["None"], "methods", ["None"], ["    ", "def", "build_inputs_with_special_tokens", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n        adding special tokens. A RoBERTa sequence has the following format:\n\n        - single sequence: ``<s> X </s>``\n        - pair of sequences: ``<s> A </s></s> B </s>``\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs to which the special tokens will be added.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n        \"\"\"", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "[", "self", ".", "cls_token_id", "]", "+", "token_ids_0", "+", "[", "self", ".", "sep_token_id", "]", "\n", "", "cls", "=", "[", "self", ".", "cls_token_id", "]", "\n", "sep", "=", "[", "self", ".", "sep_token_id", "]", "\n", "return", "cls", "+", "token_ids_0", "+", "sep", "+", "sep", "+", "token_ids_1", "+", "sep", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.create_token_label_from_sequences": [[33, 42], ["None"], "methods", ["None"], ["", "def", "create_token_label_from_sequences", "(", "\n", "self", ",", "labels_0", ":", "List", "[", "int", "]", ",", "labels_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "\n", "        ", "cls", "=", "[", "REPLACE_NONE", "]", "\n", "sep", "=", "[", "REPLACE_NONE", "]", "\n", "if", "labels_1", "is", "None", ":", "\n", "            ", "return", "cls", "+", "labels_0", "+", "sep", "\n", "", "return", "cls", "+", "labels_0", "+", "sep", "+", "sep", "+", "labels_1", "+", "sep", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.datacollator.DataCollatorForLEC.__call__": [[22, 79], ["len", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "datacollator.DataCollatorForLEC.mask_tokens", "datacollator.DataCollatorForLEC.tokenizer.num_special_tokens_to_add", "datacollator.DataCollatorForLEC.tokenizer.build_inputs_with_special_tokens", "datacollator.DataCollatorForLEC.tokenizer.create_token_label_from_sequences", "datacollator.DataCollatorForLEC.tokenizer.build_inputs_with_special_tokens", "datacollator.DataCollatorForLEC.tokenizer.create_token_label_from_sequences", "datacollator.DataCollatorForLEC.tokenizer.build_inputs_with_special_tokens", "datacollator.DataCollatorForLEC.tokenizer.create_token_label_from_sequences", "len", "len", "len", "torch.tensor", "torch.ones", "torch.tensor", "torch.ones", "torch.tensor", "torch.ones", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.datacollator.DataCollatorForLEC.mask_tokens", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.build_inputs_with_special_tokens", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.create_token_label_from_sequences", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.build_inputs_with_special_tokens", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.create_token_label_from_sequences", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.build_inputs_with_special_tokens", "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.tokenization.LecbertTokenizer.create_token_label_from_sequences"], ["def", "__call__", "(", "self", ",", "examples", ":", "List", "[", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "batch_size", "=", "len", "(", "examples", ")", "\n", "block_size", "=", "self", ".", "block_size", "-", "self", ".", "tokenizer", ".", "num_special_tokens_to_add", "(", "pair", "=", "False", ")", "\n", "\n", "ori_sent", "=", "[", "]", "\n", "ori_mask", "=", "[", "]", "\n", "syn_sent", "=", "[", "]", "\n", "syn_mask", "=", "[", "]", "\n", "ant_sent", "=", "[", "]", "\n", "ant_mask", "=", "[", "]", "\n", "ori_label", "=", "[", "]", "\n", "syn_label", "=", "[", "]", "\n", "ant_label", "=", "[", "]", "\n", "\n", "for", "example", "in", "examples", ":", "\n", "            ", "ori_sen", "=", "self", ".", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "example", "[", "\"input_ids\"", "]", "[", ":", "block_size", "]", ")", "\n", "ori_lab", "=", "self", ".", "tokenizer", ".", "create_token_label_from_sequences", "(", "[", "REPLACE_NONE", "]", "*", "len", "(", "example", "[", "\"input_ids\"", "]", "[", ":", "block_size", "]", ")", ")", "\n", "syn_sen", "=", "self", ".", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "example", "[", "\"synonym_ids\"", "]", "[", ":", "block_size", "]", ")", "\n", "syn_lab", "=", "example", "[", "\"synonym_label\"", "]", "[", ":", "block_size", "]", "\n", "syn_lab", "=", "[", "1", "if", "lb", "not", "in", "[", "REPLACE_NONE", ",", "0", "]", "else", "lb", "for", "lb", "in", "syn_lab", "]", "\n", "syn_lab", "=", "self", ".", "tokenizer", ".", "create_token_label_from_sequences", "(", "syn_lab", ")", "\n", "ant_sen", "=", "self", ".", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "example", "[", "\"antonym_ids\"", "]", "[", ":", "block_size", "]", ")", "\n", "ant_lab", "=", "example", "[", "\"antonym_label\"", "]", "[", ":", "block_size", "]", "\n", "ant_lab", "=", "[", "2", "if", "lb", "not", "in", "[", "REPLACE_NONE", ",", "0", "]", "else", "lb", "for", "lb", "in", "ant_lab", "]", "\n", "ant_lab", "=", "self", ".", "tokenizer", ".", "create_token_label_from_sequences", "(", "ant_lab", ")", "\n", "\n", "ori_sent", "+=", "[", "torch", ".", "tensor", "(", "ori_sen", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "ori_mask", "+=", "[", "torch", ".", "ones", "(", "len", "(", "ori_sen", ")", ")", "]", "\n", "syn_sent", "+=", "[", "torch", ".", "tensor", "(", "syn_sen", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "syn_mask", "+=", "[", "torch", ".", "ones", "(", "len", "(", "syn_sen", ")", ")", "]", "\n", "ant_sent", "+=", "[", "torch", ".", "tensor", "(", "ant_sen", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "ant_mask", "+=", "[", "torch", ".", "ones", "(", "len", "(", "ant_sen", ")", ")", "]", "\n", "\n", "ori_label", "+=", "[", "torch", ".", "tensor", "(", "ori_lab", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "syn_label", "+=", "[", "torch", ".", "tensor", "(", "syn_lab", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "ant_label", "+=", "[", "torch", ".", "tensor", "(", "ant_lab", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "\n", "", "input_ids", "=", "ori_sent", "+", "syn_sent", "+", "ant_sent", "\n", "attention_mask", "=", "ori_mask", "+", "syn_mask", "+", "ant_mask", "\n", "labels", "=", "ori_label", "+", "syn_label", "+", "ant_label", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "batch_size", "*", "3", "\n", "assert", "len", "(", "attention_mask", ")", "==", "batch_size", "*", "3", "\n", "assert", "len", "(", "labels", ")", "==", "batch_size", "*", "3", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "input_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "self", ".", "tokenizer", ".", "pad_token_id", ")", "\n", "attention_mask", "=", "pad_sequence", "(", "attention_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "labels", "=", "pad_sequence", "(", "labels", ",", "batch_first", "=", "True", ",", "padding_value", "=", "REPLACE_NONE", ")", "\n", "\n", "mlm_sent", ",", "mlm_label", "=", "self", ".", "mask_tokens", "(", "input_ids", "[", ":", "batch_size", "]", ")", "\n", "input_ids", "[", ":", "batch_size", "]", "=", "mlm_sent", "\n", "labels", "[", ":", "batch_size", "]", "=", "mlm_label", "\n", "\n", "return", "{", "\n", "\"input_ids\"", ":", "input_ids", ",", "\n", "\"attention_mask\"", ":", "attention_mask", ",", "\n", "\"labels\"", ":", "labels", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.lecbert.datacollator.DataCollatorForLEC.mask_tokens": [[82, 116], ["inputs.clone", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().bool", "datacollator.DataCollatorForLEC.tokenizer.convert_tokens_to_ids", "torch.randint", "ValueError", "datacollator.DataCollatorForLEC.tokenizer.get_special_tokens_mask", "torch.tensor", "inputs.clone.eq", "torch.full.masked_fill_", "torch.bernoulli().bool", "len", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli().bool", "torch.bernoulli", "torch.full", "torch.bernoulli", "torch.full"], "methods", ["None"], ["", "def", "mask_tokens", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n        \"\"\"", "\n", "\n", "if", "self", ".", "tokenizer", ".", "mask_token", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"", "\n", ")", "\n", "\n", "", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "# We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)", "\n", "probability_matrix", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "self", ".", "mlm_probability", ")", "\n", "special_tokens_mask", "=", "[", "\n", "self", ".", "tokenizer", ".", "get_special_tokens_mask", "(", "val", ",", "already_has_special_tokens", "=", "True", ")", "for", "val", "in", "labels", ".", "tolist", "(", ")", "\n", "]", "\n", "probability_matrix", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "special_tokens_mask", ",", "dtype", "=", "torch", ".", "bool", ")", ",", "value", "=", "0.0", ")", "\n", "if", "self", ".", "tokenizer", ".", "_pad_token", "is", "not", "None", ":", "\n", "            ", "padding_mask", "=", "labels", ".", "eq", "(", "self", ".", "tokenizer", ".", "pad_token_id", ")", "\n", "probability_matrix", ".", "masked_fill_", "(", "padding_mask", ",", "value", "=", "0.0", ")", "\n", "", "masked_indices", "=", "torch", ".", "bernoulli", "(", "probability_matrix", ")", ".", "bool", "(", ")", "\n", "labels", "[", "~", "masked_indices", "]", "=", "REPLACE_NONE", "# We only compute loss on masked tokens", "\n", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.8", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "\n", "inputs", "[", "indices_replaced", "]", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "mask_token", ")", "\n", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.5", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "self", ".", "tokenizer", ")", ",", "labels", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "\n", "# The rest of the time (10% of the time) we keep the masked input tokens unchanged", "\n", "return", "inputs", ",", "labels", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.MyProcess.__init__": [[31, 36], ["multiprocessing.Process.__init__"], "methods", ["home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.MyProcess.__init__"], ["    ", "def", "__init__", "(", "self", ",", "files", ",", "dirname", ",", "outname", ")", ":", "\n", "        ", "super", "(", "MyProcess", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "files", "=", "files", "\n", "self", ".", "dirname", "=", "dirname", "\n", "self", ".", "outname", "=", "outname", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.MyProcess.run": [[37, 63], ["open", "enumerate", "open.close", "os.path.exists", "os.mkdir", "os.path.join", "extract_sentence.get_articles", "str", "str", "arti.strip.strip.strip", "re.sub", "arti.strip.strip.strip", "open.write"], "methods", ["home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.get_articles"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "dirname", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "self", ".", "dirname", ")", "\n", "", "outfile", "=", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dirname", ",", "str", "(", "self", ".", "outname", ")", ")", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "for", "idx", ",", "path", "in", "enumerate", "(", "self", ".", "files", ")", ":", "\n", "#print(idx)", "\n", "            ", "articles", "=", "get_articles", "(", "path", ")", "\n", "for", "arti", "in", "articles", ":", "\n", "                ", "arti", "=", "str", "(", "arti", ")", "\n", "arti", "=", "arti", ".", "strip", "(", ")", "\n", "arti", "=", "re", ".", "sub", "(", "'[\\s]+'", ",", "' '", ",", "arti", ")", "\n", "arti", "=", "arti", ".", "strip", "(", ")", "\n", "if", "not", "arti", ":", "continue", "\n", "outfile", ".", "write", "(", "'{}\\n'", ".", "format", "(", "arti", ")", ")", "\n", "# sents = get_sentences(arti)", "\n", "# for sen in sents:", "\n", "#     sen = str(sen)", "\n", "#     sen = sen.strip()", "\n", "#     sen = re.sub('[\\n]+', ' ', sen)", "\n", "#     sen = sen.strip()", "\n", "#     if not sen: continue", "\n", "#     #if len(sen) < 2: continue", "\n", "#     #print(sen.encode('ascii'))", "\n", "#     outfile.write('{}\\n'.format(sen))", "\n", "# outfile.write('\\n')", "\n", "", "", "outfile", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.custom_seg": [[11, 17], ["len", "enumerate"], "function", ["None"], ["def", "custom_seg", "(", "doc", ")", ":", "\n", "    ", "length", "=", "len", "(", "doc", ")", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "doc", ")", ":", "\n", "        ", "if", "token", ".", "text", "in", "[", "'\"'", ",", "\"'\"", ",", "\"\u2018\"", ",", "\"\u2019\"", ",", "\"\u201c\"", ",", "\"\u201d\"", "]", "and", "index", "!=", "(", "length", "-", "1", ")", ":", "\n", "            ", "doc", "[", "index", "+", "1", "]", ".", "sent_start", "=", "False", "\n", "", "", "return", "doc", "\n", "", "nlp", ".", "add_pipe", "(", "custom_seg", ",", "before", "=", "'parser'", ")", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.get_articles": [[19, 24], ["open", "open.close", "eval", "open.readlines"], "function", ["None"], ["def", "get_articles", "(", "path", ")", ":", "\n", "    ", "file", "=", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "\n", "articles", "=", "[", "eval", "(", "x", ")", "[", "'text'", "]", "for", "x", "in", "file", ".", "readlines", "(", ")", "]", "\n", "file", ".", "close", "(", ")", "\n", "return", "articles", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.get_sentences": [[25, 29], ["nlp", "list"], "function", ["None"], ["", "def", "get_sentences", "(", "article", ")", ":", "\n", "    ", "doc", "=", "nlp", "(", "article", ")", "\n", "sents", "=", "list", "(", "doc", ".", "sents", ")", "\n", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.bisector_list": [[64, 72], ["range", "len", "ans.append", "len"], "function", ["None"], ["", "", "def", "bisector_list", "(", "tabulation", ",", "num", ")", ":", "\n", "    ", "seg", "=", "len", "(", "tabulation", ")", "//", "num", "\n", "ans", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num", ")", ":", "\n", "        ", "start", "=", "i", "*", "seg", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "seg", "if", "i", "!=", "num", "-", "1", "else", "len", "(", "tabulation", ")", "\n", "ans", ".", "append", "(", "tabulation", "[", "start", ":", "end", "]", ")", "\n", "", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.walk": [[73, 79], ["os.walk", "out.append", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kandorm_CLINE.preprocess.extract_sentence.walk"], ["", "def", "walk", "(", "path", ")", ":", "\n", "    ", "out", "=", "[", "]", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "path", ")", ":", "\n", "        ", "for", "name", "in", "files", ":", "\n", "            ", "out", ".", "append", "(", "os", ".", "path", ".", "join", "(", "root", ",", "name", ")", ")", "\n", "", "", "return", "out", "\n", "\n"]]}