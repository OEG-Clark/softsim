{"home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.noam_schedule": [[63, 67], ["None"], "function", ["None"], ["def", "noam_schedule", "(", "step", ",", "warmup_step", "=", "4000", ")", ":", "\n", "    ", "if", "step", "<=", "warmup_step", ":", "\n", "        ", "return", "step", "/", "warmup_step", "\n", "", "return", "(", "warmup_step", "**", "0.5", ")", "*", "(", "step", "**", "-", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.main": [[69, 304], ["random.seed", "numpy.random.seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "print", "torch.load", "cmlm.data.BertDataset", "cmlm.model.BertForSeq2seq.from_pretrained", "cmlm.model.convert_embedding", "BertForSeq2seq.from_pretrained.update_output_layer", "BertForSeq2seq.from_pretrained.to", "BertForSeq2seq.from_pretrained.named_modules", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "cmlm.util.RunningMeter", "BertForSeq2seq.from_pretrained.train", "time.time", "torch.cuda.is_available", "torch.device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.get_world_size", "logger.info", "ValueError", "run_cmlm_finetuning.save_training_meta", "torch.cuda.manual_seed_all", "BertForSeq2seq.from_pretrained.half", "cmlm.distributed.broadcast_tensors", "isinstance", "FusedAdam", "pytorch_pretrained_bert.optimization.BertAdam", "len", "cmlm.data.TokenBucketSampler", "torch.utils.data.DataLoader", "cmlm.data.DistributedTokenBucketSampler", "torch.utils.data.DataLoader", "TB_LOGGER.create", "tqdm.tqdm", "enumerate", "TB_LOGGER.log_scaler_dict", "torch.distributed.get_rank", "BertForSeq2seq.from_pretrained.named_parameters", "FP16_Optimizer", "FP16_Optimizer", "os.path.join", "tuple", "input_ids.size", "BertForSeq2seq.from_pretrained.", "cmlm.util.RunningMeter.", "logger.info", "torch.no_grad", "run_cmlm_finetuning.validate", "os.path.join", "torch.save", "bool", "BertForSeq2seq.from_pretrained.parameters", "ImportError", "FP16_Optimizer.backward", "model.backward", "model.item", "TB_LOGGER.add_scalar", "TB_LOGGER.step", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "BertForSeq2seq.from_pretrained.state_dict", "any", "TB_LOGGER.add_scalar", "onmt.utils.distributed.all_reduce_and_rescale_tensors", "tqdm.tqdm.update", "torch.cuda.empty_cache", "TB_LOGGER.add_scalar", "logger.info", "logger.info", "TB_LOGGER.log_scaler_dict", "isinstance", "v.cpu", "BertForSeq2seq.from_pretrained.state_dict().items", "any", "t.to", "pytorch_pretrained_bert.optimization.warmup_linear", "float", "sum", "int", "logger.info", "torch.no_grad", "run_cmlm_finetuning.validate", "os.path.join", "torch.save", "BertForSeq2seq.from_pretrained.parameters", "onmt.utils.distributed.all_gather_list", "BertForSeq2seq.from_pretrained.state_dict", "isinstance", "v.cpu", "BertForSeq2seq.from_pretrained.state_dict().items", "time.time", "BertForSeq2seq.from_pretrained.state_dict"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.convert_embedding", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.update_output_layer", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.save_training_meta", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.create", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.log_scaler_dict", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.validate", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.step", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.step", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.log_scaler_dict", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.validate"], ["", "def", "main", "(", "opts", ")", ":", "\n", "    ", "if", "opts", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "n_gpu", "=", "1", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "opts", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "opts", ".", "local_rank", ")", "\n", "# Initializes the distributed backend which will take care of", "\n", "# sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "n_gpu", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "opts", ".", "local_rank", "!=", "-", "1", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "", "opts", ".", "n_gpu", "=", "n_gpu", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "is_master", "=", "opts", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", "\n", "\n", "if", "is_master", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "\n", "", "random", ".", "seed", "(", "opts", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "opts", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "opts", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "opts", ".", "seed", ")", "\n", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "opts", ".", "bert_model", ",", "do_lower_case", "=", "'uncased'", "in", "opts", ".", "bert_model", ")", "\n", "\n", "# train_examples = None", "\n", "print", "(", "\"Loading Train Dataset\"", ",", "opts", ".", "train_file", ")", "\n", "vocab_dump", "=", "torch", ".", "load", "(", "opts", ".", "vocab_file", ")", "\n", "vocab", "=", "vocab_dump", "[", "'tgt'", "]", ".", "fields", "[", "0", "]", "[", "1", "]", ".", "vocab", ".", "stoi", "\n", "train_dataset", "=", "BertDataset", "(", "opts", ".", "train_file", ",", "tokenizer", ",", "vocab", ",", "\n", "seq_len", "=", "opts", ".", "max_seq_length", ",", "\n", "max_len", "=", "opts", ".", "max_sent_length", ")", "\n", "\n", "# Prepare model", "\n", "model", "=", "BertForSeq2seq", ".", "from_pretrained", "(", "opts", ".", "bert_model", ")", "\n", "embedding", "=", "convert_embedding", "(", "\n", "tokenizer", ",", "vocab", ",", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "model", ".", "update_output_layer", "(", "embedding", ")", "\n", "if", "opts", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "opts", ".", "local_rank", "!=", "-", "1", ":", "\n", "# need to make sure models are the same in the beginning", "\n", "        ", "params", "=", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", "\n", "broadcast_tensors", "(", "params", ")", "\n", "", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "# we might want to tune dropout for smaller dataset", "\n", "        ", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "Dropout", ")", ":", "\n", "            ", "module", ".", "p", "=", "opts", ".", "dropout", "\n", "\n", "# Prepare optimizer", "\n", "", "", "param_optimizer", "=", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "'pooler'", "not", "in", "n", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "if", "opts", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from \"", "\n", "\"https://www.github.com/nvidia/apex \"", "\n", "\"to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "opts", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "opts", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "\n", "static_loss_scale", "=", "opts", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "opts", ".", "learning_rate", ",", "\n", "warmup", "=", "opts", ".", "warmup_proportion", ",", "\n", "t_total", "=", "opts", ".", "num_train_steps", ")", "\n", "\n", "", "global_step", "=", "0", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "if", "opts", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "train_sampler", "=", "TokenBucketSampler", "(", "\n", "train_dataset", ".", "lens", ",", "\n", "bucket_size", "=", "8192", ",", "\n", "batch_size", "=", "opts", ".", "train_batch_size", ",", "\n", "droplast", "=", "True", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "\n", "batch_sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "BertDataset", ".", "pad_collate", ")", "\n", "", "else", ":", "\n", "        ", "train_sampler", "=", "DistributedTokenBucketSampler", "(", "\n", "n_gpu", ",", "opts", ".", "local_rank", ",", "\n", "train_dataset", ".", "lens", ",", "\n", "bucket_size", "=", "8192", ",", "\n", "batch_size", "=", "opts", ".", "train_batch_size", ",", "\n", "droplast", "=", "True", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "\n", "batch_sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "BertDataset", ".", "pad_collate", ")", "\n", "\n", "", "if", "is_master", ":", "\n", "        ", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "is_master", ":", "\n", "        ", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "disabled", "=", "True", "\n", "pbar", "=", "None", "\n", "", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "            ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "t", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "lm_label_ids", "=", "batch", "\n", "n_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "mask", "=", "lm_label_ids", "!=", "-", "1", "\n", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "\n", "lm_label_ids", ",", "mask", ",", "True", ")", "\n", "if", "opts", ".", "fp16", ":", "\n", "                ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "if", "opts", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "# if opts.fp16 is False, BertAdam is used that handles", "\n", "# this automatically", "\n", "                    ", "lr_this_step", "=", "opts", ".", "learning_rate", "*", "warmup_linear", "(", "\n", "global_step", "/", "opts", ".", "num_train_steps", ",", "\n", "opts", ".", "warmup_proportion", ")", "\n", "if", "lr_this_step", "<", "0", ":", "\n", "# save guard for possible miscalculation of train steps", "\n", "                        ", "lr_this_step", "=", "1e-8", "\n", "", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "\n", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# NOTE running loss not gathered across GPUs for speed", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "if", "opts", ".", "local_rank", "!=", "-", "1", ":", "\n", "# gather gradients from every processes", "\n", "                    ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "pbar", "is", "not", "None", ":", "\n", "                    ", "pbar", ".", "update", "(", "1", ")", "\n", "", "if", "global_step", "%", "5", "==", "0", ":", "\n", "                    ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "if", "global_step", "%", "100", "==", "0", ":", "\n", "                    ", "if", "opts", ".", "local_rank", "!=", "-", "1", ":", "\n", "                        ", "total", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "", "else", ":", "\n", "                        ", "total", "=", "n_examples", "\n", "", "if", "is_master", ":", "\n", "                        ", "ex_per_sec", "=", "int", "(", "total", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "logger", ".", "info", "(", "f'{total} examples trained at '", "\n", "f'{ex_per_sec} ex/s'", ")", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'ex_per_s'", ",", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"start validation at Step {global_step}\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "val_log", "=", "validate", "(", "model", ",", "\n", "opts", ".", "valid_src", ",", "opts", ".", "valid_tgt", ",", "\n", "tokenizer", ",", "vocab", ",", "device", ",", "\n", "opts", ".", "local_rank", ")", "\n", "", "logger", ".", "info", "(", "f\"Val Acc: {val_log['val_acc']}; \"", "\n", "f\"Val Loss: {val_log['val_loss']}\"", ")", "\n", "TB_LOGGER", ".", "log_scaler_dict", "(", "val_log", ")", "\n", "if", "is_master", ":", "\n", "                        ", "output_model_file", "=", "join", "(", "\n", "opts", ".", "output_dir", ",", "'ckpt'", ",", "\n", "f\"model_step_{global_step}.pt\"", ")", "\n", "# save cpu checkpoint", "\n", "state_dict", "=", "{", "k", ":", "v", ".", "cpu", "(", ")", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", "\n", "else", "v", "\n", "for", "k", ",", "v", "in", "model", ".", "state_dict", "(", ")", ".", "items", "(", ")", "}", "\n", "torch", ".", "save", "(", "state_dict", ",", "output_model_file", ")", "\n", "", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "if", "is_master", ":", "\n", "            ", "logger", ".", "info", "(", "f\"finished {n_epoch} epochs\"", ")", "\n", "", "", "if", "opts", ".", "num_train_steps", "%", "opts", ".", "valid_steps", "!=", "0", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "val_log", "=", "validate", "(", "model", ",", "opts", ".", "valid_src", ",", "opts", ".", "valid_tgt", ",", "\n", "tokenizer", ",", "vocab", ",", "device", ",", "opts", ".", "local_rank", ")", "\n", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "val_log", ")", "\n", "if", "is_master", ":", "\n", "            ", "output_model_file", "=", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ",", "\n", "f\"model_step_{global_step}.pt\"", ")", "\n", "# save cpu checkpoint", "\n", "state_dict", "=", "{", "k", ":", "v", ".", "cpu", "(", ")", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", "\n", "else", "v", "\n", "for", "k", ",", "v", "in", "model", ".", "state_dict", "(", ")", ".", "items", "(", ")", "}", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.validate": [[306, 337], ["model.eval", "model.train", "open", "open", "enumerate", "sum", "sum", "sum", "zip", "cmlm.data.convert_raw_input_to_features", "model", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "loss_fct.item", "run_cmlm_finetuning.accuracy_count", "onmt.utils.distributed.all_gather_list", "onmt.utils.distributed.all_gather_list", "onmt.utils.distributed.all_gather_list", "torch.distributed.get_rank", "torch.distributed.get_world_size", "model.squeeze", "labels.view"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_raw_input_to_features", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.accuracy_count"], ["", "", "", "def", "validate", "(", "model", ",", "valid_src", ",", "valid_tgt", ",", "toker", ",", "vocab", ",", "device", ",", "local_rank", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "val_loss", "=", "0", "\n", "n_correct", "=", "0", "\n", "n_word", "=", "0", "\n", "with", "open", "(", "valid_src", ",", "'r'", ")", "as", "src_reader", ",", "open", "(", "valid_tgt", ",", "'r'", ")", "as", "tgt_reader", ":", "\n", "        ", "for", "i", ",", "(", "src", ",", "tgt", ")", "in", "enumerate", "(", "zip", "(", "src_reader", ",", "tgt_reader", ")", ")", ":", "\n", "            ", "if", "local_rank", "!=", "-", "1", ":", "\n", "                ", "global_rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "if", "global_rank", "%", "world_size", "!=", "0", ":", "\n", "                    ", "continue", "\n", "", "", "input_ids", ",", "type_ids", ",", "mask", ",", "labels", "=", "convert_raw_input_to_features", "(", "\n", "src", ",", "tgt", ",", "toker", ",", "vocab", ",", "device", ")", "\n", "prediction_scores", "=", "model", "(", "input_ids", ",", "type_ids", ",", "mask", ")", "\n", "loss_fct", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ",", "\n", "reduction", "=", "'sum'", ")", "\n", "loss", "=", "loss_fct", "(", "prediction_scores", ".", "squeeze", "(", "0", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "n_correct", "+=", "accuracy_count", "(", "prediction_scores", ",", "labels", ")", "\n", "n_word", "+=", "(", "labels", "!=", "-", "1", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "", "", "if", "local_rank", "!=", "-", "1", ":", "\n", "        ", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "n_correct", "=", "sum", "(", "all_gather_list", "(", "n_correct", ")", ")", "\n", "n_word", "=", "sum", "(", "all_gather_list", "(", "n_word", ")", ")", "\n", "", "val_loss", "/=", "n_word", "\n", "acc", "=", "n_correct", "/", "n_word", "\n", "val_log", "=", "{", "'val_loss'", ":", "val_loss", ",", "'val_acc'", ":", "acc", "}", "\n", "model", ".", "train", "(", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.accuracy_count": [[339, 344], ["out.max"], "function", ["None"], ["", "def", "accuracy_count", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "out", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "1", "]", "\n", "mask", "=", "labels", "!=", "-", "1", "\n", "n_correct", "=", "(", "outputs", "==", "labels", ")", ".", "masked_select", "(", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "return", "n_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.run_cmlm_finetuning.save_training_meta": [[346, 380], ["os.path.exists", "os.makedirs", "os.makedirs", "open", "copy.deepcopy", "json.dump", "logger.info", "subprocess.run", "subprocess.run.stdout.decode().strip", "logger.info", "subprocess.run", "subprocess.run.stdout.decode().strip", "logger.info", "os.path.abspath", "subprocess.check_output().strip", "os.path.join", "os.path.join", "os.path.join", "vars", "os.path.dirname", "open", "json.dump", "logger.exception", "logger.warn", "subprocess.run.stdout.decode", "subprocess.run.stdout.decode", "subprocess.check_output", "os.path.join", "bool"], "function", ["None"], ["", "def", "save_training_meta", "(", "opts", ")", ":", "\n", "    ", "if", "not", "exists", "(", "opts", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "\n", "", "with", "open", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'hps.json'", ")", ",", "'w'", ")", "as", "writer", ":", "\n", "        ", "hps", "=", "copy", ".", "deepcopy", "(", "vars", "(", "opts", ")", ")", "\n", "del", "hps", "[", "'local_rank'", "]", "\n", "json", ".", "dump", "(", "hps", ",", "writer", ",", "indent", "=", "4", ")", "\n", "# git info", "\n", "", "try", ":", "\n", "        ", "logger", ".", "info", "(", "\"Waiting on git info....\"", ")", "\n", "c", "=", "subprocess", ".", "run", "(", "[", "\"git\"", ",", "\"rev-parse\"", ",", "\"--abbrev-ref\"", ",", "\"HEAD\"", "]", ",", "\n", "timeout", "=", "10", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "\n", "git_branch_name", "=", "c", ".", "stdout", ".", "decode", "(", ")", ".", "strip", "(", ")", "\n", "logger", ".", "info", "(", "\"Git branch: %s\"", ",", "git_branch_name", ")", "\n", "c", "=", "subprocess", ".", "run", "(", "[", "\"git\"", ",", "\"rev-parse\"", ",", "\"HEAD\"", "]", ",", "\n", "timeout", "=", "10", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "\n", "git_sha", "=", "c", ".", "stdout", ".", "decode", "(", ")", ".", "strip", "(", ")", "\n", "logger", ".", "info", "(", "\"Git SHA: %s\"", ",", "git_sha", ")", "\n", "git_dir", "=", "abspath", "(", "dirname", "(", "__file__", ")", ")", "\n", "git_status", "=", "subprocess", ".", "check_output", "(", "\n", "[", "'git'", ",", "'status'", ",", "'--short'", "]", ",", "\n", "cwd", "=", "git_dir", ",", "universal_newlines", "=", "True", ")", ".", "strip", "(", ")", "\n", "with", "open", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'git_info.json'", ")", ",", "\n", "'w'", ")", "as", "writer", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'branch'", ":", "git_branch_name", ",", "\n", "'is_dirty'", ":", "bool", "(", "git_status", ")", ",", "\n", "'status'", ":", "git_status", ",", "\n", "'sha'", ":", "git_sha", "}", ",", "\n", "writer", ",", "indent", "=", "4", ")", "\n", "", "", "except", "subprocess", ".", "TimeoutExpired", "as", "e", ":", "\n", "        ", "logger", ".", "exception", "(", "e", ")", "\n", "logger", ".", "warn", "(", "\"Git info not found. Moving right along...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.BertSampleDataset.__init__": [[43, 51], ["shelve.open", "dump_teacher_hiddens.BertSampleDataset.db.items", "dump_teacher_hiddens.BertSampleDataset.ids.append", "len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "corpus_path", ",", "tokenizer", ",", "num_samples", "=", "7", ")", ":", "\n", "        ", "self", ".", "db", "=", "shelve", ".", "open", "(", "corpus_path", ",", "'r'", ")", "\n", "self", ".", "ids", "=", "[", "]", "\n", "for", "i", ",", "ex", "in", "self", ".", "db", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "ex", "[", "'src'", "]", ")", "+", "len", "(", "ex", "[", "'tgt'", "]", ")", "+", "3", "<=", "512", ":", "\n", "                ", "self", ".", "ids", ".", "append", "(", "i", ")", "\n", "", "", "self", ".", "toker", "=", "tokenizer", "\n", "self", ".", "num_samples", "=", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.BertSampleDataset.__len__": [[52, 54], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.BertSampleDataset.__getitem__": [[55, 61], ["dump_teacher_hiddens.convert_example"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.convert_example"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "features", "=", "convert_example", "(", "example", "[", "'src'", "]", ",", "example", "[", "'tgt'", "]", ",", "\n", "self", ".", "toker", ",", "self", ".", "num_samples", ")", "\n", "return", "(", "id_", ",", ")", "+", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.tensor_dumps": [[23, 29], ["io.BytesIO", "numpy.save", "writer.getvalue", "tensor.cpu().numpy().astype", "tensor.cpu().numpy", "tensor.cpu"], "function", ["None"], ["def", "tensor_dumps", "(", "tensor", ")", ":", "\n", "    ", "with", "io", ".", "BytesIO", "(", ")", "as", "writer", ":", "\n", "        ", "np", ".", "save", "(", "writer", ",", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "float16", ")", ",", "\n", "allow_pickle", "=", "False", ")", "\n", "dump", "=", "writer", ".", "getvalue", "(", ")", "\n", "", "return", "dump", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.gather_hiddens": [[31, 40], ["zip", "torch.stack", "hiddens.split", "masks.split", "mask.unsqueeze().expand_as.unsqueeze().expand_as", "outputs.append", "mask.unsqueeze().expand_as.sum().item", "hid.masked_select", "mask.unsqueeze().expand_as.unsqueeze", "mask.unsqueeze().expand_as.sum"], "function", ["None"], ["", "def", "gather_hiddens", "(", "hiddens", ",", "masks", ")", ":", "\n", "    ", "outputs", "=", "[", "]", "\n", "for", "hid", ",", "mask", "in", "zip", "(", "hiddens", ".", "split", "(", "1", ",", "dim", "=", "1", ")", ",", "masks", ".", "split", "(", "1", ",", "dim", "=", "1", ")", ")", ":", "\n", "        ", "if", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "mask", "=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "hid", ")", "\n", "outputs", ".", "append", "(", "hid", ".", "masked_select", "(", "mask", ")", ")", "\n", "", "output", "=", "torch", ".", "stack", "(", "outputs", ",", "dim", "=", "0", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.convert_example": [[63, 92], ["len", "torch.cat", "toker.convert_tokens_to_ids", "torch.tensor", "torch.tensor.data.masked_fill_", "torch.tensor", "cmlm.data.convert_token_to_bert", "torch.eye().byte", "torch.zeros().byte", "enumerate", "torch.zeros().byte.sum().item", "toker.convert_tokens_to_ids", "cmlm.data.convert_token_to_bert", "list", "torch.zeros().byte", "torch.eye", "range", "range", "torch.zeros", "torch.zeros().byte.sum", "range", "range", "torch.zeros", "len", "len", "torch.zeros().byte.sum", "torch.ones().long", "len", "torch.ones"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert"], ["", "", "def", "convert_example", "(", "src", ",", "tgt", ",", "toker", ",", "num_samples", ")", ":", "\n", "    ", "src", "=", "[", "convert_token_to_bert", "(", "tok", ")", "for", "tok", "in", "src", "]", "\n", "tgt", "=", "[", "convert_token_to_bert", "(", "tok", ")", "for", "tok", "in", "tgt", "]", "+", "[", "SEP", "]", "\n", "\n", "# build the random masks", "\n", "tgt_len", "=", "len", "(", "tgt", ")", "\n", "if", "tgt_len", "<=", "num_samples", ":", "\n", "        ", "masks", "=", "torch", ".", "eye", "(", "tgt_len", ")", ".", "byte", "(", ")", "\n", "num_samples", "=", "tgt_len", "\n", "", "else", ":", "\n", "        ", "mask_inds", "=", "[", "list", "(", "range", "(", "i", ",", "tgt_len", ",", "num_samples", ")", ")", "\n", "for", "i", "in", "range", "(", "num_samples", ")", "]", "\n", "masks", "=", "torch", ".", "zeros", "(", "num_samples", ",", "tgt_len", ")", ".", "byte", "(", ")", "\n", "for", "i", ",", "indices", "in", "enumerate", "(", "mask_inds", ")", ":", "\n", "            ", "for", "j", "in", "indices", ":", "\n", "                ", "masks", ".", "data", "[", "i", ",", "j", "]", "=", "1", "\n", "", "", "", "assert", "(", "masks", ".", "sum", "(", "dim", "=", "0", ")", "!=", "torch", ".", "ones", "(", "tgt_len", ")", ".", "long", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "0", "\n", "assert", "masks", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "tgt_len", "\n", "masks", "=", "torch", ".", "cat", "(", "[", "torch", ".", "zeros", "(", "num_samples", ",", "len", "(", "src", ")", "+", "2", ")", ".", "byte", "(", ")", ",", "masks", "]", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "# make BERT inputs", "\n", "input_ids", "=", "toker", ".", "convert_tokens_to_ids", "(", "[", "CLS", "]", "+", "src", "+", "[", "SEP", "]", "+", "tgt", ")", "\n", "mask_id", "=", "toker", ".", "convert_tokens_to_ids", "(", "[", "MASK", "]", ")", "[", "0", "]", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "[", "input_ids", "for", "_", "in", "range", "(", "num_samples", ")", "]", ")", "\n", "input_ids", ".", "data", ".", "masked_fill_", "(", "masks", ",", "mask_id", ")", "\n", "token_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "*", "(", "len", "(", "src", ")", "+", "2", ")", "+", "[", "1", "]", "*", "len", "(", "tgt", ")", "\n", "for", "_", "in", "range", "(", "num_samples", ")", "]", ")", "\n", "return", "input_ids", ",", "token_ids", ",", "masks", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.batch_features": [[94, 109], ["map", "sum", "max", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "zip", "toolz.sandbox.unzip", "inp.size", "torch.zeros().long.data[].fill_", "torch.zeros().long.size", "torch.zeros().long.size", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["None"], ["", "def", "batch_features", "(", "features", ")", ":", "\n", "    ", "ids", ",", "all_input_ids", ",", "all_token_ids", ",", "all_masks", "=", "map", "(", "list", ",", "unzip", "(", "features", ")", ")", "\n", "batch_size", "=", "sum", "(", "input_ids", ".", "size", "(", "0", ")", "for", "input_ids", "in", "all_input_ids", ")", "\n", "max_len", "=", "max", "(", "input_ids", ".", "size", "(", "1", ")", "for", "input_ids", "in", "all_input_ids", ")", "\n", "input_ids", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ")", ".", "long", "(", ")", "\n", "token_ids", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ")", ".", "long", "(", ")", "\n", "attn_mask", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ")", ".", "long", "(", ")", "\n", "i", "=", "0", "\n", "for", "inp", ",", "tok", "in", "zip", "(", "all_input_ids", ",", "all_token_ids", ")", ":", "\n", "        ", "block", ",", "len_", "=", "inp", ".", "size", "(", ")", "\n", "input_ids", ".", "data", "[", "i", ":", "i", "+", "block", ",", ":", "len_", "]", "=", "inp", ".", "data", "\n", "token_ids", ".", "data", "[", "i", ":", "i", "+", "block", ",", ":", "len_", "]", "=", "tok", ".", "data", "\n", "attn_mask", ".", "data", "[", "i", ":", "i", "+", "block", ",", ":", "len_", "]", ".", "fill_", "(", "1", ")", "\n", "i", "+=", "block", "\n", "", "return", "ids", ",", "input_ids", ",", "token_ids", ",", "attn_mask", ",", "all_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.process_batch": [[111, 128], ["input_ids.cuda.cuda", "token_ids.cuda.cuda", "attn_mask.cuda.cuda", "bert.bert", "bert.cls.predictions.transform", "masks.cuda.size", "masks.cuda.cuda", "outputs.append", "dump_teacher_hiddens.gather_hiddens"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.gather_hiddens"], ["", "def", "process_batch", "(", "batch", ",", "bert", ",", "toker", ",", "num_samples", "=", "7", ")", ":", "\n", "    ", "input_ids", ",", "token_ids", ",", "attn_mask", ",", "all_masks", "=", "batch", "\n", "input_ids", "=", "input_ids", ".", "cuda", "(", ")", "\n", "token_ids", "=", "token_ids", ".", "cuda", "(", ")", "\n", "attn_mask", "=", "attn_mask", ".", "cuda", "(", ")", "\n", "hiddens", ",", "_", "=", "bert", ".", "bert", "(", "input_ids", ",", "token_ids", ",", "attn_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "hiddens", "=", "bert", ".", "cls", ".", "predictions", ".", "transform", "(", "hiddens", ")", "\n", "i", "=", "0", "\n", "outputs", "=", "[", "]", "\n", "for", "masks", "in", "all_masks", ":", "\n", "        ", "block", ",", "len_", "=", "masks", ".", "size", "(", ")", "\n", "hids", "=", "hiddens", "[", "i", ":", "i", "+", "block", ",", ":", "len_", ",", ":", "]", "\n", "masks", "=", "masks", ".", "cuda", "(", ")", "\n", "outputs", ".", "append", "(", "gather_hiddens", "(", "hids", ",", "masks", ")", ")", "\n", "i", "+=", "block", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.build_db_batched": [[130, 140], ["dump_teacher_hiddens.BertSampleDataset", "torch.utils.data.DataLoader", "tqdm.tqdm", "dump_teacher_hiddens.process_batch", "zip", "pbar.update", "len", "dump_teacher_hiddens.tensor_dumps", "len"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.process_batch", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.tensor_dumps"], ["", "def", "build_db_batched", "(", "corpus", ",", "out_db", ",", "bert", ",", "toker", ",", "batch_size", "=", "8", ")", ":", "\n", "    ", "dataset", "=", "BertSampleDataset", "(", "corpus", ",", "toker", ")", "\n", "loader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "4", ",", "collate_fn", "=", "batch_features", ")", "\n", "with", "tqdm", "(", "desc", "=", "'computing BERT features'", ",", "total", "=", "len", "(", "dataset", ")", ")", "as", "pbar", ":", "\n", "        ", "for", "ids", ",", "*", "batch", "in", "loader", ":", "\n", "            ", "outputs", "=", "process_batch", "(", "batch", ",", "bert", ",", "toker", ")", "\n", "for", "id_", ",", "output", "in", "zip", "(", "ids", ",", "outputs", ")", ":", "\n", "                ", "out_db", "[", "id_", "]", "=", "tensor_dumps", "(", "output", ")", "\n", "", "pbar", ".", "update", "(", "len", "(", "ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.main": [[142, 163], ["torch.load", "state_dict[].size", "cmlm.model.BertForSeq2seq.from_pretrained().eval().half().cuda", "BertForSeq2seq.from_pretrained().eval().half().cuda.update_output_layer_by_size", "BertForSeq2seq.from_pretrained().eval().half().cuda.load_state_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "torch.nn.Linear", "os.makedirs", "torch.save", "shelve.open", "torch.no_grad", "dump_teacher_hiddens.build_db_batched", "cmlm.model.BertForSeq2seq.from_pretrained().eval().half", "cmlm.model.BertForSeq2seq.from_pretrained().eval", "cmlm.model.BertForSeq2seq.from_pretrained"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.update_output_layer_by_size", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_hiddens.build_db_batched"], ["", "", "", "def", "main", "(", "opts", ")", ":", "\n", "# load BERT", "\n", "    ", "state_dict", "=", "torch", ".", "load", "(", "opts", ".", "ckpt", ")", "\n", "vsize", "=", "state_dict", "[", "'cls.predictions.decoder.weight'", "]", ".", "size", "(", "0", ")", "\n", "bert", "=", "BertForSeq2seq", ".", "from_pretrained", "(", "opts", ".", "bert", ")", ".", "eval", "(", ")", ".", "half", "(", ")", ".", "cuda", "(", ")", "\n", "bert", ".", "update_output_layer_by_size", "(", "vsize", ")", "\n", "bert", ".", "load_state_dict", "(", "state_dict", ")", "\n", "toker", "=", "BertTokenizer", ".", "from_pretrained", "(", "opts", ".", "bert", ",", "\n", "do_lower_case", "=", "'uncased'", "in", "opts", ".", "bert", ")", "\n", "\n", "# save the final projection layer", "\n", "linear", "=", "torch", ".", "nn", ".", "Linear", "(", "bert", ".", "config", ".", "hidden_size", ",", "bert", ".", "config", ".", "vocab_size", ")", "\n", "linear", ".", "weight", ".", "data", "=", "state_dict", "[", "'cls.predictions.decoder.weight'", "]", "\n", "linear", ".", "bias", ".", "data", "=", "state_dict", "[", "'cls.predictions.bias'", "]", "\n", "os", ".", "makedirs", "(", "opts", ".", "output", ")", "\n", "torch", ".", "save", "(", "linear", ",", "f'{opts.output}/linear.pt'", ")", "\n", "\n", "# create DB", "\n", "with", "shelve", ".", "open", "(", "f'{opts.output}/db'", ")", "as", "out_db", ",", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "build_db_batched", "(", "opts", ".", "db", ",", "out_db", ",", "bert", ",", "toker", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_topk.tensor_loads": [[16, 24], ["io.BytesIO", "numpy.load", "isinstance"], "function", ["None"], ["def", "tensor_loads", "(", "dump", ")", ":", "\n", "    ", "with", "io", ".", "BytesIO", "(", "dump", ")", "as", "reader", ":", "\n", "        ", "obj", "=", "np", ".", "load", "(", "reader", ",", "allow_pickle", "=", "False", ")", "\n", "if", "isinstance", "(", "obj", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "tensor", "=", "obj", "\n", "", "else", ":", "\n", "            ", "tensor", "=", "obj", "[", "'arr_0'", "]", "\n", "", "", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_topk.dump_topk": [[26, 32], ["io.BytesIO", "torch.save", "writer.getvalue", "logit.cpu", "index.cpu"], "function", ["None"], ["", "def", "dump_topk", "(", "topk", ")", ":", "\n", "    ", "logit", ",", "index", "=", "topk", "\n", "with", "io", ".", "BytesIO", "(", ")", "as", "writer", ":", "\n", "        ", "torch", ".", "save", "(", "(", "logit", ".", "cpu", "(", ")", ",", "index", ".", "cpu", "(", ")", ")", ",", "writer", ")", "\n", "dump", "=", "writer", ".", "getvalue", "(", ")", "\n", "", "return", "dump", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_topk.main": [[34, 44], ["torch.load().cuda", "shelve.open", "shelve.open", "tqdm.tqdm", "torch.load", "db.items", "torch.tensor().cuda", "torch.load().cuda.topk", "dump_teacher_topk.dump_topk", "len", "torch.tensor", "torch.load().cuda.", "dump_teacher_topk.tensor_loads"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_topk.dump_topk", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.None.dump_teacher_topk.tensor_loads"], ["", "def", "main", "(", "opts", ")", ":", "\n", "    ", "linear", "=", "torch", ".", "load", "(", "f'{opts.bert_hidden}/linear.pt'", ")", ".", "cuda", "(", ")", "\n", "with", "shelve", ".", "open", "(", "f'{opts.bert_hidden}/db'", ",", "'r'", ")", "as", "db", ",", "shelve", ".", "open", "(", "f'{opts.bert_hidden}/topk'", ",", "'c'", ")", "as", "topk_db", ":", "\n", "        ", "for", "key", ",", "value", "in", "tqdm", "(", "db", ".", "items", "(", ")", ",", "\n", "total", "=", "len", "(", "db", ")", ",", "desc", "=", "'computing topk...'", ")", ":", "\n", "            ", "bert_hidden", "=", "torch", ".", "tensor", "(", "tensor_loads", "(", "value", ")", ")", ".", "cuda", "(", ")", "\n", "topk", "=", "linear", "(", "bert_hidden", ")", ".", "topk", "(", "dim", "=", "-", "1", ",", "k", "=", "opts", ".", "topk", ")", "\n", "dump", "=", "dump_topk", "(", "topk", ")", "\n", "topk_db", "[", "key", "]", "=", "dump", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.convert_moses": [[25, 29], ["None"], "function", ["None"], ["def", "convert_moses", "(", "tok", ")", ":", "\n", "    ", "if", "tok", "in", "MOSES_SPECIALS", ":", "\n", "        ", "return", "MOSES_SPECIALS", "[", "tok", "]", "\n", "", "return", "tok", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.detokenize": [[31, 55], ["line.split", "words.append", "convert_moses.startswith", "convert_moses.startswith", "convert_moses.replace", "convert_moses.startswith", "words.append", "convert_moses.replace", "bert_detokenize.convert_moses", "ipdb.set_trace", "ValueError", "bert_detokenize.convert_moses"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.convert_moses", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.convert_moses"], ["", "def", "detokenize", "(", "line", ",", "moses", "=", "True", ")", ":", "\n", "    ", "word", "=", "''", "\n", "words", "=", "[", "]", "\n", "for", "tok", "in", "line", ".", "split", "(", ")", ":", "\n", "        ", "if", "tok", ".", "startswith", "(", "IN_WORD", ")", ":", "\n", "            ", "tok", "=", "tok", "[", "2", ":", "]", "\n", "if", "tok", ".", "startswith", "(", "BERT_IN_WORD", ")", ":", "\n", "                ", "tok", "=", "tok", "[", "2", ":", "]", "\n", "", "tok", "=", "tok", ".", "replace", "(", "AMP", ",", "AMP_MOSES", ")", "\n", "if", "moses", ":", "\n", "                ", "tok", "=", "convert_moses", "(", "tok", ")", "\n", "", "word", "+=", "tok", "\n", "", "else", ":", "\n", "            ", "if", "tok", ".", "startswith", "(", "BERT_IN_WORD", ")", ":", "\n", "                ", "ipdb", ".", "set_trace", "(", ")", "\n", "raise", "ValueError", "(", ")", "\n", "", "words", ".", "append", "(", "word", ")", "\n", "tok", "=", "tok", ".", "replace", "(", "AMP", ",", "AMP_MOSES", ")", "\n", "if", "moses", ":", "\n", "                ", "tok", "=", "convert_moses", "(", "tok", ")", "\n", "", "word", "=", "tok", "\n", "", "", "words", ".", "append", "(", "word", ")", "\n", "text", "=", "' '", ".", "join", "(", "words", ")", ".", "strip", "(", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.process": [[57, 62], ["tqdm.tqdm", "bert_detokenize.detokenize", "output.replace.replace", "writer.write"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.detokenize", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.write"], ["", "def", "process", "(", "reader", ",", "writer", ",", "unk", ",", "moses", "=", "True", ")", ":", "\n", "    ", "for", "line", "in", "tqdm", "(", "reader", ",", "desc", "=", "'tokenizing'", ")", ":", "\n", "        ", "output", "=", "detokenize", "(", "line", ",", "moses", ")", "\n", "output", "=", "output", ".", "replace", "(", "UNK", ",", "unk", ")", "# UNK format change", "\n", "writer", ".", "write", "(", "output", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_detokenize.main": [[64, 70], ["os.path.join", "os.path.exists", "os.makedirs", "open", "open", "bert_detokenize.process", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.process"], ["", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "if", "not", "exists", "(", "opts", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "opts", ".", "output_dir", ")", "\n", "", "output_file", "=", "join", "(", "opts", ".", "output_dir", ",", "f'{basename(opts.file)}.detok'", ")", "\n", "with", "open", "(", "opts", ".", "file", ",", "'r'", ")", "as", "reader", ",", "open", "(", "output_file", ",", "'w'", ")", "as", "writer", ":", "\n", "        ", "process", "(", "reader", ",", "writer", ",", "opts", ".", "unk", ",", "opts", ".", "moses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_prepro.make_db": [[13, 21], ["print", "tqdm.tqdm", "enumerate", "src.strip().split", "tgt.strip().split", "zip", "src.strip", "tgt.strip", "str"], "function", ["None"], ["def", "make_db", "(", "src_reader", ",", "tgt_reader", ",", "db", ")", ":", "\n", "    ", "print", "(", ")", "\n", "for", "i", ",", "(", "src", ",", "tgt", ")", "in", "tqdm", "(", "enumerate", "(", "zip", "(", "src_reader", ",", "tgt_reader", ")", ")", ")", ":", "\n", "        ", "src_toks", "=", "src", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "tgt_toks", "=", "tgt", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "src_toks", "and", "tgt_toks", ":", "\n", "            ", "dump", "=", "{", "'src'", ":", "src_toks", ",", "'tgt'", ":", "tgt_toks", "}", "\n", "", "db", "[", "str", "(", "i", ")", "]", "=", "dump", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_prepro.main": [[23, 28], ["open", "open", "shelve.open", "bert_prepro.make_db"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_prepro.make_db"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "# process the dataset", "\n", "    ", "with", "open", "(", "args", ".", "src", ")", "as", "src_reader", ",", "open", "(", "args", ".", "tgt", ")", "as", "tgt_reader", ",", "shelve", ".", "open", "(", "args", ".", "output", ",", "'n'", ")", "as", "db", ":", "\n", "        ", "make_db", "(", "src_reader", ",", "tgt_reader", ",", "db", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.tokenize": [[35, 74], ["line.replace.strip", "line.replace.replace", "line.replace.startswith", "line.replace.endswith", "line.replace.split", "ValueError", "MOSES_SPECIALS.items", "words.append", "tokens.append", "word.startswith", "ValueError", "tokens.append", "word.startswith", "words.append", "words.append", "tokens.extend", "tokens.extend", "bert_toker.tokenize", "enumerate", "len", "bert_toker.tokenize", "len"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.tokenize", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.tokenize"], ["@", "curry", "\n", "def", "tokenize", "(", "bert_toker", ",", "line", ")", ":", "\n", "    ", "if", "IN_WORD", "in", "line", ":", "\n", "# safe guard if the corpus cotains the IN_WORD tag", "\n", "        ", "raise", "ValueError", "(", ")", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "# Gigaword test set", "\n", "line", "=", "line", ".", "replace", "(", "' UNK '", ",", "f' {UNK} '", ")", "\n", "if", "line", ".", "startswith", "(", "'UNK '", ")", ":", "\n", "        ", "line", "=", "UNK", "+", "line", "[", "3", ":", "]", "\n", "", "if", "line", ".", "endswith", "(", "' UNK'", ")", ":", "\n", "        ", "line", "=", "line", "[", ":", "-", "3", "]", "+", "UNK", "\n", "\n", "", "words", "=", "[", "]", "\n", "for", "word", "in", "line", ".", "split", "(", ")", ":", "\n", "        ", "if", "word", "[", "0", "]", "==", "'&'", ":", "\n", "            ", "for", "special", ",", "char", "in", "MOSES_SPECIALS", ".", "items", "(", ")", ":", "\n", "                ", "if", "word", ".", "startswith", "(", "special", ")", ":", "\n", "                    ", "words", ".", "append", "(", "char", ")", "\n", "words", ".", "append", "(", "IN_WORD", "+", "word", "[", "len", "(", "special", ")", ":", "]", ")", "\n", "break", "\n", "", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "words", ".", "append", "(", "word", ")", "\n", "\n", "", "", "tokens", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "        ", "if", "word", "==", "UNK", ":", "\n", "            ", "tokens", ".", "append", "(", "word", ")", "\n", "", "elif", "word", "==", "HYPHEN", ":", "\n", "            ", "tokens", ".", "append", "(", "word", ")", "\n", "", "elif", "word", ".", "startswith", "(", "IN_WORD", ")", ":", "\n", "            ", "tokens", ".", "extend", "(", "IN_WORD", "+", "tok", "\n", "for", "tok", "in", "bert_toker", ".", "tokenize", "(", "word", "[", "len", "(", "IN_WORD", ")", ":", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "tokens", ".", "extend", "(", "tok", "if", "i", "==", "0", "else", "IN_WORD", "+", "tok", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "bert_toker", ".", "tokenize", "(", "word", ")", ")", ")", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.write": [[76, 78], ["writer.write"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.write"], ["", "def", "write", "(", "writer", ",", "tokens", ")", ":", "\n", "    ", "writer", ".", "write", "(", "' '", ".", "join", "(", "tokens", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.process": [[80, 87], ["multiprocessing.Pool", "tqdm.tqdm", "cytoolz.partition_all", "pool.imap", "pbar.update", "bert_tokenize.tokenize", "bert_tokenize.write", "len"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.tokenize", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.write"], ["", "def", "process", "(", "reader", ",", "writer", ",", "tokenizer", ")", ":", "\n", "    ", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ",", "tqdm", "(", "desc", "=", "'tokenizing'", ")", "as", "pbar", ":", "\n", "        ", "for", "lines", "in", "partition_all", "(", "BUF", ",", "reader", ")", ":", "\n", "            ", "for", "tokens", "in", "pool", ".", "imap", "(", "tokenize", "(", "tokenizer", ")", ",", "lines", ",", "\n", "chunksize", "=", "CHUNK", ")", ":", "\n", "                ", "write", "(", "writer", ",", "tokens", ")", "\n", "", "pbar", ".", "update", "(", "len", "(", "lines", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.main": [[89, 107], ["pytorch_pretrained_bert.BertTokenizer.from_pretrained", "glob.glob", "os.path.exists", "os.makedirs", "input_file.endswith", "os.path.join", "open.close", "gzip.open", "os.path.basename", "open", "open", "bert_tokenize.process", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.scripts.bert_tokenize.process"], ["", "", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "opts", ".", "bert", ",", "do_lower_case", "=", "'uncased'", "in", "opts", ".", "bert", ")", "\n", "for", "prefix", "in", "opts", ".", "prefixes", ":", "\n", "        ", "input_files", "=", "glob", ".", "glob", "(", "f'{prefix}*'", ")", "\n", "if", "not", "exists", "(", "opts", ".", "output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "opts", ".", "output_dir", ")", "\n", "", "for", "input_file", "in", "input_files", ":", "\n", "            ", "if", "input_file", ".", "endswith", "(", "'.gz'", ")", ":", "\n", "                ", "out_name", "=", "basename", "(", "input_file", ")", "[", ":", "-", "3", "]", "\n", "reader", "=", "gzip", ".", "open", "(", "input_file", ",", "'rt'", ")", "\n", "", "else", ":", "\n", "                ", "out_name", "=", "basename", "(", "input_file", ")", "\n", "reader", "=", "open", "(", "input_file", ",", "'r'", ")", "\n", "", "output_file", "=", "join", "(", "opts", ".", "output_dir", ",", "f'{out_name}.bert'", ")", "\n", "with", "open", "(", "output_file", ",", "'w'", ")", "as", "writer", ":", "\n", "                ", "process", "(", "reader", ",", "writer", ",", "tokenizer", ")", "\n", "", "reader", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.distributed.broadcast_tensors": [[10, 40], ["sum", "tensors[].new().zero_", "torch.distributed.broadcast", "t.numel", "buffer_t[].copy_", "t.numel", "t.view().copy_", "tensors[].new().zero_.numel", "t.numel", "tensors[].new", "t.view", "t.view"], "function", ["None"], ["def", "broadcast_tensors", "(", "tensors", ",", "rank", "=", "0", ")", ":", "\n", "    ", "\"\"\" broadcast list of tensors at once\n        this can be used to sync parameter initialization across GPUs\n\n    Args:\n        tensors: list of Tensors to brodcast\n        rank: rank to broadcast\n    \"\"\"", "\n", "# buffer size in bytes, determine equiv. # of elements based on data type", "\n", "sz", "=", "sum", "(", "t", ".", "numel", "(", ")", "for", "t", "in", "tensors", ")", "\n", "buffer_t", "=", "tensors", "[", "0", "]", ".", "new", "(", "sz", ")", ".", "zero_", "(", ")", "\n", "\n", "# copy tensors into buffer_t", "\n", "offset", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "t", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "", "assert", "offset", "==", "sz", "\n", "\n", "# broadcast", "\n", "torch", ".", "distributed", ".", "broadcast", "(", "buffer_t", ",", "rank", ")", "\n", "\n", "# copy all-reduced buffer back into tensors", "\n", "offset", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "t", ".", "view", "(", "-", "1", ")", ".", "copy_", "(", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ")", "\n", "offset", "+=", "numel", "\n", "", "assert", "offset", "==", "sz", "==", "buffer_t", ".", "numel", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BertDataset.__init__": [[31, 44], ["shelve.open", "data.BertDataset.db.items", "len", "len", "data.BertDataset.ids.append", "data.BertDataset.lens.append", "min"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "corpus_path", ",", "tokenizer", ",", "vocab", ",", "seq_len", ",", "max_len", "=", "150", ")", ":", "\n", "        ", "self", ".", "db", "=", "shelve", ".", "open", "(", "corpus_path", ",", "'r'", ")", "\n", "self", ".", "lens", "=", "[", "]", "\n", "self", ".", "ids", "=", "[", "]", "\n", "for", "i", ",", "example", "in", "self", ".", "db", ".", "items", "(", ")", ":", "\n", "            ", "src_len", "=", "len", "(", "example", "[", "'src'", "]", ")", "\n", "tgt_len", "=", "len", "(", "example", "[", "'tgt'", "]", ")", "\n", "if", "(", "src_len", "<=", "max_len", "and", "tgt_len", "<=", "max_len", ")", ":", "\n", "                ", "self", ".", "ids", ".", "append", "(", "i", ")", "\n", "self", ".", "lens", ".", "append", "(", "min", "(", "seq_len", ",", "src_len", "+", "tgt_len", "+", "3", ")", ")", "\n", "", "", "self", ".", "vocab", "=", "vocab", "# vocab for output seq2seq", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "seq_len", "=", "seq_len", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BertDataset.__len__": [[45, 47], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BertDataset.__getitem__": [[48, 64], ["data.InputExample", "data.convert_example_to_features"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_example_to_features"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "item", "=", "self", ".", "db", "[", "id_", "]", "\n", "t1", ",", "t2", "=", "item", "[", "'src'", "]", ",", "item", "[", "'tgt'", "]", "\n", "\n", "# combine to one sample", "\n", "cur_example", "=", "InputExample", "(", "guid", "=", "i", ",", "tokens_a", "=", "t1", ",", "tokens_b", "=", "t2", ")", "\n", "\n", "# transform sample to features", "\n", "cur_features", "=", "convert_example_to_features", "(", "\n", "cur_example", ",", "self", ".", "seq_len", ",", "self", ".", "tokenizer", ",", "self", ".", "vocab", ")", "\n", "\n", "features", "=", "(", "cur_features", ".", "input_ids", ",", "cur_features", ".", "input_mask", ",", "\n", "cur_features", ".", "segment_ids", ",", "cur_features", ".", "lm_label_ids", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BertDataset.pad_collate": [[65, 83], ["map", "max", "zip", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "toolz.sandbox.core.unzip", "map", "len", "ids.append", "masks.append", "segs.append", "labels.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "pad_collate", "(", "features", ")", ":", "\n", "        ", "\"\"\" pad the input features to same length\"\"\"", "\n", "input_ids", ",", "input_masks", ",", "segment_ids", ",", "lm_label_ids", "=", "map", "(", "\n", "list", ",", "unzip", "(", "features", ")", ")", "\n", "max_len", "=", "max", "(", "map", "(", "len", ",", "input_ids", ")", ")", "\n", "for", "ids", ",", "masks", ",", "segs", ",", "labels", "in", "zip", "(", "input_ids", ",", "input_masks", ",", "\n", "segment_ids", ",", "lm_label_ids", ")", ":", "\n", "            ", "while", "len", "(", "ids", ")", "<", "max_len", ":", "\n", "                ", "ids", ".", "append", "(", "0", ")", "\n", "masks", ".", "append", "(", "0", ")", "\n", "segs", ".", "append", "(", "0", ")", "\n", "labels", ".", "append", "(", "-", "1", ")", "\n", "", "", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "input_mask", "=", "torch", ".", "tensor", "(", "input_masks", ")", "\n", "segment_ids", "=", "torch", ".", "tensor", "(", "segment_ids", ")", "\n", "lm_label_ids", "=", "torch", ".", "tensor", "(", "lm_label_ids", ")", "\n", "return", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "lm_label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.InputExample.__init__": [[88, 104], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "tokens_a", ",", "tokens_b", "=", "None", ",", "lm_labels", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            tokens_a: string. The untokenized text of the first sequence. For\n                single sequence tasks, only this sequence must be specified.\n            tokens_b: (Optional) string. The untokenized text of the second\n                sequence. Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "tokens_a", "=", "tokens_a", "\n", "self", ".", "tokens_b", "=", "tokens_b", "\n", "self", ".", "lm_labels", "=", "lm_labels", "# masked words for language model", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.InputFeatures.__init__": [[109, 115], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "\n", "segment_ids", ",", "lm_label_ids", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "lm_label_ids", "=", "lm_label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BucketSampler.__init__": [[302, 307], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "lens", ",", "bucket_size", ",", "batch_size", ",", "droplast", "=", "False", ")", ":", "\n", "        ", "self", ".", "_lens", "=", "lens", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_bucket_size", "=", "bucket_size", "\n", "self", ".", "_droplast", "=", "droplast", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BucketSampler._create_ids": [[308, 310], ["list", "range", "len"], "methods", ["None"], ["", "def", "_create_ids", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "range", "(", "len", "(", "self", ".", "_lens", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BucketSampler._sort_fn": [[311, 313], ["None"], "methods", ["None"], ["", "def", "_sort_fn", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_lens", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BucketSampler.__iter__": [[314, 328], ["data.BucketSampler._create_ids", "random.shuffle", "random.shuffle", "iter", "sorted", "range", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler._create_ids"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "ids", "=", "self", ".", "_create_ids", "(", ")", "\n", "random", ".", "shuffle", "(", "ids", ")", "\n", "buckets", "=", "[", "sorted", "(", "ids", "[", "i", ":", "i", "+", "self", ".", "_bucket_size", "]", ",", "\n", "key", "=", "self", ".", "_sort_fn", ",", "reverse", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "ids", ")", ",", "self", ".", "_bucket_size", ")", "]", "\n", "batches", "=", "[", "bucket", "[", "i", ":", "i", "+", "self", ".", "_batch_size", "]", "\n", "for", "bucket", "in", "buckets", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "bucket", ")", ",", "self", ".", "_batch_size", ")", "]", "\n", "if", "self", ".", "_droplast", ":", "\n", "            ", "batches", "=", "[", "batch", "for", "batch", "in", "batches", "\n", "if", "len", "(", "batch", ")", "==", "self", ".", "_batch_size", "]", "\n", "", "random", ".", "shuffle", "(", "batches", ")", "\n", "return", "iter", "(", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.BucketSampler.__len__": [[329, 337], ["sum", "sum", "len", "len", "math.ceil"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "bucket_sizes", "=", "(", "[", "self", ".", "_bucket_size", "]", "\n", "*", "(", "len", "(", "self", ".", "_lens", ")", "//", "self", ".", "_bucket_size", ")", "\n", "+", "[", "len", "(", "self", ".", "_lens", ")", "%", "self", ".", "_bucket_size", "]", ")", "\n", "if", "self", ".", "_droplast", ":", "\n", "            ", "return", "sum", "(", "s", "//", "self", ".", "_batch_size", "for", "s", "in", "bucket_sizes", ")", "\n", "", "else", ":", "\n", "            ", "return", "sum", "(", "math", ".", "ceil", "(", "s", "/", "self", ".", "_batch_size", ")", "for", "s", "in", "bucket_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedBucketSampler.__init__": [[340, 344], ["data.BucketSampler.__init__"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_replicas", ",", "rank", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "_rank", "=", "rank", "\n", "self", ".", "_num_replicas", "=", "num_replicas", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedBucketSampler._create_ids": [[345, 347], ["data.BucketSampler._create_ids"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler._create_ids"], ["", "def", "_create_ids", "(", "self", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "_create_ids", "(", ")", "[", "self", ".", "_rank", ":", "-", "1", ":", "self", ".", "_num_replicas", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedBucketSampler.__len__": [[348, 357], ["len", "data.DistributedBucketSampler._create_ids", "sum", "sum", "math.ceil"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler._create_ids"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "num_data", "=", "len", "(", "self", ".", "_create_ids", "(", ")", ")", "\n", "bucket_sizes", "=", "(", "[", "self", ".", "_bucket_size", "]", "\n", "*", "(", "num_data", "//", "self", ".", "_bucket_size", ")", "\n", "+", "[", "num_data", "%", "self", ".", "_bucket_size", "]", ")", "\n", "if", "self", ".", "_droplast", ":", "\n", "            ", "return", "sum", "(", "s", "//", "self", ".", "_batch_size", "for", "s", "in", "bucket_sizes", ")", "\n", "", "else", ":", "\n", "            ", "return", "sum", "(", "math", ".", "ceil", "(", "s", "/", "self", ".", "_batch_size", ")", "for", "s", "in", "bucket_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.TokenBucketSampler.__init__": [[360, 365], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "lens", ",", "bucket_size", ",", "batch_size", ",", "droplast", "=", "False", ")", ":", "\n", "        ", "self", ".", "_lens", "=", "lens", "\n", "self", ".", "_max_tok", "=", "batch_size", "\n", "self", ".", "_bucket_size", "=", "bucket_size", "\n", "self", ".", "_droplast", "=", "droplast", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.TokenBucketSampler._create_ids": [[366, 368], ["list", "range", "len"], "methods", ["None"], ["", "def", "_create_ids", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "range", "(", "len", "(", "self", ".", "_lens", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.TokenBucketSampler._sort_fn": [[369, 371], ["None"], "methods", ["None"], ["", "def", "_sort_fn", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_lens", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.TokenBucketSampler.__iter__": [[372, 397], ["data.TokenBucketSampler._create_ids", "random.shuffle", "random.shuffle", "iter", "sorted", "range", "max", "batches.append", "len", "batches.append", "batch_indices.append", "ValueError", "len"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler._create_ids"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "ids", "=", "self", ".", "_create_ids", "(", ")", "\n", "random", ".", "shuffle", "(", "ids", ")", "\n", "buckets", "=", "[", "sorted", "(", "ids", "[", "i", ":", "i", "+", "self", ".", "_bucket_size", "]", ",", "\n", "key", "=", "self", ".", "_sort_fn", ",", "reverse", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "ids", ")", ",", "self", ".", "_bucket_size", ")", "]", "\n", "# fill batches until max_token (include padding)", "\n", "batches", "=", "[", "]", "\n", "for", "bucket", "in", "buckets", ":", "\n", "            ", "max_len", "=", "0", "\n", "batch_indices", "=", "[", "]", "\n", "for", "index", "in", "bucket", ":", "\n", "                ", "max_len", "=", "max", "(", "max_len", ",", "self", ".", "_lens", "[", "index", "]", ")", "\n", "if", "max_len", "*", "(", "len", "(", "batch_indices", ")", "+", "1", ")", ">", "self", ".", "_max_tok", ":", "\n", "                    ", "if", "not", "batch_indices", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "\"max_tokens too small / max_seq_len too long\"", ")", "\n", "", "batches", ".", "append", "(", "batch_indices", ")", "\n", "batch_indices", "=", "[", "index", "]", "\n", "", "else", ":", "\n", "                    ", "batch_indices", ".", "append", "(", "index", ")", "\n", "", "", "if", "not", "self", ".", "_droplast", "and", "batch_indices", ":", "\n", "                ", "batches", ".", "append", "(", "batch_indices", ")", "\n", "", "", "random", ".", "shuffle", "(", "batches", ")", "\n", "return", "iter", "(", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.TokenBucketSampler.__len__": [[398, 400], ["ValueError"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"NOT supported. \"", "\n", "\"This has some randomness across epochs\"", ")", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler.__init__": [[404, 408], ["data.TokenBucketSampler.__init__"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_replicas", ",", "rank", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "_rank", "=", "rank", "\n", "self", ".", "_num_replicas", "=", "num_replicas", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler._create_ids": [[409, 411], ["data.TokenBucketSampler._create_ids"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.DistributedTokenBucketSampler._create_ids"], ["", "def", "_create_ids", "(", "self", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "_create_ids", "(", ")", "[", "self", ".", "_rank", ":", "-", "1", ":", "self", ".", "_num_replicas", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data._truncate_seq_pair": [[117, 133], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal", "\n", "# percent of tokens from each, since if one sequence is very short then", "\n", "# each token that's truncated likely contains more information than a", "\n", "# longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert": [[135, 145], ["token.replace"], "function", ["None"], ["", "", "", "def", "convert_token_to_bert", "(", "token", ")", ":", "\n", "    ", "bert_token", "=", "token", ".", "replace", "(", "IN_WORD", ",", "''", ")", "\n", "try", ":", "\n", "        ", "bert_token", "=", "MOSES_SPECIALS", "[", "bert_token", "]", "# handle moses tokens", "\n", "", "except", "KeyError", ":", "\n", "        ", "pass", "\n", "", "if", "bert_token", "==", "UNK", ":", "\n", "# this should only happen with gigaword", "\n", "        ", "bert_token", "=", "UNK_BERT", "\n", "", "return", "bert_token", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_raw_input_to_features": [[147, 176], ["tgt_line.strip().split", "enumerate", "toker.convert_tokens_to_ids", "torch.LongTensor().to().unsqueeze", "torch.LongTensor().to().unsqueeze", "torch.LongTensor().to().unsqueeze", "torch.LongTensor().to().unsqueeze", "data.convert_token_to_bert", "random.random", "tgt_line.strip().split.append", "output_labels.append", "tgt_line.strip().split.append", "output_labels.append", "len", "src_line.strip().split", "tgt_line.strip", "random.random", "output_labels.append", "data.convert_token_to_bert", "output_labels.append", "len", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "len", "len", "src_line.strip", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert"], ["", "def", "convert_raw_input_to_features", "(", "src_line", ",", "tgt_line", ",", "\n", "toker", ",", "vocab", ",", "device", ",", "p", "=", "0.15", ")", ":", "\n", "    ", "src_toks", "=", "[", "convert_token_to_bert", "(", "tok", ")", "\n", "for", "tok", "in", "src_line", ".", "strip", "(", ")", ".", "split", "(", ")", "]", "\n", "tgt_toks", "=", "tgt_line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "output_labels", "=", "[", "]", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "tgt_toks", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "p", ":", "\n", "            ", "tgt_toks", "[", "i", "]", "=", "MASK", "\n", "output_labels", ".", "append", "(", "vocab", "[", "tok", "]", ")", "\n", "", "else", ":", "\n", "            ", "tgt_toks", "[", "i", "]", "=", "convert_token_to_bert", "(", "tok", ")", "\n", "output_labels", ".", "append", "(", "-", "1", ")", "\n", "", "", "if", "random", ".", "random", "(", ")", "<", "p", ":", "\n", "        ", "tgt_toks", ".", "append", "(", "MASK", ")", "\n", "output_labels", ".", "append", "(", "vocab", "[", "EOS", "]", ")", "\n", "", "else", ":", "\n", "        ", "tgt_toks", ".", "append", "(", "SEP", ")", "\n", "output_labels", ".", "append", "(", "-", "1", ")", "\n", "", "input_ids", "=", "toker", ".", "convert_tokens_to_ids", "(", "\n", "[", "CLS", "]", "+", "src_toks", "+", "[", "SEP", "]", "+", "tgt_toks", ")", "\n", "type_ids", "=", "[", "0", "]", "*", "(", "len", "(", "src_toks", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tgt_toks", ")", ")", "\n", "mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "labels", "=", "[", "-", "1", "]", "*", "(", "len", "(", "src_toks", ")", "+", "2", ")", "+", "output_labels", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "input_ids", ")", ".", "to", "(", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "type_ids", "=", "torch", ".", "LongTensor", "(", "type_ids", ")", ".", "to", "(", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "mask", "=", "torch", ".", "LongTensor", "(", "mask", ")", ".", "to", "(", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", ".", "to", "(", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "return", "input_ids", ",", "type_ids", ",", "mask", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.random_word": [[178, 221], ["enumerate", "random.random", "tokens.append", "output_label.append", "tokens.append", "output_label.append", "random.random", "data.convert_token_to_bert", "output_label.append", "output_label.append", "output_label.append", "warnings.warn"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert"], ["", "def", "random_word", "(", "tokens", ",", "output_vocab", ")", ":", "\n", "    ", "\"\"\"\n    NOTE: this assumes other MT prepro like moses and we try to align\n          them with BERT\n    Masking some random tokens for Language Model task with probabilities as in\n    the original BERT paper.\n    :param tokens: list of str, tokenized sentence.\n    :param output_vocab: vocab for seq2seq output\n    :return: (list of str, list of int), masked tokens and related labels for\n        LM prediction\n    \"\"\"", "\n", "output_label", "=", "[", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# mask token with 15% probability", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "0.15", ":", "\n", "# we always MASK given our purpose", "\n", "            ", "tokens", "[", "i", "]", "=", "MASK", "\n", "\n", "# append current token to output (we will predict these later)", "\n", "try", ":", "\n", "                ", "output_label", ".", "append", "(", "output_vocab", "[", "token", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "# For unknown words (should not occur with BPE vocab)", "\n", "                ", "output_label", ".", "append", "(", "output_vocab", "[", "UNK", "]", ")", "\n", "warnings", ".", "warn", "(", "f\"Cannot find token '{token}' in vocab. Using \"", "\n", "f\"{UNK} insetad\"", ")", "\n", "", "", "else", ":", "\n", "# handle input for BERT", "\n", "            ", "tokens", "[", "i", "]", "=", "convert_token_to_bert", "(", "token", ")", "\n", "\n", "# no masking token (will be ignored by loss function later)", "\n", "output_label", ".", "append", "(", "-", "1", ")", "\n", "\n", "# last SEP is used to learn EOS", "\n", "", "", "if", "random", ".", "random", "(", ")", "<", "0.15", ":", "\n", "        ", "tokens", ".", "append", "(", "MASK", ")", "\n", "output_label", ".", "append", "(", "output_vocab", "[", "EOS", "]", ")", "\n", "", "else", ":", "\n", "        ", "tokens", ".", "append", "(", "SEP", ")", "\n", "output_label", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "return", "tokens", ",", "output_label", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_example_to_features": [[223, 299], ["data._truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "data.InputFeatures", "data.convert_token_to_bert", "len", "data.random_word", "any", "tokens.append", "segment_ids.append", "len", "tokens.append", "segment_ids.append", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "lm_label_ids.append", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data._truncate_seq_pair", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.convert_token_to_bert", "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.data.random_word"], ["", "def", "convert_example_to_features", "(", "example", ",", "max_seq_length", ",", "\n", "tokenizer", ",", "output_vocab", ")", ":", "\n", "    ", "\"\"\"\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper\n    training sample with IDs, LM labels, input_mask, CLS and SEP tokens etc.\n    :param example: InputExample, containing sentence input as strings\n    :param max_seq_length: int, maximum length of sequence.\n    :param tokenizer: Tokenizer\n    :param output_vocab: vocab for seq2seq output\n    :return: InputFeatures, containing all inputs and labels of one sample as\n        IDs (as used for model training)\n    \"\"\"", "\n", "tokens_a", "=", "example", ".", "tokens_a", "\n", "tokens_b", "=", "example", ".", "tokens_b", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], EOS with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "\n", "# convert to BERT compatible inputs", "\n", "tokens_a", "=", "[", "convert_token_to_bert", "(", "tok", ")", "for", "tok", "in", "tokens_a", "]", "\n", "# only mask sent_b because it's seq2seq problem", "\n", "t1_label", "=", "[", "-", "1", "]", "*", "len", "(", "tokens_a", ")", "\n", "while", "True", ":", "\n", "        ", "tokens_b", ",", "t2_label", "=", "random_word", "(", "tokens_b", ",", "output_vocab", ")", "\n", "if", "any", "(", "label", "!=", "-", "1", "for", "label", "in", "t2_label", ")", ":", "\n", "            ", "break", "\n", "# concatenate lm labels and account for CLS, SEP", "\n", "", "", "lm_label_ids", "=", "(", "[", "-", "1", "]", "+", "t1_label", "+", "[", "-", "1", "]", "+", "t2_label", ")", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# For our MT setup", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [MASK]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "CLS", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "SEP", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "assert", "len", "(", "tokens_b", ")", ">", "0", "\n", "for", "token", "in", "tokens_b", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# NOTE the last SEP is handled differently from original BERT", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to multiples of 8 (for tensor cores)", "\n", "while", "len", "(", "input_ids", ")", "%", "8", "!=", "0", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "lm_label_ids", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "%", "8", "==", "0", "\n", "assert", "(", "len", "(", "input_ids", ")", "==", "len", "(", "input_mask", ")", "\n", "==", "len", "(", "segment_ids", ")", "==", "len", "(", "lm_label_ids", ")", ")", "\n", "\n", "features", "=", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "lm_label_ids", "=", "lm_label_ids", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.__init__": [[39, 42], ["pytorch_pretrained_bert.modeling.BertForMaskedLM.__init__", "model.BertForSeq2seq.apply"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "causal", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.update_output_layer": [[43, 48], ["output_embedding.size", "torch.nn.Parameter", "torch.zeros"], "methods", ["None"], ["", "def", "update_output_layer", "(", "self", ",", "output_embedding", ")", ":", "\n", "        ", "self", ".", "cls", ".", "predictions", ".", "decoder", ".", "weight", "=", "output_embedding", "\n", "vocab_size", "=", "output_embedding", ".", "size", "(", "0", ")", "\n", "self", ".", "cls", ".", "predictions", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "vocab_size", ")", ")", "\n", "self", ".", "config", ".", "vocab_size", "=", "vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.update_output_layer_by_size": [[49, 58], ["model.BertForSeq2seq.cls.predictions.decoder.weight.size", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.zeros"], "methods", ["None"], ["", "def", "update_output_layer_by_size", "(", "self", ",", "vocab_size", ")", ":", "\n", "        ", "if", "vocab_size", "%", "8", "!=", "0", ":", "\n", "# pad for tensor cores", "\n", "            ", "vocab_size", "+=", "(", "8", "-", "vocab_size", "%", "8", ")", "\n", "", "emb_dim", "=", "self", ".", "cls", ".", "predictions", ".", "decoder", ".", "weight", ".", "size", "(", "1", ")", "\n", "self", ".", "cls", ".", "predictions", ".", "decoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "vocab_size", ",", "emb_dim", ")", ")", "\n", "self", ".", "cls", ".", "predictions", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "vocab_size", ")", ")", "\n", "self", ".", "config", ".", "vocab_size", "=", "vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.update_embedding_layer_by_size": [[59, 67], ["model.BertForSeq2seq.cls.predictions.decoder.weight.size", "torch.nn.Embedding"], "methods", ["None"], ["", "def", "update_embedding_layer_by_size", "(", "self", ",", "vocab_size", ")", ":", "\n", "        ", "if", "vocab_size", "%", "8", "!=", "0", ":", "\n", "# pad for tensor cores", "\n", "            ", "vocab_size", "+=", "(", "8", "-", "vocab_size", "%", "8", ")", "\n", "", "emb_dim", "=", "self", ".", "cls", ".", "predictions", ".", "decoder", ".", "weight", ".", "size", "(", "1", ")", "\n", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "vocab_size", ",", "emb_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "config", ".", "vocab_size", "=", "vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.forward": [[68, 109], ["model.BertForSeq2seq.bert", "output_mask.byte.byte.byte", "sequence_output.masked_select().contiguous().view", "torch.cat.size", "model.BertForSeq2seq.cls.predictions", "super().forward", "torch.zeros", "torch.cat", "torch.nn.CrossEntropyLoss", "masked_lm_labels.masked_select", "torch.nn.CrossEntropyLoss.", "sequence_output.masked_select().contiguous", "torch.zeros().fill_", "torch.cat", "sequence_output.masked_select", "torch.zeros", "output_mask.byte.byte.unsqueeze().expand_as", "output_mask.byte.byte.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.BertForSeq2seq.forward"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "masked_lm_labels", "=", "None", ",", "output_mask", "=", "None", ",", "do_padding", "=", "True", ")", ":", "\n", "        ", "\"\"\" only computes masked logits to save some computation\"\"\"", "\n", "if", "output_mask", "is", "None", ":", "\n", "# reduce to normal forward", "\n", "            ", "return", "super", "(", ")", ".", "forward", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "masked_lm_labels", ")", "\n", "\n", "", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "# only compute masked outputs", "\n", "output_mask", "=", "output_mask", ".", "byte", "(", ")", "\n", "sequence_output_masked", "=", "sequence_output", ".", "masked_select", "(", "\n", "output_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "sequence_output", ")", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", "\n", "n_pred", ",", "hid", "=", "sequence_output_masked", ".", "size", "(", ")", "\n", "if", "do_padding", "and", "(", "n_pred", "==", "0", "or", "n_pred", "%", "8", ")", ":", "\n", "# pad for tensor cores", "\n", "            ", "n_pad", "=", "8", "-", "n_pred", "%", "8", "\n", "pad", "=", "torch", ".", "zeros", "(", "n_pad", ",", "hid", ",", "\n", "dtype", "=", "sequence_output_masked", ".", "dtype", ",", "\n", "device", "=", "sequence_output_masked", ".", "device", ")", "\n", "sequence_output_masked", "=", "torch", ".", "cat", "(", "\n", "[", "sequence_output_masked", ",", "pad", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "n_pad", "=", "0", "\n", "", "prediction_scores", "=", "self", ".", "cls", ".", "predictions", "(", "sequence_output_masked", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "lm_labels", "=", "masked_lm_labels", ".", "masked_select", "(", "output_mask", ")", "\n", "if", "n_pad", "!=", "0", ":", "\n", "                ", "pad", "=", "torch", ".", "zeros", "(", "n_pad", ",", "\n", "dtype", "=", "lm_labels", ".", "dtype", ",", "\n", "device", "=", "lm_labels", ".", "device", ")", ".", "fill_", "(", "-", "1", ")", "\n", "lm_labels", "=", "torch", ".", "cat", "(", "[", "lm_labels", ",", "pad", "]", ",", "dim", "=", "0", ")", "\n", "", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ",", "lm_labels", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.model.convert_embedding": [[15, 31], ["emb_weight.size", "vocab.items", "torch.nn.Parameter", "torch.zeros", "word.replace.replace", "emb_weight[].clone", "torch.stack", "range", "len"], "function", ["None"], ["def", "convert_embedding", "(", "toker", ",", "vocab", ",", "emb_weight", ")", ":", "\n", "    ", "\"\"\" seq2seq vs pretrained BERT embedding conversion\"\"\"", "\n", "vocab_size", "=", "emb_weight", ".", "size", "(", "1", ")", "\n", "if", "vocab_size", "%", "8", "!=", "0", ":", "\n", "# pad for tensor cores", "\n", "        ", "vocab_size", "+=", "(", "8", "-", "vocab_size", "%", "8", ")", "\n", "", "vectors", "=", "[", "torch", ".", "zeros", "(", "vocab_size", ")", "for", "_", "in", "range", "(", "len", "(", "vocab", ")", ")", "]", "\n", "for", "word", ",", "id_", "in", "vocab", ".", "items", "(", ")", ":", "\n", "        ", "word", "=", "word", ".", "replace", "(", "IN_WORD", ",", "''", ")", "\n", "if", "word", "in", "toker", ".", "vocab", ":", "\n", "            ", "bert_id", "=", "toker", ".", "vocab", "[", "word", "]", "\n", "", "else", ":", "\n", "            ", "bert_id", "=", "toker", ".", "vocab", "[", "'[UNK]'", "]", "\n", "", "vectors", "[", "id_", "]", "=", "emb_weight", "[", "bert_id", "]", ".", "clone", "(", ")", "\n", "", "embedding", "=", "nn", ".", "Parameter", "(", "torch", ".", "stack", "(", "vectors", ",", "dim", "=", "0", ")", ")", "\n", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.__init__": [[11, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_logger", "=", "None", "\n", "self", ".", "_global_step", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.create": [[15, 17], ["tensorboardX.SummaryWriter"], "methods", ["None"], ["", "def", "create", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "_logger", "=", "tensorboardX", ".", "SummaryWriter", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.noop": [[18, 20], ["None"], "methods", ["None"], ["", "def", "noop", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.step": [[21, 23], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "self", ".", "_global_step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.global_step": [[24, 27], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "global_step", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.log_scaler_dict": [[28, 41], ["log_dict.items", "isinstance", "util.Logger.log_scaler_dict", "util.Logger._logger.add_scalar"], "methods", ["home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.log_scaler_dict"], ["", "def", "log_scaler_dict", "(", "self", ",", "log_dict", ",", "prefix", "=", "''", ")", ":", "\n", "        ", "\"\"\" log a dictionary of scalar values\"\"\"", "\n", "if", "self", ".", "_logger", "is", "None", ":", "\n", "            ", "return", "\n", "", "if", "prefix", ":", "\n", "            ", "prefix", "=", "f'{prefix}_'", "\n", "", "for", "name", ",", "value", "in", "log_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "                ", "self", ".", "log_scaler_dict", "(", "value", ",", "self", ".", "_global_step", ",", "\n", "prefix", "=", "f'{prefix}{name}'", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_logger", ".", "add_scalar", "(", "f'{prefix}{name}'", ",", "value", ",", "\n", "self", ".", "_global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.Logger.__getattr__": [[42, 46], ["util.Logger._logger.__getattribute__"], "methods", ["None"], ["", "", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "if", "self", ".", "_logger", "is", "None", ":", "\n", "            ", "return", "self", ".", "noop", "\n", "", "return", "self", ".", "_logger", ".", "__getattribute__", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.__init__": [[52, 56], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "name", ",", "val", "=", "None", ",", "smooth", "=", "0.99", ")", ":", "\n", "        ", "self", ".", "_name", "=", "name", "\n", "self", ".", "_sm", "=", "smooth", "\n", "self", ".", "_val", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.__call__": [[57, 60], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_val", "=", "(", "value", "if", "self", ".", "_val", "is", "None", "\n", "else", "value", "*", "(", "1", "-", "self", ".", "_sm", ")", "+", "self", ".", "_val", "*", "self", ".", "_sm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.__str__": [[61, 63], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f'{self._name}: {self._val:.4f}'", "\n", "\n"]], "home.repos.pwc.inspect_result.ChenRocks_Distill-BERT-Textgen.cmlm.util.RunningMeter.val": [[64, 67], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "val", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_val", "\n", "", "", ""]]}