{"home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertEmbeddings.__init__": [[149, 159], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertEmbeddings.forward": [[160, 176], ["input_ids.size", "vilmodel.BertEmbeddings.word_embeddings", "vilmodel.BertEmbeddings.position_embeddings", "vilmodel.BertEmbeddings.token_type_embeddings", "vilmodel.BertEmbeddings.LayerNorm", "vilmodel.BertEmbeddings.dropout", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertSelfAttention.__init__": [[179, 196], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "\n", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertSelfAttention.transpose_for_scores": [[197, 201], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertSelfAttention.forward": [[202, 236], ["vilmodel.BertSelfAttention.query", "vilmodel.BertSelfAttention.key", "vilmodel.BertSelfAttention.value", "vilmodel.BertSelfAttention.transpose_for_scores", "vilmodel.BertSelfAttention.transpose_for_scores", "vilmodel.BertSelfAttention.transpose_for_scores", "torch.matmul", "vilmodel.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "vilmodel.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "self", ".", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertSelfOutput.__init__": [[239, 244], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertSelfOutput.forward": [[245, 250], ["vilmodel.BertSelfOutput.dense", "vilmodel.BertSelfOutput.dropout", "vilmodel.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAttention.__init__": [[253, 257], ["torch.nn.Module.__init__", "vilmodel.BertSelfAttention", "vilmodel.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAttention.prune_heads": [[258, 274], ["torch.ones", "mask.view().contiguous().eq.view().contiguous().eq.view().contiguous().eq", "[].long", "prune_linear_layer", "prune_linear_layer", "prune_linear_layer", "prune_linear_layer", "len", "len", "mask.view().contiguous().eq.view().contiguous().eq.view().contiguous", "torch.arange", "mask.view().contiguous().eq.view().contiguous().eq.view", "len"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_linear_layer", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_linear_layer", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_linear_layer", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_linear_layer"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "mask", "=", "torch", ".", "ones", "(", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ")", "\n", "for", "head", "in", "heads", ":", "\n", "            ", "mask", "[", "head", "]", "=", "0", "\n", "", "mask", "=", "mask", ".", "view", "(", "-", "1", ")", ".", "contiguous", "(", ")", ".", "eq", "(", "1", ")", "\n", "index", "=", "torch", ".", "arange", "(", "len", "(", "mask", ")", ")", "[", "mask", "]", ".", "long", "(", ")", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "# Update hyper params", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAttention.forward": [[275, 280], ["vilmodel.BertAttention.self", "vilmodel.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ",", "head_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "input_tensor", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertIntermediate.__init__": [[283, 290], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertIntermediate.forward": [[291, 295], ["vilmodel.BertIntermediate.dense", "vilmodel.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutput.__init__": [[298, 303], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutput.forward": [[304, 309], ["vilmodel.BertOutput.dense", "vilmodel.BertOutput.dropout", "vilmodel.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertLayer.__init__": [[312, 317], ["torch.nn.Module.__init__", "vilmodel.BertAttention", "vilmodel.BertIntermediate", "vilmodel.BertOutput"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertLayer.forward": [[318, 325], ["vilmodel.BertLayer.attention", "vilmodel.BertLayer.intermediate", "vilmodel.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "attention_outputs", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ",", "head_mask", ")", "\n", "attention_output", "=", "attention_outputs", "[", "0", "]", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "attention_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertEncoder.__init__": [[328, 333], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "vilmodel.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "self", ".", "output_hidden_states", "=", "config", ".", "output_hidden_states", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertEncoder.forward": [[334, 357], ["enumerate", "layer_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "\n", "all_attentions", "=", "(", ")", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "self", ".", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "# Add last layer", "\n", "", "", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_hidden_states", ",", ")", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_attentions", ",", ")", "\n", "", "return", "outputs", "# last-layer hidden state, (all hidden states), (all attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertPooler.__init__": [[360, 364], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertPooler.forward": [[365, 372], ["vilmodel.BertPooler.dense", "vilmodel.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertPredictionHeadTransform.__init__": [[375, 383], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertPredictionHeadTransform.forward": [[384, 389], ["vilmodel.BertPredictionHeadTransform.dense", "vilmodel.BertPredictionHeadTransform.transform_act_fn", "vilmodel.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertLMPredictionHead.__init__": [[392, 403], ["torch.nn.Module.__init__", "vilmodel.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "\n", "config", ".", "vocab_size", ",", "\n", "bias", "=", "False", ")", "\n", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "vocab_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertLMPredictionHead.forward": [[404, 408], ["vilmodel.BertLMPredictionHead.transform", "vilmodel.BertLMPredictionHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOnlyMLMHead.__init__": [[411, 414], ["torch.nn.Module.__init__", "vilmodel.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOnlyMLMHead.forward": [[415, 418], ["vilmodel.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOnlyNSPHead.__init__": [[421, 424], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOnlyNSPHead.forward": [[425, 428], ["vilmodel.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertPreTrainingHeads.__init__": [[431, 435], ["torch.nn.Module.__init__", "vilmodel.BertLMPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertPreTrainingHeads.forward": [[436, 440], ["vilmodel.BertPreTrainingHeads.predictions", "vilmodel.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertXAttention.__init__": [[443, 447], ["torch.nn.Module.__init__", "vilmodel.BertOutAttention", "vilmodel.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "att", "=", "BertOutAttention", "(", "config", ",", "ctx_dim", "=", "ctx_dim", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertXAttention.forward": [[448, 452], ["vilmodel.BertXAttention.att", "vilmodel.BertXAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", "=", "None", ")", ":", "\n", "        ", "output", ",", "attention_scores", "=", "self", ".", "att", "(", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "output", ",", "input_tensor", ")", "\n", "return", "attention_output", ",", "attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertXAttentionOld.__init__": [[454, 458], ["torch.nn.Module.__init__", "vilmodel.BertOutAttentionOld", "vilmodel.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "att", "=", "BertOutAttentionOld", "(", "config", ",", "ctx_dim", "=", "ctx_dim", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertXAttentionOld.forward": [[459, 463], ["vilmodel.BertXAttentionOld.att", "vilmodel.BertXAttentionOld.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", "=", "None", ")", ":", "\n", "        ", "output", "=", "self", ".", "att", "(", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttention.__init__": [[465, 483], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "# visual_dim = 2048", "\n", "if", "ctx_dim", "is", "None", ":", "\n", "            ", "ctx_dim", "=", "config", ".", "hidden_size", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttention.transpose_for_scores": [[484, 488], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttention.forward": [[489, 518], ["vilmodel.BertOutAttention.query", "vilmodel.BertOutAttention.key", "vilmodel.BertOutAttention.value", "vilmodel.BertOutAttention.transpose_for_scores", "vilmodel.BertOutAttention.transpose_for_scores", "vilmodel.BertOutAttention.transpose_for_scores", "torch.matmul", "vilmodel.BertOutAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "vilmodel.BertOutAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "context", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "context", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "context", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", ",", "attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.__init__": [[520, 538], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "# visual_dim = 2048", "\n", "if", "ctx_dim", "is", "None", ":", "\n", "            ", "ctx_dim", "=", "config", ".", "hidden_size", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores": [[539, 543], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.forward": [[544, 572], ["vilmodel.BertOutAttentionOld.query", "vilmodel.BertOutAttentionOld.key", "vilmodel.BertOutAttentionOld.value", "vilmodel.BertOutAttentionOld.transpose_for_scores", "vilmodel.BertOutAttentionOld.transpose_for_scores", "vilmodel.BertOutAttentionOld.transpose_for_scores", "torch.matmul", "vilmodel.BertOutAttentionOld.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "vilmodel.BertOutAttentionOld.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertOutAttentionOld.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "context", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "context", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "context", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.AddEncoder.__init__": [[628, 633], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "vilmodel.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "AddEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "self", ".", "output_hidden_states", "=", "config", ".", "output_hidden_states", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "vl_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.AddEncoder.forward": [[634, 671], ["attention_mask.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "enumerate", "attention_mask.unsqueeze", "layer_module", "layer_module", "next", "vilmodel.AddEncoder.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "\n", "all_attentions", "=", "(", ")", "\n", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "self", ".", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "head_mask", "is", "not", "None", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "hidden_states", ",", "extended_attention_mask", ",", "head_mask", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "hidden_states", ",", "extended_attention_mask", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "# Add last layer", "\n", "", "", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_hidden_states", ",", ")", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_attentions", ",", ")", "\n", "", "return", "outputs", "# last-layer hidden state, (all hidden states), (all attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAddModel.__init__": [[703, 733], ["pytorch_transformers.BertPreTrainedModel.__init__", "vilmodel.BertEmbeddings", "vilmodel.BertEncoder", "vilmodel.BertPooler", "logger.info", "vilmodel.AddEncoder", "vilmodel.BertAddModel.init_weights", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAddModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "\n", "self", ".", "img_dim", "=", "config", ".", "img_feature_dim", "#2054 #565", "\n", "logger", ".", "info", "(", "'BertAddModel Image Dimension: {}'", ".", "format", "(", "self", ".", "img_dim", ")", ")", "\n", "self", ".", "img_feature_type", "=", "config", ".", "img_feature_type", "\n", "self", ".", "vl_layers", "=", "config", ".", "vl_layers", "\n", "self", ".", "update_lang_bert", "=", "config", ".", "update_lang_bert", "\n", "self", ".", "update_add_layer", "=", "config", ".", "update_add_layer", "\n", "self", ".", "addlayer", "=", "AddEncoder", "(", "config", ")", "\n", "\n", "if", "config", ".", "img_feature_type", "==", "'dis_code'", ":", "\n", "            ", "self", ".", "code_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "code_voc", ",", "config", ".", "code_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "img_embedding", "=", "nn", ".", "Linear", "(", "config", ".", "code_dim", ",", "self", ".", "config", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "", "elif", "config", ".", "img_feature_type", "==", "'dis_code_t'", ":", "# transpose", "\n", "            ", "self", ".", "code_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "code_voc", ",", "config", ".", "code_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "img_embedding", "=", "nn", ".", "Linear", "(", "config", ".", "code_size", ",", "self", ".", "config", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "", "elif", "config", ".", "img_feature_type", "==", "'dis_code_scale'", ":", "# scaled", "\n", "            ", "self", ".", "input_embeddings", "=", "nn", ".", "Linear", "(", "config", ".", "code_dim", ",", "config", ".", "code_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "code_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "code_voc", ",", "config", ".", "code_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "img_embedding", "=", "nn", ".", "Linear", "(", "config", ".", "code_dim", ",", "self", ".", "config", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "img_embedding", "=", "nn", ".", "Linear", "(", "self", ".", "img_dim", ",", "self", ".", "config", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "\n", "#self.apply(self.init_weights)", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAddModel._resize_token_embeddings": [[734, 739], ["vilmodel.BertAddModel._get_resized_embeddings"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel._get_resized_embeddings"], ["", "def", "_resize_token_embeddings", "(", "self", ",", "new_num_tokens", ")", ":", "\n", "        ", "old_embeddings", "=", "self", ".", "embeddings", ".", "word_embeddings", "\n", "new_embeddings", "=", "self", ".", "_get_resized_embeddings", "(", "old_embeddings", ",", "new_num_tokens", ")", "\n", "self", ".", "embeddings", ".", "word_embeddings", "=", "new_embeddings", "\n", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAddModel._prune_heads": [[740, 747], ["heads_to_prune.items", "vilmodel.BertAddModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.BertAddModel.forward": [[748, 824], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "vilmodel.BertAddModel.embeddings", "vilmodel.BertAddModel.encoder", "torch.ones_like", "torch.zeros_like", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "text_embeds.detach.detach.detach", "text_embeds.detach.detach.size", "torch.ones().to", "torch.cat", "torch.cat", "vilmodel.BertAddModel.addlayer", "vilmodel.BertAddModel.pooler", "vilmodel.BertAddModel.pooler", "torch.ones_like.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "vilmodel.BertAddModel.code_embeddings", "vilmodel.BertAddModel.img_embedding", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "vilmodel.BertAddModel.code_embeddings", "vilmodel.BertAddModel.permute", "vilmodel.BertAddModel.img_embedding", "torch.ones", "vilmodel.BertAddModel.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "vilmodel.BertAddModel.code_embeddings", "vilmodel.BertAddModel.img_embedding", "vilmodel.BertAddModel.img_embedding", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "vilmodel.BertAddModel.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "position_ids", "=", "None", ",", "head_mask", "=", "None", ",", "img_feats", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "#if img_feats is not None: attention_mask = torch.ones_like((input_ids.shape[0], input_ids.shape[1]+img_feats.shape[1]))", "\n", "#else: attention_mask = torch.ones_like(input_ids)", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "self", ".", "config", ".", "num_hidden_layers", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "# We can specify head_mask for each layer", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# switch to fload if need + fp16 compatibility", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "num_hidden_layers", "\n", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "# bertimgmodel will send img feature before self.encoder,but bertaddmodel will add img feature after self.encoder", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", "=", "head_mask", ")", "\n", "text_embeds", "=", "encoder_outputs", "[", "0", "]", "\n", "if", "not", "self", ".", "update_lang_bert", ":", "\n", "            ", "text_embeds", "=", "text_embeds", ".", "detach", "(", ")", "\n", "\n", "\n", "", "if", "img_feats", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "img_feature_type", "==", "'dis_code'", ":", "\n", "                ", "code_emb", "=", "self", ".", "code_embeddings", "(", "img_feats", ")", "\n", "img_embedding_output", "=", "self", ".", "img_embedding", "(", "code_emb", ")", "\n", "", "elif", "self", ".", "img_feature_type", "==", "'dis_code_t'", ":", "# transpose", "\n", "                ", "code_emb", "=", "self", ".", "code_embeddings", "(", "img_feats", ")", "\n", "code_emb", "=", "code_emb", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "img_embedding_output", "=", "self", ".", "img_embedding", "(", "code_emb", ")", "\n", "", "elif", "self", ".", "img_feature_type", "==", "'dis_code_scale'", ":", "# left scaled", "\n", "                ", "code_emb", "=", "self", ".", "code_embeddings", "(", "img_feats", ")", "\n", "img_embedding_output", "=", "self", ".", "img_embedding", "(", "code_emb", ")", "\n", "", "else", ":", "\n", "                ", "img_embedding_output", "=", "self", ".", "img_embedding", "(", "img_feats", ")", "\n", "\n", "", "img_seq_len", "=", "img_feats", ".", "shape", "[", "1", "]", "\n", "batch_size", "=", "text_embeds", ".", "size", "(", "0", ")", "\n", "img_seq_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "img_seq_len", ",", "dtype", "=", "attention_mask", ".", "dtype", ")", ".", "to", "(", "attention_mask", ".", "device", ")", "\n", "att_mask", "=", "torch", ".", "cat", "(", "(", "img_seq_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "input_embeds", "=", "torch", ".", "cat", "(", "(", "img_embedding_output", ",", "text_embeds", ")", ",", "1", ")", "\n", "outputs", "=", "self", ".", "addlayer", "(", "input_embeds", ",", "att_mask", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", "[", ":", ",", "img_seq_len", ":", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "            ", "sequence_output", "=", "text_embeds", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "\n", "", "outputs", "=", "(", "sequence_output", ",", "pooled_output", ",", ")", "+", "encoder_outputs", "[", "1", ":", "]", "# add hidden_states and attentions if they are here", "\n", "return", "outputs", "# sequence_output, pooled_output, (hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayerRe.__init__": [[826, 839], ["torch.nn.Module.__init__", "vilmodel.BertAttention", "vilmodel.BertIntermediate", "vilmodel.BertOutput", "vilmodel.BertAttention", "vilmodel.BertIntermediate", "vilmodel.BertOutput", "vilmodel.BertXAttention"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "# Lang self-att and FFN layer", "\n", "self", ".", "lang_self_att", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "lang_inter", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "lang_output", "=", "BertOutput", "(", "config", ")", "\n", "# Visn self-att and FFN layer", "\n", "self", ".", "visn_self_att", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "visn_inter", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "visn_output", "=", "BertOutput", "(", "config", ")", "\n", "# The cross attention layer", "\n", "self", ".", "visual_attention", "=", "BertXAttention", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayerRe.cross_att": [[840, 844], ["vilmodel.LXRTXLayerRe.visual_attention"], "methods", ["None"], ["", "def", "cross_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "        ", "''' Cross Attention -- cross for vision not for language '''", "\n", "visn_att_output", ",", "attention_scores", "=", "self", ".", "visual_attention", "(", "visn_input", ",", "lang_input", ",", "ctx_att_mask", "=", "lang_attention_mask", ")", "\n", "return", "visn_att_output", ",", "attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayerRe.self_att": [[845, 849], ["vilmodel.LXRTXLayerRe.visn_self_att"], "methods", ["None"], ["", "def", "self_att", "(", "self", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "        ", "''' Self Attention -- on visual features with language clues '''", "\n", "visn_att_output", "=", "self", ".", "visn_self_att", "(", "visn_input", ",", "visn_attention_mask", ")", "\n", "return", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayerRe.output_fc": [[850, 855], ["vilmodel.LXRTXLayerRe.visn_inter", "vilmodel.LXRTXLayerRe.visn_output"], "methods", ["None"], ["", "def", "output_fc", "(", "self", ",", "visn_input", ")", ":", "\n", "        ", "''' Feed forward '''", "\n", "visn_inter_output", "=", "self", ".", "visn_inter", "(", "visn_input", ")", "\n", "visn_output", "=", "self", ".", "visn_output", "(", "visn_inter_output", ",", "visn_input", ")", "\n", "return", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayerRe.forward": [[856, 879], ["torch.cat", "torch.cat", "vilmodel.LXRTXLayerRe.cross_att", "vilmodel.LXRTXLayerRe.self_att", "vilmodel.LXRTXLayerRe.output_fc", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.cross_att", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.self_att", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.output_fc"], ["", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", ":", "\n", "#set_trace()", "\n", "\n", "        ", "''' visual self-attention with state '''", "\n", "visn_att_output", "=", "torch", ".", "cat", "(", "(", "lang_feats", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "visn_feats", ")", ",", "dim", "=", "1", ")", "#bs 8,768", "\n", "state_vis_mask", "=", "torch", ".", "cat", "(", "(", "lang_attention_mask", "[", ":", ",", ":", ",", ":", ",", "0", ":", "1", "]", ",", "visn_attention_mask", ")", ",", "dim", "=", "-", "1", ")", "#bs,1,1,8", "\n", "\n", "''' state and vision attend to language '''", "\n", "visn_att_output", ",", "cross_attention_scores", "=", "self", ".", "cross_att", "(", "lang_feats", "[", ":", ",", "1", ":", ",", ":", "]", ",", "lang_attention_mask", "[", ":", ",", ":", ",", ":", ",", "1", ":", "]", ",", "visn_att_output", ",", "state_vis_mask", ")", "\n", "\n", "language_attention_scores", "=", "cross_attention_scores", "[", ":", ",", ":", ",", "0", ",", ":", "]", "#([3, 12, 8, 33])", "\n", "\n", "state_visn_att_output", "=", "self", ".", "self_att", "(", "visn_att_output", ",", "state_vis_mask", ")", "\n", "state_visn_output", "=", "self", ".", "output_fc", "(", "state_visn_att_output", "[", "0", "]", ")", "\n", "\n", "visn_att_output", "=", "state_visn_output", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "lang_att_output", "=", "torch", ".", "cat", "(", "(", "state_visn_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "lang_feats", "[", ":", ",", "1", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "\n", "#visual_attention_scores = state_visn_att_output[1][:, :, 0, 1:] #[1,3,8,768]", "\n", "visual_attention_scores", "=", "state_visn_att_output", "[", ":", ",", ":", ",", "0", ",", "1", ":", "]", "\n", "\n", "return", "lang_att_output", ",", "visn_att_output", ",", "language_attention_scores", ",", "visual_attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.__init__": [[881, 896], ["torch.nn.Module.__init__", "vilmodel.BertAttention", "vilmodel.BertIntermediate", "vilmodel.BertOutput", "vilmodel.BertAttention", "vilmodel.BertIntermediate", "vilmodel.BertOutput", "vilmodel.BertXAttentionOld"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Lang self-att and FFN layer", "\n", "self", ".", "lang_self_att", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "lang_inter", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "lang_output", "=", "BertOutput", "(", "config", ")", "\n", "\n", "# Visn self-att and FFN layer", "\n", "self", ".", "visn_self_att", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "visn_inter", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "visn_output", "=", "BertOutput", "(", "config", ")", "\n", "\n", "# The cross attention layer", "\n", "self", ".", "visual_attention", "=", "BertXAttentionOld", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.cross_att": [[897, 902], ["vilmodel.LXRTXLayer.visual_attention", "vilmodel.LXRTXLayer.visual_attention"], "methods", ["None"], ["", "def", "cross_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "# Cross Attention", "\n", "        ", "lang_att_output", "=", "self", ".", "visual_attention", "(", "lang_input", ",", "visn_input", ",", "ctx_att_mask", "=", "visn_attention_mask", ")", "\n", "visn_att_output", "=", "self", ".", "visual_attention", "(", "visn_input", ",", "lang_input", ",", "ctx_att_mask", "=", "lang_attention_mask", ")", "\n", "return", "lang_att_output", ",", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.self_att": [[903, 908], ["vilmodel.LXRTXLayer.lang_self_att", "vilmodel.LXRTXLayer.visn_self_att"], "methods", ["None"], ["", "def", "self_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "# Self Attention", "\n", "        ", "lang_att_output", "=", "self", ".", "lang_self_att", "(", "lang_input", ",", "lang_attention_mask", ")", "\n", "visn_att_output", "=", "self", ".", "visn_self_att", "(", "visn_input", ",", "visn_attention_mask", ")", "\n", "return", "lang_att_output", ",", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.output_fc": [[909, 918], ["vilmodel.LXRTXLayer.lang_inter", "vilmodel.LXRTXLayer.visn_inter", "vilmodel.LXRTXLayer.lang_output", "vilmodel.LXRTXLayer.visn_output"], "methods", ["None"], ["", "def", "output_fc", "(", "self", ",", "lang_input", ",", "visn_input", ")", ":", "\n", "# FC layers", "\n", "        ", "lang_inter_output", "=", "self", ".", "lang_inter", "(", "lang_input", ")", "\n", "visn_inter_output", "=", "self", ".", "visn_inter", "(", "visn_input", ")", "\n", "\n", "# Layer output", "\n", "lang_output", "=", "self", ".", "lang_output", "(", "lang_inter_output", ",", "lang_input", ")", "\n", "visn_output", "=", "self", ".", "visn_output", "(", "visn_inter_output", ",", "visn_input", ")", "\n", "return", "lang_output", ",", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.forward": [[919, 932], ["vilmodel.LXRTXLayer.cross_att", "vilmodel.LXRTXLayer.self_att", "vilmodel.LXRTXLayer.output_fc"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.cross_att", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.self_att", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.LXRTXLayer.output_fc"], ["", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", ":", "\n", "#set_trace()", "\n", "        ", "lang_att_output", "=", "lang_feats", "\n", "visn_att_output", "=", "visn_feats", "\n", "\n", "lang_att_output", ",", "visn_att_output", "=", "self", ".", "cross_att", "(", "lang_att_output", ",", "lang_attention_mask", ",", "\n", "visn_att_output", ",", "visn_attention_mask", ")", "\n", "lang_att_output", ",", "visn_att_output", "=", "self", ".", "self_att", "(", "lang_att_output", ",", "lang_attention_mask", ",", "\n", "visn_att_output", ",", "visn_attention_mask", ")", "\n", "lang_output", ",", "visn_output", "=", "self", ".", "output_fc", "(", "lang_att_output", "[", "0", "]", ",", "visn_att_output", "[", "0", "]", ")", "\n", "\n", "return", "lang_output", ",", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.VisionEncoder.__init__": [[935, 951], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Embedding", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vision_size", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "feat_dim", "=", "vision_size", "\n", "#pos_dim = VISUAL_CONFIG.visual_pos_dim", "\n", "\n", "# Object feature encoding", "\n", "self", ".", "visn_fc", "=", "nn", ".", "Linear", "(", "feat_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "visn_layer_norm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n", "self", ".", "type_embeddings", "=", "nn", ".", "Embedding", "(", "3", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# Box position encoding", "\n", "#self.box_fc = nn.Linear(pos_dim, config.hidden_size)", "\n", "#self.box_layer_norm = BertLayerNorm(config.hidden_size, eps=1e-12)", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.VisionEncoder.forward": [[952, 965], ["len", "torch.Tensor().long().cuda().unsqueeze().repeat", "vilmodel.VisionEncoder.type_embeddings", "vilmodel.VisionEncoder.visn_fc", "vilmodel.VisionEncoder.visn_layer_norm", "vilmodel.VisionEncoder.dropout", "torch.Tensor().long().cuda().unsqueeze", "torch.Tensor().long().cuda", "torch.Tensor().long", "torch.Tensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "visn_input", ",", "visn_token", ")", ":", "\n", "#feats, boxes = visn_input", "\n", "        ", "bs", "=", "len", "(", "visn_input", ")", "\n", "visn_token_1", "=", "torch", ".", "Tensor", "(", "visn_token", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "bs", ",", "1", ")", "\n", "#set_trace()", "\n", "type_feats", "=", "self", ".", "type_embeddings", "(", "visn_token_1", ")", "\n", "#feats = visn_input + type_feats", "\n", "\n", "visn_feats", "=", "self", ".", "visn_fc", "(", "visn_input", ")", "\n", "x", "=", "visn_feats", "+", "type_feats", "\n", "x", "=", "self", ".", "visn_layer_norm", "(", "x", ")", "\n", "output", "=", "self", ".", "dropout", "(", "x", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.VisionEncoder_org.__init__": [[967, 983], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vision_size", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "feat_dim", "=", "vision_size", "\n", "#pos_dim = VISUAL_CONFIG.visual_pos_dim", "\n", "\n", "# Object feature encoding", "\n", "self", ".", "visn_fc", "=", "nn", ".", "Linear", "(", "feat_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "visn_layer_norm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n", "#        self.type_embeddings = nn.Embedding(3, config.hidden_size)", "\n", "\n", "# Box position encoding", "\n", "#self.box_fc = nn.Linear(pos_dim, config.hidden_size)", "\n", "#self.box_layer_norm = BertLayerNorm(config.hidden_size, eps=1e-12)", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.VisionEncoder_org.forward": [[984, 993], ["vilmodel.VisionEncoder_org.visn_fc", "vilmodel.VisionEncoder_org.visn_layer_norm", "vilmodel.VisionEncoder_org.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "visn_input", ")", ":", "\n", "#feats, boxes = visn_input", "\n", "#set_trace()", "\n", "        ", "feats", "=", "visn_input", "\n", "\n", "x", "=", "self", ".", "visn_fc", "(", "feats", ")", "\n", "x", "=", "self", ".", "visn_layer_norm", "(", "x", ")", "\n", "output", "=", "self", ".", "dropout", "(", "x", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.DicModel.__init__": [[1025, 1046], ["pytorch_transformers.BertPreTrainedModel.__init__", "vilmodel.BertEmbeddings", "vilmodel.BertPooler", "logger.info", "torch.nn.ModuleList", "torch.nn.ModuleList", "vilmodel.VisionEncoder", "vilmodel.VisionEncoder_org", "vilmodel.DicModel.init_weights", "vilmodel.BertLayer", "vilmodel.LXRTXLayer", "range", "range"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "DicModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "\n", "self", ".", "img_dim", "=", "config", ".", "img_feature_dim", "#2054 #565", "\n", "logger", ".", "info", "(", "'DicModel Image Dimension: {}'", ".", "format", "(", "self", ".", "img_dim", ")", ")", "\n", "self", ".", "img_feature_type", "=", "config", ".", "img_feature_type", "\n", "self", ".", "vl_layers", "=", "config", ".", "vl_layers", "\n", "self", ".", "la_layers", "=", "config", ".", "la_layers", "\n", "self", ".", "update_lang_bert", "=", "config", ".", "update_lang_bert", "\n", "self", ".", "update_add_layer", "=", "config", ".", "update_add_layer", "\n", "self", ".", "lalayer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "self", ".", "la_layers", ")", "]", "\n", ")", "\n", "self", ".", "addlayer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "LXRTXLayer", "(", "config", ")", "for", "_", "in", "range", "(", "self", ".", "vl_layers", ")", "]", "\n", ")", "\n", "self", ".", "vision_encoder", "=", "VisionEncoder", "(", "self", ".", "config", ".", "img_feature_dim", ",", "self", ".", "config", ")", "\n", "self", ".", "vision_encoder_org", "=", "VisionEncoder_org", "(", "self", ".", "config", ".", "img_feature_dim", ",", "self", ".", "config", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.DicModel._resize_token_embeddings": [[1047, 1052], ["vilmodel.DicModel._get_resized_embeddings"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel._get_resized_embeddings"], ["", "def", "_resize_token_embeddings", "(", "self", ",", "new_num_tokens", ")", ":", "\n", "        ", "old_embeddings", "=", "self", ".", "embeddings", ".", "word_embeddings", "\n", "new_embeddings", "=", "self", ".", "_get_resized_embeddings", "(", "old_embeddings", ",", "new_num_tokens", ")", "\n", "self", ".", "embeddings", ".", "word_embeddings", "=", "new_embeddings", "\n", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.DicModel._prune_heads": [[1053, 1060], ["heads_to_prune.items", "vilmodel.DicModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.DicModel.forward": [[1062, 1160], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "vilmodel.DicModel.embeddings", "vilmodel.DicModel.pooler", "torch.ones_like", "torch.zeros_like", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "layer_module", "text_embeds.detach.detach.detach", "text_embeds.detach.detach.size", "torch.ones().to.unsqueeze().unsqueeze", "extended_img_mask.to.to.to", "torch.cat", "vilmodel.DicModel.pooler", "vilmodel.DicModel.pooler", "vilmodel.DicModel.pooler", "vilmodel.DicModel.pooler", "torch.ones_like.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "len", "vilmodel.DicModel.vision_encoder", "vilmodel.DicModel.vision_encoder_org", "torch.ones().to", "layer_module", "lang_output.detach.detach.detach", "visn_output.detach.detach.detach", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "torch.ones().to.unsqueeze", "h_t.unsqueeze", "vilmodel.DicModel.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "torch.ones", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "vilmodel.DicModel.parameters", "vilmodel.DicModel.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "position_ids", "=", "None", ",", "head_mask", "=", "None", ",", "img_feats", "=", "None", ",", "img_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "#set_trace()", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "self", ".", "config", ".", "num_hidden_layers", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "# We can specify head_mask for each layer", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# switch to fload if need + fp16 compatibility", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "num_hidden_layers", "\n", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "#encoder_outputs = self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask)", "\n", "text_embeds", "=", "embedding_output", "\n", "for", "layer_module", "in", "self", ".", "lalayer", ":", "\n", "            ", "temp_output", "=", "layer_module", "(", "text_embeds", ",", "extended_attention_mask", ")", "\n", "text_embeds", "=", "temp_output", "[", "0", "]", "\n", "\n", "", "sequence_output_lang", "=", "text_embeds", "\n", "pooled_output_lang", "=", "self", ".", "pooler", "(", "sequence_output_lang", ")", "\n", "h_t", "=", "pooled_output_lang", "\n", "language_features", "=", "sequence_output_lang", "\n", "\n", "if", "not", "self", ".", "update_lang_bert", ":", "\n", "            ", "text_embeds", "=", "text_embeds", ".", "detach", "(", ")", "\n", "\n", "", "if", "img_feats", "is", "not", "None", ":", "\n", "            ", "text_mask", "=", "extended_attention_mask", "\n", "if", "len", "(", "img_feats", "[", "0", "]", ")", ">", "7", ":", "\n", "                ", "segment_ids", "=", "[", "1", "]", "*", "(", "7", ")", "+", "[", "0", "]", "*", "(", "1", ")", "+", "[", "0", "]", "*", "(", "36", ")", "\n", "img_embedding_output", "=", "self", ".", "vision_encoder", "(", "img_feats", ",", "segment_ids", ")", "\n", "", "else", ":", "\n", "                ", "img_embedding_output", "=", "self", ".", "vision_encoder_org", "(", "img_feats", ")", "\n", "\n", "", "img_seq_len", "=", "img_feats", ".", "shape", "[", "1", "]", "\n", "batch_size", "=", "text_embeds", ".", "size", "(", "0", ")", "\n", "\n", "if", "img_mask", "is", "None", ":", "\n", "                ", "img_seq_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "img_seq_len", ",", "dtype", "=", "attention_mask", ".", "dtype", ")", ".", "to", "(", "attention_mask", ".", "device", ")", "\n", "", "else", ":", "\n", "                ", "img_seq_mask", "=", "img_mask", "\n", "\n", "", "extended_img_mask", "=", "img_seq_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "extended_img_mask", "=", "extended_img_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_img_mask", "=", "(", "1.0", "-", "extended_img_mask", ")", "*", "-", "10000.0", "\n", "img_mask", "=", "extended_img_mask", "\n", "\n", "language_features", "=", "torch", ".", "cat", "(", "(", "h_t", ".", "unsqueeze", "(", "1", ")", ",", "language_features", "[", ":", ",", "1", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "lang_output", "=", "language_features", "\n", "visn_output", "=", "img_embedding_output", "\n", "\n", "for", "layer_module", "in", "self", ".", "addlayer", ":", "\n", "                ", "lang_output", ",", "visn_output", "=", "layer_module", "(", "lang_output", ",", "text_mask", ",", "visn_output", ",", "img_mask", ")", "\n", "\n", "", "if", "not", "self", ".", "update_add_layer", ":", "\n", "                ", "lang_output", "=", "lang_output", ".", "detach", "(", ")", "\n", "visn_output", "=", "visn_output", ".", "detach", "(", ")", "\n", "\n", "", "lang_output_pooler", "=", "self", ".", "pooler", "(", "lang_output", ")", "\n", "visn_output_pooler", "=", "self", ".", "pooler", "(", "visn_output", ")", "\n", "\n", "sequence_output", "=", "lang_output", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "", "else", ":", "\n", "            ", "sequence_output", "=", "text_embeds", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "\n", "", "outputs", "=", "(", "sequence_output", ",", "pooled_output", ",", "lang_output", ",", "visn_output", ",", "lang_output_pooler", ",", "visn_output_pooler", ")", "# add hidden_states and attentions if they are here    ", "\n", "return", "outputs", "# sequence_output, pooled_output, (hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.VisualFeatEncoder.__init__": [[1162, 1176], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "feat_dim", "=", "VISUAL_CONFIG", ".", "visual_feat_dim", "\n", "pos_dim", "=", "VISUAL_CONFIG", ".", "visual_pos_dim", "\n", "\n", "# Object feature encoding", "\n", "self", ".", "visn_fc", "=", "nn", ".", "Linear", "(", "feat_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "visn_layer_norm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n", "# Box position encoding", "\n", "self", ".", "box_fc", "=", "nn", ".", "Linear", "(", "pos_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "box_layer_norm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.VisualFeatEncoder.forward": [[1177, 1188], ["vilmodel.VisualFeatEncoder.visn_fc", "vilmodel.VisualFeatEncoder.visn_layer_norm", "vilmodel.VisualFeatEncoder.box_fc", "vilmodel.VisualFeatEncoder.box_layer_norm", "vilmodel.VisualFeatEncoder.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "visn_input", ")", ":", "\n", "        ", "feats", ",", "boxes", "=", "visn_input", "\n", "\n", "x", "=", "self", ".", "visn_fc", "(", "feats", ")", "\n", "x", "=", "self", ".", "visn_layer_norm", "(", "x", ")", "\n", "y", "=", "self", ".", "box_fc", "(", "boxes", ")", "\n", "y", "=", "self", ".", "box_layer_norm", "(", "y", ")", "\n", "output", "=", "(", "x", "+", "y", ")", "/", "2", "\n", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.load_tf_weights_in_bert": [[57, 122], ["os.path.abspath", "logger.info", "tf.train.list_variables", "zip", "logger.info", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "logger.info", "torch.from_numpy", "logger.error", "logger.info", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr", "getattr", "logger.info"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "config", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "logger", ".", "info", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", ",", "\"global_step\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'squad'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'classifier'", ")", "\n", "", "else", ":", "\n", "                ", "try", ":", "\n", "                    ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "except", "AttributeError", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.gelu": [[124, 131], ["torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.swish": [[133, 135], ["torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.DicAddActionPreTrain.__init__": [[11, 26], ["pytorch_transformers.BertPreTrainedModel.__init__", "vilmodel.DicModel", "pretrain_class.NextActionPrediction", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "pytorch_transformers.modeling_bert.BertOnlyMLMHead", "pretrain_class.NextActionPrediction", "pretrain_class.NextOrderPrediction", "pretrain_class.NextActionPrediction", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "pretrain_class.DicAddActionPreTrain.init_weights", "pretrain_class.DicAddActionPreTrain.tie_weights"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.DicAddActionPreTrain.tie_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "DicAddActionPreTrain", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "bert", "=", "DicModel", "(", "config", ")", "\n", "\n", "self", ".", "action", "=", "NextActionPrediction", "(", "self", ".", "config", ".", "hidden_size", ",", "self", ".", "config", ".", "action_space", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "self", ".", "mlmhead", "=", "BertOnlyMLMHead", "(", "self", ".", "config", ")", "\n", "self", ".", "is_match", "=", "NextActionPrediction", "(", "self", ".", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "order", "=", "NextOrderPrediction", "(", "self", ".", "config", ".", "hidden_size", ",", "7", ")", "\n", "self", ".", "is_class3", "=", "NextActionPrediction", "(", "self", ".", "config", ".", "hidden_size", ",", "3", ")", "\n", "self", ".", "criterion_act", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ",", "size_average", "=", "False", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.DicAddActionPreTrain.tie_weights": [[28, 30], ["pretrain_class.DicAddActionPreTrain._tie_or_clone_weights"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel._tie_or_clone_weights"], ["", "def", "tie_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "_tie_or_clone_weights", "(", "self", ".", "mlmhead", ".", "predictions", ".", "decoder", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.DicAddActionPreTrain.forward": [[32, 65], ["pretrain_class.DicAddActionPreTrain.bert", "pretrain_class.DicAddActionPreTrain.mlmhead", "pretrain_class.DicAddActionPreTrain.criterion", "pretrain_class.DicAddActionPreTrain.view", "labels.view", "pretrain_class.DicAddActionPreTrain.is_match", "pretrain_class.DicAddActionPreTrain.criterion", "pretrain_class.DicAddActionPreTrain.order", "pretrain_class.DicAddActionPreTrain.criterion", "pretrain_class.DicAddActionPreTrain.is_class3", "pretrain_class.DicAddActionPreTrain.action", "pretrain_class.DicAddActionPreTrain.criterion", "pretrain_class.DicAddActionPreTrain.criterion"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "seq", ",", "labels", ",", "ismatch", "=", "None", ",", "f_t_all", "=", "None", ",", "lang_mask", "=", "None", ",", "img_mask", "=", "None", ",", "task", "=", "None", ")", ":", "\n", "\n", "        ", "ctx", ",", "pooled_out", ",", "attended_language", ",", "attended_visual", ",", "lang_pooler", ",", "visn_pooler", "=", "self", ".", "bert", "(", "seq", ",", "attention_mask", "=", "lang_mask", ",", "img_feats", "=", "f_t_all", ",", "img_mask", "=", "img_mask", ")", "\n", "if", "task", "==", "'mlm'", ":", "\n", "            ", "lang_part", "=", "ctx", "\n", "prediction_scores", "=", "self", ".", "mlmhead", "(", "lang_part", ")", "\n", "mask_loss", "=", "self", ".", "criterion", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", "=", "mask_loss", "\n", "return", "mask_loss", ",", "prediction_scores", "\n", "\n", "", "elif", "task", "==", "'itm'", ":", "\n", "            ", "cls_part", "=", "lang_pooler", "*", "visn_pooler", "\n", "match_scores", "=", "self", ".", "is_match", "(", "cls_part", ")", "\n", "match_loss", "=", "self", ".", "criterion", "(", "match_scores", ",", "ismatch", ")", "*", "5", "\n", "return", "match_loss", ",", "match_scores", "\n", "\n", "", "elif", "task", "==", "'order'", ":", "\n", "            ", "visn_part", "=", "attended_visual", "\n", "match_scores", "=", "self", ".", "order", "(", "visn_part", ")", "\n", "match_loss", "=", "self", ".", "criterion", "(", "match_scores", ",", "ismatch", ")", "*", "5", "\n", "return", "match_loss", ",", "match_scores", "\n", "\n", "", "elif", "task", "==", "'3class'", ":", "\n", "            ", "cls_part", "=", "ctx", "[", ":", ",", "0", ",", ":", "]", "\n", "match_scores", "=", "self", ".", "is_class3", "(", "cls_part", ")", "\n", "match_loss", "=", "self", ".", "criterion", "(", "match_scores", ",", "ismatch", ")", "*", "10", "\n", "return", "match_loss", ",", "match_scores", "\n", "\n", "", "else", ":", "\n", "            ", "cls_part", "=", "ctx", "[", ":", ",", "0", ",", ":", "]", "\n", "match_scores", "=", "self", ".", "action", "(", "cls_part", ")", "\n", "match_loss", "=", "self", ".", "criterion", "(", "match_scores", ",", "ismatch", ")", "\n", "return", "match_loss", ",", "match_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.MaskedLanguageModel.__init__": [[74, 82], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "hidden", ",", "vocab_size", ")", ":", "\n", "        ", "\"\"\"\n        :param hidden: output size of BERT model\n        :param vocab_size: total vocab size\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden", ",", "vocab_size", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.MaskedLanguageModel.forward": [[83, 85], ["pretrain_class.MaskedLanguageModel.softmax", "pretrain_class.MaskedLanguageModel.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "softmax", "(", "self", ".", "linear", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.NextImgPrediction.__init__": [[94, 101], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "\"\"\"\n        :param hidden: BERT model output size\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden", ",", "2", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.NextImgPrediction.forward": [[102, 104], ["pretrain_class.NextImgPrediction.softmax", "pretrain_class.NextImgPrediction.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "softmax", "(", "self", ".", "linear", "(", "x", ")", ")", "# the 0-35 is the vision, 36th is the CLS token", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.NextActionPrediction.__init__": [[111, 118], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "hidden", ",", "actionspace", ")", ":", "\n", "        ", "\"\"\"\n        :param hidden: BERT model output size\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden", ",", "actionspace", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.NextActionPrediction.forward": [[119, 121], ["pretrain_class.NextActionPrediction.softmax", "pretrain_class.NextActionPrediction.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "softmax", "(", "self", ".", "linear", "(", "x", ")", ")", "# the 0-35 is the vision, 36th is the CLS token", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.NextOrderPrediction.__init__": [[127, 134], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "hidden", ",", "actionspace", ")", ":", "\n", "        ", "\"\"\"\n        :param hidden: BERT model output size\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden", ",", "actionspace", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.NextOrderPrediction.forward": [[135, 137], ["pretrain_class.NextOrderPrediction.softmax", "pretrain_class.NextOrderPrediction.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "softmax", "(", "self", ".", "linear", "(", "x", ")", ")", "# the 0-35 is the vision, 36th is the CLS token", "", "", "", ""]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.TextDataset.__init__": [[47, 72], ["os.path.isfile", "os.path.split", "os.path.join", "os.path.exists", "logger.info", "logger.info", "tokenizer.convert_tokens_to_ids", "range", "logger.info", "open", "pickle.load", "open", "f.read", "tokenizer.tokenize", "main.TextDataset.examples.append", "open", "pickle.dump", "tokenizer.add_special_tokens_single_sequence", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "file_path", "=", "'train'", ",", "block_size", "=", "512", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "isfile", "(", "file_path", ")", "\n", "directory", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "'cached_lm_{}_{}'", ".", "format", "(", "block_size", ",", "filename", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loading features from cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "self", ".", "examples", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"Creating features from dataset file at %s\"", ",", "directory", ")", "\n", "self", ".", "examples", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "text", "=", "f", ".", "read", "(", ")", "\n", "", "tokenized_text", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tokenized_text", ")", "-", "block_size", "+", "1", ",", "block_size", ")", ":", "# Truncate in block of block_size", "\n", "                ", "self", ".", "examples", ".", "append", "(", "tokenizer", ".", "add_special_tokens_single_sequence", "(", "tokenized_text", "[", "i", ":", "i", "+", "block_size", "]", ")", ")", "\n", "\n", "# Note that we are loosing the last truncated example here for the sake of simplicity (no padding)", "\n", "# If your dataset is small, first you should loook for a bigger one :-) and second you", "\n", "# can change this behavior by adding (model specific) padding.", "\n", "\n", "", "logger", ".", "info", "(", "\"Saving features into cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "'wb'", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "examples", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "", "", "", "def", "__len__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.TextDataset.__len__": [[72, 74], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.TextDataset.__getitem__": [[74, 76], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "self", ".", "examples", "[", "item", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.load_and_cache_examples": [[77, 80], ["main.TextDataset"], "function", ["None"], ["", "", "def", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "evaluate", "=", "False", ")", ":", "\n", "    ", "dataset", "=", "TextDataset", "(", "tokenizer", ",", "file_path", "=", "args", ".", "eval_data_file", "if", "evaluate", "else", "args", ".", "train_data_file", ",", "block_size", "=", "args", ".", "block_size", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.set_seed": [[81, 87], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", "[", "'seed'", "]", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", "[", "'seed'", "]", ")", "\n", "torch", ".", "manual_seed", "(", "args", "[", "'seed'", "]", ")", "\n", "if", "args", "[", "'n_gpu'", "]", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", "[", "'seed'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.mask_tokens": [[88, 103], ["inputs.clone", "torch.bernoulli().bool", "torch.bernoulli().bool", "tokenizer.convert_tokens_to_ids", "torch.randint", "torch.randint", "torch.bernoulli().bool", "torch.bernoulli().bool", "len", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli().bool", "torch.bernoulli().bool", "torch.full", "torch.full", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full"], "function", ["None"], ["", "", "def", "mask_tokens", "(", "inputs", ",", "tokenizer", ",", "args", ")", ":", "\n", "    ", "\"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"", "\n", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "# We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)", "\n", "masked_indices", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "args", "[", "'mlm_probability'", "]", ")", ")", ".", "bool", "(", ")", "\n", "labels", "[", "~", "masked_indices", "]", "=", "-", "1", "# We only compute loss on masked tokens", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.8", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "\n", "inputs", "[", "indices_replaced", "]", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "mask_token", ")", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.5", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "tokenizer", ")", ",", "labels", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "# The rest of the time (10% of the time) we keep the masked input tokens unchanged", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.trainval": [[104, 337], ["torch.utils.data.DataLoader", "len", "pytorch_transformers.AdamW", "pytorch_transformers.WarmupLinearSchedule", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.trange", "main.set_seed", "max", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "amp.initialize", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "len", "int", "tqdm.tqdm", "main.evaluate", "enumerate", "random.choice", "batch[].long", "inputs_mlm.to.to", "labels_mlm.to.to", "img_mask.to.to", "img_feats.to.to", "lang_attention_mask.to.to", "batch[].long", "inputs.to.to", "labels.to.to", "img_feats_itm.to.to", "ismatch.to.to", "img_mask_itm.to.to", "batch[].long", "img_feats_order.to.to", "order.to.to", "batch[].long", "img_feats_class.to.to", "isclass.to.to", "batch[].long", "numpy.concatenate", "torch.from_numpy().float", "torch.from_numpy().float", "action.to.to", "img_his_36.to.to", "img_mask_his_36.to.to", "torch.nn.parallel.DistributedDataParallel.train", "loss.mean.item", "tqdm.trange.close", "len", "ImportError", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "len", "batch[].long", "batch[].long", "batch[].long", "batch[].long", "numpy.zeros", "torch.nn.parallel.DistributedDataParallel.", "prediction_score[].argmax", "loss.mean.mean", "loss.mean.backward", "pytorch_transformers.AdamW.step", "pytorch_transformers.WarmupLinearSchedule.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "print", "print", "print", "logger.info", "tqdm.tqdm.close", "len", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "torch.from_numpy", "torch.from_numpy", "torch.nn.parallel.DistributedDataParallel.", "prediction_score.argmax().eq().sum().item", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "os.path.join", "model_to_save.save_pretrained", "torch.save", "torch.save", "logger.info", "any", "len", "torch.nn.parallel.DistributedDataParallel.", "prediction_score.argmax().eq().sum().item", "amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters", "os.path.exists", "os.makedirs", "hasattr", "os.path.join", "round", "torch.nn.parallel.DistributedDataParallel.", "torch.nn.parallel.DistributedDataParallel.", "prediction_score.argmax().eq().sum", "ismatch.to.nelement", "prediction_score.argmax().eq().sum().item", "prediction_score.argmax().eq().sum().item", "len", "len", "prediction_score.argmax().eq().sum", "order.to.nelement", "prediction_score.argmax().eq", "prediction_score.argmax().eq().sum", "action.to.nelement", "prediction_score.argmax().eq().sum", "isclass.to.nelement", "ismatch.to.cuda", "prediction_score.argmax().eq", "prediction_score.argmax", "order.to.cuda", "prediction_score.argmax().eq", "prediction_score.argmax().eq", "len", "prediction_score.argmax", "action.to.cuda", "isclass.to.cuda", "prediction_score.argmax", "prediction_score.argmax"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.set_seed", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.evaluate", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.save_pretrained"], ["", "def", "trainval", "(", "args", ",", "train_dataset", ",", "eval_dataset", ",", "model", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\" Train the model \"\"\"", "\n", "args", "[", "'train_batch_size'", "]", "=", "args", "[", "'per_gpu_train_batch_size'", "]", "*", "max", "(", "1", ",", "args", "[", "'n_gpu'", "]", ")", "\n", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "if", "args", "[", "'local_rank'", "]", "==", "-", "1", "else", "DistributedSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", "[", "'train_batch_size'", "]", ",", "num_workers", "=", "8", ")", "\n", "if", "args", "[", "'max_steps'", "]", ">", "0", ":", "\n", "        ", "t_total", "=", "args", "[", "'max_steps'", "]", "\n", "args", "[", "'num_train_epochs'", "]", "=", "args", "[", "'max_steps'", "]", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", "[", "'gradient_accumulation_steps'", "]", ")", "+", "1", "\n", "", "else", ":", "#-1", "\n", "        ", "t_total", "=", "len", "(", "train_dataloader", ")", "//", "args", "[", "'gradient_accumulation_steps'", "]", "*", "args", "[", "'num_train_epochs'", "]", "\n", "", "data_length", "=", "len", "(", "train_dataloader", ")", "\n", "# Prepare optimizer and schedule (linear warmup and decay)", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "args", "[", "'weight_decay'", "]", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", "[", "'learning_rate'", "]", ",", "eps", "=", "args", "[", "'adam_epsilon'", "]", ")", "\n", "scheduler", "=", "WarmupLinearSchedule", "(", "optimizer", ",", "warmup_steps", "=", "args", "[", "'warmup_steps'", "]", ",", "t_total", "=", "t_total", ")", "\n", "if", "args", "[", "'fp16'", "]", ":", "#False", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", "[", "'fp16_opt_level'", "]", ")", "\n", "\n", "# multi-gpu training (should be after apex fp16 initialization)", "\n", "", "if", "args", "[", "'n_gpu'", "]", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "# Distributed training (should be after apex fp16 initialization)", "\n", "", "if", "args", "[", "'local_rank'", "]", "!=", "-", "1", ":", "#-1", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", "[", "'local_rank'", "]", "]", ",", "\n", "output_device", "=", "args", "[", "'local_rank'", "]", ",", "\n", "find_unused_parameters", "=", "True", ")", "\n", "# Train!", "\n", "", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "args", "[", "'num_train_epochs'", "]", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per GPU = %d\"", ",", "args", "[", "'per_gpu_train_batch_size'", "]", ")", "\n", "logger", ".", "info", "(", "\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"", ",", "\n", "args", "[", "'train_batch_size'", "]", "*", "args", "[", "'gradient_accumulation_steps'", "]", "*", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "if", "args", "[", "'local_rank'", "]", "!=", "-", "1", "else", "1", ")", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "args", "[", "'gradient_accumulation_steps'", "]", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "t_total", ")", "\n", "\n", "global_step", "=", "0", "\n", "tr_loss", ",", "logging_loss", "=", "0.0", ",", "0.0", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "train_iterator", "=", "trange", "(", "int", "(", "args", "[", "'num_train_epochs'", "]", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "True", ")", "\n", "set_seed", "(", "args", ")", "# Added here for reproducibility (even between python 2 and 3)", "\n", "total_acc_mlm", "=", "0.0", "\n", "avg_acc_mlm", "=", "0.0", "\n", "total_acc_itm", "=", "0.0", "\n", "avg_acc_itm", "=", "0.0", "\n", "total_acc_order", "=", "0.0", "\n", "avg_acc_order", "=", "0.0", "\n", "total_acc_class3", "=", "0.0", "\n", "avg_acc_class3", "=", "0.0", "\n", "total_acc_action", "=", "0.0", "\n", "avg_acc_action", "=", "0.0", "\n", "step_mlm", "=", "1", "\n", "step_itm", "=", "1", "\n", "step_order", "=", "1", "\n", "step_class3", "=", "1", "\n", "step_action", "=", "1", "\n", "for", "epo", "in", "train_iterator", ":", "\n", "        ", "epoch_iterator", "=", "tqdm", "(", "enumerate", "(", "train_dataloader", ")", ",", "\n", "desc", "=", "\"Iteration\"", ",", "\n", "total", "=", "len", "(", "train_dataloader", ")", ",", "\n", "bar_format", "=", "\"{l_bar}{r_bar}\"", ",", "\n", "disable", "=", "False", ")", "\n", "if", "epo", "<", "3", ":", "\n", "            ", "tasks", "=", "[", "'mlm'", ",", "'itm'", ",", "'order'", ",", "'3class'", ",", "'action'", "]", "\n", "", "else", ":", "\n", "            ", "tasks", "=", "[", "'mlm'", ",", "'itm'", ",", "'order'", ",", "'3class'", ",", "'action'", "]", "\n", "", "for", "step", ",", "batch", "in", "epoch_iterator", ":", "\n", "            ", "task", "=", "random", ".", "choice", "(", "tasks", ")", "\n", "#task = '3class'", "\n", "inputs_mlm", ",", "labels_mlm", "=", "batch", "[", "'masked_text_seq'", "]", ".", "long", "(", ")", ",", "batch", "[", "'masked_text_label'", "]", ".", "long", "(", ")", "\n", "lang_attention_mask", "=", "batch", "[", "'lang_attention_mask'", "]", ".", "long", "(", ")", "#bs 80", "\n", "\n", "img_feats", "=", "batch", "[", "'feature_single'", "]", "#bs 36 2176", "\n", "img_feats", "=", "img_feats", "\n", "#actions = batch['teacher']", "\n", "img_mask", "=", "batch", "[", "'img_mask'", "]", "\n", "img_mask", "=", "img_mask", "\n", "\n", "inputs_mlm", "=", "inputs_mlm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "labels_mlm", "=", "labels_mlm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_mask", "=", "img_mask", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_feats", "=", "img_feats", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "lang_attention_mask", "=", "lang_attention_mask", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#itm", "\n", "inputs", ",", "labels", "=", "batch", "[", "'text_seq'", "]", ".", "long", "(", ")", ",", "batch", "[", "'text_label'", "]", ".", "long", "(", ")", "\n", "img_feats_itm", "=", "batch", "[", "'feature_single_itm'", "]", "\n", "img_feats_itm", "=", "img_feats_itm", "\n", "ismatch", "=", "batch", "[", "'ismatch'", "]", ".", "long", "(", ")", "\n", "img_mask_itm", "=", "batch", "[", "'img_mask_itm'", "]", "\n", "img_mask_itm", "=", "img_mask_itm", "\n", "\n", "inputs", "=", "inputs", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "labels", "=", "labels", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_feats_itm", "=", "img_feats_itm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "ismatch", "=", "ismatch", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_mask_itm", "=", "img_mask_itm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#order", "\n", "img_feats_order", "=", "batch", "[", "'feature_single_order'", "]", "\n", "img_feats_order", "=", "img_feats_order", "\n", "order", "=", "batch", "[", "'order'", "]", ".", "long", "(", ")", "\n", "\n", "img_feats_order", "=", "img_feats_order", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "order", "=", "order", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#3class", "\n", "img_feats_class", "=", "batch", "[", "'feature_single_class'", "]", "\n", "img_feats_class", "=", "img_feats_class", "\n", "isclass", "=", "batch", "[", "'isclass'", "]", ".", "long", "(", ")", "\n", "\n", "img_feats_class", "=", "img_feats_class", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "isclass", "=", "isclass", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#action", "\n", "action", "=", "batch", "[", "'teacher_embedding'", "]", ".", "long", "(", ")", "\n", "img_history", "=", "batch", "[", "'feature_his'", "]", "\n", "img_36", "=", "batch", "[", "'feature_36'", "]", "\n", "img_sep", "=", "np", ".", "zeros", "(", "(", "len", "(", "img_36", ")", ",", "1", ",", "2176", ")", ")", "+", "102", "\n", "img_his_36", "=", "np", ".", "concatenate", "(", "(", "img_history", ",", "img_sep", ",", "img_36", ")", ",", "axis", "=", "1", ")", "\n", "img_his_36", "=", "torch", ".", "from_numpy", "(", "img_his_36", ")", ".", "float", "(", ")", "\n", "img_mask_his_36", "=", "batch", "[", "'img_mask_his_36'", "]", "\n", "\n", "action", "=", "action", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_his_36", "=", "img_his_36", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_mask_his_36", "=", "img_mask_his_36", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "if", "task", "==", "'mlm'", ":", "\n", "                ", "outputs", "=", "model", "(", "inputs_mlm", ",", "labels_mlm", ",", "None", ",", "img_feats", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask", ",", "task", "=", "task", ")", "\n", "", "elif", "task", "==", "'itm'", ":", "\n", "                ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "ismatch", ",", "img_feats_itm", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask_itm", ",", "task", "=", "task", ")", "\n", "", "elif", "task", "==", "'order'", ":", "\n", "                ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "order", ",", "img_feats_order", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask", ",", "task", "=", "task", ")", "\n", "", "elif", "task", "==", "'3class'", ":", "\n", "                ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "isclass", ",", "img_feats_class", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask", ",", "task", "=", "task", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "action", ",", "img_his_36", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask_his_36", ",", "task", "=", "task", ")", "\n", "\n", "", "loss", "=", "outputs", "[", "0", "]", "# model outputs are always tuple in transformers (see doc)", "\n", "prediction_score", "=", "outputs", "[", "1", "]", "\n", "\n", "if", "task", "==", "'mlm'", ":", "\n", "                ", "bool_label", "=", "labels_mlm", ">", "0", "\n", "pred", "=", "prediction_score", "[", "bool_label", ",", ":", "]", ".", "argmax", "(", "1", ")", "\n", "valid_labels", "=", "labels_mlm", "[", "bool_label", "]", "\n", "acc_mlm", "=", "(", "pred", "==", "valid_labels", ")", ".", "type", "(", "torch", ".", "float", ")", ".", "mean", "(", ")", "*", "100.", "\n", "total_acc_mlm", "+=", "acc_mlm", "\n", "step_mlm", "=", "step_mlm", "+", "1", "\n", "avg_acc_mlm", "=", "total_acc_mlm", "/", "step_mlm", "\n", "", "elif", "task", "==", "'itm'", ":", "\n", "                ", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "ismatch", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_itm", "=", "correct", "/", "ismatch", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_itm", "+=", "acc_itm", "\n", "step_itm", "=", "step_itm", "+", "1", "\n", "avg_acc_itm", "=", "total_acc_itm", "/", "(", "step_itm", "+", "1", ")", "\n", "", "elif", "task", "==", "'order'", ":", "\n", "                ", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "order", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_order", "=", "correct", "/", "order", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_order", "+=", "acc_order", "\n", "step_order", "=", "step_order", "+", "1", "\n", "avg_acc_order", "=", "total_acc_order", "/", "step_order", "\n", "", "elif", "task", "==", "'action'", ":", "\n", "                ", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "action", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_action", "=", "correct", "/", "action", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_action", "+=", "acc_action", "\n", "step_action", "=", "step_action", "+", "1", "\n", "avg_acc_action", "=", "total_acc_action", "/", "step_action", "\n", "", "else", ":", "\n", "                ", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "isclass", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_class3", "=", "correct", "/", "isclass", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_class3", "+=", "acc_class3", "\n", "step_class3", "=", "step_class3", "+", "1", "\n", "avg_acc_class3", "=", "total_acc_class3", "/", "step_class3", "\n", "\n", "", "if", "args", "[", "'n_gpu'", "]", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu parallel training", "\n", "", "if", "args", "[", "'gradient_accumulation_steps'", "]", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", "[", "'gradient_accumulation_steps'", "]", "\n", "", "if", "args", "[", "'fp16'", "]", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", "[", "'gradient_accumulation_steps'", "]", "==", "0", ":", "\n", "                ", "if", "args", "[", "'fp16'", "]", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "args", "[", "'max_grad_norm'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", "[", "'max_grad_norm'", "]", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", "[", "'local_rank'", "]", "in", "[", "-", "1", ",", "0", "]", "and", "args", "[", "'save_steps'", "]", ">", "0", "and", "global_step", "%", "data_length", "==", "0", ":", "\n", "# Save model checkpoint", "\n", "                    ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", "[", "'output_dir'", "]", ",", "'checkpoint-{}'", ".", "format", "(", "epo", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "                        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "output_dir", ")", "\n", "torch", ".", "save", "(", "args", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'training_args.bin'", ")", ")", "\n", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "output_dir", ")", "\n", "\n", "", "", "if", "step", "%", "200", "==", "0", ":", "\n", "                ", "print", "(", "\"\\n\"", ")", "\n", "print", "(", "\"PROGRESS: {}%\"", ".", "format", "(", "round", "(", "(", "epo", "*", "len", "(", "train_dataloader", ")", "+", "step", ")", "*", "100", "/", "t_total", ",", "4", ")", ")", ")", "\n", "print", "(", "\"EVALERR: {:.4f}%,avg_acc_mlm:{:.2f},avg_acc_itm:{:.2f},avg_acc_order:{:.2f},avg_acc_class:{:.2f}, avg_acc_action:{:.2f}\"", ".", "format", "(", "tr_loss", "/", "(", "global_step", ")", ",", "avg_acc_mlm", ",", "avg_acc_itm", ",", "avg_acc_order", ",", "avg_acc_class3", ",", "avg_acc_action", ")", ")", "\n", "logger", ".", "info", "(", "\"Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, avg_acc_mlm:%.2f, avg_acc_itm:%.2f, avg_acc_order:%.2f, avg_acc_class:%.2f, avg_acc_class:%.2f \"", "%", "(", "epo", "+", "1", ",", "len", "(", "train_iterator", ")", ",", "step", "+", "1", ",", "len", "(", "train_dataloader", ")", ",", "tr_loss", "/", "(", "global_step", ")", ",", "avg_acc_mlm", ",", "avg_acc_itm", ",", "avg_acc_order", ",", "avg_acc_class3", ",", "avg_acc_action", ")", ")", "\n", "\n", "", "if", "args", "[", "'max_steps'", "]", ">", "0", "and", "global_step", ">", "args", "[", "'max_steps'", "]", ":", "#-1", "\n", "                ", "epoch_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "", "", "if", "args", "[", "'max_steps'", "]", ">", "0", "and", "global_step", ">", "args", "[", "'max_steps'", "]", ":", "\n", "            ", "train_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "", "results", "=", "evaluate", "(", "args", ",", "eval_dataset", ",", "model", ",", "tokenizer", ")", "\n", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.evaluate": [[339, 519], ["torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "model.eval", "tqdm.tqdm", "print", "torch.exp", "torch.exp", "logger.info", "print", "os.makedirs", "max", "torch.utils.data.SequentialSampler", "torch.utils.data.distributed.DistributedSampler", "len", "batch[].long", "inputs_mlm.to.to", "labels_mlm.to.to", "img_mask.to.to", "img_feats.to.to", "lang_attention_mask.to.to", "batch[].long", "inputs.to.to", "labels.to.to", "img_feats_itm.to.to", "ismatch.to.to", "img_mask_itm.to.to", "batch[].long", "img_feats_order.to.to", "order.to.to", "batch[].long", "img_feats_class.to.to", "isclass.to.to", "batch[].long", "numpy.concatenate", "torch.from_numpy().float", "torch.from_numpy().float", "action.to.to", "img_his_36.to.to", "img_mask_his_36.to.to", "torch.tensor", "torch.tensor", "os.path.exists", "batch[].long", "batch[].long", "batch[].long", "batch[].long", "numpy.zeros", "torch.no_grad", "torch.no_grad", "torch.from_numpy", "torch.from_numpy", "len", "model", "lm_loss_mlm.mean().item", "prediction_score[].argmax", "model", "lm_loss_itm.mean().item", "prediction_score.argmax().eq().sum().item", "lm_loss_mlm.mean", "model", "lm_loss_order.mean().item", "prediction_score.argmax().eq().sum().item", "lm_loss_itm.mean", "prediction_score.argmax().eq().sum", "ismatch.to.nelement", "model", "lm_loss_action.mean().item", "prediction_score.argmax().eq().sum().item", "model", "lm_loss_class.mean().item", "prediction_score.argmax().eq().sum().item", "lm_loss_order.mean", "prediction_score.argmax().eq().sum", "order.to.nelement", "prediction_score.argmax().eq", "lm_loss_action.mean", "prediction_score.argmax().eq().sum", "action.to.nelement", "lm_loss_class.mean", "prediction_score.argmax().eq().sum", "isclass.to.nelement", "ismatch.to.cuda", "prediction_score.argmax().eq", "prediction_score.argmax", "order.to.cuda", "prediction_score.argmax().eq", "prediction_score.argmax().eq", "prediction_score.argmax", "action.to.cuda", "isclass.to.cuda", "prediction_score.argmax", "prediction_score.argmax"], "function", ["None"], ["", "def", "evaluate", "(", "args", ",", "eval_dataset", ",", "model", ",", "tokenizer", ",", "prefix", "=", "\"\"", ")", ":", "\n", "# Loop to handle MNLI double evaluation (matched, mis-matched)", "\n", "    ", "eval_output_dir", "=", "args", "[", "'output_dir'", "]", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "eval_output_dir", ")", "and", "args", "[", "'local_rank'", "]", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "os", ".", "makedirs", "(", "eval_output_dir", ")", "\n", "", "args", "[", "'eval_batch_size'", "]", "=", "args", "[", "'per_gpu_eval_batch_size'", "]", "*", "max", "(", "1", ",", "args", "[", "'n_gpu'", "]", ")", "\n", "# Note that DistributedSampler samples randomly", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "if", "args", "[", "'local_rank'", "]", "==", "-", "1", "else", "DistributedSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", "[", "'eval_batch_size'", "]", ",", "num_workers", "=", "8", ")", "\n", "\n", "# Eval!", "\n", "logger", ".", "info", "(", "\"***** Running evaluation {} *****\"", ".", "format", "(", "prefix", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", "[", "'eval_batch_size'", "]", ")", "\n", "eval_loss", "=", "0.0", "\n", "eval_loss_mlm", "=", "0.0", "\n", "eval_loss_itm", "=", "0.0", "\n", "eval_loss_order", "=", "0.0", "\n", "eval_loss_class", "=", "0.0", "\n", "eval_loss_action", "=", "0.0", "\n", "nb_eval_steps", "=", "1", "\n", "nb_eval_steps_mlm", "=", "1", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "total_acc_mlm", "=", "0.0", "\n", "avg_acc_mlm", "=", "0.0", "\n", "total_acc_itm", "=", "0.0", "\n", "avg_acc_itm", "=", "0.0", "\n", "total_acc_order", "=", "0.0", "\n", "avg_acc_order", "=", "0.0", "\n", "total_acc_class", "=", "0.0", "\n", "avg_acc_class", "=", "0.0", "\n", "total_acc_action", "=", "0.0", "\n", "avg_acc_action", "=", "0.0", "\n", "step_mlm", "=", "1", "\n", "step_itm", "=", "1", "\n", "step_order", "=", "1", "\n", "step_class3", "=", "1", "\n", "step_action", "=", "1", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "        ", "inputs_mlm", ",", "labels_mlm", "=", "batch", "[", "'masked_text_seq'", "]", ".", "long", "(", ")", ",", "batch", "[", "'masked_text_label'", "]", ".", "long", "(", ")", "\n", "lang_attention_mask", "=", "batch", "[", "'lang_attention_mask'", "]", ".", "long", "(", ")", "\n", "\n", "img_feats", "=", "batch", "[", "'feature_single'", "]", "\n", "img_feats", "=", "img_feats", "\n", "img_mask", "=", "batch", "[", "'img_mask'", "]", "\n", "img_mask", "=", "img_mask", "\n", "\n", "inputs_mlm", "=", "inputs_mlm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "labels_mlm", "=", "labels_mlm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_mask", "=", "img_mask", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_feats", "=", "img_feats", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "lang_attention_mask", "=", "lang_attention_mask", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#itm", "\n", "inputs", ",", "labels", "=", "batch", "[", "'text_seq'", "]", ".", "long", "(", ")", ",", "batch", "[", "'text_label'", "]", ".", "long", "(", ")", "\n", "img_feats_itm", "=", "batch", "[", "'feature_single_itm'", "]", "\n", "img_feats_itm", "=", "img_feats_itm", "\n", "ismatch", "=", "batch", "[", "'ismatch'", "]", ".", "long", "(", ")", "\n", "img_mask_itm", "=", "batch", "[", "'img_mask_itm'", "]", "\n", "img_mask_itm", "=", "img_mask_itm", "\n", "\n", "inputs", "=", "inputs", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "labels", "=", "labels", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_feats_itm", "=", "img_feats_itm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "ismatch", "=", "ismatch", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_mask_itm", "=", "img_mask_itm", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#order", "\n", "img_feats_order", "=", "batch", "[", "'feature_single_order'", "]", "\n", "img_feats_order", "=", "img_feats_order", "\n", "order", "=", "batch", "[", "'order'", "]", ".", "long", "(", ")", "\n", "\n", "img_feats_order", "=", "img_feats_order", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "order", "=", "order", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#3class", "\n", "img_feats_class", "=", "batch", "[", "'feature_single_class'", "]", "\n", "img_feats_class", "=", "img_feats_class", "\n", "isclass", "=", "batch", "[", "'isclass'", "]", ".", "long", "(", ")", "\n", "\n", "img_feats_class", "=", "img_feats_class", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "isclass", "=", "isclass", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "#action", "\n", "action", "=", "batch", "[", "'teacher_embedding'", "]", ".", "long", "(", ")", "\n", "img_history", "=", "batch", "[", "'feature_his'", "]", "\n", "img_36", "=", "batch", "[", "'feature_36'", "]", "\n", "img_sep", "=", "np", ".", "zeros", "(", "(", "len", "(", "img_36", ")", ",", "1", ",", "2176", ")", ")", "+", "102", "\n", "img_his_36", "=", "np", ".", "concatenate", "(", "(", "img_history", ",", "img_sep", ",", "img_36", ")", ",", "axis", "=", "1", ")", "\n", "img_his_36", "=", "torch", ".", "from_numpy", "(", "img_his_36", ")", ".", "float", "(", ")", "\n", "img_mask_his_36", "=", "batch", "[", "'img_mask_his_36'", "]", "\n", "\n", "action", "=", "action", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_his_36", "=", "img_his_36", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "img_mask_his_36", "=", "img_mask_his_36", ".", "to", "(", "args", "[", "'device'", "]", ")", "\n", "\n", "tasklist", "=", "[", "'mlm'", ",", "'itm'", ",", "'order'", ",", "'3class'", ",", "'action'", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "task", "in", "tasklist", ":", "\n", "                ", "if", "task", "==", "'mlm'", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs_mlm", ",", "labels_mlm", ",", "None", ",", "img_feats", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask", ",", "task", "=", "task", ")", "\n", "lm_loss_mlm", "=", "outputs", "[", "0", "]", "\n", "eval_loss_mlm", "+=", "lm_loss_mlm", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "prediction_score", "=", "outputs", "[", "1", "]", "\n", "bool_label", "=", "labels_mlm", ">", "0", "\n", "pred", "=", "prediction_score", "[", "bool_label", ",", ":", "]", ".", "argmax", "(", "1", ")", "\n", "valid_labels", "=", "labels_mlm", "[", "bool_label", "]", "\n", "acc_mlm", "=", "(", "pred", "==", "valid_labels", ")", ".", "type", "(", "torch", ".", "float", ")", ".", "mean", "(", ")", "*", "100.", "\n", "total_acc_mlm", "+=", "acc_mlm", "\n", "avg_acc_mlm", "=", "total_acc_mlm", "/", "nb_eval_steps", "\n", "", "elif", "task", "==", "'itm'", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "ismatch", ",", "img_feats_itm", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask_itm", ",", "task", "=", "task", ")", "\n", "lm_loss_itm", "=", "outputs", "[", "0", "]", "\n", "eval_loss_itm", "+=", "lm_loss_itm", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "prediction_score", "=", "outputs", "[", "1", "]", "\n", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "ismatch", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_itm", "=", "correct", "/", "ismatch", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_itm", "+=", "acc_itm", "\n", "avg_acc_itm", "=", "total_acc_itm", "/", "nb_eval_steps", "\n", "", "elif", "task", "==", "'order'", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "order", ",", "img_feats_order", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask", ",", "task", "=", "task", ")", "\n", "lm_loss_order", "=", "outputs", "[", "0", "]", "\n", "eval_loss_order", "+=", "lm_loss_order", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "prediction_score", "=", "outputs", "[", "1", "]", "\n", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "order", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_order", "=", "correct", "/", "order", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_order", "+=", "acc_order", "\n", "avg_acc_order", "=", "total_acc_order", "/", "nb_eval_steps", "\n", "", "elif", "task", "==", "'action'", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "action", ",", "img_his_36", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask_his_36", ",", "task", "=", "task", ")", "\n", "lm_loss_action", "=", "outputs", "[", "0", "]", "\n", "eval_loss_action", "+=", "lm_loss_action", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "prediction_score", "=", "outputs", "[", "1", "]", "\n", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "action", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_action", "=", "correct", "/", "action", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_action", "+=", "acc_action", "\n", "avg_acc_action", "=", "total_acc_action", "/", "nb_eval_steps", "\n", "", "else", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs", ",", "labels", ",", "isclass", ",", "img_feats_class", ",", "lang_mask", "=", "lang_attention_mask", ",", "img_mask", "=", "img_mask", ",", "task", "=", "task", ")", "\n", "lm_loss_class", "=", "outputs", "[", "0", "]", "\n", "eval_loss_class", "+=", "lm_loss_class", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "prediction_score", "=", "outputs", "[", "1", "]", "\n", "correct", "=", "prediction_score", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "isclass", ".", "cuda", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "acc_class", "=", "correct", "/", "isclass", ".", "nelement", "(", ")", "*", "100", "\n", "total_acc_class", "+=", "acc_class", "\n", "avg_acc_class", "=", "total_acc_class", "/", "nb_eval_steps", "\n", "\n", "", "", "", "nb_eval_steps", "+=", "1", "\n", "\n", "\n", "", "print", "(", "\"nb_eval_steps:\"", ",", "nb_eval_steps", ")", "\n", "\n", "eval_loss_mlm", "=", "eval_loss_mlm", "/", "nb_eval_steps", "\n", "eval_loss_itm", "=", "eval_loss_itm", "/", "nb_eval_steps", "\n", "eval_loss_order", "=", "eval_loss_order", "/", "nb_eval_steps", "\n", "eval_loss_class", "=", "eval_loss_class", "/", "nb_eval_steps", "\n", "eval_loss_action", "=", "eval_loss_action", "/", "nb_eval_steps", "\n", "\n", "perplexity", "=", "torch", ".", "exp", "(", "torch", ".", "tensor", "(", "eval_loss", ")", ")", "\n", "\n", "result", "=", "{", "\n", "\"loss_mlm\"", ":", "eval_loss_mlm", ",", "\n", "\"loss_itm\"", ":", "eval_loss_itm", ",", "\n", "\"loss_order\"", ":", "eval_loss_order", ",", "\n", "\"loss_class\"", ":", "eval_loss_class", ",", "\n", "\"loss_action\"", ":", "eval_loss_action", ",", "\n", "\"mlm_acc\"", ":", "avg_acc_mlm", ",", "\n", "\"itm_acc\"", ":", "avg_acc_itm", ",", "\n", "\"order_acc\"", ":", "avg_acc_order", ",", "\n", "\"class_acc\"", ":", "avg_acc_class", ",", "\n", "\"action_acc\"", ":", "avg_acc_action", "\n", "}", "\n", "\n", "logger", ".", "info", "(", "\"loss_mlm: %.4f, loss_itm: %.4f, loss_order: %.4f, loss_class: %.4f, acc_mlm:%.2f, acc_itm:%.2f, acc_order:%.2f, acc_class:%.2f, acc_action:%.2f \"", "%", "(", "eval_loss_mlm", ",", "eval_loss_itm", ",", "eval_loss_order", ",", "eval_loss_class", ",", "avg_acc_mlm", ",", "avg_acc_itm", ",", "avg_acc_order", ",", "avg_acc_class", ",", "avg_acc_action", ")", ")", "\n", "\n", "print", "(", "result", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.main": [[522, 761], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "vars", "logging.basicConfig", "logger.warning", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "min", "DicAddActionPreTrain.from_pretrained.to", "logger.info", "print", "os.path.join", "os.path.join", "os.path.join", "print", "print", "print", "ValueError", "ValueError", "os.path.exists", "os.listdir", "ValueError", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "print", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "bool", "torch.distributed.barrier", "torch.distributed.barrier", "pretrain_class.DicAddActionPreTrain.from_pretrained", "print", "pretrain_class.DicAddActionPreTrain", "torch.distributed.barrier", "torch.distributed.barrier", "glob.glob", "batch_loader.NavDataset", "print", "train", "logger.info", "glob.glob", "batch_loader.NavDataset", "print", "glob.glob", "batch_loader.NavDataset", "print", "main.trainval", "logger.info", "logger.info", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "torch.save", "torch.save", "pretrain_class.DicAddActionPreTrain.from_pretrained", "tokenizer_class.from_pretrained", "DicAddActionPreTrain.from_pretrained.to", "glob.glob", "batch_loader.NavDataset", "print", "main.evaluate", "os.getenv", "os.getenv", "os.getenv", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.barrier", "os.makedirs", "hasattr", "os.path.join", "torch.distributed.barrier", "torch.distributed.barrier", "len", "len", "len", "torch.distributed.get_rank", "torch.distributed.get_rank", "os.path.exists", "len", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.trainval", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.save_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.save_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.main.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "#parser = argparse.ArgumentParser()", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Process some integers.'", ")", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--train_data_file\"", ",", "default", "=", "'data/train/'", ",", "type", "=", "str", ",", "help", "=", "\"The input training data file (a text file).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_data_file\"", ",", "default", "=", "'data/collect_traj_test/'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"An optional input evaluation data file to evaluate the perplexity on (a text file).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "'result/'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "\"bert\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The model architecture to be fine-tuned.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "\"bert-base-uncased\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The model checkpoint for weights initialization.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Train with masked-language modeling loss instead of language modeling.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_probability\"", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "\n", "help", "=", "\"Ratio of tokens to mask for masked language modeling loss\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Optional pretrained config name or path if not the same as model_name_or_path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--block_size\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Optional input sequence length after tokenization.\"", "\n", "\"The training dataset will be truncated in block of this size for training.\"", "\n", "\"Default to the model max input length for single sentence inputs (take into account special tokens).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_trainval\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval when training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--evaluate_during_training\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Run evaluation during training at each logging step.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "48", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Batch size per GPU/CPU for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "4", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Weight deay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "default", "=", "80.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_steps\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"If > 0: set total number of training steps to perform. Override num_train_epochs.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--logging_steps'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Log every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--save_steps'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Save checkpoint every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_all_checkpoints\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "'--overwrite_output_dir'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Overwrite the content of the output directory\"", ")", "\n", "parser", ".", "add_argument", "(", "'--overwrite_cache'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Overwrite the cached training and evaluation sets\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16_opt_level'", ",", "type", "=", "str", ",", "default", "=", "'O1'", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "#-1", "\n", "help", "=", "\"For distributed training: local_rank\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"For distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"For distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vision_size\"", ",", "type", "=", "int", ",", "default", "=", "2176", ",", "help", "=", "\"imgaction size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--action_space\"", ",", "type", "=", "int", ",", "default", "=", "36", ",", "help", "=", "\"action space\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vl_layers\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "\"how many fusion layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--la_layers\"", ",", "type", "=", "int", ",", "default", "=", "9", ",", "help", "=", "\"how many lang layers\"", ")", "\n", "parser", ".", "add_argument", "(", "'--update'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'update lang Bert'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_add_layer'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'update add layer'", ")", "\n", "parser", ".", "add_argument", "(", "'--include_next'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'do action classification'", ")", "\n", "parser", ".", "add_argument", "(", "'--result_dir'", ",", "type", "=", "str", ",", "default", "=", "'tasks/R2R/results/'", ",", "help", "=", "'path to the result_dir file'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot_dir'", ",", "type", "=", "str", ",", "default", "=", "'tasks/R2R/plots/'", ",", "help", "=", "'path to the plot_dir file'", ")", "\n", "parser", ".", "add_argument", "(", "'--snapshot_dir'", ",", "type", "=", "str", ",", "default", "=", "'tasks/R2R/snapshots/'", ",", "help", "=", "'path to the snapshot_dir file'", ")", "\n", "parser", ".", "add_argument", "(", "'--philly'", ",", "action", "=", "'store_true'", ",", "help", "=", "'program runs on Philly, used to redirect `write_model_path`'", ")", "\n", "parser", ".", "add_argument", "(", "\"--resume_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The model checkpoint for weights initialization.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--run_name'", ",", "type", "=", "str", ",", "help", "=", "\"name for wandb run\"", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--prevalent_only\"", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "params", "=", "vars", "(", "args", ")", "\n", "\n", "if", "params", "[", "'philly'", "]", ":", "# use philly False", "\n", "        ", "print", "(", "'Info: Use Philly, all the output folders are reset.'", ")", "\n", "RESULT_DIR", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ")", ",", "params", "[", "'result_dir'", "]", ")", "\n", "PLOT_DIR", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ")", ",", "params", "[", "'plot_dir'", "]", ")", "\n", "SNAPSHOT_DIR", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ")", ",", "params", "[", "'snapshot_dir'", "]", ")", "\n", "print", "(", "'RESULT_DIR'", ",", "RESULT_DIR", ")", "\n", "print", "(", "'PLOT_DIR'", ",", "PLOT_DIR", ")", "\n", "print", "(", "'SNAPSHOT_DIR'", ",", "SNAPSHOT_DIR", ")", "\n", "\n", "", "if", "params", "[", "'model_type'", "]", "in", "[", "\"bert\"", ",", "\"roberta\"", ",", "\"distilbert\"", "]", "and", "not", "params", "[", "'mlm'", "]", ":", "#False", "\n", "        ", "raise", "ValueError", "(", "\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"", "\n", "\"flag (masked language modeling).\"", ")", "\n", "", "if", "params", "[", "'eval_data_file'", "]", "is", "None", "and", "params", "[", "'do_eval'", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"", "\n", "\"or remove the --do_eval argument.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "params", "[", "'output_dir'", "]", ")", "and", "os", ".", "listdir", "(", "params", "[", "'output_dir'", "]", ")", "and", "params", "[", "'do_train'", "]", "and", "not", "params", "[", "'overwrite_output_dir'", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", ".", "format", "(", "params", "[", "'output_dir'", "]", ")", ")", "\n", "\n", "# Setup distant debugging if needed", "\n", "", "if", "params", "[", "'server_ip'", "]", "and", "params", "[", "'server_port'", "]", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "params", "[", "'server_ip'", "]", ",", "params", "[", "'server_port'", "]", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "# Setup CUDA, GPU & distributed training", "\n", "", "if", "params", "[", "'local_rank'", "]", "==", "-", "1", "or", "params", "[", "'no_cuda'", "]", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "params", "[", "'no_cuda'", "]", "else", "\"cpu\"", ")", "\n", "params", "[", "'n_gpu'", "]", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "print", "(", "\"You are using %d GPUs to train!!\"", "%", "(", "params", "[", "'n_gpu'", "]", ")", ")", "\n", "", "else", ":", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "#set_trace()", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "params", "[", "'local_rank'", "]", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "params", "[", "'local_rank'", "]", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "\"nccl\"", ",", "init_method", "=", "init_method", ",", "\n", "world_size", "=", "world_size", ",", "rank", "=", "rank", ")", "\n", "params", "[", "'n_gpu'", "]", "=", "1", "\n", "", "params", "[", "'device'", "]", "=", "device", "\n", "\n", "# Setup logging", "\n", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s - %(levelname)s - %(name)s -   %(message)s'", ",", "\n", "datefmt", "=", "'%m/%d/%Y %H:%M:%S'", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "params", "[", "'local_rank'", "]", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ")", "\n", "logger", ".", "warning", "(", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "params", "[", "'local_rank'", "]", ",", "device", ",", "params", "[", "'n_gpu'", "]", ",", "bool", "(", "params", "[", "'local_rank'", "]", "!=", "-", "1", ")", ",", "params", "[", "'fp16'", "]", ")", "\n", "\n", "if", "params", "[", "'local_rank'", "]", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Barrier to make sure only the first process in distributed training download model & vocab", "\n", "", "config_class", ",", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "params", "[", "'model_type'", "]", "]", "\n", "config", "=", "config_class", ".", "from_pretrained", "(", "params", "[", "'config_name'", "]", "if", "params", "[", "'config_name'", "]", "else", "params", "[", "'model_name_or_path'", "]", ")", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "params", "[", "'tokenizer_name'", "]", "if", "params", "[", "'tokenizer_name'", "]", "else", "params", "[", "'model_name_or_path'", "]", ",", "do_lower_case", "=", "params", "[", "'do_lower_case'", "]", ")", "\n", "if", "params", "[", "'block_size'", "]", "<=", "0", ":", "#-1", "\n", "        ", "params", "[", "'block_size'", "]", "=", "tokenizer", ".", "max_len_single_sentence", "# Our input block size will be the max possible for the model", "\n", "", "params", "[", "'block_size'", "]", "=", "min", "(", "params", "[", "'block_size'", "]", ",", "tokenizer", ".", "max_len_single_sentence", ")", "\n", "config", ".", "img_feature_dim", "=", "params", "[", "'vision_size'", "]", "\n", "config", ".", "img_feature_type", "=", "\"\"", "\n", "config", ".", "update_lang_bert", "=", "params", "[", "'update'", "]", "\n", "config", ".", "update_add_layer", "=", "params", "[", "'update_add_layer'", "]", "\n", "config", ".", "vl_layers", "=", "params", "[", "'vl_layers'", "]", "\n", "config", ".", "la_layers", "=", "params", "[", "'la_layers'", "]", "\n", "config", ".", "action_space", "=", "params", "[", "'action_space'", "]", "\n", "\n", "if", "params", "[", "'resume_path'", "]", "is", "not", "None", ":", "\n", "        ", "model", "=", "DicAddActionPreTrain", ".", "from_pretrained", "(", "params", "[", "'resume_path'", "]", ")", "\n", "print", "(", "\"you have loaded model from %s\"", "%", "(", "params", "[", "'resume_path'", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DicAddActionPreTrain", "(", "config", ")", "\n", "", "model", ".", "to", "(", "params", "[", "'device'", "]", ")", "\n", "\n", "if", "params", "[", "'local_rank'", "]", "==", "0", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# End of barrier to make sure only the first process in distributed training download model & vocab", "\n", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "params", ")", "\n", "\n", "# Training", "\n", "if", "params", "[", "'do_train'", "]", ":", "\n", "        ", "if", "params", "[", "'local_rank'", "]", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "jfiles", "=", "glob", ".", "glob", "(", "params", "[", "'train_data_file'", "]", "+", "\"/*.json\"", ")", "\n", "train_dataset", "=", "NavDataset", "(", "jfiles", ",", "tokenizer", ",", "feature_store", ",", "panoramic", ",", "params", ",", "feature_store_bnb", ")", "\n", "print", "(", "\"you have loaded %d  time steps\"", "%", "(", "len", "(", "train_dataset", ")", ")", ")", "\n", "\n", "if", "params", "[", "'local_rank'", "]", "==", "0", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "global_step", ",", "tr_loss", "=", "train", "(", "params", ",", "train_dataset", ",", "model", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\" global_step = %s, average loss = %s\"", ",", "global_step", ",", "tr_loss", ")", "\n", "\n", "", "if", "params", "[", "'do_trainval'", "]", ":", "\n", "        ", "if", "params", "[", "'local_rank'", "]", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache", "\n", "", "jfiles", "=", "glob", ".", "glob", "(", "params", "[", "'train_data_file'", "]", "+", "\"/*.json\"", ")", "\n", "if", "params", "[", "'prevalent_only'", "]", ":", "\n", "            ", "jfiles_bnb", "=", "None", "\n", "", "else", ":", "\n", "            ", "jfiles_bnb", "=", "'data/bnb/traj_train.json'", "\n", "", "train_dataset", "=", "NavDataset", "(", "jfiles", ",", "jfiles_bnb", ",", "tokenizer", ",", "feature_store", ",", "panoramic", ",", "params", ",", "feature_store_bnb", ")", "\n", "print", "(", "\"you have loaded %d  time steps\"", "%", "(", "len", "(", "train_dataset", ")", ")", ")", "\n", "jfiles_eval", "=", "glob", ".", "glob", "(", "params", "[", "'eval_data_file'", "]", "+", "\"/*.json\"", ")", "\n", "eval_dataset", "=", "NavDataset", "(", "jfiles_eval", ",", "None", ",", "tokenizer", ",", "feature_store", ",", "panoramic", ",", "params", ",", "feature_store_bnb", ")", "\n", "print", "(", "\"you have loaded %d  time steps\"", "%", "(", "len", "(", "eval_dataset", ")", ")", ")", "\n", "\n", "if", "params", "[", "'local_rank'", "]", "==", "0", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "global_step", ",", "tr_loss", "=", "trainval", "(", "params", ",", "train_dataset", ",", "eval_dataset", ",", "model", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\" global_step = %s, average loss = %s\"", ",", "global_step", ",", "tr_loss", ")", "\n", "\n", "\n", "# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()", "\n", "", "if", "params", "[", "'do_train'", "]", "and", "(", "params", "[", "'local_rank'", "]", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "# Create output directory if needed", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "params", "[", "'output_dir'", "]", ")", "and", "params", "[", "'local_rank'", "]", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "os", ".", "makedirs", "(", "params", "[", "'output_dir'", "]", ")", "\n", "", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "params", "[", "'output_dir'", "]", ")", "\n", "# Save a trained model, configuration and tokenizer using `save_pretrained()`.", "\n", "# They can then be reloaded using `from_pretrained()`", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "params", "[", "'output_dir'", "]", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "params", "[", "'output_dir'", "]", ")", "\n", "# Good practice: save your training arguments together with the trained model", "\n", "torch", ".", "save", "(", "params", ",", "os", ".", "path", ".", "join", "(", "params", "[", "'output_dir'", "]", ",", "'training_args.bin'", ")", ")", "\n", "# Load a trained model and vocabulary that you have fine-tuned", "\n", "model", "=", "DicAddActionPreTrain", ".", "from_pretrained", "(", "params", "[", "'output_dir'", "]", ")", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "params", "[", "'output_dir'", "]", ",", "do_lower_case", "=", "params", "[", "'do_lower_case'", "]", ")", "\n", "model", ".", "to", "(", "params", "[", "'device'", "]", ")", "\n", "\n", "\n", "", "if", "params", "[", "'do_eval'", "]", ":", "\n", "        ", "jfiles", "=", "glob", ".", "glob", "(", "params", "[", "'eval_data_file'", "]", "+", "\"/*.json\"", ")", "\n", "eval_dataset", "=", "NavDataset", "(", "jfiles", ",", "tokenizer", ",", "feature_store", ",", "panoramic", ",", "params", ",", "feature_store_bnb", ")", "\n", "print", "(", "\"you have loaded %d  time steps\"", "%", "(", "len", "(", "eval_dataset", ")", ")", ")", "\n", "\n", "if", "params", "[", "'local_rank'", "]", "==", "0", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "results", "=", "evaluate", "(", "params", ",", "eval_dataset", ",", "model", ",", "tokenizer", ")", "\n", "\n", "", "results", "=", "{", "}", "\n", "return", "results", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__init__": [[11, 17], ["feature.Feature._load"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb._load"], ["    ", "def", "__init__", "(", "self", ",", "feature_store", ",", "panoramic", ",", "max_load", "=", "-", "1", ")", ":", "\n", "        ", "self", ".", "feature_store", "=", "feature_store", "\n", "self", ".", "image_h", ",", "self", ".", "image_w", ",", "self", ".", "vfov", ",", "self", ".", "features", "=", "None", ",", "None", ",", "None", ",", "None", "\n", "self", ".", "panoramic", "=", "panoramic", "\n", "self", ".", "max_load", "=", "max_load", "\n", "self", ".", "_load", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature._load": [[18, 51], ["print", "feature.Feature.__loadResNet", "str", "feature.Feature.__loadBottomUp", "feature.Feature.__loadResNet", "feature.Feature.__loadBottomUp", "features_bottom.keys", "numpy.hstack", "feature.Feature.__loadResNet", "feature.Feature.__loadBottomUp", "features_bottom.keys", "functools.partial", "print", "feature.Feature.feature_store.split", "feature.Feature.feature_store.split", "numpy.hstack", "feature.Feature.feature_store.split", "feature.Feature.feature_store.split", "feature.Feature.feature_store.split"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadResNet", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadBottomUp", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadResNet", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadBottomUp", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadResNet", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadBottomUp"], ["", "def", "_load", "(", "self", ")", ":", "\n", "        ", "print", "(", "'Loading image features from %s'", "%", "str", "(", "self", ".", "feature_store", ")", ")", "\n", "if", "self", ".", "feature_store", "==", "'img_features/ResNet-152-imagenet.tsv'", ":", "\n", "            ", "self", ".", "features", ",", "self", ".", "image_h", ",", "self", ".", "image_w", ",", "self", ".", "vfov", "=", "self", ".", "__loadResNet", "(", "self", ".", "feature_store", ")", "\n", "self", ".", "rollout", "=", "self", ".", "rollout_single", "\n", "", "elif", "self", ".", "feature_store", "==", "'img_features/bottom_up'", ":", "\n", "            ", "self", ".", "features", ",", "self", ".", "image_h", ",", "self", ".", "image_w", ",", "self", ".", "vfov", "=", "self", ".", "__loadBottomUp", "(", "self", ".", "feature_store", ")", "\n", "self", ".", "rollout", "=", "self", ".", "rollout_single", "\n", "", "elif", "self", ".", "feature_store", "==", "'img_features/ResNet-152-imagenet.tsv+img_features/bottom_up'", ":", "\n", "            ", "features_resnet", ",", "self", ".", "image_h", ",", "self", ".", "image_w", ",", "self", ".", "vfov", "=", "self", ".", "__loadResNet", "(", "self", ".", "feature_store", ".", "split", "(", "'+'", ")", "[", "0", "]", ")", "\n", "features_bottom", ",", "_", ",", "_", ",", "_", "=", "self", ".", "__loadBottomUp", "(", "self", ".", "feature_store", ".", "split", "(", "'+'", ")", "[", "1", "]", ")", "\n", "self", ".", "features", "=", "features_bottom", "\n", "for", "key", "in", "features_bottom", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "features", "[", "key", "]", "=", "np", ".", "hstack", "(", "[", "features_resnet", "[", "key", "]", ",", "\n", "features_bottom", "[", "key", "]", "]", ")", "\n", "", "self", ".", "rollout", "=", "self", ".", "rollout_single", "\n", "", "elif", "self", ".", "feature_store", "==", "'img_features/ResNet-152-imagenet.tsv+img_features/bottom_up+bbox'", ":", "\n", "            ", "features_resnet", ",", "self", ".", "image_h", ",", "self", ".", "image_w", ",", "self", ".", "vfov", "=", "self", ".", "__loadResNet", "(", "self", ".", "feature_store", ".", "split", "(", "'+'", ")", "[", "0", "]", ")", "\n", "features_bottom", ",", "_", ",", "_", ",", "_", "=", "self", ".", "__loadBottomUp", "(", "self", ".", "feature_store", ".", "split", "(", "'+'", ")", "[", "1", "]", ")", "\n", "self", ".", "features", "=", "features_bottom", "\n", "for", "key", "in", "features_bottom", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "features", "[", "key", "]", "=", "np", ".", "hstack", "(", "[", "features_resnet", "[", "key", "]", ",", "\n", "features_bottom", "[", "key", "]", "]", ")", "\n", "", "self", ".", "rollout", "=", "functools", ".", "partial", "(", "self", ".", "rollout_with_bbox", ",", "self", ".", "feature_store", ".", "split", "(", "'+'", ")", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Image features not provided'", ")", "\n", "self", ".", "rollout", "=", "(", "lambda", "a", ",", "b", ",", "c", ":", "None", ")", "if", "not", "self", ".", "features", "else", "self", ".", "rollout_single", "\n", "self", ".", "features", ",", "self", ".", "image_h", ",", "self", ".", "image_w", ",", "self", ".", "vfov", "=", "None", ",", "480", ",", "640", ",", "60", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadResNet": [[52, 74], ["print", "open", "csv.DictReader", "int", "int", "int", "numpy.frombuffer().reshape", "numpy.frombuffer", "len", "base64.b64decode"], "methods", ["None"], ["", "", "def", "__loadResNet", "(", "self", ",", "feature_store", ")", ":", "\n", "        ", "tsv_fieldnames", "=", "[", "'scanId'", ",", "'viewpointId'", ",", "'image_w'", ",", "'image_h'", ",", "'vfov'", ",", "'features'", "]", "\n", "features", ",", "image_h", ",", "image_w", ",", "vfov", "=", "{", "}", ",", "480", ",", "640", ",", "60", "\n", "read_num", "=", "0", "\n", "while", "(", "read_num", "<", "20", ")", ":", "\n", "            ", "print", "(", "'read_num %d'", "%", "(", "read_num", ")", ")", "\n", "try", ":", "\n", "                ", "with", "open", "(", "feature_store", ",", "\"r+\"", ")", "as", "tsv_in_file", ":", "\n", "                    ", "reader", "=", "csv", ".", "DictReader", "(", "tsv_in_file", ",", "delimiter", "=", "'\\t'", ",", "fieldnames", "=", "tsv_fieldnames", ")", "\n", "for", "item", "in", "reader", ":", "\n", "                        ", "image_h", "=", "int", "(", "item", "[", "'image_h'", "]", ")", "\n", "image_w", "=", "int", "(", "item", "[", "'image_w'", "]", ")", "\n", "vfov", "=", "int", "(", "item", "[", "'vfov'", "]", ")", "\n", "long_id", "=", "item", "[", "'scanId'", "]", "+", "'_'", "+", "item", "[", "'viewpointId'", "]", "\n", "features", "[", "long_id", "]", "=", "np", ".", "frombuffer", "(", "base64", ".", "b64decode", "(", "item", "[", "'features'", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "(", "36", ",", "2048", ")", ")", "\n", "if", "self", ".", "max_load", ">", "0", "and", "len", "(", "features", ")", ">=", "self", ".", "max_load", ":", "\n", "                            ", "break", "\n", "", "", "", "break", "\n", "", "except", "OSError", ":", "\n", "                ", "read_num", "+=", "1", "\n", "\n", "", "", "return", "features", ",", "image_h", ",", "image_w", ",", "vfov", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.__loadBottomUp": [[75, 105], ["os.listdir", "os.path.join", "os.path.join", "h5py.File", "int", "int", "len", "os.path.join", "os.listdir", "os.listdir", "os.path.join", "h5py.File", "numpy.zeros", "range", "len", "numpy.mean", "len", "viewpoint.keys", "str"], "methods", ["None"], ["", "def", "__loadBottomUp", "(", "self", ",", "feature_store", ")", ":", "\n", "# tsv_fieldnames = ['scanId', 'viewpointId', 'image_w', 'image_h',", "\n", "#                   'num_boxes', 'features', 'cls_prob', 'captions']", "\n", "        ", "scanIds", "=", "os", ".", "listdir", "(", "feature_store", ")", "\n", "temp_folder", "=", "os", ".", "path", ".", "join", "(", "feature_store", ",", "scanIds", "[", "0", "]", ")", "\n", "temp_fname", "=", "os", ".", "path", ".", "join", "(", "temp_folder", ",", "os", ".", "listdir", "(", "temp_folder", ")", "[", "0", "]", ")", "\n", "with", "h5py", ".", "File", "(", "temp_fname", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "image_h", "=", "int", "(", "f", "[", "'0'", "]", "[", "'image_h'", "]", ".", "value", ")", "\n", "image_w", "=", "int", "(", "f", "[", "'0'", "]", "[", "'image_w'", "]", ".", "value", ")", "\n", "view_size", "=", "len", "(", "f", ")", "# 36", "\n", "feature_size", "=", "(", "f", "[", "'0'", "]", "[", "'features'", "]", ".", "value", ")", ".", "shape", "[", "1", "]", "# 2048", "\n", "", "vfov", "=", "60", "\n", "features", "=", "{", "}", "\n", "for", "scanId", "in", "scanIds", ":", "\n", "            ", "folder", "=", "os", ".", "path", ".", "join", "(", "feature_store", ",", "scanId", ")", "\n", "viewpointIds_h5", "=", "os", ".", "listdir", "(", "folder", ")", "\n", "for", "viewpointId_h5", "in", "viewpointIds_h5", ":", "\n", "                ", "fname", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "viewpointId_h5", ")", "\n", "with", "h5py", ".", "File", "(", "fname", ",", "\"r\"", ")", "as", "viewpoint", ":", "\n", "                    ", "assert", "len", "(", "viewpoint", ".", "keys", "(", ")", ")", "==", "36", "\n", "long_id", "=", "scanId", "+", "'_'", "+", "viewpointId_h5", "[", ":", "-", "3", "]", "# rstrip('.h5')", "\n", "temp", "=", "np", ".", "zeros", "(", "(", "view_size", ",", "feature_size", ")", ")", "\n", "for", "image_id", "in", "range", "(", "36", ")", ":", "\n", "                        ", "item", "=", "viewpoint", "[", "str", "(", "image_id", ")", "]", "\n", "temp", "[", "image_id", ",", ":", "]", "=", "np", ".", "mean", "(", "item", "[", "'features'", "]", ".", "value", ",", "0", ")", "\n", "", "features", "[", "long_id", "]", "=", "temp", "\n", "\n", "", "if", "self", ".", "max_load", ">", "0", "and", "len", "(", "features", ")", ">=", "self", ".", "max_load", ":", "\n", "                    ", "break", "\n", "", "", "", "return", "features", ",", "image_h", ",", "image_w", ",", "vfov", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.rollout_single": [[106, 111], ["None"], "methods", ["None"], ["", "def", "rollout_single", "(", "self", ",", "scanId", ",", "viewpointId", ",", "viewIndex", ")", ":", "\n", "        ", "long_id", "=", "scanId", "+", "'_'", "+", "viewpointId", "\n", "feature", "=", "self", ".", "features", "[", "long_id", "]", "\n", "feature", "=", "(", "feature", "[", "viewIndex", ",", ":", "]", ",", "feature", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature.rollout_with_bbox": [[112, 121], ["functools.lru_cache", "os.path.join", "h5py.File", "len", "viewpoint.keys", "str"], "methods", ["None"], ["", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "20000", ")", "\n", "def", "rollout_with_bbox", "(", "self", ",", "feature_store", ",", "scanId", ",", "viewpointId", ",", "viewIndex", ")", ":", "\n", "        ", "long_id", "=", "scanId", "+", "'_'", "+", "viewpointId", "\n", "fname", "=", "os", ".", "path", ".", "join", "(", "feature_store", ",", "long_id", "+", "'.h5'", ")", "\n", "with", "h5py", ".", "File", "(", "fname", ",", "\"r\"", ")", "as", "viewpoint", ":", "\n", "            ", "assert", "len", "(", "viewpoint", ".", "keys", "(", ")", ")", "==", "36", "\n", "item", "=", "viewpoint", "[", "str", "(", "viewIndex", ")", "]", "\n", "features", "=", "item", "[", "'features'", "]", ".", "value", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb.__init__": [[123, 128], ["feature.Feature_bnb._load"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb._load"], ["    ", "def", "__init__", "(", "self", ",", "feature_store", ",", "max_load", "=", "-", "1", ")", ":", "\n", "        ", "self", ".", "feature_store", "=", "feature_store", "\n", "self", ".", "features", "=", "None", "\n", "self", ".", "max_load", "=", "max_load", "\n", "self", ".", "_load", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb._load": [[129, 134], ["print", "feature.Feature_bnb.__loadResNet_bnb", "str"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb.__loadResNet_bnb"], ["", "def", "_load", "(", "self", ")", ":", "\n", "#set_trace()", "\n", "        ", "print", "(", "'Loading image features from %s'", "%", "str", "(", "self", ".", "feature_store", ")", ")", "\n", "self", ".", "features", "=", "self", ".", "__loadResNet_bnb", "(", "self", ".", "feature_store", ")", "\n", "self", ".", "rollout", "=", "self", ".", "rollout_single_bnb", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb.__loadResNet_bnb": [[135, 148], ["numpy.load"], "methods", ["None"], ["", "def", "__loadResNet_bnb", "(", "self", ",", "feature_store", ")", ":", "\n", "        ", "features", "=", "{", "}", "\n", "\n", "bnb", "=", "np", ".", "load", "(", "feature_store", ",", "allow_pickle", "=", "True", ")", "\n", "bnbimg", "=", "bnb", "[", "'data'", "]", "\n", "read_num", "=", "0", "\n", "try", ":", "\n", "            ", "for", "item", "in", "bnbimg", ":", "\n", "                ", "picid", "=", "item", "[", "'picid'", "]", "\n", "features", "[", "picid", "]", "=", "item", "[", "'features'", "]", "\n", "", "", "except", "OSError", ":", "\n", "            ", "read_num", "+=", "1", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.feature.Feature_bnb.rollout_single_bnb": [[149, 154], ["None"], "methods", ["None"], ["", "def", "rollout_single_bnb", "(", "self", ",", "picid", ")", ":", "\n", "\n", "        ", "feature", "=", "self", ".", "features", "[", "picid", "]", "\n", "\n", "return", "feature", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.__init__": [[82, 88], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "finetuning_task", "=", "kwargs", ".", "pop", "(", "'finetuning_task'", ",", "None", ")", "\n", "self", ".", "num_labels", "=", "kwargs", ".", "pop", "(", "'num_labels'", ",", "2", ")", "\n", "self", ".", "output_attentions", "=", "kwargs", ".", "pop", "(", "'output_attentions'", ",", "False", ")", "\n", "self", ".", "output_hidden_states", "=", "kwargs", ".", "pop", "(", "'output_hidden_states'", ",", "False", ")", "\n", "self", ".", "torchscript", "=", "kwargs", ".", "pop", "(", "'torchscript'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.save_pretrained": [[89, 99], ["os.path.isdir", "os.path.join", "modeling_utils.PretrainedConfig.to_json_file"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_json_file"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save a configuration object to the directory `save_directory`, so that it\n            can be re-loaded using the :func:`~pytorch_transformers.PretrainedConfig.from_pretrained` class method.\n        \"\"\"", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ",", "\"Saving path should be a directory where the model and configuration can be saved\"", "\n", "\n", "# If we save using the predefined names, we can load using `from_pretrained`", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "CONFIG_NAME", ")", "\n", "\n", "self", ".", "to_json_file", "(", "output_config_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.from_pretrained": [[100, 186], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "cls.from_json_file", "kwargs.items", "logger.info", "os.path.isdir", "pytorch_pretrained_bert.file_utils.cached_path", "logger.info", "logger.info", "hasattr", "kwargs.pop", "os.path.join", "setattr", "to_remove.append", "logger.error", "logger.error", "cls.pretrained_config_archive_map.keys"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.from_json_file"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\" Instantiate a :class:`~pytorch_transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n        Parameters:\n            pretrained_model_name_or_path: either:\n                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing a configuration file saved using the :func:`~pytorch_transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n            return_unused_kwargs: (`optional`) bool:\n                - If False, then this function returns just the final configuration object.\n                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n        Examples::\n            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n            # derived class: BertConfig\n            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n            config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n            assert config.output_attention == True\n            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n                                                               foo=False, return_unused_kwargs=True)\n            assert config.output_attention == True\n            assert unused_kwargs == {'foo': False}\n        \"\"\"", "\n", "cache_dir", "=", "kwargs", ".", "pop", "(", "'cache_dir'", ",", "None", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "'force_download'", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "'proxies'", ",", "None", ")", "\n", "return_unused_kwargs", "=", "kwargs", ".", "pop", "(", "'return_unused_kwargs'", ",", "False", ")", "\n", "\n", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_config_archive_map", ":", "\n", "            ", "config_file", "=", "cls", ".", "pretrained_config_archive_map", "[", "pretrained_model_name_or_path", "]", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CONFIG_NAME", ")", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_config_file", "=", "cached_path", "(", "config_file", ",", "cache_dir", "=", "cache_dir", ",", "force_download", "=", "force_download", ",", "proxies", "=", "proxies", ")", "\n", "", "except", "EnvironmentError", "as", "e", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_config_archive_map", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Couldn't reach server at '{}' to download pretrained model configuration file.\"", ".", "format", "(", "\n", "config_file", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "cls", ".", "pretrained_config_archive_map", ".", "keys", "(", ")", ")", ",", "\n", "config_file", ")", ")", "\n", "", "raise", "e", "\n", "", "if", "resolved_config_file", "==", "config_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading configuration file {}\"", ".", "format", "(", "config_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading configuration file {} from cache at {}\"", ".", "format", "(", "\n", "config_file", ",", "resolved_config_file", ")", ")", "\n", "\n", "# Load config", "\n", "", "config", "=", "cls", ".", "from_json_file", "(", "resolved_config_file", ")", "\n", "\n", "# Update config with kwargs if needed", "\n", "to_remove", "=", "[", "]", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "key", ")", ":", "\n", "                ", "setattr", "(", "config", ",", "key", ",", "value", ")", "\n", "to_remove", ".", "append", "(", "key", ")", "\n", "", "", "for", "key", "in", "to_remove", ":", "\n", "            ", "kwargs", ".", "pop", "(", "key", ",", "None", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config %s\"", ",", "config", ")", "\n", "if", "return_unused_kwargs", ":", "\n", "            ", "return", "config", ",", "kwargs", "\n", "", "else", ":", "\n", "            ", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.from_dict": [[187, 194], ["cls", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `Config` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "cls", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.from_json_file": [[195, 201], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.__eq__": [[202, 204], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.__repr__": [[205, 207], ["str", "modeling_utils.PretrainedConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_dict": [[208, 212], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_json_string": [[213, 216], ["json.dumps", "modeling_utils.PretrainedConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_json_file": [[217, 221], ["io.open", "writer.write", "modeling_utils.PretrainedConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PretrainedConfig.to_json_string"], ["", "def", "to_json_file", "(", "self", ",", "json_file_path", ")", ":", "\n", "        ", "\"\"\" Save this instance to a json file.\"\"\"", "\n", "with", "open", "(", "json_file_path", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.__init__": [[241, 252], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "PretrainedConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"", "\n", "\"To create a model from a pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "# Save config in model", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel._get_resized_embeddings": [[253, 285], ["old_embeddings.weight.size", "torch.nn.Embedding", "torch.nn.Embedding.to", "modeling_utils.PreTrainedModel.init_weights", "min"], "methods", ["None"], ["", "def", "_get_resized_embeddings", "(", "self", ",", "old_embeddings", ",", "new_num_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\" Build a resized Embedding Module from a provided token Embedding Module.\n            Increasing the size will add newly initialized vectors at the end\n            Reducing the size will remove vectors from the end\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: return the provided token Embedding Module.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n        \"\"\"", "\n", "if", "new_num_tokens", "is", "None", ":", "\n", "            ", "return", "old_embeddings", "\n", "\n", "", "old_num_tokens", ",", "old_embedding_dim", "=", "old_embeddings", ".", "weight", ".", "size", "(", ")", "\n", "if", "old_num_tokens", "==", "new_num_tokens", ":", "\n", "            ", "return", "old_embeddings", "\n", "\n", "# Build new embeddings", "\n", "", "new_embeddings", "=", "nn", ".", "Embedding", "(", "new_num_tokens", ",", "old_embedding_dim", ")", "\n", "new_embeddings", ".", "to", "(", "old_embeddings", ".", "weight", ".", "device", ")", "\n", "\n", "# initialize all new embeddings (in particular added tokens)", "\n", "self", ".", "init_weights", "(", "new_embeddings", ")", "\n", "\n", "# Copy word embeddings from the previous weights", "\n", "num_tokens_to_copy", "=", "min", "(", "old_num_tokens", ",", "new_num_tokens", ")", "\n", "new_embeddings", ".", "weight", ".", "data", "[", ":", "num_tokens_to_copy", ",", ":", "]", "=", "old_embeddings", ".", "weight", ".", "data", "[", ":", "num_tokens_to_copy", ",", ":", "]", "\n", "\n", "return", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel._tie_or_clone_weights": [[286, 293], ["torch.nn.Parameter", "second_module.weight.clone"], "methods", ["None"], ["", "def", "_tie_or_clone_weights", "(", "self", ",", "first_module", ",", "second_module", ")", ":", "\n", "        ", "\"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n        \"\"\"", "\n", "if", "self", ".", "config", ".", "torchscript", ":", "\n", "            ", "first_module", ".", "weight", "=", "nn", ".", "Parameter", "(", "second_module", ".", "weight", ".", "clone", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "first_module", ".", "weight", "=", "second_module", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.resize_token_embeddings": [[294, 318], ["getattr", "getattr._resize_token_embeddings", "hasattr", "modeling_utils.PreTrainedModel.tie_weights"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.DicModel._resize_token_embeddings", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.pretrain_class.DicAddActionPreTrain.tie_weights"], ["", "", "def", "resize_token_embeddings", "(", "self", ",", "new_num_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n        Arguments:\n            new_num_tokens: (`optional`) int:\n                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.\n                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the input tokens Embeddings Module of the model\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "# get the base model if needed", "\n", "model_embeds", "=", "base_model", ".", "_resize_token_embeddings", "(", "new_num_tokens", ")", "\n", "if", "new_num_tokens", "is", "None", ":", "\n", "            ", "return", "model_embeds", "\n", "\n", "# Update base model and current model config", "\n", "", "self", ".", "config", ".", "vocab_size", "=", "new_num_tokens", "\n", "base_model", ".", "vocab_size", "=", "new_num_tokens", "\n", "\n", "# Tie weights again if needed", "\n", "if", "hasattr", "(", "self", ",", "'tie_weights'", ")", ":", "\n", "            ", "self", ".", "tie_weights", "(", ")", "\n", "\n", "", "return", "model_embeds", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.prune_heads": [[319, 326], ["getattr", "getattr._prune_heads"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.vilmodel.DicModel._prune_heads"], ["", "def", "prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the base model.\n            Arguments:\n                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "# get the base model if needed", "\n", "base_model", ".", "_prune_heads", "(", "heads_to_prune", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.save_pretrained": [[327, 343], ["os.path.isdir", "model_to_save.config.save_pretrained", "os.path.join", "torch.save", "hasattr", "model_to_save.state_dict"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.save_pretrained"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save a model and its configuration file to a directory, so that it\n            can be re-loaded using the `:func:`~pytorch_transformers.PreTrainedModel.from_pretrained`` class method.\n        \"\"\"", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ",", "\"Saving path should be a directory where the model and configuration can be saved\"", "\n", "\n", "# Only save the model it-self if we are using distributed training", "\n", "model_to_save", "=", "self", ".", "module", "if", "hasattr", "(", "self", ",", "'module'", ")", "else", "self", "\n", "\n", "# Save configuration file", "\n", "model_to_save", ".", "config", ".", "save_pretrained", "(", "save_directory", ")", "\n", "\n", "# If we save using the predefined names, we can load using `from_pretrained`", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "WEIGHTS_NAME", ")", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained": [[344, 520], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling_utils.PreTrainedModel.from_pretrained.load"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "*", "model_args", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with ``model.train()``\n        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n        It is up to you to train those weights with a downstream fine-tuning task.\n        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n        Parameters:\n            pretrained_model_name_or_path: either:\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n            config: (`optional`) instance of a class derived from :class:`~pytorch_transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and :func:`~pytorch_transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~pytorch_transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n        Examples::\n            model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n            model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n            model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n        \"\"\"", "\n", "config", "=", "kwargs", ".", "pop", "(", "'config'", ",", "None", ")", "\n", "state_dict", "=", "kwargs", ".", "pop", "(", "'state_dict'", ",", "None", ")", "\n", "cache_dir", "=", "kwargs", ".", "pop", "(", "'cache_dir'", ",", "None", ")", "\n", "from_tf", "=", "kwargs", ".", "pop", "(", "'from_tf'", ",", "False", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "'force_download'", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "'proxies'", ",", "None", ")", "\n", "output_loading_info", "=", "kwargs", ".", "pop", "(", "'output_loading_info'", ",", "False", ")", "\n", "\n", "# Load config", "\n", "if", "config", "is", "None", ":", "\n", "            ", "config", ",", "model_kwargs", "=", "cls", ".", "config_class", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", ",", "*", "model_args", ",", "\n", "cache_dir", "=", "cache_dir", ",", "return_unused_kwargs", "=", "True", ",", "\n", "force_download", "=", "force_download", ",", "\n", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "model_kwargs", "=", "kwargs", "\n", "\n", "# Load model", "\n", "", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_model_archive_map", ":", "\n", "            ", "archive_file", "=", "cls", ".", "pretrained_model_archive_map", "[", "pretrained_model_name_or_path", "]", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "            ", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "                ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", ")", "\n", "", "else", ":", "\n", "                ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "                ", "archive_file", "=", "pretrained_model_name_or_path", "+", "\".index\"", "\n", "", "else", ":", "\n", "                ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ",", "force_download", "=", "force_download", ",", "proxies", "=", "proxies", ")", "\n", "", "except", "EnvironmentError", "as", "e", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_model_archive_map", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Couldn't reach server at '{}' to download pretrained weights.\"", ".", "format", "(", "\n", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "cls", ".", "pretrained_model_archive_map", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "", "raise", "e", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "\n", "# Instantiate model.", "\n", "", "model", "=", "cls", "(", "config", ",", "*", "model_args", ",", "**", "model_kwargs", ")", "\n", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "state_dict", "=", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "'cpu'", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "return", "cls", ".", "load_tf_weights", "(", "model", ",", "config", ",", "resolved_archive_file", "[", ":", "-", "6", "]", ")", "# Remove the '.index'", "\n", "\n", "# Convert old format to new format if needed from a PyTorch state_dict", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# Load from a PyTorch state_dict", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "\n", "# Make sure we are able to load base models as well as derived models (with heads)", "\n", "", "", "", "start_prefix", "=", "''", "\n", "model_to_load", "=", "model", "\n", "if", "not", "hasattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "and", "any", "(", "s", ".", "startswith", "(", "cls", ".", "base_model_prefix", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "cls", ".", "base_model_prefix", "+", "'.'", "\n", "", "if", "hasattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "and", "not", "any", "(", "s", ".", "startswith", "(", "cls", ".", "base_model_prefix", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "model_to_load", "=", "getattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "\n", "\n", "", "load", "(", "model_to_load", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "\n", "", "if", "hasattr", "(", "model", ",", "'tie_weights'", ")", ":", "\n", "            ", "model", ".", "tie_weights", "(", ")", "# make sure word embedding weights are still tied", "\n", "\n", "# Set model in evaluation mode to desactivate DropOut modules by default", "\n", "", "model", ".", "eval", "(", ")", "\n", "\n", "if", "output_loading_info", ":", "\n", "            ", "loading_info", "=", "{", "\"missing_keys\"", ":", "missing_keys", ",", "\"unexpected_keys\"", ":", "unexpected_keys", ",", "\"error_msgs\"", ":", "error_msgs", "}", "\n", "return", "model", ",", "loading_info", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.Conv1D.__init__": [[523, 533], ["torch.nn.Module.__init__", "torch.empty", "torch.nn.init.normal_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf", ",", "nx", ")", ":", "\n", "        ", "\"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n            Basically works like a Linear layer but the weights are transposed\n        \"\"\"", "\n", "super", "(", "Conv1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nf", "=", "nf", "\n", "w", "=", "torch", ".", "empty", "(", "nx", ",", "nf", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "w", ",", "std", "=", "0.02", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "w", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "nf", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.Conv1D.forward": [[534, 539], ["torch.addmm", "x.view.view.view", "x.view.view.view", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size_out", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "nf", ",", ")", "\n", "x", "=", "torch", ".", "addmm", "(", "self", ".", "bias", ",", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", ",", "self", ".", "weight", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "size_out", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PoolerStartLogits.__init__": [[543, 546], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "PoolerStartLogits", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PoolerStartLogits.forward": [[547, 559], ["modeling_utils.PoolerStartLogits.dense().squeeze", "modeling_utils.PoolerStartLogits.dense"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Args:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"", "\n", "x", "=", "self", ".", "dense", "(", "hidden_states", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "p_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "1e30", "*", "p_mask", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PoolerEndLogits.__init__": [[564, 570], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.LayerNorm", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "PoolerEndLogits", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense_0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense_1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PoolerEndLogits.forward": [[571, 599], ["modeling_utils.PoolerEndLogits.dense_0", "modeling_utils.PoolerEndLogits.activation", "modeling_utils.PoolerEndLogits.LayerNorm", "modeling_utils.PoolerEndLogits.dense_1().squeeze", "start_positions[].expand", "hidden_states.gather", "start_states.expand.expand.expand", "torch.cat", "modeling_utils.PoolerEndLogits.dense_1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_states", "=", "None", ",", "start_positions", "=", "None", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"", "\n", "assert", "start_states", "is", "not", "None", "or", "start_positions", "is", "not", "None", ",", "\"One of start_states, start_positions should be not None\"", "\n", "if", "start_positions", "is", "not", "None", ":", "\n", "            ", "slen", ",", "hsz", "=", "hidden_states", ".", "shape", "[", "-", "2", ":", "]", "\n", "start_positions", "=", "start_positions", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "start_positions", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "start_states", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ")", "# shape (bsz, slen, hsz)", "\n", "\n", "", "x", "=", "self", ".", "dense_0", "(", "torch", ".", "cat", "(", "[", "hidden_states", ",", "start_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "x", "=", "self", ".", "LayerNorm", "(", "x", ")", "\n", "x", "=", "self", ".", "dense_1", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "p_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "1e30", "*", "p_mask", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PoolerAnswerClass.__init__": [[603, 608], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "PoolerAnswerClass", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense_0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "dense_1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PoolerAnswerClass.forward": [[609, 641], ["modeling_utils.PoolerAnswerClass.dense_0", "modeling_utils.PoolerAnswerClass.activation", "modeling_utils.PoolerAnswerClass.dense_1().squeeze", "start_positions[].expand", "hidden_states.gather().squeeze", "cls_index[].expand", "hidden_states.gather().squeeze", "torch.cat", "modeling_utils.PoolerAnswerClass.dense_1", "hidden_states.gather", "hidden_states.gather"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_states", "=", "None", ",", "start_positions", "=", "None", ",", "cls_index", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span.\n            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n                position of the CLS token. If None, take the last token.\n            note(Original repo):\n                no dependency on end_feature so that we can obtain one single `cls_logits`\n                for each sample\n        \"\"\"", "\n", "hsz", "=", "hidden_states", ".", "shape", "[", "-", "1", "]", "\n", "assert", "start_states", "is", "not", "None", "or", "start_positions", "is", "not", "None", ",", "\"One of start_states, start_positions should be not None\"", "\n", "if", "start_positions", "is", "not", "None", ":", "\n", "            ", "start_positions", "=", "start_positions", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "start_positions", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, hsz)", "\n", "\n", "", "if", "cls_index", "is", "not", "None", ":", "\n", "            ", "cls_index", "=", "cls_index", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "cls_token_state", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "cls_index", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, hsz)", "\n", "", "else", ":", "\n", "            ", "cls_token_state", "=", "hidden_states", "[", ":", ",", "-", "1", ",", ":", "]", "# shape (bsz, hsz)", "\n", "\n", "", "x", "=", "self", ".", "dense_0", "(", "torch", ".", "cat", "(", "[", "start_states", ",", "cls_token_state", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "x", "=", "self", ".", "dense_1", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.SQuADHead.__init__": [[680, 688], ["torch.nn.Module.__init__", "modeling_utils.PoolerStartLogits", "modeling_utils.PoolerEndLogits", "modeling_utils.PoolerAnswerClass"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "SQuADHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "start_n_top", "=", "config", ".", "start_n_top", "\n", "self", ".", "end_n_top", "=", "config", ".", "end_n_top", "\n", "\n", "self", ".", "start_logits", "=", "PoolerStartLogits", "(", "config", ")", "\n", "self", ".", "end_logits", "=", "PoolerEndLogits", "(", "config", ")", "\n", "self", ".", "answer_class", "=", "PoolerAnswerClass", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.SQuADHead.forward": [[689, 747], ["modeling_utils.SQuADHead.start_logits", "modeling_utils.SQuADHead.end_logits", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "hidden_states.size", "torch.nn.functional.softmax", "torch.topk", "start_top_index.unsqueeze().expand", "torch.gather", "torch.einsum.unsqueeze().expand", "hidden_states.unsqueeze().expand_as", "modeling_utils.SQuADHead.end_logits", "torch.nn.functional.softmax", "torch.topk", "end_top_log_probs.view.view.view", "end_top_index.view.view.view", "torch.einsum", "modeling_utils.SQuADHead.answer_class", "modeling_utils.SQuADHead.answer_class", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "p_mask.unsqueeze", "x.squeeze_", "start_top_index.unsqueeze", "torch.einsum.unsqueeze", "hidden_states.unsqueeze", "x.dim"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "\n", "cls_index", "=", "None", ",", "is_impossible", "=", "None", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "outputs", "=", "(", ")", "\n", "\n", "start_logits", "=", "self", ".", "start_logits", "(", "hidden_states", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, let's remove the dimension added by batch splitting", "\n", "            ", "for", "x", "in", "(", "start_positions", ",", "end_positions", ",", "cls_index", ",", "is_impossible", ")", ":", "\n", "                ", "if", "x", "is", "not", "None", "and", "x", ".", "dim", "(", ")", ">", "1", ":", "\n", "                    ", "x", ".", "squeeze_", "(", "-", "1", ")", "\n", "\n", "# during training, compute the end logits based on the ground truth of the start position", "\n", "", "", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "\n", "if", "cls_index", "is", "not", "None", "and", "is_impossible", "is", "not", "None", ":", "\n", "# Predict answerability from the representation of CLS and START", "\n", "                ", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "cls_index", "=", "cls_index", ")", "\n", "loss_fct_cls", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "cls_loss", "=", "loss_fct_cls", "(", "cls_logits", ",", "is_impossible", ")", "\n", "\n", "# note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss", "\n", "total_loss", "+=", "cls_loss", "*", "0.5", "\n", "\n", "", "outputs", "=", "(", "total_loss", ",", ")", "+", "outputs", "\n", "\n", "", "else", ":", "\n", "# during inference, compute the end logits based on beam search", "\n", "            ", "bsz", ",", "slen", ",", "hsz", "=", "hidden_states", ".", "size", "(", ")", "\n", "start_log_probs", "=", "F", ".", "softmax", "(", "start_logits", ",", "dim", "=", "-", "1", ")", "# shape (bsz, slen)", "\n", "\n", "start_top_log_probs", ",", "start_top_index", "=", "torch", ".", "topk", "(", "start_log_probs", ",", "self", ".", "start_n_top", ",", "dim", "=", "-", "1", ")", "# shape (bsz, start_n_top)", "\n", "start_top_index_exp", "=", "start_top_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "torch", ".", "gather", "(", "hidden_states", ",", "-", "2", ",", "start_top_index_exp", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "start_states", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ",", "-", "1", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "\n", "hidden_states_expanded", "=", "hidden_states", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "start_states", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "p_mask", "=", "p_mask", ".", "unsqueeze", "(", "-", "1", ")", "if", "p_mask", "is", "not", "None", "else", "None", "\n", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states_expanded", ",", "start_states", "=", "start_states", ",", "p_mask", "=", "p_mask", ")", "\n", "end_log_probs", "=", "F", ".", "softmax", "(", "end_logits", ",", "dim", "=", "1", ")", "# shape (bsz, slen, start_n_top)", "\n", "\n", "end_top_log_probs", ",", "end_top_index", "=", "torch", ".", "topk", "(", "end_log_probs", ",", "self", ".", "end_n_top", ",", "dim", "=", "1", ")", "# shape (bsz, end_n_top, start_n_top)", "\n", "end_top_log_probs", "=", "end_top_log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "end_top_index", "=", "end_top_index", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "\n", "start_states", "=", "torch", ".", "einsum", "(", "\"blh,bl->bh\"", ",", "hidden_states", ",", "start_log_probs", ")", "\n", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_states", "=", "start_states", ",", "cls_index", "=", "cls_index", ")", "\n", "\n", "outputs", "=", "(", "start_top_log_probs", ",", "start_top_index", ",", "end_top_log_probs", ",", "end_top_index", ",", "cls_logits", ")", "+", "outputs", "\n", "\n", "# return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits", "\n", "# or (if labels are provided) (total_loss,)", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.SequenceSummary.__init__": [[764, 793], ["torch.nn.Module.__init__", "Identity", "Identity", "Identity", "Identity", "hasattr", "hasattr", "torch.nn.Linear", "hasattr", "torch.nn.Tanh", "hasattr", "torch.nn.Dropout", "hasattr", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "SequenceSummary", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "summary_type", "=", "config", ".", "summary_type", "if", "hasattr", "(", "config", ",", "'summary_use_proj'", ")", "else", "'last'", "\n", "if", "self", ".", "summary_type", "==", "'attn'", ":", "\n", "# We should use a standard multi-head attention module with absolute positional embedding for that.", "\n", "# Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276", "\n", "# We can probably just use the multi-head attention module of PyTorch >=1.1.0", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "self", ".", "summary", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "'summary_use_proj'", ")", "and", "config", ".", "summary_use_proj", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "'summary_proj_to_labels'", ")", "and", "config", ".", "summary_proj_to_labels", "and", "config", ".", "num_labels", ">", "0", ":", "\n", "                ", "num_classes", "=", "config", ".", "num_labels", "\n", "", "else", ":", "\n", "                ", "num_classes", "=", "config", ".", "hidden_size", "\n", "", "self", ".", "summary", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_classes", ")", "\n", "\n", "", "self", ".", "activation", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "'summary_activation'", ")", "and", "config", ".", "summary_activation", "==", "'tanh'", ":", "\n", "            ", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "", "self", ".", "first_dropout", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "'summary_first_dropout'", ")", "and", "config", ".", "summary_first_dropout", ">", "0", ":", "\n", "            ", "self", ".", "first_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "summary_first_dropout", ")", "\n", "\n", "", "self", ".", "last_dropout", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "'summary_last_dropout'", ")", "and", "config", ".", "summary_last_dropout", ">", "0", ":", "\n", "            ", "self", ".", "last_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "summary_last_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.SequenceSummary.forward": [[794, 824], ["modeling_utils.SequenceSummary.first_dropout", "modeling_utils.SequenceSummary.summary", "modeling_utils.SequenceSummary.activation", "modeling_utils.SequenceSummary.last_dropout", "hidden_states.mean", "hidden_states.gather().squeeze", "torch.full_like", "cls_index.expand.expand.unsqueeze().unsqueeze", "cls_index.expand.expand.expand", "hidden_states.gather", "cls_index.expand.expand.unsqueeze", "hidden_states.size", "cls_index.expand.expand.dim"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "cls_index", "=", "None", ")", ":", "\n", "        ", "\"\"\" hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.\n            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n                if summary_type == 'cls_index' and cls_index is None:\n                    we take the last token of the sequence as classification token\n        \"\"\"", "\n", "if", "self", ".", "summary_type", "==", "'last'", ":", "\n", "            ", "output", "=", "hidden_states", "[", ":", ",", "-", "1", "]", "\n", "", "elif", "self", ".", "summary_type", "==", "'first'", ":", "\n", "            ", "output", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "", "elif", "self", ".", "summary_type", "==", "'mean'", ":", "\n", "            ", "output", "=", "hidden_states", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "summary_type", "==", "'cls_index'", ":", "\n", "            ", "if", "cls_index", "is", "None", ":", "\n", "                ", "cls_index", "=", "torch", ".", "full_like", "(", "hidden_states", "[", "...", ",", ":", "1", ",", ":", "]", ",", "hidden_states", ".", "shape", "[", "-", "2", "]", "-", "1", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "else", ":", "\n", "                ", "cls_index", "=", "cls_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "cls_index", "=", "cls_index", ".", "expand", "(", "(", "-", "1", ",", ")", "*", "(", "cls_index", ".", "dim", "(", ")", "-", "1", ")", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "# shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states", "\n", "", "output", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "cls_index", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, XX, hidden_size)", "\n", "", "elif", "self", ".", "summary_type", "==", "'attn'", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "output", "=", "self", ".", "first_dropout", "(", "output", ")", "\n", "output", "=", "self", ".", "summary", "(", "output", ")", "\n", "output", "=", "self", ".", "activation", "(", "output", ")", "\n", "output", "=", "self", ".", "last_dropout", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_linear_layer": [[826, 849], ["index.to.to", "layer.weight.index_select().clone().detach", "list", "len", "torch.nn.Linear().to", "nn.Linear().to.weight.copy_", "layer.weight.size", "layer.weight.index_select().clone().detach.contiguous", "nn.Linear().to.bias.copy_", "layer.weight.index_select().clone", "layer.bias.clone().detach", "layer.bias[].clone().detach", "torch.nn.Linear", "layer.bias[].clone().detach.contiguous", "layer.weight.index_select", "layer.bias.clone", "layer.bias[].clone"], "function", ["None"], ["", "", "def", "prune_linear_layer", "(", "layer", ",", "index", ",", "dim", "=", "0", ")", ":", "\n", "    ", "\"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "index", "=", "index", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "W", "=", "layer", ".", "weight", ".", "index_select", "(", "dim", ",", "index", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "layer", ".", "bias", "is", "not", "None", ":", "\n", "        ", "if", "dim", "==", "1", ":", "\n", "            ", "b", "=", "layer", ".", "bias", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "b", "=", "layer", ".", "bias", "[", "index", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "", "new_size", "=", "list", "(", "layer", ".", "weight", ".", "size", "(", ")", ")", "\n", "new_size", "[", "dim", "]", "=", "len", "(", "index", ")", "\n", "new_layer", "=", "nn", ".", "Linear", "(", "new_size", "[", "1", "]", ",", "new_size", "[", "0", "]", ",", "bias", "=", "layer", ".", "bias", "is", "not", "None", ")", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "weight", ".", "copy_", "(", "W", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "True", "\n", "if", "layer", ".", "bias", "is", "not", "None", ":", "\n", "        ", "new_layer", ".", "bias", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "bias", ".", "copy_", "(", "b", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "True", "\n", "", "return", "new_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_conv1d_layer": [[851, 873], ["index.to.to", "layer.weight.index_select().clone().detach", "list", "len", "Conv1D().to", "Conv1D().to.weight.copy_", "Conv1D().to.bias.copy_", "layer.bias.clone().detach", "layer.bias[].clone().detach", "layer.weight.size", "layer.weight.index_select().clone().detach.contiguous", "layer.bias[].clone().detach.contiguous", "layer.weight.index_select().clone", "modeling_utils.Conv1D", "layer.bias.clone", "layer.bias[].clone", "layer.weight.index_select"], "function", ["None"], ["", "def", "prune_conv1d_layer", "(", "layer", ",", "index", ",", "dim", "=", "1", ")", ":", "\n", "    ", "\"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "index", "=", "index", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "W", "=", "layer", ".", "weight", ".", "index_select", "(", "dim", ",", "index", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "dim", "==", "0", ":", "\n", "        ", "b", "=", "layer", ".", "bias", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "        ", "b", "=", "layer", ".", "bias", "[", "index", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "new_size", "=", "list", "(", "layer", ".", "weight", ".", "size", "(", ")", ")", "\n", "new_size", "[", "dim", "]", "=", "len", "(", "index", ")", "\n", "new_layer", "=", "Conv1D", "(", "new_size", "[", "1", "]", ",", "new_size", "[", "0", "]", ")", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "weight", ".", "copy_", "(", "W", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "True", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "bias", ".", "copy_", "(", "b", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "True", "\n", "return", "new_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_layer": [[875, 886], ["isinstance", "modeling_utils.prune_linear_layer", "isinstance", "modeling_utils.prune_conv1d_layer", "ValueError"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_linear_layer", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.prune_conv1d_layer"], ["", "def", "prune_layer", "(", "layer", ",", "index", ",", "dim", "=", "None", ")", ":", "\n", "    ", "\"\"\" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "if", "isinstance", "(", "layer", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "return", "prune_linear_layer", "(", "layer", ",", "index", ",", "dim", "=", "0", "if", "dim", "is", "None", "else", "dim", ")", "\n", "", "elif", "isinstance", "(", "layer", ",", "Conv1D", ")", ":", "\n", "        ", "return", "prune_conv1d_layer", "(", "layer", ",", "index", ",", "dim", "=", "1", "if", "dim", "is", "None", "else", "dim", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Can't prune layer of class {}\"", ".", "format", "(", "layer", ".", "__class__", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.SplitTokenizer.__init__": [[124, 127], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "pad_idx", "=", "0", ",", "encoding_length", "=", "20", ")", ":", "\n", "        ", "self", ".", "encoding_length", "=", "encoding_length", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.SplitTokenizer.encode_sentence": [[128, 134], ["numpy.array", "len", "len", "int", "sentence.strip", "sentence.strip().split", "len", "sentence.strip"], "methods", ["None"], ["", "def", "encode_sentence", "(", "self", ",", "sentence", ")", ":", "\n", "#print(sentence)", "\n", "        ", "encoding", "=", "[", "]", "if", "len", "(", "sentence", ".", "strip", "(", ")", ")", "==", "0", "else", "[", "int", "(", "i", ")", "for", "i", "in", "sentence", ".", "strip", "(", ")", ".", "split", "(", "'_'", ")", "]", "\n", "if", "len", "(", "encoding", ")", "<", "self", ".", "encoding_length", ":", "\n", "            ", "encoding", "+=", "[", "self", ".", "pad_idx", "]", "*", "(", "self", ".", "encoding_length", "-", "len", "(", "encoding", ")", ")", "\n", "", "return", "np", ".", "array", "(", "encoding", "[", ":", "self", ".", "encoding_length", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.Tokenizer.__init__": [[140, 147], ["enumerate"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", "=", "None", ",", "encoding_length", "=", "20", ")", ":", "\n", "        ", "self", ".", "encoding_length", "=", "encoding_length", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word_to_index", "=", "{", "}", "\n", "if", "vocab", ":", "\n", "            ", "for", "i", ",", "word", "in", "enumerate", "(", "vocab", ")", ":", "\n", "                ", "self", ".", "word_to_index", "[", "word", "]", "=", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.Tokenizer.split_sentence": [[148, 158], ["s.strip().lower", "utils.Tokenizer.SENTENCE_SPLIT_REGEX.split", "all", "list", "toks.append", "s.strip", "sentence.strip", "len", "all", "s.strip"], "methods", ["None"], ["", "", "", "def", "split_sentence", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "''' Break sentence into a list of words and punctuation '''", "\n", "toks", "=", "[", "]", "\n", "for", "word", "in", "[", "s", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "s", "in", "self", ".", "SENTENCE_SPLIT_REGEX", ".", "split", "(", "sentence", ".", "strip", "(", ")", ")", "if", "len", "(", "s", ".", "strip", "(", ")", ")", ">", "0", "]", ":", "\n", "# Break up any words containing punctuation only, e.g. '!?', unless it is multiple full stops e.g. '..'", "\n", "            ", "if", "all", "(", "c", "in", "string", ".", "punctuation", "for", "c", "in", "word", ")", "and", "not", "all", "(", "c", "in", "'.'", "for", "c", "in", "word", ")", ":", "\n", "                ", "toks", "+=", "list", "(", "word", ")", "\n", "", "else", ":", "\n", "                ", "toks", ".", "append", "(", "word", ")", "\n", "", "", "return", "toks", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.Tokenizer.encode_sentence": [[159, 175], ["encoding.append", "numpy.array", "len", "sys.exit", "utils.Tokenizer.split_sentence", "len", "encoding.append", "encoding.append", "len"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.Tokenizer.split_sentence"], ["", "def", "encode_sentence", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "word_to_index", ")", "==", "0", ":", "\n", "            ", "sys", ".", "exit", "(", "'Tokenizer has no vocab'", ")", "\n", "\n", "", "encoding", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "split_sentence", "(", "sentence", ")", "[", ":", ":", "-", "1", "]", ":", "# reverse input sentences", "\n", "            ", "if", "word", "in", "self", ".", "word_to_index", ":", "\n", "                ", "encoding", ".", "append", "(", "self", ".", "word_to_index", "[", "word", "]", ")", "\n", "", "else", ":", "\n", "                ", "encoding", ".", "append", "(", "self", ".", "word_to_index", "[", "'<UNK>'", "]", ")", "\n", "", "", "encoding", ".", "append", "(", "self", ".", "word_to_index", "[", "'<EOS>'", "]", ")", "\n", "\n", "if", "len", "(", "encoding", ")", "<", "self", ".", "encoding_length", ":", "\n", "            ", "encoding", "+=", "[", "self", ".", "word_to_index", "[", "'<PAD>'", "]", "]", "*", "(", "self", ".", "encoding_length", "-", "len", "(", "encoding", ")", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "encoding", "[", ":", "self", ".", "encoding_length", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.Tokenizer.decode_sentence": [[176, 184], ["sentence.append"], "methods", ["None"], ["", "def", "decode_sentence", "(", "self", ",", "encoding", ")", ":", "\n", "        ", "sentence", "=", "[", "]", "\n", "for", "ix", "in", "encoding", ":", "\n", "            ", "if", "ix", "==", "self", ".", "word_to_index", "[", "'<PAD>'", "]", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "sentence", ".", "append", "(", "self", ".", "vocab", "[", "ix", "]", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "sentence", "[", ":", ":", "-", "1", "]", ")", "# unreverse before output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_nav_graphs": [[19, 45], ["open", "networkx.Graph", "json.load", "enumerate", "networkx.set_node_attributes", "enumerate", "numpy.array", "nx.Graph.add_edge", "utils.load_nav_graphs.distance"], "function", ["None"], ["def", "load_nav_graphs", "(", "scans", ")", ":", "\n", "    ", "''' Load connectivity graph for each scan '''", "\n", "\n", "def", "distance", "(", "pose1", ",", "pose2", ")", ":", "\n", "        ", "''' Euclidean distance between two graph poses '''", "\n", "return", "(", "(", "pose1", "[", "'pose'", "]", "[", "3", "]", "-", "pose2", "[", "'pose'", "]", "[", "3", "]", ")", "**", "2", "+", "(", "pose1", "[", "'pose'", "]", "[", "7", "]", "-", "pose2", "[", "'pose'", "]", "[", "7", "]", ")", "**", "2", "+", "(", "pose1", "[", "'pose'", "]", "[", "11", "]", "-", "pose2", "[", "'pose'", "]", "[", "11", "]", ")", "**", "2", ")", "**", "0.5", "\n", "\n", "", "graphs", "=", "{", "}", "\n", "for", "scan", "in", "scans", ":", "\n", "        ", "with", "open", "(", "'connectivity/%s_connectivity.json'", "%", "scan", ")", "as", "f", ":", "\n", "            ", "G", "=", "nx", ".", "Graph", "(", ")", "\n", "positions", "=", "{", "}", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "if", "item", "[", "'included'", "]", ":", "\n", "                    ", "for", "j", ",", "conn", "in", "enumerate", "(", "item", "[", "'unobstructed'", "]", ")", ":", "\n", "                        ", "if", "conn", "and", "data", "[", "j", "]", "[", "'included'", "]", ":", "\n", "                            ", "positions", "[", "item", "[", "'image_id'", "]", "]", "=", "np", ".", "array", "(", "[", "item", "[", "'pose'", "]", "[", "3", "]", ",", "\n", "item", "[", "'pose'", "]", "[", "7", "]", ",", "item", "[", "'pose'", "]", "[", "11", "]", "]", ")", "\n", "assert", "data", "[", "j", "]", "[", "'unobstructed'", "]", "[", "i", "]", ",", "'Graph should be undirected'", "\n", "G", ".", "add_edge", "(", "item", "[", "'image_id'", "]", ",", "data", "[", "j", "]", "[", "'image_id'", "]", ",", "weight", "=", "distance", "(", "item", ",", "data", "[", "j", "]", ")", ")", "\n", "", "", "", "", "nx", ".", "set_node_attributes", "(", "G", ",", "values", "=", "positions", ",", "name", "=", "'position'", ")", "\n", "graphs", "[", "scan", "]", "=", "G", "\n", "", "", "return", "graphs", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_transformer_index": [[47, 54], ["utils.dump_bert_index", "utils.dump_gpt_index"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_bert_index", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_gpt_index"], ["", "def", "dump_transformer_index", "(", "encoder_type", ",", "splits", ")", ":", "\n", "    ", "if", "encoder_type", "==", "'bert'", "or", "encoder_type", "==", "'vlbert'", "or", "encoder_type", "==", "'MultiDicEncoder'", ":", "\n", "        ", "dump_bert_index", "(", "splits", ")", "\n", "", "elif", "encoder_type", "==", "'gpt'", ":", "\n", "        ", "dump_gpt_index", "(", "splits", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_gpt_index": [[55, 69], ["OpenAIGPTTokenizer.from_pretrained", "utils.load_datasets", "utils.write_vocab", "OpenAIGPTTokenizer.from_pretrained.tokenize", "OpenAIGPTTokenizer.from_pretrained.convert_tokens_to_ids", "indexed_tokens.append", "str"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.write_vocab"], ["", "", "def", "dump_gpt_index", "(", "splits", ")", ":", "\n", "    ", "from", "pytorch_pretrained_bert", "import", "OpenAIGPTTokenizer", "\n", "tokenizer", "=", "OpenAIGPTTokenizer", ".", "from_pretrained", "(", "'openai-gpt'", ")", "\n", "#splits = ['train', 'val_seen', 'val_unseen', 'test']", "\n", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "data", "=", "load_datasets", "(", "[", "split", "]", ",", "encoder_type", "=", "'lstm'", ")", "# here we use lstm dataset to preprocess the data,", "\n", "indexed_tokens", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "            ", "for", "instr", "in", "item", "[", "'instructions'", "]", ":", "\n", "                ", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "instr", ")", "\n", "tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "indexed_tokens", ".", "append", "(", "'_'", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "tokens", "]", ")", ")", "\n", "", "", "write_vocab", "(", "indexed_tokens", ",", "'tasks/R2R/data/R2R_%s_gpt.txt'", "%", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_bert_index": [[71, 89], ["BertTokenizer.from_pretrained", "utils.load_datasets", "utils.write_vocab", "sent_tokenize", "BertTokenizer.from_pretrained.tokenize", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "indexed_tokens.append", "str"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.write_vocab"], ["", "", "def", "dump_bert_index", "(", "splits", ")", ":", "\n", "    ", "from", "pytorch_pretrained_bert", "import", "BertTokenizer", "\n", "from", "nltk", ".", "tokenize", "import", "sent_tokenize", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "#splits = ['train', 'val_seen', 'val_unseen', 'test']", "\n", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "data", "=", "load_datasets", "(", "[", "split", "]", ",", "encoder_type", "=", "'lstm'", ")", "# here we use lstm dataset to preprocess the data,", "\n", "indexed_tokens", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "            ", "for", "instr", "in", "item", "[", "'instructions'", "]", ":", "\n", "                ", "sents", "=", "sent_tokenize", "(", "instr", ")", "\n", "instr", "=", "'[CLS] '", "+", "(", "' [SEP] '", ".", "join", "(", "sents", ")", ")", "+", "' [SEP]'", "\n", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "instr", ")", "\n", "tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "indexed_tokens", ".", "append", "(", "'_'", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "tokens", "]", ")", ")", "\n", "", "", "write_vocab", "(", "indexed_tokens", ",", "'tasks/R2R/data/R2R_%s_bert.txt'", "%", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets": [[91, 121], ["open", "json.load", "utils.read_vocab", "enumerate", "print", "print", "os.path.exists", "utils.dump_transformer_index", "enumerate", "len", "data.pop", "err_items.append"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.read_vocab", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_transformer_index"], ["", "", "def", "load_datasets", "(", "splits", ",", "encoder_type", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "with", "open", "(", "'tasks/R2R/data/R2R_%s.json'", "%", "split", ")", "as", "f", ":", "\n", "            ", "data", "+=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "encoder_type", "in", "[", "'bert'", ",", "'gpt'", ",", "'vlbert'", ",", "'MultiVicEncoder'", ",", "'MultiDicEncoder'", "]", ":", "\n", "#filename = 'tasks/R2R/data/R2R_%s_%s.txt' % (split, encoder_type)", "\n", "#if encoder_type == 'bert' or encoder_type == 'vlbert':", "\n", "            ", "if", "encoder_type", "in", "[", "'bert'", ",", "'MultiVicEncoder'", ",", "'MultiDicEncoder'", ",", "'vlbert'", "]", ":", "\n", "                ", "filename", "=", "'tasks/R2R/data/R2R_%s_bert.txt'", "%", "(", "split", ")", "\n", "print", "(", "\"You are using vocab: %s !!\"", "%", "(", "filename", ")", ")", "\n", "", "else", ":", "\n", "                ", "filename", "=", "'tasks/R2R/data/R2R_%s_%s.txt'", "%", "(", "split", ",", "encoder_type", ")", "\n", "print", "(", "\"You are using vocab: %s !!\"", "%", "(", "filename", ")", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "dump_transformer_index", "(", "encoder_type", ",", "[", "split", "]", ")", "\n", "", "transformer_index", "=", "read_vocab", "(", "filename", ")", "\n", "j", "=", "0", "\n", "err_items", "=", "[", "]", "\n", "for", "k", ",", "item", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "for", "i", ",", "instr", "in", "enumerate", "(", "item", "[", "'instructions'", "]", ")", ":", "\n", "                    ", "item", "[", "'instructions'", "]", "[", "i", "]", "=", "transformer_index", "[", "j", "]", "\n", "if", "not", "transformer_index", "[", "j", "]", ":", "\n", "                        ", "err_items", ".", "append", "(", "k", ")", "\n", "", "j", "+=", "1", "\n", "", "", "assert", "j", "==", "len", "(", "transformer_index", ")", "\n", "for", "k", "in", "err_items", "[", ":", ":", "-", "1", "]", ":", "\n", "                ", "data", ".", "pop", "(", "k", ")", "\n", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.build_vocab": [[186, 202], ["collections.Counter", "utils.Tokenizer", "utils.load_datasets", "list", "collections.Counter.most_common", "collections.Counter.update", "list.append", "utils.Tokenizer.split_sentence"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.Tokenizer.split_sentence"], ["", "", "def", "build_vocab", "(", "splits", "=", "[", "'train'", "]", ",", "min_count", "=", "5", ",", "start_vocab", "=", "base_vocab", ")", ":", "\n", "    ", "''' Build a vocab, starting with base vocab containing a few useful tokens. '''", "\n", "count", "=", "Counter", "(", ")", "\n", "t", "=", "Tokenizer", "(", ")", "\n", "data", "=", "load_datasets", "(", "splits", ",", "encoder_type", "=", "'lstm'", ")", "#, False)", "\n", "for", "item", "in", "data", ":", "\n", "        ", "for", "instr", "in", "item", "[", "'instructions'", "]", ":", "\n", "            ", "count", ".", "update", "(", "t", ".", "split_sentence", "(", "instr", ")", ")", "\n", "\n", "", "", "vocab", "=", "list", "(", "start_vocab", ")", "\n", "for", "word", ",", "num", "in", "count", ".", "most_common", "(", ")", ":", "\n", "        ", "if", "num", ">=", "min_count", ":", "\n", "            ", "vocab", ".", "append", "(", "word", ")", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.write_vocab": [[204, 209], ["print", "open", "f.write", "len"], "function", ["None"], ["", "def", "write_vocab", "(", "vocab", ",", "path", ")", ":", "\n", "    ", "print", "(", "'Writing vocab of size %d to %s'", "%", "(", "len", "(", "vocab", ")", ",", "path", ")", ")", "\n", "with", "open", "(", "path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "vocab", ":", "\n", "            ", "f", ".", "write", "(", "\"%s\\n\"", "%", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.read_vocab": [[211, 215], ["open", "word.strip", "f.readlines"], "function", ["None"], ["", "", "", "def", "read_vocab", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "vocab", "=", "[", "word", ".", "strip", "(", ")", "for", "word", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.asMinutes": [[217, 221], ["math.floor"], "function", ["None"], ["", "def", "asMinutes", "(", "s", ")", ":", "\n", "    ", "m", "=", "math", ".", "floor", "(", "s", "/", "60", ")", "\n", "s", "-=", "m", "*", "60", "\n", "return", "'%dm %ds'", "%", "(", "m", ",", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.timeSince": [[223, 229], ["time.time", "utils.asMinutes", "utils.asMinutes"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.asMinutes", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.asMinutes"], ["", "def", "timeSince", "(", "since", ",", "percent", ")", ":", "\n", "    ", "now", "=", "time", ".", "time", "(", ")", "\n", "s", "=", "now", "-", "since", "\n", "es", "=", "s", "/", "(", "percent", ")", "\n", "rs", "=", "es", "-", "s", "\n", "return", "'%s (- %s)'", "%", "(", "asMinutes", "(", "s", ")", ",", "asMinutes", "(", "rs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.to_contiguous": [[231, 234], ["tensor.is_contiguous", "tensor.contiguous"], "function", ["None"], ["", "def", "to_contiguous", "(", "tensor", ")", ":", "# jolin", "\n", "    ", "if", "tensor", ".", "is_contiguous", "(", ")", ":", "return", "tensor", "\n", "else", ":", "return", "tensor", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.clip_gradient": [[236, 241], ["param.grad.data.clamp_"], "function", ["None"], ["", "def", "clip_gradient", "(", "optimizer", ",", "grad_clip", "=", "0.1", ")", ":", "# jolin", "\n", "    ", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "for", "param", "in", "group", "[", "'params'", "]", ":", "\n", "            ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                ", "param", ".", "grad", ".", "data", ".", "clamp_", "(", "-", "grad_clip", ",", "grad_clip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.boolean_string": [[244, 248], ["ValueError"], "function", ["None"], ["", "", "", "", "def", "boolean_string", "(", "s", ")", ":", "\n", "    ", "if", "s", "not", "in", "{", "'False'", ",", "'True'", "}", ":", "\n", "        ", "raise", "ValueError", "(", "'Not a valid boolean string'", ")", "\n", "", "return", "s", "==", "'True'", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.dump_get_navigable": [[250, 267], ["BertTokenizer.from_pretrained", "utils.load_datasets", "utils.write_vocab", "sent_tokenize", "BertTokenizer.from_pretrained.tokenize", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "indexed_tokens.append", "str"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.modeling_utils.PreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.write_vocab"], ["", "def", "dump_get_navigable", "(", ")", ":", "\n", "    ", "from", "pytorch_pretrained_bert", "import", "BertTokenizer", "\n", "from", "nltk", ".", "tokenize", "import", "sent_tokenize", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "splits", "=", "[", "'train'", ",", "'val_seen'", ",", "'val_unseen'", ",", "'test'", "]", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "data", "=", "load_datasets", "(", "[", "split", "]", ",", "False", ")", "\n", "indexed_tokens", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "            ", "for", "instr", "in", "item", "[", "'instructions'", "]", ":", "\n", "                ", "sents", "=", "sent_tokenize", "(", "instr", ")", "\n", "instr", "=", "'[CLS] '", "+", "(", "' [SEP] '", ".", "join", "(", "sents", ")", ")", "+", "' [SEP]'", "\n", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "instr", ")", "\n", "tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "indexed_tokens", ".", "append", "(", "'_'", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "tokens", "]", ")", ")", "\n", "", "", "write_vocab", "(", "indexed_tokens", ",", "'tasks/R2R/data/R2R_%s_bert.txt'", "%", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils._loc_distance": [[269, 271], ["numpy.sqrt"], "function", ["None"], ["", "", "def", "_loc_distance", "(", "loc", ")", ":", "\n", "    ", "return", "np", ".", "sqrt", "(", "loc", ".", "rel_heading", "**", "2", "+", "loc", ".", "rel_elevation", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.preprocess_get_pano_states": [[273, 350], ["os.path.exists", "sys.path.append", "MatterSim.Simulator", "MatterSim.Simulator.setRenderingEnabled", "MatterSim.Simulator.setDiscretizedViewingAngles", "MatterSim.Simulator.setCameraResolution", "MatterSim.Simulator.setCameraVFOV", "MatterSim.Simulator.init", "math.radians", "utils.load_datasets", "print", "open", "json.dump", "open", "json.load", "enumerate", "MatterSim.Simulator.newEpisode", "MatterSim.Simulator.getState", "range", "range", "range", "MatterSim.Simulator.getState", "defaultdict", "adj_dict.items", "int", "MatterSim.Simulator.makeAction", "MatterSim.Simulator.getState", "int", "MatterSim.Simulator.makeAction", "absViewIndex2points[].append", "abs", "numpy.sign", "utils._loc_distance", "MatterSim.Simulator.makeAction", "MatterSim.Simulator.makeAction", "abs", "numpy.sign"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils._loc_distance"], ["", "def", "preprocess_get_pano_states", "(", "navigable_locs_path", "=", "\"tasks/R2R/data/navigable_locs.json\"", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "navigable_locs_path", ")", ":", "\n", "        ", "return", "\n", "", "image_w", "=", "640", "\n", "image_h", "=", "480", "\n", "vfov", "=", "60", "\n", "import", "sys", "\n", "sys", ".", "path", ".", "append", "(", "'build'", ")", "\n", "import", "MatterSim", "\n", "from", "collections", "import", "defaultdict", "\n", "\n", "sim", "=", "MatterSim", ".", "Simulator", "(", ")", "\n", "sim", ".", "setRenderingEnabled", "(", "False", ")", "\n", "sim", ".", "setDiscretizedViewingAngles", "(", "True", ")", "\n", "sim", ".", "setCameraResolution", "(", "image_w", ",", "image_h", ")", "\n", "sim", ".", "setCameraVFOV", "(", "math", ".", "radians", "(", "vfov", ")", ")", "\n", "sim", ".", "init", "(", ")", "\n", "\n", "splits", "=", "[", "'train'", ",", "'val_seen'", ",", "'val_unseen'", ",", "'test'", "]", "\n", "graphs", "=", "{", "}", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "data", "=", "load_datasets", "(", "[", "split", "]", ",", "encoder_type", "=", "'lstm'", ")", "\n", "for", "item", "in", "data", ":", "\n", "# print(item.keys())", "\n", "# print(\"\")", "\n", "            ", "scan", "=", "item", "[", "\"scan\"", "]", "\n", "if", "scan", "in", "graphs", ":", "\n", "                ", "continue", "\n", "", "graphs", "[", "scan", "]", "=", "{", "}", "\n", "with", "open", "(", "'connectivity/%s_connectivity.json'", "%", "scan", ")", "as", "f", ":", "\n", "                ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "data", ")", ":", "\n", "                    ", "if", "item", "[", "'included'", "]", ":", "\n", "                        ", "viewpointId", "=", "item", "[", "'image_id'", "]", "\n", "sim", ".", "newEpisode", "(", "scan", ",", "viewpointId", ",", "0", ",", "0", ")", "\n", "state", "=", "sim", ".", "getState", "(", ")", "\n", "\n", "initViewIndex", "=", "state", ".", "viewIndex", "\n", "# 1. first look down, turning to relViewIndex 0", "\n", "elevation_delta", "=", "-", "(", "state", ".", "viewIndex", "//", "12", ")", "\n", "for", "_", "in", "range", "(", "int", "(", "abs", "(", "elevation_delta", ")", ")", ")", ":", "\n", "                            ", "''' Make possibly more than one elevation turns '''", "\n", "sim", ".", "makeAction", "(", "0", ",", "0", ",", "np", ".", "sign", "(", "elevation_delta", ")", ")", "\n", "\n", "", "adj_dict", "=", "{", "}", "\n", "for", "relViewIndex", "in", "range", "(", "36", ")", ":", "\n", "                            ", "state", "=", "sim", ".", "getState", "(", ")", "\n", "absViewIndex", "=", "state", ".", "viewIndex", "\n", "for", "loc", "in", "state", ".", "navigableLocations", "[", "1", ":", "]", ":", "\n", "                                ", "distance", "=", "_loc_distance", "(", "loc", ")", "\n", "if", "(", "loc", ".", "viewpointId", "not", "in", "adj_dict", "or", "\n", "distance", "<", "adj_dict", "[", "loc", ".", "viewpointId", "]", "[", "'distance'", "]", ")", ":", "\n", "                                    ", "adj_dict", "[", "loc", ".", "viewpointId", "]", "=", "{", "\n", "'absViewIndex'", ":", "absViewIndex", ",", "\n", "'nextViewpointId'", ":", "loc", ".", "viewpointId", ",", "\n", "'loc_rel_heading'", ":", "loc", ".", "rel_heading", ",", "\n", "'loc_rel_elevation'", ":", "loc", ".", "rel_elevation", ",", "\n", "'distance'", ":", "distance", "}", "\n", "", "", "if", "(", "relViewIndex", "+", "1", ")", "%", "12", "==", "0", ":", "\n", "                                ", "sim", ".", "makeAction", "(", "0", ",", "1", ",", "1", ")", "# Turn right and look up", "\n", "", "else", ":", "\n", "                                ", "sim", ".", "makeAction", "(", "0", ",", "1", ",", "0", ")", "# Turn right", "\n", "# 3. turn back to the original view", "\n", "", "", "for", "_", "in", "range", "(", "int", "(", "abs", "(", "-", "2", "-", "elevation_delta", ")", ")", ")", ":", "\n", "                            ", "''' Make possibly more than one elevation turns '''", "\n", "sim", ".", "makeAction", "(", "0", ",", "0", ",", "np", ".", "sign", "(", "-", "2", "-", "elevation_delta", ")", ")", "\n", "\n", "", "state", "=", "sim", ".", "getState", "(", ")", "\n", "assert", "state", ".", "viewIndex", "==", "initViewIndex", "\n", "\n", "absViewIndex2points", "=", "defaultdict", "(", "list", ")", "\n", "for", "vpId", ",", "point", "in", "adj_dict", ".", "items", "(", ")", ":", "\n", "                            ", "absViewIndex2points", "[", "point", "[", "'absViewIndex'", "]", "]", ".", "append", "(", "vpId", ")", "\n", "", "graphs", "[", "scan", "]", "[", "viewpointId", "]", "=", "(", "adj_dict", ",", "absViewIndex2points", ")", "\n", "", "", "", "", "print", "(", "'prepare cache for'", ",", "split", ",", "'done'", ")", "\n", "", "with", "open", "(", "navigable_locs_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "graphs", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.current_best": [[352, 361], ["None"], "function", ["None"], ["", "", "def", "current_best", "(", "df", ",", "v_id", ",", "best_score_name", ")", ":", "\n", "    ", "if", "best_score_name", "==", "'sr_sum'", ":", "\n", "        ", "return", "df", "[", "'val_seen success_rate'", "]", "[", "v_id", "]", "+", "df", "[", "'val_unseen success_rate'", "]", "[", "v_id", "]", "\n", "", "elif", "best_score_name", "==", "'spl_sum'", ":", "\n", "        ", "return", "df", "[", "'val_seen spl'", "]", "[", "v_id", "]", "+", "df", "[", "'val_unseen spl'", "]", "[", "v_id", "]", "\n", "", "elif", "best_score_name", "==", "'spl_unseen'", ":", "\n", "        ", "return", "df", "[", "'val_unseen spl'", "]", "[", "v_id", "]", "\n", "", "elif", "best_score_name", "==", "'sr_unseen'", ":", "\n", "        ", "return", "df", "[", "'val_unseen success_rate'", "]", "[", "v_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.show_path_steps_len": [[363, 376], ["print", "plt.hist", "plt.title", "plt.show", "utils.load_datasets", "path_lens.extend", "print", "min", "max", "len", "len", "min", "max", "range", "min", "max"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets"], ["", "", "def", "show_path_steps_len", "(", "splits", ")", ":", "\n", "    ", "''' histogram of path length in the whole dataset '''", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "path_lens", "=", "[", "]", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "data", "=", "load_datasets", "(", "[", "split", "]", ",", "False", ")", "\n", "path_lens", ".", "extend", "(", "[", "len", "(", "item", "[", "'path'", "]", ")", "for", "item", "in", "data", "]", ")", "\n", "print", "(", "len", "(", "data", ")", ")", "\n", "", "print", "(", "'min steps'", ",", "min", "(", "path_lens", ")", ",", "'max steps'", ",", "max", "(", "path_lens", ")", ")", "\n", "plt", ".", "hist", "(", "path_lens", ",", "\n", "bins", "=", "[", "i", "for", "i", "in", "range", "(", "min", "(", "path_lens", ")", ",", "max", "(", "path_lens", ")", "+", "1", ")", "]", ")", "# arguments are passed to np.histogram", "\n", "plt", ".", "title", "(", "\"Histogram with '%d-%d' bins\"", "%", "(", "(", "min", "(", "path_lens", ")", ",", "max", "(", "path_lens", ")", ")", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.show_max_navigable": [[378, 390], ["print", "open", "json.load", "len", "len"], "function", ["None"], ["", "def", "show_max_navigable", "(", ")", ":", "\n", "    ", "navigable_locs_path", "=", "\"tasks/R2R/data/navigable_locs.json\"", "\n", "with", "open", "(", "navigable_locs_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "nav_graphs", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "max_navigable", "=", "0", "\n", "for", "scan", "in", "nav_graphs", ":", "\n", "        ", "for", "viewpointId", "in", "nav_graphs", "[", "scan", "]", ":", "\n", "            ", "adj_dict", ",", "absViewIndex2points", "=", "nav_graphs", "[", "scan", "]", "[", "viewpointId", "]", "\n", "if", "max_navigable", "<", "len", "(", "adj_dict", ")", ":", "\n", "                ", "max_navigable", "=", "len", "(", "adj_dict", ")", "\n", "", "", "", "print", "(", "max_navigable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.generate_multisent_to_dataset": [[392, 413], ["print", "utils.load_datasets", "max", "max", "enumerate", "open", "json.dump", "copy.deepcopy", "sent_tokenize", "new_data.append", "counter[].append", "counter[].append", "len", "max", "len"], "function", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.utils.load_datasets"], ["", "def", "generate_multisent_to_dataset", "(", ")", ":", "\n", "    ", "from", "nltk", ".", "tokenize", "import", "sent_tokenize", "\n", "import", "copy", "\n", "splits", "=", "[", "'train'", ",", "'val_seen'", ",", "'val_unseen'", ",", "'test'", "]", "\n", "\n", "counter", "=", "(", "[", "]", ",", "[", "]", ")", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "new_data", "=", "[", "]", "\n", "data", "=", "load_datasets", "(", "[", "split", "]", ",", "encoder_type", "=", "'lstm'", ")", "# here we use lstm dataset to preprocess the data,", "\n", "for", "item", "in", "data", ":", "\n", "            ", "for", "i", ",", "instr", "in", "enumerate", "(", "item", "[", "'instructions'", "]", ")", ":", "\n", "                ", "new_item", "=", "copy", ".", "deepcopy", "(", "item", ")", "\n", "sents", "=", "sent_tokenize", "(", "instr", ")", "\n", "new_item", "[", "'path_id'", "]", "=", "\"%s_%d\"", "%", "(", "item", "[", "'path_id'", "]", ",", "i", ")", "\n", "new_item", "[", "'instructions'", "]", "=", "sents", "\n", "new_data", ".", "append", "(", "new_item", ")", "\n", "counter", "[", "0", "]", ".", "append", "(", "len", "(", "sents", ")", ")", "\n", "counter", "[", "1", "]", ".", "append", "(", "max", "(", "[", "len", "(", "sent", ")", "for", "sent", "in", "sents", "]", ")", ")", "\n", "", "", "with", "open", "(", "\"tasks/R2R/data/R2R_%s_multisent.json\"", "%", "split", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "json", ".", "dump", "(", "new_data", ",", "fout", ",", "indent", "=", "2", ",", "separators", "=", "[", "','", ",", "':'", "]", ")", "\n", "", "", "print", "(", "max", "(", "counter", "[", "0", "]", ")", ",", "max", "(", "counter", "[", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.SingleQuery.__init__": [[68, 80], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "instr_id", ",", "scan", ",", "viewpoint", ",", "viewIndex", ",", "teacher_action", ",", "teacher_action_embedding", ",", "absViewIndex", ",", "rel_heading", ",", "rel_elevation", ",", "mode", ")", ":", "\n", "        ", "self", ".", "instr_id", "=", "instr_id", "\n", "self", ".", "scan", "=", "scan", "\n", "self", ".", "viewpoint", "=", "viewpoint", "\n", "self", ".", "viewIndex", "=", "viewIndex", "\n", "self", ".", "teacher_action", "=", "teacher_action", "\n", "self", ".", "teacher_action_embedding", "=", "teacher_action_embedding", "\n", "self", ".", "absViewIndex", "=", "absViewIndex", "\n", "self", ".", "rel_heading", "=", "rel_heading", "\n", "self", ".", "rel_elevation", "=", "rel_elevation", "\n", "self", ".", "next", "=", "None", "\n", "self", ".", "mode", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.SingleQuery_bnb.__init__": [[85, 89], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "instr_id", ",", "picid", ",", "mode", ")", ":", "\n", "        ", "self", ".", "instr_id", "=", "instr_id", "\n", "self", ".", "picid", "=", "picid", "\n", "self", ".", "mode", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__init__": [[164, 206], ["tok._convert_token_to_id", "feature.Feature", "dict", "dict", "feature.Feature", "feature.Feature_bnb", "open", "json.load", "open", "json.load", "open", "json.load", "batch_loader.NavDataset.disentangle_path_bnb", "open", "json.load", "batch_loader.NavDataset.disentangle_path"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.disentangle_path_bnb", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.disentangle_path"], ["    ", "def", "__init__", "(", "self", ",", "json_dirs", ",", "bnb_dir", ",", "tok", ",", "img_path", ",", "panoramic", ",", "args", ",", "img_path_bnb", ")", ":", "\n", "\n", "# read all json files and create a list of query data", "\n", "        ", "self", ".", "json_dirs", "=", "json_dirs", "#  a list of json files", "\n", "self", ".", "tok", "=", "tok", "# should be a lang, vision, action aware tokenizer ['VCLS', 'ACLS']", "\n", "self", ".", "mask_index", "=", "tok", ".", "_convert_token_to_id", "(", "tok", ".", "mask_token", ")", "\n", "self", ".", "feature_store", "=", "Feature", "(", "img_path", ",", "panoramic", ")", "\n", "if", "args", "[", "'prevalent_only'", "]", ":", "\n", "            ", "self", ".", "feature_store_bnb", "=", "Feature", "(", "img_path", ",", "panoramic", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "feature_store_bnb", "=", "Feature_bnb", "(", "img_path_bnb", ")", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "nag_dirs", "=", "'data/neg/candidate_sm.json'", "\n", "self", ".", "nag_trajs", "=", "[", "]", "\n", "self", ".", "nag_dirs_seq", "=", "'data/neg/candidate_sm_seq.json'", "\n", "with", "open", "(", "self", ".", "nag_dirs_seq", ")", "as", "f", ":", "\n", "            ", "nag_trajs_seq", "=", "json", ".", "load", "(", "f", ")", "\n", "", "self", ".", "nag_trajs_seq", "=", "nag_trajs_seq", "\n", "\n", "if", "args", "[", "'prevalent_only'", "]", ":", "\n", "            ", "self", ".", "nag_trajs_bnb", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "nag_dirs_bnb", "=", "'data/neg/bnb_neg.json'", "\n", "with", "open", "(", "self", ".", "nag_dirs_bnb", ")", "as", "f", ":", "\n", "                ", "nag_trajs_bnb", "=", "json", ".", "load", "(", "f", ")", "\n", "self", ".", "nag_trajs_bnb", "=", "nag_trajs_bnb", "\n", "\n", "", "", "self", ".", "data", "=", "[", "]", "\n", "self", ".", "instr_refer_bnb", "=", "dict", "(", ")", "# instr_id : instr_encoding", "\n", "if", "bnb_dir", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "bnb_dir", ")", "as", "f", ":", "\n", "                ", "current_trajs", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "traj", "in", "current_trajs", ":", "\n", "                    ", "self", ".", "data", "+=", "self", ".", "disentangle_path_bnb", "(", "traj", ")", "\n", "\n", "", "", "", "self", ".", "instr_refer", "=", "dict", "(", ")", "# instr_id : instr_encoding", "\n", "if", "self", ".", "json_dirs", "is", "not", "None", ":", "\n", "            ", "for", "json_dir", "in", "self", ".", "json_dirs", ":", "\n", "                ", "with", "open", "(", "json_dir", ")", "as", "f", ":", "\n", "                    ", "current_trajs", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "traj", "in", "current_trajs", ":", "\n", "                        ", "self", ".", "data", "+=", "self", ".", "disentangle_path", "(", "traj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__getitem__": [[208, 214], ["range", "batch_loader.NavDataset.getQuery", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "batch_loader.NavDataset.items"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.getQuery"], ["", "", "", "", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "# you must return data and label pair tensor", "\n", "        ", "query", "=", "self", ".", "data", "[", "index", "]", "\n", "index_list", "=", "range", "(", "len", "(", "self", ".", "data", ")", ")", "\n", "output", "=", "self", ".", "getQuery", "(", "query", ",", "self", ".", "nag_trajs", ",", "self", ".", "nag_trajs_seq", ",", "self", ".", "nag_trajs_bnb", ")", "\n", "return", "{", "key", ":", "torch", ".", "tensor", "(", "value", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "key", ",", "value", "in", "output", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.__len__": [[216, 218], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.disentangle_path": [[220, 254], ["list", "range", "batch_loader.SingleQuery", "list.append", "len", "scan.append", "viewpoint.append", "viewIndex.append", "absViewIndex.append", "rel_heading.append", "rel_elevation.append", "teacher_action.append", "teacher_action_embedding.append"], "methods", ["None"], ["", "def", "disentangle_path", "(", "self", ",", "traj", ")", ":", "\n", "        ", "mode", "=", "'r2r'", "\n", "query", "=", "list", "(", ")", "\n", "instr_id", "=", "traj", "[", "'instr_id'", "]", "\n", "instruction", "=", "traj", "[", "'instr_encoding'", "]", "\n", "self", ".", "instr_refer", "[", "instr_id", "]", "=", "instruction", "\n", "\n", "path", "=", "traj", "[", "'path'", "]", "\n", "actions", "=", "traj", "[", "'teacher_actions'", "]", "\n", "action_emds", "=", "traj", "[", "'teacher_action_emd'", "]", "\n", "\n", "scan", ",", "viewpoint", ",", "viewIndex", ",", "absViewIndex", ",", "rel_heading", ",", "rel_elevation", ",", "teacher_action", ",", "teacher_action_embedding", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "t", "in", "range", "(", "len", "(", "path", ")", ")", ":", "\n", "            ", "scan_now", "=", "path", "[", "t", "]", "[", "0", "]", "\n", "viewpoint_now", "=", "path", "[", "t", "]", "[", "1", "]", "\n", "viewIndex_now", "=", "path", "[", "t", "]", "[", "2", "]", "\n", "teacher_action_now", "=", "actions", "[", "t", "]", "\n", "teacher_action_emd_now", "=", "action_emds", "[", "t", "]", "\n", "absViewIndex_now", ",", "rel_heading_now", ",", "rel_elevation_now", "=", "action_emds", "[", "t", "]", "\n", "\n", "scan", ".", "append", "(", "scan_now", ")", "\n", "viewpoint", ".", "append", "(", "viewpoint_now", ")", "\n", "viewIndex", ".", "append", "(", "viewIndex_now", ")", "\n", "absViewIndex", ".", "append", "(", "absViewIndex_now", ")", "\n", "rel_heading", ".", "append", "(", "rel_heading_now", ")", "\n", "rel_elevation", ".", "append", "(", "rel_elevation_now", ")", "\n", "teacher_action", ".", "append", "(", "teacher_action_now", ")", "\n", "teacher_action_embedding", ".", "append", "(", "teacher_action_emd_now", ")", "\n", "\n", "\n", "", "current_query", "=", "SingleQuery", "(", "instr_id", ",", "scan", ",", "viewpoint", ",", "viewIndex", ",", "teacher_action", ",", "teacher_action_embedding", ",", "absViewIndex", ",", "rel_heading", ",", "rel_elevation", ",", "mode", ")", "\n", "query", ".", "append", "(", "current_query", ")", "# a list of (SASA)", "\n", "\n", "return", "query", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.disentangle_path_bnb": [[256, 269], ["list", "batch_loader.SingleQuery_bnb", "list.append"], "methods", ["None"], ["", "def", "disentangle_path_bnb", "(", "self", ",", "traj", ")", ":", "\n", "        ", "mode", "=", "'bnb'", "\n", "query_bnb", "=", "list", "(", ")", "\n", "instr_id_bnb", "=", "traj", "[", "'instr_id'", "]", "\n", "instruction_bnb", "=", "traj", "[", "'instr_encoding'", "]", "\n", "self", ".", "instr_refer_bnb", "[", "instr_id_bnb", "]", "=", "instruction_bnb", "\n", "\n", "picid", "=", "traj", "[", "'path'", "]", "\n", "\n", "current_query_bnb", "=", "SingleQuery_bnb", "(", "instr_id_bnb", ",", "picid", ",", "mode", ")", "\n", "query_bnb", ".", "append", "(", "current_query_bnb", ")", "# a list of (SASA)", "\n", "\n", "return", "query_bnb", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.getQuery": [[270, 671], ["dict", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "batch_loader.mask_tokens", "batch_loader.mask_tokens_nomlm", "numpy.zeros", "numpy.zeros", "range", "numpy.random.random", "len", "numpy.random.random", "numpy.random.random", "numpy.zeros", "numpy.zeros", "numpy.concatenate", "numpy.zeros", "numpy.concatenate", "len", "batch_loader.NavDataset.feature_store_bnb.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "batch_loader.mask_tokens", "batch_loader.mask_tokens_nomlm", "numpy.zeros", "numpy.zeros", "range", "numpy.random.random", "len", "numpy.random.random", "numpy.random.random", "numpy.random.choice", "numpy.zeros", "numpy.zeros", "batch_loader.NavDataset.feature_store.rollout", "numpy.concatenate", "numpy.concatenate", "numpy.zeros", "numpy.zeros", "list", "numpy.random.choice", "[].replace", "query.instr_id.replace().replace().replace().replace().replace().replace().replace().replace().replace", "range", "numpy.array", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.array", "list", "random.sample", "numpy.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.zeros", "numpy.zeros", "list", "numpy.random.choice", "[].replace", "query.instr_id.replace().replace().replace().replace().replace().replace().replace().replace().replace", "range", "numpy.zeros", "len", "batch_loader.NavDataset.feature_store.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "numpy.random.choice", "range", "range", "numpy.random.choice", "[].replace", "len", "batch_loader.NavDataset.feature_store_bnb.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "range", "numpy.array", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.array", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "numpy.array", "list", "random.sample", "numpy.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.array", "list", "random.sample", "numpy.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "numpy.random.choice", "[].replace", "batch_loader.NavDataset.feature_store_bnb.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "numpy.zeros", "numpy.zeros", "list", "numpy.random.choice", "range", "random.sample", "numpy.zeros", "range", "numpy.array", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.array", "list", "random.sample", "numpy.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "list", "numpy.random.choice", "range", "random.sample", "range", "len", "batch_loader.NavDataset.feature_store.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "numpy.zeros", "len", "query.instr_id.replace().replace().replace().replace().replace().replace().replace().replace", "range", "range", "range", "range", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "query.instr_id.replace().replace().replace().replace().replace().replace().replace().replace", "range", "numpy.random.choice", "len", "len", "batch_loader.NavDataset.feature_store.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "range", "numpy.array", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.array", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "numpy.array", "list", "random.sample", "numpy.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.array", "list", "random.sample", "numpy.zeros", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.zeros", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.zeros", "range", "numpy.random.choice", "len", "batch_loader.NavDataset.feature_store.rollout", "batch_loader.build_viewpoint_loc_embedding_1", "numpy.concatenate", "len", "numpy.zeros", "len", "len", "numpy.random.choice", "range", "range", "range", "range", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "len", "numpy.random.choice", "query.instr_id.replace().replace().replace().replace().replace().replace().replace", "query.instr_id.replace().replace().replace().replace().replace().replace().replace", "len", "query.instr_id.replace().replace().replace().replace().replace().replace", "query.instr_id.replace().replace().replace().replace().replace().replace", "query.instr_id.replace().replace().replace().replace().replace", "query.instr_id.replace().replace().replace().replace().replace", "query.instr_id.replace().replace().replace().replace", "query.instr_id.replace().replace().replace().replace", "query.instr_id.replace().replace().replace", "query.instr_id.replace().replace().replace", "query.instr_id.replace().replace", "query.instr_id.replace().replace", "query.instr_id.replace", "query.instr_id.replace"], "methods", ["home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.mask_tokens", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.mask_tokens_nomlm", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.mask_tokens", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.mask_tokens_nomlm", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1", "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1"], ["", "def", "getQuery", "(", "self", ",", "query", ",", "nag_trajs", ",", "nag_trajs_seq", ",", "nag_trajs_bnb", ")", ":", "\n", "# prepare text tensor", "\n", "        ", "output", "=", "dict", "(", ")", "\n", "if", "query", ".", "mode", "==", "'bnb'", ":", "\n", "            ", "text_seq", "=", "torch", ".", "LongTensor", "(", "self", ".", "instr_refer_bnb", "[", "query", ".", "instr_id", "]", ")", "\n", "masked_text_seq", ",", "masked_text_label", ",", "attention_mask", "=", "mask_tokens", "(", "text_seq", ",", "self", ".", "tok", ",", "self", ".", "args", ")", "\n", "output", "[", "'masked_text_seq'", "]", "=", "masked_text_seq", "\n", "output", "[", "'masked_text_label'", "]", "=", "masked_text_label", "\n", "output", "[", "'lang_attention_mask'", "]", "=", "attention_mask", "\n", "\n", "\n", "nomasked_text_seq", ",", "nomasked_text_label", "=", "mask_tokens_nomlm", "(", "text_seq", ",", "self", ".", "tok", ",", "self", ".", "args", ")", "\n", "output", "[", "'text_seq'", "]", "=", "nomasked_text_seq", "\n", "output", "[", "'text_label'", "]", "=", "nomasked_text_label", "\n", "\n", "# prepare vision tensor", "\n", "picid", "=", "query", ".", "picid", "\n", "rel_heading", ",", "rel_elevation", "=", "0", ",", "0", "\n", "\n", "feature_single_bnb", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "img_mask_bnb", "=", "np", ".", "zeros", "(", "7", ")", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "picid", ")", ")", ":", "\n", "                ", "feature_1_bnb", "=", "self", ".", "feature_store_bnb", ".", "rollout", "(", "picid", "[", "t", "]", ")", "\n", "feature_angle_bnb", "=", "build_viewpoint_loc_embedding_1", "(", "0", ",", "0", ")", "\n", "feature_single_bnb", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "feature_1_bnb", ",", "feature_angle_bnb", ")", ",", "axis", "=", "-", "1", ")", "\n", "img_mask_bnb", "[", "t", "]", "=", "1", "\n", "\n", "", "output", "[", "'feature_single'", "]", "=", "feature_single_bnb", "#10, 2048", "\n", "output", "[", "'img_mask'", "]", "=", "img_mask_bnb", "\n", "\n", "# prepare itm vision tensor", "\n", "prob_itm", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "prob_itm", "=", "0.1", "\n", "if", "prob_itm", "<=", "0.5", ":", "\n", "                ", "output", "[", "'ismatch'", "]", "=", "1", "\n", "output", "[", "'feature_single_itm'", "]", "=", "output", "[", "'feature_single'", "]", "\n", "output", "[", "'img_mask_itm'", "]", "=", "img_mask_bnb", "\n", "", "elif", "prob_itm", "<=", "1", ":", "\n", "                ", "output", "[", "'ismatch'", "]", "=", "0", "\n", "fake_feature_single_bnb", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "fake_img_mask_bnb", "=", "np", ".", "zeros", "(", "7", ")", "\n", "#fake_feature_single_bnb = np.zeros((7,2176)) + feature_single_bnb", "\n", "can_room", "=", "list", "(", "range", "(", "len", "(", "nag_trajs_bnb", ")", ")", ")", "\n", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_room", ")", "\n", "fake_room", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'instr_id'", "]", ".", "replace", "(", "\"_0\"", ",", "\"\"", ")", "\n", "fake_picid", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'path'", "]", "\n", "true_room", "=", "query", ".", "instr_id", ".", "replace", "(", "\"_0\"", ",", "\"\"", ")", ".", "replace", "(", "\"_1\"", ",", "\"\"", ")", ".", "replace", "(", "\"_2\"", ",", "\"\"", ")", ".", "replace", "(", "\"_3\"", ",", "\"\"", ")", ".", "replace", "(", "\"_4\"", ",", "\"\"", ")", ".", "replace", "(", "\"_5\"", ",", "\"\"", ")", ".", "replace", "(", "\"_6\"", ",", "\"\"", ")", ".", "replace", "(", "\"_7\"", ",", "\"\"", ")", ".", "replace", "(", "\"_8\"", ",", "\"\"", ")", "\n", "while", "fake_room", "==", "true_room", ":", "\n", "                    ", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_room", ")", "\n", "fake_room", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'instr_id'", "]", ".", "replace", "(", "\"_0\"", ",", "\"\"", ")", "\n", "fake_picid", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'path'", "]", "\n", "\n", "", "for", "t", "in", "range", "(", "len", "(", "fake_picid", ")", ")", ":", "\n", "                    ", "fake_feature_1_bnb", "=", "self", ".", "feature_store_bnb", ".", "rollout", "(", "fake_picid", "[", "t", "]", ")", "\n", "fake_feature_angle_bnb", "=", "build_viewpoint_loc_embedding_1", "(", "0", ",", "0", ")", "\n", "fake_feature_single_bnb", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "fake_feature_1_bnb", ",", "fake_feature_angle_bnb", ")", ",", "axis", "=", "-", "1", ")", "\n", "fake_img_mask_bnb", "[", "t", "]", "=", "1", "\n", "", "output", "[", "'feature_single_itm'", "]", "=", "fake_feature_single_bnb", "#10, 2048", "\n", "output", "[", "'img_mask_itm'", "]", "=", "fake_img_mask_bnb", "\n", "\n", "# prepare order vision random", "\n", "", "len_img", "=", "len", "(", "picid", ")", "\n", "prob_order", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "prob_order", "=", "0.1", "\n", "if", "prob_order", "<", "0.3", ":", "\n", "                ", "output", "[", "'feature_single_order'", "]", "=", "feature_single_bnb", "\n", "if", "len_img", "==", "5", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "-", "1", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "elif", "len_img", "==", "6", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "else", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "", "else", ":", "\n", "                ", "num_change", "=", "3", "\n", "if", "len_img", "==", "5", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "-", "1", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "random_order", "=", "random", ".", "sample", "(", "list_order", ",", "num_change", ")", "\n", "aaa", "=", "random_order", "[", "0", "]", "\n", "bbb", "=", "random_order", "[", "1", "]", "\n", "ccc", "=", "random_order", "[", "2", "]", "\n", "list_order", "[", "aaa", "]", "=", "bbb", "\n", "list_order", "[", "bbb", "]", "=", "ccc", "\n", "list_order", "[", "ccc", "]", "=", "aaa", "\n", "feature_init", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "shuffle_feature_single", "=", "feature_single_bnb", "[", "list_order", "]", "\n", "feature_init", "[", "0", ":", "len_img", "]", "=", "shuffle_feature_single", "\n", "output", "[", "'feature_single_order'", "]", "=", "feature_init", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "elif", "len_img", "==", "6", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "random_order", "=", "random", ".", "sample", "(", "list_order", ",", "num_change", ")", "\n", "aaa", "=", "random_order", "[", "0", "]", "\n", "bbb", "=", "random_order", "[", "1", "]", "\n", "ccc", "=", "random_order", "[", "2", "]", "\n", "list_order", "[", "aaa", "]", "=", "bbb", "\n", "list_order", "[", "bbb", "]", "=", "ccc", "\n", "list_order", "[", "ccc", "]", "=", "aaa", "\n", "feature_init", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "shuffle_feature_single", "=", "feature_single_bnb", "[", "list_order", "]", "\n", "feature_init", "[", "0", ":", "len_img", "]", "=", "shuffle_feature_single", "\n", "output", "[", "'feature_single_order'", "]", "=", "feature_init", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "else", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "random_order", "=", "random", ".", "sample", "(", "list_order", ",", "num_change", ")", "\n", "aaa", "=", "random_order", "[", "0", "]", "\n", "bbb", "=", "random_order", "[", "1", "]", "\n", "ccc", "=", "random_order", "[", "2", "]", "\n", "list_order", "[", "aaa", "]", "=", "bbb", "\n", "list_order", "[", "bbb", "]", "=", "ccc", "\n", "list_order", "[", "ccc", "]", "=", "aaa", "\n", "feature_init", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "shuffle_feature_single", "=", "feature_single_bnb", "[", "list_order", "]", "\n", "feature_init", "[", "0", ":", "len_img", "]", "=", "shuffle_feature_single", "\n", "output", "[", "'feature_single_order'", "]", "=", "feature_init", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "\n", "# prepare 3-class vision random", "\n", "", "", "prob_class3", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "prob_class3", "=", "0.1", "\n", "\n", "if", "prob_class3", "<=", "0.34", ":", "\n", "                ", "output", "[", "'isclass'", "]", "=", "1", "\n", "output", "[", "'feature_single_class'", "]", "=", "output", "[", "'feature_single'", "]", "\n", "", "elif", "prob_class3", "<=", "0.67", ":", "\n", "                ", "output", "[", "'isclass'", "]", "=", "2", "\n", "fake_feature_single_class", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "if", "len", "(", "picid", ")", "==", "5", ":", "\n", "                    ", "list_order_all", "=", "torch", ".", "LongTensor", "(", "[", "3", ",", "4", ",", "0", ",", "1", ",", "2", ",", "5", ",", "6", "]", ")", "\n", "", "elif", "len", "(", "picid", ")", "==", "6", ":", "\n", "                    ", "list_order_all", "=", "torch", ".", "LongTensor", "(", "[", "3", ",", "4", ",", "5", ",", "0", ",", "1", ",", "2", ",", "6", "]", ")", "\n", "", "else", ":", "\n", "                    ", "list_order_all", "=", "torch", ".", "LongTensor", "(", "[", "3", ",", "4", ",", "5", ",", "6", ",", "0", ",", "1", ",", "2", "]", ")", "\n", "", "fake_feature_single_class", "=", "feature_single_bnb", "[", "list_order_all", "]", "\n", "output", "[", "'feature_single_class'", "]", "=", "fake_feature_single_class", "\n", "", "else", ":", "\n", "                ", "output", "[", "'isclass'", "]", "=", "0", "\n", "fake_feature_single_class", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "can_room", "=", "list", "(", "range", "(", "len", "(", "nag_trajs_bnb", ")", ")", ")", "\n", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_room", ")", "\n", "fake_room", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'instr_id'", "]", ".", "replace", "(", "\"_0\"", ",", "\"\"", ")", "\n", "fake_picid", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'path'", "]", "\n", "true_room", "=", "query", ".", "instr_id", ".", "replace", "(", "\"_0\"", ",", "\"\"", ")", ".", "replace", "(", "\"_1\"", ",", "\"\"", ")", ".", "replace", "(", "\"_2\"", ",", "\"\"", ")", ".", "replace", "(", "\"_3\"", ",", "\"\"", ")", ".", "replace", "(", "\"_4\"", ",", "\"\"", ")", ".", "replace", "(", "\"_5\"", ",", "\"\"", ")", ".", "replace", "(", "\"_6\"", ",", "\"\"", ")", ".", "replace", "(", "\"_7\"", ",", "\"\"", ")", ".", "replace", "(", "\"_8\"", ",", "\"\"", ")", "\n", "while", "fake_room", "==", "true_room", ":", "\n", "                    ", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_room", ")", "\n", "fake_room", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'instr_id'", "]", ".", "replace", "(", "\"_0\"", ",", "\"\"", ")", "\n", "fake_picid", "=", "nag_trajs_bnb", "[", "fake_index", "]", "[", "'path'", "]", "\n", "\n", "", "for", "t", "in", "range", "(", "3", ")", ":", "\n", "                    ", "fake_feature_1_class", "=", "self", ".", "feature_store_bnb", ".", "rollout", "(", "fake_picid", "[", "t", "]", ")", "\n", "fake_feature_angle_class", "=", "build_viewpoint_loc_embedding_1", "(", "0", ",", "0", ")", "\n", "fake_feature_single_class", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "fake_feature_1_class", ",", "fake_feature_angle_class", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "output", "[", "'feature_single_class'", "]", "=", "fake_feature_single_class", "\n", "\n", "#action", "\n", "", "feature_history", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "feature_36", "=", "np", ".", "zeros", "(", "(", "36", ",", "2048", ")", ")", "\n", "feature_36_with_loc_all", "=", "np", ".", "concatenate", "(", "(", "feature_36", ",", "_static_loc_embeddings", "[", "0", "]", ")", ",", "axis", "=", "-", "1", ")", "\n", "img_mask_his", "=", "np", ".", "zeros", "(", "7", ")", "\n", "\n", "output", "[", "'teacher_embedding'", "]", "=", "-", "1", "\n", "output", "[", "'feature_his'", "]", "=", "feature_history", "\n", "output", "[", "'feature_36'", "]", "=", "feature_36_with_loc_all", "\n", "output", "[", "'img_mask_his_36'", "]", "=", "np", ".", "concatenate", "(", "(", "img_mask_his", ",", "np", ".", "zeros", "(", "1", ")", ",", "np", ".", "zeros", "(", "36", ")", "+", "1", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "elif", "query", ".", "mode", "==", "'r2r'", ":", "\n", "            ", "text_seq", "=", "torch", ".", "LongTensor", "(", "self", ".", "instr_refer", "[", "query", ".", "instr_id", "]", ")", "\n", "masked_text_seq", ",", "masked_text_label", ",", "attention_mask", "=", "mask_tokens", "(", "text_seq", ",", "self", ".", "tok", ",", "self", ".", "args", ")", "\n", "output", "[", "'masked_text_seq'", "]", "=", "masked_text_seq", "\n", "output", "[", "'masked_text_label'", "]", "=", "masked_text_label", "\n", "output", "[", "'lang_attention_mask'", "]", "=", "attention_mask", "\n", "\n", "nomasked_text_seq", ",", "nomasked_text_label", "=", "mask_tokens_nomlm", "(", "text_seq", ",", "self", ".", "tok", ",", "self", ".", "args", ")", "\n", "output", "[", "'text_seq'", "]", "=", "nomasked_text_seq", "\n", "output", "[", "'text_label'", "]", "=", "nomasked_text_label", "\n", "\n", "# prepare vision tensor", "\n", "scan", ",", "viewpoint", ",", "viewindex", "=", "query", ".", "scan", ",", "query", ".", "viewpoint", ",", "query", ".", "viewIndex", "\n", "absViewIndex", ",", "rel_heading", ",", "rel_elevation", "=", "query", ".", "absViewIndex", ",", "query", ".", "rel_heading", ",", "query", ".", "rel_elevation", "\n", "\n", "feature_single", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "img_mask", "=", "np", ".", "zeros", "(", "7", ")", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "scan", ")", ")", ":", "\n", "                ", "feature_1", ",", "feature_all", "=", "self", ".", "feature_store", ".", "rollout", "(", "scan", "[", "t", "]", ",", "viewpoint", "[", "t", "]", ",", "viewindex", "[", "t", "]", ")", "\n", "feature_angle", "=", "build_viewpoint_loc_embedding_1", "(", "viewindex", "[", "t", "]", ",", "absViewIndex", "[", "t", "]", ")", "\n", "feature_single", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "feature_1", ",", "feature_angle", ")", ",", "axis", "=", "-", "1", ")", "\n", "img_mask", "[", "t", "]", "=", "1", "\n", "", "output", "[", "'feature_single'", "]", "=", "feature_single", "\n", "output", "[", "'img_mask'", "]", "=", "img_mask", "\n", "\n", "# prepare itm vision tensor", "\n", "prob_itm", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "if", "prob_itm", "<=", "0.5", ":", "\n", "                ", "output", "[", "'ismatch'", "]", "=", "1", "\n", "output", "[", "'feature_single_itm'", "]", "=", "output", "[", "'feature_single'", "]", "\n", "output", "[", "'img_mask_itm'", "]", "=", "img_mask", "\n", "", "elif", "prob_itm", "<=", "1", ":", "\n", "                ", "output", "[", "'ismatch'", "]", "=", "0", "\n", "fake_feature_single_itm", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "can_scan", "=", "list", "(", "range", "(", "len", "(", "nag_trajs_seq", ")", ")", ")", "\n", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_scan", ")", "\n", "fake_scan", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'scan'", "]", "\n", "\n", "while", "fake_scan", "==", "scan", "[", "0", "]", ":", "\n", "                    ", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_scan", ")", "\n", "fake_scan", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'scan'", "]", "\n", "if", "len", "(", "nag_trajs_seq", "[", "fake_index", "]", "[", "'path'", "]", ")", "==", "0", ":", "\n", "                        ", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_scan", ")", "\n", "fake_scan", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'scan'", "]", "\n", "\n", "", "", "list_path", "=", "range", "(", "len", "(", "nag_trajs_seq", "[", "fake_index", "]", "[", "'path'", "]", ")", ")", "\n", "random_path", "=", "random", ".", "sample", "(", "list_path", ",", "1", ")", "\n", "fake_path", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'path'", "]", "[", "random_path", "[", "0", "]", "]", "\n", "\n", "fake_img_mask", "=", "np", ".", "zeros", "(", "7", ")", "\n", "for", "t", "in", "range", "(", "len", "(", "fake_path", ")", ")", ":", "\n", "                    ", "if", "t", "==", "len", "(", "fake_path", ")", "-", "1", ":", "\n", "                        ", "fake_absviewindex_itm", "=", "-", "1", "\n", "", "else", ":", "\n", "                        ", "fake_absviewindex_itm", "=", "fake_path", "[", "t", "+", "1", "]", "[", "2", "]", "\n", "\n", "", "fake_viewpoint_itm", "=", "fake_path", "[", "t", "]", "[", "1", "]", "\n", "fake_viewindex_itm", "=", "fake_path", "[", "t", "]", "[", "2", "]", "\n", "fake_feature_1_itm", ",", "fake_feature_1_itm_all", "=", "self", ".", "feature_store", ".", "rollout", "(", "fake_scan", ",", "fake_viewpoint_itm", ",", "fake_viewindex_itm", ")", "\n", "feature_angle_itm", "=", "build_viewpoint_loc_embedding_1", "(", "fake_viewindex_itm", ",", "fake_absviewindex_itm", ")", "\n", "fake_feature_single_itm", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "fake_feature_1_itm", ",", "feature_angle_itm", ")", ",", "axis", "=", "-", "1", ")", "\n", "fake_img_mask", "[", "t", "]", "=", "1", "\n", "output", "[", "'feature_single_itm'", "]", "=", "fake_feature_single_itm", "\n", "output", "[", "'img_mask_itm'", "]", "=", "fake_img_mask", "\n", "\n", "# prepare order vision random", "\n", "", "", "len_img", "=", "len", "(", "scan", ")", "\n", "prob_order", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "\n", "if", "prob_order", "<", "0.3", ":", "\n", "                ", "output", "[", "'feature_single_order'", "]", "=", "feature_single", "\n", "if", "len_img", "==", "5", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "-", "1", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "elif", "len_img", "==", "6", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "else", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "", "else", ":", "\n", "                ", "num_change", "=", "3", "\n", "if", "len_img", "==", "5", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "-", "1", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "random_order", "=", "random", ".", "sample", "(", "list_order", ",", "num_change", ")", "\n", "aaa", "=", "random_order", "[", "0", "]", "\n", "bbb", "=", "random_order", "[", "1", "]", "\n", "ccc", "=", "random_order", "[", "2", "]", "\n", "list_order", "[", "aaa", "]", "=", "bbb", "\n", "list_order", "[", "bbb", "]", "=", "ccc", "\n", "list_order", "[", "ccc", "]", "=", "aaa", "\n", "feature_init", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "shuffle_feature_single", "=", "feature_single", "[", "list_order", "]", "\n", "feature_init", "[", "0", ":", "len_img", "]", "=", "shuffle_feature_single", "\n", "output", "[", "'feature_single_order'", "]", "=", "feature_init", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "elif", "len_img", "==", "6", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "-", "1", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "random_order", "=", "random", ".", "sample", "(", "list_order", ",", "num_change", ")", "\n", "aaa", "=", "random_order", "[", "0", "]", "\n", "bbb", "=", "random_order", "[", "1", "]", "\n", "ccc", "=", "random_order", "[", "2", "]", "\n", "list_order", "[", "aaa", "]", "=", "bbb", "\n", "list_order", "[", "bbb", "]", "=", "ccc", "\n", "list_order", "[", "ccc", "]", "=", "aaa", "\n", "feature_init", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "shuffle_feature_single", "=", "feature_single", "[", "list_order", "]", "\n", "feature_init", "[", "0", ":", "len_img", "]", "=", "shuffle_feature_single", "\n", "output", "[", "'feature_single_order'", "]", "=", "feature_init", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "", "else", ":", "\n", "                    ", "list_order_all", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ")", "\n", "list_order", "=", "list", "(", "range", "(", "len_img", ")", ")", "\n", "random_order", "=", "random", ".", "sample", "(", "list_order", ",", "num_change", ")", "\n", "aaa", "=", "random_order", "[", "0", "]", "\n", "bbb", "=", "random_order", "[", "1", "]", "\n", "ccc", "=", "random_order", "[", "2", "]", "\n", "list_order", "[", "aaa", "]", "=", "bbb", "\n", "list_order", "[", "bbb", "]", "=", "ccc", "\n", "list_order", "[", "ccc", "]", "=", "aaa", "\n", "feature_init", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "shuffle_feature_single", "=", "feature_single", "[", "list_order", "]", "\n", "feature_init", "[", "0", ":", "len_img", "]", "=", "shuffle_feature_single", "\n", "output", "[", "'feature_single_order'", "]", "=", "feature_init", "\n", "list_order_all", "[", "0", ":", "len_img", "]", "=", "list_order", "\n", "list_order_all", "=", "torch", ".", "LongTensor", "(", "list_order_all", ")", "\n", "output", "[", "'order'", "]", "=", "list_order_all", "\n", "\n", "# prepare 3-class vision random", "\n", "", "", "prob_class3", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "\n", "if", "prob_class3", "<=", "0.34", ":", "\n", "                ", "output", "[", "'isclass'", "]", "=", "1", "\n", "output", "[", "'feature_single_class'", "]", "=", "output", "[", "'feature_single'", "]", "\n", "", "elif", "prob_class3", "<=", "0.67", ":", "\n", "                ", "output", "[", "'isclass'", "]", "=", "2", "\n", "fake_feature_single_class", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "+", "feature_single", "\n", "if", "len", "(", "scan", ")", "==", "5", ":", "\n", "                    ", "list_order_all", "=", "torch", ".", "LongTensor", "(", "[", "3", ",", "4", ",", "0", ",", "1", ",", "2", ",", "5", ",", "6", "]", ")", "\n", "", "elif", "len", "(", "scan", ")", "==", "6", ":", "\n", "                    ", "list_order_all", "=", "torch", ".", "LongTensor", "(", "[", "3", ",", "4", ",", "5", ",", "0", ",", "1", ",", "2", ",", "6", "]", ")", "\n", "", "else", ":", "\n", "                    ", "list_order_all", "=", "torch", ".", "LongTensor", "(", "[", "3", ",", "4", ",", "5", ",", "6", ",", "0", ",", "1", ",", "2", "]", ")", "\n", "", "fake_feature_single_class", "=", "feature_single", "[", "list_order_all", "]", "\n", "output", "[", "'feature_single_class'", "]", "=", "fake_feature_single_class", "\n", "", "else", ":", "\n", "                ", "output", "[", "'isclass'", "]", "=", "0", "\n", "fake_feature_single_class", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "+", "feature_single", "\n", "can_scan", "=", "list", "(", "range", "(", "len", "(", "nag_trajs_seq", ")", ")", ")", "\n", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_scan", ")", "\n", "fake_scan", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'scan'", "]", "\n", "\n", "while", "fake_scan", "==", "scan", "[", "0", "]", ":", "\n", "                    ", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_scan", ")", "\n", "fake_scan", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'scan'", "]", "\n", "if", "len", "(", "nag_trajs_seq", "[", "fake_index", "]", "[", "'path'", "]", ")", "==", "0", ":", "\n", "                        ", "fake_index", "=", "np", ".", "random", ".", "choice", "(", "can_scan", ")", "\n", "fake_scan", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'scan'", "]", "\n", "\n", "", "", "list_path", "=", "range", "(", "len", "(", "nag_trajs_seq", "[", "fake_index", "]", "[", "'path'", "]", ")", ")", "\n", "random_path", "=", "random", ".", "sample", "(", "list_path", ",", "1", ")", "\n", "fake_path", "=", "nag_trajs_seq", "[", "fake_index", "]", "[", "'path'", "]", "[", "random_path", "[", "0", "]", "]", "\n", "\n", "for", "t", "in", "range", "(", "3", ")", ":", "\n", "                    ", "fake_viewpoint", "=", "fake_path", "[", "t", "]", "[", "1", "]", "\n", "fake_viewindex", "=", "fake_path", "[", "t", "]", "[", "2", "]", "\n", "fake_absviewindex", "=", "fake_path", "[", "t", "+", "1", "]", "[", "2", "]", "\n", "fake_feature_1", ",", "fake_feature_1_all", "=", "self", ".", "feature_store", ".", "rollout", "(", "fake_scan", ",", "fake_viewpoint", ",", "fake_viewindex", ")", "\n", "fake_feature_angle", "=", "build_viewpoint_loc_embedding_1", "(", "fake_viewindex", ",", "fake_absviewindex", ")", "\n", "fake_feature_single_class", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "fake_feature_1", ",", "fake_feature_angle", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "output", "[", "'feature_single_class'", "]", "=", "fake_feature_single_class", "\n", "\n", "# prepare action", "\n", "", "select_step", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "scan", ")", "-", "2", ")", "\n", "if", "select_step", "==", "0", ":", "\n", "        \t    ", "select_step", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "scan", ")", "-", "2", ")", "\n", "\n", "", "feature_history", "=", "np", ".", "zeros", "(", "(", "7", ",", "2176", ")", ")", "\n", "img_mask_his", "=", "np", ".", "zeros", "(", "7", ")", "\n", "\n", "if", "select_step", ">", "0", ":", "\n", "        \t    ", "for", "t", "in", "range", "(", "select_step", ")", ":", "\n", "            \t\t", "feature_history_1", ",", "feature_all", "=", "self", ".", "feature_store", ".", "rollout", "(", "scan", "[", "t", "]", ",", "viewpoint", "[", "t", "]", ",", "viewindex", "[", "t", "]", ")", "\n", "feature_history_angle", "=", "build_viewpoint_loc_embedding_1", "(", "viewindex", "[", "t", "]", ",", "absViewIndex", "[", "t", "]", ")", "\n", "feature_history", "[", "t", "]", "=", "np", ".", "concatenate", "(", "(", "feature_history_1", ",", "feature_angle", ")", ",", "axis", "=", "-", "1", ")", "\n", "img_mask_his", "[", "t", "]", "=", "1", "\n", "", "", "feature_36_1", ",", "feature_36", "=", "self", ".", "feature_store", ".", "rollout", "(", "scan", "[", "select_step", "+", "1", "]", ",", "viewpoint", "[", "select_step", "+", "1", "]", ",", "viewindex", "[", "select_step", "+", "1", "]", ")", "\n", "feature_36_with_loc_all", "=", "np", ".", "concatenate", "(", "(", "feature_36", ",", "_static_loc_embeddings", "[", "viewindex", "[", "select_step", "+", "1", "]", "]", ")", ",", "axis", "=", "-", "1", ")", "\n", "output", "[", "'teacher_embedding'", "]", "=", "query", ".", "teacher_action_embedding", "[", "select_step", "+", "1", "]", "[", "0", "]", "\n", "output", "[", "'feature_his'", "]", "=", "feature_history", "\n", "output", "[", "'feature_36'", "]", "=", "feature_36_with_loc_all", "\n", "\n", "output", "[", "'img_mask_his_36'", "]", "=", "np", ".", "concatenate", "(", "(", "img_mask_his", ",", "np", ".", "zeros", "(", "1", ")", ",", "np", ".", "zeros", "(", "36", ")", "+", "1", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.NavDataset.random_word": [[675, 706], ["text_seq.copy", "enumerate", "numpy.random.random", "output_label.append", "output_label.append", "output_label.append", "len", "random.randrange", "len"], "methods", ["None"], ["", "def", "random_word", "(", "self", ",", "text_seq", ")", ":", "\n", "        ", "tokens", "=", "text_seq", ".", "copy", "(", ")", "# already be [cls t1 t2 sep]", "\n", "output_label", "=", "[", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "i", "==", "0", "or", "i", "==", "len", "(", "tokens", ")", "-", "1", ":", "\n", "                ", "output_label", ".", "append", "(", "0", ")", "\n", "continue", "\n", "", "prob", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "if", "prob", "<", "0.15", ":", "\n", "                ", "prob", "/=", "0.15", "\n", "\n", "output_label", ".", "append", "(", "tokens", "[", "i", "]", ")", "\n", "\n", "# 80% randomly change token to mask token", "\n", "if", "prob", "<", "0.8", ":", "\n", "                    ", "tokens", "[", "i", "]", "=", "self", ".", "mask_index", "\n", "\n", "# 10% randomly change token to random token", "\n", "", "elif", "prob", "<", "0.9", ":", "\n", "                    ", "tokens", "[", "i", "]", "=", "random", ".", "randrange", "(", "len", "(", "self", ".", "tok", ")", ")", "\n", "\n", "# 10% randomly change token to current token", "\n", "", "else", ":", "\n", "                    ", "tokens", "[", "i", "]", "=", "tokens", "[", "i", "]", "# just keep it", "\n", "\n", "", "", "else", ":", "\n", "                ", "tokens", "[", "i", "]", "=", "tokens", "[", "i", "]", "# just keep it", "\n", "output_label", ".", "append", "(", "0", ")", "\n", "\n", "", "", "return", "tokens", ",", "output_label", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding": [[29, 48], ["numpy.zeros", "range", "numpy.sin", "numpy.cos", "numpy.sin", "numpy.cos"], "function", ["None"], ["def", "build_viewpoint_loc_embedding", "(", "viewIndex", ")", ":", "\n", "    ", "\"\"\"\n    Position embedding:\n    heading 64D + elevation 64D\n    1) heading: [sin(heading) for _ in range(1, 33)] +\n                [cos(heading) for _ in range(1, 33)]\n    2) elevation: [sin(elevation) for _ in range(1, 33)] +\n                  [cos(elevation) for _ in range(1, 33)]\n    \"\"\"", "\n", "embedding", "=", "np", ".", "zeros", "(", "(", "36", ",", "128", ")", ",", "np", ".", "float32", ")", "\n", "for", "absViewIndex", "in", "range", "(", "36", ")", ":", "\n", "        ", "relViewIndex", "=", "(", "absViewIndex", "-", "viewIndex", ")", "%", "12", "+", "(", "absViewIndex", "//", "12", ")", "*", "12", "\n", "rel_heading", "=", "(", "relViewIndex", "%", "12", ")", "*", "angle_inc", "\n", "rel_elevation", "=", "(", "relViewIndex", "//", "12", "-", "1", ")", "*", "angle_inc", "\n", "embedding", "[", "absViewIndex", ",", "0", ":", "32", "]", "=", "np", ".", "sin", "(", "rel_heading", ")", "\n", "embedding", "[", "absViewIndex", ",", "32", ":", "64", "]", "=", "np", ".", "cos", "(", "rel_heading", ")", "\n", "embedding", "[", "absViewIndex", ",", "64", ":", "96", "]", "=", "np", ".", "sin", "(", "rel_elevation", ")", "\n", "embedding", "[", "absViewIndex", ",", "96", ":", "]", "=", "np", ".", "cos", "(", "rel_elevation", ")", "\n", "", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.build_viewpoint_loc_embedding_1": [[53, 63], ["numpy.zeros", "numpy.sin", "numpy.cos", "numpy.sin", "numpy.cos"], "function", ["None"], ["def", "build_viewpoint_loc_embedding_1", "(", "viewIndex", ",", "absViewIndex", ")", ":", "\n", "    ", "embedding", "=", "np", ".", "zeros", "(", "(", "128", ")", ",", "np", ".", "float32", ")", "\n", "relViewIndex", "=", "(", "absViewIndex", "-", "viewIndex", ")", "%", "12", "+", "(", "absViewIndex", "//", "12", ")", "*", "12", "\n", "rel_heading", "=", "(", "relViewIndex", "%", "12", ")", "*", "angle_inc", "\n", "rel_elevation", "=", "(", "relViewIndex", "//", "12", "-", "1", ")", "*", "angle_inc", "\n", "embedding", "[", "0", ":", "32", "]", "=", "np", ".", "sin", "(", "rel_heading", ")", "\n", "embedding", "[", "32", ":", "64", "]", "=", "np", ".", "cos", "(", "rel_heading", ")", "\n", "embedding", "[", "64", ":", "96", "]", "=", "np", ".", "sin", "(", "rel_elevation", ")", "\n", "embedding", "[", "96", ":", "]", "=", "np", ".", "cos", "(", "rel_elevation", ")", "\n", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.new_mask_tokens": [[91, 112], ["inputs.clone", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "tokenizer.convert_tokens_to_ids", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "tokenizer.get_special_tokens_mask", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "len", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full", "torch.full", "torch.full", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full", "torch.full", "torch.full"], "function", ["None"], ["", "", "def", "new_mask_tokens", "(", "inputs", ",", "tokenizer", ",", "args", ")", ":", "\n", "    ", "\"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"", "\n", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "# We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)", "\n", "probability_matrix", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "args", ".", "mlm_probability", ")", "\n", "special_tokens_mask", "=", "[", "tokenizer", ".", "get_special_tokens_mask", "(", "val", ",", "already_has_special_tokens", "=", "True", ")", "for", "val", "in", "labels", ".", "tolist", "(", ")", "]", "\n", "probability_matrix", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "special_tokens_mask", ",", "dtype", "=", "torch", ".", "ByteTensor", ")", ",", "value", "=", "0.0", ")", "\n", "masked_indices", "=", "torch", ".", "bernoulli", "(", "probability_matrix", ")", ".", "type", "(", "torch", ".", "ByteTensor", ")", "\n", "labels", "[", "~", "masked_indices", "]", "=", "-", "1", "# We only compute loss on masked tokens", "\n", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.8", ")", ")", ".", "type", "(", "torch", ".", "ByteTensor", ")", "&", "masked_indices", "\n", "inputs", "[", "indices_replaced", "]", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "mask_token", ")", "\n", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.5", ")", ")", ".", "type", "(", "torch", ".", "ByteTensor", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "tokenizer", ")", ",", "labels", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "\n", "# The rest of the time (10% of the time) we keep the masked input tokens unchanged", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.mask_tokens_nomlm": [[113, 124], ["inputs.clone", "torch.full().masked_fill_", "torch.full().masked_fill_", "torch.full().masked_fill_", "torch.full().masked_fill_", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "inputs.clone.tolist", "torch.full", "torch.full", "torch.full", "torch.full"], "function", ["None"], ["", "def", "mask_tokens_nomlm", "(", "inputs", ",", "tokenizer", ",", "args", ")", ":", "\n", "\n", "    ", "\"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"", "\n", "\n", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "\n", "att_mask", "=", "[", "val", "==", "tokenizer", ".", "pad_token_id", "for", "val", "in", "labels", ".", "tolist", "(", ")", "]", "\n", "\n", "attention_mask", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "1", ")", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "att_mask", ",", "dtype", "=", "torch", ".", "uint8", ")", ",", "value", "=", "0", ")", "\n", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.yanyuanqiao_hop-vln.pretrain.batch_loader.mask_tokens": [[126, 161], ["inputs.clone", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.full().masked_fill_", "torch.full().masked_fill_", "torch.full().masked_fill_", "torch.full().masked_fill_", "tokenizer.convert_tokens_to_ids", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "len", "inputs.clone.tolist", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full", "torch.full", "torch.full", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli().type", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full", "torch.full", "torch.full", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.full", "torch.full", "torch.full", "torch.full"], "function", ["None"], ["", "def", "mask_tokens", "(", "inputs", ",", "tokenizer", ",", "args", ")", ":", "\n", "\n", "    ", "\"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"", "\n", "\n", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "\n", "# We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)", "\n", "#probability_matrix = torch.full(labels.shape, args.mlm_probability)", "\n", "probability_matrix", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.15", ")", "\n", "special_tokens_mask", "=", "[", "val", "in", "tokenizer", ".", "all_special_ids", "for", "val", "in", "labels", ".", "tolist", "(", ")", "]", "\n", "att_mask", "=", "[", "val", "==", "tokenizer", ".", "pad_token_id", "for", "val", "in", "labels", ".", "tolist", "(", ")", "]", "\n", "probability_matrix", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "special_tokens_mask", ",", "dtype", "=", "torch", ".", "uint8", ")", ",", "value", "=", "0.0", ")", "\n", "#masked_indices = torch.bernoulli(torch.full(labels.shape, args.mlm_probability)).type(torch.ByteTensor)", "\n", "masked_indices", "=", "torch", ".", "bernoulli", "(", "probability_matrix", ")", ".", "type", "(", "torch", ".", "ByteTensor", ")", "\n", "\n", "attention_mask", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "1", ")", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "att_mask", ",", "dtype", "=", "torch", ".", "uint8", ")", ",", "value", "=", "0", ")", "\n", "labels", "[", "~", "masked_indices", "]", "=", "-", "1", "# We only compute loss on masked tokens", "\n", "\n", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.8", ")", ")", ".", "type", "(", "torch", ".", "ByteTensor", ")", "&", "masked_indices", "\n", "\n", "inputs", "[", "indices_replaced", "]", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "mask_token", ")", "\n", "\n", "\n", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.5", ")", ")", ".", "type", "(", "torch", ".", "ByteTensor", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "tokenizer", ")", ",", "labels", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "\n", "# The rest of the time (10% of the time) we keep the masked input tokens unchanged", "\n", "return", "inputs", ",", "labels", ",", "attention_mask", "\n", "\n"]]}