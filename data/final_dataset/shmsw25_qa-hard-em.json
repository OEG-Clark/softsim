{"home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.__init__": [[41, 88], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.from_dict": [[89, 96], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.from_json_file": [[97, 103], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.to_dict": [[104, 108], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.to_json_string": [[109, 112], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTLayerNorm.__init__": [[115, 122], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "variance_epsilon", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BERTLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "config", ".", "hidden_size", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "variance_epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTLayerNorm.forward": [[123, 128], ["x.mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "gamma", "*", "x", "+", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTEmbeddings.__init__": [[130, 140], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\"\"\"Construct the embedding module from word, position and token_type embeddings.\n        \"\"\"", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTEmbeddings.forward": [[141, 156], ["input_ids.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BERTEmbeddings.word_embeddings", "modeling.BERTEmbeddings.position_embeddings", "modeling.BERTEmbeddings.token_type_embeddings", "modeling.BERTEmbeddings.LayerNorm", "modeling.BERTEmbeddings.dropout", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfAttention.__init__": [[159, 174], ["torch.Module.__init__", "int", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfAttention.transpose_for_scores": [[175, 179], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfAttention.forward": [[180, 207], ["modeling.BERTSelfAttention.query", "modeling.BERTSelfAttention.key", "modeling.BERTSelfAttention.value", "modeling.BERTSelfAttention.transpose_for_scores", "modeling.BERTSelfAttention.transpose_for_scores", "modeling.BERTSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling.BERTSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BERTSelfAttention.transpose", "math.sqrt", "torch.Softmax", "torch.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfOutput.__init__": [[210, 215], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTSelfOutput.forward": [[216, 221], ["modeling.BERTSelfOutput.dense", "modeling.BERTSelfOutput.dropout", "modeling.BERTSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTAttention.__init__": [[224, 228], ["torch.Module.__init__", "modeling.BERTSelfAttention", "modeling.BERTSelfOutput"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BERTSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BERTSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTAttention.forward": [[229, 233], ["modeling.BERTAttention.self", "modeling.BERTAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTIntermediate.__init__": [[236, 240], ["torch.Module.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "gelu", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTIntermediate.forward": [[241, 245], ["modeling.BERTIntermediate.dense", "modeling.BERTIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTOutput.__init__": [[248, 253], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTOutput.forward": [[254, 259], ["modeling.BERTOutput.dense", "modeling.BERTOutput.dropout", "modeling.BERTOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTLayer.__init__": [[261, 266], ["torch.Module.__init__", "modeling.BERTAttention", "modeling.BERTIntermediate", "modeling.BERTOutput"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BERTAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BERTIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BERTOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTLayer.forward": [[267, 272], ["modeling.BERTLayer.attention", "modeling.BERTLayer.intermediate", "modeling.BERTLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTEncoder.__init__": [[274, 279], ["torch.Module.__init__", "modeling.BERTLayer", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BERTLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "0.3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTEncoder.forward": [[280, 286], ["layer_module", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTPooler.__init__": [[288, 292], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BERTPooler.forward": [[293, 300], ["modeling.BERTPooler.dense", "modeling.BERTPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertModel.__init__": [[319, 329], ["torch.Module.__init__", "modeling.BERTEmbeddings", "modeling.BERTEncoder", "modeling.BERTPooler"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["def", "__init__", "(", "self", ",", "config", ":", "BertConfig", ")", ":", "\n", "        ", "\"\"\"Constructor for BertModel.\n\n        Args:\n            config: `BertConfig` instance.\n        \"\"\"", "\n", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embeddings", "=", "BERTEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BERTEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BERTPooler", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertModel.forward": [[330, 352], ["torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "next", "modeling.BertModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "return", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering.__init__": [[356, 376], ["torch.Module.__init__", "modeling.BertModel", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "modeling.BertForQuestionAnswering.apply", "isinstance", "isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "device", ",", "n_class", ",", "loss_type", ",", "variant_id", "=", "0", ",", "tau", "=", "None", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "# [N, L, H] => [N, L, 2]", "\n", "self", ".", "qa_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "n_class", ")", "# [N, H] => [N, n_class]", "\n", "self", ".", "device", "=", "device", "\n", "def", "init_weights", "(", "module", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "                ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BERTLayerNorm", ")", ":", "\n", "                ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "self", ".", "loss_type", "=", "loss_type", "\n", "self", ".", "tau", "=", "tau", "\n", "if", "self", ".", "loss_type", "==", "'hard-em'", ":", "\n", "            ", "assert", "tau", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._forward": [[377, 406], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "modeling.BertForQuestionAnswering.qa_classifier", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["None"], ["", "", "def", "_forward", "(", "self", ",", "batch", ")", ":", "\n", "        ", "'''\n        each batch is a list of 7 items (training) or 3 items (inference)\n            - input_ids: token id of the input sequence\n            - attention_mask: mask of the sequence (1 for present, 0 for blank)\n            - token_type_ids: indicator of type of sequence.\n            -      e.g. in QA, whether it is question or document\n            - (training) start_positions: list of start positions of the span\n            - (training) end_positions: list of end positions of the span\n            - (training) switch: list of switches (can be used for general purposes.\n            -      in this model, 0 means the answer is span, 1 means the answer is `yes`,\n            -      2 means the answer is `no`, 3 means there's no answer\n            - (training) answer_mask: list of answer mask.\n            -      e.g. if the possible spans are `[0, 7], [3, 7]`, and your `max_n_answers` is 3,\n            -      start_positions: [[0, 3, 0]]\n            -      end_positions: [[7, 7, 0]]\n            -      switch: [[0, 0, 0]]\n            -      answer_mask: [[1, 1, 0]]\n        '''", "\n", "\n", "input_ids", ",", "attention_mask", ",", "token_type_ids", "=", "batch", "[", ":", "3", "]", "\n", "all_encoder_layers", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "sequence_output", "=", "all_encoder_layers", "[", "-", "1", "]", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "switch_logits", "=", "self", ".", "qa_classifier", "(", "torch", ".", "max", "(", "sequence_output", ",", "1", ")", "[", "0", "]", ")", "\n", "return", "start_logits", ",", "end_logits", ",", "switch_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering.forward": [[407, 453], ["modeling.BertForQuestionAnswering._forward", "len", "start_logits.size", "start_positions.clamp_", "end_positions.clamp_", "answer_mask.type().to.type().to.type().to", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "len", "len", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "len", "NotImplementedError", "answer_mask.type().to.type().to.type", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "zip", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "t.unsqueeze", "numpy.random.random", "min", "modeling.BertForQuestionAnswering._take_min", "modeling.BertForQuestionAnswering._take_mml", "modeling.BertForQuestionAnswering._take_mml", "NotImplementedError", "t.unsqueeze", "t.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._forward", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._take_min", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._take_mml", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._take_mml"], ["", "def", "forward", "(", "self", ",", "batch", ",", "global_step", "=", "-", "1", ")", ":", "\n", "        ", "start_logits", ",", "end_logits", ",", "switch_logits", "=", "self", ".", "_forward", "(", "batch", ")", "\n", "if", "len", "(", "batch", ")", "==", "7", ":", "\n", "            ", "start_positions", ",", "end_positions", ",", "switch", ",", "answer_mask", "=", "batch", "[", "3", ":", "]", "\n", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "answer_mask", "=", "answer_mask", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ",", "reduce", "=", "False", ")", "\n", "# You care about the span only when switch is 0", "\n", "span_mask", "=", "answer_mask", "*", "(", "switch", "==", "0", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "\n", "start_losses", "=", "[", "(", "loss_fct", "(", "start_logits", ",", "_start_positions", ")", "*", "_span_mask", ")", "for", "(", "_start_positions", ",", "_span_mask", ")", "in", "zip", "(", "torch", ".", "unbind", "(", "start_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "span_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "end_losses", "=", "[", "(", "loss_fct", "(", "end_logits", ",", "_end_positions", ")", "*", "_span_mask", ")", "for", "(", "_end_positions", ",", "_span_mask", ")", "in", "zip", "(", "torch", ".", "unbind", "(", "end_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "span_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "switch_losses", "=", "[", "(", "loss_fct", "(", "switch_logits", ",", "_switch", ")", "*", "_answer_mask", ")", "for", "(", "_switch", ",", "_answer_mask", ")", "in", "zip", "(", "torch", ".", "unbind", "(", "switch", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "assert", "len", "(", "start_losses", ")", "==", "len", "(", "end_losses", ")", "==", "len", "(", "switch_losses", ")", "\n", "loss_tensor", "=", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "start_losses", "]", ",", "dim", "=", "1", ")", "+", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "end_losses", "]", ",", "dim", "=", "1", ")", "+", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "switch_losses", "]", ",", "dim", "=", "1", ")", "\n", "\n", "if", "self", ".", "loss_type", "==", "'first-only'", ":", "\n", "                ", "total_loss", "=", "torch", ".", "sum", "(", "start_losses", "[", "0", "]", "+", "end_losses", "[", "0", "]", "+", "switch_losses", "[", "0", "]", ")", "\n", "", "elif", "self", ".", "loss_type", "==", "\"hard-em\"", ":", "\n", "                ", "if", "numpy", ".", "random", ".", "random", "(", ")", "<", "min", "(", "global_step", "/", "self", ".", "tau", ",", "0.8", ")", ":", "\n", "                    ", "total_loss", "=", "self", ".", "_take_min", "(", "loss_tensor", ")", "\n", "", "else", ":", "\n", "                    ", "total_loss", "=", "self", ".", "_take_mml", "(", "loss_tensor", ")", "\n", "", "", "elif", "self", ".", "loss_type", "==", "\"mml\"", ":", "\n", "                ", "total_loss", "=", "self", ".", "_take_mml", "(", "loss_tensor", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "", "return", "total_loss", "\n", "\n", "", "elif", "len", "(", "batch", ")", "==", "3", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", ",", "switch_logits", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._take_min": [[454, 457], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.min", "torch.min", "torch.min", "torch.min", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["None"], ["", "", "def", "_take_min", "(", "self", ",", "loss_tensor", ")", ":", "\n", "        ", "return", "torch", ".", "sum", "(", "torch", ".", "min", "(", "\n", "loss_tensor", "+", "2", "*", "torch", ".", "max", "(", "loss_tensor", ")", "*", "(", "loss_tensor", "==", "0", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "to", "(", "self", ".", "device", ")", ",", "1", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertForQuestionAnswering._take_mml": [[458, 460], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "def", "_take_mml", "(", "self", ",", "loss_tensor", ")", ":", "\n", "        ", "return", "-", "torch", ".", "sum", "(", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "-", "loss_tensor", "-", "1e10", "*", "(", "loss_tensor", "==", "0", ")", ".", "float", "(", ")", ")", ",", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.gelu": [[30, 36], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.main.main": [[48, 314], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "logging.getLogger", "logging.getLogger.info", "logging.getLogger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "modeling.BertConfig.from_json_file", "modeling.BertForQuestionAnswering", "tokenization.FullTokenizer", "prepro.get_dataloader", "model.to.to", "os.path.exists", "os.listdir", "print", "os.path.exists", "os.makedirs", "torch.device", "torch.cuda.device_count", "torch.device", "torch.distributed.init_process_group", "bool", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "len", "prepro.get_dataloader", "logging.getLogger.info", "torch.load", "torch.nn.parallel.DistributedDataParallel", "optimization.BERTAdam", "model.to.train", "range", "logging.getLogger.info", "ValueError", "ValueError", "ValueError", "parser.parse_args.train_file.split", "parser.parse_args.init_checkpoint.endswith", "model.to.bert.load_state_dict", "model.to.load_state_dict", "torch.nn.DataParallel", "int", "enumerate", "main.predict", "logging.FileHandler", "logging.StreamHandler", "parser.parse_args.train_file.split", "filter", "model.to.", "train_losses.append", "loss.mean.backward", "type", "model.to.eval", "os.path.join", "torch.cuda.is_available", "x.startswith", "torch.load.items", "parser.parse_args.train_file.split", "prepro.get_dataloader", "t.to", "loss.mean.mean", "loss.mean.detach().cpu", "optimization.BERTAdam.step", "model.to.zero_grad", "model.to.eval", "main.predict", "logging.getLogger.info", "model.to.train", "m.eval", "model.to.named_parameters", "model.to.named_parameters", "logging.getLogger.info", "torch.save", "model.to.to", "len", "loss.mean.detach", "v.cpu", "os.path.join", "numpy.mean", "model.to.state_dict().items", "model.to.state_dict"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.get_dataloader", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.get_dataloader", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.main.predict", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.eval", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.get_dataloader", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.BERTAdam.step", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.eval", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.main.predict", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.eval", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.save"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "BERT_DIR", "=", "\"uncased_L-12_H-768_A-12/\"", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_file\"", ",", "default", "=", "BERT_DIR", "+", "\"bert_config.json\"", ",", "type", "=", "str", ",", "help", "=", "\"The config json file corresponding to the pre-trained BERT model. \"", "\n", "\"This specifies the model architecture.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file\"", ",", "default", "=", "BERT_DIR", "+", "\"vocab.txt\"", ",", "type", "=", "str", ",", "help", "=", "\"The vocabulary file that the BERT model was trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "\"out\"", ",", "type", "=", "str", ",", "help", "=", "\"The output directory where the model checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--load\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_file\"", ",", "type", "=", "str", ",", "help", "=", "\"SQuAD json for training. E.g., train-v1.1.json\"", ",", "default", "=", "\"/home/sewon/data/squad/train-v1.1.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_file\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\"", ",", "default", "=", "\"/home/sewon/data/squad/dev-v1.1.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--init_checkpoint\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Initial checkpoint (usually from a pre-trained BERT model).\"", ",", "default", "=", "BERT_DIR", "+", "\"pytorch_model.bin\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to lower case the input text. Should be True for uncased \"", "\n", "\"models and False for cased models.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "300", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. Sequences \"", "\n", "\"longer than this will be truncated, and sequences shorter than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--doc_stride\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "\"When splitting up a long document into chunks, how much stride to take between chunks.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_query_length\"", ",", "default", "=", "64", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum number of tokens for the question. Questions longer than this will \"", "\n", "\"be truncated to this length.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_predict\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "default", "=", "39", ",", "type", "=", "int", ",", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_batch_size\"", ",", "default", "=", "300", ",", "type", "=", "int", ",", "help", "=", "\"Total batch size for predictions.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "default", "=", "1000.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10% \"", "\n", "\"of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_checkpoints_steps\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "\n", "help", "=", "\"How often to save the model checkpoint.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--iterations_per_loop\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "\n", "help", "=", "\"How many steps to make in each estimator call.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_best_size\"", ",", "default", "=", "3", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The total number of n-best predictions to generate in the nbest_predictions.json \"", "\n", "\"output file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verbose_logging\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"If true, all of the warnings related to data processing will be printed. \"", "\n", "\"A number of warnings are expected for a normal SQuAD evaluation.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--accumulate_gradients\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Number of steps to accumulate gradient on (divide the batch_size and accumulate)\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumualte before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_period'", ",", "type", "=", "int", ",", "default", "=", "500", ")", "\n", "parser", ".", "add_argument", "(", "'--max_n_answers'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--n_paragraphs'", ",", "type", "=", "str", ",", "default", "=", "'40'", ")", "\n", "parser", ".", "add_argument", "(", "'--verbose'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--wait_step'", ",", "type", "=", "int", ",", "default", "=", "12", ")", "\n", "\n", "# Learning method variation", "\n", "parser", ".", "add_argument", "(", "'--loss_type'", ",", "type", "=", "str", ",", "default", "=", "\"mml\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tau'", ",", "type", "=", "float", ",", "default", "=", "12000.0", ")", "\n", "\n", "# For evaluation", "\n", "parser", ".", "add_argument", "(", "'--prefix'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "#500", "\n", "parser", ".", "add_argument", "(", "'--debug'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "print", "(", "\"Output directory () already exists and is not empty.\"", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s - %(levelname)s - %(name)s - %(message)s'", ",", "\n", "datefmt", "=", "'%m/%d/%Y %H:%M:%S'", ",", "\n", "level", "=", "logging", ".", "INFO", ",", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"log.txt\"", ")", ")", ",", "\n", "logging", ".", "StreamHandler", "(", ")", "]", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "args", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device %s n_gpu %d distributed training %r\"", ",", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "\n", "if", "args", ".", "accumulate_gradients", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid accumulate_gradients parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "accumulate_gradients", ")", ")", "\n", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "accumulate_gradients", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_predict", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_predict` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "if", "not", "args", ".", "train_file", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"If `do_train` is True, then `train_file` must be specified.\"", ")", "\n", "", "if", "not", "args", ".", "predict_file", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"If `do_train` is True, then `predict_file` must be specified.\"", ")", "\n", "\n", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "if", "not", "args", ".", "predict_file", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"If `do_predict` is True, then `predict_file` must be specified.\"", ")", "\n", "\n", "", "", "bert_config", "=", "BertConfig", ".", "from_json_file", "(", "args", ".", "bert_config_file", ")", "\n", "\n", "if", "args", ".", "do_train", "and", "args", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length %d because the BERT model \"", "\n", "\"was only trained up to sequence length %d\"", "%", "\n", "(", "args", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "\n", "", "model", "=", "BertForQuestionAnswering", "(", "bert_config", ",", "device", ",", "4", ",", "loss_type", "=", "args", ".", "loss_type", ",", "tau", "=", "args", ".", "tau", ")", "\n", "metric_name", "=", "\"EM\"", "\n", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "args", ".", "vocab_file", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "train_split", "=", "','", "in", "args", ".", "train_file", "\n", "if", "train_split", ":", "\n", "        ", "n_train_files", "=", "len", "(", "args", ".", "train_file", ".", "split", "(", "','", ")", ")", "\n", "\n", "", "eval_dataloader", ",", "eval_examples", ",", "eval_features", ",", "_", "=", "get_dataloader", "(", "\n", "logger", "=", "logger", ",", "args", "=", "args", ",", "\n", "input_file", "=", "args", ".", "predict_file", ",", "\n", "is_training", "=", "False", ",", "\n", "batch_size", "=", "args", ".", "predict_batch_size", ",", "\n", "num_epochs", "=", "1", ",", "\n", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_file", "=", "args", ".", "train_file", "\n", "if", "train_split", ":", "\n", "            ", "train_file", "=", "args", ".", "train_file", ".", "split", "(", "','", ")", "[", "0", "]", "\n", "", "train_dataloader", ",", "_", ",", "_", ",", "num_train_steps", "=", "get_dataloader", "(", "\n", "logger", "=", "logger", ",", "args", "=", "args", ",", "input_file", "=", "train_file", ",", "is_training", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "train_batch_size", ",", "\n", "num_epochs", "=", "args", ".", "num_train_epochs", ",", "\n", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "", "if", "args", ".", "init_checkpoint", "is", "not", "None", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading from {}\"", ".", "format", "(", "args", ".", "init_checkpoint", ")", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "args", ".", "init_checkpoint", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "args", ".", "do_train", "and", "args", ".", "init_checkpoint", ".", "endswith", "(", "'pytorch_model.bin'", ")", ":", "\n", "            ", "model", ".", "bert", ".", "load_state_dict", "(", "state_dict", ")", "\n", "", "else", ":", "\n", "            ", "filter", "=", "lambda", "x", ":", "x", "[", "7", ":", "]", "if", "x", ".", "startswith", "(", "'module.'", ")", "else", "x", "\n", "state_dict", "=", "{", "filter", "(", "k", ")", ":", "v", "for", "(", "k", ",", "v", ")", "in", "state_dict", ".", "items", "(", ")", "}", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "args", ".", "local_rank", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "not", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "\n", "optimizer", "=", "BERTAdam", "(", "optimizer_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_steps", ")", "\n", "\n", "global_step", "=", "0", "\n", "\n", "best_f1", "=", "(", "-", "1", ",", "-", "1", ")", "\n", "wait_step", "=", "0", "\n", "model", ".", "train", "(", ")", "\n", "global_step", "=", "0", "\n", "stop_training", "=", "False", "\n", "train_losses", "=", "[", "]", "\n", "\n", "for", "epoch", "in", "range", "(", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "            ", "if", "epoch", ">", "0", "and", "train_split", ":", "\n", "                ", "train_file", "=", "args", ".", "train_file", ".", "split", "(", "','", ")", "[", "epoch", "%", "n_train_files", "]", "\n", "train_dataloader", "=", "get_dataloader", "(", "\n", "logger", "=", "logger", ",", "args", "=", "args", ",", "input_file", "=", "train_file", ",", "is_training", "=", "True", ",", "\n", "batch_size", "=", "args", ".", "train_batch_size", ",", "\n", "num_epochs", "=", "args", ".", "num_train_epochs", ",", "\n", "tokenizer", "=", "tokenizer", ")", "[", "0", "]", "\n", "\n", "", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "                ", "global_step", "+=", "1", "\n", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", "]", "\n", "loss", "=", "model", "(", "batch", ",", "global_step", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "", "train_losses", ".", "append", "(", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "if", "global_step", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "# We have accumulated enought gradients", "\n", "model", ".", "zero_grad", "(", ")", "\n", "", "if", "global_step", "%", "args", ".", "eval_period", "==", "0", ":", "\n", "                    ", "model", ".", "eval", "(", ")", "\n", "f1", "=", "predict", "(", "logger", ",", "args", ",", "model", ",", "eval_dataloader", ",", "eval_examples", ",", "eval_features", ",", "device", ",", "write_prediction", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"Step %d Train loss %.2f EM %.2f F1 %.2f on epoch=%d\"", "%", "(", "\n", "global_step", ",", "np", ".", "mean", "(", "train_losses", ")", ",", "f1", "[", "0", "]", "*", "100", ",", "f1", "[", "1", "]", "*", "100", ",", "epoch", ")", ")", "\n", "train_losses", "=", "[", "]", "\n", "if", "best_f1", "<", "f1", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Saving model with best %s: %.2f (F1 %.2f) -> %.2f (F1 %.2f) on epoch=%d\"", "%", "(", "metric_name", ",", "best_f1", "[", "0", "]", "*", "100", ",", "best_f1", "[", "1", "]", "*", "100", ",", "f1", "[", "0", "]", "*", "100", ",", "f1", "[", "1", "]", "*", "100", ",", "epoch", ")", ")", "\n", "model_state_dict", "=", "{", "k", ":", "v", ".", "cpu", "(", ")", "for", "(", "k", ",", "v", ")", "in", "model", ".", "state_dict", "(", ")", ".", "items", "(", ")", "}", "\n", "torch", ".", "save", "(", "model_state_dict", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"best-model.pt\"", ")", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "best_f1", "=", "f1", "\n", "wait_step", "=", "0", "\n", "stop_training", "=", "False", "\n", "", "else", ":", "\n", "                        ", "wait_step", "+=", "1", "\n", "if", "wait_step", "==", "args", ".", "wait_step", ":", "\n", "                            ", "stop_training", "=", "True", "\n", "", "", "model", ".", "train", "(", ")", "\n", "", "", "if", "stop_training", ":", "\n", "                ", "break", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Training finished!\"", ")", "\n", "\n", "", "elif", "args", ".", "do_predict", ":", "\n", "        ", "if", "type", "(", "model", ")", "==", "list", ":", "\n", "            ", "model", "=", "[", "m", ".", "eval", "(", ")", "for", "m", "in", "model", "]", "\n", "", "else", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "", "f1", "=", "predict", "(", "logger", ",", "args", ",", "model", ",", "eval_dataloader", ",", "eval_examples", ",", "eval_features", ",", "\n", "device", ",", "\n", "varying_n_paragraphs", "=", "len", "(", "args", ".", "n_paragraphs", ")", ">", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.main.predict": [[316, 351], ["os.path.join", "os.path.join", "evaluate_qa.write_predictions", "tqdm.tqdm", "enumerate", "t.to", "torch.no_grad", "model", "batch_start_logits[].detach().cpu().tolist", "batch_end_logits[].detach().cpu().tolist", "batch_switch[].detach().cpu().tolist", "int", "all_results.append", "len", "len", "len", "RawResult", "batch_start_logits[].detach().cpu", "batch_end_logits[].detach().cpu", "batch_switch[].detach().cpu", "example_index.item", "int", "args.n_paragraphs.split", "batch_start_logits[].detach", "batch_end_logits[].detach", "batch_switch[].detach"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluate_qa.write_predictions"], ["", "", "def", "predict", "(", "logger", ",", "args", ",", "model", ",", "eval_dataloader", ",", "eval_examples", ",", "eval_features", ",", "device", ",", "write_prediction", "=", "True", ",", "varying_n_paragraphs", "=", "False", ")", ":", "\n", "    ", "all_results", "=", "[", "]", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "        ", "eval_dataloader", "=", "tqdm", "(", "eval_dataloader", ")", "\n", "\n", "", "for", "batch", "in", "eval_dataloader", ":", "\n", "        ", "example_indices", "=", "batch", "[", "-", "1", "]", "\n", "batch_to_feed", "=", "[", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", "[", ":", "-", "1", "]", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "batch_start_logits", ",", "batch_end_logits", ",", "batch_switch", "=", "model", "(", "batch_to_feed", ")", "\n", "assert", "len", "(", "batch_start_logits", ")", "==", "len", "(", "batch_end_logits", ")", "==", "len", "(", "batch_switch", ")", "\n", "", "for", "i", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "            ", "start_logits", "=", "batch_start_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "end_logits", "=", "batch_end_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "switch", "=", "batch_switch", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "eval_feature", "=", "eval_features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "unique_id", "=", "int", "(", "eval_feature", ".", "unique_id", ")", "\n", "all_results", ".", "append", "(", "RawResult", "(", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ",", "\n", "switch", "=", "switch", ")", ")", "\n", "\n", "", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "args", ".", "prefix", "+", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "args", ".", "prefix", "+", "\"nbest_predictions.json\"", ")", "\n", "f1", "=", "write_predictions", "(", "logger", ",", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", "if", "write_prediction", "else", "1", ",", "\n", "args", ".", "do_lower_case", ",", "\n", "output_prediction_file", "if", "write_prediction", "else", "None", ",", "\n", "output_nbest_file", "if", "write_prediction", "else", "None", ",", "\n", "args", ".", "verbose", ",", "\n", "write_prediction", "=", "write_prediction", ",", "\n", "n_paragraphs", "=", "None", "if", "not", "varying_n_paragraphs", "else", "[", "int", "(", "n", ")", "for", "n", "in", "args", ".", "n_paragraphs", ".", "split", "(", "','", ")", "]", ")", "\n", "return", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.BERTAdam.__init__": [[58, 77], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay_rate", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "not", "lr", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay_rate", "=", "weight_decay_rate", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BERTAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.BERTAdam.get_lr": [[78, 92], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.BERTAdam.step": [[93, 161], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay_rate'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay_rate'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.warmup_cosine": [[22, 26], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.warmup_constant": [[27, 31], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.optimization.warmup_linear": [[32, 36], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.get_dataloader": [[18, 71], ["input_file.replace", "os.path.exists", "sum", "int", "logger.info", "logger.info", "DataLoader.MyDataLoader", "logger.info", "prepro.read_squad_examples", "prepro.convert_examples_to_features", "logger.info", "logger.info", "n_paragraphs.split", "open", "pickle.load", "pkl.load.get", "logger.info", "len", "len", "open", "pickle.dump", "len"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.read_squad_examples", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.convert_examples_to_features"], ["def", "get_dataloader", "(", "logger", ",", "args", ",", "input_file", ",", "is_training", ",", "batch_size", ",", "num_epochs", ",", "tokenizer", ",", "index", "=", "None", ")", ":", "\n", "\n", "    ", "n_paragraphs", "=", "args", ".", "n_paragraphs", "\n", "\n", "if", "(", "not", "is_training", ")", "and", "','", "in", "n_paragraphs", ":", "\n", "        ", "n_paragraphs", "=", "n_paragraphs", ".", "split", "(", "','", ")", "[", "-", "1", "]", "\n", "\n", "", "feature_save_path", "=", "input_file", ".", "replace", "(", "'.json'", ",", "'-{}-{}-{}.pkl'", ".", "format", "(", "\n", "args", ".", "max_seq_length", ",", "n_paragraphs", ",", "args", ".", "max_n_answers", ")", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "feature_save_path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading saved features from {}\"", ".", "format", "(", "feature_save_path", ")", ")", "\n", "with", "open", "(", "feature_save_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "features", "=", "pkl", ".", "load", "(", "f", ")", "\n", "train_features", "=", "features", "[", "'features'", "]", "\n", "examples", "=", "features", ".", "get", "(", "'examples'", ",", "None", ")", "\n", "", "", "else", ":", "\n", "\n", "        ", "examples", "=", "read_squad_examples", "(", "\n", "logger", "=", "logger", ",", "args", "=", "args", ",", "input_file", "=", "input_file", ",", "debug", "=", "args", ".", "debug", ")", "\n", "\n", "train_features", "=", "convert_examples_to_features", "(", "\n", "logger", "=", "logger", ",", "\n", "args", "=", "args", ",", "\n", "examples", "=", "examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "args", ".", "doc_stride", ",", "\n", "max_query_length", "=", "args", ".", "max_query_length", ",", "\n", "max_n_answers", "=", "args", ".", "max_n_answers", "if", "is_training", "else", "1", ",", "\n", "is_training", "=", "is_training", ")", "\n", "if", "not", "args", ".", "debug", ":", "\n", "            ", "logger", ".", "info", "(", "\"Saving features to: {}\"", ".", "format", "(", "feature_save_path", ")", ")", "\n", "save_features", "=", "{", "'features'", ":", "train_features", "}", "\n", "if", "not", "is_training", ":", "\n", "                ", "save_features", "[", "'examples'", "]", "=", "examples", "\n", "", "with", "open", "(", "feature_save_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pkl", ".", "dump", "(", "save_features", ",", "f", ")", "\n", "\n", "", "", "", "n_features", "=", "sum", "(", "[", "len", "(", "f", ")", "for", "f", "in", "train_features", "]", ")", "\n", "num_train_steps", "=", "int", "(", "len", "(", "train_features", ")", "/", "batch_size", "*", "num_epochs", ")", "\n", "\n", "if", "examples", "is", "not", "None", ":", "\n", "        ", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "examples", ")", ")", "\n", "", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "n_features", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "batch_size", ")", "\n", "if", "is_training", ":", "\n", "        ", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "\n", "", "dataloader", "=", "MyDataLoader", "(", "features", "=", "train_features", ",", "batch_size", "=", "batch_size", ",", "is_training", "=", "is_training", ")", "\n", "flattened_features", "=", "[", "f", "for", "_features", "in", "train_features", "for", "f", "in", "_features", "]", "\n", "return", "dataloader", ",", "examples", ",", "flattened_features", ",", "num_train_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.read_squad_examples": [[73, 157], ["input_file.split", "logger.info", "sent.replace().replace().replace", "logger.info", "tqdm.tqdm", "zip", "examples.append", "type", "open", "doc_tokens_list1.append", "char_to_word_offset_list.append", "original_answers_list.append", "start_positions_list.append", "end_positions_list.append", "switches_list.append", "prepro_util.SquadExample", "prepro.read_squad_examples._process_sent"], "function", ["None"], ["", "def", "read_squad_examples", "(", "logger", ",", "args", ",", "input_file", ",", "debug", ")", ":", "\n", "    ", "def", "_process_sent", "(", "sent", ")", ":", "\n", "        ", "if", "type", "(", "sent", ")", "!=", "str", ":", "\n", "            ", "return", "[", "_process_sent", "(", "s", ")", "for", "s", "in", "sent", "]", "\n", "", "return", "sent", ".", "replace", "(", "'\u2013'", ",", "'-'", ")", ".", "replace", "(", "'&'", ",", "'and'", ")", ".", "replace", "(", "'&amp;'", ",", "'and'", ")", "\n", "\n", "", "input_data", "=", "[", "]", "\n", "for", "_input_file", "in", "input_file", ".", "split", "(", "','", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading {}\"", ".", "format", "(", "_input_file", ")", ")", "\n", "with", "open", "(", "_input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "this_data", "=", "[", "json", ".", "loads", "(", "line", ")", "for", "line", "in", "f", ".", "readlines", "(", ")", "]", "\n", "if", "debug", ":", "\n", "                ", "this_data", "=", "this_data", "[", ":", "50", "]", "\n", "", "input_data", "+=", "this_data", "\n", "\n", "", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "if", "args", ".", "verbose", ":", "\n", "        ", "input_data", "=", "tqdm", "(", "input_data", ")", "\n", "", "for", "entry", "in", "input_data", ":", "\n", "\n", "        ", "doc_tokens_list1", ",", "char_to_word_offset_list", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "tokens", "in", "entry", "[", "'context'", "]", ":", "\n", "            ", "paragraph_text", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "                ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                    ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                    ", "if", "prev_is_whitespace", ":", "\n", "                        ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                        ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "", "doc_tokens_list1", ".", "append", "(", "doc_tokens", ")", "\n", "char_to_word_offset_list", ".", "append", "(", "char_to_word_offset", ")", "\n", "\n", "", "question_text", "=", "entry", "[", "\"question\"", "]", "\n", "\n", "original_answers_list", ",", "start_positions_list", ",", "end_positions_list", ",", "switches_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "(", "char_to_word_offset", ",", "answers", ")", "in", "zip", "(", "char_to_word_offset_list", ",", "entry", "[", "'answers'", "]", ")", ":", "\n", "\n", "            ", "if", "len", "(", "answers", ")", "==", "0", ":", "\n", "                ", "original_answers", ",", "start_positions", ",", "end_positions", ",", "switches", "=", "[", "\"\"", "]", ",", "[", "0", "]", ",", "[", "0", "]", ",", "[", "3", "]", "\n", "", "else", ":", "\n", "                ", "original_answers", ",", "start_positions", ",", "end_positions", "=", "[", "[", "a", "[", "key", "]", "for", "a", "in", "answers", "]", "for", "key", "in", "[", "'text'", ",", "'word_start'", ",", "'word_end'", "]", "]", "\n", "switches", "=", "[", "0", "for", "_", "in", "answers", "]", "\n", "", "original_answers_list", ".", "append", "(", "original_answers", ")", "\n", "start_positions_list", ".", "append", "(", "start_positions", ")", "\n", "end_positions_list", ".", "append", "(", "end_positions", ")", "\n", "switches_list", ".", "append", "(", "switches", ")", "\n", "\n", "", "examples", ".", "append", "(", "SquadExample", "(", "\n", "qas_id", "=", "entry", "[", "'id'", "]", ",", "\n", "question_text", "=", "entry", "[", "'question'", "]", ",", "\n", "doc_tokens", "=", "entry", "[", "'context'", "]", ",", "\n", "paragraph_indices", "=", "list", "(", "range", "(", "len", "(", "entry", "[", "'context'", "]", ")", ")", ")", ",", "\n", "orig_answer_text", "=", "original_answers_list", ",", "\n", "all_answers", "=", "entry", "[", "'final_answers'", "]", ",", "\n", "start_position", "=", "start_positions_list", ",", "\n", "end_position", "=", "end_positions_list", ",", "\n", "switch", "=", "switches_list", ")", ")", "\n", "", "if", "\"test\"", "in", "input_file", ":", "\n", "        ", "return", "examples", "\n", "", "n_answers", "=", "[", "]", "\n", "for", "example", "in", "examples", ":", "\n", "        ", "has_answer", "=", "False", "\n", "for", "switches", "in", "example", ".", "switch", ":", "\n", "            ", "assert", "0", "in", "switches", "or", "switches", "==", "[", "3", "]", "\n", "if", "0", "in", "switches", ":", "\n", "                ", "n_answers", ".", "append", "(", "len", "(", "switches", ")", ")", "\n", "", "", "", "logger", ".", "info", "(", "\"# answers  = %.1f %.1f %.1f %.1f\"", "%", "(", "\n", "np", ".", "mean", "(", "n_answers", ")", ",", "np", ".", "median", "(", "n_answers", ")", ",", "\n", "np", ".", "percentile", "(", "n_answers", ",", "95", ")", ",", "np", ".", "percentile", "(", "n_answers", ",", "99", ")", ")", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro.convert_examples_to_features": [[158, 340], ["logger.info", "tqdm.tqdm", "enumerate", "tokenizer.tokenize", "zip", "features.append", "enumerate", "len", "len", "len", "len", "len", "len", "enumerate", "collections.namedtuple", "truncated.append", "enumerate", "numpy.mean", "orig_to_tok_index.append", "tokenizer.tokenize", "zip", "all", "len", "doc_spans.append", "min", "len", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "range", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "current_features.append", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "prepro._improve_answer_span", "tok_start_positions.append", "tok_end_positions.append", "len", "len", "len", "len", "len", "len", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "prepro._check_is_max_context", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "zip", "all", "range", "prepro_util.InputFeatures", "start_positions.append", "end_positions.append", "switches.append", "len", "len", "len", "sum", "len", "features_with_truncated_answers.append", "start_positions.append", "end_positions.append", "switches.append", "answer_mask.append", "len", "len", "len", "len", "len", "range", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro._improve_answer_span", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro._check_is_max_context"], ["", "def", "convert_examples_to_features", "(", "logger", ",", "args", ",", "examples", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "doc_stride", ",", "max_query_length", ",", "max_n_answers", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "1000000000", "\n", "\n", "truncated", "=", "[", "]", "\n", "features", "=", "[", "]", "\n", "features_with_truncated_answers", "=", "[", "]", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "        ", "examples", "=", "tqdm", "(", "enumerate", "(", "examples", ")", ")", "\n", "", "else", ":", "\n", "        ", "examples", "=", "enumerate", "(", "examples", ")", "\n", "\n", "", "for", "(", "example_index", ",", "example", ")", "in", "examples", ":", "\n", "        ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "            ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "assert", "len", "(", "example", ".", "doc_tokens", ")", "==", "len", "(", "example", ".", "orig_answer_text", ")", "==", "len", "(", "example", ".", "start_position", ")", "==", "len", "(", "example", ".", "end_position", ")", "==", "len", "(", "example", ".", "switch", ")", "\n", "\n", "current_features", "=", "[", "]", "\n", "\n", "for", "(", "paragraph_index", ",", "doc_tokens", ",", "original_answer_text_list", ",", "start_position_list", ",", "end_position_list", ",", "switch_list", ")", "in", "zip", "(", "example", ".", "paragraph_indices", ",", "example", ".", "doc_tokens", ",", "example", ".", "orig_answer_text", ",", "example", ".", "start_position", ",", "example", ".", "end_position", ",", "example", ".", "switch", ")", ":", "\n", "            ", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "doc_tokens", ")", ":", "\n", "                ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "[", "token", "]", ",", "basic_done", "=", "True", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                    ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "tok_start_positions", ",", "tok_end_positions", "=", "[", "]", ",", "[", "]", "\n", "\n", "if", "is_training", ":", "\n", "                ", "for", "(", "orig_answer_text", ",", "start_position", ",", "end_position", ")", "in", "zip", "(", "original_answer_text_list", ",", "start_position_list", ",", "end_position_list", ")", ":", "\n", "                    ", "tok_start_position", "=", "orig_to_tok_index", "[", "start_position", "]", "\n", "if", "end_position", "<", "len", "(", "doc_tokens", ")", "-", "1", ":", "\n", "                        ", "tok_end_position", "=", "orig_to_tok_index", "[", "end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                        ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", "\n", "tok_start_positions", ".", "append", "(", "tok_start_position", ")", "\n", "tok_end_positions", ".", "append", "(", "tok_end_position", ")", "\n", "", "to_be_same", "=", "[", "len", "(", "original_answer_text_list", ")", ",", "len", "(", "start_position_list", ")", ",", "len", "(", "end_position_list", ")", ",", "\n", "len", "(", "switch_list", ")", ",", "len", "(", "tok_start_positions", ")", ",", "len", "(", "tok_end_positions", ")", "]", "\n", "assert", "all", "(", "[", "x", "==", "to_be_same", "[", "0", "]", "for", "x", "in", "to_be_same", "]", ")", "\n", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                    ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                    ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "truncated", ".", "append", "(", "len", "(", "doc_spans", ")", ")", "\n", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "                ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "                    ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "                    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "start_positions", "=", "[", "]", "\n", "end_positions", "=", "[", "]", "\n", "switches", "=", "[", "]", "\n", "answer_mask", "=", "[", "]", "\n", "if", "is_training", ":", "\n", "                    ", "for", "(", "orig_answer_text", ",", "start_position", ",", "end_position", ",", "switch", ",", "tok_start_position", ",", "tok_end_position", ")", "in", "zip", "(", "original_answer_text_list", ",", "start_position_list", ",", "end_position_list", ",", "switch_list", ",", "tok_start_positions", ",", "tok_end_positions", ")", ":", "\n", "                        ", "if", "orig_answer_text", "not", "in", "[", "'yes'", ",", "'no'", "]", "or", "switch", "==", "3", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "                            ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "(", "tok_start_position", "<", "doc_start", "or", "\n", "tok_end_position", "<", "doc_start", "or", "\n", "tok_start_position", ">", "doc_end", "or", "tok_end_position", ">", "doc_end", ")", ":", "\n", "                                ", "continue", "\n", "", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "", "else", ":", "\n", "                            ", "start_position", ",", "end_position", "=", "0", ",", "0", "\n", "", "start_positions", ".", "append", "(", "start_position", ")", "\n", "end_positions", ".", "append", "(", "end_position", ")", "\n", "switches", ".", "append", "(", "switch", ")", "\n", "", "to_be_same", "=", "[", "len", "(", "start_positions", ")", ",", "len", "(", "end_positions", ")", ",", "len", "(", "switches", ")", "]", "\n", "assert", "all", "(", "[", "x", "==", "to_be_same", "[", "0", "]", "for", "x", "in", "to_be_same", "]", ")", "\n", "\n", "if", "sum", "(", "to_be_same", ")", "==", "0", ":", "\n", "                        ", "start_positions", "=", "[", "0", "]", "\n", "end_positions", "=", "[", "0", "]", "\n", "switches", "=", "[", "3", "]", "\n", "\n", "", "if", "len", "(", "start_positions", ")", ">", "max_n_answers", ":", "\n", "                        ", "features_with_truncated_answers", ".", "append", "(", "len", "(", "features", ")", ")", "\n", "start_positions", "=", "start_positions", "[", ":", "max_n_answers", "]", "\n", "end_positions", "=", "end_positions", "[", ":", "max_n_answers", "]", "\n", "switches", "=", "switches", "[", ":", "max_n_answers", "]", "\n", "", "answer_mask", "=", "[", "1", "for", "_", "in", "range", "(", "len", "(", "start_positions", ")", ")", "]", "\n", "for", "_", "in", "range", "(", "max_n_answers", "-", "len", "(", "start_positions", ")", ")", ":", "\n", "                        ", "start_positions", ".", "append", "(", "0", ")", "\n", "end_positions", ".", "append", "(", "0", ")", "\n", "switches", ".", "append", "(", "0", ")", "\n", "answer_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "", "current_features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "paragraph_index", "=", "paragraph_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "start_position", "=", "start_positions", ",", "\n", "end_position", "=", "end_positions", ",", "\n", "switch", "=", "switches", ",", "\n", "answer_mask", "=", "answer_mask", ")", ")", "\n", "unique_id", "+=", "1", "\n", "", "", "features", ".", "append", "(", "current_features", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"# of features per paragraph: %.1f\"", "%", "(", "np", ".", "mean", "(", "truncated", ")", ")", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro._improve_answer_span": [[341, 376], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The SQuAD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in SQuAD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro._check_is_max_context": [[378, 413], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.DataLoader.MyDataset.__init__": [[7, 34], ["torch.cat", "range", "numpy.random.permutation", "torch.arange", "torch.utils.data.DataLoader.MyDataset.input_ids.size", "torch.cat", "torch.utils.data.DataLoader.MyDataset.input_ids.size", "range", "len", "torch.utils.data.DataLoader.MyDataset.input_ids.size", "i.squeeze", "indices2.append", "indices1.append", "len", "i.squeeze", "zip"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "\n", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "switches", "=", "None", ",", "answer_mask", "=", "None", ",", "\n", "is_training", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "input_ids", ",", "self", ".", "input_mask", ",", "self", ".", "segment_ids", "=", "[", "torch", ".", "cat", "(", "[", "i", ".", "squeeze", "(", "0", ")", "for", "i", "in", "input", "]", ",", "0", ")", "for", "input", "in", "[", "input_ids", ",", "input_mask", ",", "segment_ids", "]", "]", "\n", "self", ".", "is_training", "=", "is_training", "\n", "\n", "if", "is_training", ":", "\n", "            ", "self", ".", "start_positions", ",", "self", ".", "end_positions", ",", "self", ".", "switches", ",", "self", ".", "answer_mask", "=", "[", "torch", ".", "cat", "(", "[", "i", ".", "squeeze", "(", "0", ")", "for", "i", "in", "input", "]", ",", "0", ")", "for", "input", "in", "[", "start_positions", ",", "end_positions", ",", "switches", ",", "answer_mask", "]", "]", "\n", "indices1", ",", "indices2", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "input_ids", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "switch", "=", "[", "s", "for", "(", "s", ",", "m", ")", "in", "zip", "(", "self", ".", "switches", "[", "i", "]", ",", "self", ".", "answer_mask", "[", "i", "]", ")", "if", "m", "==", "1", "]", "\n", "if", "3", "in", "switch", ":", "\n", "                    ", "indices2", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                    ", "indices1", ".", "append", "(", "i", ")", "\n", "", "", "indices", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "indices2", ")", ")", ")", "\n", "indices2", "=", "[", "indices2", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "self", ".", "positive_indices", "=", "indices1", "\n", "self", ".", "negative_indices", "=", "indices2", "\n", "self", ".", "negative_indices_offset", "=", "0", "\n", "self", ".", "length", "=", "2", "*", "len", "(", "self", ".", "positive_indices", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "example_index", "=", "torch", ".", "arange", "(", "self", ".", "input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "self", ".", "length", "=", "self", ".", "input_ids", ".", "size", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.DataLoader.MyDataset.__len__": [[35, 37], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.DataLoader.MyDataset.__getitem__": [[38, 54], ["len", "numpy.random.permutation", "int", "range", "int", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "self", ".", "is_training", ":", "\n", "            ", "if", "idx", "%", "2", "==", "0", ":", "\n", "                ", "idx", "=", "self", ".", "positive_indices", "[", "int", "(", "idx", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "negative_indices_offset", "==", "len", "(", "self", ".", "positive_indices", ")", ":", "\n", "                    ", "indices", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "self", ".", "negative_indices", ")", ")", ")", "\n", "self", ".", "negative_indices", "=", "[", "self", ".", "negative_indices", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "self", ".", "negative_indices_offset", "=", "0", "\n", "", "else", ":", "\n", "                    ", "self", ".", "negative_indices_offset", "+=", "1", "\n", "", "idx", "=", "self", ".", "negative_indices", "[", "int", "(", "idx", "/", "2", ")", "]", "\n", "", "return", "[", "b", "[", "idx", "]", "for", "b", "in", "[", "self", ".", "input_ids", ",", "self", ".", "input_mask", ",", "self", ".", "segment_ids", ",", "\n", "self", ".", "start_positions", ",", "self", ".", "end_positions", ",", "self", ".", "switches", ",", "self", ".", "answer_mask", "]", "]", "\n", "", "return", "[", "b", "[", "idx", "]", "for", "b", "in", "[", "self", ".", "input_ids", ",", "self", ".", "input_mask", ",", "self", ".", "segment_ids", ",", "\n", "self", ".", "example_index", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.DataLoader.MyDataLoader.__init__": [[58, 85], ["torch.utils.data.DataLoader.__init__", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.DataLoader.MyDataset", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader.MyDataset", "torch.utils.data.SequentialSampler", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "features", ",", "batch_size", ",", "is_training", ")", ":", "\n", "        ", "all_input_ids", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "all_input_mask", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "all_segment_ids", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "all_start_positions", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "start_position", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "all_end_positions", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "end_position", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "all_switches", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "switch", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "all_answer_mask", "=", "[", "torch", ".", "tensor", "(", "[", "f", ".", "answer_mask", "for", "f", "in", "_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "_features", "in", "features", "]", "\n", "dataset", "=", "MyDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "\n", "all_start_positions", ",", "all_end_positions", ",", "all_switches", ",", "all_answer_mask", ",", "\n", "is_training", "=", "is_training", ")", "\n", "sampler", "=", "RandomSampler", "(", "dataset", ")", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "MyDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "\n", "is_training", "=", "is_training", ")", "\n", "sampler", "=", "SequentialSampler", "(", "dataset", ")", "\n", "\n", "", "super", "(", "MyDataLoader", ",", "self", ")", ".", "__init__", "(", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.FullTokenizer.__init__": [[104, 108], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.FullTokenizer.tokenize": [[109, 121], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "type", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "basic_done", "=", "False", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "if", "basic_done", ":", "\n", "        ", "assert", "type", "(", "text", ")", "==", "list", "\n", "", "else", ":", "\n", "        ", "text", "=", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n", "", "for", "token", "in", "text", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.FullTokenizer.convert_tokens_to_ids": [[122, 124], ["tokenization.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_tokens_to_ids"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_tokens_to_ids", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer.__init__": [[129, 136], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer.tokenize": [[137, 151], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util._run_strip_accents", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer._run_strip_accents": [[152, 162], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer._run_split_on_punc": [[163, 182], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.BasicTokenizer._clean_text": [[183, 195], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.__init__": [[200, 204], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.WordpieceTokenizer.tokenize": [[205, 257], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_to_unicode": [[26, 44], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.printable_text": [[46, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.load_vocab": [[69, 82], ["collections.OrderedDict", "open", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.convert_tokens_to_ids": [[84, 90], ["ids.append"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "    ", "ids", ".", "append", "(", "vocab", "[", "token", "]", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.whitespace_tokenize": [[92, 99], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization._is_whitespace": [[259, 269], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization._is_control": [[271, 281], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization._is_punctuation": [[283, 297], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.normalize_answer": [[9, 25], ["evaluation_script.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.f1_score": [[27, 48], ["evaluation_script.normalize_answer", "evaluation_script.normalize_answer", "normalize_answer.split", "normalize_answer.split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.normalize_answer", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "normalized_prediction", "=", "normalize_answer", "(", "prediction", ")", "\n", "normalized_ground_truth", "=", "normalize_answer", "(", "ground_truth", ")", "\n", "\n", "ZERO_METRIC", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "\n", "if", "normalized_prediction", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "if", "normalized_ground_truth", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "\n", "", "prediction_tokens", "=", "normalized_prediction", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalized_ground_truth", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.exact_match_score": [[50, 52], ["evaluation_script.normalize_answer", "evaluation_script.normalize_answer"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.normalize_answer", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "(", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.update_answer": [[53, 61], ["evaluation_script.exact_match_score", "evaluation_script.f1_score"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.exact_match_score", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.f1_score"], ["", "def", "update_answer", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "em", "=", "exact_match_score", "(", "prediction", ",", "gold", ")", "\n", "f1", ",", "prec", ",", "recall", "=", "f1_score", "(", "prediction", ",", "gold", ")", "\n", "metrics", "[", "'em'", "]", "+=", "em", "\n", "metrics", "[", "'f1'", "]", "+=", "f1", "\n", "metrics", "[", "'prec'", "]", "+=", "prec", "\n", "metrics", "[", "'recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.update_sp": [[62, 83], ["set", "set", "map", "map"], "function", ["None"], ["", "def", "update_sp", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "cur_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "prediction", ")", ")", "\n", "gold_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "gold", ")", ")", "\n", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "for", "e", "in", "cur_sp_pred", ":", "\n", "        ", "if", "e", "in", "gold_sp_pred", ":", "\n", "            ", "tp", "+=", "1", "\n", "", "else", ":", "\n", "            ", "fp", "+=", "1", "\n", "", "", "for", "e", "in", "gold_sp_pred", ":", "\n", "        ", "if", "e", "not", "in", "cur_sp_pred", ":", "\n", "            ", "fn", "+=", "1", "\n", "", "", "prec", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fp", ")", "if", "tp", "+", "fp", ">", "0", "else", "0.0", "\n", "recall", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fn", ")", "if", "tp", "+", "fn", ">", "0", "else", "0.0", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "if", "prec", "+", "recall", ">", "0", "else", "0.0", "\n", "em", "=", "1.0", "if", "fp", "+", "fn", "==", "0", "else", "0.0", "\n", "metrics", "[", "'sp_em'", "]", "+=", "em", "\n", "metrics", "[", "'sp_f1'", "]", "+=", "f1", "\n", "metrics", "[", "'sp_prec'", "]", "+=", "prec", "\n", "metrics", "[", "'sp_recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.eval": [[84, 104], ["len", "metrics.keys", "print", "open", "ujson.load", "open", "ujson.load", "evaluation_script.update_answer"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.update_answer"], ["", "def", "eval", "(", "prediction_file", ",", "gold_file", ")", ":", "\n", "    ", "with", "open", "(", "prediction_file", ")", "as", "f", ":", "\n", "        ", "prediction", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "gold_file", ")", "as", "f", ":", "\n", "        ", "gold", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "metrics", "=", "{", "'em'", ":", "0", ",", "'f1'", ":", "0", ",", "'prec'", ":", "0", ",", "'recall'", ":", "0", ",", "\n", "'sp_em'", ":", "0", ",", "'sp_f1'", ":", "0", ",", "'sp_prec'", ":", "0", ",", "'sp_recall'", ":", "0", ",", "\n", "'joint_em'", ":", "0", ",", "'joint_f1'", ":", "0", ",", "'joint_prec'", ":", "0", ",", "'joint_recall'", ":", "0", "}", "\n", "\n", "for", "dp", "in", "gold", ":", "\n", "        ", "cur_id", "=", "dp", "[", "'_id'", "]", "\n", "em", ",", "prec", ",", "recall", "=", "update_answer", "(", "\n", "metrics", ",", "prediction", "[", "'answer'", "]", "[", "cur_id", "]", ",", "dp", "[", "'answer'", "]", ")", "\n", "\n", "", "N", "=", "len", "(", "gold", ")", "\n", "for", "k", "in", "metrics", ".", "keys", "(", ")", ":", "\n", "        ", "metrics", "[", "k", "]", "/=", "N", "\n", "\n", "", "print", "(", "metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.analyze": [[105, 127], ["open", "ujson.load", "open", "ujson.load", "evaluation_script.update_answer", "print", "print", "input"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluation_script.update_answer"], ["", "def", "analyze", "(", "prediction_file", ",", "gold_file", ")", ":", "\n", "    ", "with", "open", "(", "prediction_file", ")", "as", "f", ":", "\n", "        ", "prediction", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "gold_file", ")", "as", "f", ":", "\n", "        ", "gold", "=", "json", ".", "load", "(", "f", ")", "\n", "", "metrics", "=", "{", "'em'", ":", "0", ",", "'f1'", ":", "0", ",", "'prec'", ":", "0", ",", "'recall'", ":", "0", ",", "\n", "'sp_em'", ":", "0", ",", "'sp_f1'", ":", "0", ",", "'sp_prec'", ":", "0", ",", "'sp_recall'", ":", "0", ",", "\n", "'joint_em'", ":", "0", ",", "'joint_f1'", ":", "0", ",", "'joint_prec'", ":", "0", ",", "'joint_recall'", ":", "0", "}", "\n", "\n", "for", "dp", "in", "gold", ":", "\n", "        ", "cur_id", "=", "dp", "[", "'_id'", "]", "\n", "\n", "em", ",", "prec", ",", "recall", "=", "update_answer", "(", "\n", "metrics", ",", "prediction", "[", "'answer'", "]", "[", "cur_id", "]", ",", "dp", "[", "'answer'", "]", ")", "\n", "if", "(", "prec", "+", "recall", "==", "0", ")", ":", "\n", "            ", "f1", "=", "0", "\n", "", "else", ":", "\n", "            ", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "\n", "\n", "", "print", "(", "dp", "[", "'answer'", "]", ",", "prediction", "[", "'answer'", "]", "[", "cur_id", "]", ")", "\n", "print", "(", "f1", ",", "em", ")", "\n", "a", "=", "input", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.SquadExample.__init__": [[9, 28], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "paragraph_indices", "=", "None", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "all_answers", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "switch", "=", "None", ")", ":", "\n", "        ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "paragraph_indices", "=", "paragraph_indices", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "all_answers", "=", "all_answers", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "switch", "=", "switch", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.SquadExample.__str__": [[29, 31], ["prepro_util.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.SquadExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.SquadExample.__repr__": [[32, 35], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"question: \"", "+", "self", ".", "question_text", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.InputFeatures.__init__": [[38, 69], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "paragraph_index", "=", "None", ",", "\n", "doc_span_index", "=", "None", ",", "\n", "doc_tokens", "=", "None", ",", "\n", "tokens", "=", "None", ",", "\n", "token_to_orig_map", "=", "None", ",", "\n", "token_is_max_context", "=", "None", ",", "\n", "input_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "segment_ids", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "switch", "=", "None", ",", "\n", "answer_mask", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "paragraph_index", "=", "paragraph_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "switch", "=", "switch", "\n", "self", ".", "answer_mask", "=", "answer_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util._run_strip_accents": [[71, 81], ["unicodedata.normalize", "unicodedata.category", "output.append"], "function", ["None"], ["", "", "def", "_run_strip_accents", "(", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.find_span_from_text": [[83, 124], ["enumerate", "token.replace().replace.replace().replace", "len", "process.append", "answers.append", "answer.startswith", "answer.startswith", "len", "token.replace().replace.replace", "len", "len", "spans.append", "answer.replace", "spans.append", "len", "len", "len", "len"], "function", ["None"], ["", "def", "find_span_from_text", "(", "context", ",", "tokens", ",", "answer", ")", ":", "\n", "    ", "assert", "answer", "in", "context", "\n", "\n", "offset", "=", "0", "\n", "spans", "=", "[", "]", "\n", "scanning", "=", "None", "\n", "process", "=", "[", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "token", "=", "token", ".", "replace", "(", "' ##'", ",", "''", ")", ".", "replace", "(", "'##'", ",", "''", ")", "\n", "while", "context", "[", "offset", ":", "offset", "+", "len", "(", "token", ")", "]", "!=", "token", ":", "\n", "            ", "offset", "+=", "1", "\n", "if", "offset", ">=", "len", "(", "context", ")", ":", "\n", "                ", "break", "\n", "", "", "if", "scanning", "is", "not", "None", ":", "\n", "            ", "end", "=", "offset", "+", "len", "(", "token", ")", "\n", "if", "answer", ".", "startswith", "(", "context", "[", "scanning", "[", "-", "1", "]", "[", "-", "1", "]", ":", "end", "]", ")", ":", "\n", "                ", "if", "context", "[", "scanning", "[", "-", "1", "]", "[", "-", "1", "]", ":", "end", "]", "==", "answer", ":", "\n", "                    ", "span", "=", "(", "scanning", "[", "0", "]", "[", "0", "]", ",", "i", ",", "scanning", "[", "0", "]", "[", "1", "]", ")", "\n", "spans", ".", "append", "(", "span", ")", "\n", "", "elif", "len", "(", "context", "[", "scanning", "[", "-", "1", "]", "[", "-", "1", "]", ":", "end", "]", ")", ">=", "len", "(", "answer", ")", ":", "\n", "                    ", "scanning", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "scanning", "=", "None", "\n", "", "", "if", "scanning", "is", "None", "and", "answer", ".", "startswith", "(", "token", ")", ":", "\n", "            ", "if", "token", "==", "answer", ":", "\n", "                ", "spans", ".", "append", "(", "(", "i", ",", "i", ",", "offset", ")", ")", "\n", "", "if", "token", "!=", "answer", ":", "\n", "                ", "scanning", "=", "[", "(", "i", ",", "offset", ")", "]", "\n", "", "", "offset", "+=", "len", "(", "token", ")", "\n", "if", "offset", ">=", "len", "(", "context", ")", ":", "\n", "            ", "break", "\n", "", "process", ".", "append", "(", "(", "token", ",", "offset", ",", "scanning", ",", "spans", ")", ")", "\n", "\n", "", "answers", "=", "[", "]", "\n", "\n", "for", "word_start", ",", "word_end", ",", "span", "in", "spans", ":", "\n", "        ", "assert", "context", "[", "span", ":", "span", "+", "len", "(", "answer", ")", "]", "==", "answer", "or", "''", ".", "join", "(", "tokens", "[", "word_start", ":", "word_end", "+", "1", "]", ")", ".", "replace", "(", "'##'", ",", "''", ")", "!=", "answer", ".", "replace", "(", "' '", ",", "''", ")", "\n", "answers", ".", "append", "(", "{", "'text'", ":", "answer", ",", "'answer_start'", ":", "span", ",", "'word_start'", ":", "word_start", ",", "'word_end'", ":", "word_end", "}", ")", "\n", "\n", "", "return", "answers", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.detect_span": [[125, 166], ["prepro_util.find_span_from_text", "len", "orig_answer_texts.append", "start_positions.append", "end_positions.append", "switches.append", "tokenization.whitespace_tokenize", "actual_text.replace().find", "print", "cleaned_answer_text.replace", "actual_text.replace"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.prepro_util.find_span_from_text", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.tokenization.whitespace_tokenize"], ["", "def", "detect_span", "(", "_answers", ",", "context", ",", "doc_tokens", ",", "char_to_word_offset", ")", ":", "\n", "    ", "orig_answer_texts", "=", "[", "]", "\n", "start_positions", "=", "[", "]", "\n", "end_positions", "=", "[", "]", "\n", "switches", "=", "[", "]", "\n", "\n", "answers", "=", "[", "]", "\n", "for", "answer", "in", "_answers", ":", "\n", "        ", "answers", "+=", "find_span_from_text", "(", "context", ",", "doc_tokens", ",", "answer", "[", "'text'", "]", ")", "\n", "\n", "", "for", "answer", "in", "answers", ":", "\n", "        ", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "\n", "switch", "=", "0", "\n", "if", "'word_start'", "in", "answer", "and", "'word_end'", "in", "answer", ":", "\n", "            ", "start_position", "=", "answer", "[", "'word_start'", "]", "\n", "end_position", "=", "answer", "[", "'word_end'", "]", "\n", "", "else", ":", "\n", "            ", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", ".", "replace", "(", "' ##'", ",", "''", ")", ".", "replace", "(", "'##'", ",", "''", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "replace", "(", "' '", ",", "''", ")", ".", "find", "(", "cleaned_answer_text", ".", "replace", "(", "' '", ",", "''", ")", ")", "==", "-", "1", ":", "\n", "            ", "print", "(", "\"Could not find answer: '%s' vs. '%s'\"", "%", "(", "actual_text", ",", "cleaned_answer_text", ")", ")", "\n", "\n", "", "orig_answer_texts", ".", "append", "(", "orig_answer_text", ")", "\n", "start_positions", ".", "append", "(", "start_position", ")", "\n", "end_positions", ".", "append", "(", "end_position", ")", "\n", "switches", ".", "append", "(", "switch", ")", "\n", "\n", "\n", "", "return", "orig_answer_texts", ",", "switches", ",", "start_positions", ",", "end_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.save": [[15, 20], ["os.path.join", "open", "print", "json.dump"], "function", ["None"], ["def", "save", "(", "data_dir", ",", "data", ",", "data_type", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "data_type", ")", ")", "\n", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "print", "(", "\"Saving {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "json", ".", "dump", "(", "{", "'data'", ":", "data", "}", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.main": [[21, 27], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "split_nq.prepro_naturalquestions"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.prepro_naturalquestions"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'data_dir'", ",", "type", "=", "str", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "for", "data_type", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "        ", "prepro_naturalquestions", "(", "args", ".", "data_dir", ",", "data_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.prepro_naturalquestions": [[28, 70], ["range", "print", "os.path.join", "print", "len", "split_nq.save", "gzip.GzipFile", "fin.read", "fin.read.decode().split", "tqdm.tqdm", "numpy.random.seed", "numpy.random.permutation", "split_nq.save", "split_nq.save", "NotImplementedError", "str().zfill", "line.strip", "json.loads", "range", "fin.read.decode", "len", "short_data_list.append", "len", "str", "len", "answers.append", "list", "set"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.save", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.save", "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.split_nq.save"], ["", "", "def", "prepro_naturalquestions", "(", "data_dir", ",", "data_type", ")", ":", "\n", "    ", "short_data_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "50", "if", "data_type", "==", "'train'", "else", "5", ")", ":", "\n", "        ", "filename", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'v1.0'", ",", "data_type", ",", "'nq-{}-{}.jsonl.gz'", ".", "format", "(", "data_type", ",", "str", "(", "i", ")", ".", "zfill", "(", "2", ")", ")", ")", "\n", "print", "(", "\"Preprocessing {}\"", ".", "format", "(", "filename", ")", ")", "\n", "with", "gzip", ".", "GzipFile", "(", "filename", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "json_bytes", "=", "fin", ".", "read", "(", ")", "\n", "lines", "=", "json_bytes", ".", "decode", "(", "'utf-8'", ")", ".", "split", "(", "'\\n'", ")", "\n", "lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "lines", "=", "[", "line", "for", "line", "in", "lines", "if", "len", "(", "line", ")", ">", "0", "]", "\n", "for", "line", "in", "tqdm", "(", "lines", ")", ":", "\n", "                ", "d", "=", "json", ".", "loads", "(", "line", ")", "\n", "question", "=", "d", "[", "'question_text'", "]", "\n", "document", "=", "[", "t", "[", "'token'", "]", "for", "t", "in", "d", "[", "'document_tokens'", "]", "]", "\n", "answers", "=", "[", "]", "\n", "for", "annotation", "in", "d", "[", "'annotations'", "]", ":", "\n", "                    ", "for", "short_annotation", "in", "annotation", "[", "'short_answers'", "]", ":", "\n", "                        ", "if", "short_annotation", "[", "'end_token'", "]", "-", "short_annotation", "[", "'start_token'", "]", ">", "5", ":", "\n", "                            ", "continue", "\n", "", "answer", "=", "document", "[", "short_annotation", "[", "'start_token'", "]", ":", "short_annotation", "[", "'end_token'", "]", "]", "\n", "answers", ".", "append", "(", "\" \"", ".", "join", "(", "answer", ")", ")", "\n", "", "", "if", "len", "(", "answers", ")", ">", "0", ":", "\n", "                    ", "short_data_list", ".", "append", "(", "{", "\n", "'id'", ":", "d", "[", "'example_id'", "]", ",", "\n", "'question'", ":", "question", ",", "\n", "'answers'", ":", "list", "(", "set", "(", "answers", ")", ")", ",", "\n", "'orig_doc_title'", ":", "d", "[", "'document_title'", "]", "\n", "}", ")", "\n", "\n", "", "", "", "", "print", "(", "len", "(", "short_data_list", ")", ")", "\n", "\n", "if", "data_type", "==", "'dev'", ":", "\n", "        ", "save", "(", "data_dir", ",", "short_data_list", ",", "'test'", ")", "\n", "", "elif", "data_type", "==", "'train'", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "1995", ")", "\n", "indices", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "short_data_list", ")", ")", ")", "\n", "short_data_list", "=", "[", "short_data_list", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "n_dev", "=", "8757", "# same number of dev data as Lee et al (ACL 2019)", "\n", "save", "(", "data_dir", ",", "short_data_list", "[", ":", "n_dev", "]", ",", "'dev'", ")", "\n", "save", "(", "data_dir", ",", "short_data_list", "[", "n_dev", ":", "]", ",", "'train'", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluate_qa.write_predictions": [[20, 229], ["collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "example_index_to_features[].append", "tqdm.tqdm", "enumerate", "sorted", "logger.info", "logger.info", "collections.OrderedDict.values", "collections.OrderedDict.values", "zip", "enumerate", "_NbestPrediction", "enumerate", "enumerate", "sorted", "evaluate_qa._compute_softmax", "enumerate", "evaluate_qa.write_predictions.get_nbest_json"], "function", ["home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluate_qa._compute_softmax"], ["def", "write_predictions", "(", "logger", ",", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "verbose_logging", ",", "\n", "write_prediction", "=", "True", ",", "n_paragraphs", "=", "None", ")", ":", "\n", "\n", "    ", "\"\"\"Write final predictions to the json file.\"\"\"", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "\n", "[", "\"paragraph_index\"", ",", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"logit\"", ",", "\"no_answer_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "if", "verbose_logging", ":", "\n", "        ", "all_examples", "=", "tqdm", "(", "enumerate", "(", "all_examples", ")", ")", "\n", "", "else", ":", "\n", "        ", "all_examples", "=", "enumerate", "(", "all_examples", ")", "\n", "\n", "", "for", "(", "example_index", ",", "example", ")", "in", "all_examples", ":", "\n", "        ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "if", "len", "(", "features", ")", "==", "0", "and", "n_paragraphs", "is", "None", ":", "\n", "            ", "pred", "=", "_NbestPrediction", "(", "\n", "text", "=", "\"empty\"", ",", "\n", "logit", "=", "-", "1000", ",", "\n", "no_answer_logit", "=", "1000", ")", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "(", "\"empty\"", ",", "example", ".", "all_answers", ")", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "[", "pred", "]", "\n", "continue", "\n", "\n", "", "prelim_predictions", "=", "[", "]", "\n", "yn_predictions", "=", "[", "]", "\n", "\n", "if", "n_paragraphs", "is", "None", ":", "\n", "            ", "results", "=", "sorted", "(", "enumerate", "(", "features", ")", ",", "\n", "key", "=", "lambda", "f", ":", "unique_id_to_result", "[", "f", "[", "1", "]", ".", "unique_id", "]", ".", "switch", "[", "3", "]", ")", "[", ":", "1", "]", "\n", "", "else", ":", "\n", "            ", "results", "=", "enumerate", "(", "features", ")", "\n", "", "for", "(", "feature_index", ",", "feature", ")", "in", "results", ":", "\n", "            ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "scores", "=", "[", "]", "\n", "start_logits", "=", "result", ".", "start_logits", "[", ":", "len", "(", "feature", ".", "tokens", ")", "]", "\n", "end_logits", "=", "result", ".", "end_logits", "[", ":", "len", "(", "feature", ".", "tokens", ")", "]", "\n", "for", "(", "i", ",", "s", ")", "in", "enumerate", "(", "start_logits", ")", ":", "\n", "                ", "for", "(", "j", ",", "e", ")", "in", "enumerate", "(", "end_logits", "[", "i", ":", "i", "+", "10", "]", ")", ":", "\n", "                    ", "scores", ".", "append", "(", "(", "(", "i", ",", "i", "+", "j", ")", ",", "s", "+", "e", ")", ")", "\n", "\n", "", "", "scores", "=", "sorted", "(", "scores", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "cnt", "=", "0", "\n", "for", "(", "start_index", ",", "end_index", ")", ",", "score", "in", "scores", ":", "\n", "                ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                    ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                    ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                    ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "paragraph_index", "=", "feature", ".", "paragraph_index", ",", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "logit", "=", "-", "result", ".", "switch", "[", "3", "]", ",", "#score,", "\n", "no_answer_logit", "=", "result", ".", "switch", "[", "3", "]", ")", ")", "\n", "if", "n_paragraphs", "is", "None", ":", "\n", "                    ", "if", "write_predictions", "and", "len", "(", "prelim_predictions", ")", ">=", "n_best_size", ":", "\n", "                        ", "break", "\n", "", "elif", "not", "write_predictions", ":", "\n", "                        ", "break", "\n", "", "", "cnt", "+=", "1", "\n", "\n", "", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "x", ".", "logit", ",", "\n", "reverse", "=", "True", ")", "\n", "no_answer_logit", "=", "result", ".", "switch", "[", "3", "]", "\n", "\n", "def", "get_nbest_json", "(", "prelim_predictions", ")", ":", "\n", "\n", "            ", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "                ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "                    ", "break", "\n", "\n", "", "if", "pred", ".", "start_index", "==", "pred", ".", "end_index", "==", "-", "1", ":", "\n", "                    ", "final_text", "=", "\"yes\"", "\n", "", "elif", "pred", ".", "start_index", "==", "pred", ".", "end_index", "==", "-", "2", ":", "\n", "                    ", "final_text", "=", "\"no\"", "\n", "", "else", ":", "\n", "                    ", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "\n", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "feature", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ",", "logger", ",", "verbose_logging", ")", "\n", "\n", "\n", "", "if", "final_text", "in", "seen_predictions", ":", "\n", "                    ", "continue", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "logit", "=", "pred", ".", "logit", ",", "\n", "no_answer_logit", "=", "no_answer_logit", ")", ")", "\n", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "if", "not", "nbest", ":", "\n", "                ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "text", "=", "\"empty\"", ",", "logit", "=", "0.0", ",", "no_answer_logit", "=", "no_answer_logit", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "for", "entry", "in", "nbest", ":", "\n", "                ", "total_scores", ".", "append", "(", "entry", ".", "logit", ")", "\n", "\n", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "                ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "'text'", "]", "=", "entry", ".", "text", "\n", "output", "[", "'probability'", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "'logit'", "]", "=", "entry", ".", "logit", "\n", "output", "[", "'no_answer_logit'", "]", "=", "entry", ".", "no_answer_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "return", "nbest_json", "\n", "", "if", "n_paragraphs", "is", "None", ":", "\n", "            ", "nbest_json", "=", "get_nbest_json", "(", "prelim_predictions", ")", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "(", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", ",", "example", ".", "all_answers", ")", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "", "else", ":", "\n", "            ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "[", "]", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "[", "]", "\n", "for", "n", "in", "n_paragraphs", ":", "\n", "                ", "nbest_json", "=", "get_nbest_json", "(", "[", "pred", "for", "pred", "in", "prelim_predictions", "if", "pred", ".", "paragraph_index", "<", "n", "]", ")", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", ".", "append", "(", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", ")", "\n", "", "all_predictions", "[", "example", ".", "qas_id", "]", ".", "append", "(", "example", ".", "all_answers", ")", "\n", "\n", "", "", "if", "write_prediction", ":", "\n", "        ", "logger", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "\n", "with", "open", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "open", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "if", "n_paragraphs", "is", "None", ":", "\n", "        ", "f1s", ",", "ems", "=", "[", "]", ",", "[", "]", "\n", "for", "prediction", ",", "groundtruth", "in", "all_predictions", ".", "values", "(", ")", ":", "\n", "            ", "if", "len", "(", "groundtruth", ")", "==", "0", ":", "\n", "                ", "f1s", ".", "append", "(", "0", ")", "\n", "ems", ".", "append", "(", "0", ")", "\n", "continue", "\n", "", "f1s", ".", "append", "(", "max", "(", "[", "f1_score", "(", "prediction", ",", "gt", ")", "[", "0", "]", "for", "gt", "in", "groundtruth", "]", ")", ")", "\n", "ems", ".", "append", "(", "max", "(", "[", "exact_match_score", "(", "prediction", ",", "gt", ")", "for", "gt", "in", "groundtruth", "]", ")", ")", "\n", "", "final_f1", ",", "final_em", "=", "np", ".", "mean", "(", "f1s", ")", ",", "np", ".", "mean", "(", "ems", ")", "\n", "", "else", ":", "\n", "        ", "f1s", ",", "ems", "=", "[", "[", "]", "for", "_", "in", "n_paragraphs", "]", ",", "[", "[", "]", "for", "_", "in", "n_paragraphs", "]", "\n", "for", "predictions", "in", "all_predictions", ".", "values", "(", ")", ":", "\n", "            ", "groundtruth", "=", "predictions", "[", "-", "1", "]", "\n", "predictions", "=", "predictions", "[", ":", "-", "1", "]", "\n", "if", "len", "(", "groundtruth", ")", "==", "0", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "n_paragraphs", ")", ")", ":", "\n", "                    ", "f1s", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "ems", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "", "continue", "\n", "", "for", "i", ",", "prediction", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "f1s", "[", "i", "]", ".", "append", "(", "max", "(", "[", "f1_score", "(", "prediction", ",", "gt", ")", "[", "0", "]", "for", "gt", "in", "groundtruth", "]", ")", ")", "\n", "ems", "[", "i", "]", ".", "append", "(", "max", "(", "[", "exact_match_score", "(", "prediction", ",", "gt", ")", "for", "gt", "in", "groundtruth", "]", ")", ")", "\n", "", "", "for", "n", ",", "f1s_", ",", "ems_", "in", "zip", "(", "n_paragraphs", ",", "f1s", ",", "ems", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n=%d\\tF1 %.2f\\tEM %.2f\"", "%", "(", "n", ",", "np", ".", "mean", "(", "f1s_", ")", "*", "100", ",", "np", ".", "mean", "(", "ems_", ")", "*", "100", ")", ")", "\n", "", "final_f1", ",", "final_em", "=", "np", ".", "mean", "(", "f1s", "[", "-", "1", "]", ")", ",", "np", ".", "mean", "(", "ems", "[", "-", "1", "]", ")", "\n", "", "return", "final_em", ",", "final_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluate_qa.get_final_text": [[230, 298], ["tokenization.BasicTokenizer", "tok_text.find", "evaluate_qa.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "logger", ",", "verbose_logging", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.shmsw25_qa-hard-em.None.evaluate_qa._compute_softmax": [[300, 321], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "    ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "            ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "        ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "", ""]]}