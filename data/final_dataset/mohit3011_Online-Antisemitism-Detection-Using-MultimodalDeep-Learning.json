{"home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.__init__": [[30, 86], ["torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.Module.__init__", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "transformers.BertModel.from_pretrained", "transformers.RobertaModel.from_pretrained", "torchvision.models.resnet152", "main.multiModel.img_model.layer2[].register_forward_hook", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Linear", "torch.Linear", "torch.Linear", "torchvision.models.densenet161", "main.multiModel.img_model.features.denseblock4.register_forward_hook", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_labels", "=", "2", ",", "config", "=", "None", ",", "txt_flag", "=", "0", ",", "img_flag", "=", "2", ",", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", ")", ")", ":", "\n", "        ", "''' \n        Purpose: Class for creating the proposed multimodal architecture\n        Input:  num_labels: Numbers of labels (Binary==2, Multiclass==4)\n                config: Config file for the transformer models (if available)\n                txt_flag : Flag for determining the Tranformer model to be used. 0: BERT, 1: RoBERTa\n                img_flag: Flag for detemining the Image model to be used. 0: RESNET, 2: DenseNet\n        '''", "\n", "\n", "super", "(", "multiModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Common layers", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm1d", "(", "384", ",", "momentum", "=", "0.99", ")", "\n", "self", ".", "dense1", "=", "nn", ".", "Linear", "(", "in_features", "=", "384", ",", "out_features", "=", "128", ")", "#Add ReLu in forward loop", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "0.2", ")", "\n", "self", ".", "dense2", "=", "nn", ".", "Linear", "(", "in_features", "=", "128", ",", "out_features", "=", "num_labels", ")", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "imagegate", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n", "self", ".", "encoder_dense_1", "=", "nn", ".", "Linear", "(", "in_features", "=", "2816", ",", "out_features", "=", "768", ")", "\n", "self", ".", "encoder_dense_2", "=", "nn", ".", "Linear", "(", "in_features", "=", "768", ",", "out_features", "=", "384", ")", "\n", "\n", "self", ".", "decoder_dense_1", "=", "nn", ".", "Linear", "(", "in_features", "=", "384", ",", "out_features", "=", "768", ")", "\n", "self", ".", "decoder_dense_2", "=", "nn", ".", "Linear", "(", "in_features", "=", "768", ",", "out_features", "=", "2816", ")", "\n", "\n", "\n", "self", ".", "s_text_vector", "=", "[", "]", "\n", "self", ".", "s_image_vector", "=", "[", "]", "\n", "self", ".", "h_text_vector", "=", "[", "]", "\n", "self", ".", "h_image_vector", "=", "[", "]", "\n", "self", ".", "imagegate", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "intermediate_layer_output", "=", "{", "'output'", ":", "None", "}", "\n", "\n", "self", ".", "s_text_vector", "=", "np", ".", "asarray", "(", "np", ".", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ",", "768", ")", ",", "dtype", "=", "'int32'", ")", "\n", "self", ".", "s_image_vector", "=", "np", ".", "asarray", "(", "np", ".", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ",", "768", ")", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "# Txt model", "\n", "if", "txt_flag", "==", "0", ":", "\n", "            ", "self", ".", "txt_model", "=", "BertModel", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "config", "=", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "txt_model", "=", "RobertaModel", ".", "from_pretrained", "(", "'roberta-base'", ",", "config", "=", "config", ")", "\n", "\n", "# image model", "\n", "", "if", "img_flag", "==", "0", ":", "\n", "            ", "self", ".", "img_model", "=", "torchvision", ".", "models", ".", "resnet152", "(", "pretrained", "=", "True", ")", "\n", "self", ".", "img_model", ".", "layer2", "[", "-", "1", "]", ".", "register_forward_hook", "(", "self", ".", "setIntermediateValue", ")", "#Adding a forward hook for getting intermediate output", "\n", "self", ".", "intermediate_img_model", "=", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "28", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "num_ftrs", "=", "self", ".", "img_model", ".", "fc", ".", "in_features", "\n", "self", ".", "img_model", ".", "fc", "=", "nn", ".", "Linear", "(", "num_ftrs", ",", "768", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "img_model", "=", "torchvision", ".", "models", ".", "densenet161", "(", "pretrained", "=", "True", ")", "\n", "self", ".", "img_model", ".", "features", ".", "denseblock4", ".", "register_forward_hook", "(", "self", ".", "setIntermediateValue", ")", "#Adding a forward hook for getting intermediate output", "\n", "\n", "self", ".", "intermediate_img_model", "=", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "7", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "num_ftrs", "=", "self", ".", "img_model", ".", "classifier", ".", "in_features", "\n", "self", ".", "img_model", ".", "classifier", "=", "nn", ".", "Linear", "(", "num_ftrs", ",", "768", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.setIntermediateValue": [[87, 89], ["None"], "methods", ["None"], ["", "", "def", "setIntermediateValue", "(", "self", ",", "module", ",", "input", ",", "output", ")", ":", "\n", "        ", "self", ".", "intermediate_layer_output", "[", "'output'", "]", "=", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.encoder": [[90, 96], ["main.multiModel.encoder_dense_1", "main.multiModel.encoder_dense_2"], "methods", ["None"], ["", "def", "encoder", "(", "self", ",", "combined_features", ")", ":", "\n", "\n", "        ", "encoded_vector_dense_1", "=", "self", ".", "encoder_dense_1", "(", "combined_features", ")", "\n", "encoded_feature_vector", "=", "self", ".", "encoder_dense_2", "(", "encoded_vector_dense_1", ")", "\n", "\n", "return", "encoded_feature_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.decoder": [[97, 103], ["main.multiModel.decoder_dense_1", "main.multiModel.decoder_dense_2"], "methods", ["None"], ["", "def", "decoder", "(", "self", ",", "encoded_vector", ")", ":", "\n", "\n", "        ", "decoded_vector_dense_1", "=", "self", ".", "decoder_dense_1", "(", "encoded_vector", ")", "\n", "decoded_features", "=", "self", ".", "decoder_dense_2", "(", "decoded_vector_dense_1", ")", "\n", "\n", "return", "decoded_features", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.forward": [[104, 138], ["inputs[].long", "inputs[].long", "main.multiModel.img_model", "main.multiModel.intermediate_img_model", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "torch.flatten().to", "main.multiModel.imagegate", "main.multiModel.imagegate", "main.multiModel.encoder", "main.multiModel.decoder", "main.multiModel.bn", "torch.relu", "torch.relu", "torch.relu", "main.multiModel.dropout", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "inputs[].long().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "main.multiModel.dense1", "main.multiModel.dense2", "main.multiModel.txt_model", "main.multiModel.txt_model", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "inputs[].long", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.encoder", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.main.multiModel.decoder"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "imgs", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "text_input_ids_in", "=", "inputs", "[", ":", ",", "0", ",", ":", "]", ".", "long", "(", ")", "\n", "text_input_masks_in", "=", "inputs", "[", ":", ",", "1", ",", ":", "]", ".", "long", "(", ")", "\n", "\n", "if", "txt_flag", "==", "0", ":", "\n", "            ", "text_input_token_in", "=", "inputs", "[", ":", ",", "2", ",", ":", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "text_embedding_layer", "=", "self", ".", "txt_model", "(", "text_input_ids_in", ",", "attention_mask", "=", "text_input_masks_in", ",", "token_type_ids", "=", "text_input_token_in", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "text_embedding_layer", "=", "self", ".", "txt_model", "(", "text_input_ids_in", ",", "attention_mask", "=", "text_input_masks_in", ")", "[", "0", "]", "#RoBERTa doesn't require token_ids", "\n", "\n", "", "text_cls_token", "=", "text_embedding_layer", "[", ":", ",", "0", ",", ":", "]", "\n", "#print(\"text_cls_token.shape: \", text_cls_token.shape)", "\n", "\n", "img_features", "=", "self", ".", "img_model", "(", "imgs", ")", "\n", "intermediate_pooled_img_features", "=", "self", ".", "intermediate_img_model", "(", "self", ".", "intermediate_layer_output", "[", "'output'", "]", ")", "\n", "intermediate_img_features", "=", "torch", ".", "flatten", "(", "intermediate_pooled_img_features", ",", "start_dim", "=", "1", ",", "end_dim", "=", "-", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "#print(\"intermediate_features.shape :\", intermediate_features.shape)", "\n", "\n", "intermediate_features", "=", "self", ".", "imagegate", "(", "torch", ".", "cat", "(", "(", "text_cls_token", ",", "intermediate_img_features", ")", ",", "1", ")", ")", "\n", "#print(intermediate_features.shape)", "\n", "final_features", "=", "self", ".", "imagegate", "(", "torch", ".", "cat", "(", "(", "torch", ".", "cat", "(", "(", "text_cls_token", ",", "intermediate_features", ")", ",", "1", ")", ",", "img_features", ")", ",", "1", ")", ")", "\n", "\n", "\n", "encoded_vector", "=", "self", ".", "encoder", "(", "final_features", ")", "\n", "\n", "decoded_features", "=", "self", ".", "decoder", "(", "encoded_vector", ")", "\n", "\n", "X", "=", "self", ".", "bn", "(", "encoded_vector", ")", "\n", "X", "=", "F", ".", "relu", "(", "self", ".", "dense1", "(", "X", ")", ")", "\n", "X", "=", "self", ".", "dropout", "(", "X", ")", "\n", "X", "=", "F", ".", "log_softmax", "(", "self", ".", "dense2", "(", "X", ")", ")", "\n", "\n", "return", "X", ",", "final_features", ",", "decoded_features", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.experiment_check.print_configuration": [[4, 29], ["print"], "function", ["None"], ["def", "print_configuration", "(", "img_flag", ",", "multilabel", ",", "txt_flag", ",", "ocr_flag", ")", ":", "\n", "\n", "\t", "if", "img_flag", "==", "0", ":", "\n", "\t\t", "im_net", "=", "\"resnet152\"", "\n", "", "elif", "img_flag", "==", "1", ":", "\n", "\t\t", "im_net", "=", "\"resnext\"", "\n", "", "else", ":", "\n", "\t\t", "im_net", "=", "\"densenet161\"", "\n", "\n", "", "if", "multilabel", "==", "1", ":", "\n", "\t\t", "mul", "=", "\"(multilabel)\"", "\n", "", "else", ":", "\n", "\t\t", "mul", "=", "\"(binary)\"", "\n", "\n", "", "if", "txt_flag", "==", "0", ":", "\n", "\t\t", "tx_net", "=", "\"BERT\"", "\n", "", "else", ":", "\n", "\t\t", "tx_net", "=", "\"RoBERTa\"", "\n", "\n", "", "if", "ocr_flag", "==", "1", ":", "\n", "\t\t", "modality", "=", "\"Image_OCR_Text\"", "\n", "", "else", ":", "\n", "\t\t", "modality", "=", "\"Image_Text\"", "\n", "\n", "", "print", "(", "\"Configuration is\"", ",", "modality", ",", "im_net", ",", "tx_net", ",", "mul", ")", "", "", ""]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.prepare_combined_dataset_error.make_bert_input_w_OCR": [[8, 47], ["range", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.concatenate", "numpy.concatenate", "len", "tokenizer.encode_plus", "np.asarray.append", "np.asarray.append", "np.asarray.append"], "function", ["None"], ["def", "make_bert_input_w_OCR", "(", "text_data", ",", "ocr_data", ",", "max_len", ",", "tokenizer", ")", ":", "\n", "\t", "''' \n    Purpose: This function encodes the text along with OCR using the BERT encoder.\n    Input:  text_data: Text data\n            ocr_data: OCR Text data\n            max_len : Maximum length of the sequence\n\t\t\ttokenizer: BERT tokenizer from Huggingface\n    Output: new_data: [input_ids,attention_masks,token_ids]\n\t'''", "\n", "input_ids", "=", "[", "]", "\n", "attention_masks", "=", "[", "]", "\n", "token_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "text_data", ")", ")", ":", "\n", "\t\t", "encoded_dict", "=", "tokenizer", ".", "encode_plus", "(", "\n", "text_data", "[", "i", "]", ",", "# Sentence to encode.", "\n", "text_pair", "=", "ocr_data", "[", "i", "]", ",", "\n", "add_special_tokens", "=", "True", ",", "# Add '[CLS]' and '[SEP]'", "\n", "max_length", "=", "max_len", ",", "# Pad & truncate all sentences.", "\n", "pad_to_max_length", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "# Construct attn. masks.", "\n", "return_token_type_ids", "=", "True", ",", "\n", ")", "\n", "\n", "# Add the encoded sentence to the list.    ", "\n", "input_ids", ".", "append", "(", "encoded_dict", "[", "'input_ids'", "]", ")", "\n", "\n", "# And its attention mask (simply differentiates padding from non-padding).", "\n", "attention_masks", ".", "append", "(", "encoded_dict", "[", "'attention_mask'", "]", ")", "\n", "\n", "token_ids", ".", "append", "(", "encoded_dict", "[", "'token_type_ids'", "]", ")", "\n", "\n", "", "input_ids", "=", "np", ".", "asarray", "(", "input_ids", ",", "dtype", "=", "'int32'", ")", "\n", "attention_masks", "=", "np", ".", "asarray", "(", "attention_masks", ",", "dtype", "=", "'int32'", ")", "\n", "token_ids", "=", "np", ".", "asarray", "(", "token_ids", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "new_data", "=", "np", ".", "concatenate", "(", "(", "input_ids", ",", "attention_masks", ")", ",", "axis", "=", "1", ")", "\n", "new_data", "=", "np", ".", "concatenate", "(", "(", "new_data", ",", "token_ids", ")", ",", "axis", "=", "1", ")", "\n", "\n", "return", "new_data", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.prepare_combined_dataset_error.make_roberta_input_w_OCR": [[48, 87], ["range", "numpy.asarray", "numpy.asarray", "numpy.concatenate", "len", "tokenizer.encode_plus", "np.asarray.append", "np.asarray.append"], "function", ["None"], ["", "def", "make_roberta_input_w_OCR", "(", "text_data", ",", "ocr_data", ",", "max_len", ",", "tokenizer", ")", ":", "\n", "\t", "''' \n    Purpose: This function encodes the text along with OCR using the RoBERTa encoder.\n    Input:  text_data: Text data\n            ocr_data: OCR Text data\n            max_len : Maximum length of the sequence\n\t\t\ttokenizer: RoBERTa tokenizer from Huggingface\n    Output: new_data: [input_ids,attention_masks]\n\t'''", "\n", "input_ids", "=", "[", "]", "\n", "attention_masks", "=", "[", "]", "\n", "token_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "text_data", ")", ")", ":", "\n", "\t\t", "encoded_dict", "=", "tokenizer", ".", "encode_plus", "(", "\n", "text_data", "[", "i", "]", ",", "# Sentence to encode.", "\n", "text_pair", "=", "ocr_data", "[", "i", "]", ",", "\n", "add_special_tokens", "=", "True", ",", "# Add '[CLS]' and '[SEP]'", "\n", "max_length", "=", "max_len", ",", "# Pad & truncate all sentences.", "\n", "pad_to_max_length", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "# Construct attn. masks.", "\n", "add_prefix_space", "=", "True", ",", "\n", ")", "\n", "\n", "# Add the encoded sentence to the list.    ", "\n", "input_ids", ".", "append", "(", "encoded_dict", "[", "'input_ids'", "]", ")", "\n", "\n", "# And its attention mask (simply differentiates padding from non-padding).", "\n", "attention_masks", ".", "append", "(", "encoded_dict", "[", "'attention_mask'", "]", ")", "\n", "\n", "#token_ids.append(encoded_dict['token_type_ids'])", "\n", "\n", "", "input_ids", "=", "np", ".", "asarray", "(", "input_ids", ",", "dtype", "=", "'int32'", ")", "\n", "attention_masks", "=", "np", ".", "asarray", "(", "attention_masks", ",", "dtype", "=", "'int32'", ")", "\n", "#token_ids = np.asarray(token_ids, dtype='int32')", "\n", "\n", "new_data", "=", "np", ".", "concatenate", "(", "(", "input_ids", ",", "attention_masks", ")", ",", "axis", "=", "1", ")", "\n", "#new_data = np.concatenate((new_data, token_ids), axis=1)", "\n", "\n", "return", "new_data", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.prepare_combined_dataset_error.get_dataset": [[89, 165], ["twitter_text_preprocessing.prepare_dataset", "twitter_text_preprocessing.prepare_dataset", "print", "numpy.array", "numpy.array", "len", "transformers.BertTokenizer.from_pretrained", "transformers.RobertaTokenizer.from_pretrained", "range", "print", "range", "prepare_combined_dataset_error.make_bert_input_w_OCR", "make_bert_input", "prepare_combined_dataset_error.make_roberta_input_w_OCR", "make_roberta_input", "len", "len", "len", "np.array.append", "np.array.append", "np.array.append", "np.array.append", "multi_text_data.append", "multi_ocr_data.append", "numpy.array", "numpy.array", "str", "str", "str", "int", "str", "int"], "function", ["home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.prepare_dataset", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.prepare_dataset", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.prepare_combined_dataset_error.make_bert_input_w_OCR", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.prepare_combined_dataset_error.make_roberta_input_w_OCR"], ["", "def", "get_dataset", "(", "datafile", ",", "data_col", ",", "ocr_col", ",", "label_col", ",", "txt_flag", ",", "multilabel", ",", "ocr_flag", ")", ":", "\n", "\n", "\t", "''' \n    Purpose: This function gets the dataset file name and other optional flags to encoded data along with the labels\n    Input:  data_file: path of the dataset file.\n            data_col: column number for the data (text data).\n\t\t\tocr_col: column number for the OCR data.\n            label_col : column number for the labels\n\t\t\ttxt_flag: Flag indicating whether to use BERT (0) or RoBERTa (1)\n\t\t\tmultilabel: Flag indicating whether the setting is for binary labels (0) or multiclass labels (1)\n\t\t\tocr_flag: Flag indicating whether to use OCR or not.\n    Output: new_data_w_index: Contains the new encoded data with indices for getting the images. \n\t\t\tencoded_labels: Encoded labels\n\t\t\ttext_data: Text data obtained from the source datasite file after pre-processing\n\t\t\tocr_data: OCR data obtained from the source datasite file after pre-processing\n\t'''", "\n", "\n", "text_data", ",", "labels", "=", "prepare_dataset", "(", "datafile", ",", "data_col", ",", "label_col", ",", "\"word-based\"", ")", "\n", "ocr_data", ",", "labels", "=", "prepare_dataset", "(", "datafile", ",", "ocr_col", ",", "label_col", ",", "\"word-based\"", ")", "\n", "print", "(", "len", "(", "text_data", ")", ")", "\n", "\n", "max_len", "=", "100", "\n", "if", "txt_flag", "==", "0", ":", "\n", "\t\t", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "do_lower_case", "=", "True", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_len", ",", "pad_to_max_length", "=", "True", ")", "\n", "if", "ocr_flag", "==", "1", ":", "\n", "\t\t\t", "new_data", "=", "make_bert_input_w_OCR", "(", "text_data", ",", "ocr_data", ",", "max_len", ",", "tokenizer", ")", "\n", "", "else", ":", "\n", "\t\t\t", "new_data", "=", "make_bert_input", "(", "text_data", ",", "max_len", ")", "\n", "", "", "else", ":", "\n", "\t\t", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ",", "do_lower_case", "=", "True", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_len", ",", "pad_to_max_length", "=", "True", ")", "\n", "if", "ocr_flag", "==", "1", ":", "\n", "\t\t\t", "new_data", "=", "make_roberta_input_w_OCR", "(", "text_data", ",", "ocr_data", ",", "max_len", ",", "tokenizer", ")", "\n", "", "else", ":", "\n", "\t\t\t", "new_data", "=", "make_roberta_input", "(", "text_data", ",", "max_len", ")", "\n", "\n", "", "", "new_data_w_index", "=", "[", "]", "\n", "\n", "multi_text_data", "=", "[", "]", "\n", "multi_ocr_data", "=", "[", "]", "\n", "\n", "if", "multilabel", "==", "1", ":", "\n", "\t\t", "encoded_labels", "=", "[", "]", "\n", "encoding", "=", "{", "\n", "'0'", ":", "0", ",", "\n", "'1'", ":", "1", ",", "\n", "'2'", ":", "2", ",", "\n", "'3'", ":", "3", "\n", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "new_data", ")", ")", ":", "\n", "\t\t\t", "if", "labels", "[", "i", "]", "in", "[", "'0'", ",", "'1'", ",", "'2'", ",", "'3'", "]", ":", "\n", "\t\t\t\t", "new_data_w_index", ".", "append", "(", "np", ".", "array", "(", "[", "[", "str", "(", "i", "+", "1", ")", "]", ",", "new_data", "[", "i", "]", "]", ")", ")", "\n", "encoded_labels", ".", "append", "(", "encoding", "[", "str", "(", "int", "(", "labels", "[", "i", "]", ")", ")", "]", ")", "\n", "multi_text_data", ".", "append", "(", "text_data", "[", "i", "]", ")", "\n", "multi_ocr_data", ".", "append", "(", "ocr_data", "[", "i", "]", ")", "\n", "\n", "", "", "text_data", "=", "multi_text_data", "\n", "ocr_data", "=", "multi_ocr_data", "\n", "\n", "print", "(", "len", "(", "multi_text_data", ")", ")", "\n", "\n", "", "else", ":", "\n", "\t\t", "encoded_labels", "=", "[", "]", "\n", "encoding", "=", "{", "\n", "'0'", ":", "0", ",", "\n", "'1'", ":", "1", "\n", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "new_data", ")", ")", ":", "\n", "\t\t\t", "new_data_w_index", ".", "append", "(", "np", ".", "array", "(", "[", "[", "str", "(", "i", "+", "1", ")", "]", ",", "new_data", "[", "i", "]", "]", ")", ")", "\n", "encoded_labels", ".", "append", "(", "encoding", "[", "str", "(", "int", "(", "labels", "[", "i", "]", ")", ")", "]", ")", "\n", "\n", "\n", "", "", "new_data_w_index", "=", "np", ".", "array", "(", "new_data_w_index", ")", "\n", "encoded_labels", "=", "np", ".", "array", "(", "encoded_labels", ")", "\n", "\n", "\n", "return", "new_data_w_index", ",", "encoded_labels", ",", "text_data", ",", "ocr_data", "", "", ""]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.training_loop_encoder.train_loop_encoder": [[11, 122], ["torch.NLLLoss", "torch.MSELoss", "torch.Adam", "torch.device", "torch.device", "torch.device", "model.to", "time.time", "copy.deepcopy", "float", "range", "print", "print", "model.load_state_dict", "model.parameters", "model.state_dict", "print", "print", "print", "time.time", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "tqdm.tqdm", "print", "model.train", "model.eval", "inputs.to.to", "imgs.to.to", "labels.to.to", "optim.Adam.zero_grad", "loss.item", "torch.sum", "torch.sum", "torch.sum", "running_corrects.double", "val_losses.append", "copy.deepcopy", "all", "torch.set_grad_enabled", "torch.set_grad_enabled", "torch.set_grad_enabled", "model", "torch.max", "torch.max", "torch.max", "nn.NLLLoss.", "nn.MSELoss.", "model.state_dict", "print", "print", "print", "model.load_state_dict", "torch.max", "torch.max", "torch.max", "loss.backward", "optim.Adam.step", "time.time", "labels.to.long", "numpy.exp", "label_loss.item", "numpy.exp", "numpy.exp", "label_loss.item", "reconstruction_loss.item"], "function", ["None"], ["def", "train_loop_encoder", "(", "model", ",", "dataloaders", ",", "dataset_sizes", ",", "class_weights_labels", ",", "alpha", "=", "1", ",", "beta", "=", "1", ",", "custom_loss", "=", "0", ",", "num_epochs", "=", "50", ")", ":", "\n", "    ", "''' \n    Purpose: This is the training loop function used for training and validation data. We use Early stopping with patience=5.\n             We return the model which gives the lowest loss.\n    Input:  dataloaders: Train and Validation sets are loaded.\n            class_weights_labels: We use class weights to train the model due to unbalanced number of class labels.\n            alpha, beta, custom_loss: In case you want to use a custom loss, you can use custom_loss Flag = 1 \n                                      and choose alpha& beta values\n            num_epochs: We set the maximum number of epochs = 50\n    Output: returns the best model (lowest loss)\n    '''", "\n", "\n", "label_loss", "=", "nn", ".", "NLLLoss", "(", "weight", "=", "class_weights_labels", ")", "\n", "reconstruction_loss", "=", "nn", ".", "MSELoss", "(", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "2e-6", ",", "eps", "=", "1e-08", ")", "# clipnorm=1.0, add later", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "since", "=", "time", ".", "time", "(", ")", "\n", "\n", "best_model_wts", "=", "copy", ".", "deepcopy", "(", "model", ".", "state_dict", "(", ")", ")", "\n", "best_loss", "=", "float", "(", "\"inf\"", ")", "\n", "val_losses", "=", "[", "]", "\n", "\n", "patience", "=", "5", "# for early stopping", "\n", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "        ", "print", "(", "'Epoch {}/{}'", ".", "format", "(", "epoch", ",", "num_epochs", "-", "1", ")", ")", "\n", "print", "(", "'-'", "*", "10", ")", "\n", "\n", "# Each epoch has a training and validation phase", "\n", "for", "phase", "in", "[", "'train'", ",", "'validation'", "]", ":", "\n", "            ", "if", "phase", "==", "'train'", ":", "\n", "                ", "model", ".", "train", "(", ")", "# Set model to training mode", "\n", "", "else", ":", "\n", "                ", "model", ".", "eval", "(", ")", "# Set model to evaluate mode           ", "\n", "\n", "", "running_loss", "=", "0.0", "\n", "running_corrects", "=", "0", "\n", "\n", "# Iterate over data.", "\n", "for", "inputs", ",", "imgs", ",", "labels", "in", "tqdm", "(", "dataloaders", "[", "phase", "]", ")", ":", "\n", "                ", "inputs", "=", "inputs", ".", "to", "(", "device", ")", "\n", "imgs", "=", "imgs", ".", "to", "(", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "device", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# forward", "\n", "# track history if only in train", "\n", "with", "torch", ".", "set_grad_enabled", "(", "phase", "==", "'train'", ")", ":", "\n", "                    ", "outputs", ",", "combined_features", ",", "decoded_features", "=", "model", "(", "inputs", ",", "imgs", ")", "\n", "_", ",", "preds", "=", "torch", ".", "max", "(", "outputs", ",", "1", ")", "\n", "actual_labels", "=", "torch", ".", "max", "(", "labels", ".", "long", "(", ")", ",", "1", ")", "[", "1", "]", "\n", "current_label_loss", "=", "label_loss", "(", "outputs", ",", "actual_labels", ")", "\n", "current_reconstruction_loss", "=", "reconstruction_loss", "(", "combined_features", ",", "decoded_features", ")", "\n", "\n", "if", "custom_loss", "==", "1", ":", "\n", "                        ", "alpha", "=", "np", ".", "exp", "(", "current_label_loss", ".", "item", "(", ")", ")", "/", "(", "np", ".", "exp", "(", "current_label_loss", ".", "item", "(", ")", ")", "+", "np", ".", "exp", "(", "current_reconstruction_loss", ".", "item", "(", ")", ")", "+", "1e-10", ")", "\n", "beta", "=", "1", "-", "alpha", "\n", "", "loss", "=", "(", "1", "+", "alpha", ")", "*", "current_label_loss", "+", "(", "1", "+", "beta", ")", "*", "current_reconstruction_loss", "\n", "\n", "\n", "# backward + optimize only if in training phase", "\n", "if", "phase", "==", "'train'", ":", "\n", "                        ", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# statistics", "\n", "", "", "running_loss", "+=", "loss", ".", "item", "(", ")", "\n", "running_corrects", "+=", "torch", ".", "sum", "(", "preds", "==", "actual_labels", ")", "\n", "\n", "", "epoch_loss", "=", "running_loss", "/", "dataset_sizes", "[", "phase", "]", "\n", "epoch_acc", "=", "running_corrects", ".", "double", "(", ")", "/", "dataset_sizes", "[", "phase", "]", "\n", "\n", "print", "(", "'{} Loss: {:.4f} Acc: {:.4f}'", ".", "format", "(", "\n", "phase", ",", "epoch_loss", ",", "epoch_acc", ")", ")", "\n", "\n", "if", "phase", "==", "'validation'", ":", "\n", "                ", "val_losses", ".", "append", "(", "epoch_loss", ")", "\n", "\n", "# deep copy the model", "\n", "", "if", "phase", "==", "'validation'", "and", "epoch_loss", "<", "best_loss", ":", "\n", "#                 save_models(epoch,model)", "\n", "                ", "best_loss", "=", "epoch_loss", "\n", "best_model_wts", "=", "copy", ".", "deepcopy", "(", "model", ".", "state_dict", "(", ")", ")", "\n", "\n", "", "if", "phase", "==", "'validation'", "and", "epoch", ">=", "patience", ":", "\n", "                ", "last_losses", "=", "val_losses", "[", "-", "patience", ":", "]", "\n", "if", "all", "(", "x", ">", "best_loss", "for", "x", "in", "last_losses", ")", ":", "\n", "                    ", "print", "(", "\"Early stopping...\"", ")", "\n", "\n", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "since", "\n", "print", "(", "'Training complete in {:.0f}m {:.0f}s'", ".", "format", "(", "\n", "time_elapsed", "//", "60", ",", "time_elapsed", "%", "60", ")", ")", "\n", "print", "(", "'Best val Loss: {:4f}'", ".", "format", "(", "best_loss", ")", ")", "\n", "\n", "# load best model weights", "\n", "model", ".", "load_state_dict", "(", "best_model_wts", ")", "\n", "return", "model", "\n", "\n", "", "", "", "print", "(", ")", "\n", "\n", "", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "since", "\n", "print", "(", "'Training complete in {:.0f}m {:.0f}s'", ".", "format", "(", "\n", "time_elapsed", "//", "60", ",", "time_elapsed", "%", "60", ")", ")", "\n", "print", "(", "'Best val loss: {:4f}'", ".", "format", "(", "best_loss", ")", ")", "\n", "\n", "# load best model weights", "\n", "model", ".", "load_state_dict", "(", "best_model_wts", ")", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.read_csv_file": [[10, 21], ["open", "csv.reader", "enumerate", "data_list.append", "label_list.append"], "function", ["None"], ["def", "read_csv_file", "(", "file_name", ",", "data_col", ",", "label_col", ")", ":", "\n", "    ", "data_list", "=", "[", "]", "\n", "label_list", "=", "[", "]", "\n", "with", "open", "(", "file_name", ",", "'r'", ")", "as", "csv_reader", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csv_reader", ",", "delimiter", "=", "','", ")", "\n", "for", "line_number", ",", "row", "in", "enumerate", "(", "reader", ")", ":", "\n", "            ", "if", "line_number", "!=", "0", ":", "\n", "                ", "data_list", ".", "append", "(", "row", "[", "data_col", "]", ")", "\n", "label_list", ".", "append", "(", "row", "[", "label_col", "]", ")", "\n", "\n", "", "", "", "return", "data_list", ",", "label_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.clean_text": [[22, 37], ["re.sub.split", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.split"], "function", ["None"], ["", "def", "clean_text", "(", "input_string", ")", ":", "\n", "\n", "    ", "input_string", "=", "input_string", ".", "split", "(", "'\\n'", ")", "\n", "input_string", "=", "\" \"", ".", "join", "(", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'\\(\\(\\(.*\\)\\)\\)'", ",", "'bad jew conspiracy'", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'@\\S+'", ",", "\" usermention \"", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'#'", ",", "\" \"", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'\\d\\S+'", ",", "\" \"", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r\"http\\S+\"", ",", "\" \"", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'www\\S+'", ",", "\" \"", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'\\.|/|:|-'", ",", "\" \"", ",", "input_string", ")", "\n", "input_string", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "''", ",", "input_string", ")", "\n", "input_string", "=", "\" \"", ".", "join", "(", "input_string", ".", "split", "(", ")", ")", "\n", "\n", "return", "input_string", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.parse_input_character": [[39, 51], ["row.split", "train_data.append", "char_list.append"], "function", ["None"], ["", "def", "parse_input_character", "(", "data", ")", ":", "\n", "    ", "train_data", "=", "[", "]", "\n", "train_label", "=", "[", "]", "\n", "for", "row", "in", "data", ":", "\n", "        ", "temp_row", "=", "row", ".", "split", "(", ")", "\n", "char_list", "=", "[", "]", "\n", "for", "words", "in", "temp_row", ":", "\n", "            ", "for", "ch", "in", "words", ":", "\n", "                ", "char_list", ".", "append", "(", "ch", ")", "\n", "", "", "train_data", ".", "append", "(", "char_list", ")", "\n", "\n", "", "return", "train_data", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.prepare_dataset": [[54, 67], ["print", "twitter_text_preprocessing.read_csv_file", "range", "len", "twitter_text_preprocessing.clean_text", "twitter_text_preprocessing.parse_input_character"], "function", ["home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.read_csv_file", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.clean_text", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_text_preprocessing.parse_input_character"], ["", "def", "prepare_dataset", "(", "filename", ",", "data_col", ",", "label_col", ",", "network_type", ")", ":", "\n", "\n", "    ", "print", "(", "\"########################-Using Twitter Based Pre-processing-##################\"", ")", "\n", "\n", "original_data", ",", "original_labels", "=", "read_csv_file", "(", "filename", ",", "data_col", ",", "label_col", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "original_data", ")", ")", ":", "\n", "        ", "original_data", "[", "i", "]", "=", "clean_text", "(", "original_data", "[", "i", "]", ")", "\n", "\n", "", "if", "network_type", "==", "\"character-based\"", ":", "\n", "        ", "train_data", "=", "parse_input_character", "(", "original_data", ")", "\n", "return", "train_data", ",", "original_labels", "\n", "", "else", ":", "\n", "        ", "return", "original_data", ",", "original_labels", "", "", "", ""]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_testing.recall_m": [[5, 10], ["tensorflow.keras.backend.sum", "tensorflow.keras.backend.sum", "tensorflow.keras.backend.round", "tensorflow.keras.backend.round", "tensorflow.keras.backend.clip", "tensorflow.keras.backend.clip", "tensorflow.keras.backend.epsilon"], "function", ["None"], ["def", "recall_m", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "true_positives", "=", "K", ".", "sum", "(", "K", ".", "round", "(", "K", ".", "clip", "(", "y_true", "*", "y_pred", ",", "0", ",", "1", ")", ")", ")", "\n", "possible_positives", "=", "K", ".", "sum", "(", "K", ".", "round", "(", "K", ".", "clip", "(", "y_true", ",", "0", ",", "1", ")", ")", ")", "\n", "recall", "=", "true_positives", "/", "(", "possible_positives", "+", "K", ".", "epsilon", "(", ")", ")", "\n", "return", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_testing.precision_m": [[11, 16], ["tensorflow.keras.backend.sum", "tensorflow.keras.backend.sum", "tensorflow.keras.backend.round", "tensorflow.keras.backend.round", "tensorflow.keras.backend.clip", "tensorflow.keras.backend.clip", "tensorflow.keras.backend.epsilon"], "function", ["None"], ["", "def", "precision_m", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "true_positives", "=", "K", ".", "sum", "(", "K", ".", "round", "(", "K", ".", "clip", "(", "y_true", "*", "y_pred", ",", "0", ",", "1", ")", ")", ")", "\n", "predicted_positives", "=", "K", ".", "sum", "(", "K", ".", "round", "(", "K", ".", "clip", "(", "y_pred", ",", "0", ",", "1", ")", ")", ")", "\n", "precision", "=", "true_positives", "/", "(", "predicted_positives", "+", "K", ".", "epsilon", "(", ")", ")", "\n", "return", "precision", "\n", "\n"]], "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_testing.f1_m": [[17, 21], ["twitter_testing.precision_m", "twitter_testing.recall_m", "tensorflow.keras.backend.epsilon"], "function", ["home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_testing.precision_m", "home.repos.pwc.inspect_result.mohit3011_Online-Antisemitism-Detection-Using-MultimodalDeep-Learning.None.twitter_testing.recall_m"], ["", "def", "f1_m", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "precision", "=", "precision_m", "(", "y_true", ",", "y_pred", ")", "\n", "recall", "=", "recall_m", "(", "y_true", ",", "y_pred", ")", "\n", "return", "2", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "K", ".", "epsilon", "(", ")", ")", ")", "\n", "\n"]]}