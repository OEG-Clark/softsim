{"home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.Args.__init__": [[129, 164], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_path", ",", "vocab_file", ",", "bert_config_file", ",", "init_checkpoint", ",", "\n", "output_file", ",", "max_seq_length", "=", "128", ",", "bert_layer", "=", "-", "1", ",", "map_input", "=", "False", ",", "\n", "vocab_file1", "=", "None", ",", "bert_config_file1", "=", "None", ",", "init_checkpoint1", "=", "None", ",", "\n", "non_linear", "=", "False", ",", "activation", "=", "\"leaky_relu\"", ",", "n_layers", "=", "2", ",", "hidden_size", "=", "768", ",", "\n", "emb_dim", "=", "768", ")", ":", "\n", "\n", "        ", "self", ".", "adversarial", "=", "False", "\n", "self", ".", "pred", "=", "True", "\n", "self", ".", "no_cuda", "=", "False", "\n", "self", ".", "cal_sent_sim", "=", "False", "\n", "self", ".", "local_rank", "=", "-", "1", "\n", "self", ".", "batch_size", "=", "32", "\n", "self", ".", "do_lower_case", "=", "True", "\n", "self", ".", "map_input", "=", "map_input", "\n", "self", ".", "transformer", "=", "None", "\n", "self", ".", "fine_tune", "=", "False", "\n", "self", ".", "load_pred_bert", "=", "False", "\n", "\n", "self", ".", "vocab_file", "=", "vocab_file", "\n", "self", ".", "bert_config_file", "=", "bert_config_file", "\n", "self", ".", "init_checkpoint", "=", "init_checkpoint", "\n", "self", ".", "model_path", "=", "model_path", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "self", ".", "output_file", "=", "output_file", "\n", "self", ".", "bert_layer", "=", "bert_layer", "\n", "\n", "self", ".", "vocab_file1", "=", "vocab_file1", "\n", "self", ".", "bert_config_file1", "=", "bert_config_file1", "\n", "self", ".", "init_checkpoint1", "=", "init_checkpoint1", "\n", "\n", "self", ".", "non_linear", "=", "non_linear", "\n", "self", ".", "activation", "=", "activation", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "emb_dim", "=", "emb_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.__init__": [[167, 202], ["src.build_model.build_model", "src.utils.initialize_exp", "os.path.isfile", "src.load.load", "src.bert_trainer.BertTrainer", "src.bert_evaluator.BertEvaluator", "torch.device", "torch.device", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.build_model.build_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.initialize_exp", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "\n", "        ", "self", ".", "args", "=", "args", "\n", "# check parameters", "\n", "if", "self", ".", "args", ".", "adversarial", ":", "\n", "            ", "assert", "0", "<=", "self", ".", "args", ".", "dis_dropout", "<", "1", "\n", "assert", "0", "<=", "self", ".", "args", ".", "dis_input_dropout", "<", "1", "\n", "assert", "0", "<=", "self", ".", "args", ".", "dis_smooth", "<", "0.5", "\n", "assert", "self", ".", "args", ".", "dis_lambda", ">", "0", "and", "self", ".", "args", ".", "dis_steps", ">", "0", "\n", "assert", "0", "<", "self", ".", "args", ".", "lr_shrink", "<=", "1", "\n", "assert", "self", ".", "args", ".", "model_path", "is", "not", "None", "\n", "", "self", ".", "dataset", "=", "None", "\n", "# build model / trainer / evaluator", "\n", "if", "not", "self", ".", "args", ".", "pred", "and", "not", "self", ".", "args", ".", "cal_sent_sim", ":", "\n", "            ", "self", ".", "logger", "=", "initialize_exp", "(", "self", ".", "args", ")", "\n", "", "if", "self", ".", "args", ".", "adversarial", "or", "self", ".", "args", ".", "cal_sent_sim", ":", "\n", "            ", "assert", "os", ".", "path", ".", "isfile", "(", "self", ".", "args", ".", "input_file", ")", "\n", "self", ".", "dataset", ",", "unique_id_to_feature", ",", "self", ".", "features", "=", "load", "(", "self", ".", "args", ".", "vocab_file", ",", "\n", "self", ".", "args", ".", "input_file", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ",", "\n", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "\n", "local_rank", "=", "self", ".", "args", ".", "local_rank", ",", "vocab_file1", "=", "self", ".", "args", ".", "vocab_file1", ")", "\n", "", "self", ".", "bert_model", ",", "self", ".", "mapping", ",", "self", ".", "discriminator", ",", "self", ".", "bert_model1", "=", "build_model", "(", "self", ".", "args", ",", "True", ")", "\n", "\n", "if", "self", ".", "args", ".", "adversarial", "or", "self", ".", "args", ".", "pred", ":", "\n", "            ", "self", ".", "trainer", "=", "BertTrainer", "(", "self", ".", "bert_model", ",", "self", ".", "dataset", ",", "self", ".", "mapping", ",", "self", ".", "discriminator", ",", "\n", "self", ".", "args", ",", "bert_model1", "=", "self", ".", "bert_model1", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "adversarial", "or", "self", ".", "args", ".", "cal_sent_sim", ":", "\n", "            ", "self", ".", "evaluator", "=", "BertEvaluator", "(", "self", ".", "bert_model", ",", "self", ".", "dataset", ",", "self", ".", "mapping", ",", "self", ".", "discriminator", ",", "\n", "self", ".", "args", ",", "self", ".", "features", ",", "bert_model1", "=", "self", ".", "bert_model1", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", "or", "self", ".", "args", ".", "no_cuda", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "self", ".", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "self", ".", "args", ".", "local_rank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.train_adv": [[203, 275], ["bert_gan.AdvBert.logger.info", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "range", "os.path.join", "bert_gan.AdvBert.logger.info", "time.time", "bert_gan.AdvBert.evaluator.eval_sim", "bert_gan.AdvBert.evaluator.eval_dev_dis", "bert_gan.AdvBert.trainer.save_best", "bert_gan.AdvBert.logger.info", "bert_gan.AdvBert.trainer.update_lr", "input_ids_a.to.to.to", "input_mask_a.to.to.to", "input_ids_b.to.to.to", "input_mask_b.to.to.to", "bert_gan.AdvBert.trainer.get_bert", "bert_gan.AdvBert.trainer.get_bert", "bert_gan.AdvBert.trainer.dis_step", "os.path.exists", "bert_gan.AdvBert.trainer.save_epoch", "bert_gan.AdvBert.logger.info", "bert_gan.AdvBert.trainer.mapping_step", "stats_log.append", "bert_gan.AdvBert.logger.info", "time.time", "bert_gan.AdvBert.evaluator.eval_sim", "bert_gan.AdvBert.evaluator.eval_dev_dis", "bert_gan.AdvBert.trainer.save_best", "int", "numpy.mean", "len", "time.time"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_dev_dis", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.update_lr", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.dis_step", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.save_epoch", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.mapping_step", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_dev_dis", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_best"], ["", "", "def", "train_adv", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Learning loop for Adversarial Training\n        \"\"\"", "\n", "\n", "self", ".", "logger", ".", "info", "(", "'----> ADVERSARIAL TRAINING <----\\n\\n'", ")", "\n", "\n", "sampler", "=", "RandomSampler", "(", "self", ".", "dataset", ")", "\n", "train_loader", "=", "DataLoader", "(", "self", ".", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "# training loop", "\n", "for", "n_epoch", "in", "range", "(", "self", ".", "args", ".", "n_epochs", ")", ":", "\n", "            ", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "model_path", ",", "\"epoch-{}\"", ".", "format", "(", "n_epoch", ")", ")", "\n", "\n", "self", ".", "logger", ".", "info", "(", "'Starting adversarial training epoch %i...'", "%", "n_epoch", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "n_words_proc", "=", "0", "\n", "n_dis_step", "=", "0", "\n", "n_map_step", "=", "0", "\n", "stats", "=", "{", "'DIS_COSTS'", ":", "[", "]", "}", "\n", "for", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "example_indices", "in", "train_loader", ":", "\n", "                ", "input_ids_a", "=", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_a", "=", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_ids_b", "=", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_b", "=", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "src_bert", "=", "self", ".", "trainer", ".", "get_bert", "(", "input_ids_a", ",", "input_mask_a", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "\n", "tgt_bert", "=", "self", ".", "trainer", ".", "get_bert", "(", "input_ids_b", ",", "input_mask_b", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", "\n", "self", ".", "trainer", ".", "dis_step", "(", "src_bert", ",", "tgt_bert", ",", "stats", ")", "\n", "\n", "n_dis_step", "+=", "1", "\n", "if", "n_dis_step", "%", "self", ".", "args", ".", "dis_steps", "==", "0", ":", "\n", "                    ", "n_words_proc", "+=", "self", ".", "trainer", ".", "mapping_step", "(", "stats", ")", "\n", "n_map_step", "+=", "1", "\n", "\n", "# log stats", "\n", "", "if", "n_dis_step", "%", "self", ".", "args", ".", "print_every_dis_steps", "==", "0", ":", "\n", "                    ", "stats_str", "=", "[", "(", "'DIS_COSTS'", ",", "'Discriminator loss'", ")", "]", "\n", "stats_log", "=", "[", "'%s: %.4f'", "%", "(", "v", ",", "np", ".", "mean", "(", "stats", "[", "k", "]", ")", ")", "\n", "for", "k", ",", "v", "in", "stats_str", "if", "len", "(", "stats", "[", "k", "]", ")", ">", "0", "]", "\n", "stats_log", ".", "append", "(", "'%i samples/s'", "%", "int", "(", "n_words_proc", "/", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "(", "'Epoch: {:0>3d} | Step: {:0>6d} - '", ".", "format", "(", "n_epoch", ",", "n_dis_step", ")", ")", "+", "' | '", ".", "join", "(", "stats_log", ")", ")", "\n", "\n", "# reset", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "n_words_proc", "=", "0", "\n", "for", "k", ",", "_", "in", "stats_str", ":", "\n", "                        ", "del", "stats", "[", "k", "]", "[", ":", "]", "\n", "", "", "if", "n_dis_step", "%", "self", ".", "args", ".", "save_every_dis_steps", "==", "0", ":", "\n", "                    ", "metric", "=", "self", ".", "evaluator", ".", "eval_sim", "(", ")", "\n", "self", ".", "evaluator", ".", "eval_dev_dis", "(", ")", "\n", "self", ".", "trainer", ".", "save_best", "(", "metric", "[", "\"para_sim\"", "]", ",", "path", "=", "path", ")", "\n", "\n", "# embeddings / self.discriminator evaluation", "\n", "#to_log = OrderedDict({'n_epoch': n_epoch})", "\n", "", "", "metric", "=", "self", ".", "evaluator", ".", "eval_sim", "(", ")", "\n", "self", ".", "evaluator", ".", "eval_dev_dis", "(", ")", "\n", "self", ".", "trainer", ".", "save_best", "(", "metric", "[", "\"para_sim\"", "]", ",", "path", "=", "path", ")", "\n", "\n", "# JSON log / save best model / end of epoch", "\n", "#self.logger.info(\"__log__:%s\" % json.dumps(to_log))", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "                ", "self", ".", "trainer", ".", "save_epoch", "(", "path", ",", "n_epoch", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'End of epoch %i.\\n\\n'", "%", "n_epoch", ")", "\n", "\n", "# update the learning rate (stop if too small)", "\n", "self", ".", "trainer", ".", "update_lr", "(", "metric", "[", "\"para_sim\"", "]", ")", "\n", "#self.trainer.update_dis_lr(metric[\"para_sim\"])", "\n", "if", "self", ".", "trainer", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "<", "self", ".", "args", ".", "min_lr", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'Learning rate < 1e-6. BREAK.'", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.refine": [[276, 299], ["bert_gan.AdvBert.logger.info", "bert_gan.AdvBert.trainer.load_best", "range", "bert_gan.AdvBert.logger.info", "bert_gan.AdvBert.trainer.procrustes", "bert_gan.AdvBert.evaluator.eval_sim", "bert_gan.AdvBert.trainer.save_best", "bert_gan.AdvBert.logger.info"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.procrustes", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_best"], ["", "", "", "def", "refine", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Learning loop for Procrustes Iterative Refinement\n        \"\"\"", "\n", "\n", "# Get the best self.mapping according to VALIDATION_METRIC", "\n", "self", ".", "logger", ".", "info", "(", "'----> ITERATIVE PROCRUSTES REFINEMENT <----\\n\\n'", ")", "\n", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "\n", "# training loop", "\n", "for", "n_iter", "in", "range", "(", "self", ".", "args", ".", "n_refinement", ")", ":", "\n", "\n", "            ", "self", ".", "logger", ".", "info", "(", "'Starting refinement iteration %i...'", "%", "n_iter", ")", "\n", "\n", "# apply the Procrustes solution", "\n", "self", ".", "trainer", ".", "procrustes", "(", ")", "\n", "\n", "metric", "=", "self", ".", "evaluator", ".", "eval_sim", "(", ")", "\n", "\n", "# JSON log / save best model / end of epoch", "\n", "#self.logger.info(\"__log__:%s\" % json.dumps(to_log))", "\n", "self", ".", "trainer", ".", "save_best", "(", "metric", "[", "\"para_sim\"", "]", ")", "\n", "self", ".", "logger", ".", "info", "(", "'End of refinement iteration %i.\\n\\n'", "%", "n_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.pred": [[300, 354], ["bert_gan.AdvBert.trainer.load_best", "src.load.load_single", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "bert_gan.AdvBert.bert_model.eval", "open", "input_ids.to.to.to", "input_mask.to.to.to", "enumerate", "bert_gan.AdvBert.bert_model", "bert_gan.AdvBert.bert_model", "bert_gan.AdvBert.trainer.mapping", "int", "collections.OrderedDict", "enumerate", "writer.write", "bert_gan.AdvBert.detach().cpu().numpy", "collections.OrderedDict", "all_layers.append", "collections.OrderedDict", "all_out_features.append", "example_index.item", "round", "json.dumps", "bert_gan.AdvBert.detach().cpu", "x.item", "bert_gan.AdvBert.detach"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_single", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "", "def", "pred", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Map bert of source language to target space\n        \"\"\"", "\n", "assert", "self", ".", "args", ".", "src_file", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "output_file", "is", "not", "None", "\n", "\n", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "pred_dataset", ",", "unique_id_to_feature", ",", "features", "=", "load_single", "(", "self", ".", "args", ".", "vocab_file", ",", "\n", "self", ".", "args", ".", "src_file", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ",", "\n", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "\n", "local_rank", "=", "self", ".", "args", ".", "local_rank", ")", "\n", "pred_sampler", "=", "SequentialSampler", "(", "pred_dataset", ")", "\n", "pred_dataloader", "=", "DataLoader", "(", "pred_dataset", ",", "sampler", "=", "pred_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "with", "open", "(", "self", ".", "args", ".", "output_file", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "            ", "for", "input_ids", ",", "input_mask", ",", "example_indices", "in", "pred_dataloader", ":", "\n", "                ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "args", ".", "map_input", ":", "\n", "                    ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "input_mask", ",", "input_mapping", "=", "self", ".", "trainer", ".", "mapping", ")", "\n", "target_layer", "=", "all_encoder_layers", "[", "self", ".", "args", ".", "bert_layer", "]", "\n", "", "else", ":", "\n", "                    ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "input_mask", ")", "\n", "src_encoder_layer", "=", "all_encoder_layers", "[", "self", ".", "args", ".", "bert_layer", "]", "\n", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ")", "\n", "\n", "", "for", "b", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                    ", "feature", "=", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "unique_id", "=", "int", "(", "feature", ".", "unique_id", ")", "\n", "# feature = unique_id_to_feature[unique_id]", "\n", "output_json", "=", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"linex_index\"", "]", "=", "unique_id", "\n", "all_out_features", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "all_layers", "=", "[", "]", "\n", "layer_output", "=", "target_layer", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "layer_output", "=", "layer_output", "[", "b", "]", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "layers", "[", "\"index\"", "]", "=", "self", ".", "args", ".", "bert_layer", "\n", "layers", "[", "\"values\"", "]", "=", "[", "\n", "round", "(", "x", ".", "item", "(", ")", ",", "6", ")", "for", "x", "in", "layer_output", "[", "i", "]", "\n", "]", "\n", "all_layers", ".", "append", "(", "layers", ")", "\n", "out_features", "=", "OrderedDict", "(", ")", "\n", "out_features", "[", "\"token\"", "]", "=", "token", "\n", "out_features", "[", "\"layers\"", "]", "=", "all_layers", "\n", "all_out_features", ".", "append", "(", "out_features", ")", "\n", "", "output_json", "[", "\"features\"", "]", "=", "all_out_features", "\n", "writer", ".", "write", "(", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.list2bert": [[355, 409], ["bert_gan.AdvBert.trainer.load_best", "src.load.convert", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "bert_gan.AdvBert.bert_model.eval", "bert_gan.AdvBert.trainer.mapping.eval", "open", "input_ids.to.to.to", "input_mask.to.to.to", "enumerate", "bert_gan.AdvBert.bert_model", "bert_gan.AdvBert.bert_model", "bert_gan.AdvBert.trainer.mapping", "int", "collections.OrderedDict", "enumerate", "writer.write", "bert_gan.AdvBert.detach().cpu().numpy", "collections.OrderedDict", "all_layers.append", "collections.OrderedDict", "all_out_features.append", "example_index.item", "round", "json.dumps", "bert_gan.AdvBert.detach().cpu", "x.item", "bert_gan.AdvBert.detach"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "", "", "", "def", "list2bert", "(", "self", ",", "sents", ")", ":", "\n", "        ", "\"\"\"\n        Map bert of source language to target space\n        \"\"\"", "\n", "assert", "self", ".", "args", ".", "output_file", "is", "not", "None", "\n", "\n", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "pred_dataset", ",", "unique_id_to_feature", ",", "features", "=", "convert", "(", "self", ".", "args", ".", "vocab_file", ",", "\n", "sents", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ",", "\n", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "\n", "local_rank", "=", "self", ".", "args", ".", "local_rank", ")", "\n", "pred_sampler", "=", "SequentialSampler", "(", "pred_dataset", ")", "\n", "pred_dataloader", "=", "DataLoader", "(", "pred_dataset", ",", "sampler", "=", "pred_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "self", ".", "trainer", ".", "mapping", ".", "eval", "(", ")", "\n", "with", "open", "(", "self", ".", "args", ".", "output_file", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "            ", "for", "input_ids", ",", "input_mask", ",", "example_indices", "in", "pred_dataloader", ":", "\n", "                ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "args", ".", "map_input", ":", "\n", "                    ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "input_mask", ",", "input_mapping", "=", "self", ".", "trainer", ".", "mapping", ")", "\n", "target_layer", "=", "all_encoder_layers", "[", "self", ".", "args", ".", "bert_layer", "]", "\n", "", "else", ":", "\n", "                    ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "input_mask", ")", "\n", "src_encoder_layer", "=", "all_encoder_layers", "[", "self", ".", "args", ".", "bert_layer", "]", "\n", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ")", "\n", "\n", "", "for", "b", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                    ", "feature", "=", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "unique_id", "=", "int", "(", "feature", ".", "unique_id", ")", "\n", "# feature = unique_id_to_feature[unique_id]", "\n", "output_json", "=", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"linex_index\"", "]", "=", "unique_id", "\n", "all_out_features", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "all_layers", "=", "[", "]", "\n", "layer_output", "=", "target_layer", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "layer_output", "=", "layer_output", "[", "b", "]", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "layers", "[", "\"index\"", "]", "=", "self", ".", "args", ".", "bert_layer", "\n", "layers", "[", "\"values\"", "]", "=", "[", "\n", "round", "(", "x", ".", "item", "(", ")", ",", "6", ")", "for", "x", "in", "layer_output", "[", "i", "]", "\n", "]", "\n", "all_layers", ".", "append", "(", "layers", ")", "\n", "out_features", "=", "OrderedDict", "(", ")", "\n", "out_features", "[", "\"token\"", "]", "=", "token", "\n", "out_features", "[", "\"layers\"", "]", "=", "all_layers", "\n", "all_out_features", ".", "append", "(", "out_features", ")", "\n", "", "output_json", "[", "\"features\"", "]", "=", "all_out_features", "\n", "writer", ".", "write", "(", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.get_bert": [[410, 425], ["torch.no_grad", "bert_gan.AdvBert.bert_model.eval", "bert_gan.AdvBert.bert_model", "bert_gan.AdvBert.bert_model1.eval", "bert_gan.AdvBert.bert_model1"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "", "", "", "def", "get_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get BERT\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "model_id", "==", "0", "or", "self", ".", "bert_model1", "is", "None", ":", "\n", "                ", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "bert_model1", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model1", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "encoder_layer", "=", "all_encoder_layers", "[", "bert_layer", "]", "\n", "\n", "# [batch_size, seq_len, output_dim]", "\n", "", "return", "encoder_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.calculate_sim": [[426, 436], ["print", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "bert_gan.AdvBert.evaluator.calculate_sim"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.calculate_sim"], ["", "def", "calculate_sim", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Learning loop for Adversarial Training\n        \"\"\"", "\n", "\n", "print", "(", "'----> Calculate Sentence Similarity <----\\n\\n'", ")", "\n", "\n", "sampler", "=", "SequentialSampler", "(", "self", ".", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "self", ".", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "self", ".", "evaluator", ".", "calculate_sim", "(", "loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.main": [[25, 126], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "bert_gan.AdvBert", "bert_gan.AdvBert.train_adv", "bert_gan.AdvBert.refine", "bert_gan.AdvBert.pred", "bert_gan.AdvBert.calculate_sim"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.train_adv", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.refine", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.bert_gan.AdvBert.pred", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.calculate_sim"], ["def", "main", "(", ")", ":", "\n", "# main", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Unsupervised training'", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"Initialization seed\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Verbose level (2:debug, 1:info, 0:warning)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_path\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "\"Where to store experiment logs and models\"", ")", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--src_lang\"", ",", "type", "=", "str", ",", "default", "=", "'en'", ",", "help", "=", "\"Source language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_lang\"", ",", "type", "=", "str", ",", "default", "=", "'de'", ",", "help", "=", "\"Target language\"", ")", "\n", "\n", "# self.mapping", "\n", "parser", ".", "add_argument", "(", "\"--map_id_init\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "help", "=", "\"Initialize the self.mapping as an identity matrix\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_beta\"", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "\"Beta for orthogonalization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_clip_weights\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "help", "=", "\"Clip self.mapping weights\"", ")", "\n", "# self.discriminator", "\n", "parser", ".", "add_argument", "(", "\"--dis_layers\"", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "\"self.discriminator layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_hid_dim\"", ",", "type", "=", "int", ",", "default", "=", "2048", ",", "help", "=", "\"self.discriminator hidden layer dimensions\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "help", "=", "\"self.discriminator dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_input_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "\"self.discriminator input dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_steps\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"self.discriminator steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_lambda\"", ",", "type", "=", "float", ",", "default", "=", "1", ",", "help", "=", "\"self.discriminator loss feedback coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_smooth\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "\"self.discriminator smooth predictions\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_clip_weights\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "help", "=", "\"Clip self.discriminator weights\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_lr_decay\"", ",", "type", "=", "float", ",", "default", "=", "0.98", ",", "help", "=", "\"Discriminator learning rate decay (SGD only)\"", ")", "\n", "# training adversarial", "\n", "parser", ".", "add_argument", "(", "\"--adversarial\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "help", "=", "\"Use adversarial training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_epochs\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Number of epochs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "\"Batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"sgd,lr=0.1\"", ",", "help", "=", "\"self.mapping optimizer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dis_optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"sgd,lr=0.1\"", ",", "help", "=", "\"self.discriminator optimizer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr_decay\"", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "\"Learning rate decay (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "help", "=", "\"Minimum learning rate (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr_shrink\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "\"Shrink the learning rate if the validation metric decreases (1 to disable)\"", ")", "\n", "# training refinement", "\n", "parser", ".", "add_argument", "(", "\"--n_refinement\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"Number of refinement iterations (0 to disable the refinement procedure)\"", ")", "\n", "# for bert", "\n", "parser", ".", "add_argument", "(", "\"--input_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The vocabulary file that the BERT model was trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The config json file corresponding to the pre-trained BERT model. \"", "\n", "\"This specifies the model architecture.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--init_checkpoint\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Initial checkpoint (usually from a pre-trained BERT model).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The vocabulary file that the BERT model was trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_file1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The config json file corresponding to the pre-trained BERT model. \"", "\n", "\"This specifies the model architecture.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--init_checkpoint1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Initial checkpoint (usually from a pre-trained BERT model).\"", ")", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--bert_layer\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"", "\n", "\"than this will be truncated, and sequences shorter than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to lower case the input text. Should be True for uncased \"", "\n", "\"models and False for cased models.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_sent_num\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "help", "=", "\"Number of sentence pair for development(sentence similarity).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--print_every_dis_steps\"", ",", "default", "=", "100", ",", "type", "=", "int", ",", "help", "=", "\"Print every ? self.discriminator steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_every_dis_steps\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "help", "=", "\"Save every ? self.discriminator steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rm_stop_words\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to remove stop words while evaluating(sentence similarity)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rm_punc\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to remove punctuation while evaluating(sentence similarity)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stop_words_src\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Stop word file for source language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stop_words_tgt\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Stop word file for target language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_dis\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to save self.discriminator\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_non_parallel\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to add disorder sentence while evaluating(sentence similarity)\"", ")", "\n", "# For predict", "\n", "parser", ".", "add_argument", "(", "\"--pred\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "help", "=", "\"Map source bert to target space\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"The source input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"The output file of mapped source language embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--cal_sent_sim\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "help", "=", "\"Calculate sentence similarity?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sim_with_map\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Calculate similarity with mapping?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--overlap_sim\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Calculate similarity of overlap words?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--base_embed\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Use base embeddings of BERT?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_input\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Apply mapping to the BERT input embeddings?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sim_file\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"output similarity file\"", ")", "\n", "# For non-linear mapping", "\n", "parser", ".", "add_argument", "(", "\"--non_linear\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Use non-linear mapping\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--activation\"", ",", "type", "=", "str", ",", "default", "=", "'leaky_relu'", ",", "help", "=", "\"learky_relu,tanh\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"mapping layer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_size\"", ",", "type", "=", "int", ",", "default", "=", "768", ",", "help", "=", "\"mapping hidden layer size\"", ")", "\n", "# parse parameters", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "advbert", "=", "AdvBert", "(", "args", ")", "\n", "\n", "if", "not", "args", ".", "pred", "and", "args", ".", "adversarial", ":", "\n", "        ", "advbert", ".", "train_adv", "(", ")", "\n", "\n", "", "if", "args", ".", "n_refinement", ">", "0", ":", "\n", "        ", "advbert", ".", "refine", "(", ")", "\n", "\n", "", "if", "args", ".", "pred", ":", "\n", "        ", "advbert", ".", "pred", "(", ")", "\n", "\n", "", "if", "args", ".", "cal_sent_sim", ":", "\n", "        ", "advbert", ".", "calculate_sim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_transform.SupervisedMap.__init__": [[87, 110], ["os.path.isfile", "os.path.isfile", "src.utils.initialize_exp", "src.models.build_supervised_model", "src.trainer.Trainer", "src.evaluation.Evaluator", "supervised_transform.SupervisedMap.trainer.load_training_dico", "torch.cuda.is_available", "os.path.isfile", "os.path.isfile"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.initialize_exp", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.models.build_supervised_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_training_dico"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "\n", "        ", "self", ".", "params", "=", "params", "\n", "# check parameters", "\n", "assert", "not", "params", ".", "cuda", "or", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "assert", "params", ".", "dico_train", "in", "[", "\"identical_char\"", ",", "\"default\"", "]", "or", "os", ".", "path", ".", "isfile", "(", "params", ".", "dico_train", ")", "\n", "assert", "params", ".", "dico_build", "in", "[", "\"S2T\"", ",", "\"T2S\"", ",", "\"S2T|T2S\"", ",", "\"S2T&T2S\"", "]", "\n", "assert", "params", ".", "dico_max_size", "==", "0", "or", "params", ".", "dico_max_size", "<", "params", ".", "dico_max_rank", "\n", "assert", "params", ".", "dico_max_size", "==", "0", "or", "params", ".", "dico_max_size", ">", "params", ".", "dico_min_size", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "params", ".", "src_emb", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "params", ".", "tgt_emb", ")", "\n", "assert", "params", ".", "dico_eval", "==", "'default'", "or", "os", ".", "path", ".", "isfile", "(", "params", ".", "dico_eval", ")", "\n", "assert", "params", ".", "export", "in", "[", "\"\"", ",", "\"txt\"", ",", "\"pth\"", "]", "\n", "\n", "# build self.logger / model / self.trainer / evaluator", "\n", "self", ".", "logger", "=", "initialize_exp", "(", "params", ")", "\n", "src_emb", ",", "tgt_emb", ",", "mapping", ",", "_", "=", "build_supervised_model", "(", "params", ",", "False", ")", "\n", "self", ".", "trainer", "=", "Trainer", "(", "src_emb", ",", "tgt_emb", ",", "mapping", ",", "None", ",", "params", ")", "\n", "evaluator", "=", "Evaluator", "(", "self", ".", "trainer", ")", "\n", "\n", "# load a training dictionary. if a dictionary path is not provided, use a default", "\n", "# one (\"default\") or create one based on identical character strings (\"identical_char\")", "\n", "self", ".", "trainer", ".", "load_training_dico", "(", "params", ".", "dico_train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_transform.SupervisedMap.train": [[115, 173], ["range", "os.path.exists", "os.makedirs", "open", "supervised_transform.SupervisedMap.trainer.get_aligned_id_batchs", "enumerate", "len", "len", "supervised_transform.SupervisedMap.logger.info", "supervised_transform.SupervisedMap.trainer.save_best", "supervised_transform.SupervisedMap.logger.info", "supervised_transform.SupervisedMap.trainer.reload_best", "supervised_transform.SupervisedMap.trainer.export", "supervised_transform.SupervisedMap.trainer.decay_map_lr", "supervised_transform.SupervisedMap.trainer.supervised_mapping_step", "len", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "supervised_transform.SupervisedMap.logger.info", "supervised_transform.SupervisedMap.trainer.save_model", "supervised_transform.SupervisedMap.logger.info", "open.write", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "str", "avg_cos_sim.cpu", "loss.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_aligned_id_batchs", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.reload_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.export", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.decay_map_lr", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Learning loop for Procrustes Iterative Learning\n        \"\"\"", "\n", "n_without_improvement", "=", "0", "\n", "min_loss", "=", "1e6", "\n", "path4loss", "=", "self", ".", "params", ".", "model_path", "+", "'/model4loss'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path4loss", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "path4loss", ")", "\n", "", "if", "self", ".", "params", ".", "save_all", ":", "\n", "            ", "model_log", "=", "open", "(", "path4loss", "+", "'/model.log'", ",", "'w'", ")", "\n", "", "for", "n_epoch", "in", "range", "(", "self", ".", "params", ".", "n_epochs", ")", ":", "\n", "\n", "#self.logger.info('Starting epoch %i...' % n_epoch)", "\n", "            ", "if", "(", "n_epoch", "+", "1", ")", "%", "self", ".", "params", ".", "decay_step", "==", "0", ":", "\n", "                ", "self", ".", "trainer", ".", "decay_map_lr", "(", ")", "\n", "", "batches", "=", "self", ".", "trainer", ".", "get_aligned_id_batchs", "(", ")", "\n", "n_inst", "=", "0", "\n", "to_log", "=", "{", "\"avg_cosine_similarity\"", ":", "0", ",", "\"loss\"", ":", "0", "}", "\n", "for", "i", ",", "(", "src_ids", ",", "tgt_ids", ")", "in", "enumerate", "(", "batches", ")", ":", "\n", "#print (src_ids, tgt_ids)", "\n", "                ", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "src_ids", ",", "tgt_ids", ")", "\n", "n_inst", "+=", "len", "(", "src_ids", ")", "\n", "cos_sim", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "#self.logger.info(\"Step:{}, Total Instances:{}, Cosine Similarity:{:.6f}, Loss:{:.6f}\".format(i, ", "\n", "#            n_inst, cos_sim, loss_))", "\n", "to_log", "[", "\"avg_cosine_similarity\"", "]", "+=", "cos_sim", "\n", "to_log", "[", "\"loss\"", "]", "+=", "loss_", "\n", "", "to_log", "[", "\"avg_cosine_similarity\"", "]", "/=", "len", "(", "batches", ")", "\n", "to_log", "[", "\"loss\"", "]", "/=", "len", "(", "batches", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"Epoch:{}, avg cos sim:{:.6f}, avg loss:{:.6f}, instances:{}\"", ".", "format", "(", "n_epoch", ",", "to_log", "[", "\"avg_cosine_similarity\"", "]", ",", "to_log", "[", "\"loss\"", "]", ",", "n_inst", ")", ")", "\n", "\n", "if", "to_log", "[", "\"avg_cosine_similarity\"", "]", "<=", "self", ".", "trainer", ".", "best_valid_metric", "and", "to_log", "[", "\"loss\"", "]", ">=", "min_loss", ":", "\n", "                ", "n_without_improvement", "+=", "1", "\n", "", "else", ":", "\n", "                ", "n_without_improvement", "=", "0", "\n", "", "if", "to_log", "[", "\"loss\"", "]", "<", "min_loss", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\" Minimum loss : {:.6f}\"", ".", "format", "(", "to_log", "[", "\"loss\"", "]", ")", ")", "\n", "if", "self", ".", "params", ".", "save_all", ":", "\n", "                    ", "save_path", "=", "path4loss", "+", "'/epoch-'", "+", "str", "(", "n_epoch", ")", "\n", "model_log", ".", "write", "(", "\"Epoch:{}, avg cos sim:{:.6f}, avg loss:{:.6f}\\n\"", ".", "format", "(", "n_epoch", ",", "\n", "to_log", "[", "\"avg_cosine_similarity\"", "]", ",", "to_log", "[", "\"loss\"", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "save_path", "=", "path4loss", "\n", "", "self", ".", "trainer", ".", "save_model", "(", "save_path", "+", "'/best_mapping.pkl'", ")", "\n", "min_loss", "=", "to_log", "[", "\"loss\"", "]", "\n", "", "self", ".", "trainer", ".", "save_best", "(", "to_log", ",", "\"avg_cosine_similarity\"", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"Max avg cos sim:{:.6f}, Min avg loss:{:.6f}\"", ".", "format", "(", "self", ".", "trainer", ".", "best_valid_metric", ",", "min_loss", ")", ")", "\n", "#self.logger.info('End of epoch %i.\\n\\n' % n_epoch)", "\n", "if", "n_without_improvement", ">=", "self", ".", "params", ".", "quit_after_n_epochs_without_improvement", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'After {} epochs without improvement, quiting!'", ".", "format", "(", "n_without_improvement", ")", ")", "\n", "break", "\n", "\n", "# export embeddings", "\n", "", "", "if", "self", ".", "params", ".", "export", ":", "\n", "            ", "self", ".", "trainer", ".", "reload_best", "(", ")", "\n", "self", ".", "trainer", ".", "export", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_transform.SupervisedMap.test": [[174, 195], ["supervised_transform.SupervisedMap.trainer.load_best", "supervised_transform.SupervisedMap.trainer.get_aligned_id_batchs", "enumerate", "len", "len", "supervised_transform.SupervisedMap.logger.info", "len", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "supervised_transform.SupervisedMap.logger.info", "torch.no_grad", "supervised_transform.SupervisedMap.trainer.supervised_mapping_step", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "avg_cos_sim.cpu", "loss.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_aligned_id_batchs", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step"], ["", "", "def", "test", "(", "self", ")", ":", "\n", "#self.trainer.reload_best()", "\n", "        ", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "batches", "=", "self", ".", "trainer", ".", "get_aligned_id_batchs", "(", "shuffle", "=", "False", ")", "\n", "n_inst", "=", "0", "\n", "to_log", "=", "{", "\"avg_cosine_similarity\"", ":", "0", ",", "\"loss\"", ":", "0", "}", "\n", "for", "i", ",", "(", "src_ids", ",", "tgt_ids", ")", "in", "enumerate", "(", "batches", ")", ":", "\n", "#print (src_ids, tgt_ids)", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "src_ids", ",", "tgt_ids", ")", "\n", "", "n_inst", "+=", "len", "(", "src_ids", ")", "\n", "cos_sim", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"Step:{}, Total Instances:{}, Cosine Similarity:{:.6f}, Loss:{:.6f}\"", ".", "format", "(", "i", ",", "\n", "n_inst", ",", "cos_sim", ",", "loss_", ")", ")", "\n", "to_log", "[", "\"avg_cosine_similarity\"", "]", "+=", "cos_sim", "\n", "to_log", "[", "\"loss\"", "]", "+=", "loss_", "\n", "", "to_log", "[", "\"avg_cosine_similarity\"", "]", "/=", "len", "(", "batches", ")", "\n", "to_log", "[", "\"loss\"", "]", "/=", "len", "(", "batches", ")", "\n", "\n", "self", ".", "logger", ".", "info", "(", "\"Average Cosine Similarity:{:.6f}, Average Loss:{:.6f}\"", ".", "format", "(", "to_log", "[", "\"avg_cosine_similarity\"", "]", ",", "to_log", "[", "\"loss\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_transform.main": [[23, 84], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "supervised_transform.SupervisedMap", "supervised_transform.SupervisedMap.train", "supervised_transform.SupervisedMap.test", "exit"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.train", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_transform.SupervisedMap.test"], ["def", "main", "(", ")", ":", "\n", "# main", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Supervised training'", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"Initialization seed\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Verbose level (2:debug, 1:info, 0:warning)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Where to store experiment logs and models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_name\"", ",", "type", "=", "str", ",", "default", "=", "\"debug\"", ",", "help", "=", "\"Experiment name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_id\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Experiment ID\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--cuda\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "help", "=", "\"Run on GPU\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--export\"", ",", "type", "=", "str", ",", "default", "=", "\"txt\"", ",", "help", "=", "\"Export embeddings after training (txt / pth)\"", ")", "\n", "# supervised sgd learning", "\n", "parser", ".", "add_argument", "(", "\"--n_epochs\"", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "\"Number of epochs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "\"Batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"sgd,lr=0.1,weight_decay=0.01\"", ",", "help", "=", "\"self.mapping optimizer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr_decay\"", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "\"Learning rate decay (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--decay_step\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"Learning rate decay step (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "help", "=", "\"Minimum learning rate (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--quit_after_n_epochs_without_improvement\"", ",", "type", "=", "int", ",", "default", "=", "500", ",", "help", "=", "\"Quit after n epochs without improvement\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--normalize_embed\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "help", "=", "\"Normalize embeddings? (should be false with l2_dist loss)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--loss\"", ",", "type", "=", "str", ",", "default", "=", "\"cos_sim\"", ",", "help", "=", "\"loss type (cos_sim, max_margin_top-k, l2_dist)\"", ")", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--src_lang\"", ",", "type", "=", "str", ",", "default", "=", "'en'", ",", "help", "=", "\"Source language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_lang\"", ",", "type", "=", "str", ",", "default", "=", "'es'", ",", "help", "=", "\"Target language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "\"Embedding dimension\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_vocab\"", ",", "type", "=", "int", ",", "default", "=", "200000", ",", "help", "=", "\"Maximum vocabulary size (-1 to disable)\"", ")", "\n", "# training refinement", "\n", "parser", ".", "add_argument", "(", "\"--n_refinement\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"Number of refinement iterations (0 to disable the refinement procedure)\"", ")", "\n", "# dictionary creation parameters (for refinement)", "\n", "parser", ".", "add_argument", "(", "\"--dico_train\"", ",", "type", "=", "str", ",", "default", "=", "\"default\"", ",", "help", "=", "\"Path to training dictionary (default: use identical character strings)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_eval\"", ",", "type", "=", "str", ",", "default", "=", "\"default\"", ",", "help", "=", "\"Path to evaluation dictionary\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_method\"", ",", "type", "=", "str", ",", "default", "=", "'csls_knn_10'", ",", "help", "=", "\"Method used for dictionary generation (nn/invsm_beta_30/csls_knn_10)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_build\"", ",", "type", "=", "str", ",", "default", "=", "'S2T&T2S'", ",", "help", "=", "\"S2T,T2S,S2T|T2S,S2T&T2S\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_threshold\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "\"Threshold confidence for dictionary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_max_rank\"", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "help", "=", "\"Maximum dictionary words rank (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_min_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"Minimum generated dictionary size (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dico_max_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"Maximum generated dictionary size (0 to disable)\"", ")", "\n", "# reload pre-trained embeddings", "\n", "parser", ".", "add_argument", "(", "\"--src_emb\"", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Reload source embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_emb\"", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Reload target embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--normalize_embeddings\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Normalize embeddings before training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Predict cosine similarity & L2 distance with input model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_all\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Save every model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_beta\"", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "\"Beta for orthogonalization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ortho\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Apply orthogonalize after each update\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--non_linear\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Use non-linear mapping\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--activation\"", ",", "type", "=", "str", ",", "default", "=", "'leaky_relu'", ",", "help", "=", "\"learky_relu,tanh\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"mapping layer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_size\"", ",", "type", "=", "int", ",", "default", "=", "768", ",", "help", "=", "\"mapping hidden layer size\"", ")", "\n", "# false flags", "\n", "parser", ".", "add_argument", "(", "\"--transformer\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "\"self_attention|attention\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fine_tune\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Fine tune on src BERT model\"", ")", "\n", "# parse parameters", "\n", "params", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "supmap", "=", "SupervisedMap", "(", "params", ")", "\n", "\n", "if", "params", ".", "test", ":", "\n", "        ", "supmap", ".", "test", "(", ")", "\n", "exit", "(", ")", "\n", "\n", "", "supmap", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert_supervised.load_conllu": [[13, 27], ["codecs.open", "codecs.getreader", "line.strip.strip", "line.strip.startswith", "re.match", "buff.append", "line.strip.split"], "function", ["None"], ["def", "load_conllu", "(", "file", ")", ":", "\n", "  ", "with", "codecs", ".", "open", "(", "file", ",", "'rb'", ")", "as", "f", ":", "\n", "    ", "reader", "=", "codecs", ".", "getreader", "(", "'utf-8'", ")", "(", "f", ")", "\n", "buff", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "      ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "and", "not", "line", ".", "startswith", "(", "'#'", ")", ":", "\n", "        ", "if", "not", "re", ".", "match", "(", "'[0-9]+[-.][0-9]+'", ",", "line", ")", ":", "\n", "          ", "buff", ".", "append", "(", "line", ".", "split", "(", "'\\t'", ")", "[", "1", "]", ")", "\n", "", "", "elif", "buff", ":", "\n", "        ", "yield", "buff", "\n", "buff", "=", "[", "]", "\n", "", "", "if", "buff", ":", "\n", "      ", "yield", "buff", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert_supervised.list_to_bert": [[28, 52], ["supervised_bert.Args", "supervised_bert.SupervisedBert", "supervised_bert.SupervisedBert.list2bert"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.list2bert"], ["", "", "", "def", "list_to_bert", "(", "sents", ",", "bert_file", ",", "layer", ",", "map_model", ",", "bert_model", ",", "max_seq", "=", "256", ",", "batch_size", "=", "8", ",", "\n", "map_input", "=", "False", ",", "map_type", "=", "'linear'", ",", "activation", "=", "\"leaky_relu\"", ",", "n_layers", "=", "2", ",", "\n", "hidden_size", "=", "768", ",", "num_attention_heads", "=", "12", ",", "input_bert", "=", "None", ")", ":", "\n", "  ", "model_path", "=", "map_model", "\n", "bert_config_file", "=", "bert_model", "+", "'/bert_config.json'", "\n", "vocab_file", "=", "bert_model", "+", "'/vocab.txt'", "\n", "init_checkpoint", "=", "bert_model", "+", "'/bert_model'", "\n", "output_file", "=", "bert_file", "\n", "bert_layer", "=", "layer", "\n", "max_seq_length", "=", "max_seq", "\n", "if", "input_bert", "is", "not", "None", ":", "\n", "    ", "load_pred_bert", "=", "True", "\n", "", "else", ":", "\n", "    ", "load_pred_bert", "=", "False", "\n", "", "flags", "=", "Args", "(", "model_path", ",", "vocab_file", ",", "bert_config_file", ",", "init_checkpoint", ",", "output_file", ",", "\n", "max_seq_length", ",", "bert_layer", ",", "map_type", "=", "map_type", ",", "activation", "=", "activation", ",", "\n", "n_layers", "=", "n_layers", ",", "hidden_size", "=", "hidden_size", ",", "load_pred_bert", "=", "load_pred_bert", ",", "\n", "bert_file0", "=", "input_bert", ")", "\n", "flags", ".", "batch_size", "=", "batch_size", "\n", "flags", ".", "map_input", "=", "map_input", "\n", "flags", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "\n", "sup_bert", "=", "SupervisedBert", "(", "flags", ")", "\n", "sup_bert", ".", "list2bert", "(", "sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert_supervised.merge": [[53, 137], ["codecs.open", "print", "codecs.open", "fin.readline", "print", "print", "json.loads", "range", "len", "codecs.open.write", "fin.readline", "os.path.dirname", "open", "info.write", "print", "len", "len", "merged[].append", "len", "print", "print", "print", "ValueError", "json.dumps", "item[].startswith", "enumerate", "enumerate", "len", "len", "len", "zip", "[].lower", "print", "tmp_layers.append", "tmp_layers[].append", "item[].startswith", "enumerate", "len", "print", "[].lower", "merged[].append", "merged[].append", "numpy.array", "tmp_layers[].append", "list", "len", "len", "len", "print", "[].lower", "len", "numpy.array", "numpy.array", "numpy.sum", "list", "len", "len", "len", "len", "numpy.mean", "list", "len", "list", "len", "int", "list", "len", "len", "len", "len"], "function", ["None"], ["", "def", "merge", "(", "bert_file", ",", "merge_file", ",", "sents", ",", "merge_type", "=", "'sum'", ")", ":", "\n", "  ", "merge_file", "=", "merge_file", "+", "'.'", "+", "merge_type", "\n", "n", "=", "0", "\n", "n_unk", "=", "0", "\n", "n_tok", "=", "0", "\n", "fo", "=", "codecs", ".", "open", "(", "merge_file", ",", "'w'", ")", "\n", "print", "(", "\"Merge Type: {}\"", ".", "format", "(", "merge_type", ")", ")", "\n", "with", "codecs", ".", "open", "(", "bert_file", ",", "'r'", ")", "as", "fin", ":", "\n", "    ", "line", "=", "fin", ".", "readline", "(", ")", "\n", "while", "line", ":", "\n", "      ", "if", "n", "%", "100", "==", "0", ":", "\n", "        ", "print", "(", "\"\\r%d\"", "%", "n", ",", "end", "=", "''", ")", "\n", "", "bert", "=", "json", ".", "loads", "(", "line", ")", "\n", "tokens", "=", "[", "]", "\n", "merged", "=", "{", "\"linex_index\"", ":", "bert", "[", "\"linex_index\"", "]", ",", "\"features\"", ":", "[", "]", "}", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "bert", "[", "\"features\"", "]", ")", ":", "\n", "        ", "item", "=", "bert", "[", "\"features\"", "]", "[", "i", "]", "\n", "if", "item", "[", "\"token\"", "]", "==", "\"[CLS]\"", "or", "item", "[", "\"token\"", "]", "==", "\"[SEP]\"", ":", "\n", "          ", "merged", "[", "\"features\"", "]", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "\"token\"", "]", ".", "startswith", "(", "\"##\"", ")", "and", "not", "(", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "<", "len", "(", "sents", "[", "n", "]", ")", "and", "item", "[", "\"token\"", "]", "==", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "]", ")", ":", "\n", "          ", "tmp_layers", "=", "[", "]", "\n", "for", "j", ",", "layer", "in", "enumerate", "(", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", ")", ":", "\n", "#merged[\"features\"][-1][\"layers\"][j][\"values\"] = list(np.array(layer[\"values\"]) + np.array(item[\"layers\"][j][\"values\"]))", "\n", "# j-th layer", "\n", "            ", "tmp_layers", ".", "append", "(", "[", "np", ".", "array", "(", "layer", "[", "\"values\"", "]", ")", "]", ")", "\n", "tmp_layers", "[", "j", "]", ".", "append", "(", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", ")", ")", "\n", "\n", "", "item", "=", "bert", "[", "\"features\"", "]", "[", "i", "+", "1", "]", "\n", "while", "item", "[", "\"token\"", "]", ".", "startswith", "(", "\"##\"", ")", "and", "not", "(", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "<", "len", "(", "sents", "[", "n", "]", ")", "and", "item", "[", "\"token\"", "]", "==", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "]", ")", ":", "\n", "            ", "for", "j", ",", "layer", "in", "enumerate", "(", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", ")", ":", "\n", "# j-th layer", "\n", "              ", "tmp_layers", "[", "j", "]", ".", "append", "(", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", ")", ")", "\n", "", "i", "+=", "1", "\n", "item", "=", "bert", "[", "\"features\"", "]", "[", "i", "+", "1", "]", "\n", "", "for", "j", ",", "layer", "in", "enumerate", "(", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", ")", ":", "\n", "            ", "if", "merge_type", "==", "'sum'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "np", ".", "sum", "(", "tmp_layers", "[", "j", "]", ",", "0", ")", ")", "\n", "", "elif", "merge_type", "==", "'avg'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "np", ".", "mean", "(", "tmp_layers", "[", "j", "]", ",", "0", ")", ")", "\n", "", "elif", "merge_type", "==", "'first'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "tmp_layers", "[", "j", "]", "[", "0", "]", ")", "\n", "", "elif", "merge_type", "==", "'last'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "tmp_layers", "[", "j", "]", "[", "-", "1", "]", ")", "\n", "", "elif", "merge_type", "==", "'mid'", ":", "\n", "              ", "mid", "=", "int", "(", "len", "(", "tmp_layers", "[", "j", "]", ")", "/", "2", ")", "\n", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "tmp_layers", "[", "j", "]", "[", "mid", "]", ")", "\n", "", "", "if", "len", "(", "sents", "[", "n", "]", ")", "<", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", ":", "\n", "            ", "print", "(", "sents", "[", "n", "]", ",", "len", "(", "merged", "[", "\"features\"", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"token\"", "]", "=", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "2", "]", ".", "lower", "(", ")", "\n", "", "", "elif", "item", "[", "\"token\"", "]", "==", "\"[UNK]\"", ":", "\n", "          ", "n_unk", "+=", "1", "\n", "merged", "[", "\"features\"", "]", ".", "append", "(", "item", ")", "\n", "if", "len", "(", "sents", "[", "n", "]", ")", "<", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", ":", "\n", "            ", "print", "(", "sents", "[", "n", "]", ",", "len", "(", "merged", "[", "\"features\"", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"token\"", "]", "=", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "2", "]", ".", "lower", "(", ")", "\n", "", "", "else", ":", "\n", "          ", "merged", "[", "\"features\"", "]", ".", "append", "(", "item", ")", "\n", "", "i", "+=", "1", "\n", "", "try", ":", "\n", "        ", "assert", "len", "(", "merged", "[", "\"features\"", "]", ")", "==", "len", "(", "sents", "[", "n", "]", ")", "+", "2", "\n", "", "except", ":", "\n", "        ", "orig", "=", "[", "m", "[", "\"token\"", "]", "for", "m", "in", "merged", "[", "\"features\"", "]", "]", "\n", "print", "(", "'\\n'", ",", "len", "(", "merged", "[", "\"features\"", "]", ")", ",", "len", "(", "sents", "[", "n", "]", ")", ")", "\n", "print", "(", "sents", "[", "n", "]", ",", "'\\n'", ",", "orig", ")", "\n", "print", "(", "zip", "(", "sents", "[", "n", "]", ",", "orig", "[", "1", ":", "-", "1", "]", ")", ")", "\n", "raise", "ValueError", "(", "\"Sentence-{}:{}\"", ".", "format", "(", "n", ",", "' '", ".", "join", "(", "sents", "[", "n", "]", ")", ")", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "sents", "[", "n", "]", ")", ")", ":", "\n", "        ", "try", ":", "\n", "          ", "assert", "sents", "[", "n", "]", "[", "i", "]", ".", "lower", "(", ")", "==", "merged", "[", "\"features\"", "]", "[", "i", "+", "1", "]", "[", "\"token\"", "]", "\n", "", "except", ":", "\n", "          ", "print", "(", "'wrong word id:{}, word:{}'", ".", "format", "(", "i", ",", "sents", "[", "n", "]", "[", "i", "]", ")", ")", "\n", "\n", "", "", "n_tok", "+=", "len", "(", "sents", "[", "n", "]", ")", "\n", "fo", ".", "write", "(", "json", ".", "dumps", "(", "merged", ")", "+", "\"\\n\"", ")", "\n", "line", "=", "fin", ".", "readline", "(", ")", "\n", "n", "+=", "1", "\n", "", "print", "(", "'Total tokens:{}, UNK tokens:{}'", ".", "format", "(", "n_tok", ",", "n_unk", ")", ")", "\n", "info_file", "=", "os", ".", "path", ".", "dirname", "(", "merge_file", ")", "+", "'/README.txt'", "\n", "print", "(", "info_file", ")", "\n", "with", "open", "(", "info_file", ",", "'a'", ")", "as", "info", ":", "\n", "      ", "info", ".", "write", "(", "'File:{}\\nTotal tokens:{}, UNK tokens:{}\\n\\n'", ".", "format", "(", "merge_file", ",", "n_tok", ",", "n_unk", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.load_conllu": [[13, 27], ["codecs.open", "codecs.getreader", "line.strip.strip", "line.strip.startswith", "re.match", "buff.append", "line.strip.split"], "function", ["None"], ["def", "load_conllu", "(", "file", ")", ":", "\n", "  ", "with", "codecs", ".", "open", "(", "file", ",", "'rb'", ")", "as", "f", ":", "\n", "    ", "reader", "=", "codecs", ".", "getreader", "(", "'utf-8'", ")", "(", "f", ")", "\n", "buff", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "      ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "and", "not", "line", ".", "startswith", "(", "'#'", ")", ":", "\n", "        ", "if", "not", "re", ".", "match", "(", "'[0-9]+[-.][0-9]+'", ",", "line", ")", ":", "\n", "          ", "buff", ".", "append", "(", "line", ".", "split", "(", "'\\t'", ")", "[", "1", "]", ")", "\n", "", "", "elif", "buff", ":", "\n", "        ", "yield", "buff", "\n", "buff", "=", "[", "]", "\n", "", "", "if", "buff", ":", "\n", "      ", "yield", "buff", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.to_raw": [[28, 32], ["codecs.open", "fo.write"], "function", ["None"], ["", "", "", "def", "to_raw", "(", "sents", ",", "file", ")", ":", "\n", "  ", "with", "codecs", ".", "open", "(", "file", ",", "'w'", ")", "as", "fo", ":", "\n", "    ", "for", "sent", "in", "sents", ":", "\n", "      ", "fo", ".", "write", "(", "(", "\" \"", ".", "join", "(", "sent", ")", ")", ".", "encode", "(", "'utf-8'", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.bert": [[33, 39], ["print", "os.system"], "function", ["None"], ["", "", "", "def", "bert", "(", "raw_file", ",", "bert_file", ",", "gpu", ",", "layer", ",", "model", ",", "max_seq", "=", "'256'", ",", "batch_size", "=", "'8'", ")", ":", "\n", "  ", "dict", "=", "{", "'gpu'", ":", "gpu", ",", "'input'", ":", "raw_file", ",", "'output'", ":", "bert_file", ",", "'layer'", ":", "layer", ",", "'BERT_BASE_DIR'", ":", "model", ",", "'max_seq'", ":", "max_seq", ",", "'batch_size'", ":", "batch_size", "}", "\n", "cmd", "=", "\"source /users2/yxwang/work/env/py2.7/bin/activate ; CUDA_VISIBLE_DEVICES={gpu} python extract_features.py --input_file={input} --output_file={output} --vocab_file={BERT_BASE_DIR}/vocab.txt --bert_config_file={BERT_BASE_DIR}/bert_config.json --init_checkpoint={BERT_BASE_DIR}/bert_model.ckpt --layers={layer} --max_seq_length={max_seq} --batch_size={batch_size}\"", ".", "format", "(", "**", "dict", ")", "\n", "#cmd = \"./scripts/extract.sh {gpu} {input} {output} {layer}\".format(**dict)", "\n", "print", "(", "cmd", ")", "\n", "os", ".", "system", "(", "cmd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.list_to_bert": [[40, 58], ["bert_gan.Args", "bert_gan.AdvBert", "bert_gan.AdvBert.list2bert"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.list2bert"], ["", "def", "list_to_bert", "(", "sents", ",", "bert_file", ",", "layer", ",", "map_model", ",", "bert_model", ",", "max_seq", "=", "256", ",", "batch_size", "=", "8", ",", "\n", "map_input", "=", "False", ",", "non_linear", "=", "False", ",", "activation", "=", "\"leaky_relu\"", ",", "n_layers", "=", "2", ",", "\n", "hidden_size", "=", "768", ")", ":", "\n", "  ", "model_path", "=", "map_model", "\n", "bert_config_file", "=", "bert_model", "+", "'/bert_config.json'", "\n", "vocab_file", "=", "bert_model", "+", "'/vocab.txt'", "\n", "init_checkpoint", "=", "bert_model", "+", "'/bert_model'", "\n", "output_file", "=", "bert_file", "\n", "bert_layer", "=", "layer", "\n", "max_seq_length", "=", "max_seq", "\n", "flags", "=", "Args", "(", "model_path", ",", "vocab_file", ",", "bert_config_file", ",", "init_checkpoint", ",", "output_file", ",", "\n", "max_seq_length", ",", "bert_layer", ",", "non_linear", "=", "non_linear", ",", "activation", "=", "activation", ",", "\n", "n_layers", "=", "n_layers", ",", "hidden_size", "=", "hidden_size", ")", "\n", "flags", ".", "batch_size", "=", "batch_size", "\n", "flags", ".", "map_input", "=", "map_input", "\n", "\n", "adv_bert", "=", "AdvBert", "(", "flags", ")", "\n", "adv_bert", ".", "list2bert", "(", "sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.merge": [[59, 143], ["codecs.open", "print", "codecs.open", "fin.readline", "print", "print", "json.loads", "range", "len", "codecs.open.write", "fin.readline", "os.path.dirname", "open", "info.write", "print", "len", "len", "merged[].append", "len", "print", "print", "print", "ValueError", "json.dumps", "item[].startswith", "enumerate", "enumerate", "len", "len", "len", "zip", "[].lower", "print", "tmp_layers.append", "tmp_layers[].append", "item[].startswith", "enumerate", "len", "print", "[].lower", "merged[].append", "merged[].append", "numpy.array", "tmp_layers[].append", "list", "len", "len", "len", "print", "[].lower", "len", "numpy.array", "numpy.array", "numpy.sum", "list", "len", "len", "len", "len", "numpy.mean", "list", "len", "list", "len", "int", "list", "len", "len", "len", "len"], "function", ["None"], ["", "def", "merge", "(", "bert_file", ",", "merge_file", ",", "sents", ",", "merge_type", "=", "'sum'", ")", ":", "\n", "  ", "merge_file", "=", "merge_file", "+", "'.'", "+", "merge_type", "\n", "n", "=", "0", "\n", "n_unk", "=", "0", "\n", "n_tok", "=", "0", "\n", "fo", "=", "codecs", ".", "open", "(", "merge_file", ",", "'w'", ")", "\n", "print", "(", "\"Merge Type: {}\"", ".", "format", "(", "merge_type", ")", ")", "\n", "with", "codecs", ".", "open", "(", "bert_file", ",", "'r'", ")", "as", "fin", ":", "\n", "    ", "line", "=", "fin", ".", "readline", "(", ")", "\n", "while", "line", ":", "\n", "      ", "if", "n", "%", "100", "==", "0", ":", "\n", "        ", "print", "(", "\"\\r%d\"", "%", "n", ",", "end", "=", "''", ")", "\n", "", "bert", "=", "json", ".", "loads", "(", "line", ")", "\n", "tokens", "=", "[", "]", "\n", "merged", "=", "{", "\"linex_index\"", ":", "bert", "[", "\"linex_index\"", "]", ",", "\"features\"", ":", "[", "]", "}", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "bert", "[", "\"features\"", "]", ")", ":", "\n", "        ", "item", "=", "bert", "[", "\"features\"", "]", "[", "i", "]", "\n", "if", "item", "[", "\"token\"", "]", "==", "\"[CLS]\"", "or", "item", "[", "\"token\"", "]", "==", "\"[SEP]\"", ":", "\n", "          ", "merged", "[", "\"features\"", "]", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "\"token\"", "]", ".", "startswith", "(", "\"##\"", ")", "and", "not", "(", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "<", "len", "(", "sents", "[", "n", "]", ")", "and", "item", "[", "\"token\"", "]", "==", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "]", ")", ":", "\n", "          ", "tmp_layers", "=", "[", "]", "\n", "for", "j", ",", "layer", "in", "enumerate", "(", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", ")", ":", "\n", "#merged[\"features\"][-1][\"layers\"][j][\"values\"] = list(np.array(layer[\"values\"]) + np.array(item[\"layers\"][j][\"values\"]))", "\n", "# j-th layer", "\n", "            ", "tmp_layers", ".", "append", "(", "[", "np", ".", "array", "(", "layer", "[", "\"values\"", "]", ")", "]", ")", "\n", "tmp_layers", "[", "j", "]", ".", "append", "(", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", ")", ")", "\n", "\n", "", "item", "=", "bert", "[", "\"features\"", "]", "[", "i", "+", "1", "]", "\n", "while", "item", "[", "\"token\"", "]", ".", "startswith", "(", "\"##\"", ")", "and", "not", "(", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "<", "len", "(", "sents", "[", "n", "]", ")", "and", "item", "[", "\"token\"", "]", "==", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", "]", ")", ":", "\n", "            ", "for", "j", ",", "layer", "in", "enumerate", "(", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", ")", ":", "\n", "# j-th layer", "\n", "              ", "tmp_layers", "[", "j", "]", ".", "append", "(", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", ")", ")", "\n", "", "i", "+=", "1", "\n", "item", "=", "bert", "[", "\"features\"", "]", "[", "i", "+", "1", "]", "\n", "", "for", "j", ",", "layer", "in", "enumerate", "(", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", ")", ":", "\n", "            ", "if", "merge_type", "==", "'sum'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "np", ".", "sum", "(", "tmp_layers", "[", "j", "]", ",", "0", ")", ")", "\n", "", "elif", "merge_type", "==", "'avg'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "np", ".", "mean", "(", "tmp_layers", "[", "j", "]", ",", "0", ")", ")", "\n", "", "elif", "merge_type", "==", "'first'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "tmp_layers", "[", "j", "]", "[", "0", "]", ")", "\n", "", "elif", "merge_type", "==", "'last'", ":", "\n", "              ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "tmp_layers", "[", "j", "]", "[", "-", "1", "]", ")", "\n", "", "elif", "merge_type", "==", "'mid'", ":", "\n", "              ", "mid", "=", "int", "(", "len", "(", "tmp_layers", "[", "j", "]", ")", "/", "2", ")", "\n", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"layers\"", "]", "[", "j", "]", "[", "\"values\"", "]", "=", "list", "(", "tmp_layers", "[", "j", "]", "[", "mid", "]", ")", "\n", "", "", "if", "len", "(", "sents", "[", "n", "]", ")", "<", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", ":", "\n", "            ", "print", "(", "sents", "[", "n", "]", ",", "len", "(", "merged", "[", "\"features\"", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"token\"", "]", "=", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "2", "]", ".", "lower", "(", ")", "\n", "", "", "elif", "item", "[", "\"token\"", "]", "==", "\"[UNK]\"", ":", "\n", "          ", "n_unk", "+=", "1", "\n", "merged", "[", "\"features\"", "]", ".", "append", "(", "item", ")", "\n", "if", "len", "(", "sents", "[", "n", "]", ")", "<", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "1", ":", "\n", "            ", "print", "(", "sents", "[", "n", "]", ",", "len", "(", "merged", "[", "\"features\"", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "merged", "[", "\"features\"", "]", "[", "-", "1", "]", "[", "\"token\"", "]", "=", "sents", "[", "n", "]", "[", "len", "(", "merged", "[", "\"features\"", "]", ")", "-", "2", "]", ".", "lower", "(", ")", "\n", "", "", "else", ":", "\n", "          ", "merged", "[", "\"features\"", "]", ".", "append", "(", "item", ")", "\n", "", "i", "+=", "1", "\n", "", "try", ":", "\n", "        ", "assert", "len", "(", "merged", "[", "\"features\"", "]", ")", "==", "len", "(", "sents", "[", "n", "]", ")", "+", "2", "\n", "", "except", ":", "\n", "        ", "orig", "=", "[", "m", "[", "\"token\"", "]", "for", "m", "in", "merged", "[", "\"features\"", "]", "]", "\n", "print", "(", "'\\n'", ",", "len", "(", "merged", "[", "\"features\"", "]", ")", ",", "len", "(", "sents", "[", "n", "]", ")", ")", "\n", "print", "(", "sents", "[", "n", "]", ",", "'\\n'", ",", "orig", ")", "\n", "print", "(", "zip", "(", "sents", "[", "n", "]", ",", "orig", "[", "1", ":", "-", "1", "]", ")", ")", "\n", "raise", "ValueError", "(", "\"Sentence-{}:{}\"", ".", "format", "(", "n", ",", "' '", ".", "join", "(", "sents", "[", "n", "]", ")", ")", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "sents", "[", "n", "]", ")", ")", ":", "\n", "        ", "try", ":", "\n", "          ", "assert", "sents", "[", "n", "]", "[", "i", "]", ".", "lower", "(", ")", "==", "merged", "[", "\"features\"", "]", "[", "i", "+", "1", "]", "[", "\"token\"", "]", "\n", "", "except", ":", "\n", "          ", "print", "(", "'wrong word id:{}, word:{}'", ".", "format", "(", "i", ",", "sents", "[", "n", "]", "[", "i", "]", ")", ")", "\n", "\n", "", "", "n_tok", "+=", "len", "(", "sents", "[", "n", "]", ")", "\n", "fo", ".", "write", "(", "json", ".", "dumps", "(", "merged", ")", "+", "\"\\n\"", ")", "\n", "line", "=", "fin", ".", "readline", "(", ")", "\n", "n", "+=", "1", "\n", "", "print", "(", "'Total tokens:{}, UNK tokens:{}'", ".", "format", "(", "n_tok", ",", "n_unk", ")", ")", "\n", "info_file", "=", "os", ".", "path", ".", "dirname", "(", "merge_file", ")", "+", "'/README.txt'", "\n", "print", "(", "info_file", ")", "\n", "with", "open", "(", "info_file", ",", "'a'", ")", "as", "info", ":", "\n", "      ", "info", ".", "write", "(", "'File:{}\\nTotal tokens:{}, UNK tokens:{}\\n\\n'", ".", "format", "(", "merge_file", ",", "n_tok", ",", "n_unk", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.Args.__init__": [[132, 172], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_path", ",", "vocab_file", ",", "bert_config_file", ",", "init_checkpoint", ",", "\n", "output_file", ",", "max_seq_length", "=", "128", ",", "bert_layer", "=", "-", "1", ",", "map_input", "=", "False", ",", "\n", "vocab_file1", "=", "None", ",", "bert_config_file1", "=", "None", ",", "init_checkpoint1", "=", "None", ",", "\n", "map_type", "=", "'linear'", ",", "activation", "=", "\"leaky_relu\"", ",", "n_layers", "=", "2", ",", "hidden_size", "=", "768", ",", "\n", "emb_dim", "=", "768", ",", "num_attention_heads", "=", "12", ",", "attention_probs_dropout_prob", "=", "0", ",", "\n", "hidden_dropout_prob", "=", "0", ",", "load_pred_bert", "=", "False", ",", "bert_file0", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "pred", "=", "True", "\n", "self", ".", "no_cuda", "=", "False", "\n", "self", ".", "cal_sent_sim", "=", "False", "\n", "self", ".", "local_rank", "=", "-", "1", "\n", "self", ".", "batch_size", "=", "32", "\n", "self", ".", "do_lower_case", "=", "True", "\n", "self", ".", "map_input", "=", "map_input", "\n", "self", ".", "map_type", "=", "map_type", "\n", "self", ".", "eval", "=", "False", "\n", "\n", "self", ".", "vocab_file", "=", "vocab_file", "\n", "self", ".", "bert_config_file", "=", "bert_config_file", "\n", "self", ".", "init_checkpoint", "=", "init_checkpoint", "\n", "self", ".", "model_path", "=", "model_path", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "self", ".", "output_file", "=", "output_file", "\n", "self", ".", "bert_layer", "=", "bert_layer", "\n", "\n", "self", ".", "vocab_file1", "=", "vocab_file1", "\n", "self", ".", "bert_config_file1", "=", "bert_config_file1", "\n", "self", ".", "init_checkpoint1", "=", "init_checkpoint1", "\n", "\n", "self", ".", "activation", "=", "activation", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "emb_dim", "=", "emb_dim", "\n", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "\n", "self", ".", "load_pred_bert", "=", "load_pred_bert", "\n", "self", ".", "bert_file0", "=", "bert_file0", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.__init__": [[175, 194], ["src.build_model.build_model", "src.utils.initialize_exp", "torch.device", "torch.device", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.build_model.build_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.initialize_exp"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "\n", "        ", "self", ".", "args", "=", "args", "\n", "# check parameters", "\n", "if", "not", "self", ".", "args", ".", "pred", ":", "\n", "#assert 0 < self.args.lr_shrink <= 1", "\n", "            ", "assert", "self", ".", "args", ".", "model_path", "is", "not", "None", "\n", "", "self", ".", "dataset", "=", "None", "\n", "# build model / trainer / evaluator", "\n", "if", "not", "(", "self", ".", "args", ".", "pred", "or", "self", ".", "args", ".", "eval", ")", ":", "\n", "            ", "self", ".", "logger", "=", "initialize_exp", "(", "self", ".", "args", ")", "\n", "\n", "", "self", ".", "bert_model", ",", "self", ".", "bert_model1", ",", "self", ".", "mapping", "=", "build_model", "(", "self", ".", "args", ",", "True", ")", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", "or", "self", ".", "args", ".", "no_cuda", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "self", ".", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "self", ".", "args", ".", "local_rank", ")", "\n", "", "self", ".", "transformer_types", "=", "[", "'self_attention'", ",", "'attention'", ",", "'linear_self_attention'", ",", "'nonlinear_self_attention'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.train": [[195, 315], ["src.supervised_bert_trainer.SupervisedBertTrainer", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "range", "src.load.load_from_bert", "src.load.load", "os.path.exists", "os.makedirs", "open", "supervised_bert.SupervisedBert.logger.info", "supervised_bert.SupervisedBert.logger.info", "supervised_bert.SupervisedBert.trainer.decay_map_lr", "supervised_bert.SupervisedBert.logger.info", "supervised_bert.SupervisedBert.trainer.save_model", "supervised_bert.SupervisedBert.trainer.save_best", "supervised_bert.SupervisedBert.logger.info", "align_ids_a.to.to.to", "align_ids_b.to.to.to", "align_mask.to.to.to", "supervised_bert.SupervisedBert.trainer.get_indexed_mapped_bert_from_bert", "supervised_bert.SupervisedBert.trainer.get_indexed_bert_from_bert", "supervised_bert.SupervisedBert.trainer.supervised_mapping_step", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "input_ids_a.to.to.to", "input_mask_a.to.to.to", "input_ids_b.to.to.to", "input_mask_b.to.to.to", "align_ids_a.to.to.to", "align_ids_b.to.to.to", "align_mask.to.to.to", "supervised_bert.SupervisedBert.trainer.get_indexed_mapped_bert", "supervised_bert.SupervisedBert.trainer.get_indexed_bert", "supervised_bert.SupervisedBert.trainer.supervised_mapping_step", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "open.write", "torch.no_grad", "input_embs_a.to.to.to", "input_mask_a.to.to.to", "input_embs_b.to.to.to", "supervised_bert.SupervisedBert.size", "supervised_bert.SupervisedBert.size", "str", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "avg_cos_sim.cpu", "loss.cpu", "avg_cos_sim.cpu", "loss.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.decay_map_lr", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_mapped_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_mapped_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "\n", "if", "self", ".", "args", ".", "load_pred_bert", ":", "\n", "            ", "assert", "self", ".", "args", ".", "bert_file0", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "bert_file1", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "vocab_file", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "vocab_file1", "is", "not", "None", "\n", "self", ".", "dataset", ",", "unique_id_to_feature", ",", "self", ".", "features", "=", "load_from_bert", "(", "self", ".", "args", ".", "vocab_file", ",", "self", ".", "args", ".", "bert_file0", ",", "\n", "self", ".", "args", ".", "bert_file1", ",", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "n_max_sent", "=", "self", ".", "args", ".", "n_max_sent", ",", "\n", "vocab_file1", "=", "self", ".", "args", ".", "vocab_file1", ",", "align_file", "=", "self", ".", "args", ".", "align_file", ",", "\n", "align_punc", "=", "self", ".", "args", ".", "align_punc", ",", "policy", "=", "self", ".", "args", ".", "align_policy", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dataset", ",", "unique_id_to_feature", ",", "self", ".", "features", "=", "load", "(", "self", ".", "args", ".", "vocab_file", ",", "self", ".", "args", ".", "input_file", ",", "\n", "batch_size", "=", "self", ".", "args", ".", "batch_size", ",", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "local_rank", "=", "self", ".", "args", ".", "local_rank", ",", "\n", "vocab_file1", "=", "self", ".", "args", ".", "vocab_file1", ",", "align_file", "=", "self", ".", "args", ".", "align_file", ")", "\n", "", "self", ".", "trainer", "=", "SupervisedBertTrainer", "(", "self", ".", "bert_model", ",", "self", ".", "mapping", ",", "\n", "self", ".", "args", ",", "bert_model1", "=", "self", ".", "bert_model1", ",", "trans_types", "=", "self", ".", "transformer_types", ")", "\n", "\n", "sampler", "=", "RandomSampler", "(", "self", ".", "dataset", ")", "\n", "train_loader", "=", "DataLoader", "(", "self", ".", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "n_without_improvement", "=", "0", "\n", "min_loss", "=", "1e6", "\n", "path4loss", "=", "self", ".", "args", ".", "model_path", "+", "'/model4loss'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path4loss", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "path4loss", ")", "\n", "", "if", "self", ".", "args", ".", "save_all", ":", "\n", "            ", "model_log", "=", "open", "(", "path4loss", "+", "'/model.log'", ",", "'w'", ")", "\n", "\n", "# training loop", "\n", "", "for", "n_epoch", "in", "range", "(", "self", ".", "args", ".", "n_epochs", ")", ":", "\n", "#self.logger.info('Starting epoch %i...' % n_epoch)", "\n", "            ", "if", "(", "n_epoch", "+", "1", ")", "%", "self", ".", "args", ".", "decay_step", "==", "0", ":", "\n", "                ", "self", ".", "trainer", ".", "decay_map_lr", "(", ")", "\n", "", "n_inst", "=", "0", "\n", "n_batch", "=", "0", "\n", "to_log", "=", "{", "\"avg_cosine_similarity\"", ":", "0", ",", "\"loss\"", ":", "0", "}", "\n", "\n", "if", "self", ".", "args", ".", "load_pred_bert", ":", "\n", "                ", "for", "input_embs_a", ",", "input_mask_a", ",", "input_embs_b", ",", "input_mask_b", ",", "align_ids_a", ",", "align_ids_b", ",", "align_mask", ",", "example_indices", "in", "train_loader", ":", "\n", "                    ", "n_batch", "+=", "1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "input_embs_a", "=", "input_embs_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_a", "=", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_embs_b", "=", "input_embs_b", ".", "to", "(", "self", ".", "device", ")", "\n", "", "align_ids_a", "=", "align_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "align_ids_b", "=", "align_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "align_mask", "=", "align_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "#print (align_ids_a, align_ids_b, align_mask)", "\n", "src_bert", "=", "self", ".", "trainer", ".", "get_indexed_mapped_bert_from_bert", "(", "\n", "input_embs_a", ",", "input_mask_a", ",", "align_ids_a", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "tgt_bert", "=", "self", ".", "trainer", ".", "get_indexed_bert_from_bert", "(", "\n", "input_embs_b", ",", "align_ids_b", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "\n", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "src_bert", ",", "tgt_bert", ")", "\n", "n_inst", "+=", "src_bert", ".", "size", "(", ")", "[", "0", "]", "\n", "cos_sim", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "to_log", "[", "\"avg_cosine_similarity\"", "]", "+=", "cos_sim", "\n", "to_log", "[", "\"loss\"", "]", "+=", "loss_", "\n", "", "", "else", ":", "\n", "                ", "for", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "align_ids_a", ",", "align_ids_b", ",", "align_mask", ",", "example_indices", "in", "train_loader", ":", "\n", "                    ", "n_batch", "+=", "1", "\n", "input_ids_a", "=", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_a", "=", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_ids_b", "=", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_b", "=", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", "\n", "align_ids_a", "=", "align_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "align_ids_b", "=", "align_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "align_mask", "=", "align_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "#print (align_ids_a, align_ids_b, align_mask)", "\n", "src_bert", "=", "self", ".", "trainer", ".", "get_indexed_mapped_bert", "(", "\n", "input_ids_a", ",", "input_mask_a", ",", "align_ids_a", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "\n", "tgt_bert", "=", "self", ".", "trainer", ".", "get_indexed_bert", "(", "\n", "input_ids_b", ",", "input_mask_b", ",", "align_ids_b", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", "\n", "\n", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "src_bert", ",", "tgt_bert", ")", "\n", "n_inst", "+=", "src_bert", ".", "size", "(", ")", "[", "0", "]", "\n", "cos_sim", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "to_log", "[", "\"avg_cosine_similarity\"", "]", "+=", "cos_sim", "\n", "to_log", "[", "\"loss\"", "]", "+=", "loss_", "\n", "", "", "to_log", "[", "\"avg_cosine_similarity\"", "]", "/=", "n_batch", "\n", "to_log", "[", "\"loss\"", "]", "/=", "n_batch", "\n", "self", ".", "logger", ".", "info", "(", "\"Epoch:{}, avg cos sim:{:.6f}, avg loss:{:.6f}, instances:{}\"", ".", "format", "(", "n_epoch", ",", "to_log", "[", "\"avg_cosine_similarity\"", "]", ",", "to_log", "[", "\"loss\"", "]", ",", "n_inst", ")", ")", "\n", "\n", "if", "to_log", "[", "\"avg_cosine_similarity\"", "]", "<=", "self", ".", "trainer", ".", "best_valid_metric", "and", "to_log", "[", "\"loss\"", "]", ">=", "min_loss", ":", "\n", "                ", "n_without_improvement", "+=", "1", "\n", "", "else", ":", "\n", "                ", "n_without_improvement", "=", "0", "\n", "", "if", "to_log", "[", "\"loss\"", "]", "<", "min_loss", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\" Minimum loss : {:.6f}\"", ".", "format", "(", "to_log", "[", "\"loss\"", "]", ")", ")", "\n", "if", "self", ".", "args", ".", "save_all", ":", "\n", "                    ", "save_path", "=", "path4loss", "+", "'/epoch-'", "+", "str", "(", "n_epoch", ")", "\n", "model_log", ".", "write", "(", "\"Epoch:{}, avg cos sim:{:.6f}, avg loss:{:.6f}\\n\"", ".", "format", "(", "n_epoch", ",", "\n", "to_log", "[", "\"avg_cosine_similarity\"", "]", ",", "to_log", "[", "\"loss\"", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "save_path", "=", "path4loss", "\n", "", "self", ".", "trainer", ".", "save_model", "(", "save_path", "+", "'/best_mapping.pkl'", ")", "\n", "min_loss", "=", "to_log", "[", "\"loss\"", "]", "\n", "", "if", "self", ".", "args", ".", "save_sim", ":", "\n", "                ", "self", ".", "trainer", ".", "save_best", "(", "to_log", ",", "\"avg_cosine_similarity\"", ")", "\n", "", "else", ":", "\n", "                ", "if", "to_log", "[", "\"avg_cosine_similarity\"", "]", ">", "self", ".", "trainer", ".", "best_valid_metric", ":", "\n", "                    ", "self", ".", "trainer", ".", "best_valid_metric", "=", "to_log", "[", "\"avg_cosine_similarity\"", "]", "\n", "", "", "self", ".", "logger", ".", "info", "(", "\"Max avg cos sim:{:.6f}, Min avg loss:{:.6f}\"", ".", "format", "(", "self", ".", "trainer", ".", "best_valid_metric", ",", "min_loss", ")", ")", "\n", "#self.logger.info('End of epoch %i.\\n\\n' % n_epoch)", "\n", "if", "n_without_improvement", ">=", "self", ".", "args", ".", "quit_after_n_epochs_without_improvement", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'After {} epochs without improvement, quiting!'", ".", "format", "(", "n_without_improvement", ")", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd": [[316, 368], ["src.supervised_bert_trainer.SupervisedBertTrainer", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "src.load.load_from_bert", "supervised_bert.SupervisedBert.logger.info", "align_ids_a.to.to.to", "align_ids_b.to.to.to", "align_mask.to.to.to", "supervised_bert.SupervisedBert.trainer.get_indexed_bert_from_bert", "supervised_bert.SupervisedBert.trainer.get_indexed_bert_from_bert", "supervised_bert.SupervisedBert.trainer.supervised_mapping_step", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "supervised_bert.SupervisedBert.logger.info", "supervised_bert.SupervisedBert.trainer.procrustes", "supervised_bert.SupervisedBert.trainer.save_model", "supervised_bert.SupervisedBert.trainer.get_indexed_mapped_bert_from_bert", "supervised_bert.SupervisedBert.trainer.supervised_mapping_step", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "supervised_bert.SupervisedBert.logger.info", "len", "torch.no_grad", "input_embs_a.to.to.to", "input_mask_a.to.to.to", "input_embs_b.to.to.to", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "avg_cos_sim.cpu", "loss.cpu", "avg_cos_sim.cpu", "loss.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.procrustes", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_mapped_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step"], ["", "", "", "def", "svd", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "load_pred_bert", ":", "\n", "            ", "assert", "self", ".", "args", ".", "bert_file0", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "bert_file1", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "vocab_file", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "vocab_file1", "is", "not", "None", "\n", "self", ".", "dataset", ",", "unique_id_to_feature", ",", "self", ".", "features", "=", "load_from_bert", "(", "self", ".", "args", ".", "vocab_file", ",", "self", ".", "args", ".", "bert_file0", ",", "\n", "self", ".", "args", ".", "bert_file1", ",", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "n_max_sent", "=", "self", ".", "args", ".", "n_max_sent", ",", "\n", "vocab_file1", "=", "self", ".", "args", ".", "vocab_file1", ",", "align_file", "=", "self", ".", "args", ".", "align_file", ",", "\n", "align_punc", "=", "self", ".", "args", ".", "align_punc", ",", "policy", "=", "self", ".", "args", ".", "align_policy", ")", "\n", "\n", "", "self", ".", "trainer", "=", "SupervisedBertTrainer", "(", "self", ".", "bert_model", ",", "self", ".", "mapping", ",", "\n", "self", ".", "args", ",", "bert_model1", "=", "self", ".", "bert_model1", ",", "trans_types", "=", "self", ".", "transformer_types", ")", "\n", "\n", "sampler", "=", "SequentialSampler", "(", "self", ".", "dataset", ")", "\n", "train_loader", "=", "DataLoader", "(", "self", ".", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "len", "(", "self", ".", "dataset", ")", ")", "\n", "\n", "self", ".", "trainer", ".", "args", ".", "loss", "=", "'l2_dist'", "\n", "for", "input_embs_a", ",", "input_mask_a", ",", "input_embs_b", ",", "input_mask_b", ",", "align_ids_a", ",", "align_ids_b", ",", "align_mask", ",", "example_indices", "in", "train_loader", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "\"Applying SVD\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "input_embs_a", "=", "input_embs_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_a", "=", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_embs_b", "=", "input_embs_b", ".", "to", "(", "self", ".", "device", ")", "\n", "", "align_ids_a", "=", "align_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "align_ids_b", "=", "align_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "align_mask", "=", "align_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "#print (align_ids_a, align_ids_b, align_mask)", "\n", "src_bert", "=", "self", ".", "trainer", ".", "get_indexed_bert_from_bert", "(", "\n", "input_embs_a", ",", "align_ids_a", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "tgt_bert", "=", "self", ".", "trainer", ".", "get_indexed_bert_from_bert", "(", "\n", "input_embs_b", ",", "align_ids_b", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "src_bert", ",", "tgt_bert", ",", "eval_only", "=", "True", ")", "\n", "avg_cos_sim_0", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_0", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"Before mapping: avg cos sim:{:.6f}, avg l2 distance:{:.6f}\"", ".", "format", "(", "avg_cos_sim_0", ",", "loss_0", ")", ")", "\n", "\n", "self", ".", "trainer", ".", "procrustes", "(", "src_bert", ",", "tgt_bert", ")", "\n", "self", ".", "trainer", ".", "save_model", "(", "self", ".", "args", ".", "model_path", "+", "'/best_mapping.pkl'", ")", "\n", "\n", "mapped_src_bert", "=", "self", ".", "trainer", ".", "get_indexed_mapped_bert_from_bert", "(", "\n", "input_embs_a", ",", "input_mask_a", ",", "align_ids_a", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "mapped_src_bert", ",", "tgt_bert", ",", "eval_only", "=", "True", ")", "\n", "avg_cos_sim_1", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_1", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"After mapping: avg cos sim:{:.6f}, avg l2 distance:{:.6f}\"", ".", "format", "(", "avg_cos_sim_1", ",", "loss_1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval": [[369, 419], ["src.supervised_bert_trainer.SupervisedBertTrainer", "supervised_bert.SupervisedBert.trainer.load_best", "supervised_bert.SupervisedBert.trainer.mapping.eval", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "print", "src.load.load_from_bert", "align_ids_a.to.to.to", "align_ids_b.to.to.to", "align_mask.to.to.to", "supervised_bert.SupervisedBert.trainer.get_indexed_mapped_bert_from_bert", "supervised_bert.SupervisedBert.trainer.get_indexed_bert_from_bert", "supervised_bert.SupervisedBert.trainer.supervised_mapping_step", "avg_cos_sim.cpu().detach().numpy", "loss.cpu().detach().numpy", "torch.no_grad", "input_embs_a.to.to.to", "input_mask_a.to.to.to", "input_embs_b.to.to.to", "supervised_bert.SupervisedBert.size", "avg_cos_sim.cpu().detach", "loss.cpu().detach", "avg_cos_sim.cpu", "loss.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_mapped_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert_from_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "load_pred_bert", ":", "\n", "            ", "assert", "self", ".", "args", ".", "bert_file0", "is", "not", "None", "\n", "assert", "self", ".", "args", ".", "bert_file1", "is", "not", "None", "\n", "self", ".", "dataset", ",", "unique_id_to_feature", ",", "self", ".", "features", "=", "load_from_bert", "(", "self", ".", "args", ".", "vocab_file", ",", "self", ".", "args", ".", "bert_file0", ",", "\n", "self", ".", "args", ".", "bert_file1", ",", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "n_max_sent", "=", "self", ".", "args", ".", "n_max_sent", ",", "\n", "vocab_file1", "=", "self", ".", "args", ".", "vocab_file1", ",", "align_file", "=", "self", ".", "args", ".", "align_file", ")", "\n", "\n", "", "self", ".", "trainer", "=", "SupervisedBertTrainer", "(", "self", ".", "bert_model", ",", "self", ".", "mapping", ",", "\n", "self", ".", "args", ",", "bert_model1", "=", "self", ".", "bert_model1", ",", "trans_types", "=", "self", ".", "transformer_types", ")", "\n", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "self", ".", "trainer", ".", "mapping", ".", "eval", "(", ")", "\n", "\n", "sampler", "=", "SequentialSampler", "(", "self", ".", "dataset", ")", "\n", "train_loader", "=", "DataLoader", "(", "self", ".", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "n_inst", "=", "0", "\n", "n_batch", "=", "0", "\n", "to_log", "=", "{", "\"avg_cos_sim\"", ":", "0", ",", "\"loss\"", ":", "0", "}", "\n", "self", ".", "trainer", ".", "args", ".", "loss", "=", "'l2_dist'", "\n", "\n", "for", "input_embs_a", ",", "input_mask_a", ",", "input_embs_b", ",", "input_mask_b", ",", "align_ids_a", ",", "align_ids_b", ",", "align_mask", ",", "example_indices", "in", "train_loader", ":", "\n", "            ", "n_batch", "+=", "1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "input_embs_a", "=", "input_embs_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_a", "=", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_embs_b", "=", "input_embs_b", ".", "to", "(", "self", ".", "device", ")", "\n", "", "align_ids_a", "=", "align_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "align_ids_b", "=", "align_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "align_mask", "=", "align_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "#print (align_ids_a, align_ids_b, align_mask)", "\n", "src_bert", "=", "self", ".", "trainer", ".", "get_indexed_mapped_bert_from_bert", "(", "\n", "input_embs_a", ",", "input_mask_a", ",", "align_ids_a", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "tgt_bert", "=", "self", ".", "trainer", ".", "get_indexed_bert_from_bert", "(", "\n", "input_embs_b", ",", "align_ids_b", ",", "align_mask", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ")", "\n", "avg_cos_sim", ",", "loss", "=", "self", ".", "trainer", ".", "supervised_mapping_step", "(", "src_bert", ",", "tgt_bert", ",", "eval_only", "=", "True", ")", "\n", "n_inst", "+=", "src_bert", ".", "size", "(", ")", "[", "0", "]", "\n", "cos_sim", "=", "avg_cos_sim", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "loss_", "=", "loss", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "to_log", "[", "\"avg_cos_sim\"", "]", "+=", "cos_sim", "\n", "to_log", "[", "\"loss\"", "]", "+=", "loss_", "\n", "\n", "", "to_log", "[", "\"avg_cos_sim\"", "]", "/=", "n_batch", "\n", "to_log", "[", "\"loss\"", "]", "/=", "n_batch", "\n", "print", "(", "\"avg cos sim:{:.6f}, avg l2 distance:{:.6f}, instances:{}\"", ".", "format", "(", "to_log", "[", "\"avg_cos_sim\"", "]", ",", "to_log", "[", "\"loss\"", "]", ",", "n_inst", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.list2bert": [[420, 524], ["src.supervised_bert_trainer.SupervisedBertTrainer", "supervised_bert.SupervisedBert.trainer.load_best", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "supervised_bert.SupervisedBert.trainer.mapping.eval", "src.load.load_from_single_bert", "src.load.convert", "supervised_bert.SupervisedBert.bert_model.eval", "open", "input_embs.to.to.to", "input_mask.to.to.to", "enumerate", "input_ids.to.to.to", "input_mask.to.to.to", "enumerate", "supervised_bert.SupervisedBert.trainer.mapping", "int", "collections.OrderedDict", "enumerate", "writer.write", "supervised_bert.SupervisedBert.bert_model", "supervised_bert.SupervisedBert.bert_model", "int", "collections.OrderedDict", "enumerate", "writer.write", "supervised_bert.SupervisedBert.trainer.mapping", "supervised_bert.SupervisedBert.detach().cpu().numpy", "collections.OrderedDict", "all_layers.append", "collections.OrderedDict", "all_out_features.append", "supervised_bert.SupervisedBert.trainer.mapping", "supervised_bert.SupervisedBert.detach().cpu().numpy", "collections.OrderedDict", "all_layers.append", "collections.OrderedDict", "all_out_features.append", "example_index.item", "round", "json.dumps", "supervised_bert.SupervisedBert.trainer.mapping", "example_index.item", "round", "json.dumps", "supervised_bert.SupervisedBert.detach().cpu", "x.item", "supervised_bert.SupervisedBert.detach().cpu", "x.item", "supervised_bert.SupervisedBert.detach", "supervised_bert.SupervisedBert.detach"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_single_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "def", "list2bert", "(", "self", ",", "sents", ")", ":", "\n", "        ", "\"\"\"\n        Map bert of source language to target space\n        \"\"\"", "\n", "assert", "self", ".", "args", ".", "output_file", "is", "not", "None", "\n", "\n", "self", ".", "trainer", "=", "SupervisedBertTrainer", "(", "self", ".", "bert_model", ",", "self", ".", "mapping", ",", "\n", "self", ".", "args", ",", "trans_types", "=", "self", ".", "transformer_types", ")", "\n", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "\n", "if", "self", ".", "args", ".", "load_pred_bert", ":", "\n", "            ", "assert", "self", ".", "args", ".", "bert_file0", "is", "not", "None", "\n", "pred_dataset", ",", "unique_id_to_feature", ",", "features", "=", "load_from_single_bert", "(", "self", ".", "args", ".", "bert_file0", ",", "sents", ",", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ")", "\n", "", "else", ":", "\n", "            ", "pred_dataset", ",", "unique_id_to_feature", ",", "features", "=", "convert", "(", "self", ".", "args", ".", "vocab_file", ",", "\n", "sents", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ",", "\n", "do_lower_case", "=", "self", ".", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "\n", "local_rank", "=", "self", ".", "args", ".", "local_rank", ")", "\n", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "", "pred_sampler", "=", "SequentialSampler", "(", "pred_dataset", ")", "\n", "pred_dataloader", "=", "DataLoader", "(", "pred_dataset", ",", "sampler", "=", "pred_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "trainer", ".", "mapping", ".", "eval", "(", ")", "\n", "with", "open", "(", "self", ".", "args", ".", "output_file", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "            ", "if", "self", ".", "args", ".", "load_pred_bert", ":", "\n", "                ", "for", "input_embs", ",", "input_mask", ",", "example_indices", "in", "pred_dataloader", ":", "\n", "                    ", "input_embs", "=", "input_embs", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "src_encoder_layer", "=", "input_embs", "\n", "if", "self", ".", "args", ".", "map_type", "in", "self", ".", "transformer_types", ":", "\n", "                        ", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ",", "input_mask", ")", "\n", "", "elif", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "                        ", "target_layer", "=", "src_encoder_layer", "\n", "", "else", ":", "\n", "                        ", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ")", "\n", "\n", "", "for", "b", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                        ", "feature", "=", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "unique_id", "=", "int", "(", "feature", ".", "unique_id", ")", "\n", "# feature = unique_id_to_feature[unique_id]", "\n", "output_json", "=", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"linex_index\"", "]", "=", "unique_id", "\n", "all_out_features", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                            ", "all_layers", "=", "[", "]", "\n", "layer_output", "=", "target_layer", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "layer_output", "=", "layer_output", "[", "b", "]", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "layers", "[", "\"index\"", "]", "=", "self", ".", "args", ".", "bert_layer", "\n", "layers", "[", "\"values\"", "]", "=", "[", "\n", "round", "(", "x", ".", "item", "(", ")", ",", "6", ")", "for", "x", "in", "layer_output", "[", "i", "]", "\n", "]", "\n", "all_layers", ".", "append", "(", "layers", ")", "\n", "out_features", "=", "OrderedDict", "(", ")", "\n", "out_features", "[", "\"token\"", "]", "=", "token", "\n", "out_features", "[", "\"layers\"", "]", "=", "all_layers", "\n", "all_out_features", ".", "append", "(", "out_features", ")", "\n", "", "output_json", "[", "\"features\"", "]", "=", "all_out_features", "\n", "writer", ".", "write", "(", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", ")", "\n", "", "", "", "else", ":", "\n", "                ", "for", "input_ids", ",", "input_mask", ",", "example_indices", "in", "pred_dataloader", ":", "\n", "                    ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "args", ".", "map_input", ":", "\n", "                        ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "input_mask", ",", "input_mapping", "=", "self", ".", "trainer", ".", "mapping", ")", "\n", "target_layer", "=", "all_encoder_layers", "[", "self", ".", "args", ".", "bert_layer", "]", "\n", "", "else", ":", "\n", "                        ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "input_mask", ")", "\n", "src_encoder_layer", "=", "all_encoder_layers", "[", "self", ".", "args", ".", "bert_layer", "]", "\n", "if", "self", ".", "args", ".", "map_type", "in", "self", ".", "transformer_types", ":", "\n", "                            ", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ",", "input_mask", ")", "\n", "", "elif", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "                            ", "target_layer", "=", "src_encoder_layer", "\n", "", "else", ":", "\n", "                            ", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ")", "\n", "\n", "", "", "for", "b", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                        ", "feature", "=", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "unique_id", "=", "int", "(", "feature", ".", "unique_id", ")", "\n", "# feature = unique_id_to_feature[unique_id]", "\n", "output_json", "=", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"linex_index\"", "]", "=", "unique_id", "\n", "all_out_features", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                            ", "all_layers", "=", "[", "]", "\n", "layer_output", "=", "target_layer", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "layer_output", "=", "layer_output", "[", "b", "]", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "layers", "[", "\"index\"", "]", "=", "self", ".", "args", ".", "bert_layer", "\n", "layers", "[", "\"values\"", "]", "=", "[", "\n", "round", "(", "x", ".", "item", "(", ")", ",", "6", ")", "for", "x", "in", "layer_output", "[", "i", "]", "\n", "]", "\n", "all_layers", ".", "append", "(", "layers", ")", "\n", "out_features", "=", "OrderedDict", "(", ")", "\n", "out_features", "[", "\"token\"", "]", "=", "token", "\n", "out_features", "[", "\"layers\"", "]", "=", "all_layers", "\n", "all_out_features", ".", "append", "(", "out_features", ")", "\n", "", "output_json", "[", "\"features\"", "]", "=", "all_out_features", "\n", "writer", ".", "write", "(", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.transform": [[525, 577], ["src.supervised_bert_trainer.SupervisedBertTrainer", "supervised_bert.SupervisedBert.trainer.load_best", "src.load.load_from_single_bert", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "supervised_bert.SupervisedBert.trainer.mapping.eval", "open", "input_embs.to.to.to", "input_mask.to.to.to", "enumerate", "supervised_bert.SupervisedBert.trainer.mapping", "int", "collections.OrderedDict", "enumerate", "writer.write", "supervised_bert.SupervisedBert.trainer.mapping", "supervised_bert.SupervisedBert.detach().cpu().numpy", "collections.OrderedDict", "all_layers.append", "collections.OrderedDict", "all_out_features.append", "example_index.item", "round", "json.dumps", "supervised_bert.SupervisedBert.detach().cpu", "x.item", "supervised_bert.SupervisedBert.detach"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_single_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "", "", "", "", "def", "transform", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Map bert of source language to target space\n        \"\"\"", "\n", "assert", "self", ".", "args", ".", "output_file", "is", "not", "None", "\n", "\n", "self", ".", "trainer", "=", "SupervisedBertTrainer", "(", "self", ".", "bert_model", ",", "self", ".", "mapping", ",", "\n", "self", ".", "args", ",", "trans_types", "=", "self", ".", "transformer_types", ")", "\n", "self", ".", "trainer", ".", "load_best", "(", ")", "\n", "\n", "assert", "self", ".", "args", ".", "bert_file0", "is", "not", "None", "\n", "pred_dataset", ",", "unique_id_to_feature", ",", "features", "=", "load_from_single_bert", "(", "self", ".", "args", ".", "bert_file0", ",", "max_seq_length", "=", "self", ".", "args", ".", "max_seq_length", ")", "\n", "pred_sampler", "=", "SequentialSampler", "(", "pred_dataset", ")", "\n", "pred_dataloader", "=", "DataLoader", "(", "pred_dataset", ",", "sampler", "=", "pred_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "trainer", ".", "mapping", ".", "eval", "(", ")", "\n", "with", "open", "(", "self", ".", "args", ".", "output_file", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "            ", "for", "input_embs", ",", "input_mask", ",", "example_indices", "in", "pred_dataloader", ":", "\n", "                ", "input_embs", "=", "input_embs", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "src_encoder_layer", "=", "input_embs", "\n", "if", "self", ".", "args", ".", "map_type", "in", "self", ".", "transformer_types", ":", "\n", "                    ", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ",", "input_mask", ")", "\n", "", "elif", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "                    ", "target_layer", "=", "src_encoder_layer", "\n", "", "else", ":", "\n", "                    ", "target_layer", "=", "self", ".", "trainer", ".", "mapping", "(", "src_encoder_layer", ")", "\n", "\n", "", "for", "b", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                    ", "feature", "=", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "unique_id", "=", "int", "(", "feature", ".", "unique_id", ")", "\n", "# feature = unique_id_to_feature[unique_id]", "\n", "output_json", "=", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"linex_index\"", "]", "=", "unique_id", "\n", "all_out_features", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "all_layers", "=", "[", "]", "\n", "layer_output", "=", "target_layer", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "layer_output", "=", "layer_output", "[", "b", "]", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "layers", "[", "\"index\"", "]", "=", "self", ".", "args", ".", "bert_layer", "\n", "layers", "[", "\"values\"", "]", "=", "[", "\n", "round", "(", "x", ".", "item", "(", ")", ",", "6", ")", "for", "x", "in", "layer_output", "[", "i", "]", "\n", "]", "\n", "all_layers", ".", "append", "(", "layers", ")", "\n", "out_features", "=", "OrderedDict", "(", ")", "\n", "out_features", "[", "\"token\"", "]", "=", "token", "\n", "out_features", "[", "\"layers\"", "]", "=", "all_layers", "\n", "all_out_features", ".", "append", "(", "out_features", ")", "\n", "", "output_json", "[", "\"features\"", "]", "=", "all_out_features", "\n", "writer", ".", "write", "(", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.main": [[26, 129], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "supervised_bert.SupervisedBert", "supervised_bert.SupervisedBert.eval", "supervised_bert.SupervisedBert.transform", "supervised_bert.SupervisedBert.train", "supervised_bert.SupervisedBert.svd"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.transform", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.train", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd"], ["def", "main", "(", ")", ":", "\n", "# main", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Unsupervised training'", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"Initialization seed\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Verbose level (2:debug, 1:info, 0:warning)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_path\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "\"Where to store experiment logs and models\"", ")", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--src_lang\"", ",", "type", "=", "str", ",", "default", "=", "'en'", ",", "help", "=", "\"Source language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_lang\"", ",", "type", "=", "str", ",", "default", "=", "'de'", ",", "help", "=", "\"Target language\"", ")", "\n", "\n", "# self.mapping", "\n", "parser", ".", "add_argument", "(", "\"--map_id_init\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "help", "=", "\"Initialize the self.mapping as an identity matrix\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_beta\"", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "\"Beta for orthogonalization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_clip_weights\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "help", "=", "\"Clip self.mapping weights\"", ")", "\n", "# training", "\n", "parser", ".", "add_argument", "(", "\"--n_epochs\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Number of epochs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "\"Batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"sgd,lr=0.1\"", ",", "help", "=", "\"self.mapping optimizer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr_decay\"", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "\"Learning rate decay (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "help", "=", "\"Minimum learning rate (SGD only)\"", ")", "\n", "#parser.add_argument(\"--lr_shrink\", type=float, default=0.5, help=\"Shrink the learning rate if the validation metric decreases (1 to disable)\")", "\n", "parser", ".", "add_argument", "(", "\"--decay_step\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"Learning rate decay step (SGD only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_all\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Save every model?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--quit_after_n_epochs_without_improvement\"", ",", "type", "=", "int", ",", "default", "=", "500", ",", "help", "=", "\"Quit after n epochs without improvement\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--normalize_embed\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "help", "=", "\"Normalize embeddings? (should be false with l2_dist loss)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--loss\"", ",", "type", "=", "str", ",", "default", "=", "\"l2_dist\"", ",", "help", "=", "\"loss type (cos_sim, max_margin_top-k, l2_dist)\"", ")", "\n", "# for bert", "\n", "parser", ".", "add_argument", "(", "\"--input_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The vocabulary file that the BERT model was trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The config json file corresponding to the pre-trained BERT model. \"", "\n", "\"This specifies the model architecture.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--init_checkpoint\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Initial checkpoint (usually from a pre-trained BERT model).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The vocabulary file that the BERT model was trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_file1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The config json file corresponding to the pre-trained BERT model. \"", "\n", "\"This specifies the model architecture.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--init_checkpoint1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Initial checkpoint (usually from a pre-trained BERT model).\"", ")", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--bert_layer\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"", "\n", "\"than this will be truncated, and sequences shorter than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to lower case the input text. Should be True for uncased \"", "\n", "\"models and False for cased models.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rm_stop_words\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to remove stop words while evaluating(sentence similarity)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rm_punc\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to remove punctuation while evaluating(sentence similarity)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stop_words_src\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Stop word file for source language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stop_words_tgt\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Stop word file for target language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_non_parallel\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether to add disorder sentence while evaluating(sentence similarity)\"", ")", "\n", "# For predict", "\n", "parser", ".", "add_argument", "(", "\"--pred\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Map source bert to target space\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"The source input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"The output file of mapped source language embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--base_embed\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Use base embeddings of BERT?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_input\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Apply mapping to the BERT input embeddings?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sim_file\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"output similarity file\"", ")", "\n", "# For supervised learning", "\n", "parser", ".", "add_argument", "(", "\"--align_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"The alignment file of paralleled sentences\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--map_type\"", ",", "type", "=", "str", ",", "default", "=", "'linear'", ",", "help", "=", "\"svd|linear|nonlinear|self_attention|attention|linear_self_attention|nonlinear_self_attention|fine_tune\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "768", ",", "help", "=", "\"BERT embedding dimension\"", ")", "\n", "# For non-linear mapping", "\n", "#parser.add_argument(\"--non_linear\", action='store_true', default=False, help=\"Use non-linear mapping\")", "\n", "parser", ".", "add_argument", "(", "\"--activation\"", ",", "type", "=", "str", ",", "default", "=", "'leaky_relu'", ",", "help", "=", "\"learky_relu,tanh\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"mapping layer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_size\"", ",", "type", "=", "int", ",", "default", "=", "768", ",", "help", "=", "\"mapping hidden layer size\"", ")", "\n", "# For attention-based mapping", "\n", "parser", ".", "add_argument", "(", "\"--num_attention_heads\"", ",", "type", "=", "int", ",", "default", "=", "12", ",", "help", "=", "\"attention head number\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "\"attention probability dropout rate\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "\"attention hidden layer dropout rate\"", ")", "\n", "# Load from pretrained bert file", "\n", "parser", ".", "add_argument", "(", "\"--load_pred_bert\"", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"Directly load predicted BERT\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_file0\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"Input predicted BERT file for language 0\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_file1\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"Input predicted BERT file for language 1\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_max_sent\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "\"Maximum BERT sentence number\"", ")", "\n", "# Fine-tuning", "\n", "#parser.add_argument(\"--fine_tune\", action='store_true', default=False, help=\"Fine tune on src BERT model\")", "\n", "parser", ".", "add_argument", "(", "\"--save_sim\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "help", "=", "\"Save model by cosine similarity?\"", ")", "\n", "# Eval", "\n", "parser", ".", "add_argument", "(", "\"--eval\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Load model and eval?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--align_punc\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Align punctuation?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--align_policy\"", ",", "default", "=", "'1to1'", ",", "type", "=", "str", ",", "help", "=", "\"Alignment policy\"", ")", "\n", "# parse parameters", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "bert_super", "=", "SupervisedBert", "(", "args", ")", "\n", "\n", "if", "args", ".", "eval", ":", "\n", "        ", "bert_super", ".", "eval", "(", ")", "\n", "return", "\n", "", "if", "args", ".", "pred", ":", "\n", "    \t\t", "bert_super", ".", "transform", "(", ")", "\n", "", "if", "not", "(", "args", ".", "pred", "or", "args", ".", "map_type", "==", "'svd'", ")", ":", "\n", "        ", "bert_super", ".", "train", "(", ")", "\n", "", "if", "args", ".", "map_type", "==", "'svd'", ":", "\n", "        ", "bert_super", ".", "svd", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.models.Discriminator.__init__": [[16, 35], ["torch.nn.Module.__init__", "range", "layers.append", "torch.nn.Sequential", "torch.nn.Dropout", "layers.append", "torch.nn.Sigmoid", "torch.nn.Linear", "layers.append", "layers.append", "torch.nn.LeakyReLU", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "super", "(", "Discriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "emb_dim", "=", "params", ".", "emb_dim", "\n", "self", ".", "dis_layers", "=", "params", ".", "dis_layers", "\n", "self", ".", "dis_hid_dim", "=", "params", ".", "dis_hid_dim", "\n", "self", ".", "dis_dropout", "=", "params", ".", "dis_dropout", "\n", "self", ".", "dis_input_dropout", "=", "params", ".", "dis_input_dropout", "\n", "\n", "layers", "=", "[", "nn", ".", "Dropout", "(", "self", ".", "dis_input_dropout", ")", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "dis_layers", "+", "1", ")", ":", "\n", "            ", "input_dim", "=", "self", ".", "emb_dim", "if", "i", "==", "0", "else", "self", ".", "dis_hid_dim", "\n", "output_dim", "=", "1", "if", "i", "==", "self", ".", "dis_layers", "else", "self", ".", "dis_hid_dim", "\n", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", ")", "\n", "if", "i", "<", "self", ".", "dis_layers", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "LeakyReLU", "(", "0.2", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "Dropout", "(", "self", ".", "dis_dropout", ")", ")", "\n", "", "", "layers", ".", "append", "(", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.models.Discriminator.forward": [[36, 39], ["models.Discriminator.layers().view", "x.dim", "x.size", "models.Discriminator.layers"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "x", ".", "dim", "(", ")", "==", "2", "and", "x", ".", "size", "(", "1", ")", "==", "self", ".", "emb_dim", "\n", "return", "self", ".", "layers", "(", "x", ")", ".", "view", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.models.build_model": [[41, 88], ["utils.load_embeddings", "torch.nn.Embedding", "nn.Embedding.weight.data.copy_", "torch.nn.Linear", "getattr", "utils.normalize_embeddings", "len", "utils.load_embeddings", "torch.nn.Embedding", "nn.Embedding.weight.data.copy_", "torch.nn.DataParallel.weight.data.copy_", "models.Discriminator", "nn.Embedding.cuda", "torch.nn.DataParallel.cuda", "torch.cuda.device_count", "utils.normalize_embeddings", "len", "torch.diag", "nn.Embedding.cuda", "torch.nn.DataParallel.cuda", "torch.nn.DataParallel", "torch.ones", "torch.nn.DataParallel"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings"], ["", "", "def", "build_model", "(", "params", ",", "with_dis", ")", ":", "\n", "    ", "\"\"\"\n    Build all components of the model.\n    \"\"\"", "\n", "# source embeddings", "\n", "src_dico", ",", "_src_emb", "=", "load_embeddings", "(", "params", ",", "source", "=", "True", ")", "\n", "params", ".", "src_dico", "=", "src_dico", "\n", "src_emb", "=", "nn", ".", "Embedding", "(", "len", "(", "src_dico", ")", ",", "params", ".", "emb_dim", ",", "sparse", "=", "True", ")", "\n", "src_emb", ".", "weight", ".", "data", ".", "copy_", "(", "_src_emb", ")", "\n", "\n", "# target embeddings", "\n", "if", "params", ".", "tgt_lang", ":", "\n", "        ", "tgt_dico", ",", "_tgt_emb", "=", "load_embeddings", "(", "params", ",", "source", "=", "False", ")", "\n", "params", ".", "tgt_dico", "=", "tgt_dico", "\n", "tgt_emb", "=", "nn", ".", "Embedding", "(", "len", "(", "tgt_dico", ")", ",", "params", ".", "emb_dim", ",", "sparse", "=", "True", ")", "\n", "tgt_emb", ".", "weight", ".", "data", ".", "copy_", "(", "_tgt_emb", ")", "\n", "", "else", ":", "\n", "        ", "tgt_emb", "=", "None", "\n", "\n", "# mapping", "\n", "", "mapping", "=", "nn", ".", "Linear", "(", "params", ".", "emb_dim", ",", "params", ".", "emb_dim", ",", "bias", "=", "False", ")", "\n", "if", "getattr", "(", "params", ",", "'map_id_init'", ",", "True", ")", ":", "\n", "        ", "mapping", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "diag", "(", "torch", ".", "ones", "(", "params", ".", "emb_dim", ")", ")", ")", "\n", "\n", "# discriminator", "\n", "", "discriminator", "=", "Discriminator", "(", "params", ")", "if", "with_dis", "else", "None", "\n", "\n", "# cuda", "\n", "if", "params", ".", "cuda", ":", "\n", "        ", "src_emb", ".", "cuda", "(", ")", "\n", "if", "params", ".", "tgt_lang", ":", "\n", "            ", "tgt_emb", ".", "cuda", "(", ")", "\n", "", "mapping", ".", "cuda", "(", ")", "\n", "if", "with_dis", ":", "\n", "            ", "discriminator", ".", "cuda", "(", ")", "\n", "", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "mapping", "=", "torch", ".", "nn", ".", "DataParallel", "(", "mapping", ")", "\n", "if", "with_dis", ":", "\n", "                ", "discriminator", "=", "torch", ".", "nn", ".", "DataParallel", "(", "discriminator", ")", "\n", "\n", "# normalize embeddings", "\n", "", "", "", "params", ".", "src_mean", "=", "normalize_embeddings", "(", "src_emb", ".", "weight", ".", "data", ",", "params", ".", "normalize_embeddings", ")", "\n", "if", "params", ".", "tgt_lang", ":", "\n", "        ", "params", ".", "tgt_mean", "=", "normalize_embeddings", "(", "tgt_emb", ".", "weight", ".", "data", ",", "params", ".", "normalize_embeddings", ")", "\n", "\n", "", "return", "src_emb", ",", "tgt_emb", ",", "mapping", ",", "discriminator", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.models.build_supervised_model": [[89, 139], ["utils.load_embeddings", "torch.nn.Embedding", "nn.Embedding.weight.data.copy_", "utils.normalize_embeddings", "len", "utils.load_embeddings", "torch.nn.Embedding", "nn.Embedding.weight.data.copy_", "maps.nonlinear_map.NonLinearMap", "torch.nn.Linear", "getattr", "models.Discriminator", "nn.Embedding.cuda", "torch.nn.DataParallel.cuda", "torch.cuda.device_count", "utils.normalize_embeddings", "len", "torch.nn.DataParallel.weight.data.copy_", "nn.Embedding.cuda", "torch.nn.DataParallel.cuda", "torch.nn.DataParallel", "torch.diag", "torch.nn.DataParallel", "torch.ones"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings"], ["", "def", "build_supervised_model", "(", "params", ",", "with_dis", ")", ":", "\n", "    ", "\"\"\"\n    Build all components of the model.\n    \"\"\"", "\n", "# source embeddings", "\n", "src_dico", ",", "_src_emb", "=", "load_embeddings", "(", "params", ",", "source", "=", "True", ")", "\n", "params", ".", "src_dico", "=", "src_dico", "\n", "src_emb", "=", "nn", ".", "Embedding", "(", "len", "(", "src_dico", ")", ",", "params", ".", "emb_dim", ",", "sparse", "=", "True", ")", "\n", "src_emb", ".", "weight", ".", "data", ".", "copy_", "(", "_src_emb", ")", "\n", "\n", "# target embeddings", "\n", "if", "params", ".", "tgt_lang", ":", "\n", "        ", "tgt_dico", ",", "_tgt_emb", "=", "load_embeddings", "(", "params", ",", "source", "=", "False", ")", "\n", "params", ".", "tgt_dico", "=", "tgt_dico", "\n", "tgt_emb", "=", "nn", ".", "Embedding", "(", "len", "(", "tgt_dico", ")", ",", "params", ".", "emb_dim", ",", "sparse", "=", "True", ")", "\n", "tgt_emb", ".", "weight", ".", "data", ".", "copy_", "(", "_tgt_emb", ")", "\n", "", "else", ":", "\n", "        ", "tgt_emb", "=", "None", "\n", "\n", "# mapping", "\n", "", "if", "params", ".", "non_linear", ":", "\n", "        ", "mapping", "=", "NonLinearMap", "(", "params", ")", "\n", "", "else", ":", "\n", "        ", "mapping", "=", "nn", ".", "Linear", "(", "params", ".", "emb_dim", ",", "params", ".", "emb_dim", ",", "bias", "=", "False", ")", "\n", "if", "getattr", "(", "params", ",", "'map_id_init'", ",", "True", ")", ":", "\n", "            ", "mapping", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "diag", "(", "torch", ".", "ones", "(", "params", ".", "emb_dim", ")", ")", ")", "\n", "\n", "# discriminator", "\n", "", "", "discriminator", "=", "Discriminator", "(", "params", ")", "if", "with_dis", "else", "None", "\n", "\n", "# cuda", "\n", "if", "params", ".", "cuda", ":", "\n", "        ", "src_emb", ".", "cuda", "(", ")", "\n", "if", "params", ".", "tgt_lang", ":", "\n", "            ", "tgt_emb", ".", "cuda", "(", ")", "\n", "", "mapping", ".", "cuda", "(", ")", "\n", "if", "with_dis", ":", "\n", "            ", "discriminator", ".", "cuda", "(", ")", "\n", "", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "mapping", "=", "torch", ".", "nn", ".", "DataParallel", "(", "mapping", ")", "\n", "if", "with_dis", ":", "\n", "                ", "discriminator", "=", "torch", ".", "nn", ".", "DataParallel", "(", "discriminator", ")", "\n", "\n", "# normalize embeddings", "\n", "", "", "", "params", ".", "src_mean", "=", "normalize_embeddings", "(", "src_emb", ".", "weight", ".", "data", ",", "params", ".", "normalize_embeddings", ")", "\n", "if", "params", ".", "tgt_lang", ":", "\n", "        ", "params", ".", "tgt_mean", "=", "normalize_embeddings", "(", "tgt_emb", ".", "weight", ".", "data", ",", "params", ".", "normalize_embeddings", ")", "\n", "\n", "", "return", "src_emb", ",", "tgt_emb", ",", "mapping", ",", "discriminator", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.__init__": [[42, 78], ["hasattr", "hasattr", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "torch.utils.data.dataloader._DataLoaderIter", "torch.device", "torch.device", "src.utils.get_optimizer", "optim_fn", "src.utils.get_optimizer", "optim_fn", "mapping.parameters", "discriminator.parameters", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_optimizer", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_optimizer"], ["    ", "def", "__init__", "(", "self", ",", "bert_model", ",", "dataset", ",", "mapping", ",", "discriminator", ",", "args", ",", "bert_model1", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initialize trainer script.\n        \"\"\"", "\n", "self", ".", "bert_model", "=", "bert_model", "\n", "self", ".", "bert_model1", "=", "bert_model1", "\n", "if", "args", ".", "adversarial", ":", "\n", "            ", "self", ".", "dataset", "=", "dataset", "\n", "#sampler = SequentialSampler(dataset)", "\n", "sampler", "=", "RandomSampler", "(", "dataset", ")", "\n", "self", ".", "dataloader", "=", "DataLoader", "(", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "self", ".", "iter_loader", "=", "_DataLoaderIter", "(", "self", ".", "dataloader", ")", "\n", "", "self", ".", "mapping", "=", "mapping", "\n", "self", ".", "discriminator", "=", "discriminator", "\n", "self", ".", "args", "=", "args", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", "or", "self", ".", "args", ".", "no_cuda", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "self", ".", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "self", ".", "args", ".", "local_rank", ")", "\n", "\n", "# optimizers", "\n", "", "if", "hasattr", "(", "args", ",", "'map_optimizer'", ")", ":", "\n", "            ", "optim_fn", ",", "optim_args", "=", "get_optimizer", "(", "args", ".", "map_optimizer", ")", "\n", "self", ".", "map_optimizer", "=", "optim_fn", "(", "mapping", ".", "parameters", "(", ")", ",", "**", "optim_args", ")", "\n", "", "if", "hasattr", "(", "args", ",", "'dis_optimizer'", ")", ":", "\n", "            ", "optim_fn", ",", "optim_args", "=", "get_optimizer", "(", "args", ".", "dis_optimizer", ")", "\n", "self", ".", "dis_optimizer", "=", "optim_fn", "(", "discriminator", ".", "parameters", "(", ")", ",", "**", "optim_args", ")", "\n", "", "else", ":", "\n", "            ", "assert", "discriminator", "is", "None", "\n", "\n", "# best validation score", "\n", "", "self", ".", "best_valid_metric", "=", "-", "1e12", "\n", "\n", "self", ".", "decrease_lr", "=", "False", "\n", "self", ".", "decrease_dis_lr", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.get_mapping_xy": [[79, 102], ["next", "bert_trainer.BertTrainer.get_bert", "bert_trainer.BertTrainer.get_bert", "bert_trainer.BertTrainer.mapping", "bert_trainer.BertTrainer.size", "bert_trainer.BertTrainer.size", "torch.cat", "torch.FloatTensor().zero_", "torch.utils.data.dataloader._DataLoaderIter", "next", "input_ids_a.to", "input_mask_a.to", "input_ids_b.to", "input_mask_b.to", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert"], ["", "def", "get_mapping_xy", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Get x/y for mapping step\n        \"\"\"", "\n", "items", "=", "next", "(", "self", ".", "iter_loader", ",", "None", ")", "\n", "if", "items", "is", "None", ":", "\n", "            ", "self", ".", "iter_loader", "=", "_DataLoaderIter", "(", "self", ".", "dataloader", ")", "\n", "items", "=", "next", "(", "self", ".", "iter_loader", ",", "None", ")", "\n", "", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "example_indices", "=", "items", "\n", "src_emb", "=", "self", ".", "get_bert", "(", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "\n", "tgt_emb", "=", "self", ".", "get_bert", "(", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", "\n", "src_emb", "=", "self", ".", "mapping", "(", "src_emb", ")", "\n", "src_len", "=", "src_emb", ".", "size", "(", "0", ")", "\n", "tgt_len", "=", "tgt_emb", ".", "size", "(", "0", ")", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "[", "src_emb", ",", "tgt_emb", "]", ",", "0", ")", "\n", "y", "=", "torch", ".", "FloatTensor", "(", "src_len", "+", "tgt_len", ")", ".", "zero_", "(", ")", "\n", "y", "[", ":", "src_len", "]", "=", "1", "-", "self", ".", "args", ".", "dis_smooth", "\n", "y", "[", "src_len", ":", "]", "=", "self", ".", "args", ".", "dis_smooth", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.get_bert": [[103, 118], ["bert_trainer.BertTrainer.select", "torch.no_grad", "bert_trainer.BertTrainer.bert_model.eval", "bert_trainer.BertTrainer.bert_model", "bert_trainer.BertTrainer.bert_model1.eval", "bert_trainer.BertTrainer.bert_model1"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "def", "get_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get BERT\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "model_id", "==", "0", "or", "self", ".", "bert_model1", "is", "None", ":", "\n", "                ", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "bert_model1", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model1", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "encoder_layer", "=", "all_encoder_layers", "[", "bert_layer", "]", "\n", "\n", "# [batch_size, seq_len, output_dim] => [unmasked_len, output_dim]", "\n", "", "return", "self", ".", "select", "(", "encoder_layer", ",", "input_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.select": [[119, 125], ["list", "embed.masked_select().view", "embed.size", "embed.masked_select", "mask.byte().view().expand", "mask.byte().view", "mask.byte"], "methods", ["None"], ["", "def", "select", "(", "self", ",", "embed", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        Select all unmasked embed in this batch \n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "emb_dim", "=", "list", "(", "embed", ".", "size", "(", ")", ")", "\n", "return", "embed", ".", "masked_select", "(", "mask", ".", "byte", "(", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "emb_dim", ")", ")", ".", "view", "(", "-", "1", ",", "emb_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.dis_step": [[126, 157], ["bert_trainer.BertTrainer.discriminator.train", "bert_trainer.BertTrainer.size", "tgt_emb.size", "torch.cat", "torch.FloatTensor().zero_", "bert_trainer.BertTrainer.discriminator", "torch.nn.functional.binary_cross_entropy", "stats[].append", "bert_trainer.BertTrainer.dis_optimizer.zero_grad", "torch.nn.functional.binary_cross_entropy.backward", "torch.nn.utils.clip_grad_norm_", "bert_trainer.BertTrainer.dis_optimizer.step", "torch.no_grad", "bert_trainer.BertTrainer.mapping", "torch.autograd.Variable", "torch.FloatTensor().zero_.to", "torch.nn.functional.binary_cross_entropy.item", "logger.error", "exit", "bert_trainer.BertTrainer.discriminator.parameters", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.train"], ["", "def", "dis_step", "(", "self", ",", "src_emb", ",", "tgt_emb", ",", "stats", ")", ":", "\n", "        ", "\"\"\"\n        Train the discriminator.\n        \"\"\"", "\n", "self", ".", "discriminator", ".", "train", "(", ")", "\n", "src_len", "=", "src_emb", ".", "size", "(", "0", ")", "\n", "tgt_len", "=", "tgt_emb", ".", "size", "(", "0", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "src_emb", "=", "self", ".", "mapping", "(", "src_emb", ")", "\n", "\n", "", "x", "=", "torch", ".", "cat", "(", "[", "src_emb", ",", "tgt_emb", "]", ",", "0", ")", "\n", "y", "=", "torch", ".", "FloatTensor", "(", "src_len", "+", "tgt_len", ")", ".", "zero_", "(", ")", "\n", "y", "[", ":", "src_len", "]", "=", "1", "-", "self", ".", "args", ".", "dis_smooth", "\n", "y", "[", "src_len", ":", "]", "=", "self", ".", "args", ".", "dis_smooth", "\n", "\n", "# loss", "\n", "preds", "=", "self", ".", "discriminator", "(", "Variable", "(", "x", ".", "data", ")", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "preds", ",", "y", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "stats", "[", "'DIS_COSTS'", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# check NaN", "\n", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected (discriminator)\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# optim", "\n", "", "self", ".", "dis_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "discriminator", ".", "parameters", "(", ")", ",", "self", ".", "args", ".", "dis_clip_weights", ")", "\n", "self", ".", "dis_optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.mapping_step": [[160, 188], ["bert_trainer.BertTrainer.discriminator.eval", "bert_trainer.BertTrainer.get_mapping_xy", "bert_trainer.BertTrainer.discriminator", "torch.nn.functional.binary_cross_entropy", "bert_trainer.BertTrainer.map_optimizer.zero_grad", "torch.nn.functional.binary_cross_entropy.backward", "torch.nn.utils.clip_grad_norm_", "bert_trainer.BertTrainer.map_optimizer.step", "bert_trainer.BertTrainer.orthogonalize", "x.size", "logger.error", "exit", "bert_trainer.BertTrainer.mapping.parameters", "y.to"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.get_mapping_xy", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.orthogonalize"], ["", "def", "mapping_step", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"\n        Fooling discriminator training step.\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "dis_lambda", "==", "0", ":", "\n", "            ", "return", "0", "\n", "\n", "", "self", ".", "discriminator", ".", "eval", "(", ")", "\n", "\n", "# loss", "\n", "x", ",", "y", "=", "self", ".", "get_mapping_xy", "(", ")", "\n", "preds", "=", "self", ".", "discriminator", "(", "x", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "preds", ",", "1", "-", "y", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "loss", "=", "self", ".", "args", ".", "dis_lambda", "*", "loss", "\n", "\n", "# check NaN", "\n", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected (fool discriminator)\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# optim", "\n", "", "self", ".", "map_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "mapping", ".", "parameters", "(", ")", ",", "self", ".", "args", ".", "map_clip_weights", ")", "\n", "self", ".", "map_optimizer", ".", "step", "(", ")", "\n", "self", ".", "orthogonalize", "(", ")", "\n", "\n", "return", "x", ".", "size", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.procrustes": [[189, 200], ["B.transpose().mm().cpu().numpy", "scipy.linalg.svd", "scipy.linalg.svd", "scipy.linalg.svd", "scipy.linalg.svd", "W.copy_", "torch.from_numpy().type_as", "B.transpose().mm().cpu", "torch.from_numpy", "B.transpose().mm", "U.dot", "B.transpose"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd"], ["", "def", "procrustes", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n        https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n        \"\"\"", "\n", "A", "=", "self", ".", "src_emb", ".", "weight", ".", "data", "[", "self", ".", "dico", "[", ":", ",", "0", "]", "]", "\n", "B", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", "[", "self", ".", "dico", "[", ":", ",", "1", "]", "]", "\n", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "M", "=", "B", ".", "transpose", "(", "0", ",", "1", ")", ".", "mm", "(", "A", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "U", ",", "S", ",", "V_t", "=", "scipy", ".", "linalg", ".", "svd", "(", "M", ",", "full_matrices", "=", "True", ")", "\n", "W", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "U", ".", "dot", "(", "V_t", ")", ")", ".", "type_as", "(", "W", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.orthogonalize": [[201, 212], ["isinstance", "W.copy_", "W.mm", "W.transpose().mm", "W.transpose"], "methods", ["None"], ["", "def", "orthogonalize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Orthogonalize the mapping.\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "map_beta", ">", "0", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "W", "=", "self", ".", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "                ", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "", "beta", "=", "self", ".", "args", ".", "map_beta", "\n", "W", ".", "copy_", "(", "(", "1", "+", "beta", ")", "*", "W", "-", "beta", "*", "W", ".", "mm", "(", "W", ".", "transpose", "(", "0", ",", "1", ")", ".", "mm", "(", "W", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.update_lr": [[213, 236], ["max", "logger.info", "logger.info", "logger.info"], "methods", ["None"], ["", "", "def", "update_lr", "(", "self", ",", "sent_sim", ")", ":", "\n", "        ", "\"\"\"\n        Update learning rate when using SGD.\n        \"\"\"", "\n", "if", "'sgd'", "not", "in", "self", ".", "args", ".", "map_optimizer", ":", "\n", "            ", "return", "\n", "", "old_lr", "=", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "new_lr", "=", "max", "(", "self", ".", "args", ".", "min_lr", ",", "old_lr", "*", "self", ".", "args", ".", "lr_decay", ")", "\n", "if", "new_lr", "<", "old_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\"Decreasing map learning rate: {:.8f} -> {:.8f}\"", ".", "format", "(", "old_lr", ",", "new_lr", ")", ")", "\n", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "\n", "", "if", "self", ".", "args", ".", "lr_shrink", "<", "1", "and", "sent_sim", ">=", "-", "1e7", ":", "\n", "            ", "if", "sent_sim", "<", "self", ".", "best_valid_metric", ":", "\n", "                ", "logger", ".", "info", "(", "\"Validation metric is smaller than the best: {:.5f} vs {:.5f}\"", ".", "format", "(", "sent_sim", ",", "self", ".", "best_valid_metric", ")", ")", "\n", "# decrease the learning rate, only if this is the", "\n", "# second time the validation metric decreases", "\n", "if", "self", ".", "decrease_lr", ":", "\n", "                    ", "old_lr", "=", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "*=", "self", ".", "args", ".", "lr_shrink", "\n", "logger", ".", "info", "(", "\"Shrinking map learning rate: %.5f -> %.5f\"", "\n", "%", "(", "old_lr", ",", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "", "self", ".", "decrease_lr", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.update_dis_lr": [[237, 248], ["max", "logger.info"], "methods", ["None"], ["", "", "", "def", "update_dis_lr", "(", "self", ",", "sent_sim", ")", ":", "\n", "        ", "\"\"\"\n        Update learning rate when using SGD.\n        \"\"\"", "\n", "if", "'sgd'", "not", "in", "self", ".", "args", ".", "dis_optimizer", ":", "\n", "            ", "return", "\n", "", "old_lr", "=", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "new_lr", "=", "max", "(", "self", ".", "args", ".", "min_lr", ",", "old_lr", "*", "self", ".", "args", ".", "dis_lr_decay", ")", "\n", "if", "new_lr", "<", "old_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\"Decreasing discriminator learning rate: {:.8f} -> {:.8f}\"", ".", "format", "(", "old_lr", ",", "new_lr", ")", ")", "\n", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.save_best": [[261, 284], ["logger.info", "os.path.join", "logger.info", "bert_trainer.BertTrainer.save_model", "torch.save", "bert_trainer.BertTrainer.discriminator.state_dict", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model"], ["", "", "def", "save_best", "(", "self", ",", "sent_sim", ",", "path", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Save the best model for the given validation metric.\n        \"\"\"", "\n", "# best mapping for the given validation criterion", "\n", "if", "sent_sim", ">", "self", ".", "best_valid_metric", ":", "\n", "# new best mapping", "\n", "            ", "self", ".", "best_valid_metric", "=", "sent_sim", "\n", "logger", ".", "info", "(", "'### New record (sentence similarity): {:.2f}% ###'", ".", "format", "(", "sent_sim", "*", "100", ")", ")", "\n", "# save the mapping", "\n", "#if isinstance(self.mapping, torch.nn.DataParallel):", "\n", "#    W = self.mapping.module.weight.data.cpu().numpy()", "\n", "#else:", "\n", "#    W = self.mapping.weight.data.cpu().numpy()", "\n", "path", "=", "path", "if", "path", "else", "self", ".", "args", ".", "model_path", "\n", "#if not os.path.exists(path):", "\n", "#    os.makedirs(path)", "\n", "map_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'### Saving the mapping to {} ... ###'", ".", "format", "(", "map_path", ")", ")", "\n", "#torch.save(W, map_path)", "\n", "self", ".", "save_model", "(", "map_path", ")", "\n", "if", "self", ".", "args", ".", "save_dis", ":", "\n", "                ", "torch", ".", "save", "(", "self", ".", "discriminator", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'discriminator.pkl'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.save_epoch": [[285, 302], ["os.path.join", "logger.info", "bert_trainer.BertTrainer.save_model", "torch.save", "bert_trainer.BertTrainer.discriminator.state_dict", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model"], ["", "", "", "def", "save_epoch", "(", "self", ",", "path", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"\n        Save the model for the given validation metric.\n        \"\"\"", "\n", "# save the mapping", "\n", "#if isinstance(self.mapping, torch.nn.DataParallel):", "\n", "#    W = self.mapping.module.weight.data.cpu().numpy()", "\n", "#else:", "\n", "#    W = self.mapping.weight.data.cpu().numpy()", "\n", "#if not os.path.exists(path):", "\n", "#    os.makedirs(path)", "\n", "map_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'### (End of epoch {}) Saving the mapping to {} ... ###'", ".", "format", "(", "epoch", ",", "map_path", ")", ")", "\n", "#torch.save(W, map_path)", "\n", "self", ".", "save_model", "(", "map_path", ")", "\n", "if", "self", ".", "args", ".", "save_dis", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "discriminator", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'discriminator.pkl'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.save_model": [[303, 317], ["logger.info", "isinstance", "os.path.exists", "os.makedirs", "torch.save", "torch.save", "os.path.dirname", "os.path.dirname", "bert_trainer.BertTrainer.mapping.module.state_dict", "bert_trainer.BertTrainer.mapping.state_dict"], "methods", ["None"], ["", "", "def", "save_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        Save model to path.\n        \"\"\"", "\n", "#path = os.path.join(path, 'best_mapping.pkl')", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "'* Saving the mapping to %s ...'", "%", "path", ")", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "mapping", ".", "module", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "#W = self.mapping.module.weight.data.cpu().numpy()", "\n", "", "else", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "mapping", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "#W = self.mapping.weight.data.cpu().numpy()", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.reload_best": [[320, 335], ["os.path.join", "logger.info", "os.path.isfile", "torch.from_numpy", "isinstance", "W.copy_", "torch.load", "torch.from_numpy.size", "W.size", "torch.from_numpy.type_as"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["", "", "def", "reload_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload the best mapping.\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'### Reloading the best model from %s ... ###'", "%", "path", ")", "\n", "# reload the model", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "to_reload", "=", "torch", ".", "from_numpy", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "", "assert", "to_reload", ".", "size", "(", ")", "==", "W", ".", "size", "(", ")", "\n", "W", ".", "copy_", "(", "to_reload", ".", "type_as", "(", "W", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.load_best": [[336, 349], ["os.path.join", "logger.info", "os.path.isfile", "bert_trainer.BertTrainer.load_model", "print", "bert_trainer.BertTrainer.reload_best"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.reload_best"], ["", "def", "load_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload the best mapping.\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'* Loading the best model from %s ...'", "%", "path", ")", "\n", "# reload the model", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "if", "self", ".", "load_model", "(", "path", ")", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Failed while loading by state_dict, reloading by copying ...\"", ")", "\n", "self", ".", "reload_best", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.BertTrainer.load_model": [[350, 366], ["isinstance", "bert_trainer.BertTrainer.mapping.module.load_state_dict", "bert_trainer.BertTrainer.mapping.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["", "", "def", "load_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        load model from path\n        \"\"\"", "\n", "try", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "self", ".", "mapping", ".", "module", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "else", ":", "\n", "#print (self.mapping.state_dict().keys())", "\n", "#print (torch.load(path).keys())", "\n", "                ", "self", ".", "mapping", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", "except", ":", "\n", "            ", "return", "False", "\n", "", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.reload_model": [[24, 39], ["os.path.join", "logger.info", "os.path.isfile", "torch.from_numpy", "isinstance", "W.copy_", "torch.load", "torch.from_numpy.size", "W.size", "torch.from_numpy.type_as"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["def", "reload_model", "(", "mapping", ",", "model_path", ")", ":", "\n", "    ", "\"\"\"\n    Reload the best mapping.\n    \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'### Reloading the best model from %s ... ###'", "%", "path", ")", "\n", "# reload the model", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "to_reload", "=", "torch", ".", "from_numpy", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "if", "isinstance", "(", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "W", "=", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "        ", "W", "=", "mapping", ".", "weight", ".", "data", "\n", "", "assert", "to_reload", ".", "size", "(", ")", "==", "W", ".", "size", "(", ")", "\n", "W", ".", "copy_", "(", "to_reload", ".", "type_as", "(", "W", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.__init__": [[24, 51], ["hasattr", "torch.device", "torch.device", "src.utils.get_optimizer", "optim_fn", "optim_fn", "bert_model.parameters", "mapping.parameters", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_optimizer"], ["    ", "def", "__init__", "(", "self", ",", "bert_model", ",", "mapping", ",", "args", ",", "bert_model1", "=", "None", ",", "\n", "trans_types", "=", "[", "'self_attention'", ",", "'attention'", ",", "'linear_self_attention'", ",", "'nonlinear_self_attention'", "]", ")", ":", "\n", "        ", "\"\"\"\n        Initialize trainer script.\n        \"\"\"", "\n", "self", ".", "transformer_types", "=", "trans_types", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "bert_model", "=", "bert_model", "\n", "self", ".", "bert_model1", "=", "bert_model1", "\n", "self", ".", "mapping", "=", "mapping", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", "or", "self", ".", "args", ".", "no_cuda", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "self", ".", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "self", ".", "args", ".", "local_rank", ")", "\n", "\n", "# optimizers", "\n", "", "if", "hasattr", "(", "args", ",", "'map_optimizer'", ")", ":", "\n", "            ", "optim_fn", ",", "optim_args", "=", "get_optimizer", "(", "args", ".", "map_optimizer", ")", "\n", "if", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "                ", "self", ".", "map_optimizer", "=", "optim_fn", "(", "bert_model", ".", "parameters", "(", ")", ",", "**", "optim_args", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "map_optimizer", "=", "optim_fn", "(", "mapping", ".", "parameters", "(", ")", ",", "**", "optim_args", ")", "\n", "\n", "# best validation score", "\n", "", "", "self", ".", "best_valid_metric", "=", "-", "1e12", "\n", "self", ".", "decrease_lr", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.supervised_mapping_step": [[52, 121], ["supervised_bert_trainer.SupervisedBertTrainer.map_optimizer.zero_grad", "torch.where.mean.backward", "supervised_bert_trainer.SupervisedBertTrainer.map_optimizer.step", "logger.error", "exit", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "gold_scores.mean", "supervised_bert_trainer.SupervisedBertTrainer.args.loss.startswith", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "src_emb.mm", "int", "src_emb.mm.topk", "gold_scores.unsqueeze().expand_as", "torch.where", "torch.where.mean", "src_emb.norm", "tgt_emb.norm", "tgt_emb.transpose", "torch.ones_like", "torch.zeros_like", "src_emb.norm", "tgt_emb.norm", "supervised_bert_trainer.SupervisedBertTrainer.args.loss.split", "gold_scores.unsqueeze"], "methods", ["None"], ["", "def", "supervised_mapping_step", "(", "self", ",", "src_emb", ",", "tgt_emb", ",", "margin", "=", "1", ",", "eval_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Calculate the loss and backward\n        Inputs:\n            src_emb [unmasked_len, output_dim] mapped source embeddings\n            tgt_emb [unmasked_len, output_dim] target embeddings\n        Outputs:\n            avg_cos_sim/loss\n        \"\"\"", "\n", "\n", "# normalization", "\n", "if", "self", ".", "args", ".", "normalize_embed", ":", "\n", "            ", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "# (n, n)", "\n", "#scores = src_emb.mm(tgt_emb.transpose(0, 1))", "\n", "#rang = torch.arange(scores.shape[0], out=torch.LongTensor())", "\n", "#gold_scores = scores[rang, rang]", "\n", "\n", "", "if", "self", ".", "args", ".", "loss", "==", "'cos_sim'", ":", "\n", "# (n)", "\n", "            ", "gold_scores", "=", "(", "src_emb", "*", "tgt_emb", ")", ".", "sum", "(", "1", ")", "\n", "# maximize cosine similarities", "\n", "loss", "=", "-", "gold_scores", ".", "mean", "(", ")", "\n", "", "elif", "self", ".", "args", ".", "loss", "==", "'l2_dist'", ":", "\n", "# (n, d)", "\n", "            ", "sub", "=", "src_emb", "-", "tgt_emb", "\n", "# (n, d) => (n) => ()", "\n", "loss", "=", "(", "sub", "*", "sub", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "", "elif", "self", ".", "args", ".", "loss", ".", "startswith", "(", "'max_margin_top'", ")", ":", "\n", "# (n)", "\n", "            ", "gold_scores", "=", "(", "src_emb", "*", "tgt_emb", ")", ".", "sum", "(", "1", ")", "\n", "# (n, n)", "\n", "scores", "=", "src_emb", ".", "mm", "(", "tgt_emb", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "# max margin with top k elements", "\n", "k", "=", "int", "(", "self", ".", "args", ".", "loss", ".", "split", "(", "'-'", ")", "[", "1", "]", ")", "\n", "# (n, k)", "\n", "top_vals", ",", "top_ids", "=", "scores", ".", "topk", "(", "k", ",", "1", ",", "True", ")", "\n", "# (n) => (n, k)", "\n", "gold_vals", "=", "gold_scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "top_vals", ")", "\n", "# (n, k)", "\n", "margins", "=", "torch", ".", "ones_like", "(", "top_vals", ")", "*", "margin", "\n", "# (n, k)", "\n", "losses", "=", "margins", "-", "gold_vals", "+", "top_vals", "\n", "# mask out less than 0", "\n", "losses", "=", "torch", ".", "where", "(", "losses", ">", "0", ",", "losses", ",", "torch", ".", "zeros_like", "(", "losses", ")", ")", "\n", "# ()", "\n", "loss", "=", "losses", ".", "mean", "(", ")", "\n", "\n", "# check NaN", "\n", "", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected (supervised learning)\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# calculating average cosine similarity", "\n", "", "if", "not", "self", ".", "args", ".", "normalize_embed", ":", "\n", "            ", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "# (n)", "\n", "", "avg_cos_sim", "=", "(", "src_emb", "*", "tgt_emb", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "if", "eval_only", ":", "\n", "            ", "return", "avg_cos_sim", ",", "loss", "\n", "# optim", "\n", "", "self", ".", "map_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "map_optimizer", ".", "step", "(", ")", "\n", "\n", "return", "avg_cos_sim", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_unmasked_bert": [[122, 136], ["torch.no_grad", "supervised_bert_trainer.SupervisedBertTrainer.bert_model.eval", "supervised_bert_trainer.SupervisedBertTrainer.bert_model", "supervised_bert_trainer.SupervisedBertTrainer.bert_model1.eval", "supervised_bert_trainer.SupervisedBertTrainer.bert_model1"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "def", "get_unmasked_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get BERT\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "model_id", "==", "0", "or", "self", ".", "bert_model1", "is", "None", ":", "\n", "                ", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "bert_model1", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model1", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "encoder_layer", "=", "all_encoder_layers", "[", "bert_layer", "]", "\n", "# [batch_size, seq_len, output_dim]", "\n", "", "return", "encoder_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_trainable_unmasked_bert": [[137, 148], ["supervised_bert_trainer.SupervisedBertTrainer.bert_model", "supervised_bert_trainer.SupervisedBertTrainer.bert_model1"], "methods", ["None"], ["", "def", "get_trainable_unmasked_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get BERT\n        \"\"\"", "\n", "if", "model_id", "==", "0", "or", "self", ".", "bert_model1", "is", "None", ":", "\n", "            ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "else", ":", "\n", "            ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model1", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "encoder_layer", "=", "all_encoder_layers", "[", "bert_layer", "]", "\n", "# [batch_size, seq_len, output_dim]", "\n", "return", "encoder_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.select": [[149, 161], ["list", "layer.masked_select().view", "layer.size", "layer.masked_select", "mask.byte().view().expand", "mask.byte().view", "mask.byte"], "methods", ["None"], ["", "def", "select", "(", "self", ",", "layer", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        Select all unmasked embed in this batch\n        Inputs:\n            layer [batch_size, seq_len, output_dim]\n            mask [batch_size, seq_len] of 0/1\n        Outputs:\n            masked_layer [unmasked_len, output_dim]\n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "output_dim", "=", "list", "(", "layer", ".", "size", "(", ")", ")", "\n", "# [batch_size, seq_len, output_dim] => [unmasked_len, output_dim]", "\n", "return", "layer", ".", "masked_select", "(", "mask", ".", "byte", "(", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "output_dim", ")", ")", ".", "view", "(", "-", "1", ",", "output_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.rearange": [[162, 178], ["list", "list", "index.view().expand", "layer.gather", "layer.size", "index.size", "index.view"], "methods", ["None"], ["", "def", "rearange", "(", "self", ",", "layer", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        Rearange layer by index\n        Inputs: \n            layer [batch_size, seq_len, output_dim]\n            index [batch_size, max_align]\n        Outputs:\n            rearanged_layer [batch_size, max_align, output_dim]\n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "output_dim", "=", "list", "(", "layer", ".", "size", "(", ")", ")", "\n", "batch_size", ",", "max_align", "=", "list", "(", "index", ".", "size", "(", ")", ")", "\n", "#[batch_size, max_align]=>[batch_size, max_align, output_dim]", "\n", "expanded_index", "=", "index", ".", "view", "(", "batch_size", ",", "max_align", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "output_dim", ")", "\n", "#[batch_size, max_align, output_dim]", "\n", "rearanged_layer", "=", "layer", ".", "gather", "(", "1", ",", "expanded_index", ")", "\n", "return", "rearanged_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_mapped_bert": [[179, 197], ["supervised_bert_trainer.SupervisedBertTrainer.rearange", "supervised_bert_trainer.SupervisedBertTrainer.select", "supervised_bert_trainer.SupervisedBertTrainer.get_trainable_unmasked_bert", "supervised_bert_trainer.SupervisedBertTrainer.get_unmasked_bert", "supervised_bert_trainer.SupervisedBertTrainer.mapping", "supervised_bert_trainer.SupervisedBertTrainer.mapping"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.rearange", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_trainable_unmasked_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_unmasked_bert"], ["", "def", "get_indexed_mapped_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "index", ",", "align_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get bert according to index and align_mask\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "            ", "unmasked_bert", "=", "self", ".", "get_trainable_unmasked_bert", "(", "input_ids", ",", "input_mask", ",", "bert_layer", ",", "model_id", ")", "\n", "", "else", ":", "\n", "            ", "unmasked_bert", "=", "self", ".", "get_unmasked_bert", "(", "input_ids", ",", "input_mask", ",", "bert_layer", ",", "model_id", ")", "\n", "", "if", "self", ".", "args", ".", "map_type", "in", "self", ".", "transformer_types", ":", "\n", "            ", "mapped_bert", "=", "self", ".", "mapping", "(", "unmasked_bert", ",", "input_mask", ")", "\n", "", "elif", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "            ", "mapped_bert", "=", "unmasked_bert", "\n", "", "else", ":", "\n", "            ", "mapped_bert", "=", "self", ".", "mapping", "(", "unmasked_bert", ")", "\n", "", "rearanged_bert", "=", "self", ".", "rearange", "(", "mapped_bert", ",", "index", ")", "\n", "indexed_bert", "=", "self", ".", "select", "(", "rearanged_bert", ",", "align_mask", ")", "\n", "#print (unmasked_bert, '\\n', mapped_bert, '\\n', rearanged_bert, '\\n', indexed_bert)", "\n", "return", "indexed_bert", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_mapped_bert_from_bert": [[198, 219], ["supervised_bert_trainer.SupervisedBertTrainer.rearange", "supervised_bert_trainer.SupervisedBertTrainer.select", "supervised_bert_trainer.SupervisedBertTrainer.mapping", "supervised_bert_trainer.SupervisedBertTrainer.mapping", "str", "print", "unmasked_bert.cuda.cuda.cuda", "supervised_bert_trainer.SupervisedBertTrainer.mapping.cuda", "str", "print", "mapped_bert.cpu.cpu.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.rearange", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select"], ["", "def", "get_indexed_mapped_bert_from_bert", "(", "self", ",", "unmasked_bert", ",", "input_mask", ",", "index", ",", "align_mask", ",", "bert_layer", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        Get bert according to index and align_mask\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "map_type", "in", "self", ".", "transformer_types", ":", "\n", "            ", "mapped_bert", "=", "self", ".", "mapping", "(", "unmasked_bert", ",", "input_mask", ")", "\n", "", "elif", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "            ", "mapped_bert", "=", "unmasked_bert", "\n", "", "else", ":", "\n", "            ", "if", "str", "(", "self", ".", "device", ")", "==", "'cpu'", ":", "\n", "              ", "print", "(", "\"Transfering to gpu\"", ")", "\n", "unmasked_bert", "=", "unmasked_bert", ".", "cuda", "(", ")", "\n", "self", ".", "mapping", "=", "self", ".", "mapping", ".", "cuda", "(", ")", "\n", "", "mapped_bert", "=", "self", ".", "mapping", "(", "unmasked_bert", ")", "\n", "if", "str", "(", "self", ".", "device", ")", "==", "'cpu'", ":", "\n", "              ", "print", "(", "\"Trnasfering back to cpu\"", ")", "\n", "mapped_bert", "=", "mapped_bert", ".", "cpu", "(", ")", "\n", "", "", "rearanged_bert", "=", "self", ".", "rearange", "(", "mapped_bert", ",", "index", ")", "\n", "indexed_bert", "=", "self", ".", "select", "(", "rearanged_bert", ",", "align_mask", ")", "\n", "#print (unmasked_bert, '\\n', mapped_bert, '\\n', rearanged_bert, '\\n', indexed_bert)", "\n", "return", "indexed_bert", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert": [[220, 228], ["supervised_bert_trainer.SupervisedBertTrainer.get_unmasked_bert", "supervised_bert_trainer.SupervisedBertTrainer.rearange", "supervised_bert_trainer.SupervisedBertTrainer.select"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_unmasked_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.rearange", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select"], ["", "def", "get_indexed_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "index", ",", "align_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        Get bert according to index and align_mask\n        \"\"\"", "\n", "unmasked_bert", "=", "self", ".", "get_unmasked_bert", "(", "input_ids", ",", "input_mask", ",", "bert_layer", ",", "model_id", ")", "\n", "rearanged_bert", "=", "self", ".", "rearange", "(", "unmasked_bert", ",", "index", ")", "\n", "indexed_bert", "=", "self", ".", "select", "(", "rearanged_bert", ",", "align_mask", ")", "\n", "return", "indexed_bert", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.get_indexed_bert_from_bert": [[229, 236], ["supervised_bert_trainer.SupervisedBertTrainer.rearange", "supervised_bert_trainer.SupervisedBertTrainer.select"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.rearange", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select"], ["", "def", "get_indexed_bert_from_bert", "(", "self", ",", "unmasked_bert", ",", "index", ",", "align_mask", ",", "bert_layer", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        Get bert according to index and align_mask\n        \"\"\"", "\n", "rearanged_bert", "=", "self", ".", "rearange", "(", "unmasked_bert", ",", "index", ")", "\n", "indexed_bert", "=", "self", ".", "select", "(", "rearanged_bert", ",", "align_mask", ")", "\n", "return", "indexed_bert", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.procrustes": [[237, 254], ["isinstance", "B.transpose().mm().cpu().numpy", "scipy.linalg.svd", "scipy.linalg.svd", "scipy.linalg.svd", "scipy.linalg.svd", "W.copy_", "logger.info", "torch.from_numpy().type_as", "B.transpose().mm().cpu", "torch.from_numpy", "B.transpose().mm", "U.dot", "B.transpose"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd"], ["", "def", "procrustes", "(", "self", ",", "src_embs", ",", "tgt_embs", ")", ":", "\n", "        ", "\"\"\"\n        Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n        https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n        Input:\n            src_embs/tgt_embs: [vocab_size, emb_dim]\n        \"\"\"", "\n", "A", "=", "src_embs", "\n", "B", "=", "tgt_embs", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "", "M", "=", "B", ".", "transpose", "(", "0", ",", "1", ")", ".", "mm", "(", "A", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "U", ",", "S", ",", "V_t", "=", "scipy", ".", "linalg", ".", "svd", "(", "M", ",", "full_matrices", "=", "True", ")", "\n", "W", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "U", ".", "dot", "(", "V_t", ")", ")", ".", "type_as", "(", "W", ")", ")", "\n", "logger", ".", "info", "(", "\"Finished Procrustes.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.decay_map_lr": [[255, 266], ["max", "logger.info"], "methods", ["None"], ["", "def", "decay_map_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Update learning rate when using SGD.\n        \"\"\"", "\n", "if", "'sgd'", "not", "in", "self", ".", "args", ".", "map_optimizer", ":", "\n", "            ", "return", "\n", "", "old_lr", "=", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "new_lr", "=", "max", "(", "self", ".", "args", ".", "min_lr", ",", "old_lr", "*", "self", ".", "args", ".", "lr_decay", ")", "\n", "if", "new_lr", "<", "old_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\"Decreasing Mapping learning rate: %.8f -> %.8f\"", "%", "(", "old_lr", ",", "new_lr", ")", ")", "\n", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.save_best": [[267, 278], ["logger.info", "os.path.join", "supervised_bert_trainer.SupervisedBertTrainer.save_model"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model"], ["", "", "def", "save_best", "(", "self", ",", "to_log", ",", "metric", ")", ":", "\n", "        ", "\"\"\"\n        Save the best model for the given validation metric.\n        \"\"\"", "\n", "# best mapping for the given validation criterion", "\n", "if", "to_log", "[", "metric", "]", ">", "self", ".", "best_valid_metric", ":", "\n", "# new best mapping", "\n", "            ", "self", ".", "best_valid_metric", "=", "to_log", "[", "metric", "]", "\n", "logger", ".", "info", "(", "'* Best value for \"%s\": %.5f'", "%", "(", "metric", ",", "to_log", "[", "metric", "]", ")", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "self", ".", "save_model", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.save_model": [[279, 299], ["logger.info", "isinstance", "os.path.exists", "os.makedirs", "logger.info", "isinstance", "torch.save", "torch.save", "os.path.dirname", "os.path.dirname", "torch.save", "torch.save", "supervised_bert_trainer.SupervisedBertTrainer.mapping.module.state_dict", "supervised_bert_trainer.SupervisedBertTrainer.mapping.state_dict", "supervised_bert_trainer.SupervisedBertTrainer.bert_model.module.state_dict", "supervised_bert_trainer.SupervisedBertTrainer.bert_model.state_dict"], "methods", ["None"], ["", "", "def", "save_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        Save model to path.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "            ", "logger", ".", "info", "(", "'* Saving the src BERT model to %s ...'", "%", "path", ")", "\n", "if", "isinstance", "(", "self", ".", "bert_model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "torch", ".", "save", "(", "self", ".", "bert_model", ".", "module", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "", "else", ":", "\n", "                ", "torch", ".", "save", "(", "self", ".", "bert_model", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "", "return", "\n", "\n", "", "logger", ".", "info", "(", "'* Saving the mapping to %s ...'", "%", "path", ")", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "mapping", ".", "module", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "", "else", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "mapping", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.load_best": [[300, 309], ["os.path.join", "logger.info", "os.path.isfile", "supervised_bert_trainer.SupervisedBertTrainer.load_model"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_model"], ["", "", "def", "load_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload the best mapping.\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'* Loading the best model from %s ...'", "%", "path", ")", "\n", "# reload the model", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "self", ".", "load_model", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.supervised_bert_trainer.SupervisedBertTrainer.load_model": [[310, 321], ["isinstance", "supervised_bert_trainer.SupervisedBertTrainer.mapping.module.load_state_dict", "supervised_bert_trainer.SupervisedBertTrainer.mapping.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["", "def", "load_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        load model from path\n        \"\"\"", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "self", ".", "mapping", ".", "module", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "mapping", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.__init__": [[16, 22], ["dictionary.Dictionary.check_valid", "len", "len"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.check_valid"], ["    ", "def", "__init__", "(", "self", ",", "id2word", ",", "word2id", ",", "lang", ")", ":", "\n", "        ", "assert", "len", "(", "id2word", ")", "==", "len", "(", "word2id", ")", "\n", "self", ".", "id2word", "=", "id2word", "\n", "self", ".", "word2id", "=", "word2id", "\n", "self", ".", "lang", "=", "lang", "\n", "self", ".", "check_valid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.__len__": [[23, 28], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the number of words in the dictionary.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "id2word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.__getitem__": [[29, 34], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "\"\"\"\n        Returns the word of the specified index.\n        \"\"\"", "\n", "return", "self", ".", "id2word", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.__contains__": [[35, 40], ["None"], "methods", ["None"], ["", "def", "__contains__", "(", "self", ",", "w", ")", ":", "\n", "        ", "\"\"\"\n        Returns whether a word is in the dictionary.\n        \"\"\"", "\n", "return", "w", "in", "self", ".", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.__eq__": [[41, 50], ["dictionary.Dictionary.check_valid", "y.check_valid", "len", "len", "all", "range", "len"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.check_valid", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.check_valid"], ["", "def", "__eq__", "(", "self", ",", "y", ")", ":", "\n", "        ", "\"\"\"\n        Compare the dictionary with another one.\n        \"\"\"", "\n", "self", ".", "check_valid", "(", ")", "\n", "y", ".", "check_valid", "(", ")", "\n", "if", "len", "(", "self", ".", "id2word", ")", "!=", "len", "(", "y", ")", ":", "\n", "            ", "return", "False", "\n", "", "return", "self", ".", "lang", "==", "y", ".", "lang", "and", "all", "(", "self", ".", "id2word", "[", "i", "]", "==", "y", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "y", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.check_valid": [[51, 58], ["range", "len", "len", "len"], "methods", ["None"], ["", "def", "check_valid", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Check that the dictionary is valid.\n        \"\"\"", "\n", "assert", "len", "(", "self", ".", "id2word", ")", "==", "len", "(", "self", ".", "word2id", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "id2word", ")", ")", ":", "\n", "            ", "assert", "self", ".", "word2id", "[", "self", ".", "id2word", "[", "i", "]", "]", "==", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.index": [[59, 64], ["None"], "methods", ["None"], ["", "", "def", "index", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"\n        Returns the index of the specified word.\n        \"\"\"", "\n", "return", "self", ".", "word2id", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.prune": [[65, 73], ["dictionary.Dictionary.check_valid", "dictionary.Dictionary.id2word.items", "dictionary.Dictionary.id2word.items"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.check_valid"], ["", "def", "prune", "(", "self", ",", "max_vocab", ")", ":", "\n", "        ", "\"\"\"\n        Limit the vocabulary size.\n        \"\"\"", "\n", "assert", "max_vocab", ">=", "1", "\n", "self", ".", "id2word", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "id2word", ".", "items", "(", ")", "if", "k", "<", "max_vocab", "}", "\n", "self", ".", "word2id", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "id2word", ".", "items", "(", ")", "}", "\n", "self", ".", "check_valid", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates": [[17, 141], ["emb1.size", "torch.cat", "min", "range", "torch.cat", "torch.cat", "params.dico_method.startswith", "torch.cat.size", "all_pairs.masked_select().view.size", "diff.sort", "selected.unsqueeze().expand_as().clone", "torch.cat.masked_select().view", "all_pairs.masked_select().view.masked_select().view", "logger.info", "mask.unsqueeze().expand_as().clone.unsqueeze().expand_as().clone", "all_pairs.masked_select().view.masked_select().view", "params.dico_method.startswith", "emb2.mm().transpose", "emb2.mm().transpose.topk", "torch.cat.append", "torch.cat.append", "float", "range", "torch.cat", "torch.cat", "torch.cat.topk", "torch.cat.gather", "params.dico_method.startswith", "torch.arange().long().unsqueeze", "all_targets[].unsqueeze", "best_scores.cpu", "best_targets.cpu", "emb2.size", "emb1.mm", "emb2.mm().transpose.mul_().exp_", "emb2.mm().transpose.div_", "emb2.mm().transpose.topk", "torch.cat.append", "torch.cat.append", "int.isdigit", "int", "torch.from_numpy", "torch.from_numpy", "average_dist1.type_as.type_as", "average_dist2.type_as.type_as", "range", "torch.cat", "torch.cat", "all_pairs.masked_select().view.max", "selected.unsqueeze().expand_as", "torch.cat.masked_select", "all_pairs.masked_select().view.masked_select", "mask.unsqueeze().expand_as().clone.unsqueeze().expand_as", "all_pairs.masked_select().view.masked_select", "emb2.mm", "emb2[].transpose", "emb2.mm().transpose.sum().expand_as", "best_scores.cpu", "utils.get_nn_avg_dist", "utils.get_nn_avg_dist", "emb2.mm().transpose", "emb2.mm().transpose.mul_", "emb2.mm().transpose.sub_", "emb2.mm().transpose.topk", "torch.cat.append", "torch.cat.append", "torch.arange().long", "mask.unsqueeze().expand_as().clone.sum", "diff.size", "emb1[].transpose", "len", "emb2.mm().transpose.mul_", "len", "best_scores.cpu", "best_targets.cpu", "selected.unsqueeze", "mask.unsqueeze().expand_as().clone.unsqueeze", "emb2.mm().transpose.sum", "emb2.mm", "torch.arange", "emb1[].transpose", "torch.cat.size", "min", "min", "min"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist"], ["def", "get_candidates", "(", "emb1", ",", "emb2", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Get best translation pairs candidates.\n    \"\"\"", "\n", "bs", "=", "128", "\n", "\n", "all_scores", "=", "[", "]", "\n", "all_targets", "=", "[", "]", "\n", "\n", "# number of source words to consider", "\n", "n_src", "=", "emb1", ".", "size", "(", "0", ")", "\n", "if", "params", ".", "dico_max_rank", ">", "0", "and", "not", "params", ".", "dico_method", ".", "startswith", "(", "'invsm_beta_'", ")", ":", "\n", "        ", "n_src", "=", "min", "(", "params", ".", "dico_max_rank", ",", "n_src", ")", "\n", "\n", "# nearest neighbors", "\n", "", "if", "params", ".", "dico_method", "==", "'nn'", ":", "\n", "\n", "# for every source word", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "n_src", ",", "bs", ")", ":", "\n", "\n", "# compute target words scores", "\n", "            ", "scores", "=", "emb2", ".", "mm", "(", "emb1", "[", "i", ":", "min", "(", "n_src", ",", "i", "+", "bs", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "best_scores", ",", "best_targets", "=", "scores", ".", "topk", "(", "2", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "# update scores / potential targets", "\n", "all_scores", ".", "append", "(", "best_scores", ".", "cpu", "(", ")", ")", "\n", "all_targets", ".", "append", "(", "best_targets", ".", "cpu", "(", ")", ")", "\n", "\n", "", "all_scores", "=", "torch", ".", "cat", "(", "all_scores", ",", "0", ")", "\n", "all_targets", "=", "torch", ".", "cat", "(", "all_targets", ",", "0", ")", "\n", "\n", "# inverted softmax", "\n", "", "elif", "params", ".", "dico_method", ".", "startswith", "(", "'invsm_beta_'", ")", ":", "\n", "\n", "        ", "beta", "=", "float", "(", "params", ".", "dico_method", "[", "len", "(", "'invsm_beta_'", ")", ":", "]", ")", "\n", "\n", "# for every target word", "\n", "for", "i", "in", "range", "(", "0", ",", "emb2", ".", "size", "(", "0", ")", ",", "bs", ")", ":", "\n", "\n", "# compute source words scores", "\n", "            ", "scores", "=", "emb1", ".", "mm", "(", "emb2", "[", "i", ":", "i", "+", "bs", "]", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "scores", ".", "mul_", "(", "beta", ")", ".", "exp_", "(", ")", "\n", "scores", ".", "div_", "(", "scores", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "scores", ")", ")", "\n", "\n", "best_scores", ",", "best_targets", "=", "scores", ".", "topk", "(", "2", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "# update scores / potential targets", "\n", "all_scores", ".", "append", "(", "best_scores", ".", "cpu", "(", ")", ")", "\n", "all_targets", ".", "append", "(", "(", "best_targets", "+", "i", ")", ".", "cpu", "(", ")", ")", "\n", "\n", "", "all_scores", "=", "torch", ".", "cat", "(", "all_scores", ",", "1", ")", "\n", "all_targets", "=", "torch", ".", "cat", "(", "all_targets", ",", "1", ")", "\n", "\n", "all_scores", ",", "best_targets", "=", "all_scores", ".", "topk", "(", "2", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "all_targets", "=", "all_targets", ".", "gather", "(", "1", ",", "best_targets", ")", "\n", "\n", "# contextual dissimilarity measure", "\n", "", "elif", "params", ".", "dico_method", ".", "startswith", "(", "'csls_knn_'", ")", ":", "\n", "\n", "        ", "knn", "=", "params", ".", "dico_method", "[", "len", "(", "'csls_knn_'", ")", ":", "]", "\n", "assert", "knn", ".", "isdigit", "(", ")", "\n", "knn", "=", "int", "(", "knn", ")", "\n", "\n", "# average distances to k nearest neighbors", "\n", "average_dist1", "=", "torch", ".", "from_numpy", "(", "get_nn_avg_dist", "(", "emb2", ",", "emb1", ",", "knn", ")", ")", "\n", "average_dist2", "=", "torch", ".", "from_numpy", "(", "get_nn_avg_dist", "(", "emb1", ",", "emb2", ",", "knn", ")", ")", "\n", "average_dist1", "=", "average_dist1", ".", "type_as", "(", "emb1", ")", "\n", "average_dist2", "=", "average_dist2", ".", "type_as", "(", "emb2", ")", "\n", "\n", "# for every source word", "\n", "for", "i", "in", "range", "(", "0", ",", "n_src", ",", "bs", ")", ":", "\n", "\n", "# compute target words scores", "\n", "            ", "scores", "=", "emb2", ".", "mm", "(", "emb1", "[", "i", ":", "min", "(", "n_src", ",", "i", "+", "bs", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "scores", ".", "mul_", "(", "2", ")", "\n", "scores", ".", "sub_", "(", "average_dist1", "[", "i", ":", "min", "(", "n_src", ",", "i", "+", "bs", ")", "]", "[", ":", ",", "None", "]", "+", "average_dist2", "[", "None", ",", ":", "]", ")", "\n", "best_scores", ",", "best_targets", "=", "scores", ".", "topk", "(", "2", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "# update scores / potential targets", "\n", "all_scores", ".", "append", "(", "best_scores", ".", "cpu", "(", ")", ")", "\n", "all_targets", ".", "append", "(", "best_targets", ".", "cpu", "(", ")", ")", "\n", "\n", "", "all_scores", "=", "torch", ".", "cat", "(", "all_scores", ",", "0", ")", "\n", "all_targets", "=", "torch", ".", "cat", "(", "all_targets", ",", "0", ")", "\n", "\n", "", "all_pairs", "=", "torch", ".", "cat", "(", "[", "\n", "torch", ".", "arange", "(", "0", ",", "all_targets", ".", "size", "(", "0", ")", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ",", "\n", "all_targets", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", "\n", "]", ",", "1", ")", "\n", "\n", "# sanity check", "\n", "assert", "all_scores", ".", "size", "(", ")", "==", "all_pairs", ".", "size", "(", ")", "==", "(", "n_src", ",", "2", ")", "\n", "\n", "# sort pairs by score confidence", "\n", "diff", "=", "all_scores", "[", ":", ",", "0", "]", "-", "all_scores", "[", ":", ",", "1", "]", "\n", "reordered", "=", "diff", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "[", "1", "]", "\n", "all_scores", "=", "all_scores", "[", "reordered", "]", "\n", "all_pairs", "=", "all_pairs", "[", "reordered", "]", "\n", "\n", "# max dico words rank", "\n", "if", "params", ".", "dico_max_rank", ">", "0", ":", "\n", "        ", "selected", "=", "all_pairs", ".", "max", "(", "1", ")", "[", "0", "]", "<=", "params", ".", "dico_max_rank", "\n", "mask", "=", "selected", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "all_scores", ")", ".", "clone", "(", ")", "\n", "all_scores", "=", "all_scores", ".", "masked_select", "(", "mask", ")", ".", "view", "(", "-", "1", ",", "2", ")", "\n", "all_pairs", "=", "all_pairs", ".", "masked_select", "(", "mask", ")", ".", "view", "(", "-", "1", ",", "2", ")", "\n", "\n", "# max dico size", "\n", "", "if", "params", ".", "dico_max_size", ">", "0", ":", "\n", "        ", "all_scores", "=", "all_scores", "[", ":", "params", ".", "dico_max_size", "]", "\n", "all_pairs", "=", "all_pairs", "[", ":", "params", ".", "dico_max_size", "]", "\n", "\n", "# min dico size", "\n", "", "diff", "=", "all_scores", "[", ":", ",", "0", "]", "-", "all_scores", "[", ":", ",", "1", "]", "\n", "if", "params", ".", "dico_min_size", ">", "0", ":", "\n", "        ", "diff", "[", ":", "params", ".", "dico_min_size", "]", "=", "1e9", "\n", "\n", "# confidence threshold", "\n", "", "if", "params", ".", "dico_threshold", ">", "0", ":", "\n", "        ", "mask", "=", "diff", ">", "params", ".", "dico_threshold", "\n", "logger", ".", "info", "(", "\"Selected %i / %i pairs above the confidence threshold.\"", "%", "(", "mask", ".", "sum", "(", ")", ",", "diff", ".", "size", "(", "0", ")", ")", ")", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "all_pairs", ")", ".", "clone", "(", ")", "\n", "all_pairs", "=", "all_pairs", ".", "masked_select", "(", "mask", ")", ".", "view", "(", "-", "1", ",", "2", ")", "\n", "\n", "", "return", "all_pairs", ",", "all_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.build_dictionary": [[143, 179], ["logger.info", "logger.info", "torch.cat", "torch.LongTensor.cuda", "dico_builder.get_candidates", "dico_builder.get_candidates", "set", "set", "torch.LongTensor", "torch.LongTensor.size", "list", "len", "logger.warning", "set.numpy", "set.numpy", "int", "int"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates"], ["", "def", "build_dictionary", "(", "src_emb", ",", "tgt_emb", ",", "params", ",", "s2t_candidates", "=", "None", ",", "t2s_candidates", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Build a training dictionary given current embeddings / mapping.\n    \"\"\"", "\n", "logger", ".", "info", "(", "\"Building the train dictionary ...\"", ")", "\n", "s2t", "=", "'S2T'", "in", "params", ".", "dico_build", "\n", "t2s", "=", "'T2S'", "in", "params", ".", "dico_build", "\n", "assert", "s2t", "or", "t2s", "\n", "\n", "if", "s2t", ":", "\n", "        ", "if", "s2t_candidates", "is", "None", ":", "\n", "            ", "s2t_candidates", ",", "_", "=", "get_candidates", "(", "src_emb", ",", "tgt_emb", ",", "params", ")", "\n", "", "", "if", "t2s", ":", "\n", "        ", "if", "t2s_candidates", "is", "None", ":", "\n", "            ", "t2s_candidates", ",", "_", "=", "get_candidates", "(", "tgt_emb", ",", "src_emb", ",", "params", ")", "\n", "", "t2s_candidates", "=", "torch", ".", "cat", "(", "[", "t2s_candidates", "[", ":", ",", "1", ":", "]", ",", "t2s_candidates", "[", ":", ",", ":", "1", "]", "]", ",", "1", ")", "\n", "\n", "", "if", "params", ".", "dico_build", "==", "'S2T'", ":", "\n", "        ", "dico", "=", "s2t_candidates", "\n", "", "elif", "params", ".", "dico_build", "==", "'T2S'", ":", "\n", "        ", "dico", "=", "t2s_candidates", "\n", "", "else", ":", "\n", "        ", "s2t_candidates", "=", "set", "(", "[", "(", "a", ",", "b", ")", "for", "a", ",", "b", "in", "s2t_candidates", ".", "numpy", "(", ")", "]", ")", "\n", "t2s_candidates", "=", "set", "(", "[", "(", "a", ",", "b", ")", "for", "a", ",", "b", "in", "t2s_candidates", ".", "numpy", "(", ")", "]", ")", "\n", "if", "params", ".", "dico_build", "==", "'S2T|T2S'", ":", "\n", "            ", "final_pairs", "=", "s2t_candidates", "|", "t2s_candidates", "\n", "", "else", ":", "\n", "            ", "assert", "params", ".", "dico_build", "==", "'S2T&T2S'", "\n", "final_pairs", "=", "s2t_candidates", "&", "t2s_candidates", "\n", "if", "len", "(", "final_pairs", ")", "==", "0", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Empty intersection ...\"", ")", "\n", "return", "None", "\n", "", "", "dico", "=", "torch", ".", "LongTensor", "(", "list", "(", "[", "[", "int", "(", "a", ")", ",", "int", "(", "b", ")", "]", "for", "(", "a", ",", "b", ")", "in", "final_pairs", "]", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "'New train dictionary of %i pairs.'", "%", "dico", ".", "size", "(", "0", ")", ")", "\n", "return", "dico", ".", "cuda", "(", ")", "if", "params", ".", "cuda", "else", "dico", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.__init__": [[40, 87], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_dict": [[88, 95], ["bert_modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_json_file": [[96, 102], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.to_dict": [[103, 107], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.to_json_string": [[108, 111], ["json.dumps", "bert_modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTLayerNorm.__init__": [[114, 121], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "variance_epsilon", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BERTLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "config", ".", "hidden_size", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "variance_epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTLayerNorm.forward": [[122, 127], ["x.mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "gamma", "*", "x", "+", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTEmbeddings.__init__": [[129, 141], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "bert_modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\"\"\"Construct the embedding module from word, position and token_type embeddings.\n        \"\"\"", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTEmbeddings.forward": [[142, 157], ["input_ids.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "bert_modeling.BERTEmbeddings.word_embeddings", "bert_modeling.BERTEmbeddings.position_embeddings", "bert_modeling.BERTEmbeddings.token_type_embeddings", "bert_modeling.BERTEmbeddings.LayerNorm", "bert_modeling.BERTEmbeddings.dropout", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfAttention.__init__": [[160, 175], ["torch.Module.__init__", "int", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfAttention.transpose_for_scores": [[176, 182], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "# (batch, seq_len, hidden_size) => (batch, seq_len, num_head, head_size)", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "# (batch, seq_len, num_head, head_size) => (batch, num_head, seq_len, head_size)", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfAttention.forward": [[183, 216], ["bert_modeling.BERTSelfAttention.query", "bert_modeling.BERTSelfAttention.key", "bert_modeling.BERTSelfAttention.value", "bert_modeling.BERTSelfAttention.transpose_for_scores", "bert_modeling.BERTSelfAttention.transpose_for_scores", "bert_modeling.BERTSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "bert_modeling.BERTSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "bert_modeling.BERTSelfAttention.transpose", "math.sqrt", "torch.Softmax", "torch.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "# input: (batch, seq_len, hidden_size)", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "# (batch, seq_len, num_head, head_size) => (batch, num_head, seq_len, head_size)", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# (batch, num_head, seq_len, seq_len)", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "# (batch, num_head, seq_len, seq_len) * (batch, num_head, seq_len, head_size) ", "\n", "# => (batch, num_head, seq_len, head_size)", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "# (batch, num_head, seq_len, head_size) => (batch, seq_len, num_head, head_size)", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "# (batch, seq_len, num_head, head_size) => (batch, seq_len, hidden_size)", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfOutput.__init__": [[219, 224], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "bert_modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTSelfOutput.forward": [[225, 230], ["bert_modeling.BERTSelfOutput.dense", "bert_modeling.BERTSelfOutput.dropout", "bert_modeling.BERTSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTAttention.__init__": [[233, 237], ["torch.Module.__init__", "bert_modeling.BERTSelfAttention", "bert_modeling.BERTSelfOutput"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BERTSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BERTSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTAttention.forward": [[238, 242], ["bert_modeling.BERTAttention.self", "bert_modeling.BERTAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTIntermediate.__init__": [[245, 249], ["torch.Module.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "gelu", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTIntermediate.forward": [[250, 254], ["bert_modeling.BERTIntermediate.dense", "bert_modeling.BERTIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTOutput.__init__": [[257, 262], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "bert_modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTOutput.forward": [[263, 268], ["bert_modeling.BERTOutput.dense", "bert_modeling.BERTOutput.dropout", "bert_modeling.BERTOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTLayer.__init__": [[271, 276], ["torch.Module.__init__", "bert_modeling.BERTAttention", "bert_modeling.BERTIntermediate", "bert_modeling.BERTOutput"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BERTAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BERTIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BERTOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTLayer.forward": [[277, 282], ["bert_modeling.BERTLayer.attention", "bert_modeling.BERTLayer.intermediate", "bert_modeling.BERTLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTEncoder.__init__": [[285, 289], ["torch.Module.__init__", "bert_modeling.BERTLayer", "torch.ModuleList", "torch.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BERTLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTEncoder.forward": [[290, 296], ["layer_module", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTPooler.__init__": [[299, 303], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BERTPooler.forward": [[304, 311], ["bert_modeling.BERTPooler.dense", "bert_modeling.BERTPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertModel.__init__": [[330, 340], ["torch.Module.__init__", "bert_modeling.BERTEmbeddings", "bert_modeling.BERTEncoder", "bert_modeling.BERTPooler"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["def", "__init__", "(", "self", ",", "config", ":", "BertConfig", ")", ":", "\n", "        ", "\"\"\"Constructor for BertModel.\n\n        Args:\n            config: `BertConfig` instance.\n        \"\"\"", "\n", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embeddings", "=", "BERTEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BERTEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BERTPooler", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertModel.forward": [[341, 371], ["torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.float.float.float", "bert_modeling.BertModel.embeddings", "bert_modeling.BertModel.encoder", "bert_modeling.BertModel.pooler", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "input_mapping().cuda", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "input_mapping"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_mapping", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "float", "(", ")", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "if", "input_mapping", ":", "\n", "#print (embedding_output.device, input_mapping.module.weight.device)", "\n", "            ", "embedding_output", "=", "input_mapping", "(", "embedding_output", ")", ".", "cuda", "(", ")", "\n", "#print (\"mapping bert input\", embedding_output.device)", "\n", "", "all_encoder_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ")", "\n", "sequence_output", "=", "all_encoder_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "return", "all_encoder_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertForSequenceClassification.__init__": [[393, 410], ["torch.Module.__init__", "bert_modeling.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "bert_modeling.BertForSequenceClassification.apply", "isinstance", "isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n", "def", "init_weights", "(", "module", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "                ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BERTLayerNorm", ")", ":", "\n", "                ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertForSequenceClassification.forward": [[411, 422], ["bert_modeling.BertForSequenceClassification.bert", "bert_modeling.BertForSequenceClassification.dropout", "bert_modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.bert"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertForQuestionAnswering.__init__": [[442, 460], ["torch.Module.__init__", "bert_modeling.BertModel", "torch.Linear", "torch.Linear", "bert_modeling.BertForQuestionAnswering.apply", "isinstance", "isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n", "def", "init_weights", "(", "module", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "                ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BERTLayerNorm", ")", ":", "\n", "                ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertForQuestionAnswering.forward": [[461, 487], ["bert_modeling.BertForQuestionAnswering.bert", "bert_modeling.BertForQuestionAnswering.qa_outputs", "bert_modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.conllu2bert.bert"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "sequence_output", "=", "all_encoder_layers", "[", "-", "1", "]", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.gelu": [[29, 35], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.__init__": [[29, 57], ["getattr", "hasattr", "hasattr", "utils.get_optimizer", "print", "optim_fn", "utils.get_optimizer", "optim_fn", "mapping.parameters", "discriminator.parameters"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_optimizer", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_optimizer"], ["    ", "def", "__init__", "(", "self", ",", "src_emb", ",", "tgt_emb", ",", "mapping", ",", "discriminator", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Initialize trainer script.\n        \"\"\"", "\n", "self", ".", "src_emb", "=", "src_emb", "\n", "self", ".", "tgt_emb", "=", "tgt_emb", "\n", "self", ".", "src_dico", "=", "params", ".", "src_dico", "\n", "self", ".", "tgt_dico", "=", "getattr", "(", "params", ",", "'tgt_dico'", ",", "None", ")", "\n", "self", ".", "mapping", "=", "mapping", "\n", "self", ".", "discriminator", "=", "discriminator", "\n", "self", ".", "params", "=", "params", "\n", "\n", "# optimizers", "\n", "if", "hasattr", "(", "params", ",", "'map_optimizer'", ")", ":", "\n", "            ", "optim_fn", ",", "optim_params", "=", "get_optimizer", "(", "params", ".", "map_optimizer", ")", "\n", "print", "(", "\"Map optimizer parameters: \"", ",", "optim_params", ")", "\n", "self", ".", "map_optimizer", "=", "optim_fn", "(", "mapping", ".", "parameters", "(", ")", ",", "**", "optim_params", ")", "\n", "", "if", "hasattr", "(", "params", ",", "'dis_optimizer'", ")", ":", "\n", "            ", "optim_fn", ",", "optim_params", "=", "get_optimizer", "(", "params", ".", "dis_optimizer", ")", "\n", "self", ".", "dis_optimizer", "=", "optim_fn", "(", "discriminator", ".", "parameters", "(", ")", ",", "**", "optim_params", ")", "\n", "", "else", ":", "\n", "            ", "assert", "discriminator", "is", "None", "\n", "\n", "# best validation score", "\n", "", "self", ".", "best_valid_metric", "=", "-", "1e12", "\n", "\n", "self", ".", "decrease_lr", "=", "False", "\n", "self", ".", "decrease_dis_lr", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_dis_xy": [[58, 92], ["torch.LongTensor().random_", "torch.LongTensor().random_", "torch.cat", "torch.FloatTensor().zero_", "torch.autograd.Variable", "min", "src_ids.cuda.cuda.cuda", "tgt_ids.cuda.cuda.cuda", "torch.no_grad", "trainer.Trainer.src_emb", "trainer.Trainer.tgt_emb", "trainer.Trainer.mapping", "torch.autograd.Variable", "len", "len", "torch.LongTensor", "len", "torch.LongTensor", "len", "torch.autograd.Variable", "torch.autograd.Variable", "torch.no_grad", "trainer.Trainer.mapping", "torch.autograd.Variable", "torch.autograd.Variable", "torch.FloatTensor", "torch.autograd.Variable.cuda", "torch.autograd.Variable"], "methods", ["None"], ["", "def", "get_dis_xy", "(", "self", ",", "volatile", ")", ":", "\n", "        ", "\"\"\"\n        Get discriminator input batch / output target.\n        \"\"\"", "\n", "# select random word IDs", "\n", "bs", "=", "self", ".", "params", ".", "batch_size", "\n", "mf", "=", "self", ".", "params", ".", "dis_most_frequent", "\n", "assert", "mf", "<=", "min", "(", "len", "(", "self", ".", "src_dico", ")", ",", "len", "(", "self", ".", "tgt_dico", ")", ")", "\n", "src_ids", "=", "torch", ".", "LongTensor", "(", "bs", ")", ".", "random_", "(", "len", "(", "self", ".", "src_dico", ")", "if", "mf", "==", "0", "else", "mf", ")", "\n", "tgt_ids", "=", "torch", ".", "LongTensor", "(", "bs", ")", ".", "random_", "(", "len", "(", "self", ".", "tgt_dico", ")", "if", "mf", "==", "0", "else", "mf", ")", "\n", "if", "self", ".", "params", ".", "cuda", ":", "\n", "            ", "src_ids", "=", "src_ids", ".", "cuda", "(", ")", "\n", "tgt_ids", "=", "tgt_ids", ".", "cuda", "(", ")", "\n", "\n", "# get word embeddings", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "src_emb", "=", "self", ".", "src_emb", "(", "Variable", "(", "src_ids", ")", ")", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", "(", "Variable", "(", "tgt_ids", ")", ")", "\n", "", "if", "volatile", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "src_emb", "=", "self", ".", "mapping", "(", "Variable", "(", "src_emb", ".", "data", ")", ")", "\n", "tgt_emb", "=", "Variable", "(", "tgt_emb", ".", "data", ")", "\n", "", "", "else", ":", "\n", "            ", "src_emb", "=", "self", ".", "mapping", "(", "Variable", "(", "src_emb", ".", "data", ")", ")", "\n", "tgt_emb", "=", "Variable", "(", "tgt_emb", ".", "data", ")", "\n", "\n", "# input / target", "\n", "", "x", "=", "torch", ".", "cat", "(", "[", "src_emb", ",", "tgt_emb", "]", ",", "0", ")", "\n", "y", "=", "torch", ".", "FloatTensor", "(", "2", "*", "bs", ")", ".", "zero_", "(", ")", "\n", "y", "[", ":", "bs", "]", "=", "1", "-", "self", ".", "params", ".", "dis_smooth", "\n", "y", "[", "bs", ":", "]", "=", "self", ".", "params", ".", "dis_smooth", "\n", "y", "=", "Variable", "(", "y", ".", "cuda", "(", ")", "if", "self", ".", "params", ".", "cuda", "else", "y", ")", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.dis_step": [[93, 115], ["trainer.Trainer.discriminator.train", "trainer.Trainer.get_dis_xy", "trainer.Trainer.discriminator", "torch.nn.functional.binary_cross_entropy", "stats[].append", "trainer.Trainer.dis_optimizer.zero_grad", "torch.nn.functional.binary_cross_entropy.backward", "trainer.Trainer.dis_optimizer.step", "utils.clip_parameters", "torch.autograd.Variable", "torch.nn.functional.binary_cross_entropy.item", "logger.error", "exit"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.train", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_dis_xy", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.clip_parameters"], ["", "def", "dis_step", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"\n        Train the discriminator.\n        \"\"\"", "\n", "self", ".", "discriminator", ".", "train", "(", ")", "\n", "\n", "# loss", "\n", "x", ",", "y", "=", "self", ".", "get_dis_xy", "(", "volatile", "=", "True", ")", "\n", "preds", "=", "self", ".", "discriminator", "(", "Variable", "(", "x", ".", "data", ")", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "preds", ",", "y", ")", "\n", "stats", "[", "'DIS_COSTS'", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# check NaN", "\n", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected (discriminator)\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# optim", "\n", "", "self", ".", "dis_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "dis_optimizer", ".", "step", "(", ")", "\n", "clip_parameters", "(", "self", ".", "discriminator", ",", "self", ".", "params", ".", "dis_clip_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.mapping_step": [[116, 143], ["trainer.Trainer.discriminator.eval", "trainer.Trainer.get_dis_xy", "trainer.Trainer.discriminator", "torch.nn.functional.binary_cross_entropy", "trainer.Trainer.map_optimizer.zero_grad", "torch.nn.functional.binary_cross_entropy.backward", "trainer.Trainer.map_optimizer.step", "trainer.Trainer.orthogonalize", "logger.error", "exit"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_dis_xy", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.orthogonalize"], ["", "def", "mapping_step", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"\n        Fooling discriminator training step.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "dis_lambda", "==", "0", ":", "\n", "            ", "return", "0", "\n", "\n", "", "self", ".", "discriminator", ".", "eval", "(", ")", "\n", "\n", "# loss", "\n", "x", ",", "y", "=", "self", ".", "get_dis_xy", "(", "volatile", "=", "False", ")", "\n", "preds", "=", "self", ".", "discriminator", "(", "x", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "preds", ",", "1", "-", "y", ")", "\n", "loss", "=", "self", ".", "params", ".", "dis_lambda", "*", "loss", "\n", "\n", "# check NaN", "\n", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected (fool discriminator)\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# optim", "\n", "", "self", ".", "map_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "map_optimizer", ".", "step", "(", ")", "\n", "self", ".", "orthogonalize", "(", ")", "\n", "\n", "return", "2", "*", "self", ".", "params", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_aligned_id_batchs": [[144, 164], ["numpy.arange", "range", "random.shuffle", "min", "len", "batches.append", "len", "len", "len"], "methods", ["None"], ["", "def", "get_aligned_id_batchs", "(", "self", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Get aligned ids by batch\n        \"\"\"", "\n", "batches", "=", "[", "]", "\n", "# ids for aligned embeddings", "\n", "ids", "=", "np", ".", "arange", "(", "self", ".", "dico", ".", "shape", "[", "0", "]", ")", "\n", "if", "shuffle", ":", "\n", "            ", "random", ".", "shuffle", "(", "ids", ")", "\n", "", "bs", "=", "self", ".", "params", ".", "batch_size", "\n", "assert", "bs", "<=", "min", "(", "len", "(", "self", ".", "src_dico", ")", ",", "len", "(", "self", ".", "tgt_dico", ")", ")", "\n", "for", "offset", "in", "range", "(", "0", ",", "len", "(", "ids", ")", ",", "bs", ")", ":", "\n", "            ", "if", "offset", "+", "bs", "<=", "len", "(", "ids", ")", ":", "\n", "                ", "src_ids", "=", "ids", "[", "offset", ":", "offset", "+", "bs", "]", "\n", "tgt_ids", "=", "ids", "[", "offset", ":", "offset", "+", "bs", "]", "\n", "", "else", ":", "\n", "                ", "src_ids", "=", "ids", "[", "offset", ":", "]", "\n", "tgt_ids", "=", "ids", "[", "offset", ":", "]", "\n", "", "batches", ".", "append", "(", "(", "src_ids", ",", "tgt_ids", ")", ")", "\n", "", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_aligned_embs": [[165, 184], ["trainer.Trainer.mapping", "src_ids.cuda.cuda.cuda", "tgt_ids.cuda.cuda.cuda", "torch.no_grad", "trainer.Trainer.src_emb", "trainer.Trainer.tgt_emb"], "methods", ["None"], ["", "def", "get_aligned_embs", "(", "self", ",", "src_ids", ",", "tgt_ids", ")", ":", "\n", "        ", "\"\"\"\n        Get aligned embeddings.\n        \"\"\"", "\n", "\n", "src_ids", "=", "self", ".", "dico", "[", "src_ids", ",", "0", "]", "\n", "tgt_ids", "=", "self", ".", "dico", "[", "tgt_ids", ",", "1", "]", "\n", "if", "self", ".", "params", ".", "cuda", ":", "\n", "            ", "src_ids", "=", "src_ids", ".", "cuda", "(", ")", "\n", "tgt_ids", "=", "tgt_ids", ".", "cuda", "(", ")", "\n", "\n", "# get word embeddings", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "src_emb", "=", "self", ".", "src_emb", "(", "src_ids", ")", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", "(", "tgt_ids", ")", "\n", "\n", "", "src_emb", "=", "self", ".", "mapping", "(", "src_emb", ")", "\n", "\n", "return", "src_emb", ",", "tgt_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.supervised_mapping_step": [[185, 255], ["trainer.Trainer.get_aligned_embs", "trainer.Trainer.map_optimizer.zero_grad", "torch.where.mean.backward", "trainer.Trainer.map_optimizer.step", "logger.error", "exit", "trainer.Trainer.orthogonalize", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "gold_scores.mean", "trainer.Trainer.params.loss.startswith", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "src_emb.mm", "int", "src_emb.mm.topk", "gold_scores.unsqueeze().expand_as", "torch.where", "torch.where.mean", "src_emb.norm", "tgt_emb.norm", "tgt_emb.transpose", "torch.ones_like", "torch.zeros_like", "src_emb.norm", "tgt_emb.norm", "trainer.Trainer.params.loss.split", "gold_scores.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.get_aligned_embs", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.orthogonalize"], ["", "def", "supervised_mapping_step", "(", "self", ",", "src_ids", ",", "tgt_ids", ",", "margin", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        Fooling discriminator training step.\n        \"\"\"", "\n", "\n", "# loss", "\n", "src_emb", ",", "tgt_emb", "=", "self", ".", "get_aligned_embs", "(", "src_ids", ",", "tgt_ids", ")", "\n", "\n", "# normalization", "\n", "if", "self", ".", "params", ".", "normalize_embed", ":", "\n", "            ", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "# (n, n)", "\n", "#scores = src_emb.mm(tgt_emb.transpose(0, 1))", "\n", "#rang = torch.arange(scores.shape[0], out=torch.LongTensor())", "\n", "#gold_scores = scores[rang, rang]", "\n", "\n", "", "if", "self", ".", "params", ".", "loss", "==", "'cos_sim'", ":", "\n", "# (n)", "\n", "            ", "gold_scores", "=", "(", "src_emb", "*", "tgt_emb", ")", ".", "sum", "(", "1", ")", "\n", "# maximize cosine similarities", "\n", "loss", "=", "-", "gold_scores", ".", "mean", "(", ")", "\n", "", "elif", "self", ".", "params", ".", "loss", "==", "'l2_dist'", ":", "\n", "# (n, d)", "\n", "            ", "sub", "=", "src_emb", "-", "tgt_emb", "\n", "# (n, d) => (n) => ()", "\n", "loss", "=", "(", "sub", "*", "sub", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "", "elif", "self", ".", "params", ".", "loss", ".", "startswith", "(", "'max_margin_top'", ")", ":", "\n", "# (n)", "\n", "            ", "gold_scores", "=", "(", "src_emb", "*", "tgt_emb", ")", ".", "sum", "(", "1", ")", "\n", "# (n, n)", "\n", "scores", "=", "src_emb", ".", "mm", "(", "tgt_emb", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "# max margin with top k elements", "\n", "k", "=", "int", "(", "self", ".", "params", ".", "loss", ".", "split", "(", "'-'", ")", "[", "1", "]", ")", "\n", "# (n, k)", "\n", "top_vals", ",", "top_ids", "=", "scores", ".", "topk", "(", "k", ",", "1", ",", "True", ")", "\n", "# (n) => (n, k)", "\n", "gold_vals", "=", "gold_scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "top_vals", ")", "\n", "# (n, k)", "\n", "margins", "=", "torch", ".", "ones_like", "(", "top_vals", ")", "*", "margin", "\n", "# (n, k)", "\n", "losses", "=", "margins", "-", "gold_vals", "+", "top_vals", "\n", "# mask out less than 0", "\n", "losses", "=", "torch", ".", "where", "(", "losses", ">", "0", ",", "losses", ",", "torch", ".", "zeros_like", "(", "losses", ")", ")", "\n", "# ()", "\n", "loss", "=", "losses", ".", "mean", "(", ")", "\n", "\n", "# check NaN", "\n", "", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected (fool discriminator)\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# calculating average cosine similarity", "\n", "", "if", "not", "self", ".", "params", ".", "normalize_embed", ":", "\n", "            ", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "# (n)", "\n", "", "avg_cos_sim", "=", "(", "src_emb", "*", "tgt_emb", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "if", "self", ".", "params", ".", "test", ":", "\n", "            ", "return", "avg_cos_sim", ",", "loss", "\n", "# optim", "\n", "", "self", ".", "map_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "map_optimizer", ".", "step", "(", ")", "\n", "if", "self", ".", "params", ".", "ortho", ":", "\n", "#logger.info(\"Applying Orthogonalize\")", "\n", "            ", "self", ".", "orthogonalize", "(", ")", "\n", "\n", "", "return", "avg_cos_sim", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_training_dico": [[256, 280], ["evaluation.word_translation.load_identical_char_dico", "trainer.Trainer.dico.cuda", "evaluation.word_translation.load_dictionary", "evaluation.word_translation.load_dictionary", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.load_identical_char_dico", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.load_dictionary", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.load_dictionary"], ["", "def", "load_training_dico", "(", "self", ",", "dico_train", ")", ":", "\n", "        ", "\"\"\"\n        Load training dictionary.\n        \"\"\"", "\n", "word2id1", "=", "self", ".", "src_dico", ".", "word2id", "\n", "word2id2", "=", "self", ".", "tgt_dico", ".", "word2id", "\n", "\n", "# identical character strings", "\n", "if", "dico_train", "==", "\"identical_char\"", ":", "\n", "            ", "self", ".", "dico", "=", "load_identical_char_dico", "(", "word2id1", ",", "word2id2", ")", "\n", "# use one of the provided dictionary", "\n", "", "elif", "dico_train", "==", "\"default\"", ":", "\n", "            ", "filename", "=", "'%s-%s.0-5000.txt'", "%", "(", "self", ".", "params", ".", "src_lang", ",", "self", ".", "params", ".", "tgt_lang", ")", "\n", "self", ".", "dico", "=", "load_dictionary", "(", "\n", "os", ".", "path", ".", "join", "(", "DIC_EVAL_PATH", ",", "filename", ")", ",", "\n", "word2id1", ",", "word2id2", "\n", ")", "\n", "# dictionary provided by the user", "\n", "", "else", ":", "\n", "            ", "self", ".", "dico", "=", "load_dictionary", "(", "dico_train", ",", "word2id1", ",", "word2id2", ")", "\n", "\n", "# cuda", "\n", "", "if", "self", ".", "params", ".", "cuda", ":", "\n", "            ", "self", ".", "dico", "=", "self", ".", "dico", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.build_dictionary": [[281, 290], ["dico_builder.build_dictionary", "trainer.Trainer.mapping", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "src_emb.norm", "tgt_emb.norm"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.build_dictionary"], ["", "", "def", "build_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Build a dictionary from aligned embeddings.\n        \"\"\"", "\n", "src_emb", "=", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", "\n", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "self", ".", "dico", "=", "build_dictionary", "(", "src_emb", ",", "tgt_emb", ",", "self", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.procrustes": [[291, 306], ["isinstance", "B.transpose().mm().cpu().numpy", "scipy.linalg.svd", "scipy.linalg.svd", "scipy.linalg.svd", "scipy.linalg.svd", "W.copy_", "logger.info", "torch.from_numpy().type_as", "B.transpose().mm().cpu", "torch.from_numpy", "B.transpose().mm", "U.dot", "B.transpose"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.svd"], ["", "def", "procrustes", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n        https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n        \"\"\"", "\n", "A", "=", "self", ".", "src_emb", ".", "weight", ".", "data", "[", "self", ".", "dico", "[", ":", ",", "0", "]", "]", "\n", "B", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", "[", "self", ".", "dico", "[", ":", ",", "1", "]", "]", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "", "M", "=", "B", ".", "transpose", "(", "0", ",", "1", ")", ".", "mm", "(", "A", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "U", ",", "S", ",", "V_t", "=", "scipy", ".", "linalg", ".", "svd", "(", "M", ",", "full_matrices", "=", "True", ")", "\n", "W", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "U", ".", "dot", "(", "V_t", ")", ")", ".", "type_as", "(", "W", ")", ")", "\n", "logger", ".", "info", "(", "\"Finished Procrustes.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.orthogonalize": [[307, 318], ["isinstance", "W.copy_", "W.mm", "W.transpose().mm", "W.transpose"], "methods", ["None"], ["", "def", "orthogonalize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Orthogonalize the mapping.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "map_beta", ">", "0", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "W", "=", "self", ".", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "                ", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "", "beta", "=", "self", ".", "params", ".", "map_beta", "\n", "W", ".", "copy_", "(", "(", "1", "+", "beta", ")", "*", "W", "-", "beta", "*", "W", ".", "mm", "(", "W", ".", "transpose", "(", "0", ",", "1", ")", ".", "mm", "(", "W", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.decay_map_lr": [[319, 330], ["max", "logger.info"], "methods", ["None"], ["", "", "def", "decay_map_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Update learning rate when using SGD.\n        \"\"\"", "\n", "if", "'sgd'", "not", "in", "self", ".", "params", ".", "map_optimizer", ":", "\n", "            ", "return", "\n", "", "old_lr", "=", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "new_lr", "=", "max", "(", "self", ".", "params", ".", "min_lr", ",", "old_lr", "*", "self", ".", "params", ".", "lr_decay", ")", "\n", "if", "new_lr", "<", "old_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\"Decreasing Mapping learning rate: %.8f -> %.8f\"", "%", "(", "old_lr", ",", "new_lr", ")", ")", "\n", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.update_lr": [[331, 355], ["max", "logger.info", "logger.info", "logger.info"], "methods", ["None"], ["", "", "def", "update_lr", "(", "self", ",", "to_log", ",", "metric", ")", ":", "\n", "        ", "\"\"\"\n        Update learning rate when using SGD.\n        \"\"\"", "\n", "if", "'sgd'", "not", "in", "self", ".", "params", ".", "map_optimizer", ":", "\n", "            ", "return", "\n", "", "old_lr", "=", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "new_lr", "=", "max", "(", "self", ".", "params", ".", "min_lr", ",", "old_lr", "*", "self", ".", "params", ".", "lr_decay", ")", "\n", "if", "new_lr", "<", "old_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\"Decreasing learning rate: %.8f -> %.8f\"", "%", "(", "old_lr", ",", "new_lr", ")", ")", "\n", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "\n", "", "if", "self", ".", "params", ".", "lr_shrink", "<", "1", "and", "to_log", "[", "metric", "]", ">=", "-", "1e7", ":", "\n", "            ", "if", "to_log", "[", "metric", "]", "<", "self", ".", "best_valid_metric", ":", "\n", "                ", "logger", ".", "info", "(", "\"Validation metric is smaller than the best: %.5f vs %.5f\"", "\n", "%", "(", "to_log", "[", "metric", "]", ",", "self", ".", "best_valid_metric", ")", ")", "\n", "# decrease the learning rate, only if this is the", "\n", "# second time the validation metric decreases", "\n", "if", "self", ".", "decrease_lr", ":", "\n", "                    ", "old_lr", "=", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "*=", "self", ".", "params", ".", "lr_shrink", "\n", "logger", ".", "info", "(", "\"Shrinking the learning rate: %.5f -> %.5f\"", "\n", "%", "(", "old_lr", ",", "self", ".", "map_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "", "self", ".", "decrease_lr", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.update_dis_lr": [[356, 380], ["max", "logger.info", "logger.info", "logger.info"], "methods", ["None"], ["", "", "", "def", "update_dis_lr", "(", "self", ",", "to_log", ",", "metric", ")", ":", "\n", "        ", "\"\"\"\n        Update learning rate when using SGD.\n        \"\"\"", "\n", "if", "'sgd'", "not", "in", "self", ".", "params", ".", "dis_optimizer", ":", "\n", "            ", "return", "\n", "", "old_lr", "=", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "new_lr", "=", "max", "(", "self", ".", "params", ".", "min_lr", ",", "old_lr", "*", "self", ".", "params", ".", "lr_decay", ")", "\n", "if", "new_lr", "<", "old_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\"Decreasing discriminator learning rate: %.8f -> %.8f\"", "%", "(", "old_lr", ",", "new_lr", ")", ")", "\n", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "\n", "", "if", "self", ".", "params", ".", "lr_shrink", "<", "1", "and", "to_log", "[", "metric", "]", ">=", "-", "1e7", ":", "\n", "            ", "if", "to_log", "[", "metric", "]", "<", "self", ".", "best_valid_metric", ":", "\n", "                ", "logger", ".", "info", "(", "\"Validation metric is smaller than the best: %.5f vs %.5f\"", "\n", "%", "(", "to_log", "[", "metric", "]", ",", "self", ".", "best_valid_metric", ")", ")", "\n", "# decrease the learning rate, only if this is the", "\n", "# second time the validation metric decreases", "\n", "if", "self", ".", "decrease_dis_lr", ":", "\n", "                    ", "old_lr", "=", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "*=", "self", ".", "params", ".", "lr_shrink", "\n", "logger", ".", "info", "(", "\"Shrinking the discriminator learning rate: %.5f -> %.5f\"", "\n", "%", "(", "old_lr", ",", "self", ".", "dis_optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "", "self", ".", "decrease_dis_lr", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_best": [[381, 398], ["logger.info", "os.path.join", "trainer.Trainer.save_model"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model"], ["", "", "", "def", "save_best", "(", "self", ",", "to_log", ",", "metric", ")", ":", "\n", "        ", "\"\"\"\n        Save the best model for the given validation metric.\n        \"\"\"", "\n", "# best mapping for the given validation criterion", "\n", "if", "to_log", "[", "metric", "]", ">", "self", ".", "best_valid_metric", ":", "\n", "# new best mapping", "\n", "            ", "self", ".", "best_valid_metric", "=", "to_log", "[", "metric", "]", "\n", "logger", ".", "info", "(", "'* Best value for \"%s\": %.5f'", "%", "(", "metric", ",", "to_log", "[", "metric", "]", ")", ")", "\n", "\n", "#if isinstance(self.mapping, torch.nn.DataParallel):", "\n", "#    W = self.mapping.module.weight.data.cpu().numpy()", "\n", "#else:", "\n", "#    W = self.mapping.weight.data.cpu().numpy()", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "#logger.info('* Saving the mapping to %s ...' % path)", "\n", "self", ".", "save_model", "(", "path", ")", "\n", "#torch.save(W, path)", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_iter": [[400, 415], ["os.path.join", "trainer.Trainer.save_model", "str"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model"], ["", "", "def", "save_iter", "(", "self", ",", "iter", ")", ":", "\n", "        ", "\"\"\"\n        Save the current model.\n        \"\"\"", "\n", "# best mapping for the given validation criterion", "\n", "#if isinstance(self.mapping, torch.nn.DataParallel):", "\n", "#    W = self.mapping.module.weight.data.cpu().numpy()", "\n", "#else:", "\n", "#    W = self.mapping.weight.data.cpu().numpy()", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_path", ",", "'iter-'", "+", "str", "(", "iter", ")", ",", "'best_mapping.pkl'", ")", "\n", "#if not os.path.exists(os.path.dirname(path)):", "\n", "#    os.makedirs(os.path.dirname(path))", "\n", "#logger.info('* Saving the mapping to %s ...' % path)", "\n", "#path = os.path.join(self.params.model_path, 'iter-'+str(iter))", "\n", "self", ".", "save_model", "(", "path", ")", "\n", "#torch.save(W, path)", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.save_model": [[417, 431], ["logger.info", "isinstance", "os.path.exists", "os.makedirs", "torch.save", "torch.save", "os.path.dirname", "os.path.dirname", "trainer.Trainer.mapping.module.state_dict", "trainer.Trainer.mapping.state_dict"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        Save model to path.\n        \"\"\"", "\n", "#path = os.path.join(path, 'best_mapping.pkl')", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "'* Saving the mapping to %s ...'", "%", "path", ")", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "mapping", ".", "module", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "#W = self.mapping.module.weight.data.cpu().numpy()", "\n", "", "else", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "mapping", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "#W = self.mapping.weight.data.cpu().numpy()", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.reload_best": [[434, 449], ["os.path.join", "logger.info", "os.path.isfile", "torch.from_numpy", "isinstance", "W.copy_", "torch.load", "torch.from_numpy.size", "W.size", "torch.from_numpy.type_as"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["", "", "def", "reload_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload the best mapping.\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'* Reloading the best model from %s ...'", "%", "path", ")", "\n", "# reload the model", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "to_reload", "=", "torch", ".", "from_numpy", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "module", ".", "weight", ".", "data", "\n", "", "else", ":", "\n", "            ", "W", "=", "self", ".", "mapping", ".", "weight", ".", "data", "\n", "", "assert", "to_reload", ".", "size", "(", ")", "==", "W", ".", "size", "(", ")", "\n", "W", ".", "copy_", "(", "to_reload", ".", "type_as", "(", "W", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_best": [[450, 463], ["os.path.join", "logger.info", "os.path.isfile", "trainer.Trainer.load_model", "print", "trainer.Trainer.reload_best"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.reload_best"], ["", "def", "load_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload the best mapping.\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_path", ",", "'best_mapping.pkl'", ")", "\n", "logger", ".", "info", "(", "'* Loading the best model from %s ...'", "%", "path", ")", "\n", "# reload the model", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "if", "self", ".", "load_model", "(", "path", ")", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Failed while loading by state_dict, reloading by copying ...\"", ")", "\n", "self", ".", "reload_best", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_model": [[464, 478], ["isinstance", "trainer.Trainer.mapping.module.load_state_dict", "trainer.Trainer.mapping.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["", "", "def", "load_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        load model from path\n        \"\"\"", "\n", "try", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "mapping", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "self", ".", "mapping", ".", "module", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "mapping", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", "except", ":", "\n", "            ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.export": [[479, 504], ["logger.info", "utils.load_embeddings", "utils.load_embeddings", "utils.normalize_embeddings", "utils.normalize_embeddings", "logger.info", "enumerate", "utils.export_embeddings", "range", "trainer.Trainer.mapping().data.cpu", "len", "torch.no_grad", "torch.autograd.Variable", "trainer.Trainer.mapping", "torch.autograd.Variable.cuda"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.export_embeddings"], ["", "def", "export", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Export embeddings.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "\n", "# load all embeddings", "\n", "logger", ".", "info", "(", "\"Reloading all embeddings for mapping ...\"", ")", "\n", "params", ".", "src_dico", ",", "src_emb", "=", "load_embeddings", "(", "params", ",", "source", "=", "True", ",", "full_vocab", "=", "True", ")", "\n", "params", ".", "tgt_dico", ",", "tgt_emb", "=", "load_embeddings", "(", "params", ",", "source", "=", "False", ",", "full_vocab", "=", "True", ")", "\n", "\n", "# apply same normalization as during training", "\n", "normalize_embeddings", "(", "src_emb", ",", "params", ".", "normalize_embeddings", ",", "mean", "=", "params", ".", "src_mean", ")", "\n", "normalize_embeddings", "(", "tgt_emb", ",", "params", ".", "normalize_embeddings", ",", "mean", "=", "params", ".", "tgt_mean", ")", "\n", "\n", "# map source embeddings to the target space", "\n", "bs", "=", "4096", "\n", "logger", ".", "info", "(", "\"Map source embeddings to the target space ...\"", ")", "\n", "for", "i", ",", "k", "in", "enumerate", "(", "range", "(", "0", ",", "len", "(", "src_emb", ")", ",", "bs", ")", ")", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "x", "=", "Variable", "(", "src_emb", "[", "k", ":", "k", "+", "bs", "]", ")", "\n", "", "src_emb", "[", "k", ":", "k", "+", "bs", "]", "=", "self", ".", "mapping", "(", "x", ".", "cuda", "(", ")", "if", "params", ".", "cuda", "else", "x", ")", ".", "data", ".", "cpu", "(", ")", "\n", "\n", "# write embeddings to the disk", "\n", "", "export_embeddings", "(", "src_emb", ",", "tgt_emb", ",", "params", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.InputExample.__init__": [[46, 50], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "unique_id", ",", "text_a", ",", "text_b", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.InputBertExample.__init__": [[53, 59], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "unique_id", ",", "embs_a", ",", "toks_a", ",", "embs_b", ",", "toks_b", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "embs_a", "=", "embs_a", "\n", "self", ".", "toks_a", "=", "toks_a", "\n", "self", ".", "embs_b", "=", "embs_b", "\n", "self", ".", "toks_b", "=", "toks_b", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.InputSingleBertExample.__init__": [[62, 66], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "unique_id", ",", "embs", ",", "toks", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "embs", "=", "embs", "\n", "self", ".", "toks", "=", "toks", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.InputFeatures.__init__": [[70, 76], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "unique_id", ",", "tokens", ",", "input_ids", ",", "input_mask", ",", "input_type_ids", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_type_ids", "=", "input_type_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.InputBertFeatures.__init__": [[116, 121], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "unique_id", ",", "tokens", ",", "input_mask", ",", "input_embs", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_embs", "=", "input_embs", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.BiInputFeatures.__init__": [[90, 99], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "unique_id", ",", "tokens", ",", "input_ids", ",", "input_mask", ",", "input_lan_ids", ",", "\n", "align", "=", "[", "None", ",", "None", "]", ",", "align_mask", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens_a", ",", "self", ".", "tokens_b", "=", "tokens", "\n", "self", ".", "input_ids_a", ",", "self", ".", "input_ids_b", "=", "input_ids", "\n", "self", ".", "input_mask_a", ",", "self", ".", "input_mask_b", "=", "input_mask", "\n", "self", ".", "input_lan_ids_a", ",", "self", ".", "input_lan_ids_b", "=", "input_lan_ids", "\n", "self", ".", "align_ids_a", ",", "self", ".", "align_ids_b", "=", "align", "\n", "self", ".", "align_mask", "=", "align_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.BiInputBertFeatures.__init__": [[103, 112], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "unique_id", ",", "tokens", ",", "input_ids", ",", "input_mask", ",", "input_embs", ",", "\n", "align", "=", "[", "None", ",", "None", "]", ",", "align_mask", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens_a", ",", "self", ".", "tokens_b", "=", "tokens", "\n", "self", ".", "input_ids_a", ",", "self", ".", "input_ids_b", "=", "input_ids", "\n", "self", ".", "input_mask_a", ",", "self", ".", "input_mask_b", "=", "input_mask", "\n", "self", ".", "input_embs_a", ",", "self", ".", "input_embs_b", "=", "input_embs", "\n", "self", ".", "align_ids_a", ",", "self", ".", "align_ids_b", "=", "align", "\n", "self", ".", "align_mask", "=", "align_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token": [[122, 131], ["logger.info", "new_tokens.append", "new_tokens.append"], "function", ["None"], ["", "", "def", "check_token", "(", "vocab", ",", "tokens", ",", "unk_token", "=", "\"[UNK]\"", ")", ":", "\n", "    ", "new_tokens", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "if", "token", "not", "in", "vocab", ":", "\n", "            ", "logger", ".", "info", "(", "\"missed token: %s\"", "%", "token", ")", "\n", "new_tokens", ".", "append", "(", "unk_token", ")", "\n", "", "else", ":", "\n", "            ", "new_tokens", ".", "append", "(", "token", ")", "\n", "", "", "return", "new_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_examples_to_features": [[132, 258], ["enumerate", "max", "logger.info", "tokens_a_.append", "input_lan_ids_a_.append", "tokens_a_.append", "input_lan_ids_a_.append", "tokens_b_.append", "input_lan_ids_b_.append", "tokens_b_.append", "input_lan_ids_b_.append", "tokenizer.convert_tokens_to_ids", "len", "print", "load.check_token", "tokenizer.tokenize", "len", "len", "tokens_a_.append", "input_lan_ids_a_.append", "tokens_b_.append", "input_lan_ids_b_.append", "tokenizer1.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "len", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask_a_.append", "input_lan_ids_a_.append", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask_b_.append", "input_lan_ids_b_.append", "len", "len", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "features.append", "features.append", "sum", "example.text_a.split", "load.check_token", "load.check_token", "tokenizer1.tokenize", "tokenizer.tokenize", "len", "len", "len", "len", "align_ids_a.append", "align_ids_b.append", "align_mask.append", "logger.info", "logger.info", "logger.info", "load.BiInputFeatures", "load.BiInputFeatures", "example.text_b.split", "example.text_b.split", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "seq_length", ",", "tokenizer", ",", "tokenizer1", "=", "None", ",", "aligns", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "if", "aligns", ":", "\n", "        ", "lens", "=", "[", "len", "(", "a", "[", "0", "]", ")", "for", "a", "in", "aligns", "]", "\n", "max_align", "=", "max", "(", "lens", ")", "\n", "logger", ".", "info", "(", "\"aligned word number: %s\"", "%", "(", "sum", "(", "lens", ")", ")", ")", "\n", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "ex_index", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"\\r%d\"", "%", "ex_index", ",", "end", "=", "\"\"", ")", "\n", "", "if", "aligns", ":", "\n", "            ", "tokens_a", "=", "check_token", "(", "tokenizer", ".", "vocab", ",", "example", ".", "text_a", ".", "split", "(", "' '", ")", ")", "\n", "if", "tokenizer1", ":", "\n", "                ", "tokens_b", "=", "check_token", "(", "tokenizer1", ".", "vocab", ",", "example", ".", "text_b", ".", "split", "(", "' '", ")", ")", "\n", "", "else", ":", "\n", "                ", "tokens_b", "=", "check_token", "(", "tokenizer", ".", "vocab", ",", "example", ".", "text_b", ".", "split", "(", "' '", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "if", "tokenizer1", ":", "\n", "                ", "tokens_b", "=", "tokenizer1", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "", "else", ":", "\n", "                ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "", "", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "", "if", "len", "(", "tokens_b", ")", ">", "seq_length", "-", "2", ":", "\n", "            ", "tokens_b", "=", "tokens_b", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "", "tokens_a_", "=", "[", "]", "\n", "input_lan_ids_a_", "=", "[", "]", "\n", "tokens_a_", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_lan_ids_a_", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "            ", "tokens_a_", ".", "append", "(", "token", ")", "\n", "input_lan_ids_a_", ".", "append", "(", "0", ")", "\n", "", "tokens_a_", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_lan_ids_a_", ".", "append", "(", "0", ")", "\n", "\n", "tokens_b_", "=", "[", "]", "\n", "input_lan_ids_b_", "=", "[", "]", "\n", "tokens_b_", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_lan_ids_b_", ".", "append", "(", "1", ")", "\n", "for", "token", "in", "tokens_b", ":", "\n", "            ", "tokens_b_", ".", "append", "(", "token", ")", "\n", "input_lan_ids_b_", ".", "append", "(", "1", ")", "\n", "", "tokens_b_", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_lan_ids_b_", ".", "append", "(", "1", ")", "\n", "\n", "input_ids_a_", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens_a_", ")", "\n", "if", "tokenizer1", ":", "\n", "            ", "input_ids_b_", "=", "tokenizer1", ".", "convert_tokens_to_ids", "(", "tokens_b_", ")", "\n", "", "else", ":", "\n", "            ", "input_ids_b_", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens_b_", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "", "input_mask_a_", "=", "[", "1", "]", "*", "len", "(", "input_ids_a_", ")", "\n", "input_mask_b_", "=", "[", "1", "]", "*", "len", "(", "input_ids_b_", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids_a_", ")", "<", "seq_length", ":", "\n", "            ", "input_ids_a_", ".", "append", "(", "0", ")", "\n", "input_mask_a_", ".", "append", "(", "0", ")", "\n", "input_lan_ids_a_", ".", "append", "(", "0", ")", "\n", "", "while", "len", "(", "input_ids_b_", ")", "<", "seq_length", ":", "\n", "            ", "input_ids_b_", ".", "append", "(", "0", ")", "\n", "input_mask_b_", ".", "append", "(", "0", ")", "\n", "input_lan_ids_b_", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids_a_", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask_a_", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_lan_ids_a_", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_ids_b_", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask_b_", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_lan_ids_b_", ")", "==", "seq_length", "\n", "\n", "# for alignment", "\n", "if", "aligns", ":", "\n", "            ", "align_ids_a", ",", "align_ids_b", "=", "aligns", "[", "ex_index", "]", "\n", "assert", "len", "(", "align_ids_a", ")", "==", "len", "(", "align_ids_b", ")", "\n", "align_mask", "=", "[", "1", "]", "*", "len", "(", "align_ids_a", ")", "\n", "while", "len", "(", "align_ids_a", ")", "<", "max_align", ":", "\n", "                ", "align_ids_a", ".", "append", "(", "0", ")", "\n", "align_ids_b", ".", "append", "(", "0", ")", "\n", "align_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "", "assert", "len", "(", "align_ids_a", ")", "==", "max_align", "\n", "assert", "len", "(", "align_ids_b", ")", "==", "max_align", "\n", "assert", "len", "(", "align_mask", ")", "==", "max_align", "\n", "\n", "if", "ex_index", "<", "2", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "example", ".", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens_a_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_a_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_a_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_lan_ids_a_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens_b_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_b_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_b_", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_lan_ids_b_", "]", ")", ")", "\n", "if", "aligns", ":", "\n", "                ", "logger", ".", "info", "(", "\"align_ids_a: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "align_ids_a", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"align_ids_b: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "align_ids_b", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"align_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "align_mask", "]", ")", ")", "\n", "", "", "if", "aligns", ":", "\n", "            ", "features", ".", "append", "(", "\n", "BiInputFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "[", "tokens_a_", ",", "tokens_b_", "]", ",", "\n", "input_ids", "=", "[", "input_ids_a_", ",", "input_ids_b_", "]", ",", "\n", "input_mask", "=", "[", "input_mask_a_", ",", "input_mask_b_", "]", ",", "\n", "input_lan_ids", "=", "[", "input_lan_ids_a_", ",", "input_lan_ids_b_", "]", ",", "\n", "align", "=", "[", "align_ids_a", ",", "align_ids_b", "]", ",", "\n", "align_mask", "=", "align_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "features", ".", "append", "(", "\n", "BiInputFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "[", "tokens_a_", ",", "tokens_b_", "]", ",", "\n", "input_ids", "=", "[", "input_ids_a_", ",", "input_ids_b_", "]", ",", "\n", "input_mask", "=", "[", "input_mask_a_", ",", "input_mask_b_", "]", ",", "\n", "input_lan_ids", "=", "[", "input_lan_ids_a_", ",", "input_lan_ids_b_", "]", ")", ")", "\n", "", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_bert_examples_to_features": [[259, 363], ["numpy.zeros", "numpy.zeros", "enumerate", "max", "logger.info", "len", "len", "load.check_token", "tokenizer.convert_tokens_to_ids", "numpy.stack", "numpy.stack", "len", "print", "load.check_token", "load.check_token", "len", "len", "tokenizer1.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "len", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask_a.append", "np.stack.append", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask_b.append", "np.stack.append", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "features.append", "features.append", "sum", "len", "len", "len", "len", "align_ids_a.append", "align_ids_b.append", "align_mask.append", "len", "len", "len", "logger.info", "logger.info", "logger.info", "load.BiInputBertFeatures", "load.BiInputBertFeatures", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.check_token", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids"], ["", "def", "convert_bert_examples_to_features", "(", "examples", ",", "seq_length", ",", "tokenizer", ",", "tokenizer1", "=", "None", ",", "aligns", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "if", "aligns", ":", "\n", "        ", "lens", "=", "[", "len", "(", "a", "[", "0", "]", ")", "for", "a", "in", "aligns", "]", "\n", "max_align", "=", "max", "(", "lens", ")", "\n", "logger", ".", "info", "(", "\"aligned word number: %s\"", "%", "(", "sum", "(", "lens", ")", ")", ")", "\n", "", "emb_pad_a", "=", "np", ".", "zeros", "(", "len", "(", "examples", "[", "0", "]", ".", "embs_a", "[", "0", "]", ")", ")", "\n", "emb_pad_b", "=", "np", ".", "zeros", "(", "len", "(", "examples", "[", "0", "]", ".", "embs_b", "[", "0", "]", ")", ")", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "ex_index", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"\\r%d\"", "%", "ex_index", ",", "end", "=", "\"\"", ")", "\n", "", "tokens_a", "=", "check_token", "(", "tokenizer", ".", "vocab", ",", "example", ".", "toks_a", ")", "\n", "if", "tokenizer1", ":", "\n", "            ", "tokens_b", "=", "check_token", "(", "tokenizer1", ".", "vocab", ",", "example", ".", "toks_b", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", "=", "check_token", "(", "tokenizer", ".", "vocab", ",", "example", ".", "toks_b", ")", "\n", "\n", "", "assert", "len", "(", "tokens_a", ")", "<=", "seq_length", "\n", "assert", "len", "(", "tokens_b", ")", "<=", "seq_length", "\n", "\n", "input_ids_a", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens_a", ")", "\n", "if", "tokenizer1", ":", "\n", "            ", "input_ids_b", "=", "tokenizer1", ".", "convert_tokens_to_ids", "(", "tokens_b", ")", "\n", "", "else", ":", "\n", "            ", "input_ids_b", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens_b", ")", "\n", "", "input_embs_a", "=", "example", ".", "embs_a", "\n", "input_embs_b", "=", "example", ".", "embs_b", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask_a", "=", "[", "1", "]", "*", "len", "(", "input_ids_a", ")", "\n", "input_mask_b", "=", "[", "1", "]", "*", "len", "(", "input_ids_b", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids_a", ")", "<", "seq_length", ":", "\n", "            ", "input_ids_a", ".", "append", "(", "0", ")", "\n", "input_mask_a", ".", "append", "(", "0", ")", "\n", "input_embs_a", ".", "append", "(", "emb_pad_a", ")", "\n", "", "while", "len", "(", "input_ids_b", ")", "<", "seq_length", ":", "\n", "            ", "input_ids_b", ".", "append", "(", "0", ")", "\n", "input_mask_b", ".", "append", "(", "0", ")", "\n", "input_embs_b", ".", "append", "(", "emb_pad_b", ")", "\n", "\n", "", "assert", "len", "(", "input_ids_a", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask_a", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_embs_a", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_ids_b", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask_b", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_embs_b", ")", "==", "seq_length", "\n", "\n", "input_embs_a", "=", "np", ".", "stack", "(", "input_embs_a", ",", "0", ")", "\n", "input_embs_b", "=", "np", ".", "stack", "(", "input_embs_b", ",", "0", ")", "\n", "\n", "# for alignment", "\n", "if", "aligns", ":", "\n", "            ", "align_ids_a", ",", "align_ids_b", "=", "aligns", "[", "ex_index", "]", "\n", "assert", "len", "(", "align_ids_a", ")", "==", "len", "(", "align_ids_b", ")", "\n", "align_mask", "=", "[", "1", "]", "*", "len", "(", "align_ids_a", ")", "\n", "while", "len", "(", "align_ids_a", ")", "<", "max_align", ":", "\n", "                ", "align_ids_a", ".", "append", "(", "0", ")", "\n", "align_ids_b", ".", "append", "(", "0", ")", "\n", "align_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "align_ids_a", ")", "==", "max_align", "\n", "assert", "len", "(", "align_ids_b", ")", "==", "max_align", "\n", "assert", "len", "(", "align_mask", ")", "==", "max_align", "\n", "\n", "", "if", "ex_index", "<", "2", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "example", ".", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens_a", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_a", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_a", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan0 input_emb {}: \\n{}\"", ".", "format", "(", "input_embs_a", ".", "shape", ",", "input_embs_a", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens_b", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_b", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_b", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"lan1 input_emb {}: \\n{}\"", ".", "format", "(", "input_embs_b", ".", "shape", ",", "input_embs_b", ")", ")", "\n", "\n", "if", "aligns", ":", "\n", "                ", "logger", ".", "info", "(", "\"align_ids_a: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "align_ids_a", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"align_ids_b: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "align_ids_b", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"align_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "align_mask", "]", ")", ")", "\n", "", "", "if", "aligns", ":", "\n", "            ", "features", ".", "append", "(", "\n", "BiInputBertFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "[", "tokens_a", ",", "tokens_b", "]", ",", "\n", "input_ids", "=", "[", "input_ids_a", ",", "input_ids_b", "]", ",", "\n", "input_mask", "=", "[", "input_mask_a", ",", "input_mask_b", "]", ",", "\n", "input_embs", "=", "[", "input_embs_a", ",", "input_embs_b", "]", ",", "\n", "align", "=", "[", "align_ids_a", ",", "align_ids_b", "]", ",", "\n", "align_mask", "=", "align_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "features", ".", "append", "(", "\n", "BiInputBertFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "[", "tokens_a", ",", "tokens_b", "]", ",", "\n", "input_ids", "=", "[", "input_ids_a", ",", "input_ids_b", "]", ",", "\n", "input_mask", "=", "[", "input_mask_a", ",", "input_mask_b", "]", ",", "\n", "input_embs", "=", "[", "input_embs_a", ",", "input_embs_b", "]", ")", ")", "\n", "", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_examples_to_features_single": [[364, 435], ["enumerate", "tokenizer.tokenize", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "tokenizer.tokenize", "load._truncate_seq_pair", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "input_type_ids.append", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "load.InputFeatures", "len", "tokens.append", "input_type_ids.append", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load._truncate_seq_pair"], ["", "def", "convert_examples_to_features_single", "(", "examples", ",", "seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "            ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "            ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "for", "token", "in", "tokens_b", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "if", "ex_index", "<", "1", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "example", ".", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_type_ids", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load._truncate_seq_pair": [[436, 451], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.read_examples": [[453, 475], ["open", "src.tokenization.convert_to_unicode", "line.strip.strip", "re.match", "examples.append", "reader.readline", "re.match.group", "re.match.group", "load.InputExample"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_to_unicode"], ["", "", "", "def", "read_examples", "(", "input_file", ")", ":", "\n", "    ", "\"\"\"Read a list of `InputExample`s from an input file.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "unique_id", "=", "0", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "line", "=", "tokenization", ".", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "line", ":", "\n", "                ", "break", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "text_a", "=", "None", "\n", "text_b", "=", "None", "\n", "m", "=", "re", ".", "match", "(", "r\"^(.*) \\|\\|\\| (.*)$\"", ",", "line", ")", "\n", "if", "m", "is", "None", ":", "\n", "                ", "text_a", "=", "line", "\n", "", "else", ":", "\n", "                ", "text_a", "=", "m", ".", "group", "(", "1", ")", "\n", "text_b", "=", "m", ".", "group", "(", "2", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "unique_id", "=", "unique_id", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ")", ")", "\n", "unique_id", "+=", "1", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_bert": [[476, 514], ["codecs.open", "codecs.open", "f_a.readline", "f_b.readline", "json.loads", "json.loads", "examples.append", "numpy.array", "len", "len", "numpy.array", "len", "len", "load.InputBertExample", "print"], "function", ["None"], ["", "def", "load_bert", "(", "input_file_a", ",", "input_file_b", ",", "n_max_sent", "=", "None", ")", ":", "\n", "    ", "\"\"\"Read a list of `InputExample`s from an input file.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "unique_id", "=", "0", "\n", "with", "codecs", ".", "open", "(", "input_file_a", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "f_a", ",", "codecs", ".", "open", "(", "input_file_b", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "f_b", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "line_a", "=", "f_a", ".", "readline", "(", ")", "\n", "line_b", "=", "f_b", ".", "readline", "(", ")", "\n", "if", "not", "line_a", "or", "not", "line_b", ":", "\n", "                ", "break", "\n", "# src bert", "\n", "", "bert_a", "=", "json", ".", "loads", "(", "line_a", ")", "\n", "#unique_id_a = int(bert_a[\"linex_index\"])", "\n", "bert_a", "=", "bert_a", "[", "\"features\"", "]", "\n", "embs_a", "=", "[", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "0", "]", "[", "\"values\"", "]", ")", "for", "item", "in", "bert_a", "]", "\n", "toks_a", "=", "[", "item", "[", "\"token\"", "]", "for", "item", "in", "bert_a", "]", "\n", "assert", "len", "(", "embs_a", ")", "==", "len", "(", "toks_a", ")", "\n", "# tgt bert", "\n", "bert_b", "=", "json", ".", "loads", "(", "line_b", ")", "\n", "#unique_id_b = int(bert_b[\"linex_index\"])", "\n", "bert_b", "=", "bert_b", "[", "\"features\"", "]", "\n", "embs_b", "=", "[", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "0", "]", "[", "\"values\"", "]", ")", "for", "item", "in", "bert_b", "]", "\n", "toks_b", "=", "[", "item", "[", "\"token\"", "]", "for", "item", "in", "bert_b", "]", "\n", "assert", "len", "(", "embs_b", ")", "==", "len", "(", "toks_b", ")", "\n", "#try:", "\n", "#    assert unique_id_a == unique_id_b ", "\n", "#except:", "\n", "#    print (\"BERT id mismatch: id_a:{}, id_b:{}\".format(unique_id_a, unique_id_b))", "\n", "#unique_id = unique_id_a", "\n", "examples", ".", "append", "(", "\n", "InputBertExample", "(", "unique_id", "=", "unique_id", ",", "embs_a", "=", "embs_a", ",", "toks_a", "=", "toks_a", ",", "embs_b", "=", "embs_b", ",", "toks_b", "=", "toks_b", ")", ")", "\n", "if", "unique_id", "%", "1000", "==", "0", ":", "\n", "                ", "print", "(", "\"\\r%d\"", "%", "unique_id", ",", "end", "=", "\"\"", ")", "\n", "", "if", "n_max_sent", "is", "not", "None", "and", "unique_id", ">=", "n_max_sent", "-", "1", ":", "\n", "                ", "break", "\n", "", "unique_id", "+=", "1", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_aligns": [[515, 607], ["open", "fi.readline", "logger.info", "len", "aligns.append", "fi.readline", "logger.info", "pair.split", "collections.Counter", "collections.Counter", "len", "src_ids.append", "tgt_ids.append", "fi.readline.strip().split", "ValueError", "int", "len", "rm_left.append", "rm_right.append", "align.append", "a2b[].sort", "align.append", "align_.append", "fi.readline.strip", "int", "a2b[].append", "b2a[].append", "len", "len", "int", "int", "int", "int", "int", "int", "len", "len", "int", "int", "int", "int"], "function", ["None"], ["", "def", "load_aligns", "(", "file", ",", "examples", "=", "None", ",", "n_max_sent", "=", "None", ",", "align_punc", "=", "False", ",", "policy", "=", "'1to1'", ")", ":", "\n", "    ", "\"\"\"Load the word-level alignment, only one-to-one word pairs are kept\"\"\"", "\n", "#maps, rev_maps = [], []", "\n", "puncs", "=", "string", ".", "punctuation", "\n", "aligns", "=", "[", "]", "\n", "n_alg", "=", "0", "\n", "n_alg_punc", "=", "0", "\n", "with", "open", "(", "file", ",", "'r'", ")", "as", "fi", ":", "\n", "        ", "line", "=", "fi", ".", "readline", "(", ")", "\n", "while", "line", ":", "\n", "            ", "pairs", "=", "[", "pair", ".", "split", "(", "'-'", ")", "for", "pair", "in", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "]", "\n", "align", "=", "[", "]", "\n", "if", "policy", "==", "'1to1'", ":", "\n", "# remove the one-to-many and many-to-one cases", "\n", "                ", "left", "=", "collections", ".", "Counter", "(", "[", "pair", "[", "0", "]", "for", "pair", "in", "pairs", "]", ")", "\n", "right", "=", "collections", ".", "Counter", "(", "[", "pair", "[", "1", "]", "for", "pair", "in", "pairs", "]", ")", "\n", "rm_left", "=", "[", "]", "\n", "rm_right", "=", "[", "]", "\n", "for", "l", "in", "left", ":", "\n", "                    ", "if", "left", "[", "l", "]", ">", "1", ":", "\n", "                        ", "rm_left", ".", "append", "(", "l", ")", "\n", "", "", "for", "r", "in", "right", ":", "\n", "                    ", "if", "right", "[", "r", "]", ">", "1", ":", "\n", "                        ", "rm_right", ".", "append", "(", "r", ")", "\n", "", "", "for", "pair", "in", "pairs", ":", "\n", "                    ", "if", "pair", "[", "0", "]", "not", "in", "rm_left", "and", "pair", "[", "1", "]", "not", "in", "rm_right", ":", "\n", "                        ", "align", ".", "append", "(", "pair", ")", "\n", "", "", "", "elif", "policy", "in", "[", "'first'", ",", "'last'", ",", "'mid'", "]", ":", "\n", "                ", "a2b", "=", "{", "}", "\n", "b2a", "=", "{", "}", "\n", "for", "pair", "in", "pairs", ":", "\n", "                    ", "if", "int", "(", "pair", "[", "0", "]", ")", "not", "in", "a2b", ":", "\n", "                        ", "a2b", "[", "int", "(", "pair", "[", "0", "]", ")", "]", "=", "[", "int", "(", "pair", "[", "1", "]", ")", "]", "\n", "", "else", ":", "\n", "                        ", "a2b", "[", "int", "(", "pair", "[", "0", "]", ")", "]", ".", "append", "(", "int", "(", "pair", "[", "1", "]", ")", ")", "\n", "", "", "for", "a", "in", "a2b", ":", "\n", "                    ", "a2b", "[", "a", "]", ".", "sort", "(", ")", "\n", "# rm 1-to-many situation", "\n", "if", "policy", "==", "'first'", ":", "\n", "                        ", "a2b", "[", "a", "]", "=", "a2b", "[", "a", "]", "[", "0", "]", "\n", "", "elif", "policy", "==", "'last'", ":", "\n", "                        ", "a2b", "[", "a", "]", "=", "a2b", "[", "a", "]", "[", "-", "1", "]", "\n", "", "elif", "policy", "==", "'mid'", ":", "\n", "                        ", "mid", "=", "int", "(", "len", "(", "a2b", "[", "a", "]", ")", "/", "2", ")", "\n", "a2b", "[", "a", "]", "=", "a2b", "[", "a", "]", "[", "mid", "]", "\n", "", "if", "a2b", "[", "a", "]", "not", "in", "b2a", ":", "\n", "                        ", "b2a", "[", "a2b", "[", "a", "]", "]", "=", "[", "a", "]", "\n", "", "else", ":", "\n", "                        ", "b2a", "[", "a2b", "[", "a", "]", "]", ".", "append", "(", "a", ")", "\n", "", "", "for", "b", "in", "b2a", ":", "\n", "# rm many-to-1 situation", "\n", "                    ", "if", "policy", "==", "'first'", ":", "\n", "                        ", "b2a", "[", "b", "]", "=", "b2a", "[", "b", "]", "[", "0", "]", "\n", "", "elif", "policy", "==", "'last'", ":", "\n", "                        ", "b2a", "[", "b", "]", "=", "b2a", "[", "b", "]", "[", "-", "1", "]", "\n", "", "elif", "policy", "==", "'mid'", ":", "\n", "                        ", "mid", "=", "int", "(", "len", "(", "b2a", "[", "b", "]", ")", "/", "2", ")", "\n", "b2a", "[", "b", "]", "=", "b2a", "[", "b", "]", "[", "mid", "]", "\n", "", "align", ".", "append", "(", "[", "b2a", "[", "b", "]", ",", "b", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Undefined alignment policy: {}\"", ".", "format", "(", "policy", ")", ")", "\n", "", "n_alg", "+=", "len", "(", "align", ")", "\n", "if", "align_punc", ":", "\n", "                    ", "align_", "=", "[", "]", "\n", "toks_a", "=", "examples", "[", "len", "(", "aligns", ")", "]", ".", "toks_a", "\n", "toks_b", "=", "examples", "[", "len", "(", "aligns", ")", "]", ".", "toks_b", "\n", "for", "pair", "in", "align", ":", "\n", "                        ", "if", "(", "toks_a", "[", "int", "(", "pair", "[", "0", "]", ")", "+", "1", "]", "in", "puncs", "and", "toks_b", "[", "int", "(", "pair", "[", "1", "]", ")", "+", "1", "]", "not", "in", "puncs", ")", "or", "(", "\n", "toks_a", "[", "int", "(", "pair", "[", "0", "]", ")", "+", "1", "]", "not", "in", "puncs", "and", "toks_b", "[", "int", "(", "pair", "[", "1", "]", ")", "+", "1", "]", "in", "puncs", ")", ":", "\n", "#print (len(aligns), pair[0], pair[1], toks_a[int(pair[0])+1], toks_b[int(pair[1])+1])", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "                            ", "align_", ".", "append", "(", "pair", ")", "\n", "", "", "align", "=", "align_", "\n", "n_alg_punc", "+=", "len", "(", "align", ")", "\n", "\n", "", "src_ids", ",", "tgt_ids", "=", "[", "]", ",", "[", "]", "\n", "for", "pair", "in", "align", ":", "\n", "                ", "src", ",", "tgt", "=", "[", "int", "(", "n", ")", "for", "n", "in", "pair", "]", "\n", "#src_ids.append(src)", "\n", "#tgt_ids.append(tgt)", "\n", "# add one to include [CLS]", "\n", "src_ids", ".", "append", "(", "src", "+", "1", ")", "\n", "tgt_ids", ".", "append", "(", "tgt", "+", "1", ")", "\n", "", "aligns", ".", "append", "(", "(", "src_ids", ",", "tgt_ids", ")", ")", "\n", "if", "n_max_sent", "and", "len", "(", "aligns", ")", ">=", "n_max_sent", ":", "\n", "                ", "break", "\n", "", "line", "=", "fi", ".", "readline", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Before align puncs: {} (policy: {})\"", ".", "format", "(", "n_alg", ",", "policy", ")", ")", "\n", "if", "align_punc", ":", "\n", "            ", "logger", ".", "info", "(", "\"After align puncs: {}\"", ".", "format", "(", "n_alg_punc", ")", ")", "\n", "", "", "return", "aligns", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load": [[608, 659], ["src.tokenization.FullTokenizer", "src.tokenization.FullTokenizer", "load.read_examples", "load.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "load.load_aligns", "torch.tensor.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "len", "len", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.read_examples", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_examples_to_features", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_aligns"], ["", "def", "load", "(", "vocab_file", ",", "input_file", ",", "batch_size", "=", "32", ",", "do_lower_case", "=", "True", ",", "\n", "max_seq_length", "=", "128", ",", "local_rank", "=", "-", "1", ",", "vocab_file1", "=", "None", ",", "\n", "align_file", "=", "None", ")", ":", "\n", "\n", "    ", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "tokenizer1", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file1", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "examples", "=", "read_examples", "(", "input_file", ")", "\n", "\n", "aligns", "=", "None", "\n", "if", "align_file", ":", "\n", "        ", "aligns", "=", "load_aligns", "(", "align_file", ")", "\n", "try", ":", "\n", "            ", "assert", "len", "(", "examples", ")", "==", "len", "(", "aligns", ")", "\n", "", "except", ":", "\n", "            ", "raise", "ValueError", "(", "\"Number of examples({}) and alignments({}) mismatch!\"", ".", "format", "(", "len", "(", "examples", ")", ",", "len", "(", "aligns", ")", ")", ")", "\n", "\n", "", "", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "seq_length", "=", "max_seq_length", ",", "tokenizer", "=", "tokenizer", ",", "\n", "tokenizer1", "=", "tokenizer1", ",", "aligns", "=", "aligns", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "", "all_input_ids_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids_a", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ids_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids_b", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask_a", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask_b", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids_a", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "if", "align_file", ":", "\n", "        ", "all_align_ids_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "align_ids_a", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_align_ids_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "align_ids_b", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_align_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "align_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "dataset", "=", "TensorDataset", "(", "all_input_ids_a", ",", "all_input_mask_a", ",", "\n", "all_input_ids_b", ",", "all_input_mask_b", ",", "all_align_ids_a", ",", "all_align_ids_b", ",", "\n", "all_align_mask", ",", "all_example_index", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "TensorDataset", "(", "all_input_ids_a", ",", "all_input_mask_a", ",", "\n", "all_input_ids_b", ",", "all_input_mask_b", ",", "all_example_index", ")", "\n", "#if local_rank == -1:", "\n", "#    sampler = SequentialSampler(dataset)", "\n", "#sampler = RandomSampler(dataset)", "\n", "#else:", "\n", "#    sampler = DistributedSampler(dataset)", "\n", "#dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)", "\n", "\n", "", "return", "dataset", ",", "unique_id_to_feature", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_bert": [[660, 714], ["src.tokenization.FullTokenizer", "src.tokenization.FullTokenizer", "load.load_bert", "load.convert_bert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "load.load_aligns", "torch.tensor.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "len", "len", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_bert_examples_to_features", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_aligns"], ["", "def", "load_from_bert", "(", "vocab_file", ",", "input_file_a", ",", "input_file_b", ",", "do_lower_case", "=", "True", ",", "\n", "max_seq_length", "=", "128", ",", "vocab_file1", "=", "None", ",", "align_file", "=", "None", ",", "n_max_sent", "=", "None", ",", "\n", "align_punc", "=", "False", ",", "policy", "=", "'1to1'", ")", ":", "\n", "\n", "    ", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "tokenizer1", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file1", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "examples", "=", "load_bert", "(", "input_file_a", ",", "input_file_b", ",", "n_max_sent", "=", "n_max_sent", ")", "\n", "\n", "aligns", "=", "None", "\n", "if", "align_file", ":", "\n", "        ", "aligns", "=", "load_aligns", "(", "align_file", ",", "n_max_sent", "=", "n_max_sent", ",", "examples", "=", "examples", ",", "\n", "align_punc", "=", "align_punc", ",", "policy", "=", "policy", ")", "\n", "try", ":", "\n", "            ", "assert", "len", "(", "examples", ")", "==", "len", "(", "aligns", ")", "\n", "", "except", ":", "\n", "            ", "raise", "ValueError", "(", "\"Number of examples({}) and alignments({}) mismatch!\"", ".", "format", "(", "len", "(", "examples", ")", ",", "len", "(", "aligns", ")", ")", ")", "\n", "\n", "", "", "features", "=", "convert_bert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "seq_length", "=", "max_seq_length", ",", "tokenizer", "=", "tokenizer", ",", "\n", "tokenizer1", "=", "tokenizer1", ",", "aligns", "=", "aligns", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "#all_input_ids_a = torch.tensor([f.input_ids_a for f in features], dtype=torch.long)", "\n", "#all_input_ids_b = torch.tensor([f.input_ids_b for f in features], dtype=torch.long)", "\n", "", "all_input_embs_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_embs_a", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "all_input_embs_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_embs_b", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "all_input_mask_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask_a", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask_b", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_mask_a", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "if", "align_file", ":", "\n", "        ", "all_align_ids_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "align_ids_a", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_align_ids_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "align_ids_b", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_align_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "align_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "dataset", "=", "TensorDataset", "(", "all_input_embs_a", ",", "all_input_mask_a", ",", "\n", "all_input_embs_b", ",", "all_input_mask_b", ",", "all_align_ids_a", ",", "all_align_ids_b", ",", "\n", "all_align_mask", ",", "all_example_index", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "TensorDataset", "(", "all_input_embs_a", ",", "all_input_mask_a", ",", "\n", "all_input_embs_b", ",", "all_input_mask_b", ",", "all_example_index", ")", "\n", "#if local_rank == -1:", "\n", "#    sampler = SequentialSampler(dataset)", "\n", "#sampler = RandomSampler(dataset)", "\n", "#else:", "\n", "#    sampler = DistributedSampler(dataset)", "\n", "#dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)", "\n", "\n", "", "return", "dataset", ",", "unique_id_to_feature", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_single": [[715, 737], ["src.tokenization.FullTokenizer", "load.read_examples", "load.convert_examples_to_features_single", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.tensor.size"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.read_examples", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_examples_to_features_single"], ["", "def", "load_single", "(", "vocab_file", ",", "input_file", ",", "batch_size", "=", "32", ",", "do_lower_case", "=", "True", ",", "\n", "max_seq_length", "=", "128", ",", "local_rank", "=", "-", "1", ")", ":", "\n", "\n", "    ", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "examples", "=", "read_examples", "(", "input_file", ")", "\n", "\n", "features", "=", "convert_examples_to_features_single", "(", "\n", "examples", "=", "examples", ",", "seq_length", "=", "max_seq_length", ",", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "dataset", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_example_index", ")", "\n", "\n", "return", "dataset", ",", "unique_id_to_feature", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_sents_to_features": [[738, 820], ["enumerate", "examples.append", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "w.lower", "load.InputExample", "tokenizer.wordpiece_tokenizer.tokenize", "tokenizer.tokenize", "load._truncate_seq_pair", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "input_type_ids.append", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "load.InputFeatures", "tokens_a.append", "len", "tokens.append", "input_type_ids.append", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load._truncate_seq_pair"], ["", "def", "convert_sents_to_features", "(", "sents", ",", "seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "examples", "=", "[", "]", "\n", "unique_id", "=", "0", "\n", "for", "sent", "in", "sents", ":", "\n", "        ", "sent", "=", "[", "w", ".", "lower", "(", ")", "for", "w", "in", "sent", "]", "\n", "examples", ".", "append", "(", "InputExample", "(", "unique_id", "=", "unique_id", ",", "text_a", "=", "sent", ",", "text_b", "=", "None", ")", ")", "\n", "unique_id", "+=", "1", "\n", "\n", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "#tokens_a = tokenizer.tokenize(example.text_a)", "\n", "        ", "tokens_a", "=", "[", "]", "\n", "for", "token", "in", "example", ".", "text_a", ":", "\n", "            ", "for", "sub_token", "in", "tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "tokens_a", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "            ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "            ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "for", "token", "in", "tokens_b", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "if", "ex_index", "<", "1", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "example", ".", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_type_ids", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert": [[821, 841], ["src.tokenization.FullTokenizer", "load.convert_sents_to_features", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.tensor.size"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_sents_to_features"], ["", "def", "convert", "(", "vocab_file", ",", "sents", ",", "batch_size", "=", "32", ",", "do_lower_case", "=", "True", ",", "\n", "max_seq_length", "=", "128", ",", "local_rank", "=", "-", "1", ")", ":", "\n", "\n", "    ", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "features", "=", "convert_sents_to_features", "(", "\n", "sents", "=", "sents", ",", "seq_length", "=", "max_seq_length", ",", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "dataset", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_example_index", ")", "\n", "\n", "return", "dataset", ",", "unique_id_to_feature", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_single_bert": [[842, 867], ["codecs.open", "f.readline", "json.loads", "int", "examples.append", "numpy.array", "len", "len", "load.InputSingleBertExample", "print"], "function", ["None"], ["", "def", "load_single_bert", "(", "input_file", ",", "n_max_sent", "=", "None", ")", ":", "\n", "    ", "\"\"\"Read a list of `InputExample`s from an input file.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "\n", "with", "codecs", ".", "open", "(", "input_file", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "line", "=", "f", ".", "readline", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "break", "\n", "# src bert", "\n", "", "bert", "=", "json", ".", "loads", "(", "line", ")", "\n", "unique_id", "=", "int", "(", "bert", "[", "\"linex_index\"", "]", ")", "\n", "bert", "=", "bert", "[", "\"features\"", "]", "\n", "embs", "=", "[", "np", ".", "array", "(", "item", "[", "\"layers\"", "]", "[", "0", "]", "[", "\"values\"", "]", ")", "for", "item", "in", "bert", "]", "\n", "toks", "=", "[", "item", "[", "\"token\"", "]", "for", "item", "in", "bert", "]", "\n", "assert", "len", "(", "embs", ")", "==", "len", "(", "toks", ")", "\n", "\n", "examples", ".", "append", "(", "\n", "InputSingleBertExample", "(", "unique_id", "=", "unique_id", ",", "embs", "=", "embs", ",", "toks", "=", "toks", ")", ")", "\n", "if", "unique_id", "%", "1000", "==", "0", ":", "\n", "                ", "print", "(", "\"\\r%d\"", "%", "unique_id", ",", "end", "=", "\"\"", ")", "\n", "", "if", "n_max_sent", "is", "not", "None", "and", "unique_id", ">=", "n_max_sent", "-", "1", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_bert_examples_to_features_single": [[868, 908], ["numpy.zeros", "enumerate", "len", "numpy.stack", "features.append", "print", "len", "len", "len", "input_mask.append", "np.stack.append", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "load.InputBertFeatures", "str", "str"], "function", ["None"], ["", "def", "convert_bert_examples_to_features_single", "(", "examples", ",", "seq_length", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "emb_pad", "=", "np", ".", "zeros", "(", "len", "(", "examples", "[", "0", "]", ".", "embs", "[", "0", "]", ")", ")", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "ex_index", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"\\r%d\"", "%", "ex_index", ",", "end", "=", "\"\"", ")", "\n", "", "tokens", "=", "example", ".", "toks", "\n", "assert", "len", "(", "tokens", ")", "<=", "seq_length", "\n", "\n", "input_embs", "=", "example", ".", "embs", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_embs", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_mask", ")", "<", "seq_length", ":", "\n", "            ", "input_mask", ".", "append", "(", "0", ")", "\n", "input_embs", ".", "append", "(", "emb_pad", ")", "\n", "", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_embs", ")", "==", "seq_length", "\n", "\n", "input_embs", "=", "np", ".", "stack", "(", "input_embs", ",", "0", ")", "\n", "\n", "if", "ex_index", "<", "2", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "example", ".", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_emb {}: \\n{}\"", ".", "format", "(", "input_embs", ".", "shape", ",", "input_embs", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputBertFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_embs", "=", "input_embs", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_from_single_bert": [[909, 927], ["load.load_single_bert", "load.convert_bert_examples_to_features_single", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.tensor.size"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load_single_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.convert_bert_examples_to_features_single"], ["", "def", "load_from_single_bert", "(", "bert_file", ",", "max_seq_length", "=", "128", ")", ":", "\n", "\n", "    ", "examples", "=", "load_single_bert", "(", "bert_file", ")", "\n", "#assert len(sents) == len(examples)", "\n", "\n", "features", "=", "convert_bert_examples_to_features_single", "(", "examples", ",", "max_seq_length", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "", "all_input_embs", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_embs", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_mask", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "dataset", "=", "TensorDataset", "(", "all_input_embs", ",", "all_input_mask", ",", "all_example_index", ")", "\n", "\n", "return", "dataset", ",", "unique_id_to_feature", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.logger.LogFormatter.__init__": [[15, 17], ["time.time"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.logger.LogFormatter.format": [[18, 29], ["round", "record.getMessage", "message.replace.replace.replace", "time.strftime", "datetime.timedelta", "len"], "methods", ["None"], ["", "def", "format", "(", "self", ",", "record", ")", ":", "\n", "        ", "elapsed_seconds", "=", "round", "(", "record", ".", "created", "-", "self", ".", "start_time", ")", "\n", "\n", "prefix", "=", "\"%s - %s - %s\"", "%", "(", "\n", "record", ".", "levelname", ",", "\n", "time", ".", "strftime", "(", "'%x %X'", ")", ",", "\n", "timedelta", "(", "seconds", "=", "elapsed_seconds", ")", "\n", ")", "\n", "message", "=", "record", ".", "getMessage", "(", ")", "\n", "message", "=", "message", ".", "replace", "(", "'\\n'", ",", "'\\n'", "+", "' '", "*", "(", "len", "(", "prefix", ")", "+", "3", ")", ")", "\n", "return", "\"%s - %s\"", "%", "(", "prefix", ",", "message", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.logger.create_logger": [[31, 63], ["logger.LogFormatter", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger", "logging.getLogger.setLevel", "logging.getLogger.addHandler", "logging.getLogger.addHandler", "time.time"], "function", ["None"], ["", "", "def", "create_logger", "(", "filepath", ",", "vb", "=", "2", ")", ":", "\n", "    ", "\"\"\"\n    Create a logger.\n    \"\"\"", "\n", "# create log formatter", "\n", "log_formatter", "=", "LogFormatter", "(", ")", "\n", "\n", "# create file handler and set level to debug", "\n", "file_handler", "=", "logging", ".", "FileHandler", "(", "filepath", ",", "\"a\"", ")", "\n", "file_handler", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "file_handler", ".", "setFormatter", "(", "log_formatter", ")", "\n", "\n", "# create console handler and set level to info", "\n", "log_level", "=", "logging", ".", "DEBUG", "if", "vb", "==", "2", "else", "logging", ".", "INFO", "if", "vb", "==", "1", "else", "logging", ".", "WARNING", "\n", "console_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console_handler", ".", "setLevel", "(", "log_level", ")", "\n", "console_handler", ".", "setFormatter", "(", "log_formatter", ")", "\n", "\n", "# create logger and set level to debug", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "handlers", "=", "[", "]", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "logger", ".", "addHandler", "(", "console_handler", ")", "\n", "\n", "# reset logger elapsed time", "\n", "def", "reset_time", "(", ")", ":", "\n", "        ", "log_formatter", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "", "logger", ".", "reset_time", "=", "reset_time", "\n", "\n", "return", "logger", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.SubsetSampler.__init__": [[26, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "indices", "=", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.SubsetSampler.__iter__": [[29, 31], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.SubsetSampler.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.__init__": [[98, 134], ["bert_evaluator.SubsetSampler", "torch.utils.data.DataLoader", "logger.info", "torch.device", "torch.device", "bert_evaluator.load_stop_words", "bert_evaluator.load_stop_words", "len", "range", "bert_evaluator.BertEvaluator.get_nonpara_loader", "len", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.load_stop_words", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.load_stop_words", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_nonpara_loader"], ["    ", "def", "__init__", "(", "self", ",", "bert_model", ",", "dataset", ",", "mapping", ",", "discriminator", ",", "\n", "args", ",", "features", ",", "bert_model1", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initialize evaluator.\n        \"\"\"", "\n", "self", ".", "bert_model", "=", "bert_model", "\n", "self", ".", "bert_model1", "=", "bert_model1", "\n", "self", ".", "mapping", "=", "mapping", "\n", "self", ".", "discriminator", "=", "discriminator", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "features", "=", "features", "\n", "self", ".", "dev_sent_num", "=", "self", ".", "args", ".", "dev_sent_num", "\n", "if", "self", ".", "args", ".", "adversarial", ":", "\n", "            ", "assert", "self", ".", "dev_sent_num", "<=", "len", "(", "self", ".", "dataset", ")", "\n", "dev_sampler", "=", "SubsetSampler", "(", "range", "(", "self", ".", "dev_sent_num", ")", ")", "\n", "self", ".", "dev_loader", "=", "DataLoader", "(", "self", ".", "dataset", ",", "sampler", "=", "dev_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "logger", ".", "info", "(", "\"### Development sentence number: {} ###\"", ".", "format", "(", "len", "(", "dev_sampler", ")", ")", ")", "\n", "if", "self", ".", "args", ".", "eval_non_parallel", ":", "\n", "                ", "self", ".", "nonpara_loader", "=", "self", ".", "get_nonpara_loader", "(", ")", "\n", "#dis_sampler = SequentialSampler(self.dataset)", "\n", "#self.dis_loader = DataLoader(self.dataset, sampler=dis_sampler, batch_size=self.args.batch_size)", "\n", "\n", "", "", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", "or", "self", ".", "args", ".", "no_cuda", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "self", ".", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "self", ".", "args", ".", "local_rank", ")", "\n", "# \"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"", "\n", "", "self", ".", "punc", "=", "\"\"", "\n", "self", ".", "stop_words_a", "=", "\"\"", "\n", "self", ".", "stop_words_b", "=", "\"\"", "\n", "if", "self", ".", "args", ".", "rm_punc", ":", "\n", "            ", "self", ".", "punc", "=", "string", ".", "punctuation", "\n", "", "if", "self", ".", "args", ".", "rm_stop_words", ":", "\n", "            ", "self", ".", "stop_words_a", "=", "load_stop_words", "(", "self", ".", "args", ".", "stop_words_src", ")", "\n", "self", ".", "stop_words_b", "=", "load_stop_words", "(", "self", ".", "args", ".", "stop_words_tgt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_nonpara_loader": [[135, 160], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.cat", "torch.cat", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.tensor.size"], "methods", ["None"], ["", "", "def", "get_nonpara_loader", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Get nonparaed loader for penalty sent sim\n        \"\"\"", "\n", "num", "=", "self", ".", "dev_sent_num", "\n", "all_input_ids_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids_a", "for", "f", "in", "self", ".", "features", "[", ":", "num", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ids_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids_b", "for", "f", "in", "self", ".", "features", "[", ":", "num", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_a", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask_a", "for", "f", "in", "self", ".", "features", "[", ":", "num", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_b", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask_b", "for", "f", "in", "self", ".", "features", "[", ":", "num", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids_a", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "# move each b sentence to the previous a, move the first b to the last", "\n", "ids_b_0", "=", "all_input_ids_b", "[", ":", "1", "]", "\n", "all_input_ids_b_", "=", "all_input_ids_b", "[", "1", ":", "]", "\n", "all_input_ids_b", "=", "torch", ".", "cat", "(", "[", "all_input_ids_b_", ",", "ids_b_0", "]", ",", "0", ")", "\n", "mask_b_0", "=", "all_input_mask_b", "[", ":", "1", "]", "\n", "all_input_mask_b_", "=", "all_input_mask_b", "[", "1", ":", "]", "\n", "all_input_mask_b", "=", "torch", ".", "cat", "(", "[", "all_input_mask_b_", ",", "mask_b_0", "]", ",", "0", ")", "\n", "\n", "nonpara_dataset", "=", "TensorDataset", "(", "all_input_ids_a", ",", "all_input_mask_a", ",", "\n", "all_input_ids_b", ",", "all_input_mask_b", ",", "all_example_index", ")", "\n", "nonpara_sampler", "=", "SequentialSampler", "(", "nonpara_dataset", ")", "\n", "nonpara_loader", "=", "DataLoader", "(", "nonpara_dataset", ",", "sampler", "=", "nonpara_sampler", ",", "\n", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "return", "nonpara_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert": [[161, 176], ["torch.no_grad", "bert_evaluator.BertEvaluator.bert_model.eval", "bert_evaluator.BertEvaluator.bert_model", "bert_evaluator.BertEvaluator.bert_model1.eval", "bert_evaluator.BertEvaluator.bert_model1"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "def", "get_bert", "(", "self", ",", "input_ids", ",", "input_mask", ",", "bert_layer", "=", "-", "1", ",", "model_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get BERT\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "model_id", "==", "0", "or", "self", ".", "bert_model1", "is", "None", ":", "\n", "                ", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "bert_model1", ".", "eval", "(", ")", "\n", "all_encoder_layers", ",", "_", "=", "self", ".", "bert_model1", "(", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask", ")", "\n", "", "encoder_layer", "=", "all_encoder_layers", "[", "bert_layer", "]", "\n", "\n", "# [batch_size, seq_len, output_dim]", "\n", "", "return", "encoder_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select": [[177, 183], ["list", "embed.masked_select().view", "embed.size", "embed.masked_select", "mask.byte().view().expand", "mask.byte().view", "mask.byte"], "methods", ["None"], ["", "def", "select", "(", "self", ",", "embed", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        Select all unmasked embed in this batch \n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "emb_dim", "=", "list", "(", "embed", ".", "size", "(", ")", ")", "\n", "return", "embed", ".", "masked_select", "(", "mask", ".", "byte", "(", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "emb_dim", ")", ")", ".", "view", "(", "-", "1", ",", "emb_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_sim": [[184, 190], ["bert_evaluator.BertEvaluator.parallel_sim", "bert_evaluator.BertEvaluator.nonpara_sim"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.parallel_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.nonpara_sim"], ["", "def", "eval_sim", "(", "self", ")", ":", "\n", "\n", "        ", "metrics", "=", "{", "\"para_sim\"", ":", "self", ".", "parallel_sim", "(", ")", "}", "\n", "if", "self", ".", "args", ".", "eval_non_parallel", ":", "\n", "            ", "metrics", "[", "\"nonpara_sim\"", "]", "=", "self", ".", "nonpara_sim", "(", ")", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.parallel_sim": [[191, 194], ["bert_evaluator.BertEvaluator.sent_sim"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.sent_sim"], ["", "def", "parallel_sim", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "sent_sim", "(", "self", ".", "dev_loader", ",", "\"parallel\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.nonpara_sim": [[195, 198], ["bert_evaluator.BertEvaluator.sent_sim"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.sent_sim"], ["", "def", "nonpara_sim", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "sent_sim", "(", "self", ".", "nonpara_loader", ",", "\"non_parallel\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.sent_sim": [[199, 237], ["open", "numpy.mean", "logger.info", "bert_evaluator.BertEvaluator.get_bert", "bert_evaluator.BertEvaluator.get_bert().data.cpu().numpy", "bert_evaluator.BertEvaluator.mapping().data.cpu().numpy", "enumerate", "input_ids_a.to", "input_mask_a.to", "numpy.sum", "numpy.sum", "similarities.append", "open.write", "bert_evaluator.BertEvaluator.get_bert().data.cpu", "bert_evaluator.BertEvaluator.mapping().data.cpu", "input_mask_a[].data.cpu().numpy", "input_mask_b[].data.cpu().numpy", "bert_evaluator.rm_stop_words", "bert_evaluator.rm_stop_words", "bert_evaluator.cos_sim", "example_index.item", "len", "len", "numpy.mean", "numpy.mean", "example_index.item", "input_mask_a[].data.cpu", "input_mask_b[].data.cpu", "bert_evaluator.BertEvaluator.get_bert", "bert_evaluator.BertEvaluator.mapping", "example_index.item", "input_ids_b.to", "input_mask_b.to", "str"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.rm_stop_words", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.rm_stop_words", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.cos_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert"], ["", "def", "sent_sim", "(", "self", ",", "loader", ",", "type", ")", ":", "\n", "        ", "\"\"\"\n        Run all evaluations.\n        \"\"\"", "\n", "similarities", "=", "[", "]", "\n", "fo", "=", "open", "(", "self", ".", "args", ".", "model_path", "+", "'/'", "+", "type", "+", "\"_similarities.txt\"", ",", "'w'", ")", "\n", "for", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "example_indices", "in", "loader", ":", "\n", "\n", "            ", "src_bert", "=", "self", ".", "get_bert", "(", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "#.data.cpu().numpy()", "\n", "tgt_bert", "=", "self", ".", "get_bert", "(", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "src_bert", "=", "self", ".", "mapping", "(", "src_bert", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "i", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                ", "feature", "=", "self", ".", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "tokens_a", "=", "feature", ".", "tokens_a", "\n", "if", "type", "==", "\"parallel\"", ":", "\n", "                    ", "tokens_b", "=", "feature", ".", "tokens_b", "\n", "", "else", ":", "\n", "                    ", "real_id", "=", "example_index", ".", "item", "(", ")", "+", "1", "if", "example_index", ".", "item", "(", ")", "+", "1", "<", "self", ".", "dev_sent_num", "else", "0", "\n", "tokens_b", "=", "self", ".", "features", "[", "real_id", "]", ".", "tokens_b", "\n", "", "seq_len_a", "=", "np", ".", "sum", "(", "input_mask_a", "[", "i", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "seq_len_b", "=", "np", ".", "sum", "(", "input_mask_b", "[", "i", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "# [seq_len, output_dim]", "\n", "src_emb", "=", "src_bert", "[", "i", "]", "[", "1", ":", "seq_len_a", "-", "1", "]", "\n", "tgt_emb", "=", "tgt_bert", "[", "i", "]", "[", "1", ":", "seq_len_b", "-", "1", "]", "\n", "if", "self", ".", "args", ".", "rm_stop_words", "or", "self", ".", "args", ".", "rm_punc", ":", "\n", "                    ", "src_toks", ",", "src_emb", "=", "rm_stop_words", "(", "tokens_a", "[", "1", ":", "-", "1", "]", ",", "src_emb", ",", "self", ".", "stop_words_a", ",", "self", ".", "punc", ")", "\n", "tgt_toks", ",", "tgt_emb", "=", "rm_stop_words", "(", "tokens_b", "[", "1", ":", "-", "1", "]", ",", "tgt_emb", ",", "self", ".", "stop_words_b", ",", "self", ".", "punc", ")", "\n", "", "if", "len", "(", "src_emb", ")", "==", "0", "or", "len", "(", "tgt_emb", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "similarities", ".", "append", "(", "cos_sim", "(", "np", ".", "mean", "(", "src_emb", ",", "0", ")", ",", "np", ".", "mean", "(", "tgt_emb", ",", "0", ")", ")", ")", "\n", "fo", ".", "write", "(", "'sim:'", "+", "str", "(", "similarities", "[", "-", "1", "]", ")", "+", "'\\n'", "+", "' '", ".", "join", "(", "tokens_a", ")", "+", "' ||| '", "+", "' '", ".", "join", "(", "tokens_b", ")", "+", "'\\n'", ")", "\n", "#print (\"sent sim:\", similarities)", "\n", "", "", "sim_mean", "=", "np", ".", "mean", "(", "similarities", ")", "\n", "logger", ".", "info", "(", "\"Mean {} sentence similarity: {:.2f}% \"", ".", "format", "(", "type", ",", "sim_mean", "*", "100", ")", ")", "\n", "\n", "return", "sim_mean", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_dis": [[238, 265], ["bert_evaluator.BertEvaluator.discriminator.eval", "numpy.mean", "numpy.mean", "logger.info", "numpy.mean", "numpy.mean", "logger.info", "bert_evaluator.BertEvaluator.get_bert", "bert_evaluator.BertEvaluator.get_bert", "src_preds.extend", "tgt_preds.extend", "input_ids_a.to", "input_mask_a.to", "input_ids_b.to", "input_mask_b.to", "bert_evaluator.BertEvaluator.discriminator().data.cpu().tolist", "bert_evaluator.BertEvaluator.discriminator().data.cpu().tolist", "len", "len", "len", "len", "bert_evaluator.BertEvaluator.discriminator().data.cpu", "bert_evaluator.BertEvaluator.discriminator().data.cpu", "bert_evaluator.BertEvaluator.discriminator", "bert_evaluator.BertEvaluator.discriminator", "bert_evaluator.BertEvaluator.mapping", "bert_evaluator.BertEvaluator.select", "bert_evaluator.BertEvaluator.select", "input_mask_b.to", "input_mask_a.to"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select"], ["", "def", "eval_dis", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate discriminator predictions and accuracy.\n        \"\"\"", "\n", "src_preds", "=", "[", "]", "\n", "tgt_preds", "=", "[", "]", "\n", "\n", "self", ".", "discriminator", ".", "eval", "(", ")", "\n", "\n", "for", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "example_indices", "in", "self", ".", "dis_loader", ":", "\n", "\n", "            ", "src_bert", "=", "self", ".", "get_bert", "(", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "\n", "tgt_bert", "=", "self", ".", "get_bert", "(", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", "\n", "src_preds", ".", "extend", "(", "self", ".", "discriminator", "(", "self", ".", "mapping", "(", "self", ".", "select", "(", "src_bert", ",", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", ")", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", "\n", "tgt_preds", ".", "extend", "(", "self", ".", "discriminator", "(", "self", ".", "select", "(", "tgt_bert", ",", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "src_pred", "=", "np", ".", "mean", "(", "src_preds", ")", "\n", "tgt_pred", "=", "np", ".", "mean", "(", "tgt_preds", ")", "\n", "logger", ".", "info", "(", "\"Discriminator source / target predictions: {:.2f}% / {:.2f}%\"", ".", "format", "(", "src_pred", "*", "100", ",", "tgt_pred", "*", "100", ")", ")", "\n", "\n", "src_acc", "=", "np", ".", "mean", "(", "[", "x", ">=", "0.5", "for", "x", "in", "src_preds", "]", ")", "\n", "tgt_acc", "=", "np", ".", "mean", "(", "[", "x", "<", "0.5", "for", "x", "in", "tgt_preds", "]", ")", "\n", "dis_acc", "=", "(", "(", "src_acc", "*", "len", "(", "src_preds", ")", "+", "tgt_acc", "*", "len", "(", "tgt_preds", ")", ")", "/", "\n", "(", "len", "(", "src_preds", ")", "+", "len", "(", "tgt_preds", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Discriminator source / target / global accuracy: {:.2f}% / {:.2f}% / {:.2f}%\"", ".", "format", "(", "src_acc", "*", "100", ",", "tgt_acc", "*", "100", ",", "dis_acc", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.eval_dev_dis": [[270, 295], ["bert_evaluator.BertEvaluator.discriminator.eval", "numpy.mean", "numpy.mean", "logger.info", "numpy.mean", "numpy.mean", "logger.info", "bert_evaluator.BertEvaluator.get_bert", "bert_evaluator.BertEvaluator.get_bert", "src_preds.extend", "tgt_preds.extend", "input_ids_a.to", "input_mask_a.to", "input_ids_b.to", "input_mask_b.to", "bert_evaluator.BertEvaluator.discriminator().data.cpu().tolist", "bert_evaluator.BertEvaluator.discriminator().data.cpu().tolist", "len", "len", "len", "len", "bert_evaluator.BertEvaluator.discriminator().data.cpu", "bert_evaluator.BertEvaluator.discriminator().data.cpu", "bert_evaluator.BertEvaluator.discriminator", "bert_evaluator.BertEvaluator.discriminator", "bert_evaluator.BertEvaluator.mapping", "bert_evaluator.BertEvaluator.select", "bert_evaluator.BertEvaluator.select", "input_mask_b.to", "input_mask_a.to"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.select"], ["", "def", "eval_dev_dis", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate discriminator predictions and accuracy.\n        \"\"\"", "\n", "src_preds", "=", "[", "]", "\n", "tgt_preds", "=", "[", "]", "\n", "\n", "self", ".", "discriminator", ".", "eval", "(", ")", "\n", "for", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "example_indices", "in", "self", ".", "dev_loader", ":", "\n", "            ", "src_bert", "=", "self", ".", "get_bert", "(", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "\n", "tgt_bert", "=", "self", ".", "get_bert", "(", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", ",", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", "\n", "src_preds", ".", "extend", "(", "self", ".", "discriminator", "(", "self", ".", "mapping", "(", "self", ".", "select", "(", "src_bert", ",", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", ")", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", "\n", "tgt_preds", ".", "extend", "(", "self", ".", "discriminator", "(", "self", ".", "select", "(", "tgt_bert", ",", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "src_pred", "=", "np", ".", "mean", "(", "src_preds", ")", "\n", "tgt_pred", "=", "np", ".", "mean", "(", "tgt_preds", ")", "\n", "logger", ".", "info", "(", "\"Discriminator source / target predictions: {:.2f}% / {:.2f}%\"", ".", "format", "(", "src_pred", "*", "100", ",", "tgt_pred", "*", "100", ")", ")", "\n", "\n", "src_acc", "=", "np", ".", "mean", "(", "[", "x", ">=", "0.5", "for", "x", "in", "src_preds", "]", ")", "\n", "tgt_acc", "=", "np", ".", "mean", "(", "[", "x", "<", "0.5", "for", "x", "in", "tgt_preds", "]", ")", "\n", "dis_acc", "=", "(", "(", "src_acc", "*", "len", "(", "src_preds", ")", "+", "tgt_acc", "*", "len", "(", "tgt_preds", ")", ")", "/", "\n", "(", "len", "(", "src_preds", ")", "+", "len", "(", "tgt_preds", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Discriminator source / target / global accuracy: {:.2f}% / {:.2f}% / {:.2f}%\"", ".", "format", "(", "src_acc", "*", "100", ",", "tgt_acc", "*", "100", ",", "dis_acc", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.calculate_sim": [[296, 361], ["print", "src.bert_trainer.reload_model", "open", "numpy.mean", "fo.write", "input_ids_a.to.to.to", "input_mask_a.to.to.to", "input_ids_b.to.to.to", "input_mask_b.to.to.to", "bert_evaluator.BertEvaluator.data.cpu().numpy", "enumerate", "len", "bert_evaluator.BertEvaluator.bert_model.module.embeddings", "bert_evaluator.BertEvaluator.bert_model1.module.embeddings().data.cpu().numpy", "bert_evaluator.BertEvaluator.get_bert", "bert_evaluator.BertEvaluator.get_bert().data.cpu().numpy", "bert_evaluator.BertEvaluator.mapping", "numpy.sum", "numpy.sum", "len", "bert_evaluator.BertEvaluator.data.cpu", "print", "input_mask_a[].data.cpu().numpy", "input_mask_b[].data.cpu().numpy", "bert_evaluator.rm_stop_words", "bert_evaluator.rm_stop_words", "bert_evaluator.get_overlaps", "bert_evaluator.get_overlap_sim", "similarities.extend", "fo.write", "similarities.append", "fo.write", "bert_evaluator.BertEvaluator.bert_model1.module.embeddings().data.cpu", "bert_evaluator.BertEvaluator.get_bert().data.cpu", "example_index.item", "bert_evaluator.cos_sim", "input_mask_a[].data.cpu", "input_mask_b[].data.cpu", "len", "len", "numpy.mean", "numpy.mean", "bert_evaluator.BertEvaluator.bert_model1.module.embeddings", "bert_evaluator.BertEvaluator.get_bert", "str"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_trainer.reload_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.rm_stop_words", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.rm_stop_words", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.get_overlaps", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.get_overlap_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.cos_sim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.get_bert"], ["", "def", "calculate_sim", "(", "self", ",", "loader", ")", ":", "\n", "        ", "\"\"\"\n        Calculate similarities\n        \"\"\"", "\n", "\n", "#print ('----> Calculate Sentence Similarity <----\\n\\n')", "\n", "\n", "n_sent", "=", "0", "\n", "if", "self", ".", "args", ".", "sim_with_map", ":", "\n", "            ", "reload_model", "(", "self", ".", "mapping", ",", "self", ".", "args", ".", "model_path", ")", "\n", "", "outfile", "=", "self", ".", "args", ".", "sim_file", "if", "self", ".", "args", ".", "sim_file", "else", "'similarities.txt'", "\n", "similarities", "=", "[", "]", "\n", "with", "open", "(", "outfile", ",", "'w'", ")", "as", "fo", ":", "\n", "            ", "for", "input_ids_a", ",", "input_mask_a", ",", "input_ids_b", ",", "input_mask_b", ",", "example_indices", "in", "loader", ":", "\n", "                ", "input_ids_a", "=", "input_ids_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_a", "=", "input_mask_a", ".", "to", "(", "self", ".", "device", ")", "\n", "input_ids_b", "=", "input_ids_b", ".", "to", "(", "self", ".", "device", ")", "\n", "input_mask_b", "=", "input_mask_b", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "args", ".", "base_embed", ":", "\n", "                    ", "src_bert", "=", "self", ".", "bert_model", ".", "module", ".", "embeddings", "(", "input_ids_a", ",", "None", ")", "\n", "tgt_bert", "=", "self", ".", "bert_model1", ".", "module", ".", "embeddings", "(", "input_ids_b", ",", "None", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                    ", "src_bert", "=", "self", ".", "get_bert", "(", "input_ids_a", ",", "input_mask_a", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "0", ")", "\n", "tgt_bert", "=", "self", ".", "get_bert", "(", "input_ids_b", ",", "input_mask_b", ",", "\n", "bert_layer", "=", "self", ".", "args", ".", "bert_layer", ",", "model_id", "=", "1", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "if", "self", ".", "args", ".", "sim_with_map", ":", "\n", "                    ", "src_bert", "=", "self", ".", "mapping", "(", "src_bert", ")", "\n", "", "src_bert", "=", "src_bert", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "for", "i", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "                    ", "n_sent", "+=", "1", "\n", "if", "n_sent", "%", "1000", "==", "0", ":", "\n", "                        ", "print", "(", "\"\\r{}\"", ".", "format", "(", "n_sent", ")", ",", "end", "=", "\"\"", ")", "\n", "", "feature", "=", "self", ".", "features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "seq_len_a", "=", "np", ".", "sum", "(", "input_mask_a", "[", "i", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "seq_len_b", "=", "np", ".", "sum", "(", "input_mask_b", "[", "i", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "# [seq_len, output_dim]", "\n", "src_emb", "=", "src_bert", "[", "i", "]", "[", "1", ":", "seq_len_a", "-", "1", "]", "\n", "tgt_emb", "=", "tgt_bert", "[", "i", "]", "[", "1", ":", "seq_len_b", "-", "1", "]", "\n", "if", "self", ".", "args", ".", "rm_stop_words", "or", "self", ".", "args", ".", "rm_punc", ":", "\n", "                        ", "src_toks", ",", "src_emb", "=", "rm_stop_words", "(", "feature", ".", "tokens_a", "[", "1", ":", "-", "1", "]", ",", "src_emb", ",", "self", ".", "stop_words_a", ",", "self", ".", "punc", ")", "\n", "tgt_toks", ",", "tgt_emb", "=", "rm_stop_words", "(", "feature", ".", "tokens_b", "[", "1", ":", "-", "1", "]", ",", "tgt_emb", ",", "self", ".", "stop_words_b", ",", "self", ".", "punc", ")", "\n", "", "else", ":", "\n", "                        ", "src_toks", "=", "feature", ".", "tokens_a", "[", "1", ":", "-", "1", "]", "\n", "tgt_toks", "=", "feature", ".", "tokens_b", "[", "1", ":", "-", "1", "]", "\n", "# calculate overlap token sim", "\n", "", "if", "self", ".", "args", ".", "overlap_sim", ":", "\n", "                        ", "overlaps", "=", "get_overlaps", "(", "src_toks", ",", "src_emb", ",", "tgt_toks", ",", "tgt_emb", ")", "\n", "if", "not", "overlaps", ":", "\n", "                            ", "continue", "\n", "", "sims", ",", "infos", "=", "get_overlap_sim", "(", "overlaps", ")", "\n", "similarities", ".", "extend", "(", "[", "s", "[", "'sim'", "]", "for", "s", "in", "sims", "]", ")", "\n", "fo", ".", "write", "(", "' | '", ".", "join", "(", "infos", ")", "+", "'\\n'", "+", "' '", ".", "join", "(", "src_toks", ")", "+", "' ||| '", "+", "' '", ".", "join", "(", "tgt_toks", ")", "+", "'\\n'", ")", "\n", "# calculate sent sim", "\n", "", "else", ":", "\n", "                        ", "if", "len", "(", "src_emb", ")", "==", "0", "or", "len", "(", "tgt_emb", ")", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "similarities", ".", "append", "(", "cos_sim", "(", "np", ".", "mean", "(", "src_emb", ",", "0", ")", ",", "np", ".", "mean", "(", "tgt_emb", ",", "0", ")", ")", ")", "\n", "fo", ".", "write", "(", "'sim:'", "+", "str", "(", "similarities", "[", "-", "1", "]", ")", "+", "'\\n'", "+", "' '", ".", "join", "(", "feature", ".", "tokens_a", ")", "+", "' ||| '", "+", "' '", ".", "join", "(", "feature", ".", "tokens_b", ")", "+", "'\\n'", ")", "\n", "", "", "", "sim_mean", "=", "np", ".", "mean", "(", "similarities", ")", "\n", "fo", ".", "write", "(", "\"Mean similarity: {:.2f}% , Number: {}\"", ".", "format", "(", "sim_mean", "*", "100", ",", "len", "(", "similarities", ")", ")", ")", "\n", "", "print", "(", "\"Mean similarity: {:.2f}% , Number: {} \"", ".", "format", "(", "sim_mean", "*", "100", ",", "len", "(", "similarities", ")", ")", ")", "\n", "return", "sim_mean", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.BertEvaluator.dist_mean_cosine": [[362, 396], ["copy.deepcopy", "get_candidates", "get_candidates", "build_dictionary", "logger.info", "bert_evaluator.BertEvaluator.mapping", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "isinstance", "mean_cosine.item", "src_emb.norm", "tgt_emb.norm"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.build_dictionary"], ["", "def", "dist_mean_cosine", "(", "self", ",", "src_emb", ",", "tgt_emb", ",", "dico_method", "=", "'nn'", ",", "dico_build", "=", "'S2T'", ",", "dico_max_size", "=", "10000", ")", ":", "\n", "        ", "\"\"\"\n        Mean-cosine model selection criterion.\n        \"\"\"", "\n", "# get normalized embeddings", "\n", "src_emb", "=", "self", ".", "mapping", "(", "src_emb", ")", ".", "data", "\n", "tgt_emb", "=", "tgt_emb", ".", "data", "\n", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "\n", "# build dictionary", "\n", "# temp params / dictionary generation", "\n", "_args", "=", "deepcopy", "(", "self", ".", "args", ")", "\n", "_args", ".", "dico_method", "=", "dico_method", "\n", "_args", ".", "dico_build", "=", "dico_build", "\n", "_args", ".", "dico_threshold", "=", "0", "\n", "_args", ".", "dico_max_rank", "=", "10000", "\n", "_args", ".", "dico_min_size", "=", "0", "\n", "_args", ".", "dico_max_size", "=", "dico_max_size", "\n", "s2t_candidates", ",", "s2t_scores", "=", "get_candidates", "(", "src_emb", ",", "tgt_emb", ",", "_args", ")", "\n", "t2s_candidates", ",", "t2s_scores", "=", "get_candidates", "(", "tgt_emb", ",", "src_emb", ",", "_args", ")", "\n", "#print ('S2T:\\n',s2t_candidates, s2t_scores)", "\n", "#print ('T2S:\\n',t2s_candidates, t2s_scores)", "\n", "dico", "=", "build_dictionary", "(", "src_emb", ",", "tgt_emb", ",", "_args", ",", "s2t_candidates", ",", "t2s_candidates", ")", "\n", "#print ('Dico:\\n', dico)", "\n", "# mean cosine", "\n", "if", "dico", "is", "None", ":", "\n", "            ", "mean_cosine", "=", "-", "1e9", "\n", "", "else", ":", "\n", "            ", "mean_cosine", "=", "(", "src_emb", "[", "dico", "[", ":", "dico_max_size", ",", "0", "]", "]", "*", "tgt_emb", "[", "dico", "[", ":", "dico_max_size", ",", "1", "]", "]", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "", "mean_cosine", "=", "mean_cosine", ".", "item", "(", ")", "if", "isinstance", "(", "mean_cosine", ",", "torch_tensor", ")", "else", "mean_cosine", "\n", "logger", ".", "info", "(", "\"Mean cosine (%s method, %s build, %i max size): %.5f\"", "\n", "%", "(", "dico_method", ",", "_args", ".", "dico_build", ",", "dico_max_size", ",", "mean_cosine", ")", ")", "\n", "to_log", "[", "'mean_cosine-%s-%s-%i'", "%", "(", "dico_method", ",", "_args", ".", "dico_build", ",", "dico_max_size", ")", "]", "=", "mean_cosine", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.load_stop_words": [[35, 45], ["os.path.exists", "logger.info", "open", "fi.read().strip().split", "fi.read().strip", "fi.read"], "function", ["None"], ["", "", "def", "load_stop_words", "(", "file", ")", ":", "\n", "    ", "\"\"\"\n    Load stop words\n    \"\"\"", "\n", "if", "os", ".", "path", ".", "exists", "(", "file", ")", ":", "\n", "        ", "with", "open", "(", "file", ",", "'r'", ")", "as", "fi", ":", "\n", "            ", "return", "fi", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "'\\n'", ")", "\n", "", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"### Stop word file {} does not exist! ###\"", ".", "format", "(", "file", ")", ")", "\n", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.rm_stop_words": [[46, 60], ["zip", "len", "len", "numpy.array", "new_toks.append", "new_embs.append"], "function", ["None"], ["", "", "def", "rm_stop_words", "(", "tokens", ",", "embs", ",", "stop_words", ",", "puncs", ")", ":", "\n", "    ", "\"\"\"\n    Remove stop words\n    \"\"\"", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "embs", ")", "\n", "if", "stop_words", "is", "None", "and", "puncs", "is", "None", ":", "\n", "        ", "return", "tokens", ",", "embs", "\n", "", "new_toks", "=", "[", "]", "\n", "new_embs", "=", "[", "]", "\n", "for", "tok", ",", "emb", "in", "zip", "(", "tokens", ",", "embs", ")", ":", "\n", "        ", "if", "tok", "not", "in", "stop_words", "and", "tok", "not", "in", "puncs", ":", "\n", "            ", "new_toks", ".", "append", "(", "tok", ")", "\n", "new_embs", ".", "append", "(", "emb", ")", "\n", "", "", "return", "new_toks", ",", "np", ".", "array", "(", "new_embs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.get_overlaps": [[61, 79], ["collections.Counter", "collections.Counter", "enumerate", "len", "len", "len", "len", "zip", "tgt_toks.index", "overlaps.append"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dictionary.Dictionary.index"], ["", "def", "get_overlaps", "(", "src_toks", ",", "src_embs", ",", "tgt_toks", ",", "tgt_embs", ")", ":", "\n", "    ", "\"\"\"\n    Get overlaps\n    \"\"\"", "\n", "assert", "len", "(", "src_toks", ")", "==", "len", "(", "src_embs", ")", "\n", "assert", "len", "(", "tgt_toks", ")", "==", "len", "(", "tgt_embs", ")", "\n", "src_cnt", "=", "Counter", "(", "src_toks", ")", "\n", "tgt_cnt", "=", "Counter", "(", "tgt_toks", ")", "\n", "\n", "overlaps", "=", "[", "]", "\n", "for", "src_id", ",", "(", "tok", ",", "src_emb", ")", "in", "enumerate", "(", "zip", "(", "src_toks", ",", "src_embs", ")", ")", ":", "\n", "# only return overlap tokens that are unique in both source and target", "\n", "        ", "if", "not", "tok", "==", "'[UNK]'", "and", "src_cnt", "[", "tok", "]", "==", "1", "and", "tok", "in", "tgt_toks", "and", "tgt_cnt", "[", "tok", "]", "==", "1", ":", "\n", "            ", "tgt_id", "=", "tgt_toks", ".", "index", "(", "tok", ")", "\n", "overlap", "=", "{", "'src_id'", ":", "src_id", ",", "'tgt_id'", ":", "tgt_id", ",", "'token'", ":", "tok", ",", "\n", "'src_emb'", ":", "src_emb", ",", "'tgt_emb'", ":", "tgt_embs", "[", "tgt_id", "]", "}", "\n", "overlaps", ".", "append", "(", "overlap", ")", "\n", "", "", "return", "overlaps", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.get_overlap_sim": [[80, 92], ["bert_evaluator.cos_sim", "similarities.append", "infos.append"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.cos_sim"], ["", "def", "get_overlap_sim", "(", "overlaps", ")", ":", "\n", "    ", "\"\"\"\n    Calculate overlap token similarities\n    \"\"\"", "\n", "similarities", "=", "[", "]", "\n", "infos", "=", "[", "]", "\n", "for", "pair", "in", "overlaps", ":", "\n", "        ", "tok_sim", "=", "cos_sim", "(", "pair", "[", "'src_emb'", "]", ",", "pair", "[", "'tgt_emb'", "]", ")", "\n", "similarities", ".", "append", "(", "{", "'src_id'", ":", "pair", "[", "'src_id'", "]", ",", "'tgt_id'", ":", "pair", "[", "'tgt_id'", "]", ",", "\n", "'sim'", ":", "tok_sim", "}", ")", "\n", "infos", ".", "append", "(", "'token:\\\"{}\\\" (src_id:{}, tgt_id:{}), sim:{:.4f}'", ".", "format", "(", "pair", "[", "'token'", "]", ",", "pair", "[", "'src_id'", "]", ",", "pair", "[", "'tgt_id'", "]", ",", "tok_sim", ")", ")", "\n", "", "return", "similarities", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_evaluator.cos_sim": [[93, 95], ["numpy.inner", "numpy.linalg.norm", "numpy.linalg.norm"], "function", ["None"], ["", "def", "cos_sim", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "np", ".", "inner", "(", "a", ",", "b", ")", "/", "(", "norm", "(", "a", ")", "*", "norm", "(", "b", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.FullTokenizer.__init__": [[104, 108], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.FullTokenizer.tokenize": [[109, 116], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.FullTokenizer.convert_tokens_to_ids": [[117, 119], ["tokenization.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_tokens_to_ids", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer.__init__": [[124, 131], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer.tokenize": [[132, 146], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer._run_strip_accents": [[147, 157], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer._run_split_on_punc": [[158, 177], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.BasicTokenizer._clean_text": [[178, 190], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization._is_whitespace", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.__init__": [[195, 199], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.WordpieceTokenizer.tokenize": [[200, 253], ["tokenization.convert_to_unicode", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_to_unicode"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "#for token in whitespace_tokenize(text):", "\n", "for", "token", "in", "[", "text", "]", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_to_unicode": [[26, 44], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.printable_text": [[46, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.load_vocab": [[69, 82], ["collections.OrderedDict", "open", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.convert_tokens_to_ids": [[84, 90], ["ids.append"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "ids", ".", "append", "(", "vocab", "[", "token", "]", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization.whitespace_tokenize": [[92, 99], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization._is_whitespace": [[255, 265], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization._is_control": [[267, 277], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.tokenization._is_punctuation": [[279, 293], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.build_model.Discriminator.__init__": [[23, 42], ["torch.nn.Module.__init__", "range", "layers.append", "torch.nn.Sequential", "torch.nn.Dropout", "layers.append", "torch.nn.Sigmoid", "torch.nn.Linear", "layers.append", "layers.append", "torch.nn.LeakyReLU", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "bert_hidden_size", ")", ":", "\n", "        ", "super", "(", "Discriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "emb_dim", "=", "bert_hidden_size", "\n", "self", ".", "dis_layers", "=", "args", ".", "dis_layers", "\n", "self", ".", "dis_hid_dim", "=", "args", ".", "dis_hid_dim", "\n", "self", ".", "dis_dropout", "=", "args", ".", "dis_dropout", "\n", "self", ".", "dis_input_dropout", "=", "args", ".", "dis_input_dropout", "\n", "\n", "layers", "=", "[", "nn", ".", "Dropout", "(", "self", ".", "dis_input_dropout", ")", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "dis_layers", "+", "1", ")", ":", "\n", "            ", "input_dim", "=", "self", ".", "emb_dim", "if", "i", "==", "0", "else", "self", ".", "dis_hid_dim", "\n", "output_dim", "=", "1", "if", "i", "==", "self", ".", "dis_layers", "else", "self", ".", "dis_hid_dim", "\n", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", ")", "\n", "if", "i", "<", "self", ".", "dis_layers", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "LeakyReLU", "(", "0.2", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "Dropout", "(", "self", ".", "dis_dropout", ")", ")", "\n", "", "", "layers", ".", "append", "(", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.build_model.Discriminator.forward": [[43, 46], ["build_model.Discriminator.layers().view", "x.dim", "x.size", "build_model.Discriminator.layers"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "x", ".", "dim", "(", ")", "==", "2", "and", "x", ".", "size", "(", "1", ")", "==", "self", ".", "emb_dim", "\n", "return", "self", ".", "layers", "(", "x", ")", ".", "view", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.build_model.build_model": [[48, 121], ["print", "torch.device", "torch.cuda.device_count", "torch.device", "torch.distributed.init_process_group", "bool", "src.bert_modeling.BertConfig.from_json_file", "src.bert_modeling.BertModel", "torch.nn.DataParallel.to", "src.bert_modeling.BertModel", "torch.nn.DataParallel.to", "src.maps.NonLinearMap", "nn.Linear.to", "torch.nn.parallel.DistributedDataParallel", "torch.nn.DataParallel.load_state_dict", "src.bert_modeling.BertConfig.from_json_file", "torch.nn.DataParallel.load_state_dict", "src.maps.SelfAttentionMap", "torch.load", "torch.load", "src.maps.AttentionMap", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.cuda.is_available", "src.maps.LinearSelfAttentionMap", "src.maps.NonLinearSelfAttentionMap", "logger.info", "torch.nn.Linear", "getattr", "ValueError", "exit", "nn.Linear.weight.data.copy_", "torch.diag", "torch.ones"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load"], ["", "", "def", "build_model", "(", "args", ",", "with_dis", ")", ":", "\n", "    ", "\"\"\"\n    Build all components of the model.\n    \"\"\"", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "print", "(", "\"device\"", ",", "device", ",", "\"n_gpu\"", ",", "n_gpu", ",", "\"distributed training\"", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "\n", "\n", "if", "args", ".", "load_pred_bert", ":", "\n", "        ", "model", "=", "None", "\n", "model1", "=", "None", "\n", "", "else", ":", "\n", "        ", "bert_config", "=", "BertConfig", ".", "from_json_file", "(", "args", ".", "bert_config_file", ")", "\n", "model", "=", "BertModel", "(", "bert_config", ")", "\n", "if", "args", ".", "init_checkpoint", "is", "not", "None", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "init_checkpoint", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "bert_config_file1", ":", "\n", "            ", "bert_config1", "=", "BertConfig", ".", "from_json_file", "(", "args", ".", "bert_config_file1", ")", "\n", "", "model1", "=", "BertModel", "(", "bert_config1", ")", "\n", "if", "args", ".", "init_checkpoint", "is", "not", "None", ":", "\n", "            ", "model1", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "init_checkpoint1", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "", "model1", ".", "to", "(", "device", ")", "\n", "assert", "bert_config", ".", "hidden_size", "==", "bert_config1", ".", "hidden_size", "\n", "\n", "# mapping", "\n", "#if args.non_linear:", "\n", "", "if", "args", ".", "map_type", "==", "'nonlinear'", ":", "\n", "#assert args.emb_dim == bert_config.hidden_size", "\n", "        ", "mapping", "=", "NonLinearMap", "(", "args", ")", "\n", "#elif args.transformer:", "\n", "", "elif", "args", ".", "map_type", "==", "'self_attention'", ":", "\n", "        ", "mapping", "=", "SelfAttentionMap", "(", "args", ")", "\n", "", "elif", "args", ".", "map_type", "==", "'attention'", ":", "\n", "        ", "mapping", "=", "AttentionMap", "(", "args", ")", "\n", "", "elif", "args", ".", "map_type", "==", "'linear_self_attention'", ":", "\n", "        ", "mapping", "=", "LinearSelfAttentionMap", "(", "args", ")", "\n", "", "elif", "args", ".", "map_type", "==", "'nonlinear_self_attention'", ":", "\n", "        ", "mapping", "=", "NonLinearSelfAttentionMap", "(", "args", ")", "\n", "", "elif", "args", ".", "map_type", "==", "'fine_tune'", ":", "\n", "        ", "mapping", "=", "None", "\n", "", "elif", "args", ".", "map_type", "==", "'linear'", "or", "args", ".", "map_type", "==", "'svd'", ":", "\n", "#assert args.emb_dim == bert_config.hidden_size", "\n", "        ", "logger", ".", "info", "(", "\"Linear mapping:\\nEmbedding Dimension:{}\"", ".", "format", "(", "args", ".", "emb_dim", ")", ")", "\n", "mapping", "=", "nn", ".", "Linear", "(", "args", ".", "emb_dim", ",", "args", ".", "emb_dim", ",", "bias", "=", "False", ")", "\n", "if", "getattr", "(", "args", ",", "'map_id_init'", ",", "True", ")", ":", "\n", "            ", "mapping", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "diag", "(", "torch", ".", "ones", "(", "args", ".", "emb_dim", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid map type: {}\"", ".", "format", "(", "args", ".", "map_type", ")", ")", "\n", "exit", "(", "1", ")", "\n", "", "if", "mapping", ":", "\n", "        ", "mapping", ".", "to", "(", "device", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", "and", "not", "args", ".", "load_pred_bert", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "args", ".", "local_rank", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "if", "not", "args", ".", "load_pred_bert", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "model1", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model1", ")", "\n", "", "if", "mapping", ":", "\n", "            ", "mapping", "=", "torch", ".", "nn", ".", "DataParallel", "(", "mapping", ")", "\n", "\n", "", "", "return", "model", ",", "model1", ",", "mapping", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.initialize_exp": [[49, 82], ["json.dump", "logger.create_logger", "logger.create_logger.info", "logger.create_logger.info", "logger.create_logger.info", "hasattr", "logger.create_logger", "logger.create_logger.info", "logger.create_logger.info", "getattr", "numpy.random.seed", "torch.manual_seed", "os.path.exists", "os.makedirs", "print", "exit", "vars", "codecs.open", "os.path.join", "os.path.join", "torch.cuda.manual_seed", "os.path.join", "sorted", "sorted", "str", "dict().items", "str", "dict().items", "dict", "dict", "vars", "vars"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.logger.create_logger", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.logger.create_logger"], ["", "def", "initialize_exp", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Initialize experiment.\n    \"\"\"", "\n", "if", "hasattr", "(", "args", ",", "'test'", ")", "and", "args", ".", "test", ":", "\n", "        ", "logger", "=", "create_logger", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "'test.log'", ")", ",", "vb", "=", "args", ".", "verbose", ")", "\n", "logger", ".", "info", "(", "'============ Initialized logger for test ============'", ")", "\n", "logger", ".", "info", "(", "'\\n'", ".", "join", "(", "'%s: %s'", "%", "(", "k", ",", "str", "(", "v", ")", ")", "for", "k", ",", "v", "in", "sorted", "(", "dict", "(", "vars", "(", "args", ")", ")", ".", "items", "(", ")", ")", ")", ")", "\n", "return", "logger", "\n", "# initialization", "\n", "", "if", "getattr", "(", "args", ",", "'seed'", ",", "-", "1", ")", ">=", "0", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# dump parameters", "\n", "#args.exp_path = get_exp_path(args)", "\n", "#with io.open(os.path.join(args.model_path, 'args.pkl'), 'wb') as f:", "\n", "#pickle.dump(args, f)", "\n", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "model_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "model_path", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"### Model path already exists! ###\"", ")", "\n", "exit", "(", "1", ")", "\n", "", "json", ".", "dump", "(", "vars", "(", "args", ")", ",", "codecs", ".", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "\"args.json\"", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "\n", "# create logger", "\n", "logger", "=", "create_logger", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "'train.log'", ")", ",", "vb", "=", "args", ".", "verbose", ")", "\n", "logger", ".", "info", "(", "'============ Initialized logger ============'", ")", "\n", "logger", ".", "info", "(", "'\\n'", ".", "join", "(", "'%s: %s'", "%", "(", "k", ",", "str", "(", "v", ")", ")", "for", "k", ",", "v", "in", "sorted", "(", "dict", "(", "vars", "(", "args", ")", ")", ".", "items", "(", ")", ")", ")", ")", "\n", "logger", ".", "info", "(", "'The experiment will be stored in %s'", "%", "args", ".", "model_path", ")", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_fasttext_model": [[83, 93], ["fastText.load_model", "Exception"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.load_model"], ["", "def", "load_fasttext_model", "(", "path", ")", ":", "\n", "    ", "\"\"\"\n    Load a binarized fastText model.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "fastText", "\n", "", "except", "ImportError", ":", "\n", "        ", "raise", "Exception", "(", "\"Unable to import fastText. Please install fastText for Python: \"", "\n", "\"https://github.com/facebookresearch/fastText\"", ")", "\n", "", "return", "fastText", ".", "load_model", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.bow": [[95, 108], ["numpy.vstack", "embeddings.append", "len", "numpy.mean", "numpy.linalg.norm", "list", "word_vec.keys"], "function", ["None"], ["", "def", "bow", "(", "sentences", ",", "word_vec", ",", "normalize", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Get sentence representations using average bag-of-words.\n    \"\"\"", "\n", "embeddings", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "sentvec", "=", "[", "word_vec", "[", "w", "]", "for", "w", "in", "sent", "if", "w", "in", "word_vec", "]", "\n", "if", "normalize", ":", "\n", "            ", "sentvec", "=", "[", "v", "/", "np", ".", "linalg", ".", "norm", "(", "v", ")", "for", "v", "in", "sentvec", "]", "\n", "", "if", "len", "(", "sentvec", ")", "==", "0", ":", "\n", "            ", "sentvec", "=", "[", "word_vec", "[", "list", "(", "word_vec", ".", "keys", "(", ")", ")", "[", "0", "]", "]", "]", "\n", "", "embeddings", ".", "append", "(", "np", ".", "mean", "(", "sentvec", ",", "axis", "=", "0", ")", ")", "\n", "", "return", "np", ".", "vstack", "(", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.bow_idf": [[110, 125], ["numpy.vstack", "set", "embeddings.append", "len", "numpy.sum", "numpy.sum", "list", "word_vec.keys"], "function", ["None"], ["", "def", "bow_idf", "(", "sentences", ",", "word_vec", ",", "idf_dict", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Get sentence representations using weigthed IDF bag-of-words.\n    \"\"\"", "\n", "embeddings", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "sent", "=", "set", "(", "sent", ")", "\n", "list_words", "=", "[", "w", "for", "w", "in", "sent", "if", "w", "in", "word_vec", "and", "w", "in", "idf_dict", "]", "\n", "if", "len", "(", "list_words", ")", ">", "0", ":", "\n", "            ", "sentvec", "=", "[", "word_vec", "[", "w", "]", "*", "idf_dict", "[", "w", "]", "for", "w", "in", "list_words", "]", "\n", "sentvec", "=", "sentvec", "/", "np", ".", "sum", "(", "[", "idf_dict", "[", "w", "]", "for", "w", "in", "list_words", "]", ")", "\n", "", "else", ":", "\n", "            ", "sentvec", "=", "[", "word_vec", "[", "list", "(", "word_vec", ".", "keys", "(", ")", ")", "[", "0", "]", "]", "]", "\n", "", "embeddings", ".", "append", "(", "np", ".", "sum", "(", "sentvec", ",", "axis", "=", "0", ")", ")", "\n", "", "return", "np", ".", "vstack", "(", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_idf": [[127, 144], ["len", "set", "max", "numpy.log10", "idf[].get"], "function", ["None"], ["", "def", "get_idf", "(", "europarl", ",", "src_lg", ",", "tgt_lg", ",", "n_idf", ")", ":", "\n", "    ", "\"\"\"\n    Compute IDF values.\n    \"\"\"", "\n", "idf", "=", "{", "src_lg", ":", "{", "}", ",", "tgt_lg", ":", "{", "}", "}", "\n", "k", "=", "0", "\n", "for", "lg", "in", "idf", ":", "\n", "        ", "start_idx", "=", "200000", "+", "k", "*", "n_idf", "\n", "end_idx", "=", "200000", "+", "(", "k", "+", "1", ")", "*", "n_idf", "\n", "for", "sent", "in", "europarl", "[", "lg", "]", "[", "start_idx", ":", "end_idx", "]", ":", "\n", "            ", "for", "word", "in", "set", "(", "sent", ")", ":", "\n", "                ", "idf", "[", "lg", "]", "[", "word", "]", "=", "idf", "[", "lg", "]", ".", "get", "(", "word", ",", "0", ")", "+", "1", "\n", "", "", "n_doc", "=", "len", "(", "europarl", "[", "lg", "]", "[", "start_idx", ":", "end_idx", "]", ")", "\n", "for", "word", "in", "idf", "[", "lg", "]", ":", "\n", "            ", "idf", "[", "lg", "]", "[", "word", "]", "=", "max", "(", "1", ",", "np", ".", "log10", "(", "n_doc", "/", "(", "idf", "[", "lg", "]", "[", "word", "]", ")", ")", ")", "\n", "", "k", "+=", "1", "\n", "", "return", "idf", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist": [[146, 177], ["emb.transpose().contiguous.cpu().numpy", "query.cpu().numpy.cpu().numpy", "hasattr", "faiss.IndexFlatIP.add", "faiss.IndexFlatIP.search", "query[].mm.mean", "emb.transpose().contiguous.transpose().contiguous", "range", "torch.cat", "torch.cat.numpy", "faiss.StandardGpuResources", "faiss.GpuIndexFlatConfig", "faiss.GpuIndexFlatIP", "faiss.IndexFlatIP", "query[].mm", "query[].mm.topk", "torch.cat.append", "emb.transpose().contiguous.cpu", "query.cpu().numpy.cpu", "emb.transpose().contiguous.transpose", "best_distances.mean().cpu", "best_distances.mean"], "function", ["None"], ["", "def", "get_nn_avg_dist", "(", "emb", ",", "query", ",", "knn", ")", ":", "\n", "    ", "\"\"\"\n    Compute the average distance of the `knn` nearest neighbors\n    for a given set of embeddings and queries.\n    Use Faiss if available.\n    \"\"\"", "\n", "if", "FAISS_AVAILABLE", ":", "\n", "        ", "emb", "=", "emb", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "query", "=", "query", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "hasattr", "(", "faiss", ",", "'StandardGpuResources'", ")", ":", "\n", "# gpu mode", "\n", "            ", "res", "=", "faiss", ".", "StandardGpuResources", "(", ")", "\n", "config", "=", "faiss", ".", "GpuIndexFlatConfig", "(", ")", "\n", "config", ".", "device", "=", "0", "\n", "index", "=", "faiss", ".", "GpuIndexFlatIP", "(", "res", ",", "emb", ".", "shape", "[", "1", "]", ",", "config", ")", "\n", "", "else", ":", "\n", "# cpu mode", "\n", "            ", "index", "=", "faiss", ".", "IndexFlatIP", "(", "emb", ".", "shape", "[", "1", "]", ")", "\n", "", "index", ".", "add", "(", "emb", ")", "\n", "distances", ",", "_", "=", "index", ".", "search", "(", "query", ",", "knn", ")", "\n", "return", "distances", ".", "mean", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "bs", "=", "1024", "\n", "all_distances", "=", "[", "]", "\n", "emb", "=", "emb", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "query", ".", "shape", "[", "0", "]", ",", "bs", ")", ":", "\n", "            ", "distances", "=", "query", "[", "i", ":", "i", "+", "bs", "]", ".", "mm", "(", "emb", ")", "\n", "best_distances", ",", "_", "=", "distances", ".", "topk", "(", "knn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "all_distances", ".", "append", "(", "best_distances", ".", "mean", "(", "1", ")", ".", "cpu", "(", ")", ")", "\n", "", "all_distances", "=", "torch", ".", "cat", "(", "all_distances", ")", "\n", "return", "all_distances", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.bool_flag": [[179, 188], ["argparse.ArgumentTypeError", "s.lower", "s.lower"], "function", ["None"], ["", "", "def", "bool_flag", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Parse boolean arguments from the command line.\n    \"\"\"", "\n", "if", "s", ".", "lower", "(", ")", "in", "[", "'off'", ",", "'false'", ",", "'0'", "]", ":", "\n", "        ", "return", "False", "\n", "", "if", "s", ".", "lower", "(", ")", "in", "[", "'on'", ",", "'true'", ",", "'1'", "]", ":", "\n", "        ", "return", "True", "\n", "", "raise", "argparse", ".", "ArgumentTypeError", "(", "\"invalid value for a boolean flag (0 or 1)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_optimizer": [[190, 236], ["s[].split", "inspect.getargspec", "all", "Exception", "x.split", "float", "s.find", "len", "re.match", "optim_args.keys", "str", "str", "optim_args.keys", "s.find", "Exception"], "function", ["None"], ["", "def", "get_optimizer", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Parse optimizer parameters.\n    Input should be of the form:\n        - \"sgd,lr=0.01\"\n        - \"adagrad,lr=0.1,lr_decay=0.05\"\n    \"\"\"", "\n", "if", "\",\"", "in", "s", ":", "\n", "        ", "method", "=", "s", "[", ":", "s", ".", "find", "(", "','", ")", "]", "\n", "optim_args", "=", "{", "}", "\n", "for", "x", "in", "s", "[", "s", ".", "find", "(", "','", ")", "+", "1", ":", "]", ".", "split", "(", "','", ")", ":", "\n", "            ", "split", "=", "x", ".", "split", "(", "'='", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "\n", "assert", "re", ".", "match", "(", "\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\"", ",", "split", "[", "1", "]", ")", "is", "not", "None", "\n", "optim_args", "[", "split", "[", "0", "]", "]", "=", "float", "(", "split", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "method", "=", "s", "\n", "optim_args", "=", "{", "}", "\n", "", "if", "method", "==", "'adadelta'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adadelta", "\n", "", "elif", "method", "==", "'adagrad'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adagrad", "\n", "", "elif", "method", "==", "'adam'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adam", "\n", "", "elif", "method", "==", "'adamax'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adamax", "\n", "", "elif", "method", "==", "'asgd'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "ASGD", "\n", "", "elif", "method", "==", "'rmsprop'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "RMSprop", "\n", "", "elif", "method", "==", "'rprop'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Rprop", "\n", "", "elif", "method", "==", "'sgd'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "SGD", "\n", "assert", "'lr'", "in", "optim_args", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Unknown optimization method: \"%s\"'", "%", "method", ")", "\n", "\n", "# check that we give good parameters to the optimizer", "\n", "", "expected_args", "=", "inspect", ".", "getargspec", "(", "optim_fn", ".", "__init__", ")", "[", "0", "]", "\n", "assert", "expected_args", "[", ":", "2", "]", "==", "[", "'self'", ",", "'params'", "]", "\n", "if", "not", "all", "(", "k", "in", "expected_args", "[", "2", ":", "]", "for", "k", "in", "optim_args", ".", "keys", "(", ")", ")", ":", "\n", "        ", "raise", "Exception", "(", "'Unexpected parameters: expected \"%s\", got \"%s\"'", "%", "(", "\n", "str", "(", "expected_args", "[", "2", ":", "]", ")", ",", "str", "(", "optim_args", ".", "keys", "(", ")", ")", ")", ")", "\n", "\n", "", "return", "optim_fn", ",", "optim_args", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_exp_path": [[238, 264], ["os.path.join", "os.path.exists", "subprocess.Popen().wait", "os.path.exists", "subprocess.Popen().wait", "os.path.join", "os.path.isdir", "subprocess.Popen().wait", "os.path.join", "os.path.isdir", "subprocess.Popen", "subprocess.Popen", "os.path.isdir", "subprocess.Popen", "random.choice", "range"], "function", ["None"], ["", "def", "get_exp_path", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Create a directory to store the experiment.\n    \"\"\"", "\n", "# create the main dump path if it does not exist", "\n", "exp_folder", "=", "MAIN_DUMP_PATH", "if", "args", ".", "exp_path", "==", "''", "else", "args", ".", "exp_path", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "exp_folder", ")", ":", "\n", "        ", "subprocess", ".", "Popen", "(", "\"mkdir %s\"", "%", "exp_folder", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "", "assert", "args", ".", "exp_name", "!=", "''", "\n", "exp_folder", "=", "os", ".", "path", ".", "join", "(", "exp_folder", ",", "args", ".", "exp_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "exp_folder", ")", ":", "\n", "        ", "subprocess", ".", "Popen", "(", "\"mkdir %s\"", "%", "exp_folder", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "", "if", "args", ".", "exp_id", "==", "''", ":", "\n", "        ", "chars", "=", "'abcdefghijklmnopqrstuvwxyz0123456789'", "\n", "while", "True", ":", "\n", "            ", "exp_id", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "chars", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "exp_path", "=", "os", ".", "path", ".", "join", "(", "exp_folder", ",", "exp_id", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "exp_path", ")", ":", "\n", "                ", "break", "\n", "", "", "", "else", ":", "\n", "        ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "exp_folder", ",", "args", ".", "exp_id", ")", "\n", "assert", "not", "os", ".", "path", ".", "isdir", "(", "exp_path", ")", ",", "exp_path", "\n", "# create the dump folder", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "exp_path", ")", ":", "\n", "        ", "subprocess", ".", "Popen", "(", "\"mkdir %s\"", "%", "exp_path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "", "return", "exp_path", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.clip_parameters": [[266, 273], ["model.parameters", "x.data.clamp_"], "function", ["None"], ["", "def", "clip_parameters", "(", "model", ",", "clip", ")", ":", "\n", "    ", "\"\"\"\n    Clip model weights.\n    \"\"\"", "\n", "if", "clip", ">", "0", ":", "\n", "        ", "for", "x", "in", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "x", ".", "data", ".", "clamp_", "(", "-", "clip", ",", "clip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.read_txt_embeddings": [[275, 326], ["logger.info", "dictionary.Dictionary", "numpy.concatenate", "torch.from_numpy().float", "io.open", "enumerate", "len", "len", "torch.from_numpy().float.cuda", "torch.from_numpy().float.size", "len", "word2id.items", "torch.from_numpy", "len", "line.split", "line.rstrip().split", "numpy.fromstring", "len", "int", "word.lower.lower", "numpy.linalg.norm", "len", "vectors.append", "len", "line.rstrip", "logger.warning", "logger.warning"], "function", ["None"], ["", "", "", "def", "read_txt_embeddings", "(", "args", ",", "source", ",", "full_vocab", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings from a text file.\n    \"\"\"", "\n", "word2id", "=", "{", "}", "\n", "vectors", "=", "[", "]", "\n", "\n", "# load pretrained embeddings", "\n", "lang", "=", "args", ".", "src_lang", "if", "source", "else", "args", ".", "tgt_lang", "\n", "emb_path", "=", "args", ".", "src_emb", "if", "source", "else", "args", ".", "tgt_emb", "\n", "_emb_dim_file", "=", "args", ".", "emb_dim", "\n", "with", "io", ".", "open", "(", "emb_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "'\\n'", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "split", "=", "line", ".", "split", "(", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "\n", "assert", "_emb_dim_file", "==", "int", "(", "split", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "word", ",", "vect", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ",", "1", ")", "\n", "if", "not", "full_vocab", ":", "\n", "                    ", "word", "=", "word", ".", "lower", "(", ")", "\n", "", "vect", "=", "np", ".", "fromstring", "(", "vect", ",", "sep", "=", "' '", ")", "\n", "if", "np", ".", "linalg", ".", "norm", "(", "vect", ")", "==", "0", ":", "# avoid to have null embeddings", "\n", "                    ", "vect", "[", "0", "]", "=", "0.01", "\n", "", "if", "word", "in", "word2id", ":", "\n", "                    ", "if", "full_vocab", ":", "\n", "                        ", "logger", ".", "warning", "(", "\"Word '%s' found twice in %s embedding file\"", "\n", "%", "(", "word", ",", "'source'", "if", "source", "else", "'target'", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "not", "vect", ".", "shape", "==", "(", "_emb_dim_file", ",", ")", ":", "\n", "                        ", "logger", ".", "warning", "(", "\"Invalid dimension (%i) for %s word '%s' in line %i.\"", "\n", "%", "(", "vect", ".", "shape", "[", "0", "]", ",", "'source'", "if", "source", "else", "'target'", ",", "word", ",", "i", ")", ")", "\n", "continue", "\n", "", "assert", "vect", ".", "shape", "==", "(", "_emb_dim_file", ",", ")", ",", "i", "\n", "word2id", "[", "word", "]", "=", "len", "(", "word2id", ")", "\n", "vectors", ".", "append", "(", "vect", "[", "None", "]", ")", "\n", "", "", "if", "args", ".", "max_vocab", ">", "0", "and", "len", "(", "word2id", ")", ">=", "args", ".", "max_vocab", "and", "not", "full_vocab", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "assert", "len", "(", "word2id", ")", "==", "len", "(", "vectors", ")", "\n", "logger", ".", "info", "(", "\"Loaded %i pre-trained word embeddings.\"", "%", "len", "(", "vectors", ")", ")", "\n", "\n", "# compute new vocabulary / embeddings", "\n", "id2word", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "word2id", ".", "items", "(", ")", "}", "\n", "dico", "=", "Dictionary", "(", "id2word", ",", "word2id", ",", "lang", ")", "\n", "embeddings", "=", "np", ".", "concatenate", "(", "vectors", ",", "0", ")", "\n", "embeddings", "=", "torch", ".", "from_numpy", "(", "embeddings", ")", ".", "float", "(", ")", "\n", "embeddings", "=", "embeddings", ".", "cuda", "(", ")", "if", "(", "args", ".", "cuda", "and", "not", "full_vocab", ")", "else", "embeddings", "\n", "\n", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "dico", ")", ",", "args", ".", "emb_dim", ")", "\n", "return", "dico", ",", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.select_subset": [[328, 345], ["enumerate", "word.lower.lower", "len", "len", "torch.LongTensor", "len", "indexes.append", "len"], "function", ["None"], ["", "def", "select_subset", "(", "word_list", ",", "max_vocab", ")", ":", "\n", "    ", "\"\"\"\n    Select a subset of words to consider, to deal with words having embeddings\n    available in different casings. In particular, we select the embeddings of\n    the most frequent words, that are usually of better quality.\n    \"\"\"", "\n", "word2id", "=", "{", "}", "\n", "indexes", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "word_list", ")", ":", "\n", "        ", "word", "=", "word", ".", "lower", "(", ")", "\n", "if", "word", "not", "in", "word2id", ":", "\n", "            ", "word2id", "[", "word", "]", "=", "len", "(", "word2id", ")", "\n", "indexes", ".", "append", "(", "i", ")", "\n", "", "if", "max_vocab", ">", "0", "and", "len", "(", "word2id", ")", ">=", "max_vocab", ":", "\n", "            ", "break", "\n", "", "", "assert", "len", "(", "word2id", ")", "==", "len", "(", "indexes", ")", "\n", "return", "word2id", ",", "torch", ".", "LongTensor", "(", "indexes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_pth_embeddings": [[347, 369], ["torch.load", "logger.info", "embeddings.size", "utils.select_subset", "dictionary.Dictionary", "embeddings.size", "len", "len", "len", "word2id.items", "range", "len"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.load.load", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.select_subset"], ["", "def", "load_pth_embeddings", "(", "args", ",", "source", ",", "full_vocab", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings from a PyTorch binary file.\n    \"\"\"", "\n", "# reload PyTorch binary file", "\n", "lang", "=", "args", ".", "src_lang", "if", "source", "else", "args", ".", "tgt_lang", "\n", "data", "=", "torch", ".", "load", "(", "args", ".", "src_emb", "if", "source", "else", "args", ".", "tgt_emb", ")", "\n", "dico", "=", "data", "[", "'dico'", "]", "\n", "embeddings", "=", "data", "[", "'vectors'", "]", "\n", "assert", "dico", ".", "lang", "==", "lang", "\n", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "dico", ")", ",", "args", ".", "emb_dim", ")", "\n", "logger", ".", "info", "(", "\"Loaded %i pre-trained word embeddings.\"", "%", "len", "(", "dico", ")", ")", "\n", "\n", "# select a subset of word embeddings (to deal with casing)", "\n", "if", "not", "full_vocab", ":", "\n", "        ", "word2id", ",", "indexes", "=", "select_subset", "(", "[", "dico", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "dico", ")", ")", "]", ",", "args", ".", "max_vocab", ")", "\n", "id2word", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "word2id", ".", "items", "(", ")", "}", "\n", "dico", "=", "Dictionary", "(", "id2word", ",", "word2id", ",", "lang", ")", "\n", "embeddings", "=", "embeddings", "[", "indexes", "]", "\n", "\n", "", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "dico", ")", ",", "args", ".", "emb_dim", ")", "\n", "return", "dico", ",", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_bin_embeddings": [[371, 396], ["utils.load_fasttext_model", "load_fasttext_model.get_labels", "logger.info", "torch.from_numpy", "logger.info", "dictionary.Dictionary", "load_fasttext_model.get_dimension", "numpy.concatenate", "torch.from_numpy.size", "utils.select_subset", "torch.from_numpy.size", "len", "len", "word2id.items", "len", "enumerate", "load_fasttext_model.get_word_vector"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_fasttext_model", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.select_subset"], ["", "def", "load_bin_embeddings", "(", "args", ",", "source", ",", "full_vocab", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings from a fastText binary file.\n    \"\"\"", "\n", "# reload fastText binary file", "\n", "lang", "=", "args", ".", "src_lang", "if", "source", "else", "args", ".", "tgt_lang", "\n", "model", "=", "load_fasttext_model", "(", "args", ".", "src_emb", "if", "source", "else", "args", ".", "tgt_emb", ")", "\n", "words", "=", "model", ".", "get_labels", "(", ")", "\n", "assert", "model", ".", "get_dimension", "(", ")", "==", "args", ".", "emb_dim", "\n", "logger", ".", "info", "(", "\"Loaded binary model. Generating embeddings ...\"", ")", "\n", "embeddings", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "model", ".", "get_word_vector", "(", "w", ")", "[", "None", "]", "for", "w", "in", "words", "]", ",", "0", ")", ")", "\n", "logger", ".", "info", "(", "\"Generated embeddings for %i words.\"", "%", "len", "(", "words", ")", ")", "\n", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "words", ")", ",", "args", ".", "emb_dim", ")", "\n", "\n", "# select a subset of word embeddings (to deal with casing)", "\n", "if", "not", "full_vocab", ":", "\n", "        ", "word2id", ",", "indexes", "=", "select_subset", "(", "words", ",", "args", ".", "max_vocab", ")", "\n", "embeddings", "=", "embeddings", "[", "indexes", "]", "\n", "", "else", ":", "\n", "        ", "word2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "words", ")", "}", "\n", "", "id2word", "=", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", "\n", "dico", "=", "Dictionary", "(", "id2word", ",", "word2id", ",", "lang", ")", "\n", "\n", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "dico", ")", ",", "args", ".", "emb_dim", ")", "\n", "return", "dico", ",", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_embeddings": [[398, 418], ["emb_path.endswith", "emb_path.endswith", "utils.load_pth_embeddings", "utils.load_bin_embeddings", "utils.read_txt_embeddings", "type", "type"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_pth_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.load_bin_embeddings", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.read_txt_embeddings"], ["", "def", "load_embeddings", "(", "args", ",", "source", ",", "full_vocab", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings.\n    - `full_vocab == False` means that we load the `args.max_vocab` most frequent words.\n      It is used at the beginning of the experiment.\n      In that setting, if two words with a different casing occur, we lowercase both, and\n      only consider the most frequent one. For instance, if \"London\" and \"london\" are in\n      the embeddings file, we only consider the most frequent one, (in that case, probably\n      London). This is done to deal with the lowercased dictionaries.\n    - `full_vocab == True` means that we load the entire embedding text file,\n      before we export the embeddings at the end of the experiment.\n    \"\"\"", "\n", "assert", "type", "(", "source", ")", "is", "bool", "and", "type", "(", "full_vocab", ")", "is", "bool", "\n", "emb_path", "=", "args", ".", "src_emb", "if", "source", "else", "args", ".", "tgt_emb", "\n", "if", "emb_path", ".", "endswith", "(", "'.pth'", ")", ":", "\n", "        ", "return", "load_pth_embeddings", "(", "args", ",", "source", ",", "full_vocab", ")", "\n", "", "if", "emb_path", ".", "endswith", "(", "'.bin'", ")", ":", "\n", "        ", "return", "load_bin_embeddings", "(", "args", ",", "source", ",", "full_vocab", ")", "\n", "", "else", ":", "\n", "        ", "return", "read_txt_embeddings", "(", "args", ",", "source", ",", "full_vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.normalize_embeddings": [[420, 436], ["types.split", "emb.mean.cpu", "emb.sub_", "emb.mean", "emb.mean.expand_as", "emb.div_", "Exception", "emb.norm().expand_as", "emb.norm"], "function", ["None"], ["", "", "def", "normalize_embeddings", "(", "emb", ",", "types", ",", "mean", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Normalize embeddings by their norms / recenter them.\n    \"\"\"", "\n", "for", "t", "in", "types", ".", "split", "(", "','", ")", ":", "\n", "        ", "if", "t", "==", "''", ":", "\n", "            ", "continue", "\n", "", "if", "t", "==", "'center'", ":", "\n", "            ", "if", "mean", "is", "None", ":", "\n", "                ", "mean", "=", "emb", ".", "mean", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "", "emb", ".", "sub_", "(", "mean", ".", "expand_as", "(", "emb", ")", ")", "\n", "", "elif", "t", "==", "'renorm'", ":", "\n", "            ", "emb", ".", "div_", "(", "emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "emb", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Unknown normalization type: \"%s\"'", "%", "t", ")", "\n", "", "", "return", "mean", ".", "cpu", "(", ")", "if", "mean", "is", "not", "None", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.export_embeddings": [[438, 469], ["os.path.join", "os.path.join", "logger.info", "logger.info", "os.path.join", "os.path.join", "logger.info", "torch.save", "logger.info", "torch.save", "io.open", "f.write", "range", "io.open", "f.write", "range", "len", "f.write", "len", "f.write", "src_emb.size", "tgt_emb.size"], "function", ["None"], ["", "def", "export_embeddings", "(", "src_emb", ",", "tgt_emb", ",", "args", ")", ":", "\n", "    ", "\"\"\"\n    Export embeddings to a text or a PyTorch file.\n    \"\"\"", "\n", "assert", "args", ".", "export", "in", "[", "\"txt\"", ",", "\"pth\"", "]", "\n", "\n", "# text file", "\n", "if", "args", ".", "export", "==", "\"txt\"", ":", "\n", "        ", "src_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "'vectors-%s.txt'", "%", "args", ".", "src_lang", ")", "\n", "tgt_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "'vectors-%s.txt'", "%", "args", ".", "tgt_lang", ")", "\n", "# source embeddings", "\n", "logger", ".", "info", "(", "'Writing source embeddings to %s ...'", "%", "src_path", ")", "\n", "with", "io", ".", "open", "(", "src_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "u\"%i %i\\n\"", "%", "src_emb", ".", "size", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "src_dico", ")", ")", ":", "\n", "                ", "f", ".", "write", "(", "u\"%s %s\\n\"", "%", "(", "args", ".", "src_dico", "[", "i", "]", ",", "\" \"", ".", "join", "(", "'%.5f'", "%", "x", "for", "x", "in", "src_emb", "[", "i", "]", ")", ")", ")", "\n", "# target embeddings", "\n", "", "", "logger", ".", "info", "(", "'Writing target embeddings to %s ...'", "%", "tgt_path", ")", "\n", "with", "io", ".", "open", "(", "tgt_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "u\"%i %i\\n\"", "%", "tgt_emb", ".", "size", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "tgt_dico", ")", ")", ":", "\n", "                ", "f", ".", "write", "(", "u\"%s %s\\n\"", "%", "(", "args", ".", "tgt_dico", "[", "i", "]", ",", "\" \"", ".", "join", "(", "'%.5f'", "%", "x", "for", "x", "in", "tgt_emb", "[", "i", "]", ")", ")", ")", "\n", "\n", "# PyTorch file", "\n", "", "", "", "if", "args", ".", "export", "==", "\"pth\"", ":", "\n", "        ", "src_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "'vectors-%s.pth'", "%", "args", ".", "src_lang", ")", "\n", "tgt_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_path", ",", "'vectors-%s.pth'", "%", "args", ".", "tgt_lang", ")", "\n", "logger", ".", "info", "(", "'Writing source embeddings to %s ...'", "%", "src_path", ")", "\n", "torch", ".", "save", "(", "{", "'dico'", ":", "args", ".", "src_dico", ",", "'vectors'", ":", "src_emb", "}", ",", "src_path", ")", "\n", "logger", ".", "info", "(", "'Writing target embeddings to %s ...'", "%", "tgt_path", ")", "\n", "torch", ".", "save", "(", "{", "'dico'", ":", "args", ".", "tgt_dico", ",", "'vectors'", ":", "tgt_emb", "}", ",", "tgt_path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.load_identical_char_dico": [[23, 42], ["logger.info", "sorted", "torch.LongTensor", "enumerate", "len", "Exception", "len", "word2id1.keys", "len"], "function", ["None"], ["def", "load_identical_char_dico", "(", "word2id1", ",", "word2id2", ")", ":", "\n", "    ", "\"\"\"\n    Build a dictionary of identical character strings.\n    \"\"\"", "\n", "pairs", "=", "[", "(", "w1", ",", "w1", ")", "for", "w1", "in", "word2id1", ".", "keys", "(", ")", "if", "w1", "in", "word2id2", "]", "\n", "if", "len", "(", "pairs", ")", "==", "0", ":", "\n", "        ", "raise", "Exception", "(", "\"No identical character strings were found. \"", "\n", "\"Please specify a dictionary.\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Found %i pairs of identical character strings.\"", "%", "len", "(", "pairs", ")", ")", "\n", "\n", "# sort the dictionary by source word frequencies", "\n", "pairs", "=", "sorted", "(", "pairs", ",", "key", "=", "lambda", "x", ":", "word2id1", "[", "x", "[", "0", "]", "]", ")", "\n", "dico", "=", "torch", ".", "LongTensor", "(", "len", "(", "pairs", ")", ",", "2", ")", "\n", "for", "i", ",", "(", "word1", ",", "word2", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "dico", "[", "i", ",", "0", "]", "=", "word2id1", "[", "word1", "]", "\n", "dico", "[", "i", ",", "1", "]", "=", "word2id2", "[", "word2", "]", "\n", "\n", "", "return", "dico", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.load_dictionary": [[44, 81], ["os.path.isfile", "logger.info", "sorted", "torch.LongTensor", "enumerate", "io.open", "enumerate", "len", "line.rstrip().split", "line.lower", "sorted.append", "int", "int", "len", "len", "line.rstrip", "set"], "function", ["None"], ["", "def", "load_dictionary", "(", "path", ",", "word2id1", ",", "word2id2", ")", ":", "\n", "    ", "\"\"\"\n    Return a torch tensor of size (n, 2) where n is the size of the\n    loader dictionary, and sort it by source word frequency.\n    \"\"\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "\n", "pairs", "=", "[", "]", "\n", "not_found", "=", "0", "\n", "not_found1", "=", "0", "\n", "not_found2", "=", "0", "\n", "\n", "with", "io", ".", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "_", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "assert", "line", "==", "line", ".", "lower", "(", ")", "\n", "word1", ",", "word2", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", "\n", "if", "word1", "in", "word2id1", "and", "word2", "in", "word2id2", ":", "\n", "                ", "pairs", ".", "append", "(", "(", "word1", ",", "word2", ")", ")", "\n", "", "else", ":", "\n", "                ", "not_found", "+=", "1", "\n", "not_found1", "+=", "int", "(", "word1", "not", "in", "word2id1", ")", "\n", "not_found2", "+=", "int", "(", "word2", "not", "in", "word2id2", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"Found %i pairs of words in the dictionary (%i unique). \"", "\n", "\"%i other pairs contained at least one unknown word \"", "\n", "\"(%i in lang1, %i in lang2)\"", "\n", "%", "(", "len", "(", "pairs", ")", ",", "len", "(", "set", "(", "[", "x", "for", "x", ",", "_", "in", "pairs", "]", ")", ")", ",", "\n", "not_found", ",", "not_found1", ",", "not_found2", ")", ")", "\n", "\n", "# sort the dictionary by source word frequencies", "\n", "pairs", "=", "sorted", "(", "pairs", ",", "key", "=", "lambda", "x", ":", "word2id1", "[", "x", "[", "0", "]", "]", ")", "\n", "dico", "=", "torch", ".", "LongTensor", "(", "len", "(", "pairs", ")", ",", "2", ")", "\n", "for", "i", ",", "(", "word1", ",", "word2", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "dico", "[", "i", ",", "0", "]", "=", "word2id1", "[", "word1", "]", "\n", "dico", "[", "i", ",", "1", "]", "=", "word2id2", "[", "word2", "]", "\n", "\n", "", "return", "dico", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.get_word_translation_accuracy": [[83, 155], ["word_translation.load_dictionary", "os.path.join", "load_dictionary.cuda", "dico[].max", "emb1.size", "dico[].max", "emb2.size", "emb1.norm().expand_as", "emb2.norm().expand_as", "query.mm", "method.startswith", "query.mm.topk", "enumerate", "logger.info", "results.append", "emb2.transpose", "float", "range", "torch.cat", "method.startswith", "dico[].cpu().numpy", "min", "numpy.mean", "emb1.norm", "emb2.norm", "emb2.size", "emb1.mm", "query.mm.mul_().exp_", "query.mm.div_", "word_scores.append", "int.isdigit", "int", "utils.get_nn_avg_dist", "utils.get_nn_avg_dist", "torch.from_numpy().type_as", "torch.from_numpy().type_as", "query.mm", "query.mm.mul_", "query.mm.sub_", "query.mm.sub_", "Exception", "list", "emb2[].transpose", "query.mm.sum().expand_as", "query.mm.index_select", "emb2.transpose", "[].expand_as", "dico[].cpu", "matching.get", "matching.values", "len", "len", "query.mm.mul_", "len", "torch.from_numpy", "torch.from_numpy", "query.mm.sum"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.load_dictionary", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist"], ["", "def", "get_word_translation_accuracy", "(", "lang1", ",", "word2id1", ",", "emb1", ",", "lang2", ",", "word2id2", ",", "emb2", ",", "method", ",", "dico_eval", ")", ":", "\n", "    ", "\"\"\"\n    Given source and target word embeddings, and a dictionary,\n    evaluate the translation accuracy using the precision@k.\n    \"\"\"", "\n", "if", "dico_eval", "==", "'default'", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "DIC_EVAL_PATH", ",", "'%s-%s.5000-6500.txt'", "%", "(", "lang1", ",", "lang2", ")", ")", "\n", "", "else", ":", "\n", "        ", "path", "=", "dico_eval", "\n", "", "dico", "=", "load_dictionary", "(", "path", ",", "word2id1", ",", "word2id2", ")", "\n", "dico", "=", "dico", ".", "cuda", "(", ")", "if", "emb1", ".", "is_cuda", "else", "dico", "\n", "\n", "assert", "dico", "[", ":", ",", "0", "]", ".", "max", "(", ")", "<", "emb1", ".", "size", "(", "0", ")", "\n", "assert", "dico", "[", ":", ",", "1", "]", ".", "max", "(", ")", "<", "emb2", ".", "size", "(", "0", ")", "\n", "\n", "# normalize word embeddings", "\n", "emb1", "=", "emb1", "/", "emb1", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "emb1", ")", "\n", "emb2", "=", "emb2", "/", "emb2", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "emb2", ")", "\n", "\n", "# nearest neighbors", "\n", "if", "method", "==", "'nn'", ":", "\n", "        ", "query", "=", "emb1", "[", "dico", "[", ":", ",", "0", "]", "]", "\n", "scores", "=", "query", ".", "mm", "(", "emb2", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "\n", "# inverted softmax", "\n", "", "elif", "method", ".", "startswith", "(", "'invsm_beta_'", ")", ":", "\n", "        ", "beta", "=", "float", "(", "method", "[", "len", "(", "'invsm_beta_'", ")", ":", "]", ")", "\n", "bs", "=", "128", "\n", "word_scores", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "emb2", ".", "size", "(", "0", ")", ",", "bs", ")", ":", "\n", "            ", "scores", "=", "emb1", ".", "mm", "(", "emb2", "[", "i", ":", "i", "+", "bs", "]", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "scores", ".", "mul_", "(", "beta", ")", ".", "exp_", "(", ")", "\n", "scores", ".", "div_", "(", "scores", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "scores", ")", ")", "\n", "word_scores", ".", "append", "(", "scores", ".", "index_select", "(", "0", ",", "dico", "[", ":", ",", "0", "]", ")", ")", "\n", "", "scores", "=", "torch", ".", "cat", "(", "word_scores", ",", "1", ")", "\n", "\n", "# contextual dissimilarity measure", "\n", "", "elif", "method", ".", "startswith", "(", "'csls_knn_'", ")", ":", "\n", "# average distances to k nearest neighbors", "\n", "        ", "knn", "=", "method", "[", "len", "(", "'csls_knn_'", ")", ":", "]", "\n", "assert", "knn", ".", "isdigit", "(", ")", "\n", "knn", "=", "int", "(", "knn", ")", "\n", "average_dist1", "=", "get_nn_avg_dist", "(", "emb2", ",", "emb1", ",", "knn", ")", "\n", "average_dist2", "=", "get_nn_avg_dist", "(", "emb1", ",", "emb2", ",", "knn", ")", "\n", "average_dist1", "=", "torch", ".", "from_numpy", "(", "average_dist1", ")", ".", "type_as", "(", "emb1", ")", "\n", "average_dist2", "=", "torch", ".", "from_numpy", "(", "average_dist2", ")", ".", "type_as", "(", "emb2", ")", "\n", "# queries / scores", "\n", "query", "=", "emb1", "[", "dico", "[", ":", ",", "0", "]", "]", "\n", "scores", "=", "query", ".", "mm", "(", "emb2", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "scores", ".", "mul_", "(", "2", ")", "\n", "scores", ".", "sub_", "(", "average_dist1", "[", "dico", "[", ":", ",", "0", "]", "]", "[", ":", ",", "None", "]", ")", "\n", "scores", ".", "sub_", "(", "average_dist2", "[", "None", ",", ":", "]", ")", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Unknown method: \"%s\"'", "%", "method", ")", "\n", "\n", "", "results", "=", "[", "]", "\n", "top_matches", "=", "scores", ".", "topk", "(", "10", ",", "1", ",", "True", ")", "[", "1", "]", "\n", "for", "k", "in", "[", "1", ",", "5", ",", "10", "]", ":", "\n", "        ", "top_k_matches", "=", "top_matches", "[", ":", ",", ":", "k", "]", "\n", "_matching", "=", "(", "top_k_matches", "==", "dico", "[", ":", ",", "1", "]", "[", ":", ",", "None", "]", ".", "expand_as", "(", "top_k_matches", ")", ")", ".", "sum", "(", "1", ")", "\n", "# allow for multiple possible translations", "\n", "matching", "=", "{", "}", "\n", "for", "i", ",", "src_id", "in", "enumerate", "(", "dico", "[", ":", ",", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ":", "\n", "            ", "matching", "[", "src_id", "]", "=", "min", "(", "matching", ".", "get", "(", "src_id", ",", "0", ")", "+", "_matching", "[", "i", "]", ",", "1", ")", "\n", "# evaluate precision@k", "\n", "", "precision_at_k", "=", "100", "*", "np", ".", "mean", "(", "list", "(", "matching", ".", "values", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"%i source words - %s - Precision at k = %i: %f\"", "%", "\n", "(", "len", "(", "matching", ")", ",", "method", ",", "k", ",", "precision_at_k", ")", ")", "\n", "results", ".", "append", "(", "(", "'precision_at_%i'", "%", "k", ",", "precision_at_k", ")", ")", "\n", "\n", "", "return", "results", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_pairs": [[23, 41], ["os.path.isfile", "io.open", "type", "line.split.rstrip", "line.split.split", "word_pairs.append", "line.split.lower", "len", "len", "float", "os.path.basename"], "function", ["None"], ["def", "get_word_pairs", "(", "path", ",", "lower", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Return a list of (word1, word2, score) tuples from a word similarity file.\n    \"\"\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "and", "type", "(", "lower", ")", "is", "bool", "\n", "word_pairs", "=", "[", "]", "\n", "with", "io", ".", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "line", "=", "line", ".", "lower", "(", ")", "if", "lower", "else", "line", "\n", "line", "=", "line", ".", "split", "(", ")", "\n", "# ignore phrases, only consider words", "\n", "if", "len", "(", "line", ")", "!=", "3", ":", "\n", "                ", "assert", "len", "(", "line", ")", ">", "3", "\n", "assert", "'SEMEVAL17'", "in", "os", ".", "path", ".", "basename", "(", "path", ")", "or", "'EN-IT_MWS353'", "in", "path", "\n", "continue", "\n", "", "word_pairs", ".", "append", "(", "(", "line", "[", "0", "]", ",", "line", "[", "1", "]", ",", "float", "(", "line", "[", "2", "]", ")", ")", ")", "\n", "", "", "return", "word_pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id": [[43, 56], ["word2id.get", "type", "word2id.get", "word2id.get", "word.capitalize", "word.title"], "function", ["None"], ["", "def", "get_word_id", "(", "word", ",", "word2id", ",", "lower", ")", ":", "\n", "    ", "\"\"\"\n    Get a word ID.\n    If the model does not use lowercase and the evaluation file is lowercased,\n    we might be able to find an associated word.\n    \"\"\"", "\n", "assert", "type", "(", "lower", ")", "is", "bool", "\n", "word_id", "=", "word2id", ".", "get", "(", "word", ")", "\n", "if", "word_id", "is", "None", "and", "not", "lower", ":", "\n", "        ", "word_id", "=", "word2id", ".", "get", "(", "word", ".", "capitalize", "(", ")", ")", "\n", "", "if", "word_id", "is", "None", "and", "not", "lower", ":", "\n", "        ", "word_id", "=", "word2id", ".", "get", "(", "word", ".", "title", "(", ")", ")", "\n", "", "return", "word_id", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_spearman_rho": [[58, 85], ["wordsim.get_word_pairs", "len", "len", "type", "wordsim.get_word_id", "wordsim.get_word_id", "gold.append", "pred.append", "len", "u.dot", "scipy.stats.spearmanr", "numpy.linalg.norm", "numpy.linalg.norm"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_pairs", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id"], ["", "def", "get_spearman_rho", "(", "word2id1", ",", "embeddings1", ",", "path", ",", "lower", ",", "\n", "word2id2", "=", "None", ",", "embeddings2", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Compute monolingual or cross-lingual word similarity score.\n    \"\"\"", "\n", "assert", "not", "(", "(", "word2id2", "is", "None", ")", "^", "(", "embeddings2", "is", "None", ")", ")", "\n", "word2id2", "=", "word2id1", "if", "word2id2", "is", "None", "else", "word2id2", "\n", "embeddings2", "=", "embeddings1", "if", "embeddings2", "is", "None", "else", "embeddings2", "\n", "assert", "len", "(", "word2id1", ")", "==", "embeddings1", ".", "shape", "[", "0", "]", "\n", "assert", "len", "(", "word2id2", ")", "==", "embeddings2", ".", "shape", "[", "0", "]", "\n", "assert", "type", "(", "lower", ")", "is", "bool", "\n", "word_pairs", "=", "get_word_pairs", "(", "path", ")", "\n", "not_found", "=", "0", "\n", "pred", "=", "[", "]", "\n", "gold", "=", "[", "]", "\n", "for", "word1", ",", "word2", ",", "similarity", "in", "word_pairs", ":", "\n", "        ", "id1", "=", "get_word_id", "(", "word1", ",", "word2id1", ",", "lower", ")", "\n", "id2", "=", "get_word_id", "(", "word2", ",", "word2id2", ",", "lower", ")", "\n", "if", "id1", "is", "None", "or", "id2", "is", "None", ":", "\n", "            ", "not_found", "+=", "1", "\n", "continue", "\n", "", "u", "=", "embeddings1", "[", "id1", "]", "\n", "v", "=", "embeddings2", "[", "id2", "]", "\n", "score", "=", "u", ".", "dot", "(", "v", ")", "/", "(", "np", ".", "linalg", ".", "norm", "(", "u", ")", "*", "np", ".", "linalg", ".", "norm", "(", "v", ")", ")", "\n", "gold", ".", "append", "(", "similarity", ")", "\n", "pred", ".", "append", "(", "score", ")", "\n", "", "return", "spearmanr", "(", "gold", ",", "pred", ")", ".", "correlation", ",", "len", "(", "gold", ")", ",", "not_found", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_wordsim_scores": [[87, 111], ["os.path.join", "logger.info", "logger.info", "logger.info", "list", "logger.info", "os.path.isdir", "os.listdir", "filename.startswith", "os.path.join", "wordsim.get_spearman_rho", "logger.info", "language.upper", "str", "str"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_spearman_rho"], ["", "def", "get_wordsim_scores", "(", "language", ",", "word2id", ",", "embeddings", ",", "lower", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Return monolingual word similarity scores.\n    \"\"\"", "\n", "dirpath", "=", "os", ".", "path", ".", "join", "(", "MONOLINGUAL_EVAL_PATH", ",", "language", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "dirpath", ")", ":", "\n", "        ", "return", "None", "\n", "\n", "", "scores", "=", "{", "}", "\n", "separator", "=", "\"=\"", "*", "(", "30", "+", "1", "+", "10", "+", "1", "+", "13", "+", "1", "+", "12", ")", "\n", "pattern", "=", "\"%30s %10s %13s %12s\"", "\n", "logger", ".", "info", "(", "separator", ")", "\n", "logger", ".", "info", "(", "pattern", "%", "(", "\"Dataset\"", ",", "\"Found\"", ",", "\"Not found\"", ",", "\"Rho\"", ")", ")", "\n", "logger", ".", "info", "(", "separator", ")", "\n", "\n", "for", "filename", "in", "list", "(", "os", ".", "listdir", "(", "dirpath", ")", ")", ":", "\n", "        ", "if", "filename", ".", "startswith", "(", "'%s_'", "%", "(", "language", ".", "upper", "(", ")", ")", ")", ":", "\n", "            ", "filepath", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", "\n", "coeff", ",", "found", ",", "not_found", "=", "get_spearman_rho", "(", "word2id", ",", "embeddings", ",", "filepath", ",", "lower", ")", "\n", "logger", ".", "info", "(", "pattern", "%", "(", "filename", "[", ":", "-", "4", "]", ",", "str", "(", "found", ")", ",", "str", "(", "not_found", ")", ",", "\"%.4f\"", "%", "coeff", ")", ")", "\n", "scores", "[", "filename", "[", ":", "-", "4", "]", "]", "=", "coeff", "\n", "", "", "logger", ".", "info", "(", "separator", ")", "\n", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_wordanalogy_scores": [[113, 196], ["os.path.join", "logger.info", "logger.info", "logger.info", "sorted", "logger.info", "io.open", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy.mm().cpu().numpy", "enumerate", "numpy.sum", "scores.keys", "logger.info", "os.path.isdir", "numpy.sqrt", "os.path.join", "line.lower.rstrip", "line.lower.split", "wordsim.get_word_id", "wordsim.get_word_id", "wordsim.get_word_id", "wordsim.get_word_id", "any", "numpy.vstack", "float", "max", "line.lower.lower", "len", "word_ids[].append", "queries[].append", "torch.from_numpy.mm().cpu", "qs.mm().cpu().numpy.argmax", "line.lower.split", "numpy.linalg.norm", "str", "str", "torch.from_numpy.mm"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_word_id"], ["", "def", "get_wordanalogy_scores", "(", "language", ",", "word2id", ",", "embeddings", ",", "lower", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Return (english) word analogy score\n    \"\"\"", "\n", "dirpath", "=", "os", ".", "path", ".", "join", "(", "MONOLINGUAL_EVAL_PATH", ",", "language", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "dirpath", ")", "or", "language", "not", "in", "[", "\"en\"", "]", ":", "\n", "        ", "return", "None", "\n", "\n", "# normalize word embeddings", "\n", "", "embeddings", "=", "embeddings", "/", "np", ".", "sqrt", "(", "(", "embeddings", "**", "2", ")", ".", "sum", "(", "1", ")", ")", "[", ":", ",", "None", "]", "\n", "\n", "# scores by category", "\n", "scores", "=", "{", "}", "\n", "\n", "word_ids", "=", "{", "}", "\n", "queries", "=", "{", "}", "\n", "\n", "with", "io", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dirpath", ",", "'questions-words.txt'", ")", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "# new line", "\n", "            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "if", "lower", ":", "\n", "                ", "line", "=", "line", ".", "lower", "(", ")", "\n", "\n", "# new category", "\n", "", "if", "\":\"", "in", "line", ":", "\n", "                ", "assert", "line", "[", "1", "]", "==", "' '", "\n", "category", "=", "line", "[", "2", ":", "]", "\n", "assert", "category", "not", "in", "scores", "\n", "scores", "[", "category", "]", "=", "{", "'n_found'", ":", "0", ",", "'n_not_found'", ":", "0", ",", "'n_correct'", ":", "0", "}", "\n", "word_ids", "[", "category", "]", "=", "[", "]", "\n", "queries", "[", "category", "]", "=", "[", "]", "\n", "continue", "\n", "\n", "# get word IDs", "\n", "", "assert", "len", "(", "line", ".", "split", "(", ")", ")", "==", "4", ",", "line", "\n", "word1", ",", "word2", ",", "word3", ",", "word4", "=", "line", ".", "split", "(", ")", "\n", "word_id1", "=", "get_word_id", "(", "word1", ",", "word2id", ",", "lower", ")", "\n", "word_id2", "=", "get_word_id", "(", "word2", ",", "word2id", ",", "lower", ")", "\n", "word_id3", "=", "get_word_id", "(", "word3", ",", "word2id", ",", "lower", ")", "\n", "word_id4", "=", "get_word_id", "(", "word4", ",", "word2id", ",", "lower", ")", "\n", "\n", "# if at least one word is not found", "\n", "if", "any", "(", "x", "is", "None", "for", "x", "in", "[", "word_id1", ",", "word_id2", ",", "word_id3", ",", "word_id4", "]", ")", ":", "\n", "                ", "scores", "[", "category", "]", "[", "'n_not_found'", "]", "+=", "1", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "scores", "[", "category", "]", "[", "'n_found'", "]", "+=", "1", "\n", "word_ids", "[", "category", "]", ".", "append", "(", "[", "word_id1", ",", "word_id2", ",", "word_id3", ",", "word_id4", "]", ")", "\n", "# generate query vector and get nearest neighbors", "\n", "query", "=", "embeddings", "[", "word_id1", "]", "-", "embeddings", "[", "word_id2", "]", "+", "embeddings", "[", "word_id4", "]", "\n", "query", "=", "query", "/", "np", ".", "linalg", ".", "norm", "(", "query", ")", "\n", "\n", "queries", "[", "category", "]", ".", "append", "(", "query", ")", "\n", "\n", "# Compute score for each category", "\n", "", "", "", "for", "cat", "in", "queries", ":", "\n", "        ", "qs", "=", "torch", ".", "from_numpy", "(", "np", ".", "vstack", "(", "queries", "[", "cat", "]", ")", ")", "\n", "keys", "=", "torch", ".", "from_numpy", "(", "embeddings", ".", "T", ")", "\n", "values", "=", "qs", ".", "mm", "(", "keys", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# be sure we do not select input words", "\n", "for", "i", ",", "ws", "in", "enumerate", "(", "word_ids", "[", "cat", "]", ")", ":", "\n", "            ", "for", "wid", "in", "[", "ws", "[", "0", "]", ",", "ws", "[", "1", "]", ",", "ws", "[", "3", "]", "]", ":", "\n", "                ", "values", "[", "i", ",", "wid", "]", "=", "-", "1e9", "\n", "", "", "scores", "[", "cat", "]", "[", "'n_correct'", "]", "=", "np", ".", "sum", "(", "values", ".", "argmax", "(", "axis", "=", "1", ")", "==", "[", "ws", "[", "2", "]", "for", "ws", "in", "word_ids", "[", "cat", "]", "]", ")", "\n", "\n", "# pretty print", "\n", "", "separator", "=", "\"=\"", "*", "(", "30", "+", "1", "+", "10", "+", "1", "+", "13", "+", "1", "+", "12", ")", "\n", "pattern", "=", "\"%30s %10s %13s %12s\"", "\n", "logger", ".", "info", "(", "separator", ")", "\n", "logger", ".", "info", "(", "pattern", "%", "(", "\"Category\"", ",", "\"Found\"", ",", "\"Not found\"", ",", "\"Accuracy\"", ")", ")", "\n", "logger", ".", "info", "(", "separator", ")", "\n", "\n", "# compute and log accuracies", "\n", "accuracies", "=", "{", "}", "\n", "for", "k", "in", "sorted", "(", "scores", ".", "keys", "(", ")", ")", ":", "\n", "        ", "v", "=", "scores", "[", "k", "]", "\n", "accuracies", "[", "k", "]", "=", "float", "(", "v", "[", "'n_correct'", "]", ")", "/", "max", "(", "v", "[", "'n_found'", "]", ",", "1", ")", "\n", "logger", ".", "info", "(", "pattern", "%", "(", "k", ",", "str", "(", "v", "[", "'n_found'", "]", ")", ",", "str", "(", "v", "[", "'n_not_found'", "]", ")", ",", "\"%.4f\"", "%", "accuracies", "[", "k", "]", ")", ")", "\n", "", "logger", ".", "info", "(", "separator", ")", "\n", "\n", "return", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_crosslingual_wordsim_scores": [[198, 234], ["os.path.join", "os.path.join", "os.path.exists", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "wordsim.get_spearman_rho", "os.path.exists", "os.path.exists", "os.path.exists", "wordsim.get_spearman_rho", "lang1.upper", "lang2.upper", "str", "str"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_spearman_rho", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_spearman_rho"], ["", "def", "get_crosslingual_wordsim_scores", "(", "lang1", ",", "word2id1", ",", "embeddings1", ",", "\n", "lang2", ",", "word2id2", ",", "embeddings2", ",", "lower", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Return cross-lingual word similarity scores.\n    \"\"\"", "\n", "f1", "=", "os", ".", "path", ".", "join", "(", "SEMEVAL17_EVAL_PATH", ",", "'%s-%s-SEMEVAL17.txt'", "%", "(", "lang1", ",", "lang2", ")", ")", "\n", "f2", "=", "os", ".", "path", ".", "join", "(", "SEMEVAL17_EVAL_PATH", ",", "'%s-%s-SEMEVAL17.txt'", "%", "(", "lang2", ",", "lang1", ")", ")", "\n", "if", "not", "(", "os", ".", "path", ".", "exists", "(", "f1", ")", "or", "os", ".", "path", ".", "exists", "(", "f2", ")", ")", ":", "\n", "        ", "return", "None", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "f1", ")", ":", "\n", "        ", "coeff", ",", "found", ",", "not_found", "=", "get_spearman_rho", "(", "\n", "word2id1", ",", "embeddings1", ",", "f1", ",", "\n", "lower", ",", "word2id2", ",", "embeddings2", "\n", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "f2", ")", ":", "\n", "        ", "coeff", ",", "found", ",", "not_found", "=", "get_spearman_rho", "(", "\n", "word2id2", ",", "embeddings2", ",", "f2", ",", "\n", "lower", ",", "word2id1", ",", "embeddings1", "\n", ")", "\n", "\n", "", "scores", "=", "{", "}", "\n", "separator", "=", "\"=\"", "*", "(", "30", "+", "1", "+", "10", "+", "1", "+", "13", "+", "1", "+", "12", ")", "\n", "pattern", "=", "\"%30s %10s %13s %12s\"", "\n", "logger", ".", "info", "(", "separator", ")", "\n", "logger", ".", "info", "(", "pattern", "%", "(", "\"Dataset\"", ",", "\"Found\"", ",", "\"Not found\"", ",", "\"Rho\"", ")", ")", "\n", "logger", ".", "info", "(", "separator", ")", "\n", "\n", "task_name", "=", "'%s_%s_SEMEVAL17'", "%", "(", "lang1", ".", "upper", "(", ")", ",", "lang2", ".", "upper", "(", ")", ")", "\n", "logger", ".", "info", "(", "pattern", "%", "(", "task_name", ",", "str", "(", "found", ")", ",", "str", "(", "not_found", ")", ",", "\"%.4f\"", "%", "coeff", ")", ")", "\n", "scores", "[", "task_name", "]", "=", "coeff", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "None", "\n", "", "logger", ".", "info", "(", "separator", ")", "\n", "\n", "return", "scores", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.__init__": [[27, 38], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "\"\"\"\n        Initialize evaluator.\n        \"\"\"", "\n", "self", ".", "src_emb", "=", "trainer", ".", "src_emb", "\n", "self", ".", "tgt_emb", "=", "trainer", ".", "tgt_emb", "\n", "self", ".", "src_dico", "=", "trainer", ".", "src_dico", "\n", "self", ".", "tgt_dico", "=", "trainer", ".", "tgt_dico", "\n", "self", ".", "mapping", "=", "trainer", ".", "mapping", "\n", "self", ".", "discriminator", "=", "trainer", ".", "discriminator", "\n", "self", ".", "params", "=", "trainer", ".", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.monolingual_wordsim": [[39, 65], ["get_wordsim_scores", "evaluator.Evaluator.mapping().data.cpu().numpy", "get_wordsim_scores", "numpy.mean", "logger.info", "to_log.update", "numpy.mean", "logger.info", "to_log.update", "logger.info", "evaluator.Evaluator.tgt_emb.weight.data.cpu().numpy", "list", "list", "evaluator.Evaluator.mapping().data.cpu", "get_wordsim_scores.values", "tgt_ws_scores.values", "evaluator.Evaluator.tgt_emb.weight.data.cpu", "get_wordsim_scores.items", "tgt_ws_scores.items", "evaluator.Evaluator.mapping"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_wordsim_scores", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_wordsim_scores"], ["", "def", "monolingual_wordsim", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Evaluation on monolingual word similarity.\n        \"\"\"", "\n", "src_ws_scores", "=", "get_wordsim_scores", "(", "\n", "self", ".", "src_dico", ".", "lang", ",", "self", ".", "src_dico", ".", "word2id", ",", "\n", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "tgt_ws_scores", "=", "get_wordsim_scores", "(", "\n", "self", ".", "tgt_dico", ".", "lang", ",", "self", ".", "tgt_dico", ".", "word2id", ",", "\n", "self", ".", "tgt_emb", ".", "weight", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "if", "self", ".", "params", ".", "tgt_lang", "else", "None", "\n", "if", "src_ws_scores", "is", "not", "None", ":", "\n", "            ", "src_ws_monolingual_scores", "=", "np", ".", "mean", "(", "list", "(", "src_ws_scores", ".", "values", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Monolingual source word similarity score average: %.5f\"", "%", "src_ws_monolingual_scores", ")", "\n", "to_log", "[", "'src_ws_monolingual_scores'", "]", "=", "src_ws_monolingual_scores", "\n", "to_log", ".", "update", "(", "{", "'src_'", "+", "k", ":", "v", "for", "k", ",", "v", "in", "src_ws_scores", ".", "items", "(", ")", "}", ")", "\n", "", "if", "tgt_ws_scores", "is", "not", "None", ":", "\n", "            ", "tgt_ws_monolingual_scores", "=", "np", ".", "mean", "(", "list", "(", "tgt_ws_scores", ".", "values", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Monolingual target word similarity score average: %.5f\"", "%", "tgt_ws_monolingual_scores", ")", "\n", "to_log", "[", "'tgt_ws_monolingual_scores'", "]", "=", "tgt_ws_monolingual_scores", "\n", "to_log", ".", "update", "(", "{", "'tgt_'", "+", "k", ":", "v", "for", "k", ",", "v", "in", "tgt_ws_scores", ".", "items", "(", ")", "}", ")", "\n", "", "if", "src_ws_scores", "is", "not", "None", "and", "tgt_ws_scores", "is", "not", "None", ":", "\n", "            ", "ws_monolingual_scores", "=", "(", "src_ws_monolingual_scores", "+", "tgt_ws_monolingual_scores", ")", "/", "2", "\n", "logger", ".", "info", "(", "\"Monolingual word similarity score average: %.5f\"", "%", "ws_monolingual_scores", ")", "\n", "to_log", "[", "'ws_monolingual_scores'", "]", "=", "ws_monolingual_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.monolingual_wordanalogy": [[66, 89], ["get_wordanalogy_scores", "evaluator.Evaluator.mapping().data.cpu().numpy", "get_wordanalogy_scores", "numpy.mean", "logger.info", "to_log.update", "numpy.mean", "logger.info", "to_log.update", "evaluator.Evaluator.tgt_emb.weight.data.cpu().numpy", "list", "list", "evaluator.Evaluator.mapping().data.cpu", "get_wordanalogy_scores.values", "get_wordanalogy_scores.values", "evaluator.Evaluator.tgt_emb.weight.data.cpu", "get_wordanalogy_scores.items", "get_wordanalogy_scores.items", "evaluator.Evaluator.mapping"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_wordanalogy_scores", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_wordanalogy_scores"], ["", "", "def", "monolingual_wordanalogy", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Evaluation on monolingual word analogy.\n        \"\"\"", "\n", "src_analogy_scores", "=", "get_wordanalogy_scores", "(", "\n", "self", ".", "src_dico", ".", "lang", ",", "self", ".", "src_dico", ".", "word2id", ",", "\n", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "if", "self", ".", "params", ".", "tgt_lang", ":", "\n", "            ", "tgt_analogy_scores", "=", "get_wordanalogy_scores", "(", "\n", "self", ".", "tgt_dico", ".", "lang", ",", "self", ".", "tgt_dico", ".", "word2id", ",", "\n", "self", ".", "tgt_emb", ".", "weight", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "", "if", "src_analogy_scores", "is", "not", "None", ":", "\n", "            ", "src_analogy_monolingual_scores", "=", "np", ".", "mean", "(", "list", "(", "src_analogy_scores", ".", "values", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Monolingual source word analogy score average: %.5f\"", "%", "src_analogy_monolingual_scores", ")", "\n", "to_log", "[", "'src_analogy_monolingual_scores'", "]", "=", "src_analogy_monolingual_scores", "\n", "to_log", ".", "update", "(", "{", "'src_'", "+", "k", ":", "v", "for", "k", ",", "v", "in", "src_analogy_scores", ".", "items", "(", ")", "}", ")", "\n", "", "if", "self", ".", "params", ".", "tgt_lang", "and", "tgt_analogy_scores", "is", "not", "None", ":", "\n", "            ", "tgt_analogy_monolingual_scores", "=", "np", ".", "mean", "(", "list", "(", "tgt_analogy_scores", ".", "values", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Monolingual target word analogy score average: %.5f\"", "%", "tgt_analogy_monolingual_scores", ")", "\n", "to_log", "[", "'tgt_analogy_monolingual_scores'", "]", "=", "tgt_analogy_monolingual_scores", "\n", "to_log", ".", "update", "(", "{", "'tgt_'", "+", "k", ":", "v", "for", "k", ",", "v", "in", "tgt_analogy_scores", ".", "items", "(", ")", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.crosslingual_wordsim": [[90, 107], ["evaluator.Evaluator.mapping().data.cpu().numpy", "evaluator.Evaluator.tgt_emb.weight.data.cpu().numpy", "get_crosslingual_wordsim_scores", "numpy.mean", "logger.info", "to_log.update", "list", "evaluator.Evaluator.mapping().data.cpu", "evaluator.Evaluator.tgt_emb.weight.data.cpu", "get_crosslingual_wordsim_scores.values", "get_crosslingual_wordsim_scores.items", "evaluator.Evaluator.mapping"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.wordsim.get_crosslingual_wordsim_scores"], ["", "", "def", "crosslingual_wordsim", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Evaluation on cross-lingual word similarity.\n        \"\"\"", "\n", "src_emb", "=", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# cross-lingual wordsim evaluation", "\n", "src_tgt_ws_scores", "=", "get_crosslingual_wordsim_scores", "(", "\n", "self", ".", "src_dico", ".", "lang", ",", "self", ".", "src_dico", ".", "word2id", ",", "src_emb", ",", "\n", "self", ".", "tgt_dico", ".", "lang", ",", "self", ".", "tgt_dico", ".", "word2id", ",", "tgt_emb", ",", "\n", ")", "\n", "if", "src_tgt_ws_scores", "is", "None", ":", "\n", "            ", "return", "\n", "", "ws_crosslingual_scores", "=", "np", ".", "mean", "(", "list", "(", "src_tgt_ws_scores", ".", "values", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Cross-lingual word similarity score average: %.5f\"", "%", "ws_crosslingual_scores", ")", "\n", "to_log", "[", "'ws_crosslingual_scores'", "]", "=", "ws_crosslingual_scores", "\n", "to_log", ".", "update", "(", "{", "'src_tgt_'", "+", "k", ":", "v", "for", "k", ",", "v", "in", "src_tgt_ws_scores", ".", "items", "(", ")", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.word_translation": [[108, 126], ["evaluator.Evaluator.mapping", "get_word_translation_accuracy", "to_log.update"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.word_translation.get_word_translation_accuracy"], ["", "def", "word_translation", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Evaluation on word translation.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "dico_eval", "==", "\"default\"", ":", "\n", "            ", "return", "\n", "# mapped word embeddings", "\n", "", "src_emb", "=", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", "\n", "\n", "for", "method", "in", "[", "'nn'", ",", "'csls_knn_10'", "]", ":", "\n", "            ", "results", "=", "get_word_translation_accuracy", "(", "\n", "self", ".", "src_dico", ".", "lang", ",", "self", ".", "src_dico", ".", "word2id", ",", "src_emb", ",", "\n", "self", ".", "tgt_dico", ".", "lang", ",", "self", ".", "tgt_dico", ".", "word2id", ",", "tgt_emb", ",", "\n", "method", "=", "method", ",", "\n", "dico_eval", "=", "self", ".", "params", ".", "dico_eval", "\n", ")", "\n", "to_log", ".", "update", "(", "[", "(", "'%s-%s'", "%", "(", "k", ",", "method", ")", ",", "v", ")", "for", "k", ",", "v", "in", "results", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.sent_translation": [[127, 178], ["src.utils.get_idf", "hasattr", "load_europarl_data", "evaluator.Evaluator.mapping", "get_sent_translation_accuracy", "to_log.update", "get_sent_translation_accuracy", "to_log.update"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_idf", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.sent_translation.load_europarl_data", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.sent_translation.get_sent_translation_accuracy", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.sent_translation.get_sent_translation_accuracy"], ["", "", "def", "sent_translation", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Evaluation on sentence translation.\n        Only available on Europarl, for en - {de, es, fr, it} language pairs.\n        \"\"\"", "\n", "lg1", "=", "self", ".", "src_dico", ".", "lang", "\n", "lg2", "=", "self", ".", "tgt_dico", ".", "lang", "\n", "\n", "# parameters", "\n", "n_keys", "=", "200000", "\n", "n_queries", "=", "2000", "\n", "n_idf", "=", "300000", "\n", "\n", "# load europarl data", "\n", "if", "not", "hasattr", "(", "self", ",", "'europarl_data'", ")", ":", "\n", "            ", "self", ".", "europarl_data", "=", "load_europarl_data", "(", "\n", "lg1", ",", "lg2", ",", "n_max", "=", "(", "n_keys", "+", "2", "*", "n_idf", ")", "\n", ")", "\n", "\n", "# if no Europarl data for this language pair", "\n", "", "if", "not", "self", ".", "europarl_data", ":", "\n", "            ", "return", "\n", "\n", "# mapped word embeddings", "\n", "", "src_emb", "=", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", "\n", "\n", "# get idf weights", "\n", "idf", "=", "get_idf", "(", "self", ".", "europarl_data", ",", "lg1", ",", "lg2", ",", "n_idf", "=", "n_idf", ")", "\n", "\n", "for", "method", "in", "[", "'nn'", ",", "'csls_knn_10'", "]", ":", "\n", "\n", "# source <- target sentence translation", "\n", "            ", "results", "=", "get_sent_translation_accuracy", "(", "\n", "self", ".", "europarl_data", ",", "\n", "self", ".", "src_dico", ".", "lang", ",", "self", ".", "src_dico", ".", "word2id", ",", "src_emb", ",", "\n", "self", ".", "tgt_dico", ".", "lang", ",", "self", ".", "tgt_dico", ".", "word2id", ",", "tgt_emb", ",", "\n", "n_keys", "=", "n_keys", ",", "n_queries", "=", "n_queries", ",", "\n", "method", "=", "method", ",", "idf", "=", "idf", "\n", ")", "\n", "to_log", ".", "update", "(", "[", "(", "'tgt_to_src_%s-%s'", "%", "(", "k", ",", "method", ")", ",", "v", ")", "for", "k", ",", "v", "in", "results", "]", ")", "\n", "\n", "# target <- source sentence translation", "\n", "results", "=", "get_sent_translation_accuracy", "(", "\n", "self", ".", "europarl_data", ",", "\n", "self", ".", "tgt_dico", ".", "lang", ",", "self", ".", "tgt_dico", ".", "word2id", ",", "tgt_emb", ",", "\n", "self", ".", "src_dico", ".", "lang", ",", "self", ".", "src_dico", ".", "word2id", ",", "src_emb", ",", "\n", "n_keys", "=", "n_keys", ",", "n_queries", "=", "n_queries", ",", "\n", "method", "=", "method", ",", "idf", "=", "idf", "\n", ")", "\n", "to_log", ".", "update", "(", "[", "(", "'src_to_tgt_%s-%s'", "%", "(", "k", ",", "method", ")", ",", "v", ")", "for", "k", ",", "v", "in", "results", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.dist_mean_cosine": [[179, 216], ["evaluator.Evaluator.mapping", "src_emb.norm().expand_as", "tgt_emb.norm().expand_as", "copy.deepcopy", "dico_builder.get_candidates", "dico_builder.get_candidates", "dico_builder.build_dictionary", "logger.info", "isinstance", "mean_cosine.item", "src_emb.norm", "tgt_emb.norm"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.dico_builder.get_candidates", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.trainer.Trainer.build_dictionary"], ["", "", "def", "dist_mean_cosine", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Mean-cosine model selection criterion.\n        \"\"\"", "\n", "# get normalized embeddings", "\n", "src_emb", "=", "self", ".", "mapping", "(", "self", ".", "src_emb", ".", "weight", ")", ".", "data", "\n", "tgt_emb", "=", "self", ".", "tgt_emb", ".", "weight", ".", "data", "\n", "src_emb", "=", "src_emb", "/", "src_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "src_emb", ")", "\n", "tgt_emb", "=", "tgt_emb", "/", "tgt_emb", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "tgt_emb", ")", "\n", "\n", "# build dictionary", "\n", "for", "dico_method", "in", "[", "'nn'", ",", "'csls_knn_10'", "]", ":", "\n", "            ", "dico_build", "=", "'S2T'", "\n", "dico_max_size", "=", "10000", "\n", "# temp params / dictionary generation", "\n", "_params", "=", "deepcopy", "(", "self", ".", "params", ")", "\n", "_params", ".", "dico_method", "=", "dico_method", "\n", "_params", ".", "dico_build", "=", "dico_build", "\n", "_params", ".", "dico_threshold", "=", "0", "\n", "_params", ".", "dico_max_rank", "=", "10000", "\n", "_params", ".", "dico_min_size", "=", "0", "\n", "_params", ".", "dico_max_size", "=", "dico_max_size", "\n", "s2t_candidates", ",", "s2t_scores", "=", "get_candidates", "(", "src_emb", ",", "tgt_emb", ",", "_params", ")", "\n", "t2s_candidates", ",", "t2s_scores", "=", "get_candidates", "(", "tgt_emb", ",", "src_emb", ",", "_params", ")", "\n", "#print ('S2T:\\n',s2t_candidates, s2t_scores)", "\n", "#print ('T2S:\\n',t2s_candidates, t2s_scores)", "\n", "dico", "=", "build_dictionary", "(", "src_emb", ",", "tgt_emb", ",", "_params", ",", "s2t_candidates", ",", "t2s_candidates", ")", "\n", "#print ('Dico:\\n', dico)", "\n", "# mean cosine", "\n", "if", "dico", "is", "None", ":", "\n", "                ", "mean_cosine", "=", "-", "1e9", "\n", "", "else", ":", "\n", "                ", "mean_cosine", "=", "(", "src_emb", "[", "dico", "[", ":", "dico_max_size", ",", "0", "]", "]", "*", "tgt_emb", "[", "dico", "[", ":", "dico_max_size", ",", "1", "]", "]", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "", "mean_cosine", "=", "mean_cosine", ".", "item", "(", ")", "if", "isinstance", "(", "mean_cosine", ",", "torch_tensor", ")", "else", "mean_cosine", "\n", "logger", ".", "info", "(", "\"Mean cosine (%s method, %s build, %i max size): %.5f\"", "\n", "%", "(", "dico_method", ",", "_params", ".", "dico_build", ",", "dico_max_size", ",", "mean_cosine", ")", ")", "\n", "to_log", "[", "'mean_cosine-%s-%s-%i'", "%", "(", "dico_method", ",", "_params", ".", "dico_build", ",", "dico_max_size", ")", "]", "=", "mean_cosine", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.all_eval": [[217, 226], ["evaluator.Evaluator.monolingual_wordsim", "evaluator.Evaluator.crosslingual_wordsim", "evaluator.Evaluator.word_translation", "evaluator.Evaluator.sent_translation", "evaluator.Evaluator.dist_mean_cosine"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.monolingual_wordsim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.crosslingual_wordsim", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.word_translation", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.sent_translation", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.dist_mean_cosine"], ["", "", "def", "all_eval", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Run all evaluations.\n        \"\"\"", "\n", "self", ".", "monolingual_wordsim", "(", "to_log", ")", "\n", "self", ".", "crosslingual_wordsim", "(", "to_log", ")", "\n", "self", ".", "word_translation", "(", "to_log", ")", "\n", "self", ".", "sent_translation", "(", "to_log", ")", "\n", "self", ".", "dist_mean_cosine", "(", "to_log", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.evaluator.Evaluator.eval_dis": [[227, 264], ["evaluator.Evaluator.discriminator.eval", "range", "range", "numpy.mean", "numpy.mean", "logger.info", "numpy.mean", "numpy.mean", "logger.info", "evaluator.Evaluator.discriminator", "src_preds.extend", "evaluator.Evaluator.discriminator", "tgt_preds.extend", "torch.no_grad", "torch.autograd.Variable", "evaluator.Evaluator.mapping", "evaluator.Evaluator.data.cpu().tolist", "torch.no_grad", "torch.autograd.Variable", "evaluator.Evaluator.data.cpu().tolist", "evaluator.Evaluator.data.cpu", "evaluator.Evaluator.data.cpu"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.None.supervised_bert.SupervisedBert.eval"], ["", "def", "eval_dis", "(", "self", ",", "to_log", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate discriminator predictions and accuracy.\n        \"\"\"", "\n", "bs", "=", "128", "\n", "src_preds", "=", "[", "]", "\n", "tgt_preds", "=", "[", "]", "\n", "\n", "self", ".", "discriminator", ".", "eval", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "self", ".", "src_emb", ".", "num_embeddings", ",", "bs", ")", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "emb", "=", "Variable", "(", "self", ".", "src_emb", ".", "weight", "[", "i", ":", "i", "+", "bs", "]", ".", "data", ")", "\n", "", "preds", "=", "self", ".", "discriminator", "(", "self", ".", "mapping", "(", "emb", ")", ")", "\n", "src_preds", ".", "extend", "(", "preds", ".", "data", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "self", ".", "tgt_emb", ".", "num_embeddings", ",", "bs", ")", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "emb", "=", "Variable", "(", "self", ".", "tgt_emb", ".", "weight", "[", "i", ":", "i", "+", "bs", "]", ".", "data", ")", "\n", "", "preds", "=", "self", ".", "discriminator", "(", "emb", ")", "\n", "tgt_preds", ".", "extend", "(", "preds", ".", "data", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "src_pred", "=", "np", ".", "mean", "(", "src_preds", ")", "\n", "tgt_pred", "=", "np", ".", "mean", "(", "tgt_preds", ")", "\n", "logger", ".", "info", "(", "\"Discriminator source / target predictions: %.5f / %.5f\"", "\n", "%", "(", "src_pred", ",", "tgt_pred", ")", ")", "\n", "\n", "src_accu", "=", "np", ".", "mean", "(", "[", "x", ">=", "0.5", "for", "x", "in", "src_preds", "]", ")", "\n", "tgt_accu", "=", "np", ".", "mean", "(", "[", "x", "<", "0.5", "for", "x", "in", "tgt_preds", "]", ")", "\n", "dis_accu", "=", "(", "(", "src_accu", "*", "self", ".", "src_emb", ".", "num_embeddings", "+", "tgt_accu", "*", "self", ".", "tgt_emb", ".", "num_embeddings", ")", "/", "\n", "(", "self", ".", "src_emb", ".", "num_embeddings", "+", "self", ".", "tgt_emb", ".", "num_embeddings", ")", ")", "\n", "logger", ".", "info", "(", "\"Discriminator source / target / global accuracy: %.5f / %.5f / %.5f\"", "\n", "%", "(", "src_accu", ",", "tgt_accu", ",", "dis_accu", ")", ")", "\n", "\n", "to_log", "[", "'dis_accu'", "]", "=", "dis_accu", "\n", "to_log", "[", "'dis_src_pred'", "]", "=", "src_pred", "\n", "to_log", "[", "'dis_tgt_pred'", "]", "=", "tgt_pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.sent_translation.load_europarl_data": [[23, 63], ["os.path.isfile", "numpy.array", "numpy.array", "numpy.unique", "numpy.unique", "numpy.random.RandomState", "np.random.RandomState.permutation", "logger.info", "os.path.join", "os.path.join", "len", "len", "len", "os.path.isfile", "os.path.isfile", "io.open", "enumerate", "os.path.join", "os.path.join", "data[].append", "len", "line.lower", "line.rstrip().split", "line.rstrip"], "function", ["None"], ["def", "load_europarl_data", "(", "lg1", ",", "lg2", ",", "n_max", "=", "1e10", ",", "lower", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Load data parallel sentences\n    \"\"\"", "\n", "if", "not", "(", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "EUROPARL_DIR", ",", "'europarl-v7.%s-%s.%s'", "%", "(", "lg1", ",", "lg2", ",", "lg1", ")", ")", ")", "or", "\n", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "EUROPARL_DIR", ",", "'europarl-v7.%s-%s.%s'", "%", "(", "lg2", ",", "lg1", ",", "lg1", ")", ")", ")", ")", ":", "\n", "        ", "return", "None", "\n", "\n", "", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "EUROPARL_DIR", ",", "'europarl-v7.%s-%s.%s'", "%", "(", "lg2", ",", "lg1", ",", "lg1", ")", ")", ")", ":", "\n", "        ", "lg1", ",", "lg2", "=", "lg2", ",", "lg1", "\n", "\n", "# load sentences", "\n", "", "data", "=", "{", "lg1", ":", "[", "]", ",", "lg2", ":", "[", "]", "}", "\n", "for", "lg", "in", "[", "lg1", ",", "lg2", "]", ":", "\n", "        ", "fname", "=", "os", ".", "path", ".", "join", "(", "EUROPARL_DIR", ",", "'europarl-v7.%s-%s.%s'", "%", "(", "lg1", ",", "lg2", ",", "lg", ")", ")", "\n", "\n", "with", "io", ".", "open", "(", "fname", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "i", ">=", "n_max", ":", "\n", "                    ", "break", "\n", "", "line", "=", "line", ".", "lower", "(", ")", "if", "lower", "else", "line", "\n", "data", "[", "lg", "]", ".", "append", "(", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", ")", "\n", "\n", "# get only unique sentences for each language", "\n", "", "", "", "assert", "len", "(", "data", "[", "lg1", "]", ")", "==", "len", "(", "data", "[", "lg2", "]", ")", "\n", "data", "[", "lg1", "]", "=", "np", ".", "array", "(", "data", "[", "lg1", "]", ")", "\n", "data", "[", "lg2", "]", "=", "np", ".", "array", "(", "data", "[", "lg2", "]", ")", "\n", "data", "[", "lg1", "]", ",", "indices", "=", "np", ".", "unique", "(", "data", "[", "lg1", "]", ",", "return_index", "=", "True", ")", "\n", "data", "[", "lg2", "]", "=", "data", "[", "lg2", "]", "[", "indices", "]", "\n", "data", "[", "lg2", "]", ",", "indices", "=", "np", ".", "unique", "(", "data", "[", "lg2", "]", ",", "return_index", "=", "True", ")", "\n", "data", "[", "lg1", "]", "=", "data", "[", "lg1", "]", "[", "indices", "]", "\n", "\n", "# shuffle sentences", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "1234", ")", "\n", "perm", "=", "rng", ".", "permutation", "(", "len", "(", "data", "[", "lg1", "]", ")", ")", "\n", "data", "[", "lg1", "]", "=", "data", "[", "lg1", "]", "[", "perm", "]", "\n", "data", "[", "lg2", "]", "=", "data", "[", "lg2", "]", "[", "perm", "]", "\n", "\n", "logger", ".", "info", "(", "\"Loaded europarl %s-%s (%i sentences).\"", "%", "(", "lg1", ",", "lg2", ",", "len", "(", "data", "[", "lg1", "]", ")", ")", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.evaluation.sent_translation.get_sent_translation_accuracy": [[65, 137], ["emb1.cpu().numpy.cpu().numpy", "emb2.cpu().numpy.cpu().numpy", "dict", "dict", "src.utils.bow_idf", "numpy.random.RandomState", "np.random.RandomState.choice", "src.utils.bow_idf", "torch.from_numpy().float", "torch.from_numpy().float", "range", "torch.from_numpy().float.norm().expand_as", "torch.from_numpy().float.norm().expand_as", "torch.from_numpy().float.mm().transpose", "scores.cpu.cpu", "method.startswith", "scores.cpu.topk", "logger.info", "results.append", "emb1.cpu().numpy.cpu", "emb2.cpu().numpy.cpu", "torch.from_numpy", "torch.from_numpy", "float", "torch.from_numpy().float.mm().transpose", "scores.cpu.mul_().exp_", "scores.cpu.div_", "scores.cpu.cpu", "method.startswith", "top_k_matches.float().numpy().mean", "torch.from_numpy().float.norm", "torch.from_numpy().float.norm", "torch.from_numpy().float.mm", "scores.cpu.sum().expand_as", "int.isdigit", "int", "int.isdigit", "int", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().float.mm().transpose", "scores.cpu.mul_", "scores.cpu.sub_", "scores.cpu.cpu", "torch.from_numpy().float.transpose", "torch.from_numpy().float.mm", "scores.cpu.mul_", "src.utils.get_nn_avg_dist", "src.utils.get_nn_avg_dist", "top_k_matches.float().numpy", "len", "lg_query.upper", "len", "torch.from_numpy().float.transpose", "scores.cpu.sum", "len", "len", "torch.from_numpy().float.mm", "average_dist_queries[].float", "average_dist_keys[].float", "torch.from_numpy", "torch.from_numpy().float.transpose", "top_k_matches.float"], "function", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.bow_idf", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.bow_idf", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.utils.get_nn_avg_dist"], ["", "def", "get_sent_translation_accuracy", "(", "data", ",", "lg1", ",", "word2id1", ",", "emb1", ",", "lg2", ",", "word2id2", ",", "emb2", ",", "\n", "n_keys", ",", "n_queries", ",", "method", ",", "idf", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Given parallel sentences from Europarl, evaluate the\n    sentence translation accuracy using the precision@k.\n    \"\"\"", "\n", "# get word vectors dictionaries", "\n", "emb1", "=", "emb1", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "emb2", "=", "emb2", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "word_vec1", "=", "dict", "(", "[", "(", "w", ",", "emb1", "[", "word2id1", "[", "w", "]", "]", ")", "for", "w", "in", "word2id1", "]", ")", "\n", "word_vec2", "=", "dict", "(", "[", "(", "w", ",", "emb2", "[", "word2id2", "[", "w", "]", "]", ")", "for", "w", "in", "word2id2", "]", ")", "\n", "word_vect", "=", "{", "lg1", ":", "word_vec1", ",", "lg2", ":", "word_vec2", "}", "\n", "lg_keys", "=", "lg2", "\n", "lg_query", "=", "lg1", "\n", "\n", "# get n_keys pairs of sentences", "\n", "keys", "=", "data", "[", "lg_keys", "]", "[", ":", "n_keys", "]", "\n", "keys", "=", "bow_idf", "(", "keys", ",", "word_vect", "[", "lg_keys", "]", ",", "idf_dict", "=", "idf", "[", "lg_keys", "]", ")", "\n", "\n", "# get n_queries query pairs from these n_keys pairs", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "1234", ")", "\n", "idx_query", "=", "rng", ".", "choice", "(", "range", "(", "n_keys", ")", ",", "size", "=", "n_queries", ",", "replace", "=", "False", ")", "\n", "queries", "=", "data", "[", "lg_query", "]", "[", "idx_query", "]", "\n", "queries", "=", "bow_idf", "(", "queries", ",", "word_vect", "[", "lg_query", "]", ",", "idf_dict", "=", "idf", "[", "lg_query", "]", ")", "\n", "\n", "# normalize embeddings", "\n", "queries", "=", "torch", ".", "from_numpy", "(", "queries", ")", ".", "float", "(", ")", "\n", "queries", "=", "queries", "/", "queries", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "queries", ")", "\n", "keys", "=", "torch", ".", "from_numpy", "(", "keys", ")", ".", "float", "(", ")", "\n", "keys", "=", "keys", "/", "keys", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "keys", ")", "\n", "\n", "# nearest neighbors", "\n", "if", "method", "==", "'nn'", ":", "\n", "        ", "scores", "=", "keys", ".", "mm", "(", "queries", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "scores", "=", "scores", ".", "cpu", "(", ")", "\n", "\n", "# inverted softmax", "\n", "", "elif", "method", ".", "startswith", "(", "'invsm_beta_'", ")", ":", "\n", "        ", "beta", "=", "float", "(", "method", "[", "len", "(", "'invsm_beta_'", ")", ":", "]", ")", "\n", "scores", "=", "keys", ".", "mm", "(", "queries", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "scores", ".", "mul_", "(", "beta", ")", ".", "exp_", "(", ")", "\n", "scores", ".", "div_", "(", "scores", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "scores", ")", ")", "\n", "scores", "=", "scores", ".", "cpu", "(", ")", "\n", "\n", "# contextual dissimilarity measure", "\n", "", "elif", "method", ".", "startswith", "(", "'csls_knn_'", ")", ":", "\n", "        ", "knn", "=", "method", "[", "len", "(", "'csls_knn_'", ")", ":", "]", "\n", "assert", "knn", ".", "isdigit", "(", ")", "\n", "knn", "=", "int", "(", "knn", ")", "\n", "# average distances to k nearest neighbors", "\n", "knn", "=", "method", "[", "len", "(", "'csls_knn_'", ")", ":", "]", "\n", "assert", "knn", ".", "isdigit", "(", ")", "\n", "knn", "=", "int", "(", "knn", ")", "\n", "average_dist_keys", "=", "torch", ".", "from_numpy", "(", "get_nn_avg_dist", "(", "queries", ",", "keys", ",", "knn", ")", ")", "\n", "average_dist_queries", "=", "torch", ".", "from_numpy", "(", "get_nn_avg_dist", "(", "keys", ",", "queries", ",", "knn", ")", ")", "\n", "# scores", "\n", "scores", "=", "keys", ".", "mm", "(", "queries", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "scores", ".", "mul_", "(", "2", ")", "\n", "scores", ".", "sub_", "(", "average_dist_queries", "[", ":", ",", "None", "]", ".", "float", "(", ")", "+", "average_dist_keys", "[", "None", ",", ":", "]", ".", "float", "(", ")", ")", "\n", "scores", "=", "scores", ".", "cpu", "(", ")", "\n", "\n", "", "results", "=", "[", "]", "\n", "top_matches", "=", "scores", ".", "topk", "(", "10", ",", "1", ",", "True", ")", "[", "1", "]", "\n", "for", "k", "in", "[", "1", ",", "5", ",", "10", "]", ":", "\n", "        ", "top_k_matches", "=", "(", "top_matches", "[", ":", ",", ":", "k", "]", "==", "torch", ".", "from_numpy", "(", "idx_query", ")", "[", ":", ",", "None", "]", ")", ".", "sum", "(", "1", ")", "\n", "precision_at_k", "=", "100", "*", "top_k_matches", ".", "float", "(", ")", ".", "numpy", "(", ")", ".", "mean", "(", ")", "\n", "logger", ".", "info", "(", "\"%i queries (%s) - %s - Precision at k = %i: %f\"", "%", "\n", "(", "len", "(", "top_k_matches", ")", ",", "lg_query", ".", "upper", "(", ")", ",", "method", ",", "k", ",", "precision_at_k", ")", ")", "\n", "results", ".", "append", "(", "(", "'sent-precision_at_%i'", "%", "k", ",", "precision_at_k", ")", ")", "\n", "\n", "", "return", "results", "\n", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.nonlinear_map.NonLinearMap.__init__": [[12, 37], ["torch.nn.Module.__init__", "logger.info", "range", "torch.nn.Sequential", "torch.nn.LeakyReLU", "layers.append", "torch.nn.Tanh", "print", "exit", "torch.nn.Linear", "layers.append"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "NonLinearMap", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "activation", "=", "args", ".", "activation", "\n", "self", ".", "emb_dim", "=", "args", ".", "emb_dim", "\n", "self", ".", "n_layers", "=", "args", ".", "n_layers", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "logger", ".", "info", "(", "\"Non-linear mapping:\\nActivation:{}\\nLayers:{}\\nHidden Size:{}\"", ".", "format", "(", "self", ".", "activation", ",", "\n", "self", ".", "n_layers", ",", "self", ".", "hidden_size", ")", ")", "\n", "\n", "if", "args", ".", "activation", "==", "'leaky_relu'", ":", "\n", "            ", "activate", "=", "nn", ".", "LeakyReLU", "(", "0.1", ")", "\n", "", "elif", "args", ".", "activation", "==", "'tanh'", ":", "\n", "            ", "activate", "=", "nn", ".", "Tanh", "(", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Activation type: {} not defined!\"", ".", "format", "(", "parmas", ".", "activation", ")", ")", "\n", "exit", "(", "1", ")", "\n", "", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "            ", "input_dim", "=", "self", ".", "emb_dim", "if", "i", "==", "0", "else", "self", ".", "hidden_size", "\n", "output_dim", "=", "self", ".", "emb_dim", "if", "i", "==", "self", ".", "n_layers", "-", "1", "else", "self", ".", "hidden_size", "\n", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ",", "bias", "=", "True", ")", ")", "\n", "if", "i", "<", "self", ".", "n_layers", "-", "1", ":", "\n", "                ", "layers", ".", "append", "(", "activate", ")", "\n", "", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.nonlinear_map.NonLinearMap.forward": [[38, 42], ["nonlinear_map.NonLinearMap.layers", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "#assert x.dim() == 2 and x.size(1) == self.emb_dim", "\n", "        ", "assert", "x", ".", "size", "(", "-", "1", ")", "==", "self", ".", "emb_dim", "\n", "return", "self", ".", "layers", "(", "x", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.nonlinear_self_attention_map.NonLinearSelfAttentionMap.__init__": [[13, 23], ["torch.nn.Module.__init__", "logger.info", "src.maps.NonLinearMap", "src.maps.SelfAttentionMap"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        The output has the same size as the input (hidden_size)\n        \"\"\"", "\n", "super", "(", "NonLinearSelfAttentionMap", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "emb_dim", "=", "args", ".", "emb_dim", "\n", "\n", "logger", ".", "info", "(", "\"NonLinear + SelfAttention Mapping\"", ")", "\n", "self", ".", "nonlinear_map", "=", "NonLinearMap", "(", "args", ")", "\n", "self", ".", "self_map", "=", "SelfAttentionMap", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.nonlinear_self_attention_map.NonLinearSelfAttentionMap.forward": [[24, 33], ["nonlinear_self_attention_map.NonLinearSelfAttentionMap.NonLinearSelfAttentionMap.nonlinear_map", "nonlinear_self_attention_map.NonLinearSelfAttentionMap.NonLinearSelfAttentionMap.self_map"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Input: \n            input_tensor: [batch_size, seq_len, emb_dim]\n        \"\"\"", "\n", "\n", "nonlinear_output", "=", "self", ".", "nonlinear_map", "(", "input_tensor", ")", "\n", "self_output", "=", "self", ".", "self_map", "(", "nonlinear_output", ",", "attention_mask", ")", "\n", "return", "self_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.attention_map.AttentionMap.__init__": [[13, 27], ["torch.nn.Module.__init__", "src.bert_modeling.BertConfig.from_json_file", "src.bert_modeling.BERTAttention", "logger.info"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_json_file"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        The output has the same size as the input (hidden_size)\n        \"\"\"", "\n", "super", "(", "AttentionMap", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "config", "=", "BertConfig", ".", "from_json_file", "(", "args", ".", "bert_config_file", ")", "\n", "self", ".", "config", ".", "num_attention_heads", "=", "args", ".", "num_attention_heads", "\n", "self", ".", "config", ".", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", "\n", "self", ".", "config", ".", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", "\n", "self", ".", "attention", "=", "BERTAttention", "(", "self", ".", "config", ")", "\n", "logger", ".", "info", "(", "\"Attention Mapping:\\nHidden Size:{}\\nAttention Heads:{}\\nAttention Dropout:{}\\nHidden Dropout:{}\"", ".", "format", "(", "self", ".", "config", ".", "hidden_size", ",", "\n", "self", ".", "config", ".", "num_attention_heads", ",", "self", ".", "config", ".", "attention_probs_dropout_prob", ",", "\n", "self", ".", "config", ".", "hidden_dropout_prob", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.attention_map.AttentionMap.forward": [[28, 48], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.float.float.float", "attention_map.AttentionMap.attention", "torch.ones_like", "torch.ones_like.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "float", "(", ")", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "attention_output", "=", "self", ".", "attention", "(", "input_tensor", ",", "extended_attention_mask", ")", "\n", "return", "attention_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.linear_self_attention_map.LinearSelfAttentionMap.__init__": [[13, 26], ["torch.nn.Module.__init__", "logger.info", "logger.info", "torch.nn.Linear", "getattr", "src.maps.SelfAttentionMap", "linear_self_attention_map.LinearSelfAttentionMap.LinearSelfAttentionMap.linear_map.weight.data.copy_", "torch.diag", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        The output has the same size as the input (hidden_size)\n        \"\"\"", "\n", "super", "(", "LinearSelfAttentionMap", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "emb_dim", "=", "args", ".", "emb_dim", "\n", "\n", "logger", ".", "info", "(", "\"Linear + SelfAttention Mapping\"", ")", "\n", "logger", ".", "info", "(", "\"Linear mapping:\\nEmbedding Dimension:{}\"", ".", "format", "(", "args", ".", "emb_dim", ")", ")", "\n", "self", ".", "linear_map", "=", "nn", ".", "Linear", "(", "args", ".", "emb_dim", ",", "args", ".", "emb_dim", ",", "bias", "=", "False", ")", "\n", "if", "getattr", "(", "args", ",", "'map_id_init'", ",", "True", ")", ":", "\n", "            ", "self", ".", "linear_map", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "diag", "(", "torch", ".", "ones", "(", "args", ".", "emb_dim", ")", ")", ")", "\n", "", "self", ".", "self_map", "=", "SelfAttentionMap", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.linear_self_attention_map.LinearSelfAttentionMap.forward": [[27, 36], ["linear_self_attention_map.LinearSelfAttentionMap.LinearSelfAttentionMap.linear_map", "linear_self_attention_map.LinearSelfAttentionMap.LinearSelfAttentionMap.self_map"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Input: \n            input_tensor: [batch_size, seq_len, emb_dim]\n        \"\"\"", "\n", "\n", "linear_output", "=", "self", ".", "linear_map", "(", "input_tensor", ")", "\n", "self_output", "=", "self", ".", "self_map", "(", "linear_output", ",", "attention_mask", ")", "\n", "return", "self_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__": [[13, 25], ["torch.nn.Module.__init__", "src.bert_modeling.BertConfig.from_json_file", "src.bert_modeling.BERTSelfAttention", "logger.info"], "methods", ["home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.__init__", "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.src.bert_modeling.BertConfig.from_json_file"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        The output has the same size as the input (hidden_size)\n        \"\"\"", "\n", "super", "(", "SelfAttentionMap", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "config", "=", "BertConfig", ".", "from_json_file", "(", "args", ".", "bert_config_file", ")", "\n", "self", ".", "config", ".", "num_attention_heads", "=", "args", ".", "num_attention_heads", "\n", "self", ".", "config", ".", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", "\n", "self", ".", "self", "=", "BERTSelfAttention", "(", "self", ".", "config", ")", "\n", "logger", ".", "info", "(", "\"Self Attention Mapping:\\nHidden Size:{}\\nAttention Heads:{}\\nAttention Dropout:{}\"", ".", "format", "(", "self", ".", "config", ".", "hidden_size", ",", "\n", "self", ".", "config", ".", "num_attention_heads", ",", "self", ".", "config", ".", "attention_probs_dropout_prob", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WangYuxuan93_CLBT.maps.self_attention_map.SelfAttentionMap.forward": [[26, 46], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.float.float.float", "self_attention_map.SelfAttentionMap.SelfAttentionMap.self", "torch.ones_like", "torch.ones_like.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "float", "(", ")", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "extended_attention_mask", ")", "\n", "return", "self_output", "\n", "", "", ""]]}