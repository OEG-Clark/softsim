{"home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsDataset.__init__": [[37, 43], ["len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "source", ",", "target", ",", "vocab", ",", "label", ")", ":", "\n", "        ", "assert", "len", "(", "source", ")", "==", "len", "(", "target", ")", "\n", "self", ".", "source", "=", "source", "\n", "self", ".", "target", "=", "target", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsDataset.__len__": [[44, 46], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsDataset.__getitem__": [[47, 49], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "source", "[", "idx", "]", ",", "self", ".", "target", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsModel.__init__": [[52, 58], ["pytorch_lightning.LightningModule.__init__", "torch.nn.Linear", "torch.nn.Linear", "pytorch_lightning.metrics.Accuracy", "pytorch_lightning.metrics.Accuracy"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "hidden_size", ",", "num_label", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "num_label", ")", "\n", "self", ".", "train_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "self", ".", "valid_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsModel.forward": [[59, 63], ["listops.ListopsModel.decoder", "listops.ListopsModel.encoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ")", ":", "\n", "        ", "pooler_output", "=", "self", ".", "encoder", "(", "input_ids", ",", "attention_mask", ")", "[", "\"pooler_output\"", "]", "\n", "logits", "=", "self", ".", "decoder", "(", "pooler_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsModel.training_step": [[64, 71], ["listops.ListopsModel.", "torch.cross_entropy", "torch.cross_entropy", "listops.ListopsModel.train_acc", "listops.ListopsModel.log", "listops.ListopsModel.argmax"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "attention_mask", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ",", "attention_mask", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "labels", ")", "\n", "self", ".", "train_acc", "(", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "'train_acc'", ",", "self", ".", "train_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsModel.validation_step": [[72, 79], ["listops.ListopsModel.", "torch.cross_entropy", "torch.cross_entropy", "listops.ListopsModel.valid_acc", "listops.ListopsModel.log", "listops.ListopsModel.argmax"], "methods", ["None"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "attention_mask", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ",", "attention_mask", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "labels", ")", "\n", "self", ".", "valid_acc", "(", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "'valid_acc'", ",", "self", ".", "valid_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.ListopsModel.configure_optimizers": [[80, 92], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "listops.ListopsModel.parameters", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "math.sqrt"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "1e-4", ",", "weight_decay", "=", "0.0", ")", "\n", "warmup", "=", "1000", "\n", "def", "lr_lambda", "(", "step", ")", ":", "\n", "            ", "if", "step", "<", "warmup", ":", "\n", "                ", "return", "step", "/", "warmup", "\n", "", "return", "1", "/", "math", ".", "sqrt", "(", "step", "/", "warmup", ")", "\n", "", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ")", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.format_num": [[27, 31], ["str", "len", "len"], "function", ["None"], ["def", "format_num", "(", "n", ")", ":", "\n", "    ", "f", "=", "'{0:.4g}'", ".", "format", "(", "n", ")", ".", "replace", "(", "'+0'", ",", "'+'", ")", ".", "replace", "(", "'-0'", ",", "'-'", ")", "\n", "n", "=", "str", "(", "n", ")", "\n", "return", "f", "if", "len", "(", "f", ")", "<", "len", "(", "n", ")", "else", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.load_dataset": [[94, 119], ["logger.info", "all", "sorted", "sorted", "logger.info", "listops.ListopsDataset", "open", "i[].split", "int", "set", "set", "l.strip().split", "enumerate", "len", "len", "max", "l.strip", "sum", "len", "len", "len"], "function", ["None"], ["", "", "def", "load_dataset", "(", "path", ")", ":", "\n", "    ", "logger", ".", "info", "(", "f\"Loading {path}\"", ")", "\n", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "tsv", "=", "[", "l", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "for", "l", "in", "f", "]", "\n", "", "assert", "tsv", "[", "0", "]", "==", "[", "\"Source\"", ",", "\"Target\"", "]", "\n", "tsv", "=", "tsv", "[", "1", ":", "]", "\n", "assert", "all", "(", "len", "(", "i", ")", "==", "2", "for", "i", "in", "tsv", ")", "\n", "source", "=", "[", "i", "[", "0", "]", ".", "split", "(", ")", "for", "i", "in", "tsv", "]", "\n", "target", "=", "[", "int", "(", "i", "[", "1", "]", ")", "for", "i", "in", "tsv", "]", "\n", "\n", "# strip parenthesis", "\n", "source", "=", "[", "[", "j", "for", "j", "in", "i", "if", "j", "not", "in", "(", "\"(\"", ",", "\")\"", ")", "]", "for", "i", "in", "source", "]", "\n", "\n", "# build vocab", "\n", "vocab", "=", "sorted", "(", "set", "(", "j", "for", "i", "in", "source", "for", "j", "in", "i", ")", ")", "\n", "label", "=", "sorted", "(", "set", "(", "target", ")", ")", "\n", "\n", "# tokenize", "\n", "tokenizer", "=", "{", "v", ":", "i", "+", "NUM_SPECIAL_TOKEN", "for", "i", ",", "v", "in", "enumerate", "(", "vocab", ")", "}", "\n", "source", "=", "[", "[", "tokenizer", "[", "j", "]", "for", "j", "in", "i", "]", "for", "i", "in", "source", "]", "\n", "\n", "logger", ".", "info", "(", "f\"{len(source)} seqs, max len {max(len(i) for i in source)}, avg len {sum(len(i) for i in source) / len(source)}\"", ")", "\n", "\n", "dataset", "=", "ListopsDataset", "(", "source", ",", "target", ",", "vocab", ",", "label", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.load_datasets": [[121, 134], ["os.path.join", "os.path.join", "os.path.join", "listops.load_dataset", "listops.load_dataset", "listops.load_dataset", "logger.info", "len", "len"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset"], ["", "def", "load_datasets", "(", "args", ")", ":", "\n", "    ", "train_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "train_path", ")", "\n", "val_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "val_path", ")", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "test_path", ")", "\n", "\n", "train_dataset", "=", "load_dataset", "(", "train_path", ")", "\n", "val_dataset", "=", "load_dataset", "(", "val_path", ")", "\n", "test_dataset", "=", "load_dataset", "(", "test_path", ")", "\n", "assert", "train_dataset", ".", "vocab", "==", "val_dataset", ".", "vocab", "==", "test_dataset", ".", "vocab", "\n", "assert", "train_dataset", ".", "label", "==", "val_dataset", ".", "label", "==", "test_dataset", ".", "label", "\n", "logger", ".", "info", "(", "f\"#vocab {len(train_dataset.vocab)}, #label {len(train_dataset.label)}\"", ")", "\n", "\n", "return", "train_dataset", ",", "val_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.load_model": [[136, 146], ["config_type.from_dict", "model_type", "open", "json.load"], "function", ["None"], ["", "def", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "model_config", ")", "as", "f", ":", "\n", "        ", "json_config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_config", "[", "\"pad_token_id\"", "]", "=", "PAD_TOKEN_ID", "\n", "json_config", "[", "\"max_position_embeddings\"", "]", "=", "max_position_embeddings", "\n", "json_config", "[", "\"vocab_size\"", "]", "=", "vocab_size", "\n", "config_type", ",", "model_type", "=", "MODEL_MAP", "[", "json_config", "[", "\"model_type\"", "]", "]", "\n", "config", "=", "config_type", ".", "from_dict", "(", "json_config", ")", "\n", "model", "=", "model_type", "(", "config", ")", "\n", "return", "model", ",", "json_config", "[", "\"hidden_size\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.collate_fn": [[148, 162], ["len", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "len", "len"], "function", ["None"], ["", "def", "collate_fn", "(", "examples", ")", ":", "\n", "    ", "bsz", "=", "len", "(", "examples", ")", "\n", "\n", "input_ids", "=", "[", "i", "[", "0", "]", "for", "i", "in", "examples", "]", "\n", "labels", "=", "[", "i", "[", "1", "]", "for", "i", "in", "examples", "]", "\n", "\n", "max_len", "=", "max", "(", "len", "(", "i", ")", "for", "i", "in", "input_ids", ")", "\n", "pad_input_ids", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "for", "i", ",", "seq", "in", "enumerate", "(", "input_ids", ")", ":", "\n", "        ", "pad_input_ids", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "torch", ".", "LongTensor", "(", "seq", ")", "\n", "attention_mask", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "1", "\n", "", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "return", "pad_input_ids", ",", "attention_mask", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.stats_classifier": [[164, 177], ["collections.defaultdict", "range", "range", "len", "model[].update", "len", "len", "collections.defaultdict.items", "v.most_common"], "function", ["None"], ["", "def", "stats_classifier", "(", "train_dataset", ",", "test_dataset", ")", ":", "\n", "    ", "model", "=", "collections", ".", "defaultdict", "(", "collections", ".", "Counter", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "train_dataset", ")", ")", ":", "\n", "        ", "source", ",", "label", "=", "train_dataset", "[", "i", "]", "\n", "model", "[", "source", "[", "0", "]", "]", ".", "update", "(", "[", "label", "]", ")", "\n", "", "model", "=", "{", "k", ":", "v", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "for", "k", ",", "v", "in", "model", ".", "items", "(", ")", "}", "\n", "\n", "correct", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "test_dataset", ")", ")", ":", "\n", "        ", "source", ",", "label", "=", "test_dataset", "[", "i", "]", "\n", "if", "source", "[", "0", "]", "in", "model", "and", "model", "[", "source", "[", "0", "]", "]", "==", "label", ":", "\n", "            ", "correct", "+=", "1", "\n", "", "", "return", "correct", "/", "len", "(", "test_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.listops.main": [[179, 209], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "listops.load_datasets", "logger.info", "logger.info", "max", "len", "listops.load_model", "listops.ListopsModel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "pytorch_lightning.Trainer", "pl.Trainer.fit", "len", "max", "max", "max", "listops.stats_classifier", "listops.stats_classifier", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_datasets", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.stats_classifier", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.stats_classifier"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data-dir\"", ",", "default", "=", "\"../../data/lra_release/lra_release/listops-1000\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train-path\"", ",", "default", "=", "\"basic_train.tsv\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val-path\"", ",", "default", "=", "\"basic_val.tsv\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test-path\"", ",", "default", "=", "\"basic_test.tsv\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-config\"", ",", "default", "=", "\"configs/bert_prenorm_listops.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch-size\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epochs\"", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "train_dataset", ",", "val_dataset", ",", "test_dataset", "=", "load_datasets", "(", "args", ")", "\n", "logger", ".", "info", "(", "f\"Baseline on val: {stats_classifier(train_dataset, val_dataset)}\"", ")", "\n", "logger", ".", "info", "(", "f\"Baseline on test: {stats_classifier(train_dataset, test_dataset)}\"", ")", "\n", "vocab_size", "=", "NUM_SPECIAL_TOKEN", "+", "len", "(", "train_dataset", ".", "vocab", ")", "\n", "max_position_embeddings", "=", "max", "(", "\n", "max", "(", "len", "(", "i", "[", "0", "]", ")", "for", "i", "in", "train_dataset", ")", ",", "\n", "max", "(", "len", "(", "i", "[", "0", "]", ")", "for", "i", "in", "val_dataset", ")", ",", "\n", "max", "(", "len", "(", "i", "[", "0", "]", ")", "for", "i", "in", "test_dataset", ")", ",", "\n", ")", "\n", "num_label", "=", "len", "(", "train_dataset", ".", "label", ")", "\n", "encoder", ",", "hidden_size", "=", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", "\n", "model", "=", "ListopsModel", "(", "encoder", ",", "hidden_size", ",", "num_label", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "val_loader", "=", "DataLoader", "(", "val_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "gpus", "=", "8", ",", "accelerator", "=", "\"ddp\"", ",", "max_epochs", "=", "args", ".", "max_epochs", ")", "\n", "trainer", ".", "fit", "(", "model", ",", "train_loader", ",", "val_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.__init__": [[36, 41], ["len", "len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "text1", ",", "text2", ",", "label", ")", ":", "\n", "        ", "assert", "len", "(", "text1", ")", "==", "len", "(", "text2", ")", "==", "len", "(", "label", ")", "\n", "self", ".", "text1", "=", "text1", "\n", "self", ".", "text2", "=", "text2", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.__len__": [[42, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "text1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.__getitem__": [[45, 47], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "text1", "[", "idx", "]", ",", "self", ".", "text2", "[", "idx", "]", ",", "self", ".", "label", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.prune": [[48, 51], ["None"], "methods", ["None"], ["", "def", "prune", "(", "self", ",", "max_length", ")", ":", "\n", "        ", "self", ".", "text1", "=", "[", "i", "[", ":", "max_length", "]", "for", "i", "in", "self", ".", "text1", "]", "\n", "self", ".", "text2", "=", "[", "i", "[", ":", "max_length", "]", "for", "i", "in", "self", ".", "text2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingModel.__init__": [[54, 66], ["pytorch_lightning.LightningModule.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "pytorch_lightning.metrics.Accuracy", "pytorch_lightning.metrics.Accuracy", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "*", "2", ",", "hidden_size", "*", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_size", "*", "4", ",", "hidden_size", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_size", "*", "2", ",", "1", ")", ",", "\n", ")", "\n", "self", ".", "train_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "self", ".", "valid_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingModel.forward": [[67, 72], ["matching.MatchingModel.decoder().squeeze", "matching.MatchingModel.encoder", "matching.MatchingModel.encoder", "matching.MatchingModel.decoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids1", ",", "input_ids2", ",", "attention_mask1", ",", "attention_mask2", ")", ":", "\n", "        ", "pooler_output1", "=", "self", ".", "encoder", "(", "input_ids1", ",", "attention_mask1", ")", "[", "\"pooler_output\"", "]", "\n", "pooler_output2", "=", "self", ".", "encoder", "(", "input_ids2", ",", "attention_mask2", ")", "[", "\"pooler_output\"", "]", "\n", "logits", "=", "self", ".", "decoder", "(", "torch", ".", "cat", "(", "(", "pooler_output1", ",", "pooler_output2", ")", ",", "dim", "=", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingModel.training_step": [[73, 80], ["matching.MatchingModel.", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "matching.MatchingModel.train_acc", "matching.MatchingModel.log", "labels.to", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids1", ",", "input_ids2", ",", "attention_mask1", ",", "attention_mask2", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids1", ",", "input_ids2", ",", "attention_mask1", ",", "attention_mask2", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "labels", ".", "to", "(", "torch", ".", "float", ")", ")", "\n", "self", ".", "train_acc", "(", "F", ".", "sigmoid", "(", "logits", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"train_acc\"", ",", "self", ".", "train_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingModel.validation_step": [[81, 88], ["matching.MatchingModel.", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "matching.MatchingModel.valid_acc", "matching.MatchingModel.log", "labels.to", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids1", ",", "input_ids2", ",", "attention_mask1", ",", "attention_mask2", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids1", ",", "input_ids2", ",", "attention_mask1", ",", "attention_mask2", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "labels", ".", "to", "(", "torch", ".", "float", ")", ")", "\n", "self", ".", "valid_acc", "(", "F", ".", "sigmoid", "(", "logits", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"valid_acc\"", ",", "self", ".", "valid_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingModel.configure_optimizers": [[89, 101], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "matching.MatchingModel.parameters", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "math.sqrt"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "2e-4", ",", "weight_decay", "=", "0.0", ")", "\n", "warmup", "=", "1000", "\n", "def", "lr_lambda", "(", "step", ")", ":", "\n", "            ", "if", "step", "<", "warmup", ":", "\n", "                ", "return", "step", "/", "warmup", "\n", "", "return", "1", "/", "math", ".", "sqrt", "(", "step", "/", "warmup", ")", "\n", "", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ")", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.format_num": [[26, 30], ["str", "len", "len"], "function", ["None"], ["def", "format_num", "(", "n", ")", ":", "\n", "    ", "f", "=", "'{0:.4g}'", ".", "format", "(", "n", ")", ".", "replace", "(", "'+0'", ",", "'+'", ")", ".", "replace", "(", "'-0'", ",", "'-'", ")", "\n", "n", "=", "str", "(", "n", ")", "\n", "return", "f", "if", "len", "(", "f", ")", "<", "len", "(", "n", ")", "else", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.load_dataset": [[103, 128], ["logger.info", "open", "matching.MatchingDataset", "logger.info", "logger.info", "logger.info", "logger.info", "line.split", "eval", "eval", "eval", "int", "text1.append", "text2.append", "label.append", "type", "type", "type", "len", "max", "sum", "len", "sum", "len", "sum", "len", "len", "len", "zip", "len", "len", "zip"], "function", ["None"], ["", "", "def", "load_dataset", "(", "path", ")", ":", "\n", "    ", "logger", ".", "info", "(", "f\"Loading {path}\"", ")", "\n", "text1", "=", "[", "]", "\n", "text2", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "for", "line", "in", "open", "(", "path", ")", ":", "\n", "        ", "label_", ",", "_", ",", "_", ",", "text1_", ",", "text2_", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "label_", "=", "eval", "(", "label_", ")", "\n", "text1_", "=", "eval", "(", "text1_", ")", "\n", "text2_", "=", "eval", "(", "text2_", ")", "\n", "assert", "type", "(", "label_", ")", "is", "float", "\n", "assert", "type", "(", "text1_", ")", "is", "bytes", "\n", "assert", "type", "(", "text2_", ")", "is", "bytes", "\n", "label_", "=", "int", "(", "label_", ")", "\n", "assert", "label_", "in", "(", "0", ",", "1", ")", "\n", "text1", ".", "append", "(", "text1_", ")", "\n", "text2", ".", "append", "(", "text2_", ")", "\n", "label", ".", "append", "(", "label_", ")", "\n", "", "dataset", "=", "MatchingDataset", "(", "text1", ",", "text2", ",", "label", ")", "\n", "\n", "logger", ".", "info", "(", "f\"{len(text1)} pairs\"", ")", "\n", "logger", ".", "info", "(", "f\"max len {max(len(i)+len(j) for i, j in zip(text1, text2))}\"", ")", "\n", "logger", ".", "info", "(", "f\"avg len {sum(len(i)+len(j) for i, j in zip(text1, text2))/len(text1)}\"", ")", "\n", "logger", ".", "info", "(", "f\"neg ratio: {sum(1 for i in label if i == 0)/len(label)}, pos ratio: {sum(1 for i in label if i == 1)/len(label)}\"", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.load_datasets": [[130, 144], ["os.path.join", "os.path.join", "os.path.join", "matching.load_dataset", "matching.load_dataset", "matching.load_dataset", "load_dataset.prune", "load_dataset.prune", "load_dataset.prune"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.prune", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.prune", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.MatchingDataset.prune"], ["", "def", "load_datasets", "(", "args", ")", ":", "\n", "    ", "train_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"new_aan_pairs.train.tsv\"", ")", "\n", "val_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"new_aan_pairs.eval.tsv\"", ")", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"new_aan_pairs.test.tsv\"", ")", "\n", "\n", "train_dataset", "=", "load_dataset", "(", "train_path", ")", "\n", "val_dataset", "=", "load_dataset", "(", "val_path", ")", "\n", "test_dataset", "=", "load_dataset", "(", "test_path", ")", "\n", "\n", "train_dataset", ".", "prune", "(", "args", ".", "max_length", "-", "2", ")", "\n", "val_dataset", ".", "prune", "(", "args", ".", "max_length", "-", "2", ")", "\n", "test_dataset", ".", "prune", "(", "args", ".", "max_length", "-", "2", ")", "\n", "\n", "return", "train_dataset", ",", "val_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.load_model": [[146, 156], ["config_type.from_dict", "model_type", "open", "json.load"], "function", ["None"], ["", "def", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "model_config", ")", "as", "f", ":", "\n", "        ", "json_config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_config", "[", "\"pad_token_id\"", "]", "=", "PAD_TOKEN_ID", "\n", "json_config", "[", "\"max_position_embeddings\"", "]", "=", "max_position_embeddings", "\n", "json_config", "[", "\"vocab_size\"", "]", "=", "vocab_size", "\n", "config_type", ",", "model_type", "=", "MODEL_MAP", "[", "json_config", "[", "\"model_type\"", "]", "]", "\n", "config", "=", "config_type", ".", "from_dict", "(", "json_config", ")", "\n", "model", "=", "model_type", "(", "config", ")", "\n", "return", "model", ",", "json_config", "[", "\"hidden_size\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.collate_fn": [[158, 184], ["len", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "len", "int", "int", "len", "len", "len", "len"], "function", ["None"], ["", "def", "collate_fn", "(", "examples", ")", ":", "\n", "    ", "bsz", "=", "len", "(", "examples", ")", "\n", "\n", "input_ids1", "=", "[", "[", "int", "(", "j", ")", "+", "NUM_SPECIAL_TOKEN", "for", "j", "in", "i", "[", "0", "]", "]", "for", "i", "in", "examples", "]", "\n", "input_ids2", "=", "[", "[", "int", "(", "j", ")", "+", "NUM_SPECIAL_TOKEN", "for", "j", "in", "i", "[", "1", "]", "]", "for", "i", "in", "examples", "]", "\n", "labels", "=", "[", "i", "[", "2", "]", "for", "i", "in", "examples", "]", "\n", "\n", "input_ids1", "=", "[", "[", "CLS_TOKEN_ID", "]", "+", "i", "+", "[", "SEP_TOKEN_ID", "]", "for", "i", "in", "input_ids1", "]", "\n", "max_len1", "=", "max", "(", "len", "(", "i", ")", "for", "i", "in", "input_ids1", ")", "\n", "pad_input_ids1", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len1", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask1", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len1", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "for", "i", ",", "seq", "in", "enumerate", "(", "input_ids1", ")", ":", "\n", "        ", "pad_input_ids1", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "torch", ".", "LongTensor", "(", "seq", ")", "\n", "attention_mask1", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "1", "\n", "\n", "", "input_ids2", "=", "[", "[", "CLS_TOKEN_ID", "]", "+", "i", "+", "[", "SEP_TOKEN_ID", "]", "for", "i", "in", "input_ids2", "]", "\n", "max_len2", "=", "max", "(", "len", "(", "i", ")", "for", "i", "in", "input_ids2", ")", "\n", "pad_input_ids2", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len2", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask2", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len2", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "for", "i", ",", "seq", "in", "enumerate", "(", "input_ids2", ")", ":", "\n", "        ", "pad_input_ids2", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "torch", ".", "LongTensor", "(", "seq", ")", "\n", "attention_mask2", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "1", "\n", "\n", "", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "\n", "return", "pad_input_ids1", ",", "pad_input_ids2", ",", "attention_mask1", ",", "attention_mask2", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.matching.main": [[186, 205], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "matching.load_datasets", "matching.load_model", "matching.MatchingModel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "pytorch_lightning.Trainer", "pl.Trainer.fit"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_datasets", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data-dir\"", ",", "default", "=", "\"../../data/lra_release/lra_release/tsv_data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-config\"", ",", "default", "=", "\"configs/bert_prenorm_matching.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-length\"", ",", "type", "=", "int", ",", "default", "=", "4000", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch-size\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epochs\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "train_dataset", ",", "val_dataset", ",", "test_dataset", "=", "load_datasets", "(", "args", ")", "\n", "encoder", ",", "hidden_size", "=", "load_model", "(", "args", ",", "NUM_SPECIAL_TOKEN", "+", "256", ",", "args", ".", "max_length", ")", "\n", "model", "=", "MatchingModel", "(", "encoder", ",", "hidden_size", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "val_loader", "=", "DataLoader", "(", "val_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "gpus", "=", "8", ",", "accelerator", "=", "\"ddp\"", ",", "max_epochs", "=", "args", ".", "max_epochs", ")", "\n", "trainer", ".", "fit", "(", "model", ",", "train_loader", ",", "test_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.PathfinderDataset.__init__": [[38, 42], ["len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "image", ",", "label", ")", ":", "\n", "        ", "assert", "len", "(", "image", ")", "==", "len", "(", "label", ")", "\n", "self", ".", "image", "=", "image", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.PathfinderDataset.__len__": [[43, 45], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.PathfinderDataset.__getitem__": [[46, 48], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "image", "[", "idx", "]", ",", "self", ".", "label", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.ImageModel.__init__": [[51, 57], ["pytorch_lightning.LightningModule.__init__", "torch.nn.Linear", "torch.nn.Linear", "pytorch_lightning.metrics.Accuracy", "pytorch_lightning.metrics.Accuracy"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", "\n", "self", ".", "train_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "self", ".", "valid_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.ImageModel.forward": [[58, 62], ["pathfinder.ImageModel.decoder().squeeze", "pathfinder.ImageModel.encoder", "pathfinder.ImageModel.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "pooler_output", "=", "self", ".", "encoder", "(", "input_ids", ")", "[", "\"pooler_output\"", "]", "\n", "logits", "=", "self", ".", "decoder", "(", "pooler_output", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.ImageModel.training_step": [[63, 70], ["pathfinder.ImageModel.", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "pathfinder.ImageModel.train_acc", "pathfinder.ImageModel.log", "labels.to", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "labels", ".", "to", "(", "torch", ".", "float", ")", ")", "\n", "self", ".", "train_acc", "(", "F", ".", "sigmoid", "(", "logits", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"train_acc\"", ",", "self", ".", "train_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.ImageModel.validation_step": [[71, 78], ["pathfinder.ImageModel.", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "pathfinder.ImageModel.valid_acc", "pathfinder.ImageModel.log", "labels.to", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "labels", ".", "to", "(", "torch", ".", "float", ")", ")", "\n", "self", ".", "valid_acc", "(", "F", ".", "sigmoid", "(", "logits", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"valid_acc\"", ",", "self", ".", "valid_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.ImageModel.configure_optimizers": [[79, 91], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "pathfinder.ImageModel.parameters", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "math.sqrt"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "5e-4", ",", "weight_decay", "=", "0.0", ")", "\n", "warmup", "=", "4000", "\n", "def", "lr_lambda", "(", "step", ")", ":", "\n", "            ", "if", "step", "<", "warmup", ":", "\n", "                ", "return", "step", "/", "warmup", "\n", "", "return", "1", "/", "math", ".", "sqrt", "(", "step", "/", "warmup", ")", "\n", "", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ")", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.format_num": [[28, 32], ["str", "len", "len"], "function", ["None"], ["def", "format_num", "(", "n", ")", ":", "\n", "    ", "f", "=", "'{0:.4g}'", ".", "format", "(", "n", ")", ".", "replace", "(", "'+0'", ",", "'+'", ")", ".", "replace", "(", "'-0'", ",", "'-'", ")", "\n", "n", "=", "str", "(", "n", ")", "\n", "return", "f", "if", "len", "(", "f", ")", "<", "len", "(", "n", ")", "else", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.load_datasets": [[95, 137], ["logger.info", "os.listdir", "all", "range", "logger.info", "pathfinder.PathfinderDataset", "pathfinder.PathfinderDataset", "pathfinder.PathfinderDataset", "logger.info", "logger.info", "logger.info", "os.path.join", "FILENAME_PATTERN.match", "int", "len", "int", "int", "len", "FILENAME_PATTERN.match.group", "open", "range", "os.path.join", "line.split", "list", "int", "image.append", "label.append", "len", "len", "len", "len", "len", "len", "len", "PIL.Image.open", "Image.open.getdata", "sum", "len", "sum", "len", "sum", "len", "os.path.join", "logger.info", "min", "max", "os.path.join"], "function", ["None"], ["def", "load_datasets", "(", "args", ")", ":", "\n", "    ", "logger", ".", "info", "(", "f\"Loading {args.data_dir}\"", ")", "\n", "\n", "metafiles", "=", "{", "}", "\n", "for", "metafile", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"metadata\"", ")", ")", ":", "\n", "        ", "match", "=", "FILENAME_PATTERN", ".", "match", "(", "metafile", ")", "\n", "if", "not", "match", ":", "\n", "            ", "continue", "\n", "", "idx", "=", "int", "(", "match", ".", "group", "(", "1", ")", ")", "\n", "assert", "idx", "not", "in", "metafiles", "\n", "metafiles", "[", "idx", "]", "=", "metafile", "\n", "", "assert", "all", "(", "i", "in", "metafiles", "for", "i", "in", "range", "(", "len", "(", "metafiles", ")", ")", ")", "\n", "\n", "image", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "metafiles", ")", ")", ":", "\n", "        ", "metafile", "=", "metafiles", "[", "idx", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"metadata\"", ",", "metafile", ")", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "img_dir", ",", "img_name", ",", "_", ",", "label_", ",", "*", "_", "=", "line", ".", "split", "(", ")", "\n", "try", ":", "\n", "                    ", "img", "=", "Image", ".", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "img_dir", ",", "img_name", ")", ")", "\n", "", "except", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"Cannot read {os.path.join(args.data_dir, img_dir, img_name)}\"", ")", "\n", "continue", "\n", "", "assert", "img", ".", "width", "==", "img", ".", "height", "==", "args", ".", "size", "\n", "img", "=", "list", "(", "img", ".", "getdata", "(", ")", ")", "\n", "assert", "0", "<=", "min", "(", "img", ")", "and", "max", "(", "img", ")", "<", "256", "\n", "label_", "=", "int", "(", "label_", ")", "\n", "assert", "label_", "in", "(", "0", ",", "1", ")", "\n", "image", ".", "append", "(", "img", ")", "\n", "label", ".", "append", "(", "label_", ")", "\n", "", "", "", "logger", ".", "info", "(", "f\"Loaded {len(image)} samples\"", ")", "\n", "\n", "split", "=", "[", "0", ",", "int", "(", "len", "(", "image", ")", "*", "0.8", ")", ",", "int", "(", "len", "(", "image", ")", "*", "0.9", ")", ",", "len", "(", "image", ")", "]", "\n", "train_dataset", "=", "PathfinderDataset", "(", "image", "[", "split", "[", "0", "]", ":", "split", "[", "1", "]", "]", ",", "label", "[", "split", "[", "0", "]", ":", "split", "[", "1", "]", "]", ")", "\n", "val_dataset", "=", "PathfinderDataset", "(", "image", "[", "split", "[", "1", "]", ":", "split", "[", "2", "]", "]", ",", "label", "[", "split", "[", "1", "]", ":", "split", "[", "2", "]", "]", ")", "\n", "test_dataset", "=", "PathfinderDataset", "(", "image", "[", "split", "[", "2", "]", ":", "split", "[", "3", "]", "]", ",", "label", "[", "split", "[", "2", "]", ":", "split", "[", "3", "]", "]", ")", "\n", "logger", ".", "info", "(", "f\"train: {len(train_dataset)}, pos ratio: {sum(train_dataset.label)/len(train_dataset)}\"", ")", "\n", "logger", ".", "info", "(", "f\"val: {len(val_dataset)}, pos ratio: {sum(val_dataset.label)/len(val_dataset)}\"", ")", "\n", "logger", ".", "info", "(", "f\"test: {len(test_dataset)}, pos ratio: {sum(test_dataset.label)/len(test_dataset)}\"", ")", "\n", "return", "train_dataset", ",", "val_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.load_model": [[139, 149], ["config_type.from_dict", "model_type", "open", "json.load"], "function", ["None"], ["", "def", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "model_config", ")", "as", "f", ":", "\n", "        ", "json_config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_config", "[", "\"pad_token_id\"", "]", "=", "PAD_TOKEN_ID", "\n", "json_config", "[", "\"max_position_embeddings\"", "]", "=", "max_position_embeddings", "\n", "json_config", "[", "\"vocab_size\"", "]", "=", "vocab_size", "\n", "config_type", ",", "model_type", "=", "MODEL_MAP", "[", "json_config", "[", "\"model_type\"", "]", "]", "\n", "config", "=", "config_type", ".", "from_dict", "(", "json_config", ")", "\n", "model", "=", "model_type", "(", "config", ")", "\n", "return", "model", ",", "json_config", "[", "\"hidden_size\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.collate_fn": [[151, 161], ["len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["None"], ["", "def", "collate_fn", "(", "examples", ")", ":", "\n", "    ", "bsz", "=", "len", "(", "examples", ")", "\n", "\n", "input_ids", "=", "[", "i", "[", "0", "]", "for", "i", "in", "examples", "]", "\n", "labels", "=", "[", "i", "[", "1", "]", "for", "i", "in", "examples", "]", "\n", "\n", "input_ids", "=", "[", "[", "CLS_TOKEN_ID", "]", "+", "[", "NUM_SPECIAL_TOKEN", "+", "j", "for", "j", "in", "i", "]", "+", "[", "SEP_TOKEN_ID", "]", "for", "i", "in", "input_ids", "]", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "input_ids", ")", "\n", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "return", "input_ids", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.pathfinder.main": [[163, 182], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "pathfinder.load_datasets", "pathfinder.load_model", "pathfinder.ImageModel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "pytorch_lightning.Trainer", "pl.Trainer.fit"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_datasets", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data-dir\"", ",", "default", "=", "\"../../data/lra_release/lra_release/pathfinder32/curv_contour_length_14\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-config\"", ",", "default", "=", "\"configs/bert_prenorm_pathfinder.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--size\"", ",", "type", "=", "int", ",", "default", "=", "32", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch-size\"", ",", "type", "=", "int", ",", "default", "=", "32", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epochs\"", ",", "type", "=", "int", ",", "default", "=", "80", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "train_dataset", ",", "val_dataset", ",", "test_dataset", "=", "load_datasets", "(", "args", ")", "\n", "encoder", ",", "hidden_size", "=", "load_model", "(", "args", ",", "NUM_SPECIAL_TOKEN", "+", "256", ",", "args", ".", "size", "**", "2", "+", "2", ")", "\n", "model", "=", "ImageModel", "(", "encoder", ",", "hidden_size", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "val_loader", "=", "DataLoader", "(", "val_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "gpus", "=", "8", ",", "accelerator", "=", "\"ddp\"", ",", "max_epochs", "=", "args", ".", "max_epochs", ")", "\n", "trainer", ".", "fit", "(", "model", ",", "train_loader", ",", "test_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.infer_time.load_model": [[20, 34], ["config_type.from_dict", "model_type", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "open", "json.load"], "function", ["None"], ["def", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "model_config", ")", "as", "f", ":", "\n", "        ", "json_config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_config", "[", "\"pad_token_id\"", "]", "=", "PAD_TOKEN_ID", "\n", "json_config", "[", "\"max_position_embeddings\"", "]", "=", "max_position_embeddings", "\n", "json_config", "[", "\"vocab_size\"", "]", "=", "vocab_size", "\n", "config_type", ",", "model_type", "=", "MODEL_MAP", "[", "json_config", "[", "\"model_type\"", "]", "]", "\n", "config", "=", "config_type", ".", "from_dict", "(", "json_config", ")", "\n", "model", "=", "model_type", "(", "config", ")", "\n", "return", "model", ",", "json_config", "[", "\"hidden_size\"", "]", "\n", "\n", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "input_ids", ")", "\n", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.infer_time.main": [[36, 60], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "infer_time.load_model", "torch.ones", "torch.ones", "torch.no_grad", "torch.no_grad", "range", "torch.Timer", "print", "encoder.cuda.cuda", "dummy_input.cuda.cuda", "range", "torch.Timer", "print", "encoder.cuda.", "benchmark.Timer.timeit", "encoder.cuda.", "benchmark.Timer.timeit"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-config\"", ",", "default", "=", "\"configs/bert_prenorm_pathfinder.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-length\"", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab-size\"", ",", "type", "=", "int", ",", "default", "=", "259", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch-size\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--runs\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "encoder", ",", "hidden_size", "=", "load_model", "(", "args", ",", "args", ".", "vocab_size", ",", "args", ".", "max_length", ")", "\n", "dummy_input", "=", "torch", ".", "ones", "(", "args", ".", "batch_size", ",", "args", ".", "max_length", ",", "dtype", "=", "int", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "encoder", "(", "dummy_input", ")", "\n", "", "timer", "=", "benchmark", ".", "Timer", "(", "\"encoder(dummy_input)\"", ",", "globals", "=", "{", "\"encoder\"", ":", "encoder", ",", "\"dummy_input\"", ":", "dummy_input", "}", ")", "\n", "print", "(", "timer", ".", "timeit", "(", "args", ".", "runs", ")", ")", "\n", "\n", "encoder", "=", "encoder", ".", "cuda", "(", ")", "\n", "dummy_input", "=", "dummy_input", ".", "cuda", "(", ")", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "encoder", "(", "dummy_input", ")", "\n", "", "timer", "=", "benchmark", ".", "Timer", "(", "\"encoder(dummy_input)\"", ",", "globals", "=", "{", "\"encoder\"", ":", "encoder", ",", "\"dummy_input\"", ":", "dummy_input", "}", ")", "\n", "print", "(", "timer", ".", "timeit", "(", "10", "*", "args", ".", "runs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.ImdbBytesDataset.__init__": [[39, 43], ["len", "len", "codecs.encode"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "text", ",", "label", ")", ":", "\n", "        ", "assert", "len", "(", "text", ")", "==", "len", "(", "label", ")", "\n", "self", ".", "text", "=", "[", "codecs", ".", "encode", "(", "i", ")", "for", "i", "in", "text", "]", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.ImdbBytesDataset.__len__": [[44, 46], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.ImdbBytesDataset.__getitem__": [[47, 49], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "text", "[", "idx", "]", ",", "self", ".", "label", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.TextClassificationModel.__init__": [[52, 58], ["pytorch_lightning.LightningModule.__init__", "torch.nn.Linear", "torch.nn.Linear", "pytorch_lightning.metrics.Accuracy", "pytorch_lightning.metrics.Accuracy"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", "\n", "self", ".", "train_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "self", ".", "valid_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.TextClassificationModel.forward": [[59, 63], ["text_classification.TextClassificationModel.decoder().squeeze", "text_classification.TextClassificationModel.encoder", "text_classification.TextClassificationModel.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ")", ":", "\n", "        ", "pooler_output", "=", "self", ".", "encoder", "(", "input_ids", ",", "attention_mask", ")", "[", "\"pooler_output\"", "]", "\n", "logits", "=", "self", ".", "decoder", "(", "pooler_output", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.TextClassificationModel.training_step": [[64, 71], ["text_classification.TextClassificationModel.", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "text_classification.TextClassificationModel.train_acc", "text_classification.TextClassificationModel.log", "labels.to", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "attention_mask", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ",", "attention_mask", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "labels", ".", "to", "(", "torch", ".", "float", ")", ")", "\n", "self", ".", "train_acc", "(", "F", ".", "sigmoid", "(", "logits", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"train_acc\"", ",", "self", ".", "train_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.TextClassificationModel.validation_step": [[72, 79], ["text_classification.TextClassificationModel.", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "text_classification.TextClassificationModel.valid_acc", "text_classification.TextClassificationModel.log", "labels.to", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "attention_mask", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ",", "attention_mask", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "labels", ".", "to", "(", "torch", ".", "float", ")", ")", "\n", "self", ".", "valid_acc", "(", "F", ".", "sigmoid", "(", "logits", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"valid_acc\"", ",", "self", ".", "valid_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.TextClassificationModel.configure_optimizers": [[80, 92], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "text_classification.TextClassificationModel.parameters", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "math.sqrt"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "1e-5", ",", "weight_decay", "=", "0.0", ")", "\n", "warmup", "=", "4000", "\n", "def", "lr_lambda", "(", "step", ")", ":", "\n", "            ", "if", "step", "<", "warmup", ":", "\n", "                ", "return", "step", "/", "warmup", "\n", "", "return", "1", "/", "math", ".", "sqrt", "(", "step", "/", "warmup", ")", "\n", "", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ")", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.format_num": [[29, 33], ["str", "len", "len"], "function", ["None"], ["def", "format_num", "(", "n", ")", ":", "\n", "    ", "f", "=", "'{0:.4g}'", ".", "format", "(", "n", ")", ".", "replace", "(", "'+0'", ",", "'+'", ")", ".", "replace", "(", "'-0'", ",", "'-'", ")", "\n", "n", "=", "str", "(", "n", ")", "\n", "return", "f", "if", "len", "(", "f", ")", "<", "len", "(", "n", ")", "else", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset": [[96, 125], ["logger.info", "text_classification.ImdbBytesDataset", "logger.info", "logger.info", "os.path.join", "os.listdir", "range", "FILENAME_PATTERN.match", "int", "int", "os.path.join", "f.read.strip", "len", "text.append", "label.append", "FILENAME_PATTERN.match.group", "FILENAME_PATTERN.match.group", "open", "f.read", "len", "max", "sum", "sum", "sum", "len", "len", "len"], "function", ["None"], ["def", "load_dataset", "(", "path", ")", ":", "\n", "    ", "logger", ".", "info", "(", "f\"Loading {path}\"", ")", "\n", "text", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "for", "label_id", ",", "label_name", "in", "(", "(", "0", ",", "\"neg\"", ")", ",", "(", "1", ",", "\"pos\"", ")", ")", ":", "\n", "        ", "subpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "label_name", ")", "\n", "text_dict", "=", "{", "}", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "subpath", ")", ":", "\n", "            ", "match", "=", "FILENAME_PATTERN", ".", "match", "(", "filename", ")", "\n", "if", "not", "match", ":", "\n", "                ", "continue", "\n", "", "unique_id", "=", "int", "(", "match", ".", "group", "(", "1", ")", ")", "\n", "score", "=", "int", "(", "match", ".", "group", "(", "2", ")", ")", "\n", "assert", "unique_id", "not", "in", "text_dict", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "subpath", ",", "filename", ")", "\n", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "                ", "content", "=", "f", ".", "read", "(", ")", "\n", "", "content", "=", "content", ".", "strip", "(", ")", "\n", "assert", "\"\\n\"", "not", "in", "content", "\n", "text_dict", "[", "unique_id", "]", "=", "content", "\n", "", "for", "i", "in", "range", "(", "len", "(", "text_dict", ")", ")", ":", "\n", "            ", "text", ".", "append", "(", "text_dict", "[", "i", "]", ")", "\n", "label", ".", "append", "(", "label_id", ")", "\n", "", "", "dataset", "=", "ImdbBytesDataset", "(", "text", ",", "label", ")", "\n", "\n", "text", "=", "dataset", ".", "text", "\n", "logger", ".", "info", "(", "f\"{len(text)} seqs, max len {max(len(i) for i in text)}, avg len {sum(len(i) for i in text)/len(text)}\"", ")", "\n", "logger", ".", "info", "(", "f\"#neg: {sum(1 for i in label if i == 0)}, #pos: {sum(1 for i in label if i == 1)}\"", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_datasets": [[128, 139], ["os.path.join", "os.path.join", "text_classification.load_dataset", "text_classification.load_dataset"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_dataset"], ["", "def", "load_datasets", "(", "args", ")", ":", "\n", "    ", "train_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"train\"", ")", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"test\"", ")", "\n", "\n", "train_dataset", "=", "load_dataset", "(", "train_path", ")", "\n", "test_dataset", "=", "load_dataset", "(", "test_path", ")", "\n", "\n", "train_dataset", ".", "text", "=", "[", "i", "[", ":", "args", ".", "max_length", "-", "2", "]", "for", "i", "in", "train_dataset", ".", "text", "]", "\n", "test_dataset", ".", "text", "=", "[", "i", "[", ":", "args", ".", "max_length", "-", "2", "]", "for", "i", "in", "test_dataset", ".", "text", "]", "\n", "\n", "return", "train_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.load_model": [[141, 151], ["config_type.from_dict", "model_type", "open", "json.load"], "function", ["None"], ["", "def", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "model_config", ")", "as", "f", ":", "\n", "        ", "json_config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_config", "[", "\"pad_token_id\"", "]", "=", "PAD_TOKEN_ID", "\n", "json_config", "[", "\"max_position_embeddings\"", "]", "=", "max_position_embeddings", "\n", "json_config", "[", "\"vocab_size\"", "]", "=", "vocab_size", "\n", "config_type", ",", "model_type", "=", "MODEL_MAP", "[", "json_config", "[", "\"model_type\"", "]", "]", "\n", "config", "=", "config_type", ".", "from_dict", "(", "json_config", ")", "\n", "model", "=", "model_type", "(", "config", ")", "\n", "return", "model", ",", "json_config", "[", "\"hidden_size\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.collate_fn": [[153, 168], ["len", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "int", "len", "len"], "function", ["None"], ["", "def", "collate_fn", "(", "examples", ")", ":", "\n", "    ", "bsz", "=", "len", "(", "examples", ")", "\n", "\n", "input_ids", "=", "[", "[", "int", "(", "j", ")", "+", "NUM_SPECIAL_TOKEN", "for", "j", "in", "i", "[", "0", "]", "]", "for", "i", "in", "examples", "]", "\n", "labels", "=", "[", "i", "[", "1", "]", "for", "i", "in", "examples", "]", "\n", "\n", "input_ids", "=", "[", "[", "CLS_TOKEN_ID", "]", "+", "i", "+", "[", "SEP_TOKEN_ID", "]", "for", "i", "in", "input_ids", "]", "\n", "max_len", "=", "max", "(", "len", "(", "i", ")", "for", "i", "in", "input_ids", ")", "\n", "pad_input_ids", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "zeros", "(", "(", "bsz", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "for", "i", ",", "seq", "in", "enumerate", "(", "input_ids", ")", ":", "\n", "        ", "pad_input_ids", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "torch", ".", "LongTensor", "(", "seq", ")", "\n", "attention_mask", "[", "i", ",", ":", "len", "(", "seq", ")", "]", "=", "1", "\n", "", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "return", "pad_input_ids", ",", "attention_mask", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.stats_classifier": [[170, 183], ["collections.defaultdict", "range", "range", "len", "model[].update", "len", "len", "collections.defaultdict.items", "v.most_common"], "function", ["None"], ["", "def", "stats_classifier", "(", "train_dataset", ",", "test_dataset", ")", ":", "\n", "    ", "model", "=", "collections", ".", "defaultdict", "(", "collections", ".", "Counter", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "train_dataset", ")", ")", ":", "\n", "        ", "source", ",", "label", "=", "train_dataset", "[", "i", "]", "\n", "model", "[", "source", "[", "0", "]", "]", ".", "update", "(", "[", "label", "]", ")", "\n", "", "model", "=", "{", "k", ":", "v", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "for", "k", ",", "v", "in", "model", ".", "items", "(", ")", "}", "\n", "\n", "correct", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "test_dataset", ")", ")", ":", "\n", "        ", "source", ",", "label", "=", "test_dataset", "[", "i", "]", "\n", "if", "source", "[", "0", "]", "in", "model", "and", "model", "[", "source", "[", "0", "]", "]", "==", "label", ":", "\n", "            ", "correct", "+=", "1", "\n", "", "", "return", "correct", "/", "len", "(", "test_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.main": [[185, 204], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_classification.load_datasets", "logger.info", "text_classification.load_model", "text_classification.TextClassificationModel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "pytorch_lightning.Trainer", "pl.Trainer.fit", "text_classification.stats_classifier"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_datasets", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.text_classification.stats_classifier"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data-dir\"", ",", "default", "=", "\"../../data/aclImdb\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-config\"", ",", "default", "=", "\"configs/bert_prenorm_tc.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-length\"", ",", "type", "=", "int", ",", "default", "=", "4000", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch-size\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epochs\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "train_dataset", ",", "test_dataset", "=", "load_datasets", "(", "args", ")", "\n", "logger", ".", "info", "(", "f\"Baseline on test: {stats_classifier(train_dataset, test_dataset)}\"", ")", "\n", "encoder", ",", "hidden_size", "=", "load_model", "(", "args", ",", "NUM_SPECIAL_TOKEN", "+", "256", ",", "args", ".", "max_length", ")", "\n", "model", "=", "TextClassificationModel", "(", "encoder", ",", "hidden_size", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "gpus", "=", "8", ",", "accelerator", "=", "\"ddp\"", ",", "max_epochs", "=", "args", ".", "max_epochs", ")", "\n", "trainer", ".", "fit", "(", "model", ",", "train_loader", ",", "test_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.ImageModel.__init__": [[37, 43], ["pytorch_lightning.LightningModule.__init__", "torch.nn.Linear", "torch.nn.Linear", "pytorch_lightning.metrics.Accuracy", "pytorch_lightning.metrics.Accuracy"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "hidden_size", ",", "num_label", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "num_label", ")", "\n", "self", ".", "train_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "self", ".", "valid_acc", "=", "pl", ".", "metrics", ".", "Accuracy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.ImageModel.forward": [[44, 48], ["image.ImageModel.decoder", "image.ImageModel.encoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "pooler_output", "=", "self", ".", "encoder", "(", "input_ids", ")", "[", "\"pooler_output\"", "]", "\n", "logits", "=", "self", ".", "decoder", "(", "pooler_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.ImageModel.training_step": [[49, 56], ["image.ImageModel.", "torch.cross_entropy", "torch.cross_entropy", "image.ImageModel.train_acc", "image.ImageModel.log", "image.ImageModel.argmax"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "labels", ")", "\n", "self", ".", "train_acc", "(", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"train_acc\"", ",", "self", ".", "train_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.ImageModel.validation_step": [[57, 64], ["image.ImageModel.", "torch.cross_entropy", "torch.cross_entropy", "image.ImageModel.valid_acc", "image.ImageModel.log", "image.ImageModel.argmax"], "methods", ["None"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "input_ids", ",", "labels", "=", "batch", "\n", "logits", "=", "self", "(", "input_ids", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "labels", ")", "\n", "self", ".", "valid_acc", "(", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ",", "labels", ")", "\n", "self", ".", "log", "(", "\"valid_acc\"", ",", "self", ".", "valid_acc", ",", "on_step", "=", "True", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.ImageModel.configure_optimizers": [[65, 77], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "image.ImageModel.parameters", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "math.sqrt"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "1e-2", ",", "weight_decay", "=", "0.0", ")", "\n", "warmup", "=", "200", "\n", "def", "lr_lambda", "(", "step", ")", ":", "\n", "            ", "if", "step", "<", "warmup", ":", "\n", "                ", "return", "step", "/", "warmup", "\n", "", "return", "1", "/", "math", ".", "sqrt", "(", "step", "/", "warmup", ")", "\n", "", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ")", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.format_num": [[27, 31], ["str", "len", "len"], "function", ["None"], ["def", "format_num", "(", "n", ")", ":", "\n", "    ", "f", "=", "'{0:.4g}'", ".", "format", "(", "n", ")", ".", "replace", "(", "'+0'", ",", "'+'", ")", ".", "replace", "(", "'-0'", ",", "'-'", ")", "\n", "n", "=", "str", "(", "n", ")", "\n", "return", "f", "if", "len", "(", "f", ")", "<", "len", "(", "n", ")", "else", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_datasets": [[79, 92], ["torchvision.datasets.CIFAR10", "torchvision.datasets.CIFAR10", "logger.info", "logger.info", "len", "set", "len", "len"], "function", ["None"], ["", "", "def", "load_datasets", "(", "args", ")", ":", "\n", "    ", "train_dataset", "=", "torchvision", ".", "datasets", ".", "CIFAR10", "(", "args", ".", "data_dir", ",", "download", "=", "True", ",", "train", "=", "True", ")", "\n", "test_dataset", "=", "torchvision", ".", "datasets", ".", "CIFAR10", "(", "args", ".", "data_dir", ",", "download", "=", "True", ",", "train", "=", "False", ")", "\n", "\n", "for", "d", "in", "(", "train_dataset", ",", "test_dataset", ")", ":", "\n", "        ", "for", "i", ",", "l", "in", "d", ":", "\n", "            ", "assert", "i", ".", "width", "==", "i", ".", "height", "==", "args", ".", "size", "\n", "assert", "0", "<=", "l", "<", "args", ".", "num_label", "\n", "", "assert", "len", "(", "set", "(", "l", "for", "i", ",", "l", "in", "d", ")", ")", "==", "args", ".", "num_label", "\n", "\n", "", "logger", ".", "info", "(", "f\"#train: {len(train_dataset)}, #test: {len(test_dataset)}\"", ")", "\n", "logger", ".", "info", "(", "f\"image size: {train_dataset[0][0].size}, #labels: {args.num_label}\"", ")", "\n", "return", "train_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model": [[94, 104], ["config_type.from_dict", "model_type", "open", "json.load"], "function", ["None"], ["", "def", "load_model", "(", "args", ",", "vocab_size", ",", "max_position_embeddings", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "model_config", ")", "as", "f", ":", "\n", "        ", "json_config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_config", "[", "\"pad_token_id\"", "]", "=", "PAD_TOKEN_ID", "\n", "json_config", "[", "\"max_position_embeddings\"", "]", "=", "max_position_embeddings", "\n", "json_config", "[", "\"vocab_size\"", "]", "=", "vocab_size", "\n", "config_type", ",", "model_type", "=", "MODEL_MAP", "[", "json_config", "[", "\"model_type\"", "]", "]", "\n", "config", "=", "config_type", ".", "from_dict", "(", "json_config", ")", "\n", "model", "=", "model_type", "(", "config", ")", "\n", "return", "model", ",", "json_config", "[", "\"hidden_size\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.collate_fn": [[106, 116], ["len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "list", "i[].convert().getdata", "i[].convert"], "function", ["None"], ["", "def", "collate_fn", "(", "examples", ")", ":", "\n", "   ", "bsz", "=", "len", "(", "examples", ")", "\n", "\n", "input_ids", "=", "[", "list", "(", "i", "[", "0", "]", ".", "convert", "(", "\"L\"", ")", ".", "getdata", "(", ")", ")", "for", "i", "in", "examples", "]", "\n", "labels", "=", "[", "i", "[", "1", "]", "for", "i", "in", "examples", "]", "\n", "\n", "input_ids", "=", "[", "[", "CLS_TOKEN_ID", "]", "+", "[", "NUM_SPECIAL_TOKEN", "+", "j", "for", "j", "in", "i", "]", "+", "[", "SEP_TOKEN_ID", "]", "for", "i", "in", "input_ids", "]", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "input_ids", ")", "\n", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "return", "input_ids", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.main": [[118, 137], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "image.load_datasets", "image.load_model", "image.ImageModel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "pytorch_lightning.Trainer", "pl.Trainer.fit"], "function", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_datasets", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.long_range_arena.image.load_model"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data-dir\"", ",", "default", "=", "\"../../data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model-config\"", ",", "default", "=", "\"configs/bert_prenorm_image.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--size\"", ",", "type", "=", "int", ",", "default", "=", "32", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-label\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch-size\"", ",", "type", "=", "int", ",", "default", "=", "32", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epochs\"", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "train_dataset", ",", "test_dataset", "=", "load_datasets", "(", "args", ")", "\n", "encoder", ",", "hidden_size", "=", "load_model", "(", "args", ",", "NUM_SPECIAL_TOKEN", "+", "256", ",", "args", ".", "size", "**", "2", "+", "2", ")", "\n", "model", "=", "ImageModel", "(", "encoder", ",", "hidden_size", ",", "args", ".", "num_label", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "collate_fn", "=", "collate_fn", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "gpus", "=", "8", ",", "accelerator", "=", "\"ddp\"", ",", "max_epochs", "=", "args", ".", "max_epochs", ")", "\n", "trainer", ".", "fit", "(", "model", ",", "train_loader", ",", "test_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_config.Performer2DConfig.__init__": [[7, 24], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "qk_dim_multiplier", "=", "4", ",", "\n", "kernel_epsilon", "=", "1e-3", ",", "\n", "prefix", "=", "1", ",", "\n", "postfix", "=", "1", ",", "\n", "width", "=", "32", ",", "\n", "height", "=", "32", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "qk_dim_multiplier", "=", "qk_dim_multiplier", "\n", "self", ".", "kernel_epsilon", "=", "kernel_epsilon", "\n", "self", ".", "prefix", "=", "prefix", "\n", "self", ".", "postfix", "=", "postfix", "\n", "self", ".", "width", "=", "32", "\n", "self", ".", "height", "=", "32", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertEmbeddings.__init__": [[68, 82], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "bert.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertEmbeddings.forward": [[83, 110], ["bert.BertEmbeddings.token_type_embeddings", "bert.BertEmbeddings.LayerNorm", "bert.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "bert.BertEmbeddings.word_embeddings", "bert.BertEmbeddings.position_embeddings", "bert.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "self", ".", "position_embedding_type", "==", "\"absolute\"", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertSelfAttention.__init__": [[113, 136], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "ValueError", "torch.nn.Embedding", "torch.nn.Embedding", "hasattr"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "self", ".", "position_embedding_type", "==", "\"relative_key\"", "or", "self", ".", "position_embedding_type", "==", "\"relative_key_query\"", ":", "\n", "            ", "self", ".", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "distance_embedding", "=", "nn", ".", "Embedding", "(", "2", "*", "config", ".", "max_position_embeddings", "-", "1", ",", "self", ".", "attention_head_size", ")", "\n", "\n", "", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertSelfAttention.transpose_for_scores": [[137, 141], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertSelfAttention.forward": [[142, 235], ["bert.BertSelfAttention.query", "bert.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "bert.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "bert.BertSelfAttention.transpose", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "bert.BertSelfAttention.distance_embedding", "positional_embedding.to.to.to", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "bert.BertSelfAttention.transpose_for_scores", "bert.BertSelfAttention.transpose_for_scores", "hidden_states.size", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "context_layer.view.view.permute", "context_layer.view.view.size", "bert.BertSelfAttention.key", "bert.BertSelfAttention.value", "bert.BertSelfAttention.transpose_for_scores", "bert.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert.BertSelfAttention.transpose_for_scores", "bert.BertSelfAttention.transpose_for_scores", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "bert.BertSelfAttention.key", "bert.BertSelfAttention.value", "bert.BertSelfAttention.key", "bert.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "if", "self", ".", "position_embedding_type", "==", "\"relative_key\"", "or", "self", ".", "position_embedding_type", "==", "\"relative_key_query\"", ":", "\n", "            ", "seq_length", "=", "hidden_states", ".", "size", "(", ")", "[", "1", "]", "\n", "position_ids_l", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "hidden_states", ".", "device", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "position_ids_r", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "hidden_states", ".", "device", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "distance", "=", "position_ids_l", "-", "position_ids_r", "\n", "positional_embedding", "=", "self", ".", "distance_embedding", "(", "distance", "+", "self", ".", "max_position_embeddings", "-", "1", ")", "\n", "positional_embedding", "=", "positional_embedding", ".", "to", "(", "dtype", "=", "query_layer", ".", "dtype", ")", "# fp16 compatibility", "\n", "\n", "if", "self", ".", "position_embedding_type", "==", "\"relative_key\"", ":", "\n", "                ", "relative_position_scores", "=", "torch", ".", "einsum", "(", "\"bhld,lrd->bhlr\"", ",", "query_layer", ",", "positional_embedding", ")", "\n", "attention_scores", "=", "attention_scores", "+", "relative_position_scores", "\n", "", "elif", "self", ".", "position_embedding_type", "==", "\"relative_key_query\"", ":", "\n", "                ", "relative_position_scores_query", "=", "torch", ".", "einsum", "(", "\"bhld,lrd->bhlr\"", ",", "query_layer", ",", "positional_embedding", ")", "\n", "relative_position_scores_key", "=", "torch", ".", "einsum", "(", "\"bhrd,lrd->bhlr\"", ",", "key_layer", ",", "positional_embedding", ")", "\n", "attention_scores", "=", "attention_scores", "+", "relative_position_scores_query", "+", "relative_position_scores_key", "\n", "\n", "", "", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertSelfOutput.__init__": [[238, 243], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertSelfOutput.forward": [[244, 249], ["bert.BertSelfOutput.dense", "bert.BertSelfOutput.dropout", "bert.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertAttention.__init__": [[252, 257], ["torch.nn.Module.__init__", "bert.BertSelfAttention", "bert.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertAttention.prune_heads": [[258, 275], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "bert.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertAttention.forward": [[276, 298], ["bert.BertAttention.self", "bert.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertIntermediate.__init__": [[301, 308], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertIntermediate.forward": [[309, 313], ["bert.BertIntermediate.dense", "bert.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertOutput.__init__": [[316, 321], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertOutput.forward": [[322, 327], ["bert.BertOutput.dense", "bert.BertOutput.dropout", "bert.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertLayer.__init__": [[330, 342], ["torch.nn.Module.__init__", "bert.BertAttention", "bert.BertIntermediate", "bert.BertOutput", "bert.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertLayer.forward": [[343, 405], ["bert.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "bert.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertLayer.feed_forward_chunk": [[406, 410], ["bert.BertLayer.intermediate", "bert.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertEncoder.__init__": [[413, 417], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "bert.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertEncoder.forward": [[418, 506], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "bert.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertPooler.__init__": [[510, 514], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertPooler.forward": [[515, 522], ["bert.BertPooler.dense", "bert.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertPreTrainedModel._init_weights": [[534, 549], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertModel.__init__": [[637, 647], ["transformers.modeling_utils.PreTrainedModel.__init__", "bert.BertEmbeddings", "bert.BertEncoder", "bert.BertModel.init_weights", "bert.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertModel.get_input_embeddings": [[648, 650], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertModel.set_input_embeddings": [[651, 653], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertModel._prune_heads": [[654, 661], ["heads_to_prune.items", "bert.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert.BertModel.forward": [[662, 791], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "bert.BertModel.get_extended_attention_mask", "bert.BertModel.get_head_mask", "bert.BertModel.embeddings", "bert.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "bert.BertModel.invert_attention_mask", "bert.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_config.LinformerConfig.__init__": [[7, 14], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "linformer_k", "=", "256", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "linformer_k", "=", "linformer_k", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.SinusoidalPositionalEmbedding.__init__": [[66, 75], ["torch.nn.Module.__init__", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "bert_prenorm_sin.SinusoidalPositionalEmbedding.register_buffer", "math.log", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.cat().view.unsqueeze", "torch.cat().view.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "max_position_embeddings", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "embedding_dim", "%", "2", "==", "0", "\n", "half_dim", "=", "hidden_size", "//", "2", "\n", "emb", "=", "math", ".", "log", "(", "10000", ")", "/", "(", "half_dim", "-", "1", ")", "\n", "emb", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "half_dim", ",", "dtype", "=", "torch", ".", "float", ")", "*", "-", "emb", ")", "\n", "emb", "=", "torch", ".", "arange", "(", "max_position_embeddings", ",", "dtype", "=", "torch", ".", "float", ")", ".", "unsqueeze", "(", "1", ")", "*", "emb", ".", "unsqueeze", "(", "0", ")", "\n", "emb", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "emb", ")", ",", "torch", ".", "cos", "(", "emb", ")", "]", ",", "dim", "=", "1", ")", ".", "view", "(", "max_position_embeddings", ",", "-", "1", ")", "\n", "self", ".", "register_buffer", "(", "\"weights\"", ",", "emb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.SinusoidalPositionalEmbedding.forward": [[76, 79], ["bert_prenorm_sin.SinusoidalPositionalEmbedding.weights.index_select().view().detach", "bert_prenorm_sin.SinusoidalPositionalEmbedding.weights.index_select().view", "bert_prenorm_sin.SinusoidalPositionalEmbedding.weights.index_select", "positions_ids.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "position_ids", ")", ":", "\n", "        ", "bsz", ",", "seq_len", "=", "position_ids", ".", "shape", "\n", "return", "self", ".", "weights", ".", "index_select", "(", "0", ",", "positions_ids", ".", "view", "(", "-", "1", ")", ")", ".", "view", "(", "bsz", ",", "seq_len", ",", "-", "1", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertEmbeddings.__init__": [[83, 97], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "bert_prenorm_sin.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertEmbeddings.forward": [[98, 125], ["bert_prenorm_sin.BertEmbeddings.token_type_embeddings", "bert_prenorm_sin.BertEmbeddings.LayerNorm", "bert_prenorm_sin.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "bert_prenorm_sin.BertEmbeddings.word_embeddings", "bert_prenorm_sin.BertEmbeddings.position_embeddings", "bert_prenorm_sin.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfAttention.__init__": [[128, 157], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "ValueError", "bert_prenorm_sin.BertSelfAttention.generate_random_permutation", "bert_prenorm_sin.BertSelfAttention.expand_permutation", "bert_prenorm_sin.BertSelfAttention.register_buffer", "hasattr", "bert_prenorm_sin.BertSelfAttention.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "raw_permutation", "=", "self", ".", "generate_random_permutation", "(", "\n", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ",", "\n", "0xdeadbeefdeadbeef", ",", "\n", ")", "\n", "permutation", "=", "self", ".", "expand_permutation", "(", "config", ".", "max_position_embeddings", ",", "raw_permutation", ")", "\n", "self", ".", "register_buffer", "(", "\"permutation\"", ",", "permutation", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfAttention.generate_random_permutation": [[158, 164], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["", "def", "generate_random_permutation", "(", "self", ",", "num_head", ",", "head_size", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "permutation", "=", "[", "torch", ".", "randperm", "(", "head_size", ",", "generator", "=", "rng", ")", "for", "_", "in", "range", "(", "num_head", ")", "]", "\n", "permutation", "=", "torch", ".", "stack", "(", "permutation", ",", "dim", "=", "0", ")", "\n", "return", "permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfAttention.expand_permutation": [[165, 174], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "expand_permutation", "(", "self", ",", "max_seq_length", ",", "permutation", ")", ":", "\n", "        ", "num_head", ",", "head_size", "=", "permutation", ".", "shape", "\n", "expanded", "=", "[", "torch", ".", "arange", "(", "head_size", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_head", ",", "head_size", ")", "]", "\n", "for", "_", "in", "range", "(", "max_seq_length", "-", "1", ")", ":", "\n", "            ", "previous", "=", "expanded", "[", "-", "1", "]", "\n", "current", "=", "previous", ".", "gather", "(", "-", "1", ",", "permutation", ")", "\n", "expanded", ".", "append", "(", "current", ")", "\n", "", "expanded", "=", "torch", ".", "stack", "(", "expanded", ",", "dim", "=", "1", ")", "\n", "return", "expanded", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfAttention.transpose_for_scores": [[175, 179], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfAttention.forward": [[180, 262], ["bert_prenorm_sin.BertSelfAttention.LayerNorm", "bert_prenorm_sin.BertSelfAttention.query", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "bert_prenorm_sin.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "query_layer.gather.gather.gather", "bert_prenorm_sin.BertSelfAttention.gather", "bert_prenorm_sin.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "bert_prenorm_sin.BertSelfAttention.permutation[].expand_as", "bert_prenorm_sin.BertSelfAttention.permutation[].expand_as", "context_layer.view.view.permute", "context_layer.view.view.size", "bert_prenorm_sin.BertSelfAttention.key", "bert_prenorm_sin.BertSelfAttention.value", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "bert_prenorm_sin.BertSelfAttention.transpose_for_scores", "bert_prenorm_sin.BertSelfAttention.key", "bert_prenorm_sin.BertSelfAttention.value", "bert_prenorm_sin.BertSelfAttention.key", "bert_prenorm_sin.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "query_layer", "=", "query_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "query_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "query_layer", ")", ")", "\n", "key_layer", "=", "key_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "key_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "key_layer", ")", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfOutput.__init__": [[265, 269], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertSelfOutput.forward": [[270, 275], ["bert_prenorm_sin.BertSelfOutput.dense", "bert_prenorm_sin.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertAttention.__init__": [[278, 283], ["torch.nn.Module.__init__", "bert_prenorm_sin.BertSelfAttention", "bert_prenorm_sin.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertAttention.prune_heads": [[284, 301], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "bert_prenorm_sin.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertAttention.forward": [[302, 324], ["bert_prenorm_sin.BertAttention.self", "bert_prenorm_sin.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertIntermediate.__init__": [[327, 335], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertIntermediate.forward": [[336, 341], ["bert_prenorm_sin.BertIntermediate.LayerNorm", "bert_prenorm_sin.BertIntermediate.dense", "bert_prenorm_sin.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertOutput.__init__": [[344, 348], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertOutput.forward": [[349, 354], ["bert_prenorm_sin.BertOutput.dense", "bert_prenorm_sin.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertLayer.__init__": [[357, 369], ["torch.nn.Module.__init__", "bert_prenorm_sin.BertAttention", "bert_prenorm_sin.BertIntermediate", "bert_prenorm_sin.BertOutput", "bert_prenorm_sin.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertLayer.forward": [[370, 432], ["bert_prenorm_sin.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "bert_prenorm_sin.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertLayer.feed_forward_chunk": [[433, 437], ["bert_prenorm_sin.BertLayer.intermediate", "bert_prenorm_sin.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertEncoder.__init__": [[440, 444], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "bert_prenorm_sin.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertEncoder.forward": [[445, 533], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "bert_prenorm_sin.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertPooler.__init__": [[537, 541], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertPooler.forward": [[542, 549], ["bert_prenorm_sin.BertPooler.dense", "bert_prenorm_sin.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertPreTrainedModel._init_weights": [[561, 576], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertModel.__init__": [[664, 674], ["transformers.modeling_utils.PreTrainedModel.__init__", "bert_prenorm_sin.BertEmbeddings", "bert_prenorm_sin.BertEncoder", "bert_prenorm_sin.BertModel.init_weights", "bert_prenorm_sin.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertModel.get_input_embeddings": [[675, 677], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertModel.set_input_embeddings": [[678, 680], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertModel._prune_heads": [[681, 688], ["heads_to_prune.items", "bert_prenorm_sin.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm_sin.BertModel.forward": [[689, 818], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "bert_prenorm_sin.BertModel.get_extended_attention_mask", "bert_prenorm_sin.BertModel.get_head_mask", "bert_prenorm_sin.BertModel.embeddings", "bert_prenorm_sin.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "bert_prenorm_sin.BertModel.invert_attention_mask", "bert_prenorm_sin.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_config.PerformerConfig.__init__": [[7, 16], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "qk_dim_multiplier", "=", "4", ",", "\n", "kernel_epsilon", "=", "1e-3", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "qk_dim_multiplier", "=", "qk_dim_multiplier", "\n", "self", ".", "kernel_epsilon", "=", "kernel_epsilon", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertEmbeddings.__init__": [[68, 82], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "bert_prenorm.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertEmbeddings.forward": [[83, 110], ["bert_prenorm.BertEmbeddings.token_type_embeddings", "bert_prenorm.BertEmbeddings.LayerNorm", "bert_prenorm.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "bert_prenorm.BertEmbeddings.word_embeddings", "bert_prenorm.BertEmbeddings.position_embeddings", "bert_prenorm.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfAttention.__init__": [[113, 142], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "ValueError", "bert_prenorm.BertSelfAttention.generate_random_permutation", "bert_prenorm.BertSelfAttention.expand_permutation", "bert_prenorm.BertSelfAttention.register_buffer", "hasattr", "bert_prenorm.BertSelfAttention.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "raw_permutation", "=", "self", ".", "generate_random_permutation", "(", "\n", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ",", "\n", "0xdeadbeefdeadbeef", ",", "\n", ")", "\n", "permutation", "=", "self", ".", "expand_permutation", "(", "config", ".", "max_position_embeddings", ",", "raw_permutation", ")", "\n", "self", ".", "register_buffer", "(", "\"permutation\"", ",", "permutation", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfAttention.generate_random_permutation": [[143, 149], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["", "def", "generate_random_permutation", "(", "self", ",", "num_head", ",", "head_size", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "permutation", "=", "[", "torch", ".", "randperm", "(", "head_size", ",", "generator", "=", "rng", ")", "for", "_", "in", "range", "(", "num_head", ")", "]", "\n", "permutation", "=", "torch", ".", "stack", "(", "permutation", ",", "dim", "=", "0", ")", "\n", "return", "permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfAttention.expand_permutation": [[150, 159], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "expand_permutation", "(", "self", ",", "max_seq_length", ",", "permutation", ")", ":", "\n", "        ", "num_head", ",", "head_size", "=", "permutation", ".", "shape", "\n", "expanded", "=", "[", "torch", ".", "arange", "(", "head_size", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_head", ",", "head_size", ")", "]", "\n", "for", "_", "in", "range", "(", "max_seq_length", "-", "1", ")", ":", "\n", "            ", "previous", "=", "expanded", "[", "-", "1", "]", "\n", "current", "=", "previous", ".", "gather", "(", "-", "1", ",", "permutation", ")", "\n", "expanded", ".", "append", "(", "current", ")", "\n", "", "expanded", "=", "torch", ".", "stack", "(", "expanded", ",", "dim", "=", "1", ")", "\n", "return", "expanded", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfAttention.transpose_for_scores": [[160, 164], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfAttention.forward": [[165, 247], ["bert_prenorm.BertSelfAttention.LayerNorm", "bert_prenorm.BertSelfAttention.query", "bert_prenorm.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "bert_prenorm.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "query_layer.gather.gather.gather", "bert_prenorm.BertSelfAttention.gather", "bert_prenorm.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "bert_prenorm.BertSelfAttention.transpose_for_scores", "bert_prenorm.BertSelfAttention.transpose_for_scores", "bert_prenorm.BertSelfAttention.permutation[].expand_as", "bert_prenorm.BertSelfAttention.permutation[].expand_as", "context_layer.view.view.permute", "context_layer.view.view.size", "bert_prenorm.BertSelfAttention.key", "bert_prenorm.BertSelfAttention.value", "bert_prenorm.BertSelfAttention.transpose_for_scores", "bert_prenorm.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_prenorm.BertSelfAttention.transpose_for_scores", "bert_prenorm.BertSelfAttention.transpose_for_scores", "bert_prenorm.BertSelfAttention.key", "bert_prenorm.BertSelfAttention.value", "bert_prenorm.BertSelfAttention.key", "bert_prenorm.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "query_layer", "=", "query_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "query_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "query_layer", ")", ")", "\n", "key_layer", "=", "key_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "key_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "key_layer", ")", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfOutput.__init__": [[250, 254], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertSelfOutput.forward": [[255, 260], ["bert_prenorm.BertSelfOutput.dense", "bert_prenorm.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertAttention.__init__": [[263, 268], ["torch.nn.Module.__init__", "bert_prenorm.BertSelfAttention", "bert_prenorm.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertAttention.prune_heads": [[269, 286], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "bert_prenorm.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertAttention.forward": [[287, 309], ["bert_prenorm.BertAttention.self", "bert_prenorm.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertIntermediate.__init__": [[312, 320], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertIntermediate.forward": [[321, 326], ["bert_prenorm.BertIntermediate.LayerNorm", "bert_prenorm.BertIntermediate.dense", "bert_prenorm.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertOutput.__init__": [[329, 333], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertOutput.forward": [[334, 339], ["bert_prenorm.BertOutput.dense", "bert_prenorm.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertLayer.__init__": [[342, 354], ["torch.nn.Module.__init__", "bert_prenorm.BertAttention", "bert_prenorm.BertIntermediate", "bert_prenorm.BertOutput", "bert_prenorm.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertLayer.forward": [[355, 417], ["bert_prenorm.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "bert_prenorm.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertLayer.feed_forward_chunk": [[418, 422], ["bert_prenorm.BertLayer.intermediate", "bert_prenorm.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertEncoder.__init__": [[425, 429], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "bert_prenorm.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertEncoder.forward": [[430, 518], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "bert_prenorm.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertPooler.__init__": [[522, 526], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertPooler.forward": [[527, 534], ["bert_prenorm.BertPooler.dense", "bert_prenorm.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertPreTrainedModel._init_weights": [[546, 561], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertModel.__init__": [[649, 659], ["transformers.modeling_utils.PreTrainedModel.__init__", "bert_prenorm.BertEmbeddings", "bert_prenorm.BertEncoder", "bert_prenorm.BertModel.init_weights", "bert_prenorm.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertModel.get_input_embeddings": [[660, 662], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertModel.set_input_embeddings": [[663, 665], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertModel._prune_heads": [[666, 673], ["heads_to_prune.items", "bert_prenorm.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.bert_prenorm.BertModel.forward": [[674, 803], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "bert_prenorm.BertModel.get_extended_attention_mask", "bert_prenorm.BertModel.get_head_mask", "bert_prenorm.BertModel.embeddings", "bert_prenorm.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "bert_prenorm.BertModel.invert_attention_mask", "bert_prenorm.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_attention.PerformerAttention.__init__": [[6, 9], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "kernel_epsilon", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "kernel_epsilon", "=", "kernel_epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_attention.PerformerAttention.forward": [[10, 31], ["torch.nn.functional.relu", "torch.nn.functional.relu", "key.transpose().sum", "mask.unsqueeze().unsqueeze", "key.transpose", "key.transpose", "mask.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Parameters:\n            query: torch.tensor(batch_size, num_heads, sequence_length, dim)\n            key: torch.tensor(batch_size, num_heads, sequence_length, dim)\n            value: torch.tensor(batch_size, num_heads, sequence_length, dim)\n            mask: torch.tensor(batch_size, sequence_length)\n        Returns:\n            context: torch.tensor(batch_size, num_heads, sequence_length, dim)\n        \"\"\"", "\n", "query", "=", "nn", ".", "functional", ".", "relu", "(", "query", ")", "+", "self", ".", "kernel_epsilon", "\n", "key", "=", "nn", ".", "functional", ".", "relu", "(", "key", ")", "+", "self", ".", "kernel_epsilon", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "key", "=", "key", "*", "mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "numerator", "=", "query", "@", "(", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", "@", "value", ")", "\n", "denominator", "=", "query", "@", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "output", "=", "numerator", "/", "denominator", "\n", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_config.NystromformerConfig.__init__": [[7, 14], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_landmarks", "=", "64", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "num_landmarks", "=", "num_landmarks", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertEmbeddings.__init__": [[70, 84], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "performer_prenorm.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertEmbeddings.forward": [[85, 112], ["performer_prenorm.BertEmbeddings.token_type_embeddings", "performer_prenorm.BertEmbeddings.LayerNorm", "performer_prenorm.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "performer_prenorm.BertEmbeddings.word_embeddings", "performer_prenorm.BertEmbeddings.position_embeddings", "performer_prenorm.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfAttention.__init__": [[115, 146], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "performer_attention.PerformerAttention", "ValueError", "performer_prenorm.BertSelfAttention.generate_random_permutation", "performer_prenorm.BertSelfAttention.expand_permutation", "performer_prenorm.BertSelfAttention.register_buffer", "hasattr", "performer_prenorm.BertSelfAttention.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "config", ".", "qk_dim_multiplier", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "config", ".", "qk_dim_multiplier", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "raw_permutation", "=", "self", ".", "generate_random_permutation", "(", "\n", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", "*", "config", ".", "qk_dim_multiplier", ",", "\n", "0xdeadbeefdeadbeef", ",", "\n", ")", "\n", "permutation", "=", "self", ".", "expand_permutation", "(", "config", ".", "max_position_embeddings", ",", "raw_permutation", ")", "\n", "self", ".", "register_buffer", "(", "\"permutation\"", ",", "permutation", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "self", ".", "performer_attention", "=", "PerformerAttention", "(", "config", ".", "kernel_epsilon", ")", "\n", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfAttention.generate_random_permutation": [[147, 153], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["", "def", "generate_random_permutation", "(", "self", ",", "num_head", ",", "head_size", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "permutation", "=", "[", "torch", ".", "randperm", "(", "head_size", ",", "generator", "=", "rng", ")", "for", "_", "in", "range", "(", "num_head", ")", "]", "\n", "permutation", "=", "torch", ".", "stack", "(", "permutation", ",", "dim", "=", "0", ")", "\n", "return", "permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfAttention.expand_permutation": [[154, 163], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "expand_permutation", "(", "self", ",", "max_seq_length", ",", "permutation", ")", ":", "\n", "        ", "num_head", ",", "head_size", "=", "permutation", ".", "shape", "\n", "expanded", "=", "[", "torch", ".", "arange", "(", "head_size", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_head", ",", "head_size", ")", "]", "\n", "for", "_", "in", "range", "(", "max_seq_length", "-", "1", ")", ":", "\n", "            ", "previous", "=", "expanded", "[", "-", "1", "]", "\n", "current", "=", "previous", ".", "gather", "(", "-", "1", ",", "permutation", ")", "\n", "expanded", ".", "append", "(", "current", ")", "\n", "", "expanded", "=", "torch", ".", "stack", "(", "expanded", ",", "dim", "=", "1", ")", "\n", "return", "expanded", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfAttention.transpose_for_scores": [[164, 168], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "-", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfAttention.forward": [[169, 233], ["performer_prenorm.BertSelfAttention.LayerNorm", "performer_prenorm.BertSelfAttention.query", "performer_prenorm.BertSelfAttention.transpose_for_scores", "performer_prenorm.BertSelfAttention.performer_attention", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "query_layer.gather.gather.gather", "performer_prenorm.BertSelfAttention.gather", "performer_prenorm.BertSelfAttention.transpose_for_scores", "performer_prenorm.BertSelfAttention.transpose_for_scores", "performer_prenorm.BertSelfAttention.permutation[].expand_as", "performer_prenorm.BertSelfAttention.permutation[].expand_as", "context_layer.view.view.permute", "context_layer.view.view.size", "performer_prenorm.BertSelfAttention.key", "performer_prenorm.BertSelfAttention.value", "performer_prenorm.BertSelfAttention.transpose_for_scores", "performer_prenorm.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "performer_prenorm.BertSelfAttention.transpose_for_scores", "performer_prenorm.BertSelfAttention.transpose_for_scores", "performer_prenorm.BertSelfAttention.key", "performer_prenorm.BertSelfAttention.value", "performer_prenorm.BertSelfAttention.key", "performer_prenorm.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "query_layer", "=", "query_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "query_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "query_layer", ")", ")", "\n", "key_layer", "=", "key_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "key_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "key_layer", ")", ")", "\n", "\n", "", "context_layer", "=", "self", ".", "performer_attention", "(", "query_layer", ",", "key_layer", ",", "value_layer", ",", "attention_mask", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "assert", "output_attentions", "is", "False", "\n", "outputs", "=", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfOutput.__init__": [[236, 240], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertSelfOutput.forward": [[241, 246], ["performer_prenorm.BertSelfOutput.dense", "performer_prenorm.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertAttention.__init__": [[249, 254], ["torch.nn.Module.__init__", "performer_prenorm.BertSelfAttention", "performer_prenorm.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertAttention.prune_heads": [[255, 272], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "performer_prenorm.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertAttention.forward": [[273, 295], ["performer_prenorm.BertAttention.self", "performer_prenorm.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertIntermediate.__init__": [[298, 306], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertIntermediate.forward": [[307, 312], ["performer_prenorm.BertIntermediate.LayerNorm", "performer_prenorm.BertIntermediate.dense", "performer_prenorm.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertOutput.__init__": [[315, 319], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertOutput.forward": [[320, 325], ["performer_prenorm.BertOutput.dense", "performer_prenorm.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertLayer.__init__": [[328, 340], ["torch.nn.Module.__init__", "performer_prenorm.BertAttention", "performer_prenorm.BertIntermediate", "performer_prenorm.BertOutput", "performer_prenorm.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertLayer.forward": [[341, 403], ["performer_prenorm.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "performer_prenorm.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertLayer.feed_forward_chunk": [[404, 408], ["performer_prenorm.BertLayer.intermediate", "performer_prenorm.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertEncoder.__init__": [[411, 415], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "performer_prenorm.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertEncoder.forward": [[416, 504], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "performer_prenorm.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertPooler.__init__": [[508, 512], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertPooler.forward": [[513, 520], ["performer_prenorm.BertPooler.dense", "performer_prenorm.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertPreTrainedModel._init_weights": [[532, 547], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel.__init__": [[635, 645], ["transformers.modeling_utils.PreTrainedModel.__init__", "performer_prenorm.BertEmbeddings", "performer_prenorm.BertEncoder", "performer_prenorm.BertModel.init_weights", "performer_prenorm.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel.get_input_embeddings": [[646, 648], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel.set_input_embeddings": [[649, 651], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel._prune_heads": [[652, 659], ["heads_to_prune.items", "performer_prenorm.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel.forward": [[660, 789], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "performer_prenorm.BertModel.get_extended_attention_mask", "performer_prenorm.BertModel.get_head_mask", "performer_prenorm.BertModel.embeddings", "performer_prenorm.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "performer_prenorm.BertModel.invert_attention_mask", "performer_prenorm.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel.get_extended_attention_mask": [[791, 793], ["attention_mask.to"], "methods", ["None"], ["", "def", "get_extended_attention_mask", "(", "self", ",", "attention_mask", ",", "input_shape", ",", "device", ")", ":", "\n", "        ", "return", "attention_mask", ".", "to", "(", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_prenorm.BertModel.invert_attention_mask": [[794, 796], ["encoder_attention_mask.to"], "methods", ["None"], ["", "def", "invert_attention_mask", "(", "self", ",", "encoder_attention_mask", ")", ":", "\n", "        ", "return", "1.0", "-", "encoder_attention_mask", ".", "to", "(", "self", ".", "dtype", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertEmbeddings.__init__": [[68, 82], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "nystromformer_prenorm.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertEmbeddings.forward": [[83, 110], ["nystromformer_prenorm.BertEmbeddings.token_type_embeddings", "nystromformer_prenorm.BertEmbeddings.LayerNorm", "nystromformer_prenorm.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "nystromformer_prenorm.BertEmbeddings.word_embeddings", "nystromformer_prenorm.BertEmbeddings.position_embeddings", "nystromformer_prenorm.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.__init__": [[113, 145], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "ValueError", "nystromformer_prenorm.BertSelfAttention.generate_random_permutation", "nystromformer_prenorm.BertSelfAttention.expand_permutation", "nystromformer_prenorm.BertSelfAttention.register_buffer", "hasattr", "nystromformer_prenorm.BertSelfAttention.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "raw_permutation", "=", "self", ".", "generate_random_permutation", "(", "\n", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ",", "\n", "0xdeadbeefdeadbeef", ",", "\n", ")", "\n", "permutation", "=", "self", ".", "expand_permutation", "(", "config", ".", "max_position_embeddings", ",", "raw_permutation", ")", "\n", "self", ".", "register_buffer", "(", "\"permutation\"", ",", "permutation", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n", "self", ".", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "num_landmarks", "=", "config", ".", "num_landmarks", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.iterative_inv": [[146, 154], ["torch.eye", "torch.eye", "torch.eye", "torch.eye", "range", "mat.size", "K.transpose", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.max", "torch.max", "torch.max", "torch.max", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["None"], ["", "def", "iterative_inv", "(", "self", ",", "mat", ",", "n_iter", "=", "6", ")", ":", "\n", "        ", "I", "=", "torch", ".", "eye", "(", "mat", ".", "size", "(", "-", "1", ")", ",", "device", "=", "mat", ".", "device", ")", "\n", "K", "=", "mat", "\n", "V", "=", "1", "/", "torch", ".", "max", "(", "torch", ".", "sum", "(", "K", ",", "dim", "=", "-", "2", ")", ")", "*", "K", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "for", "_", "in", "range", "(", "n_iter", ")", ":", "\n", "            ", "KV", "=", "torch", ".", "matmul", "(", "K", ",", "V", ")", "\n", "V", "=", "torch", ".", "matmul", "(", "0.25", "*", "V", ",", "13", "*", "I", "-", "torch", ".", "matmul", "(", "KV", ",", "15", "*", "I", "-", "torch", ".", "matmul", "(", "KV", ",", "7", "*", "I", "-", "KV", ")", ")", ")", "\n", "", "return", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.generate_random_permutation": [[155, 161], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["", "def", "generate_random_permutation", "(", "self", ",", "num_head", ",", "head_size", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "permutation", "=", "[", "torch", ".", "randperm", "(", "head_size", ",", "generator", "=", "rng", ")", "for", "_", "in", "range", "(", "num_head", ")", "]", "\n", "permutation", "=", "torch", ".", "stack", "(", "permutation", ",", "dim", "=", "0", ")", "\n", "return", "permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.expand_permutation": [[162, 171], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "expand_permutation", "(", "self", ",", "max_seq_length", ",", "permutation", ")", ":", "\n", "        ", "num_head", ",", "head_size", "=", "permutation", ".", "shape", "\n", "expanded", "=", "[", "torch", ".", "arange", "(", "head_size", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_head", ",", "head_size", ")", "]", "\n", "for", "_", "in", "range", "(", "max_seq_length", "-", "1", ")", ":", "\n", "            ", "previous", "=", "expanded", "[", "-", "1", "]", "\n", "current", "=", "previous", ".", "gather", "(", "-", "1", ",", "permutation", ")", "\n", "expanded", ".", "append", "(", "current", ")", "\n", "", "expanded", "=", "torch", ".", "stack", "(", "expanded", ",", "dim", "=", "1", ")", "\n", "return", "expanded", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.transpose_for_scores": [[172, 176], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.forward": [[177, 274], ["nystromformer_prenorm.BertSelfAttention.LayerNorm", "nystromformer_prenorm.BertSelfAttention.query", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad.reshape().mean", "torch.nn.functional.pad.reshape().mean", "torch.nn.functional.pad.reshape().mean", "torch.nn.functional.pad.reshape().mean", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "query_layer.gather.gather.gather", "nystromformer_prenorm.BertSelfAttention.gather", "math.sqrt", "math.sqrt", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "nystromformer_prenorm.BertSelfAttention.permutation[].expand_as", "nystromformer_prenorm.BertSelfAttention.permutation[].expand_as", "math.sqrt", "math.sqrt", "torch.nn.functional.pad.reshape", "torch.nn.functional.pad.reshape", "torch.nn.functional.pad.reshape", "torch.nn.functional.pad.reshape", "torch.nn.functional.pad.reshape().mean.transpose", "torch.nn.functional.pad.reshape().mean.transpose", "nystromformer_prenorm.BertSelfAttention.transpose", "nystromformer_prenorm.BertSelfAttention.iterative_inv", "context_layer.view.view.permute", "context_layer.view.view.size", "nystromformer_prenorm.BertSelfAttention.key", "nystromformer_prenorm.BertSelfAttention.value", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "nystromformer_prenorm.BertSelfAttention.transpose_for_scores", "nystromformer_prenorm.BertSelfAttention.key", "nystromformer_prenorm.BertSelfAttention.value", "nystromformer_prenorm.BertSelfAttention.key", "nystromformer_prenorm.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfAttention.iterative_inv", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "query_layer", "=", "query_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "query_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "query_layer", ")", ")", "\n", "key_layer", "=", "key_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "key_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "key_layer", ")", ")", "\n", "\n", "", "query_layer", "=", "query_layer", "/", "math", ".", "sqrt", "(", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", ")", "\n", "key_layer", "=", "key_layer", "/", "math", ".", "sqrt", "(", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", ")", "\n", "\n", "pad_length", "=", "(", "self", ".", "max_position_embeddings", "+", "self", ".", "num_landmarks", "-", "1", ")", "//", "self", ".", "num_landmarks", "*", "self", ".", "num_landmarks", "\n", "query_pad", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "query_layer", ",", "(", "0", ",", "0", ",", "0", ",", "pad_length", "-", "self", ".", "max_position_embeddings", ")", ",", "mode", "=", "\"reflect\"", ")", "\n", "key_pad", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "key_layer", ",", "(", "0", ",", "0", ",", "0", ",", "pad_length", "-", "self", ".", "max_position_embeddings", ")", ",", "mode", "=", "\"reflect\"", ")", "\n", "query_landmarks", "=", "query_pad", ".", "reshape", "(", "-", "1", ",", "self", ".", "num_attention_heads", ",", "self", ".", "num_landmarks", ",", "pad_length", "//", "self", ".", "num_landmarks", ",", "self", ".", "attention_head_size", ")", ".", "mean", "(", "dim", "=", "-", "2", ")", "\n", "key_landmarks", "=", "key_pad", ".", "reshape", "(", "-", "1", ",", "self", ".", "num_attention_heads", ",", "self", ".", "num_landmarks", ",", "pad_length", "//", "self", ".", "num_landmarks", ",", "self", ".", "attention_head_size", ")", ".", "mean", "(", "dim", "=", "-", "2", ")", "\n", "\n", "kernel_1", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "torch", ".", "matmul", "(", "query_layer", ",", "key_landmarks", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "kernel_2", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "torch", ".", "matmul", "(", "query_landmarks", ",", "key_landmarks", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "kernel_3", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "torch", ".", "matmul", "(", "query_landmarks", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "torch", ".", "matmul", "(", "kernel_1", ",", "self", ".", "iterative_inv", "(", "kernel_2", ")", ")", ",", "torch", ".", "matmul", "(", "kernel_3", ",", "value_layer", ")", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "# attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))", "\n", "\n", "# attention_scores = attention_scores / math.sqrt(self.attention_head_size)", "\n", "# if attention_mask is not None:", "\n", "#     # Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "#     attention_scores = attention_scores + attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# attention_probs = nn.Softmax(dim=-1)(attention_scores)", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "# attention_probs = self.dropout(attention_probs)", "\n", "\n", "# Mask heads if we want to", "\n", "# if head_mask is not None:", "\n", "#     attention_probs = attention_probs * head_mask", "\n", "\n", "# context_layer = torch.matmul(attention_probs, value_layer)", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfOutput.__init__": [[277, 281], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertSelfOutput.forward": [[282, 287], ["nystromformer_prenorm.BertSelfOutput.dense", "nystromformer_prenorm.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertAttention.__init__": [[290, 295], ["torch.nn.Module.__init__", "nystromformer_prenorm.BertSelfAttention", "nystromformer_prenorm.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertAttention.prune_heads": [[296, 313], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "nystromformer_prenorm.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertAttention.forward": [[314, 336], ["nystromformer_prenorm.BertAttention.self", "nystromformer_prenorm.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertIntermediate.__init__": [[339, 347], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertIntermediate.forward": [[348, 353], ["nystromformer_prenorm.BertIntermediate.LayerNorm", "nystromformer_prenorm.BertIntermediate.dense", "nystromformer_prenorm.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertOutput.__init__": [[356, 360], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertOutput.forward": [[361, 366], ["nystromformer_prenorm.BertOutput.dense", "nystromformer_prenorm.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertLayer.__init__": [[369, 381], ["torch.nn.Module.__init__", "nystromformer_prenorm.BertAttention", "nystromformer_prenorm.BertIntermediate", "nystromformer_prenorm.BertOutput", "nystromformer_prenorm.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertLayer.forward": [[382, 444], ["nystromformer_prenorm.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "nystromformer_prenorm.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertLayer.feed_forward_chunk": [[445, 449], ["nystromformer_prenorm.BertLayer.intermediate", "nystromformer_prenorm.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertEncoder.__init__": [[452, 456], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "nystromformer_prenorm.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertEncoder.forward": [[457, 545], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "nystromformer_prenorm.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertPooler.__init__": [[549, 553], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertPooler.forward": [[554, 561], ["nystromformer_prenorm.BertPooler.dense", "nystromformer_prenorm.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertPreTrainedModel._init_weights": [[573, 588], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertModel.__init__": [[676, 686], ["transformers.modeling_utils.PreTrainedModel.__init__", "nystromformer_prenorm.BertEmbeddings", "nystromformer_prenorm.BertEncoder", "nystromformer_prenorm.BertModel.init_weights", "nystromformer_prenorm.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertModel.get_input_embeddings": [[687, 689], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertModel.set_input_embeddings": [[690, 692], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertModel._prune_heads": [[693, 700], ["heads_to_prune.items", "nystromformer_prenorm.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.nystromformer_prenorm.BertModel.forward": [[701, 830], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "nystromformer_prenorm.BertModel.get_extended_attention_mask", "nystromformer_prenorm.BertModel.get_head_mask", "nystromformer_prenorm.BertModel.embeddings", "nystromformer_prenorm.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "nystromformer_prenorm.BertModel.invert_attention_mask", "nystromformer_prenorm.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertEmbeddings.__init__": [[70, 94], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "performer_2d_prenorm.BertEmbeddings.register_buffer", "performer_2d_prenorm.BertEmbeddings.register_buffer", "getattr", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.arange().repeat_interleave", "torch.arange().repeat_interleave", "torch.arange().repeat_interleave", "torch.arange().repeat_interleave", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "x_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "width", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "y_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "height", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"x_position_ids\"", ",", "torch", ".", "cat", "(", "[", "\n", "torch", ".", "tensor", "(", "[", "0", "]", ")", ".", "repeat", "(", "config", ".", "prefix", ")", ",", "\n", "torch", ".", "arange", "(", "config", ".", "width", ")", ".", "repeat", "(", "config", ".", "height", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "config", ".", "width", "-", "1", "]", ")", ".", "repeat", "(", "config", ".", "postfix", ")", ",", "\n", "]", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "register_buffer", "(", "\"y_position_ids\"", ",", "torch", ".", "cat", "(", "[", "\n", "torch", ".", "tensor", "(", "[", "0", "]", ")", ".", "repeat", "(", "config", ".", "prefix", ")", ",", "\n", "torch", ".", "arange", "(", "config", ".", "height", ")", ".", "repeat_interleave", "(", "config", ".", "width", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "config", ".", "height", "-", "1", "]", ")", ".", "repeat", "(", "config", ".", "postfix", ")", ",", "\n", "]", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertEmbeddings.forward": [[95, 122], ["performer_2d_prenorm.BertEmbeddings.token_type_embeddings", "performer_2d_prenorm.BertEmbeddings.LayerNorm", "performer_2d_prenorm.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "performer_2d_prenorm.BertEmbeddings.word_embeddings", "performer_2d_prenorm.BertEmbeddings.size", "performer_2d_prenorm.BertEmbeddings.x_position_embeddings", "performer_2d_prenorm.BertEmbeddings.y_position_embeddings"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "x_position_ids", "=", "self", ".", "x_position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "y_position_ids", "=", "self", ".", "y_position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "x_position_ids", ")", "+", "self", ".", "y_position_embeddings", "(", "y_position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfAttention.__init__": [[125, 157], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "performer_attention.PerformerAttention", "ValueError", "performer_2d_prenorm.BertSelfAttention.generate_random_permutation", "performer_2d_prenorm.BertSelfAttention.expand_permutation", "performer_2d_prenorm.BertSelfAttention.register_buffer", "hasattr", "performer_2d_prenorm.BertSelfAttention.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "config", ".", "qk_dim_multiplier", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "config", ".", "qk_dim_multiplier", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "raw_permutation", "=", "self", ".", "generate_random_permutation", "(", "\n", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", "*", "config", ".", "qk_dim_multiplier", "//", "2", ",", "\n", "0xdeadbeefdeadbeef", ",", "\n", ")", "\n", "permutation", "=", "self", ".", "expand_permutation", "(", "raw_permutation", ",", "\n", "config", ".", "prefix", ",", "config", ".", "postfix", ",", "config", ".", "width", ",", "config", ".", "height", ")", "\n", "self", ".", "register_buffer", "(", "\"permutation\"", ",", "permutation", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "self", ".", "performer_attention", "=", "PerformerAttention", "(", "config", ".", "kernel_epsilon", ")", "\n", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfAttention.generate_random_permutation": [[158, 164], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["", "def", "generate_random_permutation", "(", "self", ",", "num_head", ",", "head_size", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "permutation", "=", "[", "torch", ".", "randperm", "(", "head_size", ",", "generator", "=", "rng", ")", "for", "_", "in", "range", "(", "num_head", ")", "]", "\n", "permutation", "=", "torch", ".", "stack", "(", "permutation", ",", "dim", "=", "0", ")", "\n", "return", "permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfAttention.expand_permutation": [[165, 190], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "max", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.arange().repeat_interleave", "torch.arange().repeat_interleave", "torch.arange().repeat_interleave", "torch.arange().repeat_interleave", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.tensor().repeat", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "expand_permutation", "(", "self", ",", "permutation", ",", "prefix", ",", "postfix", ",", "width", ",", "height", ")", ":", "\n", "        ", "num_head", ",", "head_size", "=", "permutation", ".", "shape", "\n", "expanded", "=", "[", "torch", ".", "arange", "(", "head_size", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_head", ",", "head_size", ")", "]", "\n", "for", "_", "in", "range", "(", "max", "(", "width", ",", "height", ")", "-", "1", ")", ":", "\n", "            ", "previous", "=", "expanded", "[", "-", "1", "]", "\n", "current", "=", "previous", ".", "gather", "(", "-", "1", ",", "permutation", ")", "\n", "expanded", ".", "append", "(", "current", ")", "\n", "", "expanded", "=", "torch", ".", "stack", "(", "expanded", ",", "dim", "=", "1", ")", "\n", "\n", "x_position_ids", "=", "torch", ".", "cat", "(", "[", "\n", "torch", ".", "tensor", "(", "[", "0", "]", ")", ".", "repeat", "(", "prefix", ")", ",", "\n", "torch", ".", "arange", "(", "width", ")", ".", "repeat", "(", "height", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "width", "-", "1", "]", ")", ".", "repeat", "(", "postfix", ")", ",", "\n", "]", ")", "\n", "y_position_ids", "=", "torch", ".", "cat", "(", "[", "\n", "torch", ".", "tensor", "(", "[", "0", "]", ")", ".", "repeat", "(", "prefix", ")", ",", "\n", "torch", ".", "arange", "(", "height", ")", ".", "repeat_interleave", "(", "width", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "height", "-", "1", "]", ")", ".", "repeat", "(", "postfix", ")", ",", "\n", "]", ")", "\n", "\n", "x_permutation", "=", "expanded", "[", ":", ",", "x_position_ids", "]", "\n", "y_permutation", "=", "expanded", "[", ":", ",", "y_position_ids", "]", "\n", "y_permutation", "+=", "expanded", ".", "shape", "[", "-", "1", "]", "\n", "xy_permutation", "=", "torch", ".", "cat", "(", "(", "x_permutation", ",", "y_permutation", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "xy_permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfAttention.transpose_for_scores": [[191, 195], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "-", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfAttention.forward": [[196, 260], ["performer_2d_prenorm.BertSelfAttention.LayerNorm", "performer_2d_prenorm.BertSelfAttention.query", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "performer_2d_prenorm.BertSelfAttention.performer_attention", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "query_layer.gather.gather.gather", "performer_2d_prenorm.BertSelfAttention.gather", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "performer_2d_prenorm.BertSelfAttention.permutation[].expand_as", "performer_2d_prenorm.BertSelfAttention.permutation[].expand_as", "context_layer.view.view.permute", "context_layer.view.view.size", "performer_2d_prenorm.BertSelfAttention.key", "performer_2d_prenorm.BertSelfAttention.value", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "performer_2d_prenorm.BertSelfAttention.transpose_for_scores", "performer_2d_prenorm.BertSelfAttention.key", "performer_2d_prenorm.BertSelfAttention.value", "performer_2d_prenorm.BertSelfAttention.key", "performer_2d_prenorm.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "query_layer", "=", "query_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "query_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "query_layer", ")", ")", "\n", "key_layer", "=", "key_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "key_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "key_layer", ")", ")", "\n", "\n", "", "context_layer", "=", "self", ".", "performer_attention", "(", "query_layer", ",", "key_layer", ",", "value_layer", ",", "attention_mask", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "assert", "output_attentions", "is", "False", "\n", "outputs", "=", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfOutput.__init__": [[263, 267], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertSelfOutput.forward": [[268, 273], ["performer_2d_prenorm.BertSelfOutput.dense", "performer_2d_prenorm.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertAttention.__init__": [[276, 281], ["torch.nn.Module.__init__", "performer_2d_prenorm.BertSelfAttention", "performer_2d_prenorm.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertAttention.prune_heads": [[282, 299], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "performer_2d_prenorm.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertAttention.forward": [[300, 322], ["performer_2d_prenorm.BertAttention.self", "performer_2d_prenorm.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertIntermediate.__init__": [[325, 333], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertIntermediate.forward": [[334, 339], ["performer_2d_prenorm.BertIntermediate.LayerNorm", "performer_2d_prenorm.BertIntermediate.dense", "performer_2d_prenorm.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertOutput.__init__": [[342, 346], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertOutput.forward": [[347, 352], ["performer_2d_prenorm.BertOutput.dense", "performer_2d_prenorm.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertLayer.__init__": [[355, 367], ["torch.nn.Module.__init__", "performer_2d_prenorm.BertAttention", "performer_2d_prenorm.BertIntermediate", "performer_2d_prenorm.BertOutput", "performer_2d_prenorm.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertLayer.forward": [[368, 430], ["performer_2d_prenorm.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "performer_2d_prenorm.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertLayer.feed_forward_chunk": [[431, 435], ["performer_2d_prenorm.BertLayer.intermediate", "performer_2d_prenorm.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertEncoder.__init__": [[438, 442], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "performer_2d_prenorm.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertEncoder.forward": [[443, 531], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "performer_2d_prenorm.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertPooler.__init__": [[535, 539], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertPooler.forward": [[540, 547], ["performer_2d_prenorm.BertPooler.dense", "performer_2d_prenorm.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertPreTrainedModel._init_weights": [[559, 574], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.__init__": [[662, 672], ["transformers.modeling_utils.PreTrainedModel.__init__", "performer_2d_prenorm.BertEmbeddings", "performer_2d_prenorm.BertEncoder", "performer_2d_prenorm.BertModel.init_weights", "performer_2d_prenorm.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_input_embeddings": [[673, 675], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.set_input_embeddings": [[676, 678], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel._prune_heads": [[679, 686], ["heads_to_prune.items", "performer_2d_prenorm.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.forward": [[687, 816], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "performer_2d_prenorm.BertModel.get_extended_attention_mask", "performer_2d_prenorm.BertModel.get_head_mask", "performer_2d_prenorm.BertModel.embeddings", "performer_2d_prenorm.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "performer_2d_prenorm.BertModel.invert_attention_mask", "performer_2d_prenorm.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask": [[818, 820], ["attention_mask.to"], "methods", ["None"], ["", "def", "get_extended_attention_mask", "(", "self", ",", "attention_mask", ",", "input_shape", ",", "device", ")", ":", "\n", "        ", "return", "attention_mask", ".", "to", "(", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask": [[821, 823], ["encoder_attention_mask.to"], "methods", ["None"], ["", "def", "invert_attention_mask", "(", "self", ",", "encoder_attention_mask", ")", ":", "\n", "        ", "return", "1.0", "-", "encoder_attention_mask", ".", "to", "(", "self", ".", "dtype", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertEmbeddings.__init__": [[70, 84], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "reformer_prenorm.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertEmbeddings.forward": [[85, 112], ["reformer_prenorm.BertEmbeddings.token_type_embeddings", "reformer_prenorm.BertEmbeddings.LayerNorm", "reformer_prenorm.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "reformer_prenorm.BertEmbeddings.word_embeddings", "reformer_prenorm.BertEmbeddings.position_embeddings", "reformer_prenorm.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertSelfAttention.__init__": [[115, 119], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "transformers.models.reformer.modeling_reformer.LSHSelfAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "self", "=", "LSHSelfAttention", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertSelfAttention.forward": [[120, 132], ["reformer_prenorm.BertSelfAttention.LayerNorm", "reformer_prenorm.BertSelfAttention.self"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "*", "args", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "if", "\"attention_mask\"", "in", "kwargs", ":", "\n", "            ", "del", "kwargs", "[", "\"attention_mask\"", "]", "\n", "", "outputs", "=", "self", ".", "self", "(", "hidden_states", ",", "None", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertSelfOutput.__init__": [[135, 139], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertSelfOutput.forward": [[140, 145], ["reformer_prenorm.BertSelfOutput.dense", "reformer_prenorm.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertAttention.__init__": [[148, 153], ["torch.nn.Module.__init__", "reformer_prenorm.BertSelfAttention", "reformer_prenorm.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertAttention.prune_heads": [[154, 171], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "reformer_prenorm.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertAttention.forward": [[172, 194], ["reformer_prenorm.BertAttention.self", "reformer_prenorm.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertIntermediate.__init__": [[197, 205], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertIntermediate.forward": [[206, 211], ["reformer_prenorm.BertIntermediate.LayerNorm", "reformer_prenorm.BertIntermediate.dense", "reformer_prenorm.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertOutput.__init__": [[214, 218], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertOutput.forward": [[219, 224], ["reformer_prenorm.BertOutput.dense", "reformer_prenorm.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertLayer.__init__": [[227, 239], ["torch.nn.Module.__init__", "reformer_prenorm.BertAttention", "reformer_prenorm.BertIntermediate", "reformer_prenorm.BertOutput", "reformer_prenorm.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertLayer.forward": [[240, 302], ["reformer_prenorm.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "reformer_prenorm.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertLayer.feed_forward_chunk": [[303, 307], ["reformer_prenorm.BertLayer.intermediate", "reformer_prenorm.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertEncoder.__init__": [[310, 314], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "reformer_prenorm.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertEncoder.forward": [[315, 403], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "reformer_prenorm.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertPooler.__init__": [[407, 411], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertPooler.forward": [[412, 419], ["reformer_prenorm.BertPooler.dense", "reformer_prenorm.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertPreTrainedModel._init_weights": [[431, 446], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertModel.__init__": [[534, 544], ["transformers.modeling_utils.PreTrainedModel.__init__", "reformer_prenorm.BertEmbeddings", "reformer_prenorm.BertEncoder", "reformer_prenorm.BertModel.init_weights", "reformer_prenorm.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertModel.get_input_embeddings": [[545, 547], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertModel.set_input_embeddings": [[548, 550], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertModel._prune_heads": [[551, 558], ["heads_to_prune.items", "reformer_prenorm.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.reformer_prenorm.BertModel.forward": [[559, 688], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "reformer_prenorm.BertModel.get_extended_attention_mask", "reformer_prenorm.BertModel.get_head_mask", "reformer_prenorm.BertModel.embeddings", "reformer_prenorm.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "reformer_prenorm.BertModel.invert_attention_mask", "reformer_prenorm.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertEmbeddings.__init__": [[68, 82], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "linformer_prenorm.BertEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertEmbeddings.forward": [[83, 110], ["linformer_prenorm.BertEmbeddings.token_type_embeddings", "linformer_prenorm.BertEmbeddings.LayerNorm", "linformer_prenorm.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "linformer_prenorm.BertEmbeddings.word_embeddings", "linformer_prenorm.BertEmbeddings.position_embeddings", "linformer_prenorm.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.__init__": [[113, 151], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "ValueError", "tensor.uniform_", "linformer_prenorm.BertSelfAttention.__init__.init_"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "def", "init_", "(", "tensor", ")", ":", "\n", "            ", "dim", "=", "tensor", ".", "shape", "[", "-", "1", "]", "\n", "std", "=", "1", "/", "math", ".", "sqrt", "(", "dim", ")", "\n", "tensor", ".", "uniform_", "(", "-", "std", ",", "std", ")", "\n", "return", "tensor", "\n", "\n", "", "self", ".", "proj_k", "=", "nn", ".", "Parameter", "(", "init_", "(", "torch", ".", "zeros", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "linformer_k", ")", ")", ")", "\n", "self", ".", "proj_v", "=", "nn", ".", "Parameter", "(", "init_", "(", "torch", ".", "zeros", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "linformer_k", ")", ")", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "raw_permutation", "=", "self", ".", "generate_random_permutation", "(", "\n", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ",", "\n", "0xdeadbeefdeadbeef", ",", "\n", ")", "\n", "permutation", "=", "self", ".", "expand_permutation", "(", "config", ".", "max_position_embeddings", ",", "raw_permutation", ")", "\n", "self", ".", "register_buffer", "(", "\"permutation\"", ",", "permutation", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.generate_random_permutation": [[152, 158], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["", "def", "generate_random_permutation", "(", "self", ",", "num_head", ",", "head_size", ",", "seed", ")", ":", "\n", "        ", "rng", "=", "torch", ".", "Generator", "(", ")", "\n", "rng", ".", "manual_seed", "(", "seed", ")", "\n", "permutation", "=", "[", "torch", ".", "randperm", "(", "head_size", ",", "generator", "=", "rng", ")", "for", "_", "in", "range", "(", "num_head", ")", "]", "\n", "permutation", "=", "torch", ".", "stack", "(", "permutation", ",", "dim", "=", "0", ")", "\n", "return", "permutation", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.expand_permutation": [[159, 168], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "expand_permutation", "(", "self", ",", "max_seq_length", ",", "permutation", ")", ":", "\n", "        ", "num_head", ",", "head_size", "=", "permutation", ".", "shape", "\n", "expanded", "=", "[", "torch", ".", "arange", "(", "head_size", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_head", ",", "head_size", ")", "]", "\n", "for", "_", "in", "range", "(", "max_seq_length", "-", "1", ")", ":", "\n", "            ", "previous", "=", "expanded", "[", "-", "1", "]", "\n", "current", "=", "previous", ".", "gather", "(", "-", "1", ",", "permutation", ")", "\n", "expanded", ".", "append", "(", "current", ")", "\n", "", "expanded", "=", "torch", ".", "stack", "(", "expanded", ",", "dim", "=", "1", ")", "\n", "return", "expanded", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores": [[169, 173], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.forward": [[174, 259], ["linformer_prenorm.BertSelfAttention.LayerNorm", "linformer_prenorm.BertSelfAttention.query", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "linformer_prenorm.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "query_layer.gather.gather.gather", "linformer_prenorm.BertSelfAttention.gather", "linformer_prenorm.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "linformer_prenorm.BertSelfAttention.permutation[].expand_as", "linformer_prenorm.BertSelfAttention.permutation[].expand_as", "context_layer.view.view.permute", "context_layer.view.view.size", "linformer_prenorm.BertSelfAttention.key", "linformer_prenorm.BertSelfAttention.value", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "linformer_prenorm.BertSelfAttention.transpose_for_scores", "linformer_prenorm.BertSelfAttention.key", "linformer_prenorm.BertSelfAttention.value", "linformer_prenorm.BertSelfAttention.key", "linformer_prenorm.BertSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "", "if", "\"relative_key_query\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "query_layer", "=", "query_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "query_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "query_layer", ")", ")", "\n", "key_layer", "=", "key_layer", ".", "gather", "(", "-", "1", ",", "self", ".", "permutation", "[", ":", ",", ":", ",", ":", "key_layer", ".", "shape", "[", "2", "]", "]", ".", "expand_as", "(", "key_layer", ")", ")", "\n", "\n", "", "key_layer", "=", "torch", ".", "einsum", "(", "'bhnd,nk->bhkd'", ",", "key_layer", ",", "self", ".", "proj_k", ")", "\n", "value_layer", "=", "torch", ".", "einsum", "(", "'bhnd,nk->bhkd'", ",", "value_layer", ",", "self", ".", "proj_v", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# if attention_mask is not None:", "\n", "#     # Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "#     attention_scores = attention_scores + attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfOutput.__init__": [[262, 266], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertSelfOutput.forward": [[267, 272], ["linformer_prenorm.BertSelfOutput.dense", "linformer_prenorm.BertSelfOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.__init__": [[275, 280], ["torch.nn.Module.__init__", "linformer_prenorm.BertSelfAttention", "linformer_prenorm.BertSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads": [[281, 298], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "linformer_prenorm.BertAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.forward": [[299, 321], ["linformer_prenorm.BertAttention.self", "linformer_prenorm.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertIntermediate.__init__": [[324, 332], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertIntermediate.forward": [[333, 338], ["linformer_prenorm.BertIntermediate.LayerNorm", "linformer_prenorm.BertIntermediate.dense", "linformer_prenorm.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertOutput.__init__": [[341, 345], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertOutput.forward": [[346, 351], ["linformer_prenorm.BertOutput.dense", "linformer_prenorm.BertOutput.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "hidden_states", "+", "input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertLayer.__init__": [[354, 366], ["torch.nn.Module.__init__", "linformer_prenorm.BertAttention", "linformer_prenorm.BertIntermediate", "linformer_prenorm.BertOutput", "linformer_prenorm.BertAttention"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertLayer.forward": [[367, 429], ["linformer_prenorm.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "linformer_prenorm.BertLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertLayer.feed_forward_chunk": [[430, 434], ["linformer_prenorm.BertLayer.intermediate", "linformer_prenorm.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertEncoder.__init__": [[437, 441], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "linformer_prenorm.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertEncoder.forward": [[442, 530], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warn", "linformer_prenorm.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertPooler.__init__": [[534, 538], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertPooler.forward": [[539, 546], ["linformer_prenorm.BertPooler.dense", "linformer_prenorm.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertPreTrainedModel._init_weights": [[558, 573], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertModel.__init__": [[661, 671], ["transformers.modeling_utils.PreTrainedModel.__init__", "linformer_prenorm.BertEmbeddings", "linformer_prenorm.BertEncoder", "linformer_prenorm.BertModel.init_weights", "linformer_prenorm.BertPooler"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertModel.get_input_embeddings": [[672, 674], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertModel.set_input_embeddings": [[675, 677], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertModel._prune_heads": [[678, 685], ["heads_to_prune.items", "linformer_prenorm.BertModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.linformer_prenorm.BertModel.forward": [[686, 815], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "linformer_prenorm.BertModel.get_extended_attention_mask", "linformer_prenorm.BertModel.get_head_mask", "linformer_prenorm.BertModel.embeddings", "linformer_prenorm.BertModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "BERT_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "linformer_prenorm.BertModel.invert_attention_mask", "linformer_prenorm.BertModel.pooler", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.models.performer_2d_prenorm.BertModel.invert_attention_mask"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "BERT_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-p.__init__.PerformerMultiheadAttention.__init__": [[14, 17], ["fairseq.modules.multihead_attention.MultiheadAttention.__init__", "__init__.PerformerMultiheadAttention.register_buffer", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__"], ["from", "transformers", "import", "ReformerConfig", "\n", "from", ".", "reformer_prenorm", "import", "BertModel", "as", "ReformerModel", "\n", "from", ".", "bert_prenorm_sin", "import", "BertModel", "as", "BertPrenormSinModel", "\n", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-p.__init__.PerformerMultiheadAttention.generate_random_permutation": [[18, 24], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["\n", "MODEL_MAP", "=", "{", "\n", "\"bert\"", ":", "(", "BertConfig", ",", "BertModel", ")", ",", "\n", "\"bert-prenorm\"", ":", "(", "BertConfig", ",", "BertPrenormModel", ")", ",", "\n", "\"performer-prenorm\"", ":", "(", "PerformerConfig", ",", "PerformerPrenormModel", ")", ",", "\n", "\"performer-2d\"", ":", "(", "Performer2DConfig", ",", "Performer2DModel", ")", ",", "\n", "\"linformer\"", ":", "(", "LinformerConfig", ",", "LinformerPrenormModel", ")", ",", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-p.__init__.PerformerMultiheadAttention.expand_permutation": [[25, 34], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["\"nystromformer\"", ":", "(", "NystromformerConfig", ",", "NystromformerPrenormModel", ")", ",", "\n", "\"reformer\"", ":", "(", "ReformerConfig", ",", "ReformerModel", ")", ",", "\n", "\"bert-prenorm-sin\"", ":", "(", "BertConfig", ",", "BertPrenormSinModel", ")", ",", "\n", "}", "\n", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-p.__init__.PerformerMultiheadAttention.forward": [[35, 90], ["query.size", "__init__.PerformerMultiheadAttention.q_proj", "__init__.PerformerMultiheadAttention.k_proj", "__init__.PerformerMultiheadAttention.v_proj", "q.view().permute.view().permute.view().permute", "k.view().permute.view().permute.view().permute", "v.view().permute.view().permute.view().permute", "fast_transformers.causal_product.causal_dot_product", "__init__.PerformerMultiheadAttention.permute().reshape", "__init__.PerformerMultiheadAttention.out_proj", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "q.view().permute.view().permute.view", "k.view().permute.view().permute.view", "v.view().permute.view().permute.view", "__init__.PerformerMultiheadAttention.permute", "__init__.PerformerMultiheadAttention.ratio.unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-p.__init__.PerformerDecoderLayer.build_self_attention": [[93, 105], ["__init__.PerformerMultiheadAttention", "getattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute.__init__.PerformerMultiheadAttention.__init__": [[14, 20], ["fairseq.modules.multihead_attention.MultiheadAttention.__init__", "__init__.PerformerMultiheadAttention.generate_random_permutation", "__init__.PerformerMultiheadAttention.expand_permutation", "__init__.PerformerMultiheadAttention.register_buffer", "__init__.PerformerMultiheadAttention.register_buffer", "__init__.PerformerMultiheadAttention.unsqueeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["from", "transformers", "import", "ReformerConfig", "\n", "from", ".", "reformer_prenorm", "import", "BertModel", "as", "ReformerModel", "\n", "from", ".", "bert_prenorm_sin", "import", "BertModel", "as", "BertPrenormSinModel", "\n", "\n", "\n", "MODEL_MAP", "=", "{", "\n", "\"bert\"", ":", "(", "BertConfig", ",", "BertModel", ")", ",", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute.__init__.PerformerMultiheadAttention.generate_random_permutation": [[21, 27], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["\"bert-prenorm\"", ":", "(", "BertConfig", ",", "BertPrenormModel", ")", ",", "\n", "\"performer-prenorm\"", ":", "(", "PerformerConfig", ",", "PerformerPrenormModel", ")", ",", "\n", "\"performer-2d\"", ":", "(", "Performer2DConfig", ",", "Performer2DModel", ")", ",", "\n", "\"linformer\"", ":", "(", "LinformerConfig", ",", "LinformerPrenormModel", ")", ",", "\n", "\"nystromformer\"", ":", "(", "NystromformerConfig", ",", "NystromformerPrenormModel", ")", ",", "\n", "\"reformer\"", ":", "(", "ReformerConfig", ",", "ReformerModel", ")", ",", "\n", "\"bert-prenorm-sin\"", ":", "(", "BertConfig", ",", "BertPrenormSinModel", ")", ",", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute.__init__.PerformerMultiheadAttention.expand_permutation": [[28, 37], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["}", "\n", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute.__init__.PerformerMultiheadAttention.forward": [[38, 96], ["query.size", "__init__.PerformerMultiheadAttention.q_proj", "__init__.PerformerMultiheadAttention.k_proj", "__init__.PerformerMultiheadAttention.v_proj", "q.gather.gather.view().permute", "k.gather.gather.view().permute", "v.view().permute.view().permute.view().permute", "q.gather.gather.gather", "k.gather.gather.gather", "fast_transformers.causal_product.causal_dot_product", "__init__.PerformerMultiheadAttention.permute().reshape", "__init__.PerformerMultiheadAttention.out_proj", "__init__.PerformerMultiheadAttention.permutation[].expand_as", "__init__.PerformerMultiheadAttention.permutation[].expand_as", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "q.gather.gather.view", "k.gather.gather.view", "v.view().permute.view().permute.view", "__init__.PerformerMultiheadAttention.permute", "__init__.PerformerMultiheadAttention.ratio.unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute.__init__.PerformerDecoderLayer.build_self_attention": [[99, 111], ["__init__.PerformerMultiheadAttention", "getattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__": [[14, 19], ["fairseq.modules.multihead_attention.MultiheadAttention.__init__", "__init__.PerformerMultiheadAttention.generate_random_permutation", "__init__.PerformerMultiheadAttention.expand_permutation", "__init__.PerformerMultiheadAttention.register_buffer", "__init__.PerformerMultiheadAttention.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.__init__", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation", "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation"], ["from", "transformers", "import", "ReformerConfig", "\n", "from", ".", "reformer_prenorm", "import", "BertModel", "as", "ReformerModel", "\n", "from", ".", "bert_prenorm_sin", "import", "BertModel", "as", "BertPrenormSinModel", "\n", "\n", "\n", "MODEL_MAP", "=", "{", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.generate_random_permutation": [[20, 26], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "range"], "methods", ["None"], ["\"bert\"", ":", "(", "BertConfig", ",", "BertModel", ")", ",", "\n", "\"bert-prenorm\"", ":", "(", "BertConfig", ",", "BertPrenormModel", ")", ",", "\n", "\"performer-prenorm\"", ":", "(", "PerformerConfig", ",", "PerformerPrenormModel", ")", ",", "\n", "\"performer-2d\"", ":", "(", "Performer2DConfig", ",", "Performer2DModel", ")", ",", "\n", "\"linformer\"", ":", "(", "LinformerConfig", ",", "LinformerPrenormModel", ")", ",", "\n", "\"nystromformer\"", ":", "(", "NystromformerConfig", ",", "NystromformerPrenormModel", ")", ",", "\n", "\"reformer\"", ":", "(", "ReformerConfig", ",", "ReformerModel", ")", ",", "\n"]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.expand_permutation": [[27, 36], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "previous.gather", "torch.stack.append", "torch.stack.append", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["\"bert-prenorm-sin\"", ":", "(", "BertConfig", ",", "BertPrenormSinModel", ")", ",", "\n", "}", "\n", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerMultiheadAttention.forward": [[37, 92], ["query.size", "__init__.PerformerMultiheadAttention.q_proj", "__init__.PerformerMultiheadAttention.k_proj", "__init__.PerformerMultiheadAttention.v_proj", "q.gather.gather.view().permute", "k.gather.gather.view().permute", "v.view().permute.view().permute.view().permute", "q.gather.gather.gather", "k.gather.gather.gather", "fast_transformers.causal_product.causal_dot_product", "__init__.PerformerMultiheadAttention.permute().reshape", "__init__.PerformerMultiheadAttention.out_proj", "__init__.PerformerMultiheadAttention.permutation[].expand_as", "__init__.PerformerMultiheadAttention.permutation[].expand_as", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "q.gather.gather.view", "k.gather.gather.view", "v.view().permute.view().permute.view", "__init__.PerformerMultiheadAttention.permute"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.permute-r.__init__.PerformerDecoderLayer.build_self_attention": [[95, 107], ["__init__.PerformerMultiheadAttention", "getattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.performer.__init__.PerformerMultiheadAttention.forward": [[14, 65], ["query.size", "__init__.PerformerMultiheadAttention.q_proj", "__init__.PerformerMultiheadAttention.k_proj", "__init__.PerformerMultiheadAttention.v_proj", "q.view().permute.view().permute.view().permute", "k.view().permute.view().permute.view().permute", "v.view().permute.view().permute.view().permute", "fast_transformers.causal_product.causal_dot_product", "__init__.PerformerMultiheadAttention.permute().reshape", "__init__.PerformerMultiheadAttention.out_proj", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "q.view().permute.view().permute.view", "k.view().permute.view().permute.view", "v.view().permute.view().permute.view", "__init__.PerformerMultiheadAttention.permute"], "methods", ["None"], ["from", "transformers", "import", "ReformerConfig", "\n", "from", ".", "reformer_prenorm", "import", "BertModel", "as", "ReformerModel", "\n", "from", ".", "bert_prenorm_sin", "import", "BertModel", "as", "BertPrenormSinModel", "\n", "\n", "\n", "MODEL_MAP", "=", "{", "\n", "\"bert\"", ":", "(", "BertConfig", ",", "BertModel", ")", ",", "\n", "\"bert-prenorm\"", ":", "(", "BertConfig", ",", "BertPrenormModel", ")", ",", "\n", "\"performer-prenorm\"", ":", "(", "PerformerConfig", ",", "PerformerPrenormModel", ")", ",", "\n", "\"performer-2d\"", ":", "(", "Performer2DConfig", ",", "Performer2DModel", ")", ",", "\n", "\"linformer\"", ":", "(", "LinformerConfig", ",", "LinformerPrenormModel", ")", ",", "\n", "\"nystromformer\"", ":", "(", "NystromformerConfig", ",", "NystromformerPrenormModel", ")", ",", "\n", "\"reformer\"", ":", "(", "ReformerConfig", ",", "ReformerModel", ")", ",", "\n", "\"bert-prenorm-sin\"", ":", "(", "BertConfig", ",", "BertPrenormSinModel", ")", ",", "\n", "}", "\n", ""]], "home.repos.pwc.inspect_result.cpcp1998_permuteformer.performer.__init__.PerformerDecoderLayer.build_self_attention": [[68, 80], ["__init__.PerformerMultiheadAttention", "getattr"], "methods", ["None"], []]}