{"home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.feature_extraction_launch": [[27, 94], ["torch.no_grad", "model.eval", "logger.info", "enumerate", "meter.reset", "meter.iter_tic", "len", "isinstance", "labels.cuda.cuda", "video_idx.cuda.cuda", "meta.items", "model", "meta[].cpu", "meta[].cpu", "slowfast.is_master_proc", "slowfast.synchronize", "NotImplementedError", "range", "inputs.cuda.cuda", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat.detach().cpu", "torch.cat.detach", "torch.cat.detach().cpu", "torch.cat.detach", "torch.cat.detach().cpu", "torch.cat.detach", "slowfast.get_world_size", "range", "meter.iter_toc", "meter.log_iter_stats", "len", "inputs[].cuda", "range", "val.cuda", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "[].append", "len", "val[].cuda", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach", "int", "int", "dict", "feats[].squeeze", "metadata[].item", "metadata[].item"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.synchronize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_iter_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned"], ["@", "torch", ".", "no_grad", "(", ")", "\n", "def", "feature_extraction_launch", "(", "loader", ",", "model", ",", "meter", ",", "cfg", ",", "feature_bank", ")", ":", "\n", "# Enable eval mode.", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "logger", ".", "info", "(", "'extract feature for {} iters'", ".", "format", "(", "len", "(", "loader", ")", ")", ")", "\n", "for", "cur_iter", ",", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "_", ",", "_", ",", "_", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "\n", "        ", "meter", ".", "iter_tic", "(", ")", "\n", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "            ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                    ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# Transfer the data to the current GPU device.", "\n", "", "labels", "=", "labels", ".", "cuda", "(", ")", "\n", "video_idx", "=", "video_idx", ".", "cuda", "(", ")", "\n", "for", "key", ",", "val", "in", "meta", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "val", ",", "(", "list", ",", ")", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "val", ")", ")", ":", "\n", "                        ", "val", "[", "i", "]", "=", "val", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                    ", "meta", "[", "key", "]", "=", "val", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "", "", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "# Compute the predictions.", "\n", "            ", "feat", ",", "ctx", "=", "model", "(", "inputs", ",", "meta", "[", "\"boxes\"", "]", ",", "meta", "[", "\"metadata\"", "]", ",", "extract", "=", "True", ")", "\n", "\n", "ori_boxes", "=", "meta", "[", "\"ori_boxes\"", "]", ".", "cpu", "(", ")", "\n", "metadata", "=", "meta", "[", "\"metadata\"", "]", ".", "cpu", "(", ")", "\n", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                ", "ori_boxes", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "ori_boxes", ")", ",", "dim", "=", "0", ")", "\n", "metadata", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "metadata", ")", ",", "dim", "=", "0", ")", "\n", "feats", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "feat", ")", ",", "dim", "=", "0", ")", "\n", "\n", "", "feats", "=", "feats", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "cfg", ".", "NUM_GPUS", "else", "feats", ".", "detach", "(", ")", "\n", "ori_boxes", "=", "(", "\n", "ori_boxes", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "cfg", ".", "NUM_GPUS", "else", "ori_boxes", ".", "detach", "(", ")", "\n", ")", "\n", "metadata", "=", "(", "\n", "metadata", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "cfg", ".", "NUM_GPUS", "else", "metadata", ".", "detach", "(", ")", "\n", ")", "\n", "\n", "if", "du", ".", "is_master_proc", "(", "du", ".", "get_world_size", "(", ")", ")", ":", "\n", "                ", "num", "=", "metadata", ".", "shape", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "num", ")", ":", "\n", "                    ", "vid", ",", "sec", "=", "int", "(", "metadata", "[", "i", ",", "0", "]", ".", "item", "(", ")", ")", ",", "int", "(", "metadata", "[", "i", ",", "1", "]", ".", "item", "(", ")", ")", "\n", "\n", "if", "vid", "not", "in", "feature_bank", ":", "\n", "                        ", "feature_bank", "[", "vid", "]", "=", "dict", "(", ")", "\n", "\n", "", "if", "sec", "not", "in", "feature_bank", "[", "vid", "]", ":", "\n", "                        ", "feature_bank", "[", "vid", "]", "[", "sec", "]", "=", "[", "]", "\n", "\n", "", "feature_bank", "[", "vid", "]", "[", "sec", "]", ".", "append", "(", "feats", "[", "i", "]", ".", "squeeze", "(", ")", ")", "\n", "\n", "", "meter", ".", "iter_toc", "(", ")", "\n", "meter", ".", "log_iter_stats", "(", "cur_epoch", "=", "0", ",", "cur_iter", "=", "cur_iter", ")", "\n", "", "du", ".", "synchronize", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "", "", "meter", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.write": [[95, 118], ["lmdb.open", "lmdb.open.begin", "env.begin.commit", "lmdb.open.close", "os.path.join", "os.path.join", "pickle.dumps", "env.begin.put", "logger.info", "env.begin.commit", "lmdb.open.begin", "feat_key.encode"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close"], ["", "def", "write", "(", "feature_bank", ",", "cfg", ")", ":", "\n", "\n", "# save as lmdb", "\n", "\n", "    ", "env", "=", "lmdb", ".", "open", "(", "os", ".", "path", ".", "join", "(", "cfg", ".", "AVA", ".", "FEATURE_BANK_PATH", ",", "'rdb'", ")", ",", "map_size", "=", "3e10", ")", "\n", "txn", "=", "env", ".", "begin", "(", "write", "=", "True", ")", "\n", "count", "=", "0", "\n", "\n", "for", "split", "in", "feature_bank", ":", "\n", "        ", "for", "vid", "in", "feature_bank", "[", "split", "]", ":", "\n", "            ", "for", "sec", "in", "feature_bank", "[", "split", "]", "[", "vid", "]", ":", "\n", "                ", "feat_key", "=", "f\"{split}/{vid}/{sec}/feature\"", "\n", "feat_val", "=", "pickle", ".", "dumps", "(", "feature_bank", "[", "split", "]", "[", "vid", "]", "[", "sec", "]", ")", "\n", "txn", ".", "put", "(", "key", "=", "feat_key", ".", "encode", "(", ")", ",", "value", "=", "feat_val", ")", "\n", "\n", "count", "+=", "1", "\n", "if", "count", "%", "2000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"commit for {count} frames\"", ")", "\n", "txn", ".", "commit", "(", ")", "\n", "txn", "=", "env", ".", "begin", "(", "write", "=", "True", ")", "\n", "\n", "", "", "", "", "txn", ".", "commit", "(", ")", "\n", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.extract_feature": [[119, 169], ["slowfast.init_distributed_training", "numpy.random.seed", "torch.manual_seed", "slowfast.setup_logging", "logger.info", "logger.info", "slowfast.models.build_model", "slowfast.load_test_checkpoint", "slowfast.datasets.loader.construct_loader", "slowfast.datasets.loader.construct_loader", "slowfast.utils.meters.AVAMeter", "slowfast.utils.meters.AVAMeter", "extract_feature.feature_extraction_launch", "extract_feature.feature_extraction_launch", "slowfast.is_master_proc", "slowfast.is_master_proc", "slowfast.log_model_info", "dict", "dict", "slowfast.is_master_proc", "os.makedirs", "os.makedirs", "len", "len", "slowfast.get_world_size", "extract_feature.write", "os.exists"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_distributed_training", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.feature_extraction_launch", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.feature_extraction_launch", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.write"], ["", "def", "extract_feature", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Perform multi-view testing on the pretrained video model.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "# Set up environment.", "\n", "du", ".", "init_distributed_training", "(", "cfg", ")", "\n", "# Set random seed from configs.", "\n", "np", ".", "random", ".", "seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "torch", ".", "manual_seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "\n", "# Setup logging format.", "\n", "logging", ".", "setup_logging", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "\n", "# Print config.", "\n", "logger", ".", "info", "(", "\"Extracting AVA features with config:\"", ")", "\n", "logger", ".", "info", "(", "cfg", ")", "\n", "\n", "# Build the video model and print model statistics.", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "cfg", ".", "LOG_MODEL_INFO", ":", "\n", "        ", "misc", ".", "log_model_info", "(", "model", ",", "cfg", ",", "use_train_input", "=", "False", ")", "\n", "\n", "", "cu", ".", "load_test_checkpoint", "(", "cfg", ",", "model", ")", "\n", "\n", "# Create video testing loaders.", "\n", "train_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"train\"", ")", "\n", "test_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"test\"", ")", "\n", "\n", "# Create video feature bank", "\n", "# format {video_idx: {sec: {name: [feature1, feature2, ....]}}} name in ['feature', 'context']", "\n", "feature_bank", "=", "{", "\n", "'train'", ":", "dict", "(", ")", ",", "\n", "'test'", ":", "dict", "(", ")", "\n", "}", "\n", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "not", "osp", ".", "exists", "(", "cfg", ".", "AVA", ".", "FEATURE_BANK_PATH", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cfg", ".", "AVA", ".", "FEATURE_BANK_PATH", ")", "\n", "\n", "", "test_meter", "=", "AVAMeter", "(", "len", "(", "test_loader", ")", ",", "cfg", ",", "mode", "=", "\"test\"", ")", "\n", "train_meter", "=", "AVAMeter", "(", "len", "(", "train_loader", ")", ",", "cfg", ",", "mode", "=", "\"test\"", ")", "\n", "\n", "# main process for feature extraction", "\n", "feature_extraction_launch", "(", "train_loader", ",", "model", ",", "train_meter", ",", "cfg", ",", "feature_bank", "[", "'train'", "]", ")", "\n", "feature_extraction_launch", "(", "test_loader", ",", "model", ",", "test_meter", ",", "cfg", ",", "feature_bank", "[", "'test'", "]", ")", "\n", "\n", "if", "du", ".", "is_master_proc", "(", "du", ".", "get_world_size", "(", ")", ")", ":", "\n", "        ", "write", "(", "feature_bank", ",", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.demo_net.run_demo": [[23, 81], ["numpy.random.seed", "torch.manual_seed", "slowfast.utils.logging.setup_logging", "logger.info", "logger.info", "logger.info", "logger.info", "slowfast.visualization.video_visualizer.VideoVisualizer", "slowfast.visualization.predictor.ActionPredictor", "slowfast.visualization.utils.init_task_info", "slowfast.visualization.predictor.Detectron2Predictor", "slowfast.visualization.predictor.ActionPredictor.", "slowfast.visualization.predictor.draw_predictions", "cv2.waitKey", "slowfast.visualization.predictor.Detectron2Predictor."], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.init_task_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.predictor.draw_predictions"], ["def", "run_demo", "(", "cfg", ",", "frame_provider", ")", ":", "\n", "    ", "\"\"\"\n    Run demo visualization.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        frame_provider (iterator): Python iterator that return task objects that are filled\n            with necessary information such as `frames`, `id` and `num_buffer_frames` for the\n            prediction and visualization pipeline.\n    \"\"\"", "\n", "# Set random seed from configs.", "\n", "np", ".", "random", ".", "seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "torch", ".", "manual_seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "# Setup logging format.", "\n", "logging", ".", "setup_logging", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "# Print config.", "\n", "logger", ".", "info", "(", "\"Run demo with config:\"", ")", "\n", "logger", ".", "info", "(", "cfg", ")", "\n", "assert", "cfg", ".", "NUM_GPUS", "<=", "1", ",", "\"Cannot run demo on multiple GPUs.\"", "\n", "# Print config.", "\n", "logger", ".", "info", "(", "\"Run demo with config:\"", ")", "\n", "logger", ".", "info", "(", "cfg", ")", "\n", "video_vis", "=", "VideoVisualizer", "(", "\n", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "cfg", ".", "DEMO", ".", "LABEL_FILE_PATH", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "TOPK_PREDS", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "COLORMAP", ",", "\n", ")", "\n", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "        ", "object_detector", "=", "Detectron2Predictor", "(", "cfg", ")", "\n", "\n", "", "model", "=", "ActionPredictor", "(", "cfg", ")", "\n", "\n", "seq_len", "=", "cfg", ".", "DATA", ".", "NUM_FRAMES", "*", "cfg", ".", "DATA", ".", "SAMPLING_RATE", "\n", "assert", "(", "\n", "cfg", ".", "DEMO", ".", "BUFFER_SIZE", "<=", "seq_len", "//", "2", "\n", ")", ",", "\"Buffer size cannot be greater than half of sequence length.\"", "\n", "init_task_info", "(", "\n", "frame_provider", ".", "display_height", ",", "\n", "frame_provider", ".", "display_width", ",", "\n", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "\n", "cfg", ".", "DEMO", ".", "CLIP_VIS_SIZE", ",", "\n", ")", "\n", "for", "able_to_read", ",", "task", "in", "frame_provider", ":", "\n", "        ", "if", "not", "able_to_read", ":", "\n", "            ", "break", "\n", "\n", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "            ", "task", "=", "object_detector", "(", "task", ")", "\n", "\n", "", "task", "=", "model", "(", "task", ")", "\n", "frames", "=", "draw_predictions", "(", "task", ",", "video_vis", ")", "\n", "# hit Esc to quit the demo.", "\n", "key", "=", "cv2", ".", "waitKey", "(", "1", ")", "\n", "if", "key", "==", "27", ":", "\n", "            ", "break", "\n", "", "yield", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.demo_net.demo": [[83, 102], ["slowfast.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox", "slowfast.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.", "slowfast.visualization.demo_loader.VideoReader", "tqdm.tqdm", "slowfast.visualization.demo_loader.VideoReader.clean", "demo_net.run_demo", "slowfast.visualization.demo_loader.VideoReader.display"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.clean", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.demo_net.run_demo", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display"], ["", "", "def", "demo", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Run inference on an input video or stream from webcam.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "# AVA format-specific visualization with precomputed boxes.", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", "and", "cfg", ".", "DEMO", ".", "PREDS_BOXES", "!=", "\"\"", ":", "\n", "        ", "precomputed_box_vis", "=", "AVAVisualizerWithPrecomputedBox", "(", "cfg", ")", "\n", "precomputed_box_vis", "(", ")", "\n", "", "else", ":", "\n", "        ", "frame_provider", "=", "VideoReader", "(", "cfg", ")", "\n", "\n", "for", "frames", "in", "tqdm", ".", "tqdm", "(", "run_demo", "(", "cfg", ",", "frame_provider", ")", ")", ":", "\n", "            ", "for", "frame", "in", "frames", ":", "\n", "                ", "frame_provider", ".", "display", "(", "frame", ")", "\n", "\n", "", "", "frame_provider", ".", "clean", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.test_net.prepare_data": [[25, 47], ["isinstance", "labels.cuda.cuda", "video_idx.cuda.cuda", "meta.items", "range", "range", "inputs.cuda.cuda", "isinstance", "len", "len", "inputs[].cuda", "range", "val.cuda", "FBs[].cuda", "len", "val[].cuda"], "function", ["None"], ["def", "prepare_data", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "    ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# Transfer the data to the current GPU device.", "\n", "", "labels", "=", "labels", ".", "cuda", "(", ")", "\n", "video_idx", "=", "video_idx", ".", "cuda", "(", ")", "\n", "for", "key", ",", "val", "in", "meta", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "val", ",", "(", "list", ",", ")", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "val", ")", ")", ":", "\n", "                ", "val", "[", "i", "]", "=", "val", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "            ", "meta", "[", "key", "]", "=", "val", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "FBs", ")", ")", ":", "\n", "        ", "FBs", "[", "i", "]", "=", "FBs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "if", "FBs", "[", "i", "]", "is", "not", "None", "else", "None", "\n", "\n", "", "return", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.test_net.perform_test": [[49, 149], ["torch.no_grad", "torch.no_grad", "model.eval", "test_meter.iter_tic", "logger.info", "enumerate", "slowfast.is_master_proc", "test_meter.reset", "test_meter.iter_tic", "writer.plot_eval", "slowfast.get_local_size", "test_meter.finalize_metrics", "len", "test_net.prepare_data", "meta[].cpu", "meta[].cpu", "test_meter.iter_toc", "test_meter.update_stats", "test_meter.log_iter_stats", "model", "test_meter.iter_toc", "test_meter.update_stats", "test_meter.log_iter_stats", "pred.clone().detach", "label.clone().detach", "model", "model", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "preds.cpu.detach().cpu", "preds.cpu.detach", "torch.cat.detach().cpu", "torch.cat.detach", "torch.cat.detach().cpu", "torch.cat.detach", "slowfast.all_gather", "preds.cpu.cpu", "labels.cpu.cpu", "video_idx.cpu.cpu", "preds.cpu.detach", "labels.cpu.detach", "video_idx.cpu.detach", "pred.cpu", "label.cpu", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "pred.clone", "label.clone", "preds.cpu.detach", "torch.cat.detach", "torch.cat.detach"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.plot_eval", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.finalize_metrics", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.prepare_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_iter_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_iter_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "perform_test", "(", "test_loader", ",", "model", ",", "test_meter", ",", "cfg", ",", "writer", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    For classification:\n    Perform mutli-view testing that uniformly samples N clips from a video along\n    its temporal axis. For each clip, it takes 3 crops to cover the spatial\n    dimension, followed by averaging the softmax scores across all Nx3 views to\n    form a video-level prediction. All video predictions are compared to\n    ground-truth labels and the final testing performance is logged.\n    For detection:\n    Perform fully-convolutional testing on the full frames without crop.\n    Args:\n        test_loader (loader): video testing loader.\n        model (model): the pretrained video model to test.\n        test_meter (TestMeter): testing meters to log and ensemble the testing\n            results.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        writer (TensorboardWriter object, optional): TensorboardWriter object\n            to writer Tensorboard log.\n    \"\"\"", "\n", "# Enable eval mode.", "\n", "model", ".", "eval", "(", ")", "\n", "test_meter", ".", "iter_tic", "(", ")", "\n", "\n", "logger", ".", "info", "(", "'online test for {} iters'", ".", "format", "(", "len", "(", "test_loader", ")", ")", ")", "\n", "for", "cur_iter", ",", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", "in", "enumerate", "(", "test_loader", ")", ":", "\n", "        ", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "\n", "            ", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", "=", "prepare_data", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", "\n", "\n", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "# Compute the predictions.", "\n", "\n", "            ", "if", "cfg", ".", "LSTC", ".", "ENABLE", ":", "\n", "                ", "preds", ",", "sp", ",", "lp", "=", "model", "(", "inputs", ",", "bboxes", "=", "meta", "[", "\"boxes\"", "]", ",", "extract", "=", "False", ",", "FBs", "=", "FBs", ",", "BTs", "=", "BTs", ")", "\n", "", "else", ":", "\n", "                ", "preds", "=", "model", "(", "inputs", ",", "bboxes", "=", "meta", "[", "\"boxes\"", "]", ",", "extract", "=", "False", ",", "FBs", "=", "FBs", ",", "BTs", "=", "BTs", ")", "\n", "\n", "", "ori_boxes", "=", "meta", "[", "\"ori_boxes\"", "]", ".", "cpu", "(", ")", "\n", "metadata", "=", "meta", "[", "\"metadata\"", "]", ".", "cpu", "(", ")", "\n", "\n", "# imgs = inputs[-1][:, :, 16].cpu()", "\n", "# slow = inputs[0].cpu()", "\n", "# attn = attn.cpu()", "\n", "# visualize_tensor(slow, imgs, meta[\"boxes\"].cpu(), metadata, attn, cfg)", "\n", "# visualize_results(imgs, meta[\"boxes\"].cpu(), metadata, sp, lp, cfg, labelmap)", "\n", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                ", "ori_boxes", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "ori_boxes", ")", ",", "dim", "=", "0", ")", "\n", "metadata", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "metadata", ")", ",", "dim", "=", "0", ")", "\n", "preds", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "preds", ")", ",", "dim", "=", "0", ")", "\n", "\n", "", "preds", "=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "cfg", ".", "NUM_GPUS", "else", "preds", ".", "detach", "(", ")", "\n", "ori_boxes", "=", "(", "\n", "ori_boxes", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "cfg", ".", "NUM_GPUS", "else", "ori_boxes", ".", "detach", "(", ")", "\n", ")", "\n", "metadata", "=", "(", "\n", "metadata", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "cfg", ".", "NUM_GPUS", "else", "metadata", ".", "detach", "(", ")", "\n", ")", "\n", "\n", "test_meter", ".", "iter_toc", "(", ")", "\n", "# Update and log stats.", "\n", "test_meter", ".", "update_stats", "(", "preds", ",", "ori_boxes", ",", "metadata", ")", "\n", "test_meter", ".", "log_iter_stats", "(", "cur_epoch", "=", "0", ",", "cur_iter", "=", "cur_iter", ")", "\n", "", "else", ":", "\n", "# Perform the forward pass.", "\n", "            ", "preds", "=", "model", "(", "inputs", ")", "\n", "\n", "# Gather all the predictions across all the devices to perform ensemble.", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                ", "preds", ",", "labels", ",", "video_idx", "=", "du", ".", "all_gather", "(", "\n", "[", "preds", ",", "labels", ",", "video_idx", "]", "\n", ")", "\n", "", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "                ", "preds", "=", "preds", ".", "cpu", "(", ")", "\n", "labels", "=", "labels", ".", "cpu", "(", ")", "\n", "video_idx", "=", "video_idx", ".", "cpu", "(", ")", "\n", "", "test_meter", ".", "iter_toc", "(", ")", "\n", "# Update and log stats.", "\n", "test_meter", ".", "update_stats", "(", "\n", "preds", ".", "detach", "(", ")", ",", "labels", ".", "detach", "(", ")", ",", "video_idx", ".", "detach", "(", ")", "\n", ")", "\n", "test_meter", ".", "log_iter_stats", "(", "cur_iter", ")", "\n", "", "test_meter", ".", "iter_tic", "(", ")", "\n", "# Log epoch stats and print the final testing results.", "\n", "", "if", "writer", "is", "not", "None", "and", "not", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "        ", "all_preds", "=", "[", "pred", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "pred", "in", "test_meter", ".", "video_preds", "]", "\n", "all_labels", "=", "[", "\n", "label", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "label", "in", "test_meter", ".", "video_labels", "\n", "]", "\n", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "all_preds", "=", "[", "pred", ".", "cpu", "(", ")", "for", "pred", "in", "all_preds", "]", "\n", "all_labels", "=", "[", "label", ".", "cpu", "(", ")", "for", "label", "in", "all_labels", "]", "\n", "", "writer", ".", "plot_eval", "(", "preds", "=", "all_preds", ",", "labels", "=", "all_labels", ")", "\n", "\n", "", "if", "du", ".", "is_master_proc", "(", "du", ".", "get_local_size", "(", ")", ")", ":", "\n", "        ", "test_meter", ".", "finalize_metrics", "(", ")", "\n", "", "test_meter", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.test_net.test": [[151, 222], ["slowfast.init_distributed_training", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "slowfast.setup_logging", "logger.info", "logger.info", "slowfast.models.build_model", "slowfast.load_test_checkpoint", "slowfast.datasets.loader.construct_loader", "slowfast.synchronize", "logger.info", "time.time", "test_net.perform_test", "time.time", "logger.info", "loader.construct_loader.dataset.close", "slowfast.is_master_proc", "slowfast.log_model_info", "slowfast.utils.meters.TestMeter", "slowfast.is_master_proc", "slowfast.TensorboardWriter", "tb.TensorboardWriter.close", "len", "slowfast.utils.meters.AVAMeter", "slowfast.utils.meters.HieveMeter", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_distributed_training", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.synchronize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.test_net.perform_test", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close"], ["", "def", "test", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Perform multi-view testing on the pretrained video model.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "# Set up environment.", "\n", "du", ".", "init_distributed_training", "(", "cfg", ")", "\n", "# Set random seed from configs.", "\n", "np", ".", "random", ".", "seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "torch", ".", "manual_seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "\n", "# Setup logging format.", "\n", "logging", ".", "setup_logging", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "\n", "# Print config.", "\n", "logger", ".", "info", "(", "\"Test with config:\"", ")", "\n", "logger", ".", "info", "(", "cfg", ")", "\n", "\n", "# Build the video model and print model statistics.", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "cfg", ".", "LOG_MODEL_INFO", ":", "\n", "        ", "misc", ".", "log_model_info", "(", "model", ",", "cfg", ",", "use_train_input", "=", "False", ")", "\n", "\n", "", "cu", ".", "load_test_checkpoint", "(", "cfg", ",", "model", ")", "\n", "\n", "# Create video testing loaders.", "\n", "test_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"test\"", ")", "\n", "du", ".", "synchronize", "(", ")", "\n", "logger", ".", "info", "(", "\"Testing model for {} iterations\"", ".", "format", "(", "len", "(", "test_loader", ")", ")", ")", "\n", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "        ", "assert", "cfg", ".", "NUM_GPUS", "==", "cfg", ".", "TEST", ".", "BATCH_SIZE", "or", "cfg", ".", "NUM_GPUS", "==", "0", "\n", "if", "cfg", ".", "TEST", ".", "DATASET", "==", "'ava'", ":", "\n", "            ", "test_meter", "=", "AVAMeter", "(", "len", "(", "test_loader", ")", ",", "cfg", ",", "mode", "=", "\"test\"", ")", "\n", "", "else", ":", "\n", "            ", "test_meter", "=", "HieveMeter", "(", "len", "(", "test_loader", ")", ",", "cfg", ",", "mode", "=", "\"test\"", ")", "\n", "", "", "else", ":", "\n", "        ", "assert", "(", "\n", "len", "(", "test_loader", ".", "dataset", ")", "\n", "%", "(", "cfg", ".", "TEST", ".", "NUM_ENSEMBLE_VIEWS", "*", "cfg", ".", "TEST", ".", "NUM_SPATIAL_CROPS", ")", "\n", "==", "0", "\n", ")", "\n", "# Create meters for multi-view testing.", "\n", "test_meter", "=", "TestMeter", "(", "\n", "len", "(", "test_loader", ".", "dataset", ")", "\n", "//", "(", "cfg", ".", "TEST", ".", "NUM_ENSEMBLE_VIEWS", "*", "cfg", ".", "TEST", ".", "NUM_SPATIAL_CROPS", ")", ",", "\n", "cfg", ".", "TEST", ".", "NUM_ENSEMBLE_VIEWS", "*", "cfg", ".", "TEST", ".", "NUM_SPATIAL_CROPS", ",", "\n", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "len", "(", "test_loader", ")", ",", "\n", "cfg", ".", "DATA", ".", "MULTI_LABEL", ",", "\n", "cfg", ".", "DATA", ".", "ENSEMBLE_METHOD", ",", "\n", ")", "\n", "\n", "# Set up writer for logging to Tensorboard format.", "\n", "", "if", "cfg", ".", "TENSORBOARD", ".", "ENABLE", "and", "du", ".", "is_master_proc", "(", "\n", "cfg", ".", "NUM_GPUS", "*", "cfg", ".", "NUM_SHARDS", "\n", ")", ":", "\n", "        ", "writer", "=", "tb", ".", "TensorboardWriter", "(", "cfg", ")", "\n", "", "else", ":", "\n", "        ", "writer", "=", "None", "\n", "\n", "# # Perform multi-view test on the entire dataset.", "\n", "", "tic", "=", "time", ".", "time", "(", ")", "\n", "perform_test", "(", "test_loader", ",", "model", ",", "test_meter", ",", "cfg", ",", "writer", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "f\"total inference is {(toc - tic):.3f}s\"", ")", "\n", "test_loader", ".", "dataset", ".", "close", "(", ")", "\n", "if", "writer", "is", "not", "None", ":", "\n", "        ", "writer", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.visualization.run_visualization": [[24, 150], ["slowfast.visualization.utils.process_layer_index_data", "logger.info", "slowfast.visualization.utils.GetWeightAndActivation", "slowfast.visualization.video_visualizer.VideoVisualizer", "logger.info", "slowfast.visualization.utils.GetWeightAndActivation.get_weights", "writer.plot_weights_and_activations", "isinstance", "meta.items", "slowfast.visualization.utils.GetWeightAndActivation.get_activations", "slowfast.visualization.utils.GetWeightAndActivation.get_activations", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "isinstance", "max", "slowfast.all_gather_unaligned", "range", "logger.info", "range", "inputs.cuda.cuda", "isinstance", "range", "pred.cpu", "box.cpu", "max", "range", "len", "inputs[].cuda", "range", "val.cuda", "len", "range", "inp.cpu", "len", "val[].cuda", "len", "[].cpu", "enumerate", "writer.plot_weights_and_activations", "torch.Tensor().permute().unsqueeze.permute", "slowfast.revert_tensor_normalize", "slowfast.visualization.video_visualizer.VideoVisualizer.draw_clip", "torch.Tensor().permute().unsqueeze", "writer.add_video", "torch.Tensor().permute", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.process_layer_index_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation.get_weights", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.plot_weights_and_activations", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation.get_activations", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation.get_activations", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.plot_weights_and_activations", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.revert_tensor_normalize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_video"], ["def", "run_visualization", "(", "vis_loader", ",", "model", ",", "cfg", ",", "writer", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Run model visualization (weights, activations and model inputs) and visualize\n    them on Tensorboard.\n    Args:\n        vis_loader (loader): video visualization loader.\n        model (model): the video model to visualize.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        writer (TensorboardWriter, optional): TensorboardWriter object\n            to writer Tensorboard log.\n    \"\"\"", "\n", "n_devices", "=", "cfg", ".", "NUM_GPUS", "*", "cfg", ".", "NUM_SHARDS", "\n", "prefix", "=", "\"module/\"", "if", "n_devices", ">", "1", "else", "\"\"", "\n", "# Get a list of selected layer names and indexing.", "\n", "layer_ls", ",", "indexing_dict", "=", "process_layer_index_data", "(", "\n", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "LAYER_LIST", ",", "layer_name_prefix", "=", "prefix", "\n", ")", "\n", "logger", ".", "info", "(", "\"Start Model Visualization.\"", ")", "\n", "# Register hooks for activations.", "\n", "model_vis", "=", "GetWeightAndActivation", "(", "model", ",", "layer_ls", ")", "\n", "\n", "if", "writer", "is", "not", "None", "and", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "MODEL_WEIGHTS", ":", "\n", "        ", "layer_weights", "=", "model_vis", ".", "get_weights", "(", ")", "\n", "writer", ".", "plot_weights_and_activations", "(", "\n", "layer_weights", ",", "tag", "=", "\"Layer Weights/\"", ",", "heat_map", "=", "False", "\n", ")", "\n", "\n", "", "video_vis", "=", "VideoVisualizer", "(", "\n", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "CLASS_NAMES_PATH", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "TOPK_PREDS", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "COLORMAP", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Finish drawing weights.\"", ")", "\n", "global_idx", "=", "-", "1", "\n", "for", "inputs", ",", "_", ",", "_", ",", "meta", "in", "vis_loader", ":", "\n", "        ", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "            ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                    ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "for", "key", ",", "val", "in", "meta", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "val", ",", "(", "list", ",", ")", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "val", ")", ")", ":", "\n", "                        ", "val", "[", "i", "]", "=", "val", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                    ", "meta", "[", "key", "]", "=", "val", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "", "", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "            ", "activations", ",", "preds", "=", "model_vis", ".", "get_activations", "(", "\n", "inputs", ",", "meta", "[", "\"boxes\"", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "activations", ",", "preds", "=", "model_vis", ".", "get_activations", "(", "inputs", ")", "\n", "", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "inputs", "=", "du", ".", "all_gather_unaligned", "(", "inputs", ")", "\n", "activations", "=", "du", ".", "all_gather_unaligned", "(", "activations", ")", "\n", "preds", "=", "du", ".", "all_gather_unaligned", "(", "preds", ")", "\n", "if", "isinstance", "(", "inputs", "[", "0", "]", ",", "list", ")", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                    ", "for", "j", "in", "range", "(", "len", "(", "inputs", "[", "0", "]", ")", ")", ":", "\n", "                        ", "inputs", "[", "i", "]", "[", "j", "]", "=", "inputs", "[", "i", "]", "[", "j", "]", ".", "cpu", "(", ")", "\n", "", "", "", "else", ":", "\n", "                ", "inputs", "=", "[", "inp", ".", "cpu", "(", ")", "for", "inp", "in", "inputs", "]", "\n", "", "preds", "=", "[", "pred", ".", "cpu", "(", ")", "for", "pred", "in", "preds", "]", "\n", "", "else", ":", "\n", "            ", "inputs", ",", "activations", ",", "preds", "=", "[", "inputs", "]", ",", "[", "activations", "]", ",", "[", "preds", "]", "\n", "\n", "", "boxes", "=", "[", "None", "]", "*", "max", "(", "n_devices", ",", "1", ")", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", "and", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "boxes", "=", "du", ".", "all_gather_unaligned", "(", "meta", "[", "\"boxes\"", "]", ")", "\n", "boxes", "=", "[", "box", ".", "cpu", "(", ")", "for", "box", "in", "boxes", "]", "\n", "\n", "", "if", "writer", "is", "not", "None", ":", "\n", "            ", "total_vids", "=", "0", "\n", "for", "i", "in", "range", "(", "max", "(", "n_devices", ",", "1", ")", ")", ":", "\n", "                ", "cur_input", "=", "inputs", "[", "i", "]", "\n", "cur_activations", "=", "activations", "[", "i", "]", "\n", "cur_batch_size", "=", "cur_input", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "cur_preds", "=", "preds", "[", "i", "]", "\n", "cur_boxes", "=", "boxes", "[", "i", "]", "\n", "for", "cur_batch_idx", "in", "range", "(", "cur_batch_size", ")", ":", "\n", "                    ", "global_idx", "+=", "1", "\n", "total_vids", "+=", "1", "\n", "if", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "INPUT_VIDEO", ":", "\n", "                        ", "for", "path_idx", ",", "input_pathway", "in", "enumerate", "(", "cur_input", ")", ":", "\n", "                            ", "if", "cfg", ".", "TEST", ".", "DATASET", "==", "\"ava\"", "and", "cfg", ".", "AVA", ".", "BGR", ":", "\n", "                                ", "video", "=", "input_pathway", "[", "\n", "cur_batch_idx", ",", "[", "2", ",", "1", ",", "0", "]", ",", "...", "\n", "]", "\n", "", "else", ":", "\n", "                                ", "video", "=", "input_pathway", "[", "cur_batch_idx", "]", "\n", "# Permute to (T, H, W, C) from (C, T, H, W).", "\n", "", "video", "=", "video", ".", "permute", "(", "1", ",", "2", ",", "3", ",", "0", ")", "\n", "video", "=", "data_utils", ".", "revert_tensor_normalize", "(", "\n", "video", ",", "cfg", ".", "DATA", ".", "MEAN", ",", "cfg", ".", "DATA", ".", "STD", "\n", ")", "\n", "bboxes", "=", "(", "\n", "None", "if", "cur_boxes", "is", "None", "else", "cur_boxes", "[", ":", ",", "1", ":", "]", "\n", ")", "\n", "video", "=", "video_vis", ".", "draw_clip", "(", "\n", "video", ",", "cur_preds", ",", "bboxes", "=", "bboxes", "\n", ")", "\n", "video", "=", "(", "\n", "torch", ".", "Tensor", "(", "video", ")", "\n", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ")", "\n", "writer", ".", "add_video", "(", "\n", "video", ",", "\n", "tag", "=", "\"Input {}/Input from pathway {}\"", ".", "format", "(", "\n", "global_idx", ",", "path_idx", "+", "1", "\n", ")", ",", "\n", ")", "\n", "", "", "if", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "ACTIVATIONS", ":", "\n", "                        ", "writer", ".", "plot_weights_and_activations", "(", "\n", "cur_activations", ",", "\n", "tag", "=", "\"Input {}/Activations: \"", ".", "format", "(", "global_idx", ")", ",", "\n", "batch_idx", "=", "cur_batch_idx", ",", "\n", "indexing_dict", "=", "indexing_dict", ",", "\n", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"Visualized {} videos...\"", ".", "format", "(", "total_vids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.visualization.visualize": [[152, 200], ["slowfast.init_distributed_training", "numpy.random.seed", "torch.manual_seed", "slowfast.setup_logging", "logger.info", "logger.info", "slowfast.models.build_model", "slowfast.load_test_checkpoint", "slowfast.datasets.loader.construct_loader", "logger.info", "slowfast.is_master_proc", "visualization.run_visualization", "slowfast.is_master_proc", "slowfast.log_model_info", "slowfast.TensorboardWriter", "tb.TensorboardWriter.close", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_distributed_training", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.visualization.run_visualization", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close"], ["", "", "", "def", "visualize", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Perform layer weights and activations visualization on the model.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "if", "cfg", ".", "TENSORBOARD", ".", "ENABLE", "and", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "ENABLE", ":", "\n", "# Set up environment.", "\n", "        ", "du", ".", "init_distributed_training", "(", "cfg", ")", "\n", "# Set random seed from configs.", "\n", "np", ".", "random", ".", "seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "torch", ".", "manual_seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "\n", "# Setup logging format.", "\n", "logging", ".", "setup_logging", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "\n", "# Print config.", "\n", "logger", ".", "info", "(", "\"Model Visualization with config:\"", ")", "\n", "logger", ".", "info", "(", "cfg", ")", "\n", "\n", "# Build the video model and print model statistics.", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "cfg", ".", "LOG_MODEL_INFO", ":", "\n", "            ", "misc", ".", "log_model_info", "(", "model", ",", "cfg", ",", "use_train_input", "=", "False", ")", "\n", "\n", "", "cu", ".", "load_test_checkpoint", "(", "cfg", ",", "model", ")", "\n", "\n", "# Create video testing loaders.", "\n", "vis_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"test\"", ")", "\n", "logger", ".", "info", "(", "\n", "\"Visualize model for {} data points\"", ".", "format", "(", "len", "(", "vis_loader", ")", ")", "\n", ")", "\n", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "            ", "assert", "cfg", ".", "NUM_GPUS", "==", "cfg", ".", "TEST", ".", "BATCH_SIZE", "or", "cfg", ".", "NUM_GPUS", "==", "0", "\n", "\n", "# Set up writer for logging to Tensorboard format.", "\n", "", "if", "du", ".", "is_master_proc", "(", "cfg", ".", "NUM_GPUS", "*", "cfg", ".", "NUM_SHARDS", ")", ":", "\n", "            ", "writer", "=", "tb", ".", "TensorboardWriter", "(", "cfg", ")", "\n", "", "else", ":", "\n", "            ", "writer", "=", "None", "\n", "\n", "# Run visualization on the model", "\n", "", "run_visualization", "(", "vis_loader", ",", "model", ",", "cfg", ",", "writer", ")", "\n", "\n", "if", "writer", "is", "not", "None", ":", "\n", "            ", "writer", ".", "close", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.benchmark.main": [[15, 21], ["slowfast.utils.parser.parse_args", "slowfast.utils.parser.load_config", "slowfast.utils.misc.launch_job"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.parse_args", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.load_config", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.launch_job"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "cfg", "=", "load_config", "(", "args", ")", "\n", "\n", "launch_job", "(", "\n", "cfg", "=", "cfg", ",", "init_method", "=", "args", ".", "init_method", ",", "func", "=", "benchmark_data_loading", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.prepare_data": [[29, 51], ["isinstance", "labels.cuda.cuda", "video_idx.cuda.cuda", "meta.items", "range", "range", "inputs.cuda.cuda", "isinstance", "len", "len", "inputs[].cuda", "range", "val.cuda", "FBs[].cuda", "len", "val[].cuda"], "function", ["None"], ["def", "prepare_data", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "    ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# Transfer the data to the current GPU device.", "\n", "", "labels", "=", "labels", ".", "cuda", "(", ")", "\n", "video_idx", "=", "video_idx", ".", "cuda", "(", ")", "\n", "for", "key", ",", "val", "in", "meta", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "val", ",", "(", "list", ",", ")", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "val", ")", ")", ":", "\n", "                ", "val", "[", "i", "]", "=", "val", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "            ", "meta", "[", "key", "]", "=", "val", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "FBs", ")", ")", ":", "\n", "        ", "FBs", "[", "i", "]", "=", "FBs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "if", "FBs", "[", "i", "]", "is", "not", "None", "else", "None", "\n", "\n", "", "return", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.train_epoch": [[53, 193], ["model.train", "train_meter.iter_tic", "len", "logger.info", "enumerate", "slowfast.is_master_proc", "train_meter.reset", "logger.info", "train_net.freeze_bn", "slowfast.get_epoch_lr", "slowfast.set_lr", "loss_fun", "slowfast.check_nan_losses", "optimizer.zero_grad", "loss.item.backward", "optimizer.step", "slowfast.is_master_proc", "slowfast.synchronize", "train_meter.iter_tic", "slowfast.get_local_size", "train_meter.log_epoch_stats", "train_net.prepare_data", "model", "slowfast.get_loss_func", "loss.item.item", "train_meter.iter_toc", "train_meter.update_stats", "train_meter.iter_toc", "train_meter.update_stats", "slowfast.get_local_size", "train_meter.log_iter_stats", "model", "model", "writer.add_scalars", "loss.item.item", "slowfast.topks_correct", "writer.add_scalars", "float", "slowfast.all_reduce", "slowfast.all_reduce", "slowfast.all_reduce", "loss.item.item", "top1_err.item", "top5_err.item", "inputs[].size", "max", "model.size"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.train", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.freeze_bn", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.get_epoch_lr", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.set_lr", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.check_nan_losses", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.GroupGather.backward", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.synchronize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_epoch_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.prepare_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.losses.get_loss_func", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_iter_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_scalars", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_scalars", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_reduce", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_reduce", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_reduce"], ["", "def", "train_epoch", "(", "\n", "train_loader", ",", "model", ",", "optimizer", ",", "train_meter", ",", "cur_epoch", ",", "cfg", ",", "writer", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Perform the video training for one epoch.\n    Args:\n        train_loader (loader): video training loader.\n        model (model): the video model to train.\n        optimizer (optim): the optimizer to perform optimization on the model's\n            parameters.\n        train_meter (TrainMeter): training meters to log the training performance.\n        cur_epoch (int): current epoch of training.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        writer (TensorboardWriter, optional): TensorboardWriter object\n            to writer Tensorboard log.\n    \"\"\"", "\n", "# Enable train mode.", "\n", "model", ".", "train", "(", ")", "\n", "\n", "if", "cfg", ".", "BN", ".", "FREEZE", ":", "\n", "        ", "logger", ".", "info", "(", "'Freeze BN Layers'", ")", "\n", "freeze_bn", "(", "model", ")", "\n", "\n", "", "train_meter", ".", "iter_tic", "(", ")", "\n", "data_size", "=", "len", "(", "train_loader", ")", "\n", "logger", ".", "info", "(", "'training for {} iterations'", ".", "format", "(", "data_size", ")", ")", "\n", "\n", "for", "cur_iter", ",", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "\n", "        ", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", "=", "prepare_data", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", "\n", "\n", "# Update the learning rate.", "\n", "", "lr", "=", "optim", ".", "get_epoch_lr", "(", "cur_epoch", "+", "float", "(", "cur_iter", ")", "/", "data_size", ",", "cfg", ")", "\n", "optim", ".", "set_lr", "(", "optimizer", ",", "lr", ")", "\n", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "# Compute the predictions.", "\n", "            ", "if", "cfg", ".", "LSTC", ".", "ENABLE", ":", "\n", "                ", "preds", ",", "sub_pred", ",", "bank_pred", "=", "model", "(", "inputs", ",", "bboxes", "=", "meta", "[", "\"boxes\"", "]", ",", "extract", "=", "False", ",", "FBs", "=", "FBs", ",", "BTs", "=", "BTs", ")", "\n", "", "else", ":", "\n", "                ", "preds", "=", "model", "(", "inputs", ",", "bboxes", "=", "meta", "[", "\"boxes\"", "]", ",", "extract", "=", "False", ",", "FBs", "=", "None", ",", "BTs", "=", "None", ")", "\n", "sub_pred", ",", "bank_pred", "=", "None", ",", "None", "\n", "", "", "else", ":", "\n", "# Perform the forward pass.", "\n", "            ", "preds", "=", "model", "(", "inputs", ")", "\n", "# Explicitly declare reduction to mean.", "\n", "\n", "", "loss_fun", "=", "losses", ".", "get_loss_func", "(", "cfg", ".", "MODEL", ".", "LOSS_FUNC", ")", "(", "reduction", "=", "\"mean\"", ")", "\n", "\n", "# Compute the loss.", "\n", "loss", "=", "loss_fun", "(", "preds", ",", "labels", ")", "\n", "\n", "# check Nan Loss.", "\n", "misc", ".", "check_nan_losses", "(", "loss", ")", "\n", "\n", "# Perform the backward pass.", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "# Update the parameters.", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "\n", "            ", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                ", "loss", "=", "du", ".", "all_reduce", "(", "[", "loss", "]", ")", "[", "0", "]", "\n", "", "loss", "=", "loss", ".", "item", "(", ")", "\n", "\n", "train_meter", ".", "iter_toc", "(", ")", "\n", "# Update and log stats.", "\n", "train_meter", ".", "update_stats", "(", "None", ",", "None", ",", "None", ",", "loss", ",", "lr", ")", "\n", "# write to tensorboard format if available.", "\n", "if", "writer", "is", "not", "None", ":", "\n", "                ", "writer", ".", "add_scalars", "(", "\n", "{", "\"Train/loss\"", ":", "loss", ",", "\"Train/lr\"", ":", "lr", "}", ",", "\n", "global_step", "=", "data_size", "*", "cur_epoch", "+", "cur_iter", ",", "\n", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "top1_err", ",", "top5_err", "=", "None", ",", "None", "\n", "if", "cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "# Gather all the predictions across all the devices.", "\n", "                ", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                    ", "[", "loss", "]", "=", "du", ".", "all_reduce", "(", "[", "loss", "]", ")", "\n", "", "loss", "=", "loss", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "# Compute the errors.", "\n", "                ", "num_topks_correct", "=", "metrics", ".", "topks_correct", "(", "preds", ",", "labels", ",", "(", "1", ",", "5", ")", ")", "\n", "top1_err", ",", "top5_err", "=", "[", "\n", "(", "1.0", "-", "x", "/", "preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "for", "x", "in", "num_topks_correct", "\n", "]", "\n", "\n", "# Gather all the predictions across all the devices.", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                    ", "loss", ",", "top1_err", ",", "top5_err", "=", "du", ".", "all_reduce", "(", "\n", "[", "loss", ",", "top1_err", ",", "top5_err", "]", "\n", ")", "\n", "\n", "# Copy the stats from GPU to CPU (sync point).", "\n", "", "loss", ",", "top1_err", ",", "top5_err", "=", "(", "\n", "loss", ".", "item", "(", ")", ",", "\n", "top1_err", ".", "item", "(", ")", ",", "\n", "top5_err", ".", "item", "(", ")", ",", "\n", ")", "\n", "\n", "", "train_meter", ".", "iter_toc", "(", ")", "\n", "# Update and log stats.", "\n", "train_meter", ".", "update_stats", "(", "\n", "top1_err", ",", "\n", "top5_err", ",", "\n", "loss", ",", "\n", "lr", ",", "\n", "inputs", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "*", "max", "(", "\n", "cfg", ".", "NUM_GPUS", ",", "1", "\n", ")", ",", "# If running  on CPU (cfg.NUM_GPUS == 1), use 1 to represent 1 CPU.", "\n", ")", "\n", "# write to tensorboard format if available.", "\n", "if", "writer", "is", "not", "None", ":", "\n", "                ", "writer", ".", "add_scalars", "(", "\n", "{", "\n", "\"Train/loss\"", ":", "loss", ",", "\n", "\"Train/lr\"", ":", "lr", ",", "\n", "\"Train/Top1_err\"", ":", "top1_err", ",", "\n", "\"Train/Top5_err\"", ":", "top5_err", ",", "\n", "}", ",", "\n", "global_step", "=", "data_size", "*", "cur_epoch", "+", "cur_iter", ",", "\n", ")", "\n", "", "", "if", "du", ".", "is_master_proc", "(", "du", ".", "get_local_size", "(", ")", ")", ":", "\n", "            ", "train_meter", ".", "log_iter_stats", "(", "cur_epoch", ",", "cur_iter", ")", "\n", "", "du", ".", "synchronize", "(", ")", "\n", "train_meter", ".", "iter_tic", "(", ")", "\n", "\n", "# Log epoch stats.", "\n", "", "if", "du", ".", "is_master_proc", "(", "du", ".", "get_local_size", "(", ")", ")", ":", "\n", "        ", "train_meter", ".", "log_epoch_stats", "(", "cur_epoch", ")", "\n", "", "train_meter", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.eval_epoch": [[195, 311], ["torch.no_grad", "torch.no_grad", "model.eval", "val_meter.iter_tic", "enumerate", "val_meter.log_epoch_stats", "val_meter.reset", "slowfast.is_master_proc", "slowfast.synchronize", "val_meter.iter_tic", "train_net.prepare_data", "list", "list.sort", "val_meter.iter_toc", "val_meter.update_stats", "model", "val_meter.update_predictions", "slowfast.get_local_size", "val_meter.log_iter_stats", "writer.add_scalars", "writer.plot_eval", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "set", "model", "model", "torch.cat", "torch.cat", "preds.cpu.cpu", "ori_boxes.cpu.cpu", "metadata.cpu.cpu", "slowfast.topks_correct", "val_meter.iter_toc", "val_meter.update_stats", "pred.clone().detach", "label.clone().detach", "slowfast.all_gather_unaligned", "slowfast.all_gather_unaligned", "metadata[].detach().cpu().numpy().tolist", "slowfast.all_gather_unaligned", "slowfast.all_gather", "slowfast.all_reduce", "top1_err.item", "top5_err.item", "writer.add_scalars", "pred.cpu", "label.cpu", "inputs[].size", "max", "pred.clone", "label.clone", "metadata[].detach().cpu().numpy", "preds.cpu.size", "metadata[].detach().cpu", "len", "metadata[].detach"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_epoch_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.synchronize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.prepare_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_predictions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_iter_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_scalars", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.plot_eval", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_reduce", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_scalars"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "eval_epoch", "(", "val_loader", ",", "model", ",", "val_meter", ",", "cur_epoch", ",", "cfg", ",", "writer", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Evaluate the model on the val set.\n    Args:\n        val_loader (loader): data loader to provide validation data.\n        model (model): model to evaluate the performance.\n        val_meter (ValMeter): meter instance to record and calculate the metrics.\n        cur_epoch (int): number of the current epoch of training.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        writer (TensorboardWriter, optional): TensorboardWriter object\n            to writer Tensorboard log.\n    \"\"\"", "\n", "\n", "# Evaluation mode enabled. The running stats would not be updated.", "\n", "model", ".", "eval", "(", ")", "\n", "val_meter", ".", "iter_tic", "(", ")", "\n", "\n", "for", "cur_iter", ",", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "        ", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", "=", "prepare_data", "(", "inputs", ",", "labels", ",", "video_idx", ",", "meta", ",", "FBs", ",", "BTs", ")", "\n", "\n", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "# Compute the predictions.", "\n", "            ", "ori_boxes", "=", "meta", "[", "\"ori_boxes\"", "]", "\n", "metadata", "=", "meta", "[", "\"metadata\"", "]", "\n", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                ", "ori_boxes", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "ori_boxes", ")", ",", "dim", "=", "0", ")", "\n", "metadata", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "metadata", ")", ",", "dim", "=", "0", ")", "\n", "\n", "", "meta_set", "=", "list", "(", "set", "(", "metadata", "[", ":", ",", "0", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", ")", "\n", "meta_set", ".", "sort", "(", ")", "\n", "if", "cfg", ".", "LSTC", ".", "ENABLE", ":", "\n", "                ", "preds", ",", "sub_pred", ",", "bank_pred", "=", "model", "(", "inputs", ",", "bboxes", "=", "meta", "[", "\"boxes\"", "]", ",", "extract", "=", "False", ",", "FBs", "=", "FBs", ",", "BTs", "=", "BTs", ")", "\n", "", "else", ":", "\n", "                ", "preds", "=", "model", "(", "inputs", ",", "bboxes", "=", "meta", "[", "\"boxes\"", "]", ",", "extract", "=", "False", ",", "FBs", "=", "FBs", ",", "BTs", "=", "BTs", ")", "\n", "\n", "", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                ", "preds", "=", "torch", ".", "cat", "(", "du", ".", "all_gather_unaligned", "(", "preds", ")", ",", "dim", "=", "0", ")", "\n", "\n", "", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "                ", "preds", "=", "preds", ".", "cpu", "(", ")", "\n", "ori_boxes", "=", "ori_boxes", ".", "cpu", "(", ")", "\n", "metadata", "=", "metadata", ".", "cpu", "(", ")", "\n", "\n", "", "val_meter", ".", "iter_toc", "(", ")", "\n", "# Update and log stats.", "\n", "val_meter", ".", "update_stats", "(", "preds", ",", "ori_boxes", ",", "metadata", ")", "\n", "\n", "", "else", ":", "\n", "            ", "preds", "=", "model", "(", "inputs", ")", "\n", "\n", "if", "cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "                ", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                    ", "preds", ",", "labels", "=", "du", ".", "all_gather", "(", "[", "preds", ",", "labels", "]", ")", "\n", "", "", "else", ":", "\n", "# Compute the errors.", "\n", "                ", "num_topks_correct", "=", "metrics", ".", "topks_correct", "(", "preds", ",", "labels", ",", "(", "1", ",", "5", ")", ")", "\n", "\n", "# Combine the errors across the GPUs.", "\n", "top1_err", ",", "top5_err", "=", "[", "\n", "(", "1.0", "-", "x", "/", "preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "for", "x", "in", "num_topks_correct", "\n", "]", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "                    ", "top1_err", ",", "top5_err", "=", "du", ".", "all_reduce", "(", "[", "top1_err", ",", "top5_err", "]", ")", "\n", "\n", "# Copy the errors from GPU to CPU (sync point).", "\n", "", "top1_err", ",", "top5_err", "=", "top1_err", ".", "item", "(", ")", ",", "top5_err", ".", "item", "(", ")", "\n", "\n", "val_meter", ".", "iter_toc", "(", ")", "\n", "# Update and log stats.", "\n", "val_meter", ".", "update_stats", "(", "\n", "top1_err", ",", "\n", "top5_err", ",", "\n", "inputs", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "*", "max", "(", "\n", "cfg", ".", "NUM_GPUS", ",", "1", "\n", ")", ",", "# If running  on CPU (cfg.NUM_GPUS == 1), use 1 to represent 1 CPU.", "\n", ")", "\n", "# write to tensorboard format if available.", "\n", "if", "writer", "is", "not", "None", ":", "\n", "                    ", "writer", ".", "add_scalars", "(", "\n", "{", "\"Val/Top1_err\"", ":", "top1_err", ",", "\"Val/Top5_err\"", ":", "top5_err", "}", ",", "\n", "global_step", "=", "len", "(", "val_loader", ")", "*", "cur_epoch", "+", "cur_iter", ",", "\n", ")", "\n", "\n", "", "", "val_meter", ".", "update_predictions", "(", "preds", ",", "labels", ")", "\n", "\n", "", "if", "du", ".", "is_master_proc", "(", "du", ".", "get_local_size", "(", ")", ")", ":", "\n", "            ", "val_meter", ".", "log_iter_stats", "(", "cur_epoch", ",", "cur_iter", ")", "\n", "", "du", ".", "synchronize", "(", ")", "\n", "\n", "val_meter", ".", "iter_tic", "(", ")", "\n", "\n", "# Log epoch stats.", "\n", "", "val_meter", ".", "log_epoch_stats", "(", "cur_epoch", ")", "\n", "# write to tensorboard format if available.", "\n", "if", "writer", "is", "not", "None", ":", "\n", "        ", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "            ", "writer", ".", "add_scalars", "(", "\n", "{", "\"Val/mAP\"", ":", "val_meter", ".", "full_map", "}", ",", "global_step", "=", "cur_epoch", "\n", ")", "\n", "", "else", ":", "\n", "            ", "all_preds", "=", "[", "pred", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "pred", "in", "val_meter", ".", "all_preds", "]", "\n", "all_labels", "=", "[", "label", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "label", "in", "val_meter", ".", "all_labels", "]", "\n", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "                ", "all_preds", "=", "[", "pred", ".", "cpu", "(", ")", "for", "pred", "in", "all_preds", "]", "\n", "all_labels", "=", "[", "label", ".", "cpu", "(", ")", "for", "label", "in", "all_labels", "]", "\n", "", "writer", ".", "plot_eval", "(", "\n", "preds", "=", "all_preds", ",", "labels", "=", "all_labels", ",", "global_step", "=", "cur_epoch", "\n", ")", "\n", "\n", "", "", "val_meter", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.calculate_and_update_precise_bn": [[313, 335], ["fvcore.nn.precise_bn.update_bn_stats", "train_net.calculate_and_update_precise_bn._gen_loader"], "function", ["None"], ["", "def", "calculate_and_update_precise_bn", "(", "loader", ",", "model", ",", "num_iters", "=", "200", ",", "use_gpu", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Update the stats in bn layers by calculate the precise stats.\n    Args:\n        loader (loader): data loader to provide training data.\n        model (model): model to update the bn stats.\n        num_iters (int): number of iterations to compute and update the bn stats.\n        use_gpu (bool): whether to use GPU or not.\n    \"\"\"", "\n", "\n", "def", "_gen_loader", "(", ")", ":", "\n", "        ", "for", "inputs", ",", "_", ",", "_", ",", "_", "in", "loader", ":", "\n", "            ", "if", "use_gpu", ":", "\n", "                ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                        ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                    ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "yield", "inputs", "\n", "\n", "# Update the bn stats.", "\n", "", "", "update_bn_stats", "(", "model", ",", "_gen_loader", "(", ")", ",", "num_iters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.build_trainer": [[337, 380], ["slowfast.models.build_model", "slowfast.construct_optimizer", "slowfast.datasets.loader.construct_loader", "slowfast.datasets.loader.construct_loader", "slowfast.datasets.loader.construct_loader", "slowfast.utils.meters.TrainMeter", "slowfast.utils.meters.ValMeter", "slowfast.is_master_proc", "slowfast.log_model_info", "len", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.construct_optimizer", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info"], ["", "def", "build_trainer", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Build training model and its associated tools, including optimizer,\n    dataloaders and meters.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    Returns:\n        model (nn.Module): training model.\n        optimizer (Optimizer): optimizer.\n        train_loader (DataLoader): training data loader.\n        val_loader (DataLoader): validatoin data loader.\n        precise_bn_loader (DataLoader): training data loader for computing\n            precise BN.\n        train_meter (TrainMeter): tool for measuring training stats.\n        val_meter (ValMeter): tool for measuring validation stats.\n    \"\"\"", "\n", "# Build the video model and print model statistics.", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "cfg", ".", "LOG_MODEL_INFO", ":", "\n", "        ", "misc", ".", "log_model_info", "(", "model", ",", "cfg", ",", "use_train_input", "=", "True", ")", "\n", "\n", "# Construct the optimizer.", "\n", "", "optimizer", "=", "optim", ".", "construct_optimizer", "(", "model", ",", "cfg", ")", "\n", "\n", "# Create the video train and val loaders.", "\n", "train_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"train\"", ")", "\n", "val_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"val\"", ")", "\n", "precise_bn_loader", "=", "loader", ".", "construct_loader", "(", "\n", "cfg", ",", "\"train\"", ",", "is_precise_bn", "=", "True", "\n", ")", "\n", "# Create meters.", "\n", "train_meter", "=", "TrainMeter", "(", "len", "(", "train_loader", ")", ",", "cfg", ")", "\n", "val_meter", "=", "ValMeter", "(", "len", "(", "val_loader", ")", ",", "cfg", ")", "\n", "\n", "return", "(", "\n", "model", ",", "\n", "optimizer", ",", "\n", "train_loader", ",", "\n", "val_loader", ",", "\n", "precise_bn_loader", ",", "\n", "train_meter", ",", "\n", "val_meter", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.freeze_bn": [[382, 390], ["isinstance", "model.modules", "isinstance", "m.eval"], "function", ["None"], ["", "def", "freeze_bn", "(", "model", ")", ":", "\n", "\n", "    ", "if", "isinstance", "(", "model", ",", "(", "nn", ".", "parallel", ".", "DistributedDataParallel", ",", "nn", ".", "DataParallel", ")", ")", ":", "\n", "        ", "model", "=", "model", ".", "module", "\n", "\n", "", "for", "m", "in", "model", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "(", "nn", ".", "BatchNorm3d", ",", "nn", ".", "BatchNorm2d", ",", "nn", ".", "BatchNorm1d", ")", ")", ":", "\n", "            ", "m", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.train": [[391, 519], ["slowfast.init_distributed_training", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "slowfast.setup_logging", "logger.info", "logger.info", "slowfast.models.build_model", "slowfast.construct_optimizer", "slowfast.load_train_checkpoint", "slowfast.datasets.loader.construct_loader", "slowfast.datasets.loader.construct_loader", "slowfast.synchronize", "logger.info", "range", "loader.construct_loader.dataset.close", "loader.construct_loader.dataset.close", "slowfast.utils.multigrid.MultigridSchedule", "slowfast.utils.multigrid.MultigridSchedule.init_multigrid", "pprint.pformat", "slowfast.is_master_proc", "slowfast.log_model_info", "slowfast.utils.meters.TrainMeter", "slowfast.utils.meters.ValMeter", "slowfast.is_master_proc", "slowfast.TensorboardWriter", "slowfast.datasets.loader.shuffle_dataset", "train_net.train_epoch", "slowfast.aggregate_sub_bn_stats", "slowfast.is_checkpoint_epoch", "slowfast.is_eval_epoch", "tb.TensorboardWriter.close", "slowfast.utils.multigrid.MultigridSchedule.update_long_cycle", "slowfast.utils.meters.AVAMeter", "slowfast.utils.meters.AVAMeter", "slowfast.utils.meters.HieveMeter", "slowfast.utils.meters.HieveMeter", "len", "len", "slowfast.utils.multigrid.MultigridSchedule.update_long_cycle", "logger.info", "train_net.calculate_and_update_precise_bn", "slowfast.save_checkpoint", "train_net.eval_epoch", "len", "len", "len", "len", "train_net.build_trainer", "slowfast.has_checkpoint", "logger.info", "slowfast.load_checkpoint", "len", "min", "slowfast.get_last_checkpoint", "fvcore.nn.precise_bn.get_bn_modules", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_distributed_training", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.construct_optimizer", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_train_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.synchronize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.init_multigrid", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.shuffle_dataset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.train_epoch", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.aggregate_sub_bn_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.is_checkpoint_epoch", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.is_eval_epoch", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.update_long_cycle", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.update_long_cycle", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.calculate_and_update_precise_bn", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.save_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.eval_epoch", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.build_trainer", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.has_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_last_checkpoint"], ["", "", "", "def", "train", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Train a video model for many epochs on train set and evaluate it on val set.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "# Set up environment.", "\n", "du", ".", "init_distributed_training", "(", "cfg", ")", "\n", "# Set random seed from configs.", "\n", "np", ".", "random", ".", "seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "torch", ".", "manual_seed", "(", "cfg", ".", "RNG_SEED", ")", "\n", "\n", "# Setup logging format.", "\n", "logging", ".", "setup_logging", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "\n", "# Init multigrid.", "\n", "multigrid", "=", "None", "\n", "if", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE", "or", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE", ":", "\n", "        ", "multigrid", "=", "MultigridSchedule", "(", ")", "\n", "cfg", "=", "multigrid", ".", "init_multigrid", "(", "cfg", ")", "\n", "if", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE", ":", "\n", "            ", "cfg", ",", "_", "=", "multigrid", ".", "update_long_cycle", "(", "cfg", ",", "cur_epoch", "=", "0", ")", "\n", "# Print config.", "\n", "", "", "logger", ".", "info", "(", "\"Train with config:\"", ")", "\n", "logger", ".", "info", "(", "pprint", ".", "pformat", "(", "cfg", ")", ")", "\n", "\n", "# Build the video model and print model statistics.", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "cfg", ".", "LOG_MODEL_INFO", ":", "\n", "        ", "misc", ".", "log_model_info", "(", "model", ",", "cfg", ",", "use_train_input", "=", "True", ")", "\n", "\n", "# Construct the optimizer.", "\n", "", "optimizer", "=", "optim", ".", "construct_optimizer", "(", "model", ",", "cfg", ")", "\n", "\n", "# Load a checkpoint to resume training if applicable.", "\n", "start_epoch", "=", "cu", ".", "load_train_checkpoint", "(", "cfg", ",", "model", ",", "optimizer", ")", "\n", "\n", "# Create the video train and val loaders.", "\n", "train_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"train\"", ")", "\n", "val_loader", "=", "loader", ".", "construct_loader", "(", "cfg", ",", "\"val\"", ")", "\n", "# precise_bn_loader = loader.construct_loader(", "\n", "#     cfg, \"train\", is_precise_bn=True", "\n", "# )", "\n", "du", ".", "synchronize", "(", ")", "\n", "\n", "# Create meters.", "\n", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "        ", "if", "cfg", ".", "TRAIN", ".", "DATASET", "==", "'ava'", ":", "\n", "            ", "train_meter", "=", "AVAMeter", "(", "len", "(", "train_loader", ")", ",", "cfg", ",", "mode", "=", "\"train\"", ")", "\n", "val_meter", "=", "AVAMeter", "(", "len", "(", "val_loader", ")", ",", "cfg", ",", "mode", "=", "\"val\"", ")", "\n", "", "else", ":", "\n", "            ", "train_meter", "=", "HieveMeter", "(", "len", "(", "train_loader", ")", ",", "cfg", ",", "mode", "=", "\"train\"", ")", "\n", "val_meter", "=", "HieveMeter", "(", "len", "(", "val_loader", ")", ",", "cfg", ",", "mode", "=", "\"val\"", ")", "\n", "", "", "else", ":", "\n", "        ", "train_meter", "=", "TrainMeter", "(", "len", "(", "train_loader", ")", ",", "cfg", ")", "\n", "val_meter", "=", "ValMeter", "(", "len", "(", "val_loader", ")", ",", "cfg", ")", "\n", "\n", "# set up writer for logging to Tensorboard format.", "\n", "", "if", "cfg", ".", "TENSORBOARD", ".", "ENABLE", "and", "du", ".", "is_master_proc", "(", "\n", "cfg", ".", "NUM_GPUS", "*", "cfg", ".", "NUM_SHARDS", "\n", ")", ":", "\n", "        ", "writer", "=", "tb", ".", "TensorboardWriter", "(", "cfg", ")", "\n", "", "else", ":", "\n", "        ", "writer", "=", "None", "\n", "\n", "# Perform the training loop.", "\n", "", "logger", ".", "info", "(", "\"Start epoch: {}\"", ".", "format", "(", "start_epoch", "+", "1", ")", ")", "\n", "\n", "for", "cur_epoch", "in", "range", "(", "start_epoch", ",", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", ")", ":", "\n", "        ", "if", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE", ":", "\n", "            ", "cfg", ",", "changed", "=", "multigrid", ".", "update_long_cycle", "(", "cfg", ",", "cur_epoch", ")", "\n", "if", "changed", ":", "\n", "                ", "(", "\n", "model", ",", "\n", "optimizer", ",", "\n", "train_loader", ",", "\n", "val_loader", ",", "\n", "precise_bn_loader", ",", "\n", "train_meter", ",", "\n", "val_meter", ",", "\n", ")", "=", "build_trainer", "(", "cfg", ")", "\n", "\n", "# Load checkpoint.", "\n", "if", "cu", ".", "has_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ")", ":", "\n", "                    ", "last_checkpoint", "=", "cu", ".", "get_last_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "assert", "\"{:05d}.pyth\"", ".", "format", "(", "cur_epoch", ")", "in", "last_checkpoint", "\n", "", "else", ":", "\n", "                    ", "last_checkpoint", "=", "cfg", ".", "TRAIN", ".", "CHECKPOINT_FILE_PATH", "\n", "", "logger", ".", "info", "(", "\"Load from {}\"", ".", "format", "(", "last_checkpoint", ")", ")", "\n", "cu", ".", "load_checkpoint", "(", "\n", "last_checkpoint", ",", "model", ",", "cfg", ".", "NUM_GPUS", ">", "1", ",", "optimizer", "\n", ")", "\n", "\n", "# Shuffle the dataset.", "\n", "", "", "loader", ".", "shuffle_dataset", "(", "train_loader", ",", "cur_epoch", ")", "\n", "# Train for one epoch.", "\n", "train_epoch", "(", "\n", "train_loader", ",", "model", ",", "optimizer", ",", "train_meter", ",", "cur_epoch", ",", "cfg", ",", "writer", "\n", ")", "\n", "\n", "# Compute precise BN stats.", "\n", "if", "cfg", ".", "BN", ".", "USE_PRECISE_STATS", "and", "len", "(", "get_bn_modules", "(", "model", ")", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'synchronize BN states'", ")", "\n", "calculate_and_update_precise_bn", "(", "\n", "precise_bn_loader", ",", "\n", "model", ",", "\n", "min", "(", "cfg", ".", "BN", ".", "NUM_BATCHES_PRECISE", ",", "len", "(", "precise_bn_loader", ")", ")", ",", "\n", "cfg", ".", "NUM_GPUS", ">", "0", ",", "\n", ")", "\n", "", "_", "=", "misc", ".", "aggregate_sub_bn_stats", "(", "model", ")", "\n", "\n", "# Save a checkpoint.", "\n", "if", "cu", ".", "is_checkpoint_epoch", "(", "\n", "cfg", ",", "cur_epoch", ",", "None", "if", "multigrid", "is", "None", "else", "multigrid", ".", "schedule", "\n", ")", ":", "\n", "            ", "cu", ".", "save_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ",", "model", ",", "optimizer", ",", "cur_epoch", ",", "cfg", ")", "\n", "# Evaluate the model on validation set.", "\n", "", "if", "misc", ".", "is_eval_epoch", "(", "\n", "cfg", ",", "cur_epoch", ",", "None", "if", "multigrid", "is", "None", "else", "multigrid", ".", "schedule", "\n", ")", ":", "\n", "\n", "            ", "eval_epoch", "(", "val_loader", ",", "model", ",", "val_meter", ",", "cur_epoch", ",", "cfg", ",", "writer", ")", "\n", "\n", "", "", "train_loader", ".", "dataset", ".", "close", "(", ")", "\n", "val_loader", ".", "dataset", ".", "close", "(", ")", "\n", "if", "writer", "is", "not", "None", ":", "\n", "        ", "writer", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.run_net.main": [[13, 31], ["slowfast.utils.parser.parse_args", "slowfast.utils.parser.load_config", "slowfast.utils.misc.launch_job", "slowfast.utils.misc.launch_job", "demo_net.demo"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.parse_args", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.load_config", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.launch_job", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.launch_job", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.demo_net.demo"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n    Main function to spawn the train and test process.\n    \"\"\"", "\n", "args", "=", "parse_args", "(", ")", "\n", "cfg", "=", "load_config", "(", "args", ")", "\n", "\n", "# Perform training.", "\n", "if", "cfg", ".", "TRAIN", ".", "ENABLE", ":", "\n", "        ", "launch_job", "(", "cfg", "=", "cfg", ",", "args", "=", "args", ",", "func", "=", "train", ",", "start_method", "=", "'cmd'", ")", "\n", "\n", "# Perform multi-clip testing.", "\n", "", "if", "cfg", ".", "TEST", ".", "ENABLE", ":", "\n", "        ", "launch_job", "(", "cfg", "=", "cfg", ",", "args", "=", "args", ",", "func", "=", "test", ",", "start_method", "=", "'cmd'", ")", "\n", "\n", "# Run demo.", "\n", "", "if", "cfg", ".", "DEMO", ".", "ENABLE", ":", "\n", "        ", "demo", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multiprocessing.run": [[9, 51], ["torch.cuda.set_device", "func", "torch.distributed.init_process_group"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_process_group"], ["def", "run", "(", "\n", "local_rank", ",", "num_proc", ",", "func", ",", "init_method", ",", "shard_id", ",", "num_shards", ",", "backend", ",", "cfg", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Runs a function from a child process.\n    Args:\n        local_rank (int): rank of the current process on the current machine.\n        num_proc (int): number of processes per machine.\n        func (function): function to execute on each of the process.\n        init_method (string): method to initialize the distributed training.\n            TCP initialization: equiring a network address reachable from all\n            processes followed by the port.\n            Shared file-system initializaerall machines for the distributed\n            training job.\n        backend (string): three distributed backends ('nccl', 'gloo', 'mpi') are\n            supports, each with different capabilities. Details can be found\n            here:tion: makes use of a file system that\n            is shared and visible from all machines. The URL should start with\n            file:// and contain a path to a non-existent file on a shared file\n            system.\n        shard_id (int): the rank of the current machine.\n        num_shards (int): number of ov\n            https://pytorch.org/docs/stable/distributed.html\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "# Initialize the process group.", "\n", "world_size", "=", "num_proc", "*", "num_shards", "\n", "rank", "=", "shard_id", "*", "num_proc", "+", "local_rank", "\n", "\n", "try", ":", "\n", "        ", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "backend", "=", "backend", ",", "\n", "init_method", "=", "init_method", ",", "\n", "world_size", "=", "world_size", ",", "\n", "rank", "=", "rank", ",", "\n", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "raise", "e", "\n", "\n", "", "torch", ".", "cuda", ".", "set_device", "(", "local_rank", ")", "\n", "func", "(", "cfg", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.make_image_key": [[49, 52], ["int"], "function", ["None"], ["def", "make_image_key", "(", "video_id", ",", "timestamp", ")", ":", "\n", "    ", "\"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"", "\n", "return", "\"%s,%04d\"", "%", "(", "video_id", ",", "int", "(", "timestamp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv": [[54, 94], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "logger.info", "fvcore.common.file_io.PathManager.open", "csv.reader", "ava_eval_helper.make_image_key", "int", "boxes[].append", "labels[].append", "scores[].append", "len", "float", "float"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.make_image_key"], ["", "def", "read_csv", "(", "csv_file", ",", "class_whitelist", "=", "None", ",", "load_score", "=", "False", ")", ":", "\n", "    ", "\"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels\n        not in this set are skipped.\n    Returns:\n      boxes: A dictionary mapping each unique image key (string) to a list of\n        boxes, given as coordinates [y1, x1, y2, x2].\n      labels: A dictionary mapping each unique image key (string) to a list of\n        integer class lables, matching the corresponding box in `boxes`.\n      scores: A dictionary mapping each unique image key (string) to a list of\n        score values lables, matching the corresponding label in `labels`. If\n        scores are not provided in the csv, then they will default to 1.0.\n    \"\"\"", "\n", "boxes", "=", "defaultdict", "(", "list", ")", "\n", "labels", "=", "defaultdict", "(", "list", ")", "\n", "scores", "=", "defaultdict", "(", "list", ")", "\n", "ignored_item", "=", "0", "\n", "with", "PathManager", ".", "open", "(", "csv_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "assert", "len", "(", "row", ")", "in", "[", "7", ",", "8", "]", ",", "\"Wrong number of columns: \"", "+", "row", "\n", "image_key", "=", "make_image_key", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ")", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "[", "float", "(", "n", ")", "for", "n", "in", "row", "[", "2", ":", "6", "]", "]", "\n", "action_id", "=", "int", "(", "row", "[", "6", "]", ")", "\n", "if", "class_whitelist", "and", "action_id", "not", "in", "class_whitelist", ":", "\n", "                ", "ignored_item", "+=", "1", "\n", "continue", "\n", "", "score", "=", "1.0", "\n", "if", "load_score", ":", "\n", "                ", "score", "=", "float", "(", "row", "[", "7", "]", ")", "\n", "", "boxes", "[", "image_key", "]", ".", "append", "(", "[", "y1", ",", "x1", ",", "y2", ",", "x2", "]", ")", "\n", "labels", "[", "image_key", "]", ".", "append", "(", "action_id", ")", "\n", "scores", "[", "image_key", "]", ".", "append", "(", "score", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"{} items with ignored labels are removed from groundtruth list\"", ".", "format", "(", "ignored_item", ")", ")", "\n", "\n", "return", "boxes", ",", "labels", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_exclusions": [[96, 112], ["set", "fvcore.common.file_io.PathManager.open", "csv.reader", "set.add", "len", "ava_eval_helper.make_image_key"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.make_image_key"], ["", "def", "read_exclusions", "(", "exclusions_file", ")", ":", "\n", "    ", "\"\"\"Reads a CSV file of excluded timestamps.\n    Args:\n      exclusions_file: A file object containing a csv of video-id,timestamp.\n    Returns:\n      A set of strings containing excluded image keys, e.g. \"aaaaaaaaaaa,0904\",\n      or an empty set if exclusions file is None.\n    \"\"\"", "\n", "excluded", "=", "set", "(", ")", "\n", "if", "exclusions_file", ":", "\n", "        ", "with", "PathManager", ".", "open", "(", "exclusions_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "assert", "len", "(", "row", ")", "==", "2", ",", "\"Expected only 2 columns, got: \"", "+", "row", "\n", "excluded", ".", "add", "(", "make_image_key", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ")", ")", "\n", "", "", "", "return", "excluded", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_labelmap": [[114, 130], ["set", "fvcore.common.file_io.PathManager.open", "line.startswith", "line.split", "line.startswith", "line.startswith", "int", "labelmap.append", "set.add", "line.strip().split", "line.strip"], "function", ["None"], ["", "def", "read_labelmap", "(", "labelmap_file", ")", ":", "\n", "    ", "\"\"\"Read label map and class ids.\"\"\"", "\n", "\n", "labelmap", "=", "[", "]", "\n", "class_ids", "=", "set", "(", ")", "\n", "name", "=", "\"\"", "\n", "class_id", "=", "\"\"", "\n", "with", "PathManager", ".", "open", "(", "labelmap_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "if", "line", ".", "startswith", "(", "\"  name:\"", ")", ":", "\n", "                ", "name", "=", "line", ".", "split", "(", "'\"'", ")", "[", "1", "]", "\n", "", "elif", "line", ".", "startswith", "(", "\"  id:\"", ")", "or", "line", ".", "startswith", "(", "\"  label_id:\"", ")", ":", "\n", "                ", "class_id", "=", "int", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "[", "-", "1", "]", ")", "\n", "labelmap", ".", "append", "(", "{", "\"id\"", ":", "class_id", ",", "\"name\"", ":", "name", "}", ")", "\n", "class_ids", ".", "add", "(", "class_id", ")", "\n", "", "", "", "return", "labelmap", ",", "class_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_ava_from_files": [[132, 140], ["ava_eval_helper.read_labelmap", "ava_eval_helper.read_exclusions", "ava_eval_helper.read_csv", "ava_eval_helper.read_csv", "ava_eval_helper.run_evaluation"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_labelmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_exclusions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.run_evaluation"], ["", "def", "evaluate_ava_from_files", "(", "labelmap", ",", "groundtruth", ",", "detections", ",", "exclusions", ")", ":", "\n", "    ", "\"\"\"Run AVA evaluation given annotation/prediction files.\"\"\"", "\n", "\n", "categories", ",", "class_whitelist", "=", "read_labelmap", "(", "labelmap", ")", "\n", "excluded_keys", "=", "read_exclusions", "(", "exclusions", ")", "\n", "groundtruth", "=", "read_csv", "(", "groundtruth", ",", "class_whitelist", ",", "load_score", "=", "False", ")", "\n", "detections", "=", "read_csv", "(", "detections", ",", "class_whitelist", ",", "load_score", "=", "True", ")", "\n", "run_evaluation", "(", "categories", ",", "groundtruth", ",", "detections", ",", "excluded_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_hieve": [[142, 171], ["time.time", "ava_eval_helper.get_ava_eval_data", "logger.info", "ava_eval_helper.write_results", "logger.info", "len", "time.time"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.get_ava_eval_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.write_results"], ["", "def", "evaluate_hieve", "(", "\n", "preds", ",", "\n", "original_boxes", ",", "\n", "metadata", ",", "\n", "class_whitelist", ",", "\n", "video_idx_to_name", "=", "None", ",", "\n", "name", "=", "\"latest\"", ",", "\n", "output_dir", "=", "\".\"", "\n", ")", ":", "\n", "    ", "eval_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "detections", "=", "get_ava_eval_data", "(", "\n", "preds", ",", "\n", "original_boxes", ",", "\n", "metadata", ",", "\n", "class_whitelist", ",", "\n", "video_idx_to_name", "=", "video_idx_to_name", ",", "\n", "sec_format", "=", "\"%d\"", ",", "\n", "dataset", "=", "\"hieve\"", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Evaluating with %d unique detection frames\"", "%", "len", "(", "detections", "[", "0", "]", ")", "\n", ")", "\n", "\n", "write_results", "(", "detections", ",", "\"{}/detections_{}.csv\"", ".", "format", "(", "output_dir", ",", "name", ")", ")", "\n", "\n", "logger", ".", "info", "(", "\"HIEVE eval done in %f seconds.\"", "%", "(", "time", ".", "time", "(", ")", "-", "eval_start", ")", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_ava": [[173, 219], ["time.time", "ava_eval_helper.get_ava_eval_data", "logger.info", "logger.info", "ava_eval_helper.write_results", "ava_eval_helper.write_results", "ava_eval_helper.run_evaluation", "logger.info", "len", "len", "open", "category.split", "category.split.index", "f.write", "time.time"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.get_ava_eval_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.write_results", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.write_results", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.run_evaluation", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.write"], ["", "def", "evaluate_ava", "(", "\n", "preds", ",", "\n", "original_boxes", ",", "\n", "metadata", ",", "\n", "excluded_keys", ",", "\n", "class_whitelist", ",", "\n", "categories", ",", "\n", "groundtruth", "=", "None", ",", "\n", "video_idx_to_name", "=", "None", ",", "\n", "name", "=", "\"latest\"", ",", "\n", "save_per_class", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Run AVA evaluation given numpy arrays.\"\"\"", "\n", "\n", "eval_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "detections", "=", "get_ava_eval_data", "(", "\n", "preds", ",", "\n", "original_boxes", ",", "\n", "metadata", ",", "\n", "class_whitelist", ",", "\n", "video_idx_to_name", "=", "video_idx_to_name", ",", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"Evaluating with %d unique GT frames.\"", "%", "len", "(", "groundtruth", "[", "0", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"Evaluating with %d unique detection frames\"", "%", "len", "(", "detections", "[", "0", "]", ")", "\n", ")", "\n", "\n", "write_results", "(", "detections", ",", "\"detections_%s.csv\"", "%", "name", ")", "\n", "write_results", "(", "groundtruth", ",", "\"groundtruth_%s.csv\"", "%", "name", ")", "\n", "\n", "results", "=", "run_evaluation", "(", "categories", ",", "groundtruth", ",", "detections", ",", "excluded_keys", ")", "\n", "\n", "if", "save_per_class", ":", "\n", "        ", "with", "open", "(", "'per_class.csv'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "category", "in", "results", ":", "\n", "                ", "if", "'mAP@0.5IOU'", "in", "category", ":", "\n", "                    ", "continue", "\n", "\n", "", "items", "=", "category", ".", "split", "(", "'/'", ")", "\n", "idx", "=", "items", ".", "index", "(", "'AP@0.5IOU'", ")", "\n", "f", ".", "write", "(", "'/'", ".", "join", "(", "items", "[", "idx", "+", "1", ":", "]", ")", "+", "',{}\\n'", ".", "format", "(", "results", "[", "category", "]", ")", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"AVA eval done in %f seconds.\"", "%", "(", "time", ".", "time", "(", ")", "-", "eval_start", ")", ")", "\n", "return", "results", "[", "\"PascalBoxes_Precision/mAP@0.5IOU\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.run_evaluation": [[221, 295], ["slowfast.utils.ava_evaluation.object_detection_evaluation.PascalDetectionEvaluator", "object_detection_evaluation.PascalDetectionEvaluator.evaluate", "pprint.pprint", "object_detection_evaluation.PascalDetectionEvaluator.add_single_ground_truth_image_info", "gt_keys.append", "object_detection_evaluation.PascalDetectionEvaluator.add_single_detected_image_info", "pred_keys.append", "logging.info", "logging.info", "numpy.array", "numpy.array", "numpy.zeros", "numpy.array", "numpy.array", "numpy.array", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_detected_image_info"], ["", "def", "run_evaluation", "(", "\n", "categories", ",", "groundtruth", ",", "detections", ",", "excluded_keys", ",", "verbose", "=", "True", "\n", ")", ":", "\n", "    ", "\"\"\"AVA evaluation main logic.\"\"\"", "\n", "\n", "pascal_evaluator", "=", "object_detection_evaluation", ".", "PascalDetectionEvaluator", "(", "\n", "categories", "\n", ")", "\n", "\n", "boxes", ",", "labels", ",", "_", "=", "groundtruth", "\n", "\n", "gt_keys", "=", "[", "]", "\n", "pred_keys", "=", "[", "]", "\n", "\n", "for", "image_key", "in", "boxes", ":", "\n", "        ", "if", "image_key", "in", "excluded_keys", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "(", "\n", "\"Found excluded timestamp in ground truth: %s. \"", "\n", "\"It will be ignored.\"", "\n", ")", ",", "\n", "image_key", ",", "\n", ")", "\n", "continue", "\n", "", "pascal_evaluator", ".", "add_single_ground_truth_image_info", "(", "\n", "image_key", ",", "\n", "{", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_boxes", ":", "np", ".", "array", "(", "\n", "boxes", "[", "image_key", "]", ",", "dtype", "=", "float", "\n", ")", ",", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_classes", ":", "np", ".", "array", "(", "\n", "labels", "[", "image_key", "]", ",", "dtype", "=", "int", "\n", ")", ",", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_difficult", ":", "np", ".", "zeros", "(", "\n", "len", "(", "boxes", "[", "image_key", "]", ")", ",", "dtype", "=", "bool", "\n", ")", ",", "\n", "}", ",", "\n", ")", "\n", "\n", "gt_keys", ".", "append", "(", "image_key", ")", "\n", "\n", "", "boxes", ",", "labels", ",", "scores", "=", "detections", "\n", "\n", "for", "image_key", "in", "boxes", ":", "\n", "        ", "if", "image_key", "in", "excluded_keys", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "(", "\n", "\"Found excluded timestamp in detections: %s. \"", "\n", "\"It will be ignored.\"", "\n", ")", ",", "\n", "image_key", ",", "\n", ")", "\n", "continue", "\n", "", "pascal_evaluator", ".", "add_single_detected_image_info", "(", "\n", "image_key", ",", "\n", "{", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_boxes", ":", "np", ".", "array", "(", "\n", "boxes", "[", "image_key", "]", ",", "dtype", "=", "float", "\n", ")", ",", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_classes", ":", "np", ".", "array", "(", "\n", "labels", "[", "image_key", "]", ",", "dtype", "=", "int", "\n", ")", ",", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_scores", ":", "np", ".", "array", "(", "\n", "scores", "[", "image_key", "]", ",", "dtype", "=", "float", "\n", ")", ",", "\n", "}", ",", "\n", ")", "\n", "\n", "pred_keys", ".", "append", "(", "image_key", ")", "\n", "\n", "", "metrics", "=", "pascal_evaluator", ".", "evaluate", "(", ")", "\n", "\n", "pprint", ".", "pprint", "(", "metrics", ",", "indent", "=", "2", ")", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.get_ava_eval_data": [[297, 347], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "range", "int", "int", "boxes[].tolist", "numpy.round", "numpy.round", "scores[].tolist", "enumerate", "torch.max", "cls_idx.item.item", "score.item.item", "out_scores[].append", "out_labels[].append", "out_boxes[].append", "out_scores[].append", "out_labels[].append", "out_boxes[].append"], "function", ["None"], ["", "def", "get_ava_eval_data", "(", "\n", "scores", ",", "\n", "boxes", ",", "\n", "metadata", ",", "\n", "class_whitelist", ",", "\n", "verbose", "=", "False", ",", "\n", "video_idx_to_name", "=", "None", ",", "\n", "sec_format", "=", "\"%04d\"", ",", "\n", "dataset", "=", "\"ava\"", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Convert our data format into the data format used in official AVA\n    evaluation.\n    \"\"\"", "\n", "\n", "out_scores", "=", "defaultdict", "(", "list", ")", "\n", "out_labels", "=", "defaultdict", "(", "list", ")", "\n", "out_boxes", "=", "defaultdict", "(", "list", ")", "\n", "count", "=", "0", "\n", "for", "i", "in", "range", "(", "scores", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "video_idx", "=", "int", "(", "np", ".", "round", "(", "metadata", "[", "i", "]", "[", "0", "]", ")", ")", "\n", "sec", "=", "int", "(", "np", ".", "round", "(", "metadata", "[", "i", "]", "[", "1", "]", ")", ")", "\n", "\n", "video", "=", "video_idx_to_name", "[", "video_idx", "]", "\n", "\n", "key", "=", "video", "+", "\",\"", "+", "sec_format", "%", "(", "sec", ")", "\n", "batch_box", "=", "boxes", "[", "i", "]", ".", "tolist", "(", ")", "\n", "# The first is batch idx.", "\n", "batch_box", "=", "[", "batch_box", "[", "j", "]", "for", "j", "in", "[", "0", ",", "2", ",", "1", ",", "4", ",", "3", "]", "]", "\n", "\n", "if", "dataset", "==", "\"ava\"", ":", "\n", "            ", "one_scores", "=", "scores", "[", "i", "]", ".", "tolist", "(", ")", "\n", "for", "cls_idx", ",", "score", "in", "enumerate", "(", "one_scores", ")", ":", "\n", "                ", "if", "cls_idx", "+", "1", "in", "class_whitelist", ":", "\n", "                    ", "out_scores", "[", "key", "]", ".", "append", "(", "score", ")", "\n", "out_labels", "[", "key", "]", ".", "append", "(", "cls_idx", "+", "1", ")", "\n", "out_boxes", "[", "key", "]", ".", "append", "(", "batch_box", "[", "1", ":", "]", ")", "\n", "count", "+=", "1", "\n", "", "", "", "else", ":", "\n", "            ", "one_scores", "=", "scores", "[", "i", "]", "\n", "score", ",", "cls_idx", "=", "torch", ".", "max", "(", "one_scores", ",", "dim", "=", "-", "1", ")", "\n", "cls_idx", "=", "cls_idx", ".", "item", "(", ")", "\n", "score", "=", "score", ".", "item", "(", ")", "\n", "\n", "out_scores", "[", "key", "]", ".", "append", "(", "score", ")", "\n", "out_labels", "[", "key", "]", ".", "append", "(", "cls_idx", "+", "1", ")", "\n", "out_boxes", "[", "key", "]", ".", "append", "(", "batch_box", "[", "1", ":", "]", ")", "\n", "count", "+=", "1", "\n", "\n", "", "", "return", "out_boxes", ",", "out_labels", ",", "out_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.write_results": [[349, 364], ["time.time", "logger.info", "logger.info", "fvcore.common.file_io.PathManager.open", "boxes.keys", "zip", "f.write", "time.time"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.write"], ["", "def", "write_results", "(", "detections", ",", "filename", ")", ":", "\n", "    ", "\"\"\"Write prediction results into official formats.\"\"\"", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "boxes", ",", "labels", ",", "scores", "=", "detections", "\n", "with", "PathManager", ".", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "key", "in", "boxes", ".", "keys", "(", ")", ":", "\n", "            ", "for", "box", ",", "label", ",", "score", "in", "zip", "(", "boxes", "[", "key", "]", ",", "labels", "[", "key", "]", ",", "scores", "[", "key", "]", ")", ":", "\n", "                ", "f", ".", "write", "(", "\n", "\"%s,%.03f,%.03f,%.03f,%.03f,%d,%.04f\\n\"", "\n", "%", "(", "key", ",", "box", "[", "1", "]", ",", "box", "[", "0", "]", ",", "box", "[", "3", "]", ",", "box", "[", "2", "]", ",", "label", ",", "score", ")", "\n", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"AVA results wrote to %s\"", "%", "filename", ")", "\n", "logger", ".", "info", "(", "\"\\ttook %d seconds.\"", "%", "(", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging._suppress_print": [[17, 26], ["None"], "function", ["None"], ["def", "_suppress_print", "(", ")", ":", "\n", "    ", "\"\"\"\n    Suppresses printing from the current process.\n    \"\"\"", "\n", "\n", "def", "print_pass", "(", "*", "objects", ",", "sep", "=", "\" \"", ",", "end", "=", "\"\\n\"", ",", "file", "=", "sys", ".", "stdout", ",", "flush", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "builtins", ".", "print", "=", "print_pass", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging._cached_log_stream": [[27, 30], ["functools.lru_cache", "fvcore.common.file_io.PathManager.open"], "function", ["None"], ["", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "None", ")", "\n", "def", "_cached_log_stream", "(", "filename", ")", ":", "\n", "    ", "return", "PathManager", ".", "open", "(", "filename", ",", "\"a\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging": [[32, 71], ["slowfast.is_master_proc", "logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "slowfast.is_master_proc", "logging.basicConfig", "logging._suppress_print", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "slowfast.is_master_proc", "os.path.join", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "slowfast.get_world_size", "logging._cached_log_stream"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging._suppress_print", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging._cached_log_stream"], ["", "def", "setup_logging", "(", "output_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Sets up the logging for multiple processes. Only enable the logging for the\n    master process, and suppress logging for the non-master processes.\n    \"\"\"", "\n", "# Set up logging format.", "\n", "_FORMAT", "=", "\"[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\"", "\n", "\n", "if", "du", ".", "is_master_proc", "(", ")", ":", "\n", "# Enable logging for the master process.", "\n", "        ", "logging", ".", "root", ".", "handlers", "=", "[", "]", "\n", "logging", ".", "basicConfig", "(", "\n", "level", "=", "logging", ".", "INFO", ",", "format", "=", "_FORMAT", ",", "stream", "=", "sys", ".", "stdout", "\n", ")", "\n", "", "else", ":", "\n", "# Suppress logging for non-master processes.", "\n", "        ", "_suppress_print", "(", ")", "\n", "\n", "# setup root logger", "\n", "", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "plain_formatter", "=", "logging", ".", "Formatter", "(", "\n", "\"[%(asctime)s][%(levelname)s] %(name)s: %(lineno)4d: %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d %H:%M:%S\"", ",", "\n", ")", "\n", "\n", "if", "du", ".", "is_master_proc", "(", ")", ":", "\n", "        ", "ch", "=", "logging", ".", "StreamHandler", "(", "stream", "=", "sys", ".", "stdout", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "ch", ".", "setFormatter", "(", "plain_formatter", ")", "\n", "logger", ".", "addHandler", "(", "ch", ")", "\n", "\n", "", "if", "output_dir", "is", "not", "None", "and", "du", ".", "is_master_proc", "(", "du", ".", "get_world_size", "(", ")", ")", ":", "\n", "        ", "filename", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"stdout.log\"", ")", "\n", "fh", "=", "logging", ".", "StreamHandler", "(", "_cached_log_stream", "(", "filename", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "fh", ".", "setFormatter", "(", "plain_formatter", ")", "\n", "logger", ".", "addHandler", "(", "fh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.get_logger": [[73, 81], ["logging.getLogger"], "function", ["None"], ["", "", "def", "get_logger", "(", "name", ")", ":", "\n", "    ", "\"\"\"\n    Retrieve the logger with the specified name or, if name is None, return a\n    logger which is the root logger of the hierarchy.\n    Args:\n        name (string): name of the logger.\n    \"\"\"", "\n", "return", "logging", ".", "getLogger", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats": [[83, 96], ["simplejson.dumps", "logging.get_logger", "get_logger.info", "isinstance", "decimal.Decimal", "stats.items"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.get_logger"], ["", "def", "log_json_stats", "(", "stats", ")", ":", "\n", "    ", "\"\"\"\n    Logs json stats.\n    Args:\n        stats (dict): a dictionary of statistical information to log.\n    \"\"\"", "\n", "stats", "=", "{", "\n", "k", ":", "decimal", ".", "Decimal", "(", "\"{:.6f}\"", ".", "format", "(", "v", ")", ")", "if", "isinstance", "(", "v", ",", "float", ")", "else", "v", "\n", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", "\n", "}", "\n", "json_stats", "=", "simplejson", ".", "dumps", "(", "stats", ",", "sort_keys", "=", "True", ",", "use_decimal", "=", "True", ")", "\n", "logger", "=", "get_logger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"json_stats: {:s}\"", ".", "format", "(", "json_stats", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather": [[15, 35], ["torch.get_world_size", "torch.all_gather", "gather_list.append", "output_tensor.append", "torch.ones_like", "torch.ones_like", "torch.cat", "torch.cat", "range"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather"], ["def", "all_gather", "(", "tensors", ")", ":", "\n", "    ", "\"\"\"\n    All gathers the provided tensors from all processes across machines.\n    Args:\n        tensors (list): tensors to perform all gather across all processes in\n        all machines.\n    \"\"\"", "\n", "\n", "gather_list", "=", "[", "]", "\n", "output_tensor", "=", "[", "]", "\n", "world_size", "=", "dist", ".", "get_world_size", "(", ")", "\n", "for", "tensor", "in", "tensors", ":", "\n", "        ", "tensor_placeholder", "=", "[", "\n", "torch", ".", "ones_like", "(", "tensor", ")", "for", "_", "in", "range", "(", "world_size", ")", "\n", "]", "\n", "dist", ".", "all_gather", "(", "tensor_placeholder", ",", "tensor", ",", "async_op", "=", "False", ")", "\n", "gather_list", ".", "append", "(", "tensor_placeholder", ")", "\n", "", "for", "gathered_tensor", "in", "gather_list", ":", "\n", "        ", "output_tensor", ".", "append", "(", "torch", ".", "cat", "(", "gathered_tensor", ",", "dim", "=", "0", ")", ")", "\n", "", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_reduce": [[37, 54], ["torch.all_reduce", "torch.get_world_size", "tensor.mul_"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_reduce", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size"], ["", "def", "all_reduce", "(", "tensors", ",", "average", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    All reduce the provided tensors from all processes across machines.\n    Args:\n        tensors (list): tensors to perform all reduce across all processes in\n        all machines.\n        average (bool): scales the reduced tensor by the number of overall\n        processes across all machines.\n    \"\"\"", "\n", "\n", "for", "tensor", "in", "tensors", ":", "\n", "        ", "dist", ".", "all_reduce", "(", "tensor", ",", "async_op", "=", "False", ")", "\n", "", "if", "average", ":", "\n", "        ", "world_size", "=", "dist", ".", "get_world_size", "(", ")", "\n", "for", "tensor", "in", "tensors", ":", "\n", "            ", "tensor", ".", "mul_", "(", "1.0", "/", "world_size", ")", "\n", "", "", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_process_group": [[56, 91], ["torch.cuda.set_device", "torch.cuda.set_device", "torch.init_process_group"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_process_group"], ["", "def", "init_process_group", "(", "\n", "local_rank", ",", "\n", "local_world_size", ",", "\n", "shard_id", ",", "\n", "num_shards", ",", "\n", "init_method", ",", "\n", "dist_backend", "=", "\"nccl\"", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Initializes the default process group.\n    Args:\n        local_rank (int): the rank on the current local machine.\n        local_world_size (int): the world size (number of processes running) on\n        the current local machine.\n        shard_id (int): the shard index (machine rank) of the current machine.\n        num_shards (int): number of shards for distributed training.\n        init_method (string): supporting three different methods for\n            initializing process groups:\n            \"file\": use shared file system to initialize the groups across\n            different processes.\n            \"tcp\": use tcp address to initialize the groups across different\n        dist_backend (string): backend to use for distributed training. Options\n            includes gloo, mpi and nccl, the details can be found here:\n            https://pytorch.org/docs/stable/distributed.html\n    \"\"\"", "\n", "# Sets the GPU to use.", "\n", "torch", ".", "cuda", ".", "set_device", "(", "local_rank", ")", "\n", "# Initialize the process group.", "\n", "proc_rank", "=", "local_rank", "+", "shard_id", "*", "local_world_size", "\n", "world_size", "=", "local_world_size", "*", "num_shards", "\n", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "dist_backend", ",", "\n", "init_method", "=", "init_method", ",", "\n", "world_size", "=", "world_size", ",", "\n", "rank", "=", "proc_rank", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc": [[94, 102], ["torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.get_rank"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_rank"], ["", "def", "is_master_proc", "(", "num_gpus", "=", "8", ")", ":", "\n", "    ", "\"\"\"\n    Determines if the current process is the master process.\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "dist", ".", "get_rank", "(", ")", "%", "num_gpus", "==", "0", "\n", "", "else", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size": [[104, 113], ["torch.get_world_size", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size"], ["", "", "def", "get_world_size", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get the size of the world.\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_rank": [[115, 124], ["torch.get_rank", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_rank"], ["", "def", "get_rank", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get the rank of the current process.\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.synchronize": [[126, 139], ["torch.get_world_size", "torch.barrier", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size"], ["", "def", "synchronize", "(", ")", ":", "\n", "    ", "\"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "\n", "", "world_size", "=", "dist", ".", "get_world_size", "(", ")", "\n", "if", "world_size", "==", "1", ":", "\n", "        ", "return", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed._get_global_gloo_group": [[141, 153], ["functools.lru_cache", "torch.get_backend", "torch.new_group"], "function", ["None"], ["", "@", "functools", ".", "lru_cache", "(", ")", "\n", "def", "_get_global_gloo_group", "(", ")", ":", "\n", "    ", "\"\"\"\n    Return a process group based on gloo backend, containing all the ranks\n    The result is cached.\n    Returns:\n        (group): pytorch dist group.\n    \"\"\"", "\n", "if", "dist", ".", "get_backend", "(", ")", "==", "\"nccl\"", ":", "\n", "        ", "return", "dist", ".", "new_group", "(", "backend", "=", "\"gloo\"", ")", "\n", "", "else", ":", "\n", "        ", "return", "dist", ".", "group", ".", "WORLD", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed._serialize_to_tensor": [[155, 181], ["torch.get_backend", "torch.device", "torch.device", "pickle.dumps", "torch.ByteStorage.from_buffer", "torch.ByteStorage.from_buffer", "torch.ByteTensor().to", "torch.ByteTensor().to", "len", "logging.getLogger", "logging.getLogger.warning", "torch.ByteTensor", "torch.ByteTensor", "distributed.get_rank", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_rank"], ["", "", "def", "_serialize_to_tensor", "(", "data", ",", "group", ")", ":", "\n", "    ", "\"\"\"\n    Seriialize the tensor to ByteTensor. Note that only `gloo` and `nccl`\n        backend is supported.\n    Args:\n        data (data): data to be serialized.\n        group (group): pytorch dist group.\n    Returns:\n        tensor (ByteTensor): tensor that serialized.\n    \"\"\"", "\n", "\n", "backend", "=", "dist", ".", "get_backend", "(", "group", ")", "\n", "assert", "backend", "in", "[", "\"gloo\"", ",", "\"nccl\"", "]", "\n", "device", "=", "torch", ".", "device", "(", "\"cpu\"", "if", "backend", "==", "\"gloo\"", "else", "\"cuda\"", ")", "\n", "\n", "buffer", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "if", "len", "(", "buffer", ")", ">", "1024", "**", "3", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Rank {} trying to all-gather {:.2f} GB of data on device {}\"", ".", "format", "(", "\n", "get_rank", "(", ")", ",", "len", "(", "buffer", ")", "/", "(", "1024", "**", "3", ")", ",", "device", "\n", ")", "\n", ")", "\n", "", "storage", "=", "torch", ".", "ByteStorage", ".", "from_buffer", "(", "buffer", ")", "\n", "tensor", "=", "torch", ".", "ByteTensor", "(", "storage", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed._pad_to_largest_tensor": [[183, 217], ["torch.get_world_size", "torch.tensor", "torch.tensor", "torch.all_gather", "max", "torch.zeros", "torch.zeros", "int", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat.numel", "range", "size.item"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather"], ["", "def", "_pad_to_largest_tensor", "(", "tensor", ",", "group", ")", ":", "\n", "    ", "\"\"\"\n    Padding all the tensors from different GPUs to the largest ones.\n    Args:\n        tensor (tensor): tensor to pad.\n        group (group): pytorch dist group.\n    Returns:\n        list[int]: size of the tensor, on each rank\n        Tensor: padded tensor that has the max size\n    \"\"\"", "\n", "world_size", "=", "dist", ".", "get_world_size", "(", "group", "=", "group", ")", "\n", "assert", "(", "\n", "world_size", ">=", "1", "\n", ")", ",", "\"comm.gather/all_gather must be called from ranks within the given group!\"", "\n", "local_size", "=", "torch", ".", "tensor", "(", "\n", "[", "tensor", ".", "numel", "(", ")", "]", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "tensor", ".", "device", "\n", ")", "\n", "size_list", "=", "[", "\n", "torch", ".", "zeros", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "tensor", ".", "device", ")", "\n", "for", "_", "in", "range", "(", "world_size", ")", "\n", "]", "\n", "dist", ".", "all_gather", "(", "size_list", ",", "local_size", ",", "group", "=", "group", ")", "\n", "size_list", "=", "[", "int", "(", "size", ".", "item", "(", ")", ")", "for", "size", "in", "size_list", "]", "\n", "\n", "max_size", "=", "max", "(", "size_list", ")", "\n", "\n", "# we pad the tensor because torch all_gather does not support", "\n", "# gathering tensors of different shapes", "\n", "if", "local_size", "!=", "max_size", ":", "\n", "        ", "padding", "=", "torch", ".", "zeros", "(", "\n", "(", "max_size", "-", "local_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "tensor", ".", "device", "\n", ")", "\n", "tensor", "=", "torch", ".", "cat", "(", "(", "tensor", ",", "padding", ")", ",", "dim", "=", "0", ")", "\n", "", "return", "size_list", ",", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather_unaligned": [[219, 256], ["distributed._serialize_to_tensor", "distributed._pad_to_largest_tensor", "max", "torch.all_gather", "zip", "distributed.get_world_size", "distributed._get_global_gloo_group", "torch.get_world_size", "torch.empty", "torch.empty", "data_list.append", "_serialize_to_tensor.cpu().numpy().tobytes", "pickle.loads", "_serialize_to_tensor.cpu().numpy", "_serialize_to_tensor.cpu"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed._serialize_to_tensor", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed._pad_to_largest_tensor", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed._get_global_gloo_group", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size"], ["", "def", "all_gather_unaligned", "(", "data", ",", "group", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors).\n\n    Args:\n        data: any picklable object\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"", "\n", "if", "get_world_size", "(", ")", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "_get_global_gloo_group", "(", ")", "\n", "", "if", "dist", ".", "get_world_size", "(", "group", ")", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "\n", "", "tensor", "=", "_serialize_to_tensor", "(", "data", ",", "group", ")", "\n", "\n", "size_list", ",", "tensor", "=", "_pad_to_largest_tensor", "(", "tensor", ",", "group", ")", "\n", "max_size", "=", "max", "(", "size_list", ")", "\n", "\n", "# receiving Tensor from all ranks", "\n", "tensor_list", "=", "[", "\n", "torch", ".", "empty", "(", "(", "max_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "tensor", ".", "device", ")", "\n", "for", "_", "in", "size_list", "\n", "]", "\n", "dist", ".", "all_gather", "(", "tensor_list", ",", "tensor", ",", "group", "=", "group", ")", "\n", "\n", "data_list", "=", "[", "]", "\n", "for", "size", ",", "tensor", "in", "zip", "(", "size_list", ",", "tensor_list", ")", ":", "\n", "        ", "buffer", "=", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tobytes", "(", ")", "[", ":", "size", "]", "\n", "data_list", ".", "append", "(", "pickle", ".", "loads", "(", "buffer", ")", ")", "\n", "\n", "", "return", "data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.init_distributed_training": [[258, 274], ["range", "torch.get_world_size", "list", "torch.new_group", "range"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size"], ["", "def", "init_distributed_training", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Initialize variables needed for distributed training.\n    \"\"\"", "\n", "if", "cfg", ".", "NUM_GPUS", "<=", "1", ":", "\n", "        ", "return", "\n", "", "num_gpus_per_machine", "=", "cfg", ".", "NUM_GPUS", "\n", "num_machines", "=", "dist", ".", "get_world_size", "(", ")", "//", "num_gpus_per_machine", "\n", "for", "i", "in", "range", "(", "num_machines", ")", ":", "\n", "        ", "ranks_on_i", "=", "list", "(", "\n", "range", "(", "i", "*", "num_gpus_per_machine", ",", "(", "i", "+", "1", ")", "*", "num_gpus_per_machine", ")", "\n", ")", "\n", "pg", "=", "dist", ".", "new_group", "(", "ranks_on_i", ")", "\n", "if", "i", "==", "cfg", ".", "SHARD_ID", ":", "\n", "            ", "global", "_LOCAL_PROCESS_GROUP", "\n", "_LOCAL_PROCESS_GROUP", "=", "pg", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_main_proc_rank": [[275, 277], ["None"], "function", ["None"], ["", "", "", "def", "get_main_proc_rank", "(", ")", "->", "int", ":", "\n", "    ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size": [[278, 289], ["torch.get_world_size", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_world_size"], ["", "def", "get_local_size", "(", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", "group", "=", "_LOCAL_PROCESS_GROUP", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_rank": [[291, 302], ["torch.get_rank", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_rank"], ["", "def", "get_local_rank", "(", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "assert", "_LOCAL_PROCESS_GROUP", "is", "not", "None", "\n", "return", "dist", ".", "get_rank", "(", "group", "=", "_LOCAL_PROCESS_GROUP", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct": [[9, 43], ["torch.topk", "top_max_k_inds.t.t", "labels.view().expand_as", "top_max_k_inds.t.eq", "preds.size", "labels.size", "max", "top_max_k_correct[].view().float().sum", "labels.view", "top_max_k_correct[].view().float", "top_max_k_correct[].view"], "function", ["None"], ["def", "topks_correct", "(", "preds", ",", "labels", ",", "ks", ")", ":", "\n", "    ", "\"\"\"\n    Given the predictions, labels, and a list of top-k values, compute the\n    number of correct predictions for each top-k value.\n\n    Args:\n        preds (array): array of predictions. Dimension is batchsize\n            N x ClassNum.\n        labels (array): array of labels. Dimension is batchsize N.\n        ks (list): list of top-k values. For example, ks = [1, 5] correspods\n            to top-1 and top-5.\n\n    Returns:\n        topks_correct (list): list of numbers, where the `i`-th entry\n            corresponds to the number of top-`ks[i]` correct predictions.\n    \"\"\"", "\n", "assert", "preds", ".", "size", "(", "0", ")", "==", "labels", ".", "size", "(", "\n", "0", "\n", ")", ",", "\"Batch dim of predictions and labels must match\"", "\n", "# Find the top max_k predictions for each sample", "\n", "_top_max_k_vals", ",", "top_max_k_inds", "=", "torch", ".", "topk", "(", "\n", "preds", ",", "max", "(", "ks", ")", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", "\n", ")", "\n", "# (batch_size, max_k) -> (max_k, batch_size).", "\n", "top_max_k_inds", "=", "top_max_k_inds", ".", "t", "(", ")", "\n", "# (batch_size, ) -> (max_k, batch_size).", "\n", "rep_max_k_labels", "=", "labels", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "top_max_k_inds", ")", "\n", "# (i, j) = 1 if top i-th prediction for the j-th sample is correct.", "\n", "top_max_k_correct", "=", "top_max_k_inds", ".", "eq", "(", "rep_max_k_labels", ")", "\n", "# Compute the number of topk correct predictions for each k.", "\n", "topks_correct", "=", "[", "\n", "top_max_k_correct", "[", ":", "k", ",", ":", "]", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "for", "k", "in", "ks", "\n", "]", "\n", "return", "topks_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topk_errors": [[45, 55], ["metrics.topks_correct", "preds.size"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct"], ["", "def", "topk_errors", "(", "preds", ",", "labels", ",", "ks", ")", ":", "\n", "    ", "\"\"\"\n    Computes the top-k error for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"", "\n", "num_topks_correct", "=", "topks_correct", "(", "preds", ",", "labels", ",", "ks", ")", "\n", "return", "[", "(", "1.0", "-", "x", "/", "preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "for", "x", "in", "num_topks_correct", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topk_accuracies": [[57, 67], ["metrics.topks_correct", "preds.size"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct"], ["", "def", "topk_accuracies", "(", "preds", ",", "labels", ",", "ks", ")", ":", "\n", "    ", "\"\"\"\n    Computes the top-k accuracy for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"", "\n", "num_topks_correct", "=", "topks_correct", "(", "preds", ",", "labels", ",", "ks", ")", "\n", "return", "[", "(", "x", "/", "preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "for", "x", "in", "num_topks_correct", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_lr_at_epoch": [[9, 28], ["lr_policy.get_lr_func", "lr_policy.get_lr_func"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_lr_func", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_lr_func"], ["def", "get_lr_at_epoch", "(", "cfg", ",", "cur_epoch", ")", ":", "\n", "    ", "\"\"\"\n    Retrieve the learning rate of the current epoch with the option to perform\n    warm up in the beginning of the training stage.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"", "\n", "lr", "=", "get_lr_func", "(", "cfg", ".", "SOLVER", ".", "LR_POLICY", ")", "(", "cfg", ",", "cur_epoch", ")", "\n", "# Perform warm up.", "\n", "if", "cur_epoch", "<", "cfg", ".", "SOLVER", ".", "WARMUP_EPOCHS", ":", "\n", "        ", "lr_start", "=", "cfg", ".", "SOLVER", ".", "WARMUP_START_LR", "\n", "lr_end", "=", "get_lr_func", "(", "cfg", ".", "SOLVER", ".", "LR_POLICY", ")", "(", "\n", "cfg", ",", "cfg", ".", "SOLVER", ".", "WARMUP_EPOCHS", "\n", ")", "\n", "alpha", "=", "(", "lr_end", "-", "lr_start", ")", "/", "cfg", ".", "SOLVER", ".", "WARMUP_EPOCHS", "\n", "lr", "=", "cur_epoch", "*", "alpha", "+", "lr_start", "\n", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.lr_func_cosine": [[30, 45], ["math.cos"], "function", ["None"], ["", "def", "lr_func_cosine", "(", "cfg", ",", "cur_epoch", ")", ":", "\n", "    ", "\"\"\"\n    Retrieve the learning rate to specified values at specified epoch with the\n    cosine learning rate schedule. Details can be found in:\n    Ilya Loshchilov, and  Frank Hutter\n    SGDR: Stochastic Gradient Descent With Warm Restarts.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"", "\n", "return", "(", "\n", "cfg", ".", "SOLVER", ".", "BASE_LR", "\n", "*", "(", "math", ".", "cos", "(", "math", ".", "pi", "*", "cur_epoch", "/", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", ")", "+", "1.0", ")", "\n", "*", "0.5", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.lr_func_steps_with_relative_lrs": [[48, 59], ["lr_policy.get_step_index"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_step_index"], ["", "def", "lr_func_steps_with_relative_lrs", "(", "cfg", ",", "cur_epoch", ")", ":", "\n", "    ", "\"\"\"\n    Retrieve the learning rate to specified values at specified epoch with the\n    steps with relative learning rate schedule.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"", "\n", "ind", "=", "get_step_index", "(", "cfg", ",", "cur_epoch", ")", "\n", "return", "cfg", ".", "SOLVER", ".", "LRS", "[", "ind", "]", "*", "cfg", ".", "SOLVER", ".", "BASE_LR", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_step_index": [[61, 74], ["enumerate"], "function", ["None"], ["", "def", "get_step_index", "(", "cfg", ",", "cur_epoch", ")", ":", "\n", "    ", "\"\"\"\n    Retrieves the lr step index for the given epoch.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"", "\n", "steps", "=", "cfg", ".", "SOLVER", ".", "STEPS", "+", "[", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "]", "\n", "for", "ind", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "# NoQA", "\n", "        ", "if", "cur_epoch", "<", "step", ":", "\n", "            ", "break", "\n", "", "", "return", "ind", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_lr_func": [[76, 87], ["globals", "NotImplementedError", "globals"], "function", ["None"], ["", "def", "get_lr_func", "(", "lr_policy", ")", ":", "\n", "    ", "\"\"\"\n    Given the configs, retrieve the specified lr policy function.\n    Args:\n        lr_policy (string): the learning rate policy to use for the job.\n    \"\"\"", "\n", "policy", "=", "\"lr_func_\"", "+", "lr_policy", "\n", "if", "policy", "not", "in", "globals", "(", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Unknown LR policy: {}\"", ".", "format", "(", "lr_policy", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "globals", "(", ")", "[", "policy", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.parse_args": [[13, 71], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "argparse.ArgumentParser.print_help"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "\"\"\"\n    Parse the following arguments for a default parser for PySlowFast users.\n    Args:\n        shard_id (int): shard id for the current machine. Starts from 0 to\n            num_shards - 1. If single machine is used, then set shard id to 0.\n        num_shards (int): number of shards using by the job.\n        init_method (str): initialization method to launch the job with multiple\n            devices. Options includes TCP or shared file-system for\n            initialization. details can be find in\n            https://pytorch.org/docs/stable/distributed.html#tcp-initialization\n        cfg (str): path to the config file.\n        opts (argument): provide addtional options from the command line, it\n            overwrites the config loaded from file.\n        \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Provide SlowFast video training and testing pipeline.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--shard_id\"", ",", "\n", "help", "=", "\"The shard id of current node, Starts from 0 to num_shards - 1\"", ",", "\n", "default", "=", "0", ",", "\n", "type", "=", "int", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num_shards\"", ",", "\n", "help", "=", "\"Number of shards using by the job\"", ",", "\n", "default", "=", "1", ",", "\n", "type", "=", "int", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--init_method\"", ",", "\n", "help", "=", "\"Initialization method, includes TCP or shared file-system\"", ",", "\n", "default", "=", "\"tcp://localhost:9999\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--cfg\"", ",", "\n", "dest", "=", "\"cfg_file\"", ",", "\n", "help", "=", "\"Path to the config file\"", ",", "\n", "default", "=", "\"configs/Kinetics/SLOWFAST_4x16_R50.yaml\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--local_rank\"", ",", "\n", "help", "=", "\"Rank number of current device\"", ",", "\n", "default", "=", "-", "1", ",", "\n", "type", "=", "int", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"opts\"", ",", "\n", "help", "=", "\"See slowfast/config/defaults.py for all options\"", ",", "\n", "default", "=", "None", ",", "\n", "nargs", "=", "argparse", ".", "REMAINDER", ",", "\n", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "1", ":", "\n", "        ", "parser", ".", "print_help", "(", ")", "\n", "", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.parser.load_config": [[73, 101], ["slowfast.config.defaults.get_cfg", "hasattr", "hasattr", "slowfast.make_checkpoint_dir", "slowfast.config.defaults.get_cfg.merge_from_file", "slowfast.config.defaults.get_cfg.merge_from_list", "hasattr", "hasattr"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.config.defaults.get_cfg", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.make_checkpoint_dir"], ["", "def", "load_config", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Given the arguemnts, load and initialize the configs.\n    Args:\n        args (argument): arguments includes `shard_id`, `num_shards`,\n            `init_method`, `cfg_file`, and `opts`.\n    \"\"\"", "\n", "# Setup cfg.", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "# Load config from cfg.", "\n", "if", "args", ".", "cfg_file", "is", "not", "None", ":", "\n", "        ", "cfg", ".", "merge_from_file", "(", "args", ".", "cfg_file", ")", "\n", "# Load config from command line, overwrite config from opts.", "\n", "", "if", "args", ".", "opts", "is", "not", "None", ":", "\n", "        ", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "\n", "# Inherit parameters from args.", "\n", "", "if", "hasattr", "(", "args", ",", "\"num_shards\"", ")", "and", "hasattr", "(", "args", ",", "\"shard_id\"", ")", ":", "\n", "        ", "cfg", ".", "NUM_SHARDS", "=", "args", ".", "num_shards", "\n", "cfg", ".", "SHARD_ID", "=", "args", ".", "shard_id", "\n", "", "if", "hasattr", "(", "args", ",", "\"rng_seed\"", ")", ":", "\n", "        ", "cfg", ".", "RNG_SEED", "=", "args", ".", "rng_seed", "\n", "", "if", "hasattr", "(", "args", ",", "\"output_dir\"", ")", ":", "\n", "        ", "cfg", ".", "OUTPUT_DIR", "=", "args", ".", "output_dir", "\n", "\n", "# Create the checkpoint dir.", "\n", "", "cu", ".", "make_checkpoint_dir", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "return", "cfg", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.weight_init_helper.init_weights": [[10, 45], ["model.modules", "isinstance", "isinstance", "fvcore.nn.weight_init.c2_msra_fill", "isinstance", "m.weight.data.normal_", "m.bias.data.zero_", "hasattr", "m.weight.data.fill_", "m.bias.data.zero_"], "function", ["None"], ["def", "init_weights", "(", "model", ",", "fc_init_std", "=", "0.01", ",", "zero_init_final_bn", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Performs ResNet style weight initialization.\n    Args:\n        fc_init_std (float): the expected standard deviation for fc layer.\n        zero_init_final_bn (bool): if True, zero initialize the final bn for\n            every bottleneck.\n    \"\"\"", "\n", "for", "m", "in", "model", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "            ", "\"\"\"\n            Follow the initialization method proposed in:\n            {He, Kaiming, et al.\n            \"Delving deep into rectifiers: Surpassing human-level\n            performance on imagenet classification.\"\n            arXiv preprint arXiv:1502.01852 (2015)}\n            \"\"\"", "\n", "c2_msra_fill", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "            ", "if", "(", "\n", "hasattr", "(", "m", ",", "\"transform_final_bn\"", ")", "\n", "and", "m", ".", "transform_final_bn", "\n", "and", "zero_init_final_bn", "\n", ")", ":", "\n", "                ", "batchnorm_weight", "=", "0.0", "\n", "", "else", ":", "\n", "                ", "batchnorm_weight", "=", "1.0", "\n", "", "if", "m", ".", "weight", "is", "not", "None", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "batchnorm_weight", ")", "\n", "", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "fc_init_std", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.c2_model_loading.get_name_convert_func": [[9, 113], ["re.sub"], "function", ["None"], ["def", "get_name_convert_func", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get the function to convert Caffe2 layer names to PyTorch layer names.\n    Returns:\n        (func): function to convert parameter name from Caffe2 format to PyTorch\n        format.\n    \"\"\"", "\n", "pairs", "=", "[", "\n", "# ------------------------------------------------------------", "\n", "# 'nonlocal_conv3_1_theta_w' -> 's3.pathway0_nonlocal3.conv_g.weight'", "\n", "[", "\n", "r\"^nonlocal_conv([0-9]+)_([0-9]+)_(.*)\"", ",", "\n", "r\"s\\1.pathway0_nonlocal\\2_\\3\"", ",", "\n", "]", ",", "\n", "# 'theta' -> 'conv_theta'", "\n", "[", "r\"^(.*)_nonlocal([0-9]+)_(theta)(.*)\"", ",", "r\"\\1_nonlocal\\2.conv_\\3\\4\"", "]", ",", "\n", "# 'g' -> 'conv_g'", "\n", "[", "r\"^(.*)_nonlocal([0-9]+)_(g)(.*)\"", ",", "r\"\\1_nonlocal\\2.conv_\\3\\4\"", "]", ",", "\n", "# 'phi' -> 'conv_phi'", "\n", "[", "r\"^(.*)_nonlocal([0-9]+)_(phi)(.*)\"", ",", "r\"\\1_nonlocal\\2.conv_\\3\\4\"", "]", ",", "\n", "# 'out' -> 'conv_out'", "\n", "[", "r\"^(.*)_nonlocal([0-9]+)_(out)(.*)\"", ",", "r\"\\1_nonlocal\\2.conv_\\3\\4\"", "]", ",", "\n", "# 'nonlocal_conv4_5_bn_s' -> 's4.pathway0_nonlocal3.bn.weight'", "\n", "[", "r\"^(.*)_nonlocal([0-9]+)_(bn)_(.*)\"", ",", "r\"\\1_nonlocal\\2.\\3.\\4\"", "]", ",", "\n", "# ------------------------------------------------------------", "\n", "# 't_pool1_subsample_bn' -> 's1_fuse.conv_f2s.bn.running_mean'", "\n", "[", "r\"^t_pool1_subsample_bn_(.*)\"", ",", "r\"s1_fuse.bn.\\1\"", "]", ",", "\n", "# 't_pool1_subsample' -> 's1_fuse.conv_f2s'", "\n", "[", "r\"^t_pool1_subsample_(.*)\"", ",", "r\"s1_fuse.conv_f2s.\\1\"", "]", ",", "\n", "# 't_res4_5_branch2c_bn_subsample_bn_rm' -> 's4_fuse.conv_f2s.bias'", "\n", "[", "\n", "r\"^t_res([0-9]+)_([0-9]+)_branch2c_bn_subsample_bn_(.*)\"", ",", "\n", "r\"s\\1_fuse.bn.\\3\"", ",", "\n", "]", ",", "\n", "# 't_pool1_subsample' -> 's1_fuse.conv_f2s'", "\n", "[", "\n", "r\"^t_res([0-9]+)_([0-9]+)_branch2c_bn_subsample_(.*)\"", ",", "\n", "r\"s\\1_fuse.conv_f2s.\\3\"", ",", "\n", "]", ",", "\n", "# ------------------------------------------------------------", "\n", "# 'res4_4_branch_2c_bn_b' -> 's4.pathway0_res4.branch2.c_bn_b'", "\n", "[", "\n", "r\"^res([0-9]+)_([0-9]+)_branch([0-9]+)([a-z])_(.*)\"", ",", "\n", "r\"s\\1.pathway0_res\\2.branch\\3.\\4_\\5\"", ",", "\n", "]", ",", "\n", "# 'res_conv1_bn_' -> 's1.pathway0_stem.bn.'", "\n", "[", "r\"^res_conv1_bn_(.*)\"", ",", "r\"s1.pathway0_stem.bn.\\1\"", "]", ",", "\n", "# 'conv1_w_momentum' -> 's1.pathway0_stem.conv.'", "\n", "[", "r\"^conv1_(.*)\"", ",", "r\"s1.pathway0_stem.conv.\\1\"", "]", ",", "\n", "# 'res4_0_branch1_w' -> 'S4.pathway0_res0.branch1.weight'", "\n", "[", "\n", "r\"^res([0-9]+)_([0-9]+)_branch([0-9]+)_(.*)\"", ",", "\n", "r\"s\\1.pathway0_res\\2.branch\\3_\\4\"", ",", "\n", "]", ",", "\n", "# 'res_conv1_' -> 's1.pathway0_stem.conv.'", "\n", "[", "r\"^res_conv1_(.*)\"", ",", "r\"s1.pathway0_stem.conv.\\1\"", "]", ",", "\n", "# ------------------------------------------------------------", "\n", "# 'res4_4_branch_2c_bn_b' -> 's4.pathway0_res4.branch2.c_bn_b'", "\n", "[", "\n", "r\"^t_res([0-9]+)_([0-9]+)_branch([0-9]+)([a-z])_(.*)\"", ",", "\n", "r\"s\\1.pathway1_res\\2.branch\\3.\\4_\\5\"", ",", "\n", "]", ",", "\n", "# 'res_conv1_bn_' -> 's1.pathway0_stem.bn.'", "\n", "[", "r\"^t_res_conv1_bn_(.*)\"", ",", "r\"s1.pathway1_stem.bn.\\1\"", "]", ",", "\n", "# 'conv1_w_momentum' -> 's1.pathway0_stem.conv.'", "\n", "[", "r\"^t_conv1_(.*)\"", ",", "r\"s1.pathway1_stem.conv.\\1\"", "]", ",", "\n", "# 'res4_0_branch1_w' -> 'S4.pathway0_res0.branch1.weight'", "\n", "[", "\n", "r\"^t_res([0-9]+)_([0-9]+)_branch([0-9]+)_(.*)\"", ",", "\n", "r\"s\\1.pathway1_res\\2.branch\\3_\\4\"", ",", "\n", "]", ",", "\n", "# 'res_conv1_' -> 's1.pathway0_stem.conv.'", "\n", "[", "r\"^t_res_conv1_(.*)\"", ",", "r\"s1.pathway1_stem.conv.\\1\"", "]", ",", "\n", "# ------------------------------------------------------------", "\n", "# pred_ -> head.projection.", "\n", "[", "r\"pred_(.*)\"", ",", "r\"head.projection.\\1\"", "]", ",", "\n", "# '.bn_b' -> '.weight'", "\n", "[", "r\"(.*)bn.b\\Z\"", ",", "r\"\\1bn.bias\"", "]", ",", "\n", "# '.bn_s' -> '.weight'", "\n", "[", "r\"(.*)bn.s\\Z\"", ",", "r\"\\1bn.weight\"", "]", ",", "\n", "# '_bn_rm' -> '.running_mean'", "\n", "[", "r\"(.*)bn.rm\\Z\"", ",", "r\"\\1bn.running_mean\"", "]", ",", "\n", "# '_bn_riv' -> '.running_var'", "\n", "[", "r\"(.*)bn.riv\\Z\"", ",", "r\"\\1bn.running_var\"", "]", ",", "\n", "# '_b' -> '.bias'", "\n", "[", "r\"(.*)[\\._]b\\Z\"", ",", "r\"\\1.bias\"", "]", ",", "\n", "# '_w' -> '.weight'", "\n", "[", "r\"(.*)[\\._]w\\Z\"", ",", "r\"\\1.weight\"", "]", ",", "\n", "]", "\n", "\n", "def", "convert_caffe2_name_to_pytorch", "(", "caffe2_layer_name", ")", ":", "\n", "        ", "\"\"\"\n        Convert the caffe2_layer_name to pytorch format by apply the list of\n        regular expressions.\n        Args:\n            caffe2_layer_name (str): caffe2 layer name.\n        Returns:\n            (str): pytorch layer name.\n        \"\"\"", "\n", "for", "source", ",", "dest", "in", "pairs", ":", "\n", "            ", "caffe2_layer_name", "=", "re", ".", "sub", "(", "source", ",", "dest", ",", "caffe2_layer_name", ")", "\n", "", "return", "caffe2_layer_name", "\n", "\n", "", "return", "convert_caffe2_name_to_pytorch", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.benchmark.benchmark_data_loading": [[20, 102], ["slowfast.utils.env.setup_environment", "numpy.random.seed", "torch.manual_seed", "slowfast.setup_logging", "logger.info", "logger.info", "fvcore.common.timer.Timer", "slowfast.datasets.loader.construct_loader", "logger.info", "range", "logger.info", "pprint.pformat", "fvcore.common.timer.Timer", "fvcore.common.timer.Timer", "enumerate", "epoch_times.append", "slowfast.cpu_mem_usage", "logger.info", "logger.info", "fvcore.common.timer.Timer.seconds", "slowfast.datasets.loader.shuffle_dataset", "tqdm.tqdm", "fvcore.common.timer.Timer.seconds", "numpy.mean", "numpy.std", "iter_times.append", "slowfast.cpu_mem_usage", "logger.info", "fvcore.common.timer.Timer.reset", "len", "numpy.mean", "numpy.std", "len", "fvcore.common.timer.Timer.seconds", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.env.setup_environment", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.shuffle_dataset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["cfg", "=", "cfg", ",", "init_method", "=", "args", ".", "init_method", ",", "func", "=", "benchmark_data_loading", "\n", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.__init__": [[52, 69], ["meters.ScalarMeter", "fvcore.common.timer.Timer"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "overall_iters", ",", "cfg", ",", "mode", ")", ":", "\n", "        ", "\"\"\"\n            overall_iters (int): the overall number of iterations of one epoch.\n            cfg (CfgNode): configs.\n            mode (str): `train`, `val`, or `test` mode.\n        \"\"\"", "\n", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "name", "=", "f\"{cfg.DATA.TEST_CROP_SIZE}\"", "\n", "\n", "self", ".", "lr", "=", "None", "\n", "self", ".", "loss", "=", "ScalarMeter", "(", "cfg", ".", "LOG_PERIOD", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "iter_timer", "=", "Timer", "(", ")", "\n", "self", ".", "all_preds", "=", "[", "]", "\n", "self", ".", "all_ori_boxes", "=", "[", "]", "\n", "self", ".", "all_metadata", "=", "[", "]", "\n", "self", ".", "overall_iters", "=", "overall_iters", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.log_iter_stats": [[70, 115], ["str", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "meters.BaseMeter.iter_timer.seconds", "datetime.timedelta", "meters.BaseMeter.iter_timer.seconds", "meters.BaseMeter.loss.get_win_median", "int", "meters.BaseMeter.iter_timer.seconds", "NotImplementedError", "meters.BaseMeter.iter_timer.seconds"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median"], ["", "def", "log_iter_stats", "(", "self", ",", "cur_epoch", ",", "cur_iter", ")", ":", "\n", "        ", "\"\"\"\n        Log the stats.\n        Args:\n            cur_epoch (int): the current epoch.\n            cur_iter (int): the current iteration.\n        \"\"\"", "\n", "\n", "if", "(", "cur_iter", "+", "1", ")", "%", "self", ".", "cfg", ".", "LOG_PERIOD", "!=", "0", ":", "\n", "            ", "return", "\n", "\n", "", "eta_sec", "=", "self", ".", "iter_timer", ".", "seconds", "(", ")", "*", "(", "self", ".", "overall_iters", "-", "cur_iter", ")", "\n", "eta", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_sec", ")", ")", ")", "\n", "if", "self", ".", "mode", "==", "\"train\"", ":", "\n", "            ", "stats", "=", "{", "\n", "\"_type\"", ":", "\"{}_iter\"", ".", "format", "(", "self", ".", "mode", ")", ",", "\n", "\"cur_epoch\"", ":", "\"{}\"", ".", "format", "(", "cur_epoch", "+", "1", ")", ",", "\n", "\"cur_iter\"", ":", "\"{}\"", ".", "format", "(", "cur_iter", "+", "1", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"mode\"", ":", "self", ".", "mode", ",", "\n", "\"loss\"", ":", "self", ".", "loss", ".", "get_win_median", "(", ")", ",", "\n", "\"lr\"", ":", "self", ".", "lr", ",", "\n", "}", "\n", "", "elif", "self", ".", "mode", "==", "\"val\"", ":", "\n", "            ", "stats", "=", "{", "\n", "\"_type\"", ":", "\"{}_iter\"", ".", "format", "(", "self", ".", "mode", ")", ",", "\n", "\"cur_epoch\"", ":", "\"{}\"", ".", "format", "(", "cur_epoch", "+", "1", ")", ",", "\n", "\"cur_iter\"", ":", "\"{}\"", ".", "format", "(", "cur_iter", "+", "1", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"mode\"", ":", "self", ".", "mode", ",", "\n", "}", "\n", "", "elif", "self", ".", "mode", "==", "\"test\"", ":", "\n", "            ", "stats", "=", "{", "\n", "\"_type\"", ":", "\"{}_iter\"", ".", "format", "(", "self", ".", "mode", ")", ",", "\n", "\"cur_iter\"", ":", "\"{}\"", ".", "format", "(", "cur_iter", "+", "1", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"mode\"", ":", "self", ".", "mode", ",", "\n", "}", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Unknown mode: {}\"", ".", "format", "(", "self", ".", "mode", ")", ")", "\n", "\n", "", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.iter_tic": [[116, 121], ["meters.BaseMeter.iter_timer.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "iter_tic", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Start to record time.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.iter_toc": [[122, 127], ["meters.BaseMeter.iter_timer.pause"], "methods", ["None"], ["", "def", "iter_toc", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Stop to record time.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "pause", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.reset": [[128, 137], ["meters.BaseMeter.loss.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset the Meter.\n        \"\"\"", "\n", "self", ".", "loss", ".", "reset", "(", ")", "\n", "\n", "self", ".", "all_preds", "=", "[", "]", "\n", "self", ".", "all_ori_boxes", "=", "[", "]", "\n", "self", ".", "all_metadata", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.update_stats": [[138, 156], ["meters.BaseMeter.all_preds.append", "meters.BaseMeter.all_ori_boxes.append", "meters.BaseMeter.all_metadata.append", "meters.BaseMeter.loss.add_value"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value"], ["", "def", "update_stats", "(", "self", ",", "preds", ",", "ori_boxes", ",", "metadata", ",", "loss", "=", "None", ",", "lr", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Update the current stats.\n        Args:\n            preds (tensor): prediction embedding.\n            ori_boxes (tensor): original boxes (x1, y1, x2, y2).\n            metadata (tensor): metadata of the AVA data.\n            loss (float): loss value.\n            lr (float): learning rate.\n        \"\"\"", "\n", "if", "self", ".", "mode", "in", "[", "\"val\"", ",", "\"test\"", "]", ":", "\n", "            ", "self", ".", "all_preds", ".", "append", "(", "preds", ")", "\n", "self", ".", "all_ori_boxes", ".", "append", "(", "ori_boxes", ")", "\n", "self", ".", "all_metadata", ".", "append", "(", "metadata", ")", "\n", "", "if", "loss", "is", "not", "None", ":", "\n", "            ", "self", ".", "loss", ".", "add_value", "(", "loss", ")", "\n", "", "if", "lr", "is", "not", "None", ":", "\n", "            ", "self", ".", "lr", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.finalize_metrics": [[157, 159], ["None"], "methods", ["None"], ["", "", "def", "finalize_metrics", "(", "self", ",", "log", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.BaseMeter.log_epoch_stats": [[160, 177], ["meters.BaseMeter.finalize_metrics", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.finalize_metrics", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage"], ["", "def", "log_epoch_stats", "(", "self", ",", "cur_epoch", ")", ":", "\n", "        ", "\"\"\"\n        Log the stats of the current epoch.\n        Args:\n            cur_epoch (int): the number of current epoch.\n        \"\"\"", "\n", "if", "self", ".", "mode", "in", "[", "\"val\"", ",", "\"test\"", "]", ":", "\n", "            ", "self", ".", "finalize_metrics", "(", "log", "=", "False", ")", "\n", "stats", "=", "{", "\n", "\"_type\"", ":", "\"{}_epoch\"", ".", "format", "(", "self", ".", "mode", ")", ",", "\n", "\"cur_epoch\"", ":", "\"{}\"", ".", "format", "(", "cur_epoch", "+", "1", ")", ",", "\n", "\"mode\"", ":", "self", ".", "mode", ",", "\n", "\"map\"", ":", "self", ".", "full_map", ",", "\n", "\"gpu_mem\"", ":", "\"{:.2f} GB\"", ".", "format", "(", "misc", ".", "gpu_mem_usage", "(", ")", ")", ",", "\n", "\"RAM\"", ":", "\"{:.2f}/{:.2f} GB\"", ".", "format", "(", "*", "misc", ".", "cpu_mem_usage", "(", ")", ")", ",", "\n", "}", "\n", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.AVAMeter.__init__": [[183, 208], ["meters.BaseMeter.__init__", "slowfast.utils.ava_eval_helper.read_exclusions", "slowfast.utils.ava_eval_helper.read_exclusions", "slowfast.utils.ava_eval_helper.read_exclusions", "slowfast.utils.ava_eval_helper.read_exclusions", "slowfast.utils.ava_eval_helper.read_labelmap", "slowfast.utils.ava_eval_helper.read_labelmap", "slowfast.utils.ava_eval_helper.read_labelmap", "slowfast.utils.ava_eval_helper.read_labelmap", "os.path.join", "slowfast.utils.ava_eval_helper.read_csv", "slowfast.utils.ava_eval_helper.read_csv", "slowfast.utils.ava_eval_helper.read_csv", "slowfast.utils.ava_eval_helper.read_csv", "meters.get_ava_mini_groundtruth", "slowfast.load_image_lists", "slowfast.load_image_lists", "slowfast.load_image_lists", "slowfast.load_image_lists", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_exclusions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_exclusions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_exclusions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_exclusions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_labelmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_labelmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_labelmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_labelmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.read_csv", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.get_ava_mini_groundtruth", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists"], ["def", "__init__", "(", "self", ",", "overall_iters", ",", "cfg", ",", "mode", ")", ":", "\n", "        ", "\"\"\"\n            overall_iters (int): the overall number of iterations of one epoch.\n            cfg (CfgNode): configs.\n            mode (str): `train`, `val`, or `test` mode.\n        \"\"\"", "\n", "super", "(", "AVAMeter", ",", "self", ")", ".", "__init__", "(", "overall_iters", ",", "cfg", ",", "mode", ")", "\n", "if", "cfg", ".", "AVA", ".", "TEST_FORCE_FLIP", ":", "\n", "            ", "self", ".", "name", "+=", "f\"_flip\"", "\n", "\n", "", "self", ".", "full_ava_test", "=", "cfg", ".", "AVA", ".", "FULL_TEST_ON_VAL", "\n", "self", ".", "excluded_keys", "=", "read_exclusions", "(", "\n", "os", ".", "path", ".", "join", "(", "cfg", ".", "AVA", ".", "ANNOTATION_DIR", ",", "cfg", ".", "AVA", ".", "EXCLUSION_FILE", ")", "\n", ")", "\n", "self", ".", "categories", ",", "self", ".", "class_whitelist", "=", "read_labelmap", "(", "\n", "os", ".", "path", ".", "join", "(", "cfg", ".", "AVA", ".", "ANNOTATION_DIR", ",", "cfg", ".", "AVA", ".", "LABEL_MAP_FILE", ")", "\n", ")", "\n", "gt_filename", "=", "os", ".", "path", ".", "join", "(", "\n", "cfg", ".", "AVA", ".", "ANNOTATION_DIR", ",", "cfg", ".", "AVA", ".", "GROUNDTRUTH_FILE", "\n", ")", "\n", "self", ".", "full_groundtruth", "=", "read_csv", "(", "gt_filename", ",", "self", ".", "class_whitelist", ")", "\n", "self", ".", "mini_groundtruth", "=", "get_ava_mini_groundtruth", "(", "self", ".", "full_groundtruth", ")", "\n", "\n", "_", ",", "self", ".", "video_idx_to_name", "=", "ava_helper", ".", "load_image_lists", "(", "\n", "cfg", ",", "mode", "==", "\"train\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.AVAMeter.finalize_metrics": [[210, 237], ["torch.cat", "torch.cat", "torch.cat", "slowfast.utils.ava_eval_helper.evaluate_ava", "slowfast.utils.ava_eval_helper.evaluate_ava", "slowfast.utils.ava_eval_helper.evaluate_ava", "slowfast.utils.ava_eval_helper.evaluate_ava", "torch.cat.tolist", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_ava", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_ava", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_ava", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_ava", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats"], ["", "def", "finalize_metrics", "(", "self", ",", "log", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and log the final AVA metrics.\n        \"\"\"", "\n", "all_preds", "=", "torch", ".", "cat", "(", "self", ".", "all_preds", ",", "dim", "=", "0", ")", "\n", "all_ori_boxes", "=", "torch", ".", "cat", "(", "self", ".", "all_ori_boxes", ",", "dim", "=", "0", ")", "\n", "all_metadata", "=", "torch", ".", "cat", "(", "self", ".", "all_metadata", ",", "dim", "=", "0", ")", "\n", "\n", "if", "self", ".", "mode", "==", "\"test\"", "or", "(", "self", ".", "full_ava_test", "and", "self", ".", "mode", "==", "\"val\"", ")", ":", "\n", "            ", "groundtruth", "=", "self", ".", "full_groundtruth", "\n", "", "else", ":", "\n", "            ", "groundtruth", "=", "self", ".", "mini_groundtruth", "\n", "\n", "", "self", ".", "full_map", "=", "evaluate_ava", "(", "\n", "all_preds", ",", "\n", "all_ori_boxes", ",", "\n", "all_metadata", ".", "tolist", "(", ")", ",", "\n", "self", ".", "excluded_keys", ",", "\n", "self", ".", "class_whitelist", ",", "\n", "self", ".", "categories", ",", "\n", "groundtruth", "=", "groundtruth", ",", "\n", "video_idx_to_name", "=", "self", ".", "video_idx_to_name", ",", "\n", "name", "=", "self", ".", "name", "\n", ")", "\n", "if", "log", ":", "\n", "            ", "stats", "=", "{", "\"mode\"", ":", "self", ".", "mode", ",", "\"map\"", ":", "self", ".", "full_map", "}", "\n", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.HieveMeter.__init__": [[244, 255], ["meters.AVAMeter.__init__", "range", "slowfast.load_image_lists", "slowfast.load_image_lists", "slowfast.load_image_lists", "slowfast.load_image_lists"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists"], ["def", "__init__", "(", "self", ",", "overall_iters", ",", "cfg", ",", "mode", ")", ":", "\n", "        ", "\"\"\"\n            overall_iters (int): the overall number of iterations of one epoch.\n            cfg (CfgNode): configs.\n            mode (str): `train`, `val`, or `test` mode.\n        \"\"\"", "\n", "super", "(", "HieveMeter", ",", "self", ")", ".", "__init__", "(", "overall_iters", ",", "cfg", ",", "mode", ")", "\n", "self", ".", "class_whitelist", "=", "range", "(", "1", ",", "15", ")", "\n", "\n", "_", ",", "self", ".", "video_idx_to_name", "=", "ava_helper", ".", "load_image_lists", "(", "\n", "cfg", ",", "mode", "==", "\"train\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.HieveMeter.finalize_metrics": [[257, 272], ["torch.cat", "torch.cat", "torch.cat", "slowfast.utils.ava_eval_helper.evaluate_hieve", "slowfast.utils.ava_eval_helper.evaluate_hieve", "slowfast.utils.ava_eval_helper.evaluate_hieve", "slowfast.utils.ava_eval_helper.evaluate_hieve", "torch.cat.tolist"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_hieve", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_hieve", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_hieve", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.ava_eval_helper.evaluate_hieve"], ["", "def", "finalize_metrics", "(", "self", ",", "log", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and log the final metrics.\n        \"\"\"", "\n", "all_preds", "=", "torch", ".", "cat", "(", "self", ".", "all_preds", ",", "dim", "=", "0", ")", "\n", "all_ori_boxes", "=", "torch", ".", "cat", "(", "self", ".", "all_ori_boxes", ",", "dim", "=", "0", ")", "\n", "all_metadata", "=", "torch", ".", "cat", "(", "self", ".", "all_metadata", ",", "dim", "=", "0", ")", "\n", "\n", "self", ".", "full_map", "=", "evaluate_hieve", "(", "\n", "all_preds", ",", "\n", "all_ori_boxes", ",", "\n", "all_metadata", ".", "tolist", "(", ")", ",", "\n", "self", ".", "class_whitelist", ",", "\n", "video_idx_to_name", "=", "self", ".", "video_idx_to_name", ",", "\n", "output_dir", "=", "self", ".", "cfg", ".", "OUTPUT_DIR", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.__init__": [[282, 324], ["fvcore.common.timer.Timer", "torch.zeros", "torch.zeros().long", "meters.TestMeter.reset", "torch.zeros", "torch.zeros().long", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_videos", ",", "\n", "num_clips", ",", "\n", "num_cls", ",", "\n", "overall_iters", ",", "\n", "multi_label", "=", "False", ",", "\n", "ensemble_method", "=", "\"sum\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Construct tensors to store the predictions and labels. Expect to get\n        num_clips predictions from each video, and calculate the metrics on\n        num_videos videos.\n        Args:\n            num_videos (int): number of videos to test.\n            num_clips (int): number of clips sampled from each video for\n                aggregating the final prediction for the video.\n            num_cls (int): number of classes for each prediction.\n            overall_iters (int): overall iterations for testing.\n            multi_label (bool): if True, use map as the metric.\n            ensemble_method (str): method to perform the ensemble, options\n                include \"sum\", and \"max\".\n        \"\"\"", "\n", "\n", "self", ".", "iter_timer", "=", "Timer", "(", ")", "\n", "self", ".", "num_clips", "=", "num_clips", "\n", "self", ".", "overall_iters", "=", "overall_iters", "\n", "self", ".", "multi_label", "=", "multi_label", "\n", "self", ".", "ensemble_method", "=", "ensemble_method", "\n", "# Initialize tensors.", "\n", "self", ".", "video_preds", "=", "torch", ".", "zeros", "(", "(", "num_videos", ",", "num_cls", ")", ")", "\n", "if", "multi_label", ":", "\n", "            ", "self", ".", "video_preds", "-=", "1e10", "\n", "\n", "", "self", ".", "video_labels", "=", "(", "\n", "torch", ".", "zeros", "(", "(", "num_videos", ",", "num_cls", ")", ")", "\n", "if", "multi_label", "\n", "else", "torch", ".", "zeros", "(", "(", "num_videos", ")", ")", ".", "long", "(", ")", "\n", ")", "\n", "self", ".", "clip_count", "=", "torch", ".", "zeros", "(", "(", "num_videos", ")", ")", ".", "long", "(", ")", "\n", "# Reset metric.", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.reset": [[325, 334], ["meters.TestMeter.clip_count.zero_", "meters.TestMeter.video_preds.zero_", "meters.TestMeter.video_labels.zero_"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset the metric.\n        \"\"\"", "\n", "self", ".", "clip_count", ".", "zero_", "(", ")", "\n", "self", ".", "video_preds", ".", "zero_", "(", ")", "\n", "if", "self", ".", "multi_label", ":", "\n", "            ", "self", ".", "video_preds", "-=", "1e10", "\n", "", "self", ".", "video_labels", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.update_stats": [[335, 369], ["range", "int", "meters.TestMeter.video_labels[].sum", "torch.equal", "meters.TestMeter.video_labels[].type", "labels[].type", "torch.max", "NotImplementedError"], "methods", ["None"], ["", "def", "update_stats", "(", "self", ",", "preds", ",", "labels", ",", "clip_ids", ")", ":", "\n", "        ", "\"\"\"\n        Collect the predictions from the current batch and perform on-the-flight\n        summation as ensemble.\n        Args:\n            preds (tensor): predictions from the current batch. Dimension is\n                N x C where N is the batch size and C is the channel size\n                (num_cls).\n            labels (tensor): the corresponding labels of the current batch.\n                Dimension is N.\n            clip_ids (tensor): clip indexes of the current batch, dimension is\n                N.\n        \"\"\"", "\n", "for", "ind", "in", "range", "(", "preds", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "vid_id", "=", "int", "(", "clip_ids", "[", "ind", "]", ")", "//", "self", ".", "num_clips", "\n", "if", "self", ".", "video_labels", "[", "vid_id", "]", ".", "sum", "(", ")", ">", "0", ":", "\n", "                ", "assert", "torch", ".", "equal", "(", "\n", "self", ".", "video_labels", "[", "vid_id", "]", ".", "type", "(", "torch", ".", "FloatTensor", ")", ",", "\n", "labels", "[", "ind", "]", ".", "type", "(", "torch", ".", "FloatTensor", ")", ",", "\n", ")", "\n", "", "self", ".", "video_labels", "[", "vid_id", "]", "=", "labels", "[", "ind", "]", "\n", "if", "self", ".", "ensemble_method", "==", "\"sum\"", ":", "\n", "                ", "self", ".", "video_preds", "[", "vid_id", "]", "+=", "preds", "[", "ind", "]", "\n", "", "elif", "self", ".", "ensemble_method", "==", "\"max\"", ":", "\n", "                ", "self", ".", "video_preds", "[", "vid_id", "]", "=", "torch", ".", "max", "(", "\n", "self", ".", "video_preds", "[", "vid_id", "]", ",", "preds", "[", "ind", "]", "\n", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\n", "\"Ensemble Method {} is not supported\"", ".", "format", "(", "\n", "self", ".", "ensemble_method", "\n", ")", "\n", ")", "\n", "", "self", ".", "clip_count", "[", "vid_id", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.log_iter_stats": [[370, 385], ["str", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "meters.TestMeter.iter_timer.seconds", "datetime.timedelta", "meters.TestMeter.iter_timer.seconds", "int"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats"], ["", "", "def", "log_iter_stats", "(", "self", ",", "cur_iter", ")", ":", "\n", "        ", "\"\"\"\n        Log the stats.\n        Args:\n            cur_iter (int): the current iteration of testing.\n        \"\"\"", "\n", "eta_sec", "=", "self", ".", "iter_timer", ".", "seconds", "(", ")", "*", "(", "self", ".", "overall_iters", "-", "cur_iter", ")", "\n", "eta", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_sec", ")", ")", ")", "\n", "stats", "=", "{", "\n", "\"split\"", ":", "\"test_iter\"", ",", "\n", "\"cur_iter\"", ":", "\"{}\"", ".", "format", "(", "cur_iter", "+", "1", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "}", "\n", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.iter_tic": [[386, 388], ["meters.TestMeter.iter_timer.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "iter_tic", "(", "self", ")", ":", "\n", "        ", "self", ".", "iter_timer", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.iter_toc": [[389, 391], ["meters.TestMeter.iter_timer.pause"], "methods", ["None"], ["", "def", "iter_toc", "(", "self", ")", ":", "\n", "        ", "self", ".", "iter_timer", ".", "pause", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TestMeter.finalize_metrics": [[392, 431], ["slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "all", "logger.warning", "meters.get_map", "slowfast.topks_correct", "slowfast.topks_correct", "slowfast.topks_correct", "slowfast.topks_correct", "zip", "meters.TestMeter.video_preds.cpu().numpy", "meters.TestMeter.video_labels.cpu().numpy", "len", "meters.TestMeter.video_preds.cpu", "meters.TestMeter.video_labels.cpu", "meters.TestMeter.video_preds.size", "len", "len", "enumerate", "meters.TestMeter.clip_count.tolist"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.get_map", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.metrics.topks_correct"], ["", "def", "finalize_metrics", "(", "self", ",", "ks", "=", "(", "1", ",", "5", ")", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and log the final ensembled metrics.\n        ks (tuple): list of top-k values for topk_accuracies. For example,\n            ks = (1, 5) correspods to top-1 and top-5 accuracy.\n        \"\"\"", "\n", "if", "not", "all", "(", "self", ".", "clip_count", "==", "self", ".", "num_clips", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"clip count {} ~= num clips {}\"", ".", "format", "(", "\n", "\", \"", ".", "join", "(", "\n", "[", "\n", "\"{}: {}\"", ".", "format", "(", "i", ",", "k", ")", "\n", "for", "i", ",", "k", "in", "enumerate", "(", "self", ".", "clip_count", ".", "tolist", "(", ")", ")", "\n", "]", "\n", ")", ",", "\n", "self", ".", "num_clips", ",", "\n", ")", "\n", ")", "\n", "\n", "", "stats", "=", "{", "\"split\"", ":", "\"test_final\"", "}", "\n", "if", "self", ".", "multi_label", ":", "\n", "            ", "map", "=", "get_map", "(", "\n", "self", ".", "video_preds", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "self", ".", "video_labels", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "stats", "[", "\"map\"", "]", "=", "map", "\n", "", "else", ":", "\n", "            ", "num_topks_correct", "=", "metrics", ".", "topks_correct", "(", "\n", "self", ".", "video_preds", ",", "self", ".", "video_labels", ",", "ks", "\n", ")", "\n", "topks", "=", "[", "\n", "(", "x", "/", "self", ".", "video_preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "\n", "for", "x", "in", "num_topks_correct", "\n", "]", "\n", "assert", "len", "(", "{", "len", "(", "ks", ")", ",", "len", "(", "topks", ")", "}", ")", "==", "1", "\n", "for", "k", ",", "topk", "in", "zip", "(", "ks", ",", "topks", ")", ":", "\n", "                ", "stats", "[", "\"top{}_acc\"", ".", "format", "(", "k", ")", "]", "=", "\"{:.{prec}f}\"", ".", "format", "(", "\n", "topk", ",", "prec", "=", "2", "\n", ")", "\n", "", "", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.__init__": [[440, 448], ["collections.deque"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "window_size", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            window_size (int): size of the max length of the deque.\n        \"\"\"", "\n", "self", ".", "deque", "=", "deque", "(", "maxlen", "=", "window_size", ")", "\n", "self", ".", "total", "=", "0.0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.reset": [[449, 456], ["meters.ScalarMeter.deque.clear"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.clear"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset the deque.\n        \"\"\"", "\n", "self", ".", "deque", ".", "clear", "(", ")", "\n", "self", ".", "total", "=", "0.0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value": [[457, 464], ["meters.ScalarMeter.deque.append"], "methods", ["None"], ["", "def", "add_value", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"\n        Add a new scalar value to the deque.\n        \"\"\"", "\n", "self", ".", "deque", ".", "append", "(", "value", ")", "\n", "self", ".", "count", "+=", "1", "\n", "self", ".", "total", "+=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median": [[465, 470], ["numpy.median"], "methods", ["None"], ["", "def", "get_win_median", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate the current median value of the deque.\n        \"\"\"", "\n", "return", "np", ".", "median", "(", "self", ".", "deque", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_avg": [[471, 476], ["numpy.mean"], "methods", ["None"], ["", "def", "get_win_avg", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate the current average value of the deque.\n        \"\"\"", "\n", "return", "np", ".", "mean", "(", "self", ".", "deque", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_global_avg": [[477, 482], ["None"], "methods", ["None"], ["", "def", "get_global_avg", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate the global mean value.\n        \"\"\"", "\n", "return", "self", ".", "total", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.__init__": [[489, 509], ["fvcore.common.timer.Timer", "meters.ScalarMeter", "meters.ScalarMeter", "meters.ScalarMeter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "epoch_iters", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            epoch_iters (int): the overall number of iterations of one epoch.\n            cfg (CfgNode): configs.\n        \"\"\"", "\n", "self", ".", "_cfg", "=", "cfg", "\n", "self", ".", "epoch_iters", "=", "epoch_iters", "\n", "self", ".", "MAX_EPOCH", "=", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "*", "epoch_iters", "\n", "self", ".", "iter_timer", "=", "Timer", "(", ")", "\n", "self", ".", "loss", "=", "ScalarMeter", "(", "cfg", ".", "LOG_PERIOD", ")", "\n", "self", ".", "loss_total", "=", "0.0", "\n", "self", ".", "lr", "=", "None", "\n", "# Current minibatch errors (smoothed over a window).", "\n", "self", ".", "mb_top1_err", "=", "ScalarMeter", "(", "cfg", ".", "LOG_PERIOD", ")", "\n", "self", ".", "mb_top5_err", "=", "ScalarMeter", "(", "cfg", ".", "LOG_PERIOD", ")", "\n", "# Number of misclassified examples.", "\n", "self", ".", "num_top1_mis", "=", "0", "\n", "self", ".", "num_top5_mis", "=", "0", "\n", "self", ".", "num_samples", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.reset": [[510, 522], ["meters.TrainMeter.loss.reset", "meters.TrainMeter.mb_top1_err.reset", "meters.TrainMeter.mb_top5_err.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset the Meter.\n        \"\"\"", "\n", "self", ".", "loss", ".", "reset", "(", ")", "\n", "self", ".", "loss_total", "=", "0.0", "\n", "self", ".", "lr", "=", "None", "\n", "self", ".", "mb_top1_err", ".", "reset", "(", ")", "\n", "self", ".", "mb_top5_err", ".", "reset", "(", ")", "\n", "self", ".", "num_top1_mis", "=", "0", "\n", "self", ".", "num_top5_mis", "=", "0", "\n", "self", ".", "num_samples", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.iter_tic": [[523, 528], ["meters.TrainMeter.iter_timer.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "iter_tic", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Start to record time.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.iter_toc": [[529, 534], ["meters.TrainMeter.iter_timer.pause"], "methods", ["None"], ["", "def", "iter_toc", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Stop to record time.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "pause", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.update_stats": [[535, 557], ["meters.TrainMeter.loss.add_value", "meters.TrainMeter.mb_top1_err.add_value", "meters.TrainMeter.mb_top5_err.add_value"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value"], ["", "def", "update_stats", "(", "self", ",", "top1_err", ",", "top5_err", ",", "loss", ",", "lr", ",", "mb_size", ")", ":", "\n", "        ", "\"\"\"\n        Update the current stats.\n        Args:\n            top1_err (float): top1 error rate.\n            top5_err (float): top5 error rate.\n            loss (float): loss value.\n            lr (float): learning rate.\n            mb_size (int): mini batch size.\n        \"\"\"", "\n", "self", ".", "loss", ".", "add_value", "(", "loss", ")", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "loss_total", "+=", "loss", "*", "mb_size", "\n", "self", ".", "num_samples", "+=", "mb_size", "\n", "\n", "if", "not", "self", ".", "_cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "# Current minibatch stats", "\n", "            ", "self", ".", "mb_top1_err", ".", "add_value", "(", "top1_err", ")", "\n", "self", ".", "mb_top5_err", ".", "add_value", "(", "top5_err", ")", "\n", "# Aggregate stats", "\n", "self", ".", "num_top1_mis", "+=", "top1_err", "*", "mb_size", "\n", "self", ".", "num_top5_mis", "+=", "top5_err", "*", "mb_size", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.log_iter_stats": [[558, 585], ["str", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "meters.TrainMeter.iter_timer.seconds", "datetime.timedelta", "meters.TrainMeter.iter_timer.seconds", "meters.TrainMeter.loss.get_win_median", "meters.TrainMeter.mb_top1_err.get_win_median", "meters.TrainMeter.mb_top5_err.get_win_median", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "int"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage"], ["", "", "def", "log_iter_stats", "(", "self", ",", "cur_epoch", ",", "cur_iter", ")", ":", "\n", "        ", "\"\"\"\n        log the stats of the current iteration.\n        Args:\n            cur_epoch (int): the number of current epoch.\n            cur_iter (int): the number of current iteration.\n        \"\"\"", "\n", "if", "(", "cur_iter", "+", "1", ")", "%", "self", ".", "_cfg", ".", "LOG_PERIOD", "!=", "0", ":", "\n", "            ", "return", "\n", "", "eta_sec", "=", "self", ".", "iter_timer", ".", "seconds", "(", ")", "*", "(", "\n", "self", ".", "MAX_EPOCH", "-", "(", "cur_epoch", "*", "self", ".", "epoch_iters", "+", "cur_iter", "+", "1", ")", "\n", ")", "\n", "eta", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_sec", ")", ")", ")", "\n", "stats", "=", "{", "\n", "\"_type\"", ":", "\"train_iter\"", ",", "\n", "\"epoch\"", ":", "\"{}/{}\"", ".", "format", "(", "cur_epoch", "+", "1", ",", "self", ".", "_cfg", ".", "SOLVER", ".", "MAX_EPOCH", ")", ",", "\n", "\"iter\"", ":", "\"{}/{}\"", ".", "format", "(", "cur_iter", "+", "1", ",", "self", ".", "epoch_iters", ")", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"loss\"", ":", "self", ".", "loss", ".", "get_win_median", "(", ")", ",", "\n", "\"lr\"", ":", "self", ".", "lr", ",", "\n", "\"gpu_mem\"", ":", "\"{:.2f} GB\"", ".", "format", "(", "misc", ".", "gpu_mem_usage", "(", ")", ")", ",", "\n", "}", "\n", "if", "not", "self", ".", "_cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "            ", "stats", "[", "\"top1_err\"", "]", "=", "self", ".", "mb_top1_err", ".", "get_win_median", "(", ")", "\n", "stats", "[", "\"top5_err\"", "]", "=", "self", ".", "mb_top5_err", ".", "get_win_median", "(", ")", "\n", "", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.TrainMeter.log_epoch_stats": [[586, 613], ["str", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "meters.TrainMeter.iter_timer.seconds", "datetime.timedelta", "meters.TrainMeter.iter_timer.seconds", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "int", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage"], ["", "def", "log_epoch_stats", "(", "self", ",", "cur_epoch", ")", ":", "\n", "        ", "\"\"\"\n        Log the stats of the current epoch.\n        Args:\n            cur_epoch (int): the number of current epoch.\n        \"\"\"", "\n", "eta_sec", "=", "self", ".", "iter_timer", ".", "seconds", "(", ")", "*", "(", "\n", "self", ".", "MAX_EPOCH", "-", "(", "cur_epoch", "+", "1", ")", "*", "self", ".", "epoch_iters", "\n", ")", "\n", "eta", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_sec", ")", ")", ")", "\n", "stats", "=", "{", "\n", "\"_type\"", ":", "\"train_epoch\"", ",", "\n", "\"epoch\"", ":", "\"{}/{}\"", ".", "format", "(", "cur_epoch", "+", "1", ",", "self", ".", "_cfg", ".", "SOLVER", ".", "MAX_EPOCH", ")", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"lr\"", ":", "self", ".", "lr", ",", "\n", "\"gpu_mem\"", ":", "\"{:.2f} GB\"", ".", "format", "(", "misc", ".", "gpu_mem_usage", "(", ")", ")", ",", "\n", "\"RAM\"", ":", "\"{:.2f}/{:.2f} GB\"", ".", "format", "(", "*", "misc", ".", "cpu_mem_usage", "(", ")", ")", ",", "\n", "}", "\n", "if", "not", "self", ".", "_cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "            ", "top1_err", "=", "self", ".", "num_top1_mis", "/", "self", ".", "num_samples", "\n", "top5_err", "=", "self", ".", "num_top5_mis", "/", "self", ".", "num_samples", "\n", "avg_loss", "=", "self", ".", "loss_total", "/", "self", ".", "num_samples", "\n", "stats", "[", "\"top1_err\"", "]", "=", "top1_err", "\n", "stats", "[", "\"top5_err\"", "]", "=", "top5_err", "\n", "stats", "[", "\"loss\"", "]", "=", "avg_loss", "\n", "", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.__init__": [[620, 641], ["fvcore.common.timer.Timer", "meters.ScalarMeter", "meters.ScalarMeter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_iter", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            max_iter (int): the max number of iteration of the current epoch.\n            cfg (CfgNode): configs.\n        \"\"\"", "\n", "self", ".", "_cfg", "=", "cfg", "\n", "self", ".", "max_iter", "=", "max_iter", "\n", "self", ".", "iter_timer", "=", "Timer", "(", ")", "\n", "# Current minibatch errors (smoothed over a window).", "\n", "self", ".", "mb_top1_err", "=", "ScalarMeter", "(", "cfg", ".", "LOG_PERIOD", ")", "\n", "self", ".", "mb_top5_err", "=", "ScalarMeter", "(", "cfg", ".", "LOG_PERIOD", ")", "\n", "# Min errors (over the full val set).", "\n", "self", ".", "min_top1_err", "=", "100.0", "\n", "self", ".", "min_top5_err", "=", "100.0", "\n", "# Number of misclassified examples.", "\n", "self", ".", "num_top1_mis", "=", "0", "\n", "self", ".", "num_top5_mis", "=", "0", "\n", "self", ".", "num_samples", "=", "0", "\n", "self", ".", "all_preds", "=", "[", "]", "\n", "self", ".", "all_labels", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset": [[642, 654], ["meters.ValMeter.iter_timer.reset", "meters.ValMeter.mb_top1_err.reset", "meters.ValMeter.mb_top5_err.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset the Meter.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "reset", "(", ")", "\n", "self", ".", "mb_top1_err", ".", "reset", "(", ")", "\n", "self", ".", "mb_top5_err", ".", "reset", "(", ")", "\n", "self", ".", "num_top1_mis", "=", "0", "\n", "self", ".", "num_top5_mis", "=", "0", "\n", "self", ".", "num_samples", "=", "0", "\n", "self", ".", "all_preds", "=", "[", "]", "\n", "self", ".", "all_labels", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_tic": [[655, 660], ["meters.ValMeter.iter_timer.reset"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.reset"], ["", "def", "iter_tic", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Start to record time.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.iter_toc": [[661, 666], ["meters.ValMeter.iter_timer.pause"], "methods", ["None"], ["", "def", "iter_toc", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Stop to record time.\n        \"\"\"", "\n", "self", ".", "iter_timer", ".", "pause", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_stats": [[667, 680], ["meters.ValMeter.mb_top1_err.add_value", "meters.ValMeter.mb_top5_err.add_value"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.add_value"], ["", "def", "update_stats", "(", "self", ",", "top1_err", ",", "top5_err", ",", "mb_size", ")", ":", "\n", "        ", "\"\"\"\n        Update the current stats.\n        Args:\n            top1_err (float): top1 error rate.\n            top5_err (float): top5 error rate.\n            mb_size (int): mini batch size.\n        \"\"\"", "\n", "self", ".", "mb_top1_err", ".", "add_value", "(", "top1_err", ")", "\n", "self", ".", "mb_top5_err", ".", "add_value", "(", "top5_err", ")", "\n", "self", ".", "num_top1_mis", "+=", "top1_err", "*", "mb_size", "\n", "self", ".", "num_top5_mis", "+=", "top5_err", "*", "mb_size", "\n", "self", ".", "num_samples", "+=", "mb_size", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.update_predictions": [[681, 691], ["meters.ValMeter.all_preds.append", "meters.ValMeter.all_labels.append"], "methods", ["None"], ["", "def", "update_predictions", "(", "self", ",", "preds", ",", "labels", ")", ":", "\n", "        ", "\"\"\"\n        Update predictions and labels.\n        Args:\n            preds (tensor): model output predictions.\n            labels (tensor): labels.\n        \"\"\"", "\n", "# TODO: merge update_prediction with update_stats.", "\n", "self", ".", "all_preds", ".", "append", "(", "preds", ")", "\n", "self", ".", "all_labels", ".", "append", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_iter_stats": [[692, 715], ["str", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "meters.ValMeter.iter_timer.seconds", "datetime.timedelta", "meters.ValMeter.iter_timer.seconds", "meters.ValMeter.mb_top1_err.get_win_median", "meters.ValMeter.mb_top5_err.get_win_median", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "int"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ScalarMeter.get_win_median", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage"], ["", "def", "log_iter_stats", "(", "self", ",", "cur_epoch", ",", "cur_iter", ")", ":", "\n", "        ", "\"\"\"\n        log the stats of the current iteration.\n        Args:\n            cur_epoch (int): the number of current epoch.\n            cur_iter (int): the number of current iteration.\n        \"\"\"", "\n", "if", "(", "cur_iter", "+", "1", ")", "%", "self", ".", "_cfg", ".", "LOG_PERIOD", "!=", "0", ":", "\n", "            ", "return", "\n", "", "eta_sec", "=", "self", ".", "iter_timer", ".", "seconds", "(", ")", "*", "(", "self", ".", "max_iter", "-", "cur_iter", "-", "1", ")", "\n", "eta", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_sec", ")", ")", ")", "\n", "stats", "=", "{", "\n", "\"_type\"", ":", "\"val_iter\"", ",", "\n", "\"epoch\"", ":", "\"{}/{}\"", ".", "format", "(", "cur_epoch", "+", "1", ",", "self", ".", "_cfg", ".", "SOLVER", ".", "MAX_EPOCH", ")", ",", "\n", "\"iter\"", ":", "\"{}/{}\"", ".", "format", "(", "cur_iter", "+", "1", ",", "self", ".", "max_iter", ")", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"eta\"", ":", "eta", ",", "\n", "\"gpu_mem\"", ":", "\"{:.2f} GB\"", ".", "format", "(", "misc", ".", "gpu_mem_usage", "(", ")", ")", ",", "\n", "}", "\n", "if", "not", "self", ".", "_cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "            ", "stats", "[", "\"top1_err\"", "]", "=", "self", ".", "mb_top1_err", ".", "get_win_median", "(", ")", "\n", "stats", "[", "\"top5_err\"", "]", "=", "self", ".", "mb_top5_err", ".", "get_win_median", "(", ")", "\n", "", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.ValMeter.log_epoch_stats": [[716, 746], ["slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "slowfast.log_json_stats", "meters.ValMeter.iter_timer.seconds", "meters.get_map", "min", "min", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "slowfast.gpu_mem_usage", "torch.cat().cpu().numpy", "torch.cat().cpu().numpy", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "slowfast.cpu_mem_usage", "torch.cat().cpu", "torch.cat().cpu", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.log_json_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.get_map", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage"], ["", "def", "log_epoch_stats", "(", "self", ",", "cur_epoch", ")", ":", "\n", "        ", "\"\"\"\n        Log the stats of the current epoch.\n        Args:\n            cur_epoch (int): the number of current epoch.\n        \"\"\"", "\n", "stats", "=", "{", "\n", "\"_type\"", ":", "\"val_epoch\"", ",", "\n", "\"epoch\"", ":", "\"{}/{}\"", ".", "format", "(", "cur_epoch", "+", "1", ",", "self", ".", "_cfg", ".", "SOLVER", ".", "MAX_EPOCH", ")", ",", "\n", "\"time_diff\"", ":", "self", ".", "iter_timer", ".", "seconds", "(", ")", ",", "\n", "\"gpu_mem\"", ":", "\"{:.2f} GB\"", ".", "format", "(", "misc", ".", "gpu_mem_usage", "(", ")", ")", ",", "\n", "\"RAM\"", ":", "\"{:.2f}/{:.2f} GB\"", ".", "format", "(", "*", "misc", ".", "cpu_mem_usage", "(", ")", ")", ",", "\n", "}", "\n", "if", "self", ".", "_cfg", ".", "DATA", ".", "MULTI_LABEL", ":", "\n", "            ", "stats", "[", "\"map\"", "]", "=", "get_map", "(", "\n", "torch", ".", "cat", "(", "self", ".", "all_preds", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "torch", ".", "cat", "(", "self", ".", "all_labels", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "top1_err", "=", "self", ".", "num_top1_mis", "/", "self", ".", "num_samples", "\n", "top5_err", "=", "self", ".", "num_top5_mis", "/", "self", ".", "num_samples", "\n", "self", ".", "min_top1_err", "=", "min", "(", "self", ".", "min_top1_err", ",", "top1_err", ")", "\n", "self", ".", "min_top5_err", "=", "min", "(", "self", ".", "min_top5_err", ",", "top5_err", ")", "\n", "\n", "stats", "[", "\"top1_err\"", "]", "=", "top1_err", "\n", "stats", "[", "\"top5_err\"", "]", "=", "top5_err", "\n", "stats", "[", "\"min_top1_err\"", "]", "=", "self", ".", "min_top1_err", "\n", "stats", "[", "\"min_top5_err\"", "]", "=", "self", ".", "min_top5_err", "\n", "\n", "", "logging", ".", "log_json_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.get_ava_mini_groundtruth": [[29, 45], ["range", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "full_groundtruth[].keys", "int", "key.split"], "function", ["None"], ["def", "get_ava_mini_groundtruth", "(", "full_groundtruth", ")", ":", "\n", "    ", "\"\"\"\n    Get the groundtruth annotations corresponding the \"subset\" of AVA val set.\n    We define the subset to be the frames such that (second % 4 == 0).\n    We optionally use subset for faster evaluation during training\n    (in order to track training progress).\n    Args:\n        full_groundtruth(dict): list of groundtruth.\n    \"\"\"", "\n", "ret", "=", "[", "defaultdict", "(", "list", ")", ",", "defaultdict", "(", "list", ")", ",", "defaultdict", "(", "list", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "for", "key", "in", "full_groundtruth", "[", "i", "]", ".", "keys", "(", ")", ":", "\n", "            ", "if", "int", "(", "key", ".", "split", "(", "\",\"", ")", "[", "1", "]", ")", "%", "4", "==", "0", ":", "\n", "                ", "ret", "[", "i", "]", "[", "key", "]", "=", "full_groundtruth", "[", "i", "]", "[", "key", "]", "\n", "", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.meters.get_map": [[748, 773], ["logger.info", "numpy.mean", "sklearn.metrics.average_precision_score", "print", "numpy.all", "numpy.all"], "function", ["None"], ["", "", "def", "get_map", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "\"\"\"\n    Compute mAP for multi-label case.\n    Args:\n        preds (numpy tensor): num_examples x num_classes.\n        labels (numpy tensor): num_examples x num_classes.\n    Returns:\n        mean_ap (int): final mAP score.\n    \"\"\"", "\n", "\n", "logger", ".", "info", "(", "\"Getting mAP for {} examples\"", ".", "format", "(", "preds", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "preds", "=", "preds", "[", ":", ",", "~", "(", "np", ".", "all", "(", "labels", "==", "0", ",", "axis", "=", "0", ")", ")", "]", "\n", "labels", "=", "labels", "[", ":", ",", "~", "(", "np", ".", "all", "(", "labels", "==", "0", ",", "axis", "=", "0", ")", ")", "]", "\n", "aps", "=", "[", "0", "]", "\n", "try", ":", "\n", "        ", "aps", "=", "average_precision_score", "(", "labels", ",", "preds", ",", "average", "=", "None", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "print", "(", "\n", "\"Average precision requires a sufficient number of samples \\\n            in a batch which are missing in this sample.\"", "\n", ")", "\n", "\n", "", "mean_ap", "=", "np", ".", "mean", "(", "aps", ")", "\n", "return", "mean_ap", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.check_nan_losses": [[26, 34], ["math.isnan", "RuntimeError", "datetime.datetime.now"], "function", ["None"], ["def", "check_nan_losses", "(", "loss", ")", ":", "\n", "    ", "\"\"\"\n    Determine whether the loss is NaN (not a number).\n    Args:\n        loss (loss): loss to check whether is NaN.\n    \"\"\"", "\n", "if", "math", ".", "isnan", "(", "loss", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"ERROR: Got NaN losses {}\"", ".", "format", "(", "datetime", ".", "now", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.params_count": [[36, 43], ["numpy.sum().item", "numpy.sum", "p.numel", "model.parameters"], "function", ["None"], ["", "", "def", "params_count", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    Compute the number of parameters.\n    Args:\n        model (model): model to count the number of parameters.\n    \"\"\"", "\n", "return", "np", ".", "sum", "(", "[", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage": [[45, 54], ["torch.cuda.is_available", "torch.cuda.max_memory_allocated"], "function", ["None"], ["", "def", "gpu_mem_usage", "(", ")", ":", "\n", "    ", "\"\"\"\n    Compute the GPU memory usage for the current device (GB).\n    \"\"\"", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "mem_usage_bytes", "=", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "\n", "", "else", ":", "\n", "        ", "mem_usage_bytes", "=", "0", "\n", "", "return", "mem_usage_bytes", "/", "1024", "**", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.cpu_mem_usage": [[56, 68], ["psutil.virtual_memory"], "function", ["None"], ["", "def", "cpu_mem_usage", "(", ")", ":", "\n", "    ", "\"\"\"\n    Compute the system memory (RAM) usage for the current device (GB).\n    Returns:\n        usage (float): used memory (GB).\n        total (float): total memory (GB).\n    \"\"\"", "\n", "vram", "=", "psutil", ".", "virtual_memory", "(", ")", "\n", "usage", "=", "(", "vram", ".", "total", "-", "vram", ".", "available", ")", "/", "1024", "**", "3", "\n", "total", "=", "vram", ".", "total", "/", "1024", "**", "3", "\n", "\n", "return", "usage", ",", "total", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc._get_model_analysis_input": [[70, 113], ["slowfast.datasets.utils.pack_pathway_output", "range", "torch.rand", "torch.rand", "len", "model_inputs[].unsqueeze", "torch.tensor", "model_inputs[].cuda", "bbox.cuda.cuda"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.pack_pathway_output"], ["", "def", "_get_model_analysis_input", "(", "cfg", ",", "use_train_input", ")", ":", "\n", "    ", "\"\"\"\n    Return a dummy input for model analysis with batch size 1. The input is\n        used for analyzing the model (counting flops and activations etc.).\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        use_train_input (bool): if True, return the input for training. Otherwise,\n            return the input for testing.\n\n    Returns:\n        inputs: the input for model analysis.\n    \"\"\"", "\n", "rgb_dimension", "=", "3", "\n", "if", "use_train_input", ":", "\n", "        ", "input_tensors", "=", "torch", ".", "rand", "(", "\n", "rgb_dimension", ",", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", ",", "\n", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", ",", "\n", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "input_tensors", "=", "torch", ".", "rand", "(", "\n", "rgb_dimension", ",", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", ",", "\n", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "\n", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "\n", ")", "\n", "", "model_inputs", "=", "pack_pathway_output", "(", "cfg", ",", "input_tensors", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "model_inputs", ")", ")", ":", "\n", "        ", "model_inputs", "[", "i", "]", "=", "model_inputs", "[", "i", "]", ".", "unsqueeze", "(", "0", ")", "\n", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "model_inputs", "[", "i", "]", "=", "model_inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# If detection is enabled, count flops for one proposal.", "\n", "", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "        ", "bbox", "=", "torch", ".", "tensor", "(", "[", "[", "0", ",", "0", ",", "1.0", ",", "0", ",", "1.0", "]", "]", ")", "\n", "if", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "bbox", "=", "bbox", ".", "cuda", "(", ")", "\n", "", "inputs", "=", "(", "model_inputs", ",", "bbox", ")", "\n", "", "else", ":", "\n", "        ", "inputs", "=", "(", "model_inputs", ",", ")", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_model_stats": [[115, 148], ["model.eval", "misc._get_model_analysis_input", "model_stats_fun", "sum", "model.train", "count_dict.values"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc._get_model_analysis_input", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.train_net.train"], ["", "def", "get_model_stats", "(", "model", ",", "cfg", ",", "mode", ",", "use_train_input", ")", ":", "\n", "    ", "\"\"\"\n    Compute statistics for the current model given the config.\n    Args:\n        model (model): model to perform analysis.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        mode (str): Options include `flop` or `activation`. Compute either flop\n            (gflops) or activation count (mega).\n        use_train_input (bool): if True, compute statistics for training. Otherwise,\n            compute statistics for testing.\n\n    Returns:\n        float: the total number of count of the given model.\n    \"\"\"", "\n", "assert", "mode", "in", "[", "\n", "\"flop\"", ",", "\n", "\"activation\"", ",", "\n", "]", ",", "\"'{}' not supported for model analysis\"", ".", "format", "(", "mode", ")", "\n", "if", "mode", "==", "\"flop\"", ":", "\n", "        ", "model_stats_fun", "=", "flop_count", "\n", "", "elif", "mode", "==", "\"activation\"", ":", "\n", "        ", "model_stats_fun", "=", "activation_count", "\n", "\n", "# Set model to evaluation mode for analysis.", "\n", "# Evaluation mode can avoid getting stuck with sync batchnorm.", "\n", "", "model_mode", "=", "model", ".", "training", "\n", "model", ".", "eval", "(", ")", "\n", "inputs", "=", "_get_model_analysis_input", "(", "cfg", ",", "use_train_input", ")", "\n", "count_dict", ",", "_", "=", "model_stats_fun", "(", "model", ",", "inputs", ")", "\n", "count", "=", "sum", "(", "count_dict", ".", "values", "(", ")", ")", "\n", "model", ".", "train", "(", "model_mode", ")", "\n", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info": [[150, 176], ["logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "os.system", "misc.params_count", "misc.gpu_mem_usage", "misc.get_model_stats", "misc.get_model_stats"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.params_count", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.gpu_mem_usage", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_model_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_model_stats"], ["", "def", "log_model_info", "(", "model", ",", "cfg", ",", "use_train_input", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Log info, includes number of parameters, gpu usage, gflops and activation count.\n        The model info is computed when the model is in validation mode.\n    Args:\n        model (model): model to log the info.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        use_train_input (bool): if True, log info for training. Otherwise,\n            log info for testing.\n    \"\"\"", "\n", "logger", ".", "info", "(", "\"Model:\\n{}\"", ".", "format", "(", "model", ")", ")", "\n", "logger", ".", "info", "(", "\"Params: {:,}\"", ".", "format", "(", "params_count", "(", "model", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Mem: {:,} MB\"", ".", "format", "(", "gpu_mem_usage", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"Flops: {:,} G\"", ".", "format", "(", "\n", "get_model_stats", "(", "model", ",", "cfg", ",", "\"flop\"", ",", "use_train_input", ")", "\n", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Activations: {:,} M\"", ".", "format", "(", "\n", "get_model_stats", "(", "model", ",", "cfg", ",", "\"activation\"", ",", "use_train_input", ")", "\n", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"nvidia-smi\"", ")", "\n", "os", ".", "system", "(", "\"nvidia-smi\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.is_eval_epoch": [[178, 200], ["max"], "function", ["None"], ["", "def", "is_eval_epoch", "(", "cfg", ",", "cur_epoch", ",", "multigrid_schedule", ")", ":", "\n", "    ", "\"\"\"\n    Determine if the model should be evaluated at the current epoch.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (int): current epoch.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"", "\n", "if", "cur_epoch", "+", "1", "==", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", ":", "\n", "        ", "return", "True", "\n", "", "if", "multigrid_schedule", "is", "not", "None", ":", "\n", "        ", "prev_epoch", "=", "0", "\n", "for", "s", "in", "multigrid_schedule", ":", "\n", "            ", "if", "cur_epoch", "<", "s", "[", "-", "1", "]", ":", "\n", "                ", "period", "=", "max", "(", "\n", "(", "s", "[", "-", "1", "]", "-", "prev_epoch", ")", "//", "cfg", ".", "MULTIGRID", ".", "EVAL_FREQ", "+", "1", ",", "1", "\n", ")", "\n", "return", "(", "s", "[", "-", "1", "]", "-", "1", "-", "cur_epoch", ")", "%", "period", "==", "0", "\n", "", "prev_epoch", "=", "s", "[", "-", "1", "]", "\n", "\n", "", "", "return", "(", "cur_epoch", "+", "1", ")", "%", "cfg", ".", "TRAIN", ".", "EVAL_PERIOD", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.plot_input": [[202, 229], ["matplotlib.pyplot.subplots", "range", "f.savefig", "tensor.min", "tensor.max", "ax[].axis", "ax[].imshow", "tensor[].permute", "ax[].text", "len", "ax[].vlines", "ax[].vlines", "ax[].hlines", "ax[].hlines", "len"], "function", ["None"], ["", "def", "plot_input", "(", "tensor", ",", "bboxes", "=", "(", ")", ",", "texts", "=", "(", ")", ",", "path", "=", "\"./tmp_vis.png\"", ")", ":", "\n", "    ", "\"\"\"\n    Plot the input tensor with the optional bounding box and save it to disk.\n    Args:\n        tensor (tensor): a tensor with shape of `NxCxHxW`.\n        bboxes (tuple): bounding boxes with format of [[x, y, h, w]].\n        texts (tuple): a tuple of string to plot.\n        path (str): path to the image to save to.\n    \"\"\"", "\n", "tensor", "=", "tensor", "-", "tensor", ".", "min", "(", ")", "\n", "tensor", "=", "tensor", "/", "tensor", ".", "max", "(", ")", "\n", "f", ",", "ax", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "tensor", ".", "shape", "[", "0", "]", ",", "figsize", "=", "(", "50", ",", "20", ")", ")", "\n", "for", "i", "in", "range", "(", "tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "ax", "[", "i", "]", ".", "axis", "(", "\"off\"", ")", "\n", "ax", "[", "i", "]", ".", "imshow", "(", "tensor", "[", "i", "]", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "# ax[1][0].axis('off')", "\n", "if", "bboxes", "is", "not", "None", "and", "len", "(", "bboxes", ")", ">", "i", ":", "\n", "            ", "for", "box", "in", "bboxes", "[", "i", "]", ":", "\n", "                ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "box", "\n", "ax", "[", "i", "]", ".", "vlines", "(", "x1", ",", "y1", ",", "y2", ",", "colors", "=", "\"g\"", ",", "linestyles", "=", "\"solid\"", ")", "\n", "ax", "[", "i", "]", ".", "vlines", "(", "x2", ",", "y1", ",", "y2", ",", "colors", "=", "\"g\"", ",", "linestyles", "=", "\"solid\"", ")", "\n", "ax", "[", "i", "]", ".", "hlines", "(", "y1", ",", "x1", ",", "x2", ",", "colors", "=", "\"g\"", ",", "linestyles", "=", "\"solid\"", ")", "\n", "ax", "[", "i", "]", ".", "hlines", "(", "y2", ",", "x1", ",", "x2", ",", "colors", "=", "\"g\"", ",", "linestyles", "=", "\"solid\"", ")", "\n", "\n", "", "", "if", "texts", "is", "not", "None", "and", "len", "(", "texts", ")", ">", "i", ":", "\n", "            ", "ax", "[", "i", "]", ".", "text", "(", "0", ",", "0", ",", "texts", "[", "i", "]", ")", "\n", "", "", "f", ".", "savefig", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.frozen_bn_stats": [[231, 240], ["model.modules", "isinstance", "m.eval"], "function", ["None"], ["", "def", "frozen_bn_stats", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    Set all the bn layers to eval mode.\n    Args:\n        model (model): model to set bn layers to eval mode.\n    \"\"\"", "\n", "for", "m", "in", "model", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "            ", "m", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.aggregate_sub_bn_stats": [[242, 258], ["module.children", "isinstance", "child.aggregate_stats", "misc.aggregate_sub_bn_stats"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.SubBatchNorm3d.aggregate_stats", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.aggregate_sub_bn_stats"], ["", "", "", "def", "aggregate_sub_bn_stats", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Recursively find all SubBN modules and aggregate sub-BN stats.\n    Args:\n        module (nn.Module)\n    Returns:\n        count (int): number of SubBN module found.\n    \"\"\"", "\n", "count", "=", "0", "\n", "for", "child", "in", "module", ".", "children", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "child", ",", "SubBatchNorm3d", ")", ":", "\n", "            ", "child", ".", "aggregate_stats", "(", ")", "\n", "count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "count", "+=", "aggregate_sub_bn_stats", "(", "child", ")", "\n", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.launch_job": [[260, 308], ["func", "torch.multiprocessing.spawn", "int", "int", "slowfast.run", "os.getenv", "os.getenv"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multiprocessing.run"], ["", "def", "launch_job", "(", "cfg", ",", "args", ",", "func", ",", "daemon", "=", "False", ",", "start_method", "=", "'spawn'", ")", ":", "\n", "    ", "\"\"\"\n    Run 'func' on one or more GPUs, specified in cfg\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        init_method (str): initialization method to launch the job with multiple\n            devices.\n        func (function): job to run on GPU(s)\n        daemon (bool): The spawned processes\u2019 daemon flag. If set to True,\n            daemonic processes will be created\n        start_method (str): in ['spawn' or 'cmd']\n    \"\"\"", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "        ", "if", "start_method", "==", "'spawn'", ":", "\n", "# invoke DDP with multiprocessing.spawn", "\n", "            ", "torch", ".", "multiprocessing", ".", "spawn", "(", "\n", "mpu", ".", "run", ",", "\n", "nprocs", "=", "cfg", ".", "NUM_GPUS", ",", "\n", "args", "=", "(", "\n", "cfg", ".", "NUM_GPUS", ",", "\n", "func", ",", "\n", "args", ".", "init_method", ",", "\n", "cfg", ".", "SHARD_ID", ",", "\n", "cfg", ".", "NUM_SHARDS", ",", "\n", "cfg", ".", "DIST_BACKEND", ",", "\n", "cfg", ",", "\n", ")", ",", "\n", "daemon", "=", "daemon", ",", "\n", ")", "\n", "", "elif", "start_method", "==", "'cmd'", ":", "\n", "# invoke DDP with command line settings", "\n", "# python3", "\n", "            ", "init_method", "=", "\"env://\"", "\n", "local_rank", "=", "int", "(", "os", ".", "getenv", "(", "\"LOCAL_RANK\"", ",", "args", ".", "local_rank", ")", ")", "\n", "world_size", "=", "int", "(", "os", ".", "getenv", "(", "\"WORLD_SIZE\"", ",", "cfg", ".", "NUM_GPUS", ")", ")", "\n", "mpu", ".", "run", "(", "\n", "local_rank", "=", "local_rank", ",", "\n", "num_proc", "=", "world_size", ",", "\n", "func", "=", "func", ",", "\n", "init_method", "=", "init_method", ",", "\n", "shard_id", "=", "cfg", ".", "SHARD_ID", ",", "\n", "num_shards", "=", "cfg", ".", "NUM_SHARDS", ",", "\n", "backend", "=", "cfg", ".", "DIST_BACKEND", ",", "\n", "cfg", "=", "cfg", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "func", "(", "cfg", "=", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_class_names": [[310, 380], ["max", "json.load.items", "json.load.values", "json.load.items", "fvcore.common.file_io.PathManager.open", "json.load", "print", "fvcore.common.file_io.PathManager.open", "json.load", "print", "fvcore.common.file_io.PathManager.open", "f.read().split", "print", "json.load.get", "f.read", "json.load.get"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "", "def", "get_class_names", "(", "path", ",", "parent_path", "=", "None", ",", "subset_path", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Read json file with entries {classname: index} and return\n    an array of class names in order.\n    If parent_path is provided, load and map all children to their ids.\n    Args:\n        path (str): path to class ids json file.\n            File must be in the format {\"class1\": id1, \"class2\": id2, ...}\n        parent_path (Optional[str]): path to parent-child json file.\n            File must be in the format {\"parent1\": [\"child1\", \"child2\", ...], ...}\n        subset_path (Optional[str]): path to text file containing a subset\n            of class names, separated by newline characters.\n    Returns:\n        class_names (list of strs): list of class names.\n        class_parents (dict): a dictionary where key is the name of the parent class\n            and value is a list of ids of the children classes.\n        subset_ids (list of ints): list of ids of the classes provided in the\n            subset file.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "with", "PathManager", ".", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "class2idx", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "except", "Exception", "as", "err", ":", "\n", "        ", "print", "(", "\"Fail to load file from {} with error {}\"", ".", "format", "(", "path", ",", "err", ")", ")", "\n", "return", "\n", "\n", "", "max_key", "=", "max", "(", "class2idx", ".", "values", "(", ")", ")", "\n", "class_names", "=", "[", "None", "]", "*", "(", "max_key", "+", "1", ")", "\n", "\n", "for", "k", ",", "i", "in", "class2idx", ".", "items", "(", ")", ":", "\n", "        ", "class_names", "[", "i", "]", "=", "k", "\n", "\n", "", "class_parent", "=", "None", "\n", "if", "parent_path", "is", "not", "None", "and", "parent_path", "!=", "\"\"", ":", "\n", "        ", "try", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "parent_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "d_parent", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "except", "EnvironmentError", "as", "err", ":", "\n", "            ", "print", "(", "\n", "\"Fail to load file from {} with error {}\"", ".", "format", "(", "\n", "parent_path", ",", "err", "\n", ")", "\n", ")", "\n", "return", "\n", "", "class_parent", "=", "{", "}", "\n", "for", "parent", ",", "children", "in", "d_parent", ".", "items", "(", ")", ":", "\n", "            ", "indices", "=", "[", "\n", "class2idx", "[", "c", "]", "for", "c", "in", "children", "if", "class2idx", ".", "get", "(", "c", ")", "is", "not", "None", "\n", "]", "\n", "class_parent", "[", "parent", "]", "=", "indices", "\n", "\n", "", "", "subset_ids", "=", "None", "\n", "if", "subset_path", "is", "not", "None", "and", "subset_path", "!=", "\"\"", ":", "\n", "        ", "try", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "subset_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "subset", "=", "f", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "subset_ids", "=", "[", "\n", "class2idx", "[", "name", "]", "\n", "for", "name", "in", "subset", "\n", "if", "class2idx", ".", "get", "(", "name", ")", "is", "not", "None", "\n", "]", "\n", "", "", "except", "EnvironmentError", "as", "err", ":", "\n", "            ", "print", "(", "\n", "\"Fail to load file from {} with error {}\"", ".", "format", "(", "\n", "subset_path", ",", "err", "\n", ")", "\n", ")", "\n", "return", "\n", "\n", "", "", "return", "class_names", ",", "class_parent", ",", "subset_ids", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.init_multigrid": [[18, 62], ["multigrid.MultigridSchedule.get_long_cycle_schedule", "int", "int"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.get_long_cycle_schedule"], ["def", "init_multigrid", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Update cfg based on multigrid settings.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific\n                hyperparameters. Details can be seen in\n                slowfast/config/defaults.py.\n        Returns:\n            cfg (configs): the updated cfg.\n        \"\"\"", "\n", "self", ".", "schedule", "=", "None", "\n", "# We may modify cfg.TRAIN.BATCH_SIZE, cfg.DATA.NUM_FRAMES, and", "\n", "# cfg.DATA.TRAIN_CROP_SIZE during training, so we store their original", "\n", "# value in cfg and use them as global variables.", "\n", "cfg", ".", "MULTIGRID", ".", "DEFAULT_B", "=", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", "\n", "cfg", ".", "MULTIGRID", ".", "DEFAULT_T", "=", "cfg", ".", "DATA", ".", "NUM_FRAMES", "\n", "cfg", ".", "MULTIGRID", ".", "DEFAULT_S", "=", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", "\n", "\n", "if", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE", ":", "\n", "            ", "self", ".", "schedule", "=", "self", ".", "get_long_cycle_schedule", "(", "cfg", ")", "\n", "cfg", ".", "SOLVER", ".", "STEPS", "=", "[", "0", "]", "+", "[", "s", "[", "-", "1", "]", "for", "s", "in", "self", ".", "schedule", "]", "\n", "# Fine-tuning phase.", "\n", "cfg", ".", "SOLVER", ".", "STEPS", "[", "-", "1", "]", "=", "(", "\n", "cfg", ".", "SOLVER", ".", "STEPS", "[", "-", "2", "]", "+", "cfg", ".", "SOLVER", ".", "STEPS", "[", "-", "1", "]", "\n", ")", "//", "2", "\n", "cfg", ".", "SOLVER", ".", "LRS", "=", "[", "\n", "cfg", ".", "SOLVER", ".", "GAMMA", "**", "s", "[", "0", "]", "*", "s", "[", "1", "]", "[", "0", "]", "for", "s", "in", "self", ".", "schedule", "\n", "]", "\n", "# Fine-tuning phase.", "\n", "cfg", ".", "SOLVER", ".", "LRS", "=", "cfg", ".", "SOLVER", ".", "LRS", "[", ":", "-", "1", "]", "+", "[", "\n", "cfg", ".", "SOLVER", ".", "LRS", "[", "-", "2", "]", ",", "\n", "cfg", ".", "SOLVER", ".", "LRS", "[", "-", "1", "]", ",", "\n", "]", "\n", "\n", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "=", "self", ".", "schedule", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "\n", "", "elif", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE", ":", "\n", "            ", "cfg", ".", "SOLVER", ".", "STEPS", "=", "[", "\n", "int", "(", "s", "*", "cfg", ".", "MULTIGRID", ".", "EPOCH_FACTOR", ")", "for", "s", "in", "cfg", ".", "SOLVER", ".", "STEPS", "\n", "]", "\n", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "=", "int", "(", "\n", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "*", "cfg", ".", "MULTIGRID", ".", "EPOCH_FACTOR", "\n", ")", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.update_long_cycle": [[63, 122], ["multigrid.get_current_long_cycle_shape", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "float", "int", "logger.info", "int", "logger.info"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.get_current_long_cycle_shape"], ["", "def", "update_long_cycle", "(", "self", ",", "cfg", ",", "cur_epoch", ")", ":", "\n", "        ", "\"\"\"\n        Before every epoch, check if long cycle shape should change. If it\n            should, update cfg accordingly.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific\n                hyperparameters. Details can be seen in\n                slowfast/config/defaults.py.\n            cur_epoch (int): current epoch index.\n        Returns:\n            cfg (configs): the updated cfg.\n            changed (bool): do we change long cycle shape at this epoch?\n        \"\"\"", "\n", "base_b", ",", "base_t", ",", "base_s", "=", "get_current_long_cycle_shape", "(", "\n", "self", ".", "schedule", ",", "cur_epoch", "\n", ")", "\n", "if", "base_s", "!=", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", "or", "base_t", "!=", "cfg", ".", "DATA", ".", "NUM_FRAMES", ":", "\n", "\n", "            ", "cfg", ".", "DATA", ".", "NUM_FRAMES", "=", "base_t", "\n", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", "=", "base_s", "\n", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", "=", "base_b", "*", "cfg", ".", "MULTIGRID", ".", "DEFAULT_B", "\n", "\n", "bs_factor", "=", "(", "\n", "float", "(", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", "/", "cfg", ".", "NUM_GPUS", ")", "\n", "/", "cfg", ".", "MULTIGRID", ".", "BN_BASE_SIZE", "\n", ")", "\n", "\n", "if", "bs_factor", "<", "1", ":", "\n", "                ", "cfg", ".", "BN", ".", "NORM_TYPE", "=", "\"sync_batchnorm\"", "\n", "cfg", ".", "BN", ".", "NUM_SYNC_DEVICES", "=", "int", "(", "1.0", "/", "bs_factor", ")", "\n", "", "elif", "bs_factor", ">", "1", ":", "\n", "                ", "cfg", ".", "BN", ".", "NORM_TYPE", "=", "\"sub_batchnorm\"", "\n", "cfg", ".", "BN", ".", "NUM_SPLITS", "=", "int", "(", "bs_factor", ")", "\n", "", "else", ":", "\n", "                ", "cfg", ".", "BN", ".", "NORM_TYPE", "=", "\"batchnorm\"", "\n", "\n", "", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE_SAMPLING_RATE", "=", "cfg", ".", "DATA", ".", "SAMPLING_RATE", "*", "(", "\n", "cfg", ".", "MULTIGRID", ".", "DEFAULT_T", "//", "cfg", ".", "DATA", ".", "NUM_FRAMES", "\n", ")", "\n", "logger", ".", "info", "(", "\"Long cycle updates:\"", ")", "\n", "logger", ".", "info", "(", "\"\\tBN.NORM_TYPE: {}\"", ".", "format", "(", "cfg", ".", "BN", ".", "NORM_TYPE", ")", ")", "\n", "if", "cfg", ".", "BN", ".", "NORM_TYPE", "==", "\"sync_batchnorm\"", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"\\tBN.NUM_SYNC_DEVICES: {}\"", ".", "format", "(", "cfg", ".", "BN", ".", "NUM_SYNC_DEVICES", ")", "\n", ")", "\n", "", "elif", "cfg", ".", "BN", ".", "NORM_TYPE", "==", "\"sub_batchnorm\"", ":", "\n", "                ", "logger", ".", "info", "(", "\"\\tBN.NUM_SPLITS: {}\"", ".", "format", "(", "cfg", ".", "BN", ".", "NUM_SPLITS", ")", ")", "\n", "", "logger", ".", "info", "(", "\"\\tTRAIN.BATCH_SIZE: {}\"", ".", "format", "(", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"\\tDATA.NUM_FRAMES x LONG_CYCLE_SAMPLING_RATE: {}x{}\"", ".", "format", "(", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", ",", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE_SAMPLING_RATE", "\n", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"\\tDATA.TRAIN_CROP_SIZE: {}\"", ".", "format", "(", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", ")", "\n", ")", "\n", "return", "cfg", ",", "True", "\n", "", "else", ":", "\n", "            ", "return", "cfg", ",", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.MultigridSchedule.get_long_cycle_schedule": [[123, 213], ["float", "range", "schedule.append", "multigrid.print_schedule", "int", "int", "avg_bs.append", "all_shapes.append", "enumerate", "sum", "final_schedule.append", "round", "round", "numpy.mean", "len", "schedule.append", "int", "sum", "int", "round", "round"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.print_schedule"], ["", "", "def", "get_long_cycle_schedule", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Based on multigrid hyperparameters, define the schedule of a long cycle.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific\n                hyperparameters. Details can be seen in\n                slowfast/config/defaults.py.\n        Returns:\n            schedule (list): Specifies a list long cycle base shapes and their\n                corresponding training epochs.\n        \"\"\"", "\n", "\n", "steps", "=", "cfg", ".", "SOLVER", ".", "STEPS", "\n", "\n", "default_size", "=", "float", "(", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", "*", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", "**", "2", "\n", ")", "\n", "default_iters", "=", "steps", "[", "-", "1", "]", "\n", "\n", "# Get shapes and average batch size for each long cycle shape.", "\n", "avg_bs", "=", "[", "]", "\n", "all_shapes", "=", "[", "]", "\n", "for", "t_factor", ",", "s_factor", "in", "cfg", ".", "MULTIGRID", ".", "LONG_CYCLE_FACTORS", ":", "\n", "            ", "base_t", "=", "int", "(", "round", "(", "cfg", ".", "DATA", ".", "NUM_FRAMES", "*", "t_factor", ")", ")", "\n", "base_s", "=", "int", "(", "round", "(", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", "*", "s_factor", ")", ")", "\n", "if", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE", ":", "\n", "                ", "shapes", "=", "[", "\n", "[", "\n", "base_t", ",", "\n", "cfg", ".", "MULTIGRID", ".", "DEFAULT_S", "\n", "*", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE_FACTORS", "[", "0", "]", ",", "\n", "]", ",", "\n", "[", "\n", "base_t", ",", "\n", "cfg", ".", "MULTIGRID", ".", "DEFAULT_S", "\n", "*", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE_FACTORS", "[", "1", "]", ",", "\n", "]", ",", "\n", "[", "base_t", ",", "base_s", "]", ",", "\n", "]", "\n", "", "else", ":", "\n", "                ", "shapes", "=", "[", "[", "base_t", ",", "base_s", "]", "]", "\n", "\n", "# (T, S) -> (B, T, S)", "\n", "", "shapes", "=", "[", "\n", "[", "int", "(", "round", "(", "default_size", "/", "(", "s", "[", "0", "]", "*", "s", "[", "1", "]", "*", "s", "[", "1", "]", ")", ")", ")", ",", "s", "[", "0", "]", ",", "s", "[", "1", "]", "]", "\n", "for", "s", "in", "shapes", "\n", "]", "\n", "avg_bs", ".", "append", "(", "np", ".", "mean", "(", "[", "s", "[", "0", "]", "for", "s", "in", "shapes", "]", ")", ")", "\n", "all_shapes", ".", "append", "(", "shapes", ")", "\n", "\n", "# Get schedule regardless of cfg.MULTIGRID.EPOCH_FACTOR.", "\n", "", "total_iters", "=", "0", "\n", "schedule", "=", "[", "]", "\n", "for", "step_index", "in", "range", "(", "len", "(", "steps", ")", "-", "1", ")", ":", "\n", "            ", "step_epochs", "=", "steps", "[", "step_index", "+", "1", "]", "-", "steps", "[", "step_index", "]", "\n", "\n", "for", "long_cycle_index", ",", "shapes", "in", "enumerate", "(", "all_shapes", ")", ":", "\n", "                ", "cur_epochs", "=", "(", "\n", "step_epochs", "*", "avg_bs", "[", "long_cycle_index", "]", "/", "sum", "(", "avg_bs", ")", "\n", ")", "\n", "\n", "cur_iters", "=", "cur_epochs", "/", "avg_bs", "[", "long_cycle_index", "]", "\n", "total_iters", "+=", "cur_iters", "\n", "schedule", ".", "append", "(", "(", "step_index", ",", "shapes", "[", "-", "1", "]", ",", "cur_epochs", ")", ")", "\n", "\n", "", "", "iter_saving", "=", "default_iters", "/", "total_iters", "\n", "\n", "final_step_epochs", "=", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "-", "steps", "[", "-", "1", "]", "\n", "\n", "# We define the fine-tuning phase to have the same amount of iteration", "\n", "# saving as the rest of the training.", "\n", "ft_epochs", "=", "final_step_epochs", "/", "iter_saving", "*", "avg_bs", "[", "-", "1", "]", "\n", "\n", "schedule", ".", "append", "(", "(", "step_index", "+", "1", ",", "all_shapes", "[", "-", "1", "]", "[", "2", "]", ",", "ft_epochs", ")", ")", "\n", "\n", "# Obtrain final schedule given desired cfg.MULTIGRID.EPOCH_FACTOR.", "\n", "x", "=", "(", "\n", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", "\n", "*", "cfg", ".", "MULTIGRID", ".", "EPOCH_FACTOR", "\n", "/", "sum", "(", "s", "[", "-", "1", "]", "for", "s", "in", "schedule", ")", "\n", ")", "\n", "\n", "final_schedule", "=", "[", "]", "\n", "total_epochs", "=", "0", "\n", "for", "s", "in", "schedule", ":", "\n", "            ", "epochs", "=", "s", "[", "2", "]", "*", "x", "\n", "total_epochs", "+=", "epochs", "\n", "final_schedule", ".", "append", "(", "(", "s", "[", "0", "]", ",", "s", "[", "1", "]", ",", "int", "(", "round", "(", "total_epochs", ")", ")", ")", ")", "\n", "", "print_schedule", "(", "final_schedule", ")", "\n", "return", "final_schedule", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.print_schedule": [[215, 222], ["logger.info", "logger.info"], "function", ["None"], ["", "", "def", "print_schedule", "(", "schedule", ")", ":", "\n", "    ", "\"\"\"\n    Log schedule.\n    \"\"\"", "\n", "logger", ".", "info", "(", "\"Long cycle index\\tBase shape\\tEpochs\"", ")", "\n", "for", "s", "in", "schedule", ":", "\n", "        ", "logger", ".", "info", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "s", "[", "0", "]", ",", "s", "[", "1", "]", ",", "s", "[", "2", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.multigrid.get_current_long_cycle_shape": [[224, 241], ["None"], "function", ["None"], ["", "", "def", "get_current_long_cycle_shape", "(", "schedule", ",", "epoch", ")", ":", "\n", "    ", "\"\"\"\n    Given a schedule and epoch index, return the long cycle base shape.\n    Args:\n        schedule (configs): configs that contains training and multigrid specific\n            hyperparameters. Details can be seen in\n            slowfast/config/defaults.py.\n        cur_epoch (int): current epoch index.\n    Returns:\n        shapes (list): A list describing the base shape in a long cycle:\n            [batch size relative to default,\n            number of frames, spatial dimension].\n    \"\"\"", "\n", "for", "s", "in", "schedule", ":", "\n", "        ", "if", "epoch", "<", "s", "[", "-", "1", "]", ":", "\n", "            ", "return", "s", "[", "1", "]", "\n", "", "", "return", "schedule", "[", "-", "1", "]", "[", "1", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.bn_helper.compute_and_update_bn_stats": [[10, 78], ["torch.no_grad", "enumerate", "enumerate", "torch.zeros_like", "torch.zeros_like", "itertools.islice", "isinstance", "model", "enumerate", "model.modules", "any", "range", "inputs.cuda.cuda", "len", "inputs[].float().cuda", "isinstance", "inputs[].float"], "function", ["None"], ["@", "torch", ".", "no_grad", "(", ")", "\n", "def", "compute_and_update_bn_stats", "(", "model", ",", "data_loader", ",", "num_batches", "=", "200", ")", ":", "\n", "    ", "\"\"\"\n    Compute and update the batch norm stats to make it more precise. During\n    training both bn stats and the weight are changing after every iteration,\n    so the bn can not precisely reflect the latest stats of the current model.\n    Here the bn stats is recomputed without change of weights, to make the\n    running mean and running var more precise.\n    Args:\n        model (model): the model using to compute and update the bn stats.\n        data_loader (dataloader): dataloader using to provide inputs.\n        num_batches (int): running iterations using to compute the stats.\n    \"\"\"", "\n", "\n", "# Prepares all the bn layers.", "\n", "bn_layers", "=", "[", "\n", "m", "\n", "for", "m", "in", "model", ".", "modules", "(", ")", "\n", "if", "any", "(", "\n", "(", "\n", "isinstance", "(", "m", ",", "bn_type", ")", "\n", "for", "bn_type", "in", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", ",", "\n", "torch", ".", "nn", ".", "BatchNorm2d", ",", "\n", "torch", ".", "nn", ".", "BatchNorm3d", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "]", "\n", "\n", "# In order to make the running stats only reflect the current batch, the", "\n", "# momentum is disabled.", "\n", "# bn.running_mean = (1 - momentum) * bn.running_mean + momentum * batch_mean", "\n", "# Setting the momentum to 1.0 to compute the stats without momentum.", "\n", "momentum_actual", "=", "[", "bn", ".", "momentum", "for", "bn", "in", "bn_layers", "]", "\n", "for", "bn", "in", "bn_layers", ":", "\n", "        ", "bn", ".", "momentum", "=", "1.0", "\n", "\n", "# Calculates the running iterations for precise stats computation.", "\n", "", "running_mean", "=", "[", "torch", ".", "zeros_like", "(", "bn", ".", "running_mean", ")", "for", "bn", "in", "bn_layers", "]", "\n", "running_square_mean", "=", "[", "torch", ".", "zeros_like", "(", "bn", ".", "running_var", ")", "for", "bn", "in", "bn_layers", "]", "\n", "\n", "for", "ind", ",", "(", "inputs", ",", "_", ",", "_", ")", "in", "enumerate", "(", "\n", "itertools", ".", "islice", "(", "data_loader", ",", "num_batches", ")", "\n", ")", ":", "\n", "# Forwards the model to update the bn stats.", "\n", "        ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "float", "(", ")", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "            ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "model", "(", "inputs", ")", "\n", "\n", "for", "i", ",", "bn", "in", "enumerate", "(", "bn_layers", ")", ":", "\n", "# Accumulates the bn stats.", "\n", "            ", "running_mean", "[", "i", "]", "+=", "(", "bn", ".", "running_mean", "-", "running_mean", "[", "i", "]", ")", "/", "(", "ind", "+", "1", ")", "\n", "# $E(x^2) = Var(x) + E(x)^2$.", "\n", "cur_square_mean", "=", "bn", ".", "running_var", "+", "bn", ".", "running_mean", "**", "2", "\n", "running_square_mean", "[", "i", "]", "+=", "(", "\n", "cur_square_mean", "-", "running_square_mean", "[", "i", "]", "\n", ")", "/", "(", "ind", "+", "1", ")", "\n", "\n", "", "", "for", "i", ",", "bn", "in", "enumerate", "(", "bn_layers", ")", ":", "\n", "        ", "bn", ".", "running_mean", "=", "running_mean", "[", "i", "]", "\n", "# Var(x) = $E(x^2) - E(x)^2$.", "\n", "bn", ".", "running_var", "=", "running_square_mean", "[", "i", "]", "-", "bn", ".", "running_mean", "**", "2", "\n", "# Sets the precise bn stats.", "\n", "bn", ".", "momentum", "=", "momentum_actual", "[", "i", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.make_checkpoint_dir": [[21, 35], ["os.path.join", "slowfast.is_master_proc", "fvcore.common.file_io.PathManager.exists", "fvcore.common.file_io.PathManager.mkdirs"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc"], ["def", "make_checkpoint_dir", "(", "path_to_job", ")", ":", "\n", "    ", "\"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"", "\n", "checkpoint_dir", "=", "os", ".", "path", ".", "join", "(", "path_to_job", ",", "\"checkpoints\"", ")", "\n", "# Create the checkpoint dir from the master process", "\n", "if", "du", ".", "is_master_proc", "(", ")", "and", "not", "PathManager", ".", "exists", "(", "checkpoint_dir", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "PathManager", ".", "mkdirs", "(", "checkpoint_dir", ")", "\n", "", "except", "FileExistsError", ":", "\n", "            ", "pass", "\n", "", "", "return", "checkpoint_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_checkpoint_dir": [[37, 44], ["os.path.join"], "function", ["None"], ["", "def", "get_checkpoint_dir", "(", "path_to_job", ")", ":", "\n", "    ", "\"\"\"\n    Get path for storing checkpoints.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"", "\n", "return", "os", ".", "path", ".", "join", "(", "path_to_job", ",", "\"checkpoints\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_path_to_checkpoint": [[46, 55], ["os.path.join", "checkpoint.get_checkpoint_dir"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_checkpoint_dir"], ["", "def", "get_path_to_checkpoint", "(", "path_to_job", ",", "epoch", ")", ":", "\n", "    ", "\"\"\"\n    Get the full path to a checkpoint file.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n        epoch (int): the number of epoch for the checkpoint.\n    \"\"\"", "\n", "name", "=", "\"checkpoint_epoch_{:05d}.pyth\"", ".", "format", "(", "epoch", ")", "\n", "return", "os", ".", "path", ".", "join", "(", "get_checkpoint_dir", "(", "path_to_job", ")", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_last_checkpoint": [[57, 71], ["checkpoint.get_checkpoint_dir", "os.path.join", "fvcore.common.file_io.PathManager.exists", "fvcore.common.file_io.PathManager.ls", "len", "sorted"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_checkpoint_dir"], ["", "def", "get_last_checkpoint", "(", "path_to_job", ")", ":", "\n", "    ", "\"\"\"\n    Get the last checkpoint from the checkpointing folder.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"", "\n", "\n", "d", "=", "get_checkpoint_dir", "(", "path_to_job", ")", "\n", "names", "=", "PathManager", ".", "ls", "(", "d", ")", "if", "PathManager", ".", "exists", "(", "d", ")", "else", "[", "]", "\n", "names", "=", "[", "f", "for", "f", "in", "names", "if", "\"checkpoint\"", "in", "f", "]", "\n", "assert", "len", "(", "names", ")", ">", "0", ",", "\"No checkpoints found in '{}'.\"", ".", "format", "(", "d", ")", "\n", "# Sort the checkpoints by epoch.", "\n", "name", "=", "sorted", "(", "names", ")", "[", "-", "1", "]", "\n", "return", "os", ".", "path", ".", "join", "(", "d", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.has_checkpoint": [[73, 82], ["checkpoint.get_checkpoint_dir", "any", "fvcore.common.file_io.PathManager.exists", "fvcore.common.file_io.PathManager.ls"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_checkpoint_dir"], ["", "def", "has_checkpoint", "(", "path_to_job", ")", ":", "\n", "    ", "\"\"\"\n    Determines if the given directory contains a checkpoint.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"", "\n", "d", "=", "get_checkpoint_dir", "(", "path_to_job", ")", "\n", "files", "=", "PathManager", ".", "ls", "(", "d", ")", "if", "PathManager", ".", "exists", "(", "d", ")", "else", "[", "]", "\n", "return", "any", "(", "\"checkpoint\"", "in", "f", "for", "f", "in", "files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.is_checkpoint_epoch": [[84, 105], ["max"], "function", ["None"], ["", "def", "is_checkpoint_epoch", "(", "cfg", ",", "cur_epoch", ",", "multigrid_schedule", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Determine if a checkpoint should be saved on current epoch.\n    Args:\n        cfg (CfgNode): configs to save.\n        cur_epoch (int): current number of epoch of the model.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"", "\n", "if", "cur_epoch", "+", "1", "==", "cfg", ".", "SOLVER", ".", "MAX_EPOCH", ":", "\n", "        ", "return", "True", "\n", "", "if", "multigrid_schedule", "is", "not", "None", ":", "\n", "        ", "prev_epoch", "=", "0", "\n", "for", "s", "in", "multigrid_schedule", ":", "\n", "            ", "if", "cur_epoch", "<", "s", "[", "-", "1", "]", ":", "\n", "                ", "period", "=", "max", "(", "\n", "(", "s", "[", "-", "1", "]", "-", "prev_epoch", ")", "//", "cfg", ".", "MULTIGRID", ".", "EVAL_FREQ", "+", "1", ",", "1", "\n", ")", "\n", "return", "(", "s", "[", "-", "1", "]", "-", "1", "-", "cur_epoch", ")", "%", "period", "==", "0", "\n", "", "prev_epoch", "=", "s", "[", "-", "1", "]", "\n", "\n", "", "", "return", "(", "cur_epoch", "+", "1", ")", "%", "cfg", ".", "TRAIN", ".", "CHECKPOINT_PERIOD", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.save_checkpoint": [[107, 137], ["fvcore.common.file_io.PathManager.mkdirs", "checkpoint.sub_to_normal_bn", "checkpoint.get_path_to_checkpoint", "slowfast.is_master_proc", "checkpoint.get_checkpoint_dir", "model.module.state_dict", "model.state_dict", "optimizer.state_dict", "cfg.dump", "fvcore.common.file_io.PathManager.open", "torch.save"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.sub_to_normal_bn", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_path_to_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.is_master_proc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_checkpoint_dir"], ["", "def", "save_checkpoint", "(", "path_to_job", ",", "model", ",", "optimizer", ",", "epoch", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Save a checkpoint.\n    Args:\n        model (model): model to save the weight to the checkpoint.\n        optimizer (optim): optimizer to save the historical state.\n        epoch (int): current number of epoch of the model.\n        cfg (CfgNode): configs to save.\n    \"\"\"", "\n", "# Save checkpoints only from the master process.", "\n", "if", "not", "du", ".", "is_master_proc", "(", "cfg", ".", "NUM_GPUS", "*", "cfg", ".", "NUM_SHARDS", ")", ":", "\n", "        ", "return", "\n", "# Ensure that the checkpoint dir exists.", "\n", "", "PathManager", ".", "mkdirs", "(", "get_checkpoint_dir", "(", "path_to_job", ")", ")", "\n", "# Omit the DDP wrapper in the multi-gpu setting.", "\n", "sd", "=", "model", ".", "module", ".", "state_dict", "(", ")", "if", "cfg", ".", "NUM_GPUS", ">", "1", "else", "model", ".", "state_dict", "(", ")", "\n", "normalized_sd", "=", "sub_to_normal_bn", "(", "sd", ")", "\n", "\n", "# Record the state.", "\n", "checkpoint", "=", "{", "\n", "\"epoch\"", ":", "epoch", ",", "\n", "\"model_state\"", ":", "normalized_sd", ",", "\n", "\"optimizer_state\"", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"cfg\"", ":", "cfg", ".", "dump", "(", ")", ",", "\n", "}", "\n", "# Write the checkpoint.", "\n", "path_to_checkpoint", "=", "get_path_to_checkpoint", "(", "path_to_job", ",", "epoch", "+", "1", ")", "\n", "with", "PathManager", ".", "open", "(", "path_to_checkpoint", ",", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "torch", ".", "save", "(", "checkpoint", ",", "f", ")", "\n", "", "return", "path_to_checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.inflate_weight": [[139, 176], ["collections.OrderedDict", "state_dict_2d.items", "v3d.clone", "state_dict_3d.keys", "logger.info", "len", "len", "v2d.unsqueeze().repeat", "logger.info", "v2d.unsqueeze"], "function", ["None"], ["", "def", "inflate_weight", "(", "state_dict_2d", ",", "state_dict_3d", ")", ":", "\n", "    ", "\"\"\"\n    Inflate 2D model weights in state_dict_2d to the 3D model weights in\n    state_dict_3d. The details can be found in:\n    Joao Carreira, and Andrew Zisserman.\n    \"Quo vadis, action recognition? a new model and the kinetics dataset.\"\n    Args:\n        state_dict_2d (OrderedDict): a dict of parameters from a 2D model.\n        state_dict_3d (OrderedDict): a dict of parameters from a 3D model.\n    Returns:\n        state_dict_inflated (OrderedDict): a dict of inflated parameters.\n    \"\"\"", "\n", "state_dict_inflated", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v2d", "in", "state_dict_2d", ".", "items", "(", ")", ":", "\n", "        ", "assert", "k", "in", "state_dict_3d", ".", "keys", "(", ")", "\n", "v3d", "=", "state_dict_3d", "[", "k", "]", "\n", "# Inflate the weight of 2D conv to 3D conv.", "\n", "if", "len", "(", "v2d", ".", "shape", ")", "==", "4", "and", "len", "(", "v3d", ".", "shape", ")", "==", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Inflate {}: {} -> {}: {}\"", ".", "format", "(", "k", ",", "v2d", ".", "shape", ",", "k", ",", "v3d", ".", "shape", ")", "\n", ")", "\n", "# Dimension need to be match.", "\n", "assert", "v2d", ".", "shape", "[", "-", "2", ":", "]", "==", "v3d", ".", "shape", "[", "-", "2", ":", "]", "\n", "assert", "v2d", ".", "shape", "[", ":", "2", "]", "==", "v3d", ".", "shape", "[", ":", "2", "]", "\n", "v3d", "=", "(", "\n", "v2d", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "v3d", ".", "shape", "[", "2", "]", ",", "1", ",", "1", ")", "/", "v3d", ".", "shape", "[", "2", "]", "\n", ")", "\n", "", "elif", "v2d", ".", "shape", "==", "v3d", ".", "shape", ":", "\n", "            ", "v3d", "=", "v2d", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Unexpected {}: {} -|> {}: {}\"", ".", "format", "(", "\n", "k", ",", "v2d", ".", "shape", ",", "k", ",", "v3d", ".", "shape", "\n", ")", "\n", ")", "\n", "", "state_dict_inflated", "[", "k", "]", "=", "v3d", ".", "clone", "(", ")", "\n", "", "return", "state_dict_inflated", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint": [[178, 291], ["fvcore.common.file_io.PathManager.exists", "collections.OrderedDict", "slowfast.utils.c2_model_loading.get_name_convert_func", "caffe2_checkpoint[].keys", "ms.load_state_dict", "checkpoint.normal_to_sub_bn", "fvcore.common.file_io.PathManager.open", "pickle.load", "slowfast.utils.c2_model_loading.get_name_convert_func.", "checkpoint.c2_normal_to_sub_bn", "fvcore.common.file_io.PathManager.open", "torch.load", "model.module.state_dict", "model.state_dict", "checkpoint.inflate_weight", "ms.load_state_dict", "ms.load_state_dict", "torch.load.keys", "ms.state_dict", "ms.state_dict", "optimizer.load_state_dict", "numpy.concatenate", "tuple", "torch.tensor().clone", "logger.warn", "any", "logger.warn", "ms.state_dict", "len", "len", "logger.info", "torch.tensor", "tuple", "tuple"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.c2_model_loading.get_name_convert_func", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.normal_to_sub_bn", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.c2_normal_to_sub_bn", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.inflate_weight", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate"], ["", "def", "load_checkpoint", "(", "\n", "path_to_checkpoint", ",", "\n", "model", ",", "\n", "data_parallel", "=", "True", ",", "\n", "optimizer", "=", "None", ",", "\n", "inflation", "=", "False", ",", "\n", "convert_from_caffe2", "=", "False", ",", "\n", "verbose", "=", "False", ",", "\n", "strict", "=", "True", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Load the checkpoint from the given file. If inflation is True, inflate the\n    2D Conv weights from the checkpoint to 3D Conv.\n    Args:\n        path_to_checkpoint (string): path to the checkpoint to load.\n        model (model): model to load the weights from the checkpoint.\n        data_parallel (bool): if true, model is wrapped by\n        torch.nn.parallel.DistributedDataParallel.\n        optimizer (optim): optimizer to load the historical state.\n        inflation (bool): if True, inflate the weights from the checkpoint.\n        convert_from_caffe2 (bool): if True, load the model from caffe2 and\n            convert it to pytorch.\n    Returns:\n        (int): the number of training epoch of the checkpoint.\n    \"\"\"", "\n", "assert", "PathManager", ".", "exists", "(", "\n", "path_to_checkpoint", "\n", ")", ",", "\"Checkpoint '{}' not found\"", ".", "format", "(", "path_to_checkpoint", ")", "\n", "# Account for the DDP wrapper in the multi-gpu setting.", "\n", "ms", "=", "model", ".", "module", "if", "data_parallel", "else", "model", "\n", "if", "convert_from_caffe2", ":", "\n", "        ", "with", "PathManager", ".", "open", "(", "path_to_checkpoint", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "caffe2_checkpoint", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "\"latin1\"", ")", "\n", "", "state_dict", "=", "OrderedDict", "(", ")", "\n", "name_convert_func", "=", "get_name_convert_func", "(", ")", "\n", "for", "key", "in", "caffe2_checkpoint", "[", "\"blobs\"", "]", ".", "keys", "(", ")", ":", "\n", "            ", "converted_key", "=", "name_convert_func", "(", "key", ")", "\n", "converted_key", "=", "c2_normal_to_sub_bn", "(", "converted_key", ",", "ms", ".", "state_dict", "(", ")", ")", "\n", "if", "converted_key", "in", "ms", ".", "state_dict", "(", ")", ":", "\n", "                ", "c2_blob_shape", "=", "caffe2_checkpoint", "[", "\"blobs\"", "]", "[", "key", "]", ".", "shape", "\n", "model_blob_shape", "=", "ms", ".", "state_dict", "(", ")", "[", "converted_key", "]", ".", "shape", "\n", "# Load BN stats to Sub-BN.", "\n", "if", "(", "\n", "len", "(", "model_blob_shape", ")", "==", "1", "\n", "and", "len", "(", "c2_blob_shape", ")", "==", "1", "\n", "and", "model_blob_shape", "[", "0", "]", ">", "c2_blob_shape", "[", "0", "]", "\n", "and", "model_blob_shape", "[", "0", "]", "%", "c2_blob_shape", "[", "0", "]", "==", "0", "\n", ")", ":", "\n", "                    ", "caffe2_checkpoint", "[", "\"blobs\"", "]", "[", "key", "]", "=", "np", ".", "concatenate", "(", "\n", "[", "caffe2_checkpoint", "[", "\"blobs\"", "]", "[", "key", "]", "]", "\n", "*", "(", "model_blob_shape", "[", "0", "]", "//", "c2_blob_shape", "[", "0", "]", ")", "\n", ")", "\n", "c2_blob_shape", "=", "caffe2_checkpoint", "[", "\"blobs\"", "]", "[", "key", "]", ".", "shape", "\n", "\n", "", "if", "c2_blob_shape", "==", "tuple", "(", "model_blob_shape", ")", ":", "\n", "                    ", "state_dict", "[", "converted_key", "]", "=", "torch", ".", "tensor", "(", "\n", "caffe2_checkpoint", "[", "\"blobs\"", "]", "[", "key", "]", "\n", ")", ".", "clone", "(", ")", "\n", "if", "verbose", ":", "\n", "                        ", "logger", ".", "info", "(", "\n", "\"{}: {} => {}: {}\"", ".", "format", "(", "\n", "key", ",", "\n", "c2_blob_shape", ",", "\n", "converted_key", ",", "\n", "tuple", "(", "model_blob_shape", ")", ",", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"!! {}: {} does not match {}: {}\"", ".", "format", "(", "\n", "key", ",", "\n", "c2_blob_shape", ",", "\n", "converted_key", ",", "\n", "tuple", "(", "model_blob_shape", ")", ",", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "not", "any", "(", "\n", "prefix", "in", "key", "for", "prefix", "in", "[", "\"momentum\"", ",", "\"lr\"", ",", "\"model_iter\"", "]", "\n", ")", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"!! {}: can not be converted, got {}\"", ".", "format", "(", "\n", "key", ",", "converted_key", "\n", ")", "\n", ")", "\n", "", "", "", "ms", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "epoch", "=", "-", "1", "\n", "", "else", ":", "\n", "# Load the checkpoint on CPU to avoid GPU mem spike.", "\n", "        ", "with", "PathManager", ".", "open", "(", "path_to_checkpoint", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "f", ",", "map_location", "=", "\"cpu\"", ")", "\n", "", "model_state_dict_3d", "=", "(", "\n", "model", ".", "module", ".", "state_dict", "(", ")", "if", "data_parallel", "else", "model", ".", "state_dict", "(", ")", "\n", ")", "\n", "checkpoint", "[", "\"model_state\"", "]", "=", "normal_to_sub_bn", "(", "\n", "checkpoint", "[", "\"model_state\"", "]", ",", "model_state_dict_3d", "\n", ")", "\n", "if", "inflation", ":", "\n", "# Try to inflate the model.", "\n", "            ", "inflated_model_dict", "=", "inflate_weight", "(", "\n", "checkpoint", "[", "\"model_state\"", "]", ",", "model_state_dict_3d", "\n", ")", "\n", "ms", ".", "load_state_dict", "(", "inflated_model_dict", ",", "strict", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "ms", ".", "load_state_dict", "(", "checkpoint", "[", "\"model_state\"", "]", ",", "strict", "=", "strict", ")", "\n", "# Load the optimizer state (commonly not done when fine-tuning)", "\n", "if", "optimizer", "and", "\"optimizer_state\"", "in", "checkpoint", ":", "\n", "                ", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "\"optimizer_state\"", "]", ")", "\n", "", "", "if", "\"epoch\"", "in", "checkpoint", ".", "keys", "(", ")", ":", "\n", "            ", "epoch", "=", "checkpoint", "[", "\"epoch\"", "]", "\n", "", "else", ":", "\n", "            ", "epoch", "=", "-", "1", "\n", "", "", "return", "epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.sub_to_normal_bn": [[293, 332], ["copy.deepcopy", "key.endswith", "key.endswith", "key.endswith", "copy.deepcopy.pop", "len", "all", "new_sd[].size", "key.split", "new_sd[].size"], "function", ["None"], ["", "def", "sub_to_normal_bn", "(", "sd", ")", ":", "\n", "    ", "\"\"\"\n    Convert the Sub-BN paprameters to normal BN parameters in a state dict.\n    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and\n    `bn.split_bn`. `bn.split_bn` is used during training and\n    \"compute_precise_bn\". Before saving or evaluation, its stats are copied to\n    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal\n    BN layers.\n    Args:\n        sd (OrderedDict): a dict of parameters whitch might contain Sub-BN\n        parameters.\n    Returns:\n        new_sd (OrderedDict): a dict with Sub-BN parameters reshaped to\n        normal parameters.\n    \"\"\"", "\n", "new_sd", "=", "copy", ".", "deepcopy", "(", "sd", ")", "\n", "modifications", "=", "[", "\n", "(", "\"bn.bn.running_mean\"", ",", "\"bn.running_mean\"", ")", ",", "\n", "(", "\"bn.bn.running_var\"", ",", "\"bn.running_var\"", ")", ",", "\n", "(", "\"bn.split_bn.num_batches_tracked\"", ",", "\"bn.num_batches_tracked\"", ")", ",", "\n", "]", "\n", "to_remove", "=", "[", "\"bn.bn.\"", ",", "\".split_bn.\"", "]", "\n", "for", "key", "in", "sd", ":", "\n", "        ", "for", "before", ",", "after", "in", "modifications", ":", "\n", "            ", "if", "key", ".", "endswith", "(", "before", ")", ":", "\n", "                ", "new_key", "=", "key", ".", "split", "(", "before", ")", "[", "0", "]", "+", "after", "\n", "new_sd", "[", "new_key", "]", "=", "new_sd", ".", "pop", "(", "key", ")", "\n", "\n", "", "", "for", "rm", "in", "to_remove", ":", "\n", "            ", "if", "rm", "in", "key", "and", "key", "in", "new_sd", ":", "\n", "                ", "del", "new_sd", "[", "key", "]", "\n", "\n", "", "", "", "for", "key", "in", "new_sd", ":", "\n", "        ", "if", "key", ".", "endswith", "(", "\"bn.weight\"", ")", "or", "key", ".", "endswith", "(", "\"bn.bias\"", ")", ":", "\n", "            ", "if", "len", "(", "new_sd", "[", "key", "]", ".", "size", "(", ")", ")", "==", "4", ":", "\n", "                ", "assert", "all", "(", "d", "==", "1", "for", "d", "in", "new_sd", "[", "key", "]", ".", "size", "(", ")", "[", "1", ":", "]", ")", "\n", "new_sd", "[", "key", "]", "=", "new_sd", "[", "key", "]", "[", ":", ",", "0", ",", "0", ",", "0", "]", "\n", "\n", "", "", "", "return", "new_sd", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.c2_normal_to_sub_bn": [[334, 352], ["key.replace"], "function", ["None"], ["", "def", "c2_normal_to_sub_bn", "(", "key", ",", "model_keys", ")", ":", "\n", "    ", "\"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        key (OrderedDict): source dict of parameters.\n        mdoel_key (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"", "\n", "if", "\"bn.running_\"", "in", "key", ":", "\n", "        ", "if", "key", "in", "model_keys", ":", "\n", "            ", "return", "key", "\n", "\n", "", "new_key", "=", "key", ".", "replace", "(", "\"bn.running_\"", ",", "\"bn.split_bn.running_\"", ")", "\n", "if", "new_key", "in", "model_keys", ":", "\n", "            ", "return", "new_key", "\n", "", "", "else", ":", "\n", "        ", "return", "key", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.normal_to_sub_bn": [[354, 393], ["key.replace", "key.replace", "checkpoint_sd.pop", "torch.cat", "logger.info", "len", "len"], "function", ["None"], ["", "", "def", "normal_to_sub_bn", "(", "checkpoint_sd", ",", "model_sd", ")", ":", "\n", "    ", "\"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        checkpoint_sd (OrderedDict): source dict of parameters.\n        model_sd (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"", "\n", "for", "key", "in", "model_sd", ":", "\n", "        ", "if", "key", "not", "in", "checkpoint_sd", ":", "\n", "            ", "if", "\"bn.split_bn.\"", "in", "key", ":", "\n", "                ", "load_key", "=", "key", ".", "replace", "(", "\"bn.split_bn.\"", ",", "\"bn.\"", ")", "\n", "bn_key", "=", "key", ".", "replace", "(", "\"bn.split_bn.\"", ",", "\"bn.bn.\"", ")", "\n", "checkpoint_sd", "[", "key", "]", "=", "checkpoint_sd", ".", "pop", "(", "load_key", ")", "\n", "checkpoint_sd", "[", "bn_key", "]", "=", "checkpoint_sd", "[", "key", "]", "\n", "\n", "", "", "", "for", "key", "in", "model_sd", ":", "\n", "        ", "if", "key", "in", "checkpoint_sd", ":", "\n", "            ", "model_blob_shape", "=", "model_sd", "[", "key", "]", ".", "shape", "\n", "c2_blob_shape", "=", "checkpoint_sd", "[", "key", "]", ".", "shape", "\n", "\n", "if", "(", "\n", "len", "(", "model_blob_shape", ")", "==", "1", "\n", "and", "len", "(", "c2_blob_shape", ")", "==", "1", "\n", "and", "model_blob_shape", "[", "0", "]", ">", "c2_blob_shape", "[", "0", "]", "\n", "and", "model_blob_shape", "[", "0", "]", "%", "c2_blob_shape", "[", "0", "]", "==", "0", "\n", ")", ":", "\n", "                ", "before_shape", "=", "checkpoint_sd", "[", "key", "]", ".", "shape", "\n", "checkpoint_sd", "[", "key", "]", "=", "torch", ".", "cat", "(", "\n", "[", "checkpoint_sd", "[", "key", "]", "]", "\n", "*", "(", "model_blob_shape", "[", "0", "]", "//", "c2_blob_shape", "[", "0", "]", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"{} {} -> {}\"", ".", "format", "(", "\n", "key", ",", "before_shape", ",", "checkpoint_sd", "[", "key", "]", ".", "shape", "\n", ")", "\n", ")", "\n", "", "", "", "return", "checkpoint_sd", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint": [[395, 441], ["logger.info", "checkpoint.load_checkpoint", "checkpoint.has_checkpoint", "checkpoint.get_last_checkpoint", "logger.info", "checkpoint.load_checkpoint", "logger.info", "checkpoint.load_checkpoint", "logger.info"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.has_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_last_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint"], ["", "def", "load_test_checkpoint", "(", "cfg", ",", "model", ")", ":", "\n", "    ", "\"\"\"\n    Loading checkpoint logic for testing.\n    \"\"\"", "\n", "# Load a checkpoint to test if applicable.", "\n", "if", "cfg", ".", "TEST", ".", "CHECKPOINT_FILE_PATH", "!=", "\"\"", ":", "\n", "        ", "logger", ".", "info", "(", "\"loading checkpoint from {}\"", ".", "format", "(", "cfg", ".", "TEST", ".", "CHECKPOINT_FILE_PATH", ")", ")", "\n", "# If no checkpoint found in MODEL_VIS.CHECKPOINT_FILE_PATH or in the current", "\n", "# checkpoint folder, try to load checkpoint from", "\n", "# TEST.CHECKPOINT_FILE_PATH and test it.", "\n", "load_checkpoint", "(", "\n", "cfg", ".", "TEST", ".", "CHECKPOINT_FILE_PATH", ",", "\n", "model", ",", "\n", "cfg", ".", "NUM_GPUS", ">", "1", ",", "\n", "None", ",", "\n", "inflation", "=", "False", ",", "\n", "convert_from_caffe2", "=", "cfg", ".", "TEST", ".", "CHECKPOINT_TYPE", "==", "\"caffe2\"", ",", "\n", "verbose", "=", "cfg", ".", "TEST", ".", "CHECKPOINT_VERBOSE", ",", "\n", "strict", "=", "True", "\n", ")", "\n", "", "elif", "has_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ")", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "logger", ".", "info", "(", "\"loading checkpoint from {}\"", ".", "format", "(", "last_checkpoint", ")", ")", "\n", "load_checkpoint", "(", "\n", "last_checkpoint", ",", "\n", "model", ",", "\n", "cfg", ".", "NUM_GPUS", ">", "1", ",", "\n", "verbose", "=", "cfg", ".", "TEST", ".", "CHECKPOINT_VERBOSE", "\n", ")", "\n", "", "elif", "cfg", ".", "TRAIN", ".", "CHECKPOINT_FILE_PATH", "!=", "\"\"", ":", "\n", "# If no checkpoint found in TEST.CHECKPOINT_FILE_PATH or in the current", "\n", "# checkpoint folder, try to load checkpoint from", "\n", "# TRAIN.CHECKPOINT_FILE_PATH and test it.", "\n", "        ", "logger", ".", "info", "(", "\"loading checkpoint from {}\"", ".", "format", "(", "cfg", ".", "TRAIN", ".", "CHECKPOINT_FILE_PATH", ")", ")", "\n", "load_checkpoint", "(", "\n", "cfg", ".", "TRAIN", ".", "CHECKPOINT_FILE_PATH", ",", "\n", "model", ",", "\n", "cfg", ".", "NUM_GPUS", ">", "1", ",", "\n", "None", ",", "\n", "inflation", "=", "False", ",", "\n", "convert_from_caffe2", "=", "cfg", ".", "TRAIN", ".", "CHECKPOINT_TYPE", "==", "\"caffe2\"", ",", "\n", "verbose", "=", "cfg", ".", "TEST", ".", "CHECKPOINT_VERBOSE", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "\"Unknown way of loading checkpoint. Using with random initialization, only for debugging.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_train_checkpoint": [[444, 487], ["checkpoint.has_checkpoint", "checkpoint.get_last_checkpoint", "logger.info", "checkpoint.load_checkpoint", "logger.info", "checkpoint.load_checkpoint"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.has_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.get_last_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_checkpoint"], ["", "", "def", "load_train_checkpoint", "(", "cfg", ",", "model", ",", "optimizer", ")", ":", "\n", "    ", "\"\"\"\n    Loading checkpoint logic for training.\n    \"\"\"", "\n", "if", "cfg", ".", "TRAIN", ".", "AUTO_RESUME", "and", "has_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ")", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "cfg", ".", "OUTPUT_DIR", ")", "\n", "logger", ".", "info", "(", "\"Load from last checkpoint, {}.\"", ".", "format", "(", "last_checkpoint", ")", ")", "\n", "checkpoint_epoch", "=", "load_checkpoint", "(", "\n", "last_checkpoint", ",", "\n", "model", ",", "\n", "cfg", ".", "NUM_GPUS", ">", "1", ",", "\n", "optimizer", ",", "\n", "verbose", "=", "cfg", ".", "TRAIN", ".", "CHECKPOINT_VERBOSE", ",", "\n", "strict", "=", "True", "\n", ")", "\n", "start_epoch", "=", "checkpoint_epoch", "+", "1", "\n", "", "elif", "cfg", ".", "TRAIN", ".", "CHECKPOINT_FILE_PATH", "!=", "\"\"", ":", "\n", "        ", "logger", ".", "info", "(", "\"Load from given checkpoint file.\"", ")", "\n", "checkpoint_epoch", "=", "load_checkpoint", "(", "\n", "cfg", ".", "TRAIN", ".", "CHECKPOINT_FILE_PATH", ",", "\n", "model", ",", "\n", "cfg", ".", "NUM_GPUS", ">", "1", ",", "\n", "None", ",", "\n", "inflation", "=", "cfg", ".", "TRAIN", ".", "CHECKPOINT_INFLATE", ",", "\n", "convert_from_caffe2", "=", "cfg", ".", "TRAIN", ".", "CHECKPOINT_TYPE", "==", "\"caffe2\"", ",", "\n", "verbose", "=", "cfg", ".", "TRAIN", ".", "CHECKPOINT_VERBOSE", ",", "\n", "strict", "=", "False", "\n", ")", "\n", "# checkpoint_epoch = load_checkpoint(", "\n", "#     cfg.TRAIN.CHECKPOINT_FILE_PATH,", "\n", "#     model,", "\n", "#     cfg.NUM_GPUS > 1,", "\n", "#     optimizer=None,", "\n", "#     inflation=cfg.TRAIN.CHECKPOINT_INFLATE,", "\n", "#     convert_from_caffe2=cfg.TRAIN.CHECKPOINT_TYPE == \"caffe2\",", "\n", "#     verbose=cfg.TRAIN.CHECKPOINT_VERBOSE,", "\n", "#     strict=False", "\n", "# )", "\n", "start_epoch", "=", "0", "\n", "", "else", ":", "\n", "        ", "start_epoch", "=", "0", "\n", "\n", "", "return", "start_epoch", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.env.setup_environment": [[11, 16], ["None"], "function", ["None"], ["def", "setup_environment", "(", ")", ":", "\n", "    ", "global", "_ENV_SETUP_DONE", "\n", "if", "_ENV_SETUP_DONE", ":", "\n", "        ", "return", "\n", "", "_ENV_SETUP_DONE", "=", "True", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.__init__": [[37, 66], ["np_box_list.BoxList.__init__", "isinstance", "ValueError", "len", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "self", ",", "box_data", ",", "mask_data", ")", ":", "\n", "        ", "\"\"\"Constructs box collection.\n\n    Args:\n      box_data: a numpy array of shape [N, 4] representing box coordinates\n      mask_data: a numpy array of shape [N, height, width] representing masks\n        with values are in {0,1}. The masks correspond to the full\n        image. The height and the width will be equal to image height and width.\n\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n      ValueError: if mask data is not a numpy array\n      ValueError: if invalid dimension for mask data\n    \"\"\"", "\n", "super", "(", "BoxMaskList", ",", "self", ")", ".", "__init__", "(", "box_data", ")", "\n", "if", "not", "isinstance", "(", "mask_data", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Mask data must be a numpy array.\"", ")", "\n", "", "if", "len", "(", "mask_data", ".", "shape", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid dimensions for mask data.\"", ")", "\n", "", "if", "mask_data", ".", "dtype", "!=", "np", ".", "uint8", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid data type for mask data: uint8 is required.\"", "\n", ")", "\n", "", "if", "mask_data", ".", "shape", "[", "0", "]", "!=", "box_data", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"There should be the same number of boxes and masks.\"", "\n", ")", "\n", "", "self", ".", "data", "[", "\"masks\"", "]", "=", "mask_data", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks": [[67, 74], ["np_box_mask_list.BoxMaskList.get_field"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "get_masks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Convenience function for accessing masks.\n\n    Returns:\n      a numpy array of shape [N, height, width] representing masks\n    \"\"\"", "\n", "return", "self", ".", "get_field", "(", "\"masks\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation.__init__": [[42, 52], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_groundtruth_classes", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"Initialized PerImageEvaluation by evaluation parameters.\n\n    Args:\n      num_groundtruth_classes: Number of ground truth object classes\n      matching_iou_threshold: A ratio of area intersection to union, which is\n          the threshold to consider whether a detection is true positive or not\n    \"\"\"", "\n", "self", ".", "matching_iou_threshold", "=", "matching_iou_threshold", "\n", "self", ".", "num_groundtruth_classes", "=", "num_groundtruth_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation.compute_object_detection_metrics": [[53, 127], ["per_image_evaluation.PerImageEvaluation._remove_invalid_boxes", "per_image_evaluation.PerImageEvaluation._compute_tp_fp"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._remove_invalid_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp"], ["", "def", "compute_object_detection_metrics", "(", "\n", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", ",", "\n", "groundtruth_is_group_of_list", ",", "\n", "detected_masks", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Evaluates detections as being tp, fp or ignored from a single image.\n\n    The evaluation is done in two stages:\n     1. All detections are matched to non group-of boxes; true positives are\n        determined and detections matched to difficult boxes are ignored.\n     2. Detections that are determined as false positives are matched against\n        group-of boxes and ignored if matched.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the metrics will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      scores: A list of C float numpy arrays. Each numpy array is of\n          shape [K, 1], representing K scores detected with object class\n          label c\n      tp_fp_labels: A list of C boolean numpy arrays. Each numpy array\n          is of shape [K, 1], representing K True/False positive label of\n          object instances detected with class label c\n    \"\"\"", "\n", "(", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", ",", "\n", ")", "=", "self", ".", "_remove_invalid_boxes", "(", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", ",", "\n", ")", "\n", "scores", ",", "tp_fp_labels", "=", "self", ".", "_compute_tp_fp", "(", "\n", "detected_boxes", "=", "detected_boxes", ",", "\n", "detected_scores", "=", "detected_scores", ",", "\n", "detected_class_labels", "=", "detected_class_labels", ",", "\n", "groundtruth_boxes", "=", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", "=", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", "=", "groundtruth_is_difficult_list", ",", "\n", "groundtruth_is_group_of_list", "=", "groundtruth_is_group_of_list", ",", "\n", "detected_masks", "=", "detected_masks", ",", "\n", "groundtruth_masks", "=", "groundtruth_masks", ",", "\n", ")", "\n", "\n", "return", "scores", ",", "tp_fp_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp": [[128, 222], ["range", "ValueError", "ValueError", "per_image_evaluation.PerImageEvaluation._get_ith_class_arrays", "per_image_evaluation.PerImageEvaluation._compute_tp_fp_for_single_class", "result_scores.append", "result_tp_fp_labels.append"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_ith_class_arrays", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp_for_single_class"], ["", "def", "_compute_tp_fp", "(", "\n", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", ",", "\n", "groundtruth_is_group_of_list", ",", "\n", "detected_masks", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Labels true/false positives of detections of an image across all classes.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag\n      detected_masks: (optional) A np.uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A np.uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      result_scores: A list of float numpy arrays. Each numpy array is of\n          shape [K, 1], representing K scores detected with object class\n          label c\n      result_tp_fp_labels: A list of boolean numpy array. Each numpy array is of\n          shape [K, 1], representing K True/False positive label of object\n          instances detected with class label c\n\n    Raises:\n      ValueError: If detected masks is not None but groundtruth masks are None,\n        or the other way around.\n    \"\"\"", "\n", "if", "detected_masks", "is", "not", "None", "and", "groundtruth_masks", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Detected masks is available but groundtruth masks is not.\"", "\n", ")", "\n", "", "if", "detected_masks", "is", "None", "and", "groundtruth_masks", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Groundtruth masks is available but detected masks is not.\"", "\n", ")", "\n", "\n", "", "result_scores", "=", "[", "]", "\n", "result_tp_fp_labels", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_groundtruth_classes", ")", ":", "\n", "            ", "groundtruth_is_difficult_list_at_ith_class", "=", "groundtruth_is_difficult_list", "[", "\n", "groundtruth_class_labels", "==", "i", "\n", "]", "\n", "groundtruth_is_group_of_list_at_ith_class", "=", "groundtruth_is_group_of_list", "[", "\n", "groundtruth_class_labels", "==", "i", "\n", "]", "\n", "(", "\n", "gt_boxes_at_ith_class", ",", "\n", "gt_masks_at_ith_class", ",", "\n", "detected_boxes_at_ith_class", ",", "\n", "detected_scores_at_ith_class", ",", "\n", "detected_masks_at_ith_class", ",", "\n", ")", "=", "self", ".", "_get_ith_class_arrays", "(", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_masks", ",", "\n", "detected_class_labels", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_masks", ",", "\n", "groundtruth_class_labels", ",", "\n", "i", ",", "\n", ")", "\n", "scores", ",", "tp_fp_labels", "=", "self", ".", "_compute_tp_fp_for_single_class", "(", "\n", "detected_boxes", "=", "detected_boxes_at_ith_class", ",", "\n", "detected_scores", "=", "detected_scores_at_ith_class", ",", "\n", "groundtruth_boxes", "=", "gt_boxes_at_ith_class", ",", "\n", "groundtruth_is_difficult_list", "=", "groundtruth_is_difficult_list_at_ith_class", ",", "\n", "groundtruth_is_group_of_list", "=", "groundtruth_is_group_of_list_at_ith_class", ",", "\n", "detected_masks", "=", "detected_masks_at_ith_class", ",", "\n", "groundtruth_masks", "=", "gt_masks_at_ith_class", ",", "\n", ")", "\n", "result_scores", ".", "append", "(", "scores", ")", "\n", "result_tp_fp_labels", ".", "append", "(", "tp_fp_labels", ")", "\n", "", "return", "result_scores", ",", "result_tp_fp_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_overlaps_and_scores_box_mode": [[223, 260], ["np_box_list.BoxList", "np_box_list.BoxList.add_field", "np_box_list.BoxList", "np_box_list_ops.iou", "np_box_list.BoxList.get_field", "np_box_list.BoxList.num_boxes"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.iou", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes"], ["", "def", "_get_overlaps_and_scores_box_mode", "(", "\n", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_is_group_of_list", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Computes overlaps and scores between detected and groudntruth boxes.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n\n    Returns:\n      iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_non_group_of_boxlist.num_boxes() == 0 it will be None.\n      ioa: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_group_of_boxlist.num_boxes() == 0 it will be None.\n      scores: The score of the detected boxlist.\n      num_boxes: Number of non-maximum suppressed detected boxes.\n    \"\"\"", "\n", "detected_boxlist", "=", "np_box_list", ".", "BoxList", "(", "detected_boxes", ")", "\n", "detected_boxlist", ".", "add_field", "(", "\"scores\"", ",", "detected_scores", ")", "\n", "gt_non_group_of_boxlist", "=", "np_box_list", ".", "BoxList", "(", "\n", "groundtruth_boxes", "[", "~", "groundtruth_is_group_of_list", "]", "\n", ")", "\n", "iou", "=", "np_box_list_ops", ".", "iou", "(", "detected_boxlist", ",", "gt_non_group_of_boxlist", ")", "\n", "scores", "=", "detected_boxlist", ".", "get_field", "(", "\"scores\"", ")", "\n", "num_boxes", "=", "detected_boxlist", ".", "num_boxes", "(", ")", "\n", "return", "iou", ",", "None", ",", "scores", ",", "num_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp_for_single_class": [[261, 351], ["per_image_evaluation.PerImageEvaluation._get_overlaps_and_scores_box_mode", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.argmax", "numpy.zeros", "range", "numpy.array", "numpy.array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_overlaps_and_scores_box_mode"], ["", "def", "_compute_tp_fp_for_single_class", "(", "\n", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_is_difficult_list", ",", "\n", "groundtruth_is_group_of_list", ",", "\n", "detected_masks", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Labels boxes detected with the same class from the same image as tp/fp.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not. If a\n          groundtruth box is difficult, every detection matching this box\n          is ignored.\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      Two arrays of the same size, containing all boxes that were evaluated as\n      being true positives or false positives; if a box matched to a difficult\n      box or to a group-of box, it is ignored.\n\n      scores: A numpy array representing the detection scores.\n      tp_fp_labels: a boolean numpy array indicating whether a detection is a\n          true positive.\n    \"\"\"", "\n", "if", "detected_boxes", ".", "size", "==", "0", ":", "\n", "            ", "return", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "float", ")", ",", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "\n", "", "(", "\n", "iou", ",", "\n", "_", ",", "\n", "scores", ",", "\n", "num_detected_boxes", ",", "\n", ")", "=", "self", ".", "_get_overlaps_and_scores_box_mode", "(", "\n", "detected_boxes", "=", "detected_boxes", ",", "\n", "detected_scores", "=", "detected_scores", ",", "\n", "groundtruth_boxes", "=", "groundtruth_boxes", ",", "\n", "groundtruth_is_group_of_list", "=", "groundtruth_is_group_of_list", ",", "\n", ")", "\n", "\n", "if", "groundtruth_boxes", ".", "size", "==", "0", ":", "\n", "            ", "return", "scores", ",", "np", ".", "zeros", "(", "num_detected_boxes", ",", "dtype", "=", "bool", ")", "\n", "\n", "", "tp_fp_labels", "=", "np", ".", "zeros", "(", "num_detected_boxes", ",", "dtype", "=", "bool", ")", "\n", "is_matched_to_difficult_box", "=", "np", ".", "zeros", "(", "num_detected_boxes", ",", "dtype", "=", "bool", ")", "\n", "is_matched_to_group_of_box", "=", "np", ".", "zeros", "(", "num_detected_boxes", ",", "dtype", "=", "bool", ")", "\n", "\n", "# The evaluation is done in two stages:", "\n", "# 1. All detections are matched to non group-of boxes; true positives are", "\n", "#    determined and detections matched to difficult boxes are ignored.", "\n", "# 2. Detections that are determined as false positives are matched against", "\n", "#    group-of boxes and ignored if matched.", "\n", "\n", "# Tp-fp evaluation for non-group of boxes (if any).", "\n", "if", "iou", ".", "shape", "[", "1", "]", ">", "0", ":", "\n", "            ", "groundtruth_nongroup_of_is_difficult_list", "=", "groundtruth_is_difficult_list", "[", "\n", "~", "groundtruth_is_group_of_list", "\n", "]", "\n", "max_overlap_gt_ids", "=", "np", ".", "argmax", "(", "iou", ",", "axis", "=", "1", ")", "\n", "is_gt_box_detected", "=", "np", ".", "zeros", "(", "iou", ".", "shape", "[", "1", "]", ",", "dtype", "=", "bool", ")", "\n", "for", "i", "in", "range", "(", "num_detected_boxes", ")", ":", "\n", "                ", "gt_id", "=", "max_overlap_gt_ids", "[", "i", "]", "\n", "if", "iou", "[", "i", ",", "gt_id", "]", ">=", "self", ".", "matching_iou_threshold", ":", "\n", "                    ", "if", "not", "groundtruth_nongroup_of_is_difficult_list", "[", "gt_id", "]", ":", "\n", "                        ", "if", "not", "is_gt_box_detected", "[", "gt_id", "]", ":", "\n", "                            ", "tp_fp_labels", "[", "i", "]", "=", "True", "\n", "is_gt_box_detected", "[", "gt_id", "]", "=", "True", "\n", "", "", "else", ":", "\n", "                        ", "is_matched_to_difficult_box", "[", "i", "]", "=", "True", "\n", "\n", "", "", "", "", "return", "(", "\n", "scores", "[", "~", "is_matched_to_difficult_box", "&", "~", "is_matched_to_group_of_box", "]", ",", "\n", "tp_fp_labels", "[", "\n", "~", "is_matched_to_difficult_box", "&", "~", "is_matched_to_group_of_box", "\n", "]", ",", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_ith_class_arrays": [[354, 409], ["None"], "methods", ["None"], ["", "def", "_get_ith_class_arrays", "(", "\n", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_masks", ",", "\n", "detected_class_labels", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_masks", ",", "\n", "groundtruth_class_labels", ",", "\n", "class_index", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Returns numpy arrays belonging to class with index `class_index`.\n\n    Args:\n      detected_boxes: A numpy array containing detected boxes.\n      detected_scores: A numpy array containing detected scores.\n      detected_masks: A numpy array containing detected masks.\n      detected_class_labels: A numpy array containing detected class labels.\n      groundtruth_boxes: A numpy array containing groundtruth boxes.\n      groundtruth_masks: A numpy array containing groundtruth masks.\n      groundtruth_class_labels: A numpy array containing groundtruth class\n        labels.\n      class_index: An integer index.\n\n    Returns:\n      gt_boxes_at_ith_class: A numpy array containing groundtruth boxes labeled\n        as ith class.\n      gt_masks_at_ith_class: A numpy array containing groundtruth masks labeled\n        as ith class.\n      detected_boxes_at_ith_class: A numpy array containing detected boxes\n        corresponding to the ith class.\n      detected_scores_at_ith_class: A numpy array containing detected scores\n        corresponding to the ith class.\n      detected_masks_at_ith_class: A numpy array containing detected masks\n        corresponding to the ith class.\n    \"\"\"", "\n", "selected_groundtruth", "=", "groundtruth_class_labels", "==", "class_index", "\n", "gt_boxes_at_ith_class", "=", "groundtruth_boxes", "[", "selected_groundtruth", "]", "\n", "if", "groundtruth_masks", "is", "not", "None", ":", "\n", "            ", "gt_masks_at_ith_class", "=", "groundtruth_masks", "[", "selected_groundtruth", "]", "\n", "", "else", ":", "\n", "            ", "gt_masks_at_ith_class", "=", "None", "\n", "", "selected_detections", "=", "detected_class_labels", "==", "class_index", "\n", "detected_boxes_at_ith_class", "=", "detected_boxes", "[", "selected_detections", "]", "\n", "detected_scores_at_ith_class", "=", "detected_scores", "[", "selected_detections", "]", "\n", "if", "detected_masks", "is", "not", "None", ":", "\n", "            ", "detected_masks_at_ith_class", "=", "detected_masks", "[", "selected_detections", "]", "\n", "", "else", ":", "\n", "            ", "detected_masks_at_ith_class", "=", "None", "\n", "", "return", "(", "\n", "gt_boxes_at_ith_class", ",", "\n", "gt_masks_at_ith_class", ",", "\n", "detected_boxes_at_ith_class", ",", "\n", "detected_scores_at_ith_class", ",", "\n", "detected_masks_at_ith_class", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation._remove_invalid_boxes": [[411, 453], ["numpy.logical_and"], "methods", ["None"], ["", "def", "_remove_invalid_boxes", "(", "\n", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Removes entries with invalid boxes.\n\n    A box is invalid if either its xmax is smaller than its xmin, or its ymax\n    is smaller than its ymin.\n\n    Args:\n      detected_boxes: A float numpy array of size [num_boxes, 4] containing box\n        coordinates in [ymin, xmin, ymax, xmax] format.\n      detected_scores: A float numpy array of size [num_boxes].\n      detected_class_labels: A int32 numpy array of size [num_boxes].\n      detected_masks: A uint8 numpy array of size [num_boxes, height, width].\n\n    Returns:\n      valid_detected_boxes: A float numpy array of size [num_valid_boxes, 4]\n        containing box coordinates in [ymin, xmin, ymax, xmax] format.\n      valid_detected_scores: A float numpy array of size [num_valid_boxes].\n      valid_detected_class_labels: A int32 numpy array of size\n        [num_valid_boxes].\n      valid_detected_masks: A uint8 numpy array of size\n        [num_valid_boxes, height, width].\n    \"\"\"", "\n", "valid_indices", "=", "np", ".", "logical_and", "(", "\n", "detected_boxes", "[", ":", ",", "0", "]", "<", "detected_boxes", "[", ":", ",", "2", "]", ",", "\n", "detected_boxes", "[", ":", ",", "1", "]", "<", "detected_boxes", "[", ":", ",", "3", "]", ",", "\n", ")", "\n", "detected_boxes", "=", "detected_boxes", "[", "valid_indices", "]", "\n", "detected_scores", "=", "detected_scores", "[", "valid_indices", "]", "\n", "detected_class_labels", "=", "detected_class_labels", "[", "valid_indices", "]", "\n", "if", "detected_masks", "is", "not", "None", ":", "\n", "            ", "detected_masks", "=", "detected_masks", "[", "valid_indices", "]", "\n", "", "return", "[", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.__init__": [[39, 63], ["isinstance", "ValueError", "ValueError", "ValueError", "np_box_list.BoxList._is_valid_boxes", "ValueError", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList._is_valid_boxes"], ["def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "\"\"\"Constructs box collection.\n\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "data", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"data must be a numpy array.\"", ")", "\n", "", "if", "len", "(", "data", ".", "shape", ")", "!=", "2", "or", "data", ".", "shape", "[", "1", "]", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid dimensions for box data.\"", ")", "\n", "", "if", "data", ".", "dtype", "!=", "np", ".", "float32", "and", "data", ".", "dtype", "!=", "np", ".", "float64", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid data type for box data: float is required.\"", "\n", ")", "\n", "", "if", "not", "self", ".", "_is_valid_boxes", "(", "data", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid box data. data must be a numpy array of \"", "\n", "\"N*[y_min, x_min, y_max, x_max]\"", "\n", ")", "\n", "", "self", ".", "data", "=", "{", "\"boxes\"", ":", "data", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes": [[64, 67], ["None"], "methods", ["None"], ["", "def", "num_boxes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return number of boxes held in collections.\"\"\"", "\n", "return", "self", ".", "data", "[", "\"boxes\"", "]", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_extra_fields": [[68, 71], ["np_box_list.BoxList.data.keys"], "methods", ["None"], ["", "def", "get_extra_fields", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return all non-box fields.\"\"\"", "\n", "return", "[", "k", "for", "k", "in", "self", ".", "data", ".", "keys", "(", ")", "if", "k", "!=", "\"boxes\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field": [[72, 74], ["None"], "methods", ["None"], ["", "def", "has_field", "(", "self", ",", "field", ")", ":", "\n", "        ", "return", "field", "in", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field": [[75, 91], ["np_box_list.BoxList.has_field", "ValueError", "ValueError", "len", "np_box_list.BoxList.num_boxes"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes"], ["", "def", "add_field", "(", "self", ",", "field", ",", "field_data", ")", ":", "\n", "        ", "\"\"\"Add data to a specified field.\n\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n      field_data: a numpy array of [N, ...] representing the data associated\n          with the field.\n    Raises:\n      ValueError: if the field is already exist or the dimension of the field\n          data does not matches the number of boxes.\n    \"\"\"", "\n", "if", "self", ".", "has_field", "(", "field", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Field \"", "+", "field", "+", "\"already exists\"", ")", "\n", "", "if", "len", "(", "field_data", ".", "shape", ")", "<", "1", "or", "field_data", ".", "shape", "[", "0", "]", "!=", "self", ".", "num_boxes", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid dimensions for field data\"", ")", "\n", "", "self", ".", "data", "[", "field", "]", "=", "field_data", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get": [[92, 99], ["np_box_list.BoxList.get_field"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "get", "(", "self", ")", ":", "\n", "        ", "\"\"\"Convenience function for accesssing box coordinates.\n\n    Returns:\n      a numpy array of shape [N, 4] representing box corners\n    \"\"\"", "\n", "return", "self", ".", "get_field", "(", "\"boxes\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field": [[100, 115], ["np_box_list.BoxList.has_field", "ValueError"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field"], ["", "def", "get_field", "(", "self", ",", "field", ")", ":", "\n", "        ", "\"\"\"Accesses data associated with the specified field in the box collection.\n\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n\n    Returns:\n      a numpy 1-d array representing data of an associated field\n\n    Raises:\n      ValueError: if invalid field\n    \"\"\"", "\n", "if", "not", "self", ".", "has_field", "(", "field", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"field {} does not exist\"", ".", "format", "(", "field", ")", ")", "\n", "", "return", "self", ".", "data", "[", "field", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_coordinates": [[116, 128], ["np_box_list.BoxList.get"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "get_coordinates", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get corner coordinates of boxes.\n\n    Returns:\n     a list of 4 1-d numpy arrays [y_min, x_min, y_max, x_max]\n    \"\"\"", "\n", "box_coordinates", "=", "self", ".", "get", "(", ")", "\n", "y_min", "=", "box_coordinates", "[", ":", ",", "0", "]", "\n", "x_min", "=", "box_coordinates", "[", ":", ",", "1", "]", "\n", "y_max", "=", "box_coordinates", "[", ":", ",", "2", "]", "\n", "x_max", "=", "box_coordinates", "[", ":", ",", "3", "]", "\n", "return", "[", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList._is_valid_boxes": [[129, 144], ["range"], "methods", ["None"], ["", "def", "_is_valid_boxes", "(", "self", ",", "data", ")", ":", "\n", "        ", "\"\"\"Check whether data fullfills the format of N*[ymin, xmin, ymax, xmin].\n\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n\n    Returns:\n      a boolean indicating whether all ymax of boxes are equal or greater than\n          ymin, and all xmax of boxes are equal or greater than xmin.\n    \"\"\"", "\n", "if", "data", ".", "shape", "[", "0", "]", ">", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "data", "[", "i", ",", "0", "]", ">", "data", "[", "i", ",", "2", "]", "or", "data", "[", "i", ",", "1", "]", ">", "data", "[", "i", ",", "3", "]", ":", "\n", "                    ", "return", "False", "\n", "", "", "", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util._validate_label_map": [[29, 41], ["ValueError"], "function", ["None"], ["def", "_validate_label_map", "(", "label_map", ")", ":", "\n", "    ", "\"\"\"Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  \"\"\"", "\n", "for", "item", "in", "label_map", ".", "item", ":", "\n", "        ", "if", "item", ".", "id", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Label map ids should be >= 1.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.create_category_index": [[43, 60], ["None"], "function", ["None"], ["", "", "", "def", "create_category_index", "(", "categories", ")", ":", "\n", "    ", "\"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      'id': (required) an integer id uniquely identifying this category.\n      'name': (required) string representing category name\n        e.g., 'cat', 'dog', 'pizza'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the 'id' field of each category.\n  \"\"\"", "\n", "category_index", "=", "{", "}", "\n", "for", "cat", "in", "categories", ":", "\n", "        ", "category_index", "[", "cat", "[", "\"id\"", "]", "]", "=", "cat", "\n", "", "return", "category_index", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.get_max_label_map_index": [[62, 72], ["max"], "function", ["None"], ["", "def", "get_max_label_map_index", "(", "label_map", ")", ":", "\n", "    ", "\"\"\"Get maximum index in label map.\n\n  Args:\n    label_map: a StringIntLabelMapProto\n\n  Returns:\n    an integer\n  \"\"\"", "\n", "return", "max", "(", "[", "item", ".", "id", "for", "item", "in", "label_map", ".", "item", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.convert_label_map_to_categories": [[74, 127], ["range", "categories.append", "logging.info", "item.HasField", "list_of_ids_already_added.append", "categories.append"], "function", ["None"], ["", "def", "convert_label_map_to_categories", "(", "\n", "label_map", ",", "max_num_classes", ",", "use_display_name", "=", "True", "\n", ")", ":", "\n", "    ", "\"\"\"Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    'id': (required) an integer id uniquely identifying this category.\n    'name': (required) string representing category name\n      e.g., 'cat', 'dog', 'pizza'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load 'display_name' field\n      as category name.  If False or if the display_name field does not exist,\n      uses 'name' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  \"\"\"", "\n", "categories", "=", "[", "]", "\n", "list_of_ids_already_added", "=", "[", "]", "\n", "if", "not", "label_map", ":", "\n", "        ", "label_id_offset", "=", "1", "\n", "for", "class_id", "in", "range", "(", "max_num_classes", ")", ":", "\n", "            ", "categories", ".", "append", "(", "\n", "{", "\n", "\"id\"", ":", "class_id", "+", "label_id_offset", ",", "\n", "\"name\"", ":", "\"category_{}\"", ".", "format", "(", "class_id", "+", "label_id_offset", ")", ",", "\n", "}", "\n", ")", "\n", "", "return", "categories", "\n", "", "for", "item", "in", "label_map", ".", "item", ":", "\n", "        ", "if", "not", "0", "<", "item", ".", "id", "<=", "max_num_classes", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "\"Ignore item %d since it falls outside of requested \"", "\n", "\"label range.\"", ",", "\n", "item", ".", "id", ",", "\n", ")", "\n", "continue", "\n", "", "if", "use_display_name", "and", "item", ".", "HasField", "(", "\"display_name\"", ")", ":", "\n", "            ", "name", "=", "item", ".", "display_name", "\n", "", "else", ":", "\n", "            ", "name", "=", "item", ".", "name", "\n", "", "if", "item", ".", "id", "not", "in", "list_of_ids_already_added", ":", "\n", "            ", "list_of_ids_already_added", ".", "append", "(", "item", ".", "id", ")", "\n", "categories", ".", "append", "(", "{", "\"id\"", ":", "item", ".", "id", ",", "\"name\"", ":", "name", "}", ")", "\n", "", "", "return", "categories", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.load_labelmap": [[129, 146], ["label_map_util._validate_label_map", "open", "fid.read", "string_int_label_map_pb2.StringIntLabelMap", "text_format.Merge", "string_int_label_map_pb2.StringIntLabelMap.ParseFromString"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util._validate_label_map"], ["", "def", "load_labelmap", "(", "path", ")", ":", "\n", "    ", "\"\"\"Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  \"\"\"", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "fid", ":", "\n", "        ", "label_map_string", "=", "fid", ".", "read", "(", ")", "\n", "label_map", "=", "string_int_label_map_pb2", ".", "StringIntLabelMap", "(", ")", "\n", "try", ":", "\n", "            ", "text_format", ".", "Merge", "(", "label_map_string", ",", "label_map", ")", "\n", "", "except", "text_format", ".", "ParseError", ":", "\n", "            ", "label_map", ".", "ParseFromString", "(", "label_map_string", ")", "\n", "", "", "_validate_label_map", "(", "label_map", ")", "\n", "return", "label_map", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.get_label_map_dict": [[148, 166], ["label_map_util.load_labelmap"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.load_labelmap"], ["", "def", "get_label_map_dict", "(", "label_map_path", ",", "use_display_name", "=", "False", ")", ":", "\n", "    ", "\"\"\"Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n    use_display_name: whether to use the label map items' display names as keys.\n\n  Returns:\n    A dictionary mapping label names to id.\n  \"\"\"", "\n", "label_map", "=", "load_labelmap", "(", "label_map_path", ")", "\n", "label_map_dict", "=", "{", "}", "\n", "for", "item", "in", "label_map", ".", "item", ":", "\n", "        ", "if", "use_display_name", ":", "\n", "            ", "label_map_dict", "[", "item", ".", "display_name", "]", "=", "item", ".", "id", "\n", "", "else", ":", "\n", "            ", "label_map_dict", "[", "item", ".", "name", "]", "=", "item", ".", "id", "\n", "", "", "return", "label_map_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.create_category_index_from_labelmap": [[168, 183], ["label_map_util.load_labelmap", "max", "label_map_util.convert_label_map_to_categories", "label_map_util.create_category_index"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.load_labelmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.convert_label_map_to_categories", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.create_category_index"], ["", "def", "create_category_index_from_labelmap", "(", "label_map_path", ")", ":", "\n", "    ", "\"\"\"Reads a label map and returns a category index.\n\n  Args:\n    label_map_path: Path to `StringIntLabelMap` proto text file.\n\n  Returns:\n    A category index, which is a dictionary that maps integer ids to dicts\n    containing categories, e.g.\n    {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}, ...}\n  \"\"\"", "\n", "label_map", "=", "load_labelmap", "(", "label_map_path", ")", "\n", "max_num_classes", "=", "max", "(", "item", ".", "id", "for", "item", "in", "label_map", ".", "item", ")", "\n", "categories", "=", "convert_label_map_to_categories", "(", "label_map", ",", "max_num_classes", ")", "\n", "return", "create_category_index", "(", "categories", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.create_class_agnostic_category_index": [[185, 188], ["None"], "function", ["None"], ["", "def", "create_class_agnostic_category_index", "(", ")", ":", "\n", "    ", "\"\"\"Creates a category index with a single `object` class.\"\"\"", "\n", "return", "{", "1", ":", "{", "\"id\"", ":", "1", ",", "\"name\"", ":", "\"object\"", "}", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_precision_recall": [[21, 72], ["numpy.argsort", "labels.astype.astype", "numpy.cumsum", "numpy.cumsum", "ValueError", "ValueError", "numpy.sum", "ValueError", "len", "len", "ValueError", "np.cumsum.astype", "np.cumsum.astype", "isinstance", "len", "isinstance", "len"], "function", ["None"], ["\n", "assert", "preds", ".", "size", "(", "0", ")", "==", "labels", ".", "size", "(", "\n", "0", "\n", ")", ",", "\"Batch dim of predictions and labels must match\"", "\n", "# Find the top max_k predictions for each sample", "\n", "_top_max_k_vals", ",", "top_max_k_inds", "=", "torch", ".", "topk", "(", "\n", "preds", ",", "max", "(", "ks", ")", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", "\n", ")", "\n", "# (batch_size, max_k) -> (max_k, batch_size).", "\n", "top_max_k_inds", "=", "top_max_k_inds", ".", "t", "(", ")", "\n", "# (batch_size, ) -> (max_k, batch_size).", "\n", "rep_max_k_labels", "=", "labels", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "top_max_k_inds", ")", "\n", "# (i, j) = 1 if top i-th prediction for the j-th sample is correct.", "\n", "top_max_k_correct", "=", "top_max_k_inds", ".", "eq", "(", "rep_max_k_labels", ")", "\n", "# Compute the number of topk correct predictions for each k.", "\n", "topks_correct", "=", "[", "\n", "top_max_k_correct", "[", ":", "k", ",", ":", "]", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "for", "k", "in", "ks", "\n", "]", "\n", "return", "topks_correct", "\n", "\n", "\n", "", "def", "topk_errors", "(", "preds", ",", "labels", ",", "ks", ")", ":", "\n", "    ", "\"\"\"\n    Computes the top-k error for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"", "\n", "num_topks_correct", "=", "topks_correct", "(", "preds", ",", "labels", ",", "ks", ")", "\n", "return", "[", "(", "1.0", "-", "x", "/", "preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "for", "x", "in", "num_topks_correct", "]", "\n", "\n", "\n", "", "def", "topk_accuracies", "(", "preds", ",", "labels", ",", "ks", ")", ":", "\n", "    ", "\"\"\"\n    Computes the top-k accuracy for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"", "\n", "num_topks_correct", "=", "topks_correct", "(", "preds", ",", "labels", ",", "ks", ")", "\n", "return", "[", "(", "x", "/", "preds", ".", "size", "(", "0", ")", ")", "*", "100.0", "for", "x", "in", "num_topks_correct", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_average_precision": [[74, 126], ["numpy.concatenate", "numpy.concatenate", "range", "numpy.sum", "ValueError", "ValueError", "len", "len", "ValueError", "ValueError", "ValueError", "all", "ValueError", "numpy.maximum", "ValueError", "isinstance", "isinstance", "numpy.amin", "numpy.amax", "numpy.amin", "numpy.amax", "len", "numpy.where", "range", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate"], []], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_cor_loc": [[128, 154], ["numpy.errstate", "numpy.where"], "function", ["None"], []], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.area": [[45, 56], ["boxlist.get_coordinates"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_coordinates"], ["", "def", "area", "(", "boxlist", ")", ":", "\n", "    ", "\"\"\"Computes area of boxes.\n\n  Args:\n    boxlist: BoxList holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  \"\"\"", "\n", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "=", "boxlist", ".", "get_coordinates", "(", ")", "\n", "return", "(", "y_max", "-", "y_min", ")", "*", "(", "x_max", "-", "x_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.intersection": [[58, 69], ["np_box_ops.intersection", "boxlist1.get", "boxlist2.get"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "intersection", "(", "boxlist1", ",", "boxlist2", ")", ":", "\n", "    ", "\"\"\"Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"", "\n", "return", "np_box_ops", ".", "intersection", "(", "boxlist1", ".", "get", "(", ")", ",", "boxlist2", ".", "get", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.iou": [[71, 82], ["np_box_ops.iou", "boxlist1.get", "boxlist2.get"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.iou", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "iou", "(", "boxlist1", ",", "boxlist2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"", "\n", "return", "np_box_ops", ".", "iou", "(", "boxlist1", ".", "get", "(", ")", ",", "boxlist2", ".", "get", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.ioa": [[84, 99], ["np_box_ops.ioa", "boxlist1.get", "boxlist2.get"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.ioa", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "ioa", "(", "boxlist1", ",", "boxlist2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2's area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"", "\n", "return", "np_box_ops", ".", "ioa", "(", "boxlist1", ".", "get", "(", ")", ",", "boxlist2", ".", "get", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.gather": [[101, 134], ["np_box_list.BoxList", "boxlist.get_extra_fields", "boxlist.get_field", "np_box_list.BoxList.add_field", "ValueError", "boxlist.get", "numpy.amax", "boxlist.num_boxes", "numpy.amin"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes"], ["", "def", "gather", "(", "boxlist", ",", "indices", ",", "fields", "=", "None", ")", ":", "\n", "    ", "\"\"\"Gather boxes from BoxList according to indices and return new BoxList.\n\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n        specified by indices\n\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n        indices are not of type int_\n  \"\"\"", "\n", "if", "indices", ".", "size", ":", "\n", "        ", "if", "np", ".", "amax", "(", "indices", ")", ">=", "boxlist", ".", "num_boxes", "(", ")", "or", "np", ".", "amin", "(", "indices", ")", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"indices are out of valid range.\"", ")", "\n", "", "", "subboxlist", "=", "np_box_list", ".", "BoxList", "(", "boxlist", ".", "get", "(", ")", "[", "indices", ",", ":", "]", ")", "\n", "if", "fields", "is", "None", ":", "\n", "        ", "fields", "=", "boxlist", ".", "get_extra_fields", "(", ")", "\n", "", "for", "field", "in", "fields", ":", "\n", "        ", "extra_field_data", "=", "boxlist", ".", "get_field", "(", "field", ")", "\n", "subboxlist", ".", "add_field", "(", "field", ",", "extra_field_data", "[", "indices", ",", "...", "]", ")", "\n", "", "return", "subboxlist", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.sort_by_field": [[136, 165], ["boxlist.get_field", "numpy.argsort", "np_box_list_ops.gather", "boxlist.has_field", "ValueError", "len", "ValueError", "ValueError", "boxlist.get_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "sort_by_field", "(", "boxlist", ",", "field", ",", "order", "=", "SortOrder", ".", "DESCEND", ")", ":", "\n", "    ", "\"\"\"Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) 'descend' or 'ascend'. Default is descend.\n\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n\n  Raises:\n    ValueError: if specified field does not exist or is not of single dimension.\n    ValueError: if the order is not either descend or ascend.\n  \"\"\"", "\n", "if", "not", "boxlist", ".", "has_field", "(", "field", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Field \"", "+", "field", "+", "\" does not exist\"", ")", "\n", "", "if", "len", "(", "boxlist", ".", "get_field", "(", "field", ")", ".", "shape", ")", "!=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Field \"", "+", "field", "+", "\"should be single dimension.\"", ")", "\n", "", "if", "order", "!=", "SortOrder", ".", "DESCEND", "and", "order", "!=", "SortOrder", ".", "ASCEND", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid sort order\"", ")", "\n", "\n", "", "field_to_sort", "=", "boxlist", ".", "get_field", "(", "field", ")", "\n", "sorted_indices", "=", "np", ".", "argsort", "(", "field_to_sort", ")", "\n", "if", "order", "==", "SortOrder", ".", "DESCEND", ":", "\n", "        ", "sorted_indices", "=", "sorted_indices", "[", ":", ":", "-", "1", "]", "\n", "", "return", "gather", "(", "boxlist", ",", "sorted_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.non_max_suppression": [[167, 240], ["np_box_list_ops.filter_scores_greater_than", "np_box_list_ops.sort_by_field", "sort_by_field.get", "sort_by_field.num_boxes", "numpy.full", "range", "np_box_list_ops.gather", "sort_by_field.has_field", "ValueError", "ValueError", "ValueError", "sort_by_field.num_boxes", "numpy.array", "sort_by_field.num_boxes", "numpy.arange", "np_box_list_ops.gather", "np.arange.append", "np_box_ops.iou", "numpy.squeeze", "numpy.logical_and", "numpy.where", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.filter_scores_greater_than", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.sort_by_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.iou"], ["", "def", "non_max_suppression", "(", "\n", "boxlist", ",", "max_output_size", "=", "10000", ",", "iou_threshold", "=", "1.0", ",", "score_threshold", "=", "-", "10.0", "\n", ")", ":", "\n", "    ", "\"\"\"Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n      representing detection scores. All scores belong to the same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if 'scores' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  \"\"\"", "\n", "if", "not", "boxlist", ".", "has_field", "(", "\"scores\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Field scores does not exist\"", ")", "\n", "", "if", "iou_threshold", "<", "0.0", "or", "iou_threshold", ">", "1.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"IOU threshold must be in [0, 1]\"", ")", "\n", "", "if", "max_output_size", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"max_output_size must be bigger than 0.\"", ")", "\n", "\n", "", "boxlist", "=", "filter_scores_greater_than", "(", "boxlist", ",", "score_threshold", ")", "\n", "if", "boxlist", ".", "num_boxes", "(", ")", "==", "0", ":", "\n", "        ", "return", "boxlist", "\n", "\n", "", "boxlist", "=", "sort_by_field", "(", "boxlist", ",", "\"scores\"", ")", "\n", "\n", "# Prevent further computation if NMS is disabled.", "\n", "if", "iou_threshold", "==", "1.0", ":", "\n", "        ", "if", "boxlist", ".", "num_boxes", "(", ")", ">", "max_output_size", ":", "\n", "            ", "selected_indices", "=", "np", ".", "arange", "(", "max_output_size", ")", "\n", "return", "gather", "(", "boxlist", ",", "selected_indices", ")", "\n", "", "else", ":", "\n", "            ", "return", "boxlist", "\n", "\n", "", "", "boxes", "=", "boxlist", ".", "get", "(", ")", "\n", "num_boxes", "=", "boxlist", ".", "num_boxes", "(", ")", "\n", "# is_index_valid is True only for all remaining valid boxes,", "\n", "is_index_valid", "=", "np", ".", "full", "(", "num_boxes", ",", "1", ",", "dtype", "=", "bool", ")", "\n", "selected_indices", "=", "[", "]", "\n", "num_output", "=", "0", "\n", "for", "i", "in", "range", "(", "num_boxes", ")", ":", "\n", "        ", "if", "num_output", "<", "max_output_size", ":", "\n", "            ", "if", "is_index_valid", "[", "i", "]", ":", "\n", "                ", "num_output", "+=", "1", "\n", "selected_indices", ".", "append", "(", "i", ")", "\n", "is_index_valid", "[", "i", "]", "=", "False", "\n", "valid_indices", "=", "np", ".", "where", "(", "is_index_valid", ")", "[", "0", "]", "\n", "if", "valid_indices", ".", "size", "==", "0", ":", "\n", "                    ", "break", "\n", "\n", "", "intersect_over_union", "=", "np_box_ops", ".", "iou", "(", "\n", "np", ".", "expand_dims", "(", "boxes", "[", "i", ",", ":", "]", ",", "axis", "=", "0", ")", ",", "boxes", "[", "valid_indices", ",", ":", "]", "\n", ")", "\n", "intersect_over_union", "=", "np", ".", "squeeze", "(", "intersect_over_union", ",", "axis", "=", "0", ")", "\n", "is_index_valid", "[", "valid_indices", "]", "=", "np", ".", "logical_and", "(", "\n", "is_index_valid", "[", "valid_indices", "]", ",", "\n", "intersect_over_union", "<=", "iou_threshold", ",", "\n", ")", "\n", "", "", "", "return", "gather", "(", "boxlist", ",", "np", ".", "array", "(", "selected_indices", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.multi_class_non_max_suppression": [[242, 319], ["boxlist.get_field", "boxlist.num_boxes", "range", "np_box_list_ops.concatenate", "np_box_list_ops.sort_by_field", "ValueError", "isinstance", "ValueError", "boxlist.has_field", "ValueError", "len", "numpy.reshape", "ValueError", "np_box_list.BoxList", "numpy.reshape", "np_box_list.BoxList.add_field", "np_box_list_ops.filter_scores_greater_than", "np_box_list_ops.non_max_suppression", "non_max_suppression.add_field", "selected_boxes_list.append", "len", "ValueError", "boxlist.get", "ValueError", "numpy.zeros_like", "non_max_suppression.get_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.sort_by_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.filter_scores_greater_than", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.non_max_suppression", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "multi_class_non_max_suppression", "(", "\n", "boxlist", ",", "score_thresh", ",", "iou_thresh", ",", "max_output_size", "\n", ")", ":", "\n", "    ", "\"\"\"Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n      representing detection scores.  This scores field is a tensor that can\n      be 1 dimensional (in the case of a single class) or 2-dimensional, which\n      which case we assume that it takes the shape [num_boxes, num_classes].\n      We further assume that this rank is known statically and that\n      scores.shape[1] is also known (i.e., the number of classes is fixed\n      and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n\n  Returns:\n    a BoxList holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have\n      a valid scores field.\n  \"\"\"", "\n", "if", "not", "0", "<=", "iou_thresh", "<=", "1.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"thresh must be between 0 and 1\"", ")", "\n", "", "if", "not", "isinstance", "(", "boxlist", ",", "np_box_list", ".", "BoxList", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"boxlist must be a BoxList\"", ")", "\n", "", "if", "not", "boxlist", ".", "has_field", "(", "\"scores\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"input boxlist must have 'scores' field\"", ")", "\n", "", "scores", "=", "boxlist", ".", "get_field", "(", "\"scores\"", ")", "\n", "if", "len", "(", "scores", ".", "shape", ")", "==", "1", ":", "\n", "        ", "scores", "=", "np", ".", "reshape", "(", "scores", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "", "elif", "len", "(", "scores", ".", "shape", ")", "==", "2", ":", "\n", "        ", "if", "scores", ".", "shape", "[", "1", "]", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"scores field must have statically defined second \"", "\"dimension\"", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"scores field must be of rank 1 or 2\"", ")", "\n", "", "num_boxes", "=", "boxlist", ".", "num_boxes", "(", ")", "\n", "num_scores", "=", "scores", ".", "shape", "[", "0", "]", "\n", "num_classes", "=", "scores", ".", "shape", "[", "1", "]", "\n", "\n", "if", "num_boxes", "!=", "num_scores", ":", "\n", "        ", "raise", "ValueError", "(", "\"Incorrect scores field length: actual vs expected.\"", ")", "\n", "\n", "", "selected_boxes_list", "=", "[", "]", "\n", "for", "class_idx", "in", "range", "(", "num_classes", ")", ":", "\n", "        ", "boxlist_and_class_scores", "=", "np_box_list", ".", "BoxList", "(", "boxlist", ".", "get", "(", ")", ")", "\n", "class_scores", "=", "np", ".", "reshape", "(", "scores", "[", "0", ":", "num_scores", ",", "class_idx", "]", ",", "[", "-", "1", "]", ")", "\n", "boxlist_and_class_scores", ".", "add_field", "(", "\"scores\"", ",", "class_scores", ")", "\n", "boxlist_filt", "=", "filter_scores_greater_than", "(", "\n", "boxlist_and_class_scores", ",", "score_thresh", "\n", ")", "\n", "nms_result", "=", "non_max_suppression", "(", "\n", "boxlist_filt", ",", "\n", "max_output_size", "=", "max_output_size", ",", "\n", "iou_threshold", "=", "iou_thresh", ",", "\n", "score_threshold", "=", "score_thresh", ",", "\n", ")", "\n", "nms_result", ".", "add_field", "(", "\n", "\"classes\"", ",", "np", ".", "zeros_like", "(", "nms_result", ".", "get_field", "(", "\"scores\"", ")", ")", "+", "class_idx", "\n", ")", "\n", "selected_boxes_list", ".", "append", "(", "nms_result", ")", "\n", "", "selected_boxes", "=", "concatenate", "(", "selected_boxes_list", ")", "\n", "sorted_boxes", "=", "sort_by_field", "(", "selected_boxes", ",", "\"scores\"", ")", "\n", "return", "sorted_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.scale": [[321, 347], ["numpy.array_split", "np_box_list.BoxList", "boxlist.get_extra_fields", "boxlist.get", "numpy.hstack", "boxlist.get_field", "np_box_list.BoxList.add_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field"], ["", "def", "scale", "(", "boxlist", ",", "y_scale", ",", "x_scale", ")", ":", "\n", "    ", "\"\"\"Scale box coordinates in x and y dimensions.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: float\n    x_scale: float\n\n  Returns:\n    boxlist: BoxList holding N boxes\n  \"\"\"", "\n", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "=", "np", ".", "array_split", "(", "boxlist", ".", "get", "(", ")", ",", "4", ",", "axis", "=", "1", ")", "\n", "y_min", "=", "y_scale", "*", "y_min", "\n", "y_max", "=", "y_scale", "*", "y_max", "\n", "x_min", "=", "x_scale", "*", "x_min", "\n", "x_max", "=", "x_scale", "*", "x_max", "\n", "scaled_boxlist", "=", "np_box_list", ".", "BoxList", "(", "\n", "np", ".", "hstack", "(", "[", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "]", ")", "\n", ")", "\n", "\n", "fields", "=", "boxlist", ".", "get_extra_fields", "(", ")", "\n", "for", "field", "in", "fields", ":", "\n", "        ", "extra_field_data", "=", "boxlist", ".", "get_field", "(", "field", ")", "\n", "scaled_boxlist", ".", "add_field", "(", "field", ",", "extra_field_data", ")", "\n", "\n", "", "return", "scaled_boxlist", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.clip_to_window": [[349, 383], ["numpy.array_split", "numpy.fmax", "numpy.fmax", "numpy.fmax", "numpy.fmax", "np_box_list.BoxList", "np_box_list_ops._copy_extra_fields", "np_box_list_ops.area", "numpy.reshape().astype", "np_box_list_ops.gather", "boxlist.get", "numpy.fmin", "numpy.fmin", "numpy.fmin", "numpy.fmin", "numpy.hstack", "numpy.reshape", "numpy.nonzero", "numpy.greater"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops._copy_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "clip_to_window", "(", "boxlist", ",", "window", ")", ":", "\n", "    ", "\"\"\"Clip bounding boxes to a window.\n\n  This op clips input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a numpy array of shape [4] representing the\n            [y_min, x_min, y_max, x_max] window to which the op\n            should clip boxes.\n\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  \"\"\"", "\n", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "=", "np", ".", "array_split", "(", "boxlist", ".", "get", "(", ")", ",", "4", ",", "axis", "=", "1", ")", "\n", "win_y_min", "=", "window", "[", "0", "]", "\n", "win_x_min", "=", "window", "[", "1", "]", "\n", "win_y_max", "=", "window", "[", "2", "]", "\n", "win_x_max", "=", "window", "[", "3", "]", "\n", "y_min_clipped", "=", "np", ".", "fmax", "(", "np", ".", "fmin", "(", "y_min", ",", "win_y_max", ")", ",", "win_y_min", ")", "\n", "y_max_clipped", "=", "np", ".", "fmax", "(", "np", ".", "fmin", "(", "y_max", ",", "win_y_max", ")", ",", "win_y_min", ")", "\n", "x_min_clipped", "=", "np", ".", "fmax", "(", "np", ".", "fmin", "(", "x_min", ",", "win_x_max", ")", ",", "win_x_min", ")", "\n", "x_max_clipped", "=", "np", ".", "fmax", "(", "np", ".", "fmin", "(", "x_max", ",", "win_x_max", ")", ",", "win_x_min", ")", "\n", "clipped", "=", "np_box_list", ".", "BoxList", "(", "\n", "np", ".", "hstack", "(", "[", "y_min_clipped", ",", "x_min_clipped", ",", "y_max_clipped", ",", "x_max_clipped", "]", ")", "\n", ")", "\n", "clipped", "=", "_copy_extra_fields", "(", "clipped", ",", "boxlist", ")", "\n", "areas", "=", "area", "(", "clipped", ")", "\n", "nonzero_area_indices", "=", "np", ".", "reshape", "(", "\n", "np", ".", "nonzero", "(", "np", ".", "greater", "(", "areas", ",", "0.0", ")", ")", ",", "[", "-", "1", "]", "\n", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "return", "gather", "(", "clipped", ",", "nonzero_area_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.prune_non_overlapping_boxes": [[385, 408], ["np_box_list_ops.ioa", "numpy.amax", "numpy.greater_equal", "np_box_list_ops.gather", "numpy.array", "numpy.nonzero"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.ioa", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather"], ["", "def", "prune_non_overlapping_boxes", "(", "boxlist1", ",", "boxlist2", ",", "minoverlap", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n\n  Returns:\n    A pruned boxlist with size [N', 4].\n  \"\"\"", "\n", "intersection_over_area", "=", "ioa", "(", "boxlist2", ",", "boxlist1", ")", "# [M, N] tensor", "\n", "intersection_over_area", "=", "np", ".", "amax", "(", "\n", "intersection_over_area", ",", "axis", "=", "0", "\n", ")", "# [N] tensor", "\n", "keep_bool", "=", "np", ".", "greater_equal", "(", "intersection_over_area", ",", "np", ".", "array", "(", "minoverlap", ")", ")", "\n", "keep_inds", "=", "np", ".", "nonzero", "(", "keep_bool", ")", "[", "0", "]", "\n", "new_boxlist1", "=", "gather", "(", "boxlist1", ",", "keep_inds", ")", "\n", "return", "new_boxlist1", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.prune_outside_window": [[410, 446], ["numpy.array_split", "numpy.hstack", "numpy.reshape", "boxlist.get", "numpy.where", "np_box_list_ops.gather", "numpy.less", "numpy.less", "numpy.greater", "numpy.greater", "numpy.logical_not", "numpy.max"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather"], ["", "def", "prune_outside_window", "(", "boxlist", ",", "window", ")", ":", "\n", "    ", "\"\"\"Prunes bounding boxes that fall outside a given window.\n\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also ClipToWindow which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n            of the window.\n\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  \"\"\"", "\n", "\n", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "=", "np", ".", "array_split", "(", "boxlist", ".", "get", "(", ")", ",", "4", ",", "axis", "=", "1", ")", "\n", "win_y_min", "=", "window", "[", "0", "]", "\n", "win_x_min", "=", "window", "[", "1", "]", "\n", "win_y_max", "=", "window", "[", "2", "]", "\n", "win_x_max", "=", "window", "[", "3", "]", "\n", "coordinate_violations", "=", "np", ".", "hstack", "(", "\n", "[", "\n", "np", ".", "less", "(", "y_min", ",", "win_y_min", ")", ",", "\n", "np", ".", "less", "(", "x_min", ",", "win_x_min", ")", ",", "\n", "np", ".", "greater", "(", "y_max", ",", "win_y_max", ")", ",", "\n", "np", ".", "greater", "(", "x_max", ",", "win_x_max", ")", ",", "\n", "]", "\n", ")", "\n", "valid_indices", "=", "np", ".", "reshape", "(", "\n", "np", ".", "where", "(", "np", ".", "logical_not", "(", "np", ".", "max", "(", "coordinate_violations", ",", "axis", "=", "1", ")", ")", ")", ",", "[", "-", "1", "]", "\n", ")", "\n", "return", "gather", "(", "boxlist", ",", "valid_indices", ")", ",", "valid_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.concatenate": [[448, 501], ["np_box_list.BoxList", "isinstance", "ValueError", "ValueError", "numpy.vstack", "boxlists[].get_extra_fields", "numpy.concatenate", "np_box_list.BoxList.add_field", "isinstance", "ValueError", "boxlists[].get_field", "boxlist.get", "boxlist.has_field", "ValueError", "boxlist.get_field", "ValueError", "boxlist.get_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "concatenate", "(", "boxlists", ",", "fields", "=", "None", ")", ":", "\n", "    ", "\"\"\"Concatenate list of BoxLists.\n\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  \"\"\"", "\n", "if", "not", "isinstance", "(", "boxlists", ",", "list", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"boxlists should be a list\"", ")", "\n", "", "if", "not", "boxlists", ":", "\n", "        ", "raise", "ValueError", "(", "\"boxlists should have nonzero length\"", ")", "\n", "", "for", "boxlist", "in", "boxlists", ":", "\n", "        ", "if", "not", "isinstance", "(", "boxlist", ",", "np_box_list", ".", "BoxList", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"all elements of boxlists should be BoxList objects\"", "\n", ")", "\n", "", "", "concatenated", "=", "np_box_list", ".", "BoxList", "(", "\n", "np", ".", "vstack", "(", "[", "boxlist", ".", "get", "(", ")", "for", "boxlist", "in", "boxlists", "]", ")", "\n", ")", "\n", "if", "fields", "is", "None", ":", "\n", "        ", "fields", "=", "boxlists", "[", "0", "]", ".", "get_extra_fields", "(", ")", "\n", "", "for", "field", "in", "fields", ":", "\n", "        ", "first_field_shape", "=", "boxlists", "[", "0", "]", ".", "get_field", "(", "field", ")", ".", "shape", "\n", "first_field_shape", "=", "first_field_shape", "[", "1", ":", "]", "\n", "for", "boxlist", "in", "boxlists", ":", "\n", "            ", "if", "not", "boxlist", ".", "has_field", "(", "field", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"boxlist must contain all requested fields\"", ")", "\n", "", "field_shape", "=", "boxlist", ".", "get_field", "(", "field", ")", ".", "shape", "\n", "field_shape", "=", "field_shape", "[", "1", ":", "]", "\n", "if", "field_shape", "!=", "first_field_shape", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"field %s must have same shape for all boxlists \"", "\n", "\"except for the 0th dimension.\"", "%", "field", "\n", ")", "\n", "", "", "concatenated_field", "=", "np", ".", "concatenate", "(", "\n", "[", "boxlist", ".", "get_field", "(", "field", ")", "for", "boxlist", "in", "boxlists", "]", ",", "axis", "=", "0", "\n", ")", "\n", "concatenated", ".", "add_field", "(", "field", ",", "concatenated_field", ")", "\n", "", "return", "concatenated", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.filter_scores_greater_than": [[503, 537], ["boxlist.get_field", "numpy.reshape().astype", "np_box_list_ops.gather", "isinstance", "ValueError", "boxlist.has_field", "ValueError", "len", "ValueError", "ValueError", "len", "numpy.reshape", "numpy.where", "numpy.greater"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field"], ["", "def", "filter_scores_greater_than", "(", "boxlist", ",", "thresh", ")", ":", "\n", "    ", "\"\"\"Filter to keep only boxes with score exceeding a given threshold.\n\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n      representing detection scores.\n    thresh: scalar threshold\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  \"\"\"", "\n", "if", "not", "isinstance", "(", "boxlist", ",", "np_box_list", ".", "BoxList", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"boxlist must be a BoxList\"", ")", "\n", "", "if", "not", "boxlist", ".", "has_field", "(", "\"scores\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"input boxlist must have 'scores' field\"", ")", "\n", "", "scores", "=", "boxlist", ".", "get_field", "(", "\"scores\"", ")", "\n", "if", "len", "(", "scores", ".", "shape", ")", ">", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Scores should have rank 1 or 2\"", ")", "\n", "", "if", "len", "(", "scores", ".", "shape", ")", "==", "2", "and", "scores", ".", "shape", "[", "1", "]", "!=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Scores should have rank 1 or have shape \"", "\n", "\"consistent with [None, 1]\"", "\n", ")", "\n", "", "high_score_indices", "=", "np", ".", "reshape", "(", "\n", "np", ".", "where", "(", "np", ".", "greater", "(", "scores", ",", "thresh", ")", ")", ",", "[", "-", "1", "]", "\n", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "return", "gather", "(", "boxlist", ",", "high_score_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops.change_coordinate_frame": [[539, 570], ["np_box_list_ops.scale", "np_box_list_ops._copy_extra_fields", "np_box_list.BoxList", "boxlist.get"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops._copy_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "change_coordinate_frame", "(", "boxlist", ",", "window", ")", ":", "\n", "    ", "\"\"\"Change coordinate frame of the boxlist to be relative to window's frame.\n\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: a size 4 1-D numpy array.\n\n  Returns:\n    Returns a BoxList object with N boxes.\n  \"\"\"", "\n", "win_height", "=", "window", "[", "2", "]", "-", "window", "[", "0", "]", "\n", "win_width", "=", "window", "[", "3", "]", "-", "window", "[", "1", "]", "\n", "boxlist_new", "=", "scale", "(", "\n", "np_box_list", ".", "BoxList", "(", "\n", "boxlist", ".", "get", "(", ")", "-", "[", "window", "[", "0", "]", ",", "window", "[", "1", "]", ",", "window", "[", "0", "]", ",", "window", "[", "1", "]", "]", "\n", ")", ",", "\n", "1.0", "/", "win_height", ",", "\n", "1.0", "/", "win_width", ",", "\n", ")", "\n", "_copy_extra_fields", "(", "boxlist_new", ",", "boxlist", ")", "\n", "\n", "return", "boxlist_new", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops._copy_extra_fields": [[572, 587], ["boxlist_to_copy_from.get_extra_fields", "boxlist_to_copy_to.add_field", "boxlist_to_copy_from.get_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "_copy_extra_fields", "(", "boxlist_to_copy_to", ",", "boxlist_to_copy_from", ")", ":", "\n", "    ", "\"\"\"Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  \"\"\"", "\n", "for", "field", "in", "boxlist_to_copy_from", ".", "get_extra_fields", "(", ")", ":", "\n", "        ", "boxlist_to_copy_to", ".", "add_field", "(", "\n", "field", ",", "boxlist_to_copy_from", ".", "get_field", "(", "field", ")", "\n", ")", "\n", "", "return", "boxlist_to_copy_to", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list_ops._update_valid_indices_by_removing_high_iou_boxes": [[589, 594], ["numpy.max", "numpy.logical_and"], "function", ["None"], ["", "def", "_update_valid_indices_by_removing_high_iou_boxes", "(", "\n", "selected_indices", ",", "is_index_valid", ",", "intersect_over_union", ",", "threshold", "\n", ")", ":", "\n", "    ", "max_iou", "=", "np", ".", "max", "(", "intersect_over_union", "[", ":", ",", "selected_indices", "]", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "logical_and", "(", "is_index_valid", ",", "max_iou", "<=", "threshold", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_ops.area": [[31, 41], ["None"], "function", ["None"], ["def", "area", "(", "boxes", ")", ":", "\n", "    ", "\"\"\"Computes area of boxes.\n\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  \"\"\"", "\n", "return", "(", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", ")", "*", "(", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_ops.intersection": [[43, 69], ["numpy.split", "numpy.split", "numpy.minimum", "numpy.maximum", "numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.maximum", "numpy.transpose", "numpy.transpose", "numpy.zeros", "numpy.transpose", "numpy.transpose", "numpy.zeros"], "function", ["None"], ["", "def", "intersection", "(", "boxes1", ",", "boxes2", ")", ":", "\n", "    ", "\"\"\"Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"", "\n", "[", "y_min1", ",", "x_min1", ",", "y_max1", ",", "x_max1", "]", "=", "np", ".", "split", "(", "boxes1", ",", "4", ",", "axis", "=", "1", ")", "\n", "[", "y_min2", ",", "x_min2", ",", "y_max2", ",", "x_max2", "]", "=", "np", ".", "split", "(", "boxes2", ",", "4", ",", "axis", "=", "1", ")", "\n", "\n", "all_pairs_min_ymax", "=", "np", ".", "minimum", "(", "y_max1", ",", "np", ".", "transpose", "(", "y_max2", ")", ")", "\n", "all_pairs_max_ymin", "=", "np", ".", "maximum", "(", "y_min1", ",", "np", ".", "transpose", "(", "y_min2", ")", ")", "\n", "intersect_heights", "=", "np", ".", "maximum", "(", "\n", "np", ".", "zeros", "(", "all_pairs_max_ymin", ".", "shape", ")", ",", "\n", "all_pairs_min_ymax", "-", "all_pairs_max_ymin", ",", "\n", ")", "\n", "all_pairs_min_xmax", "=", "np", ".", "minimum", "(", "x_max1", ",", "np", ".", "transpose", "(", "x_max2", ")", ")", "\n", "all_pairs_max_xmin", "=", "np", ".", "maximum", "(", "x_min1", ",", "np", ".", "transpose", "(", "x_min2", ")", ")", "\n", "intersect_widths", "=", "np", ".", "maximum", "(", "\n", "np", ".", "zeros", "(", "all_pairs_max_xmin", ".", "shape", ")", ",", "\n", "all_pairs_min_xmax", "-", "all_pairs_max_xmin", ",", "\n", ")", "\n", "return", "intersect_heights", "*", "intersect_widths", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_ops.iou": [[71, 90], ["np_box_ops.intersection", "np_box_ops.area", "np_box_ops.area", "numpy.expand_dims", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area"], ["", "def", "iou", "(", "boxes1", ",", "boxes2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"", "\n", "intersect", "=", "intersection", "(", "boxes1", ",", "boxes2", ")", "\n", "area1", "=", "area", "(", "boxes1", ")", "\n", "area2", "=", "area", "(", "boxes2", ")", "\n", "union", "=", "(", "\n", "np", ".", "expand_dims", "(", "area1", ",", "axis", "=", "1", ")", "\n", "+", "np", ".", "expand_dims", "(", "area2", ",", "axis", "=", "0", ")", "\n", "-", "intersect", "\n", ")", "\n", "return", "intersect", "/", "union", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_ops.ioa": [[92, 109], ["np_box_ops.intersection", "numpy.expand_dims", "np_box_ops.area"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area"], ["", "def", "ioa", "(", "boxes1", ",", "boxes2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2's area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"", "\n", "intersect", "=", "intersection", "(", "boxes1", ",", "boxes2", ")", "\n", "areas", "=", "np", ".", "expand_dims", "(", "area", "(", "boxes2", ")", ",", "axis", "=", "0", ")", "\n", "return", "intersect", "/", "areas", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.box_list_to_box_mask_list": [[33, 55], ["np_box_mask_list.BoxMaskList", "boxlist.get_extra_fields", "boxlist.has_field", "ValueError", "boxlist.get", "boxlist.get_field", "boxlist.get_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_extra_fields", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["def", "box_list_to_box_mask_list", "(", "boxlist", ")", ":", "\n", "    ", "\"\"\"Converts a BoxList containing 'masks' into a BoxMaskList.\n\n  Args:\n    boxlist: An np_box_list.BoxList object.\n\n  Returns:\n    An np_box_mask_list.BoxMaskList object.\n\n  Raises:\n    ValueError: If boxlist does not contain `masks` as a field.\n  \"\"\"", "\n", "if", "not", "boxlist", ".", "has_field", "(", "\"masks\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"boxlist does not contain mask field.\"", ")", "\n", "", "box_mask_list", "=", "np_box_mask_list", ".", "BoxMaskList", "(", "\n", "box_data", "=", "boxlist", ".", "get", "(", ")", ",", "mask_data", "=", "boxlist", ".", "get_field", "(", "\"masks\"", ")", "\n", ")", "\n", "extra_fields", "=", "boxlist", ".", "get_extra_fields", "(", ")", "\n", "for", "key", "in", "extra_fields", ":", "\n", "        ", "if", "key", "!=", "\"masks\"", ":", "\n", "            ", "box_mask_list", ".", "data", "[", "key", "]", "=", "boxlist", ".", "get_field", "(", "key", ")", "\n", "", "", "return", "box_mask_list", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.area": [[57, 67], ["np_mask_ops.area", "box_mask_list.get_masks"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks"], ["", "def", "area", "(", "box_mask_list", ")", ":", "\n", "    ", "\"\"\"Computes area of masks.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes and masks\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas\n  \"\"\"", "\n", "return", "np_mask_ops", ".", "area", "(", "box_mask_list", ".", "get_masks", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.intersection": [[69, 81], ["np_mask_ops.intersection", "box_mask_list1.get_masks", "box_mask_list2.get_masks"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks"], ["", "def", "intersection", "(", "box_mask_list1", ",", "box_mask_list2", ")", ":", "\n", "    ", "\"\"\"Compute pairwise intersection areas between masks.\n\n  Args:\n    box_mask_list1: BoxMaskList holding N boxes and masks\n    box_mask_list2: BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"", "\n", "return", "np_mask_ops", ".", "intersection", "(", "\n", "box_mask_list1", ".", "get_masks", "(", ")", ",", "box_mask_list2", ".", "get_masks", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.iou": [[84, 96], ["np_mask_ops.iou", "box_mask_list1.get_masks", "box_mask_list2.get_masks"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.iou", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks"], ["", "def", "iou", "(", "box_mask_list1", ",", "box_mask_list2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-union between box and mask collections.\n\n  Args:\n    box_mask_list1: BoxMaskList holding N boxes and masks\n    box_mask_list2: BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"", "\n", "return", "np_mask_ops", ".", "iou", "(", "\n", "box_mask_list1", ".", "get_masks", "(", ")", ",", "box_mask_list2", ".", "get_masks", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.ioa": [[99, 115], ["np_mask_ops.ioa", "box_mask_list1.get_masks", "box_mask_list2.get_masks"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.ioa", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks"], ["", "def", "ioa", "(", "box_mask_list1", ",", "box_mask_list2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-area between box and mask collections.\n\n  Intersection-over-area (ioa) between two masks mask1 and mask2 is defined as\n  their intersection area over mask2's area. Note that ioa is not symmetric,\n  that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n\n  Args:\n    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks\n    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"", "\n", "return", "np_mask_ops", ".", "ioa", "(", "\n", "box_mask_list1", ".", "get_masks", "(", ")", ",", "box_mask_list2", ".", "get_masks", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather": [[118, 147], ["np_box_mask_list_ops.box_list_to_box_mask_list", "np_box_list_ops.gather", "fields.append"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.box_list_to_box_mask_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather"], ["", "def", "gather", "(", "box_mask_list", ",", "indices", ",", "fields", "=", "None", ")", ":", "\n", "    ", "\"\"\"Gather boxes from np_box_mask_list.BoxMaskList according to indices.\n\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the box_mask_list (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n\n  Returns:\n    subbox_mask_list: a np_box_mask_list.BoxMaskList corresponding to the subset\n        of the input box_mask_list specified by indices\n\n  Raises:\n    ValueError: if specified field is not contained in box_mask_list or if the\n        indices are not of type int_\n  \"\"\"", "\n", "if", "fields", "is", "not", "None", ":", "\n", "        ", "if", "\"masks\"", "not", "in", "fields", ":", "\n", "            ", "fields", ".", "append", "(", "\"masks\"", ")", "\n", "", "", "return", "box_list_to_box_mask_list", "(", "\n", "np_box_list_ops", ".", "gather", "(", "\n", "boxlist", "=", "box_mask_list", ",", "indices", "=", "indices", ",", "fields", "=", "fields", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.sort_by_field": [[151, 170], ["np_box_mask_list_ops.box_list_to_box_mask_list", "np_box_list_ops.sort_by_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.box_list_to_box_mask_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.sort_by_field"], ["", "def", "sort_by_field", "(", "\n", "box_mask_list", ",", "field", ",", "order", "=", "np_box_list_ops", ".", "SortOrder", ".", "DESCEND", "\n", ")", ":", "\n", "    ", "\"\"\"Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    box_mask_list: BoxMaskList holding N boxes.\n    field: A BoxMaskList field for sorting and reordering the BoxMaskList.\n    order: (Optional) 'descend' or 'ascend'. Default is descend.\n\n  Returns:\n    sorted_box_mask_list: A sorted BoxMaskList with the field in the specified\n      order.\n  \"\"\"", "\n", "return", "box_list_to_box_mask_list", "(", "\n", "np_box_list_ops", ".", "sort_by_field", "(", "\n", "boxlist", "=", "box_mask_list", ",", "field", "=", "field", ",", "order", "=", "order", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.non_max_suppression": [[174, 253], ["np_box_mask_list_ops.filter_scores_greater_than", "np_box_mask_list_ops.sort_by_field", "sort_by_field.get_masks", "sort_by_field.num_boxes", "numpy.full", "range", "np_box_mask_list_ops.gather", "sort_by_field.has_field", "ValueError", "ValueError", "ValueError", "sort_by_field.num_boxes", "numpy.array", "sort_by_field.num_boxes", "numpy.arange", "np_box_mask_list_ops.gather", "np.arange.append", "np_mask_ops.iou", "numpy.squeeze", "numpy.logical_and", "numpy.where", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.filter_scores_greater_than", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.sort_by_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.iou"], ["", "def", "non_max_suppression", "(", "\n", "box_mask_list", ",", "\n", "max_output_size", "=", "10000", ",", "\n", "iou_threshold", "=", "1.0", ",", "\n", "score_threshold", "=", "-", "10.0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain\n      a 'scores' field representing detection scores. All scores belong to the\n      same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n\n  Returns:\n    an np_box_mask_list.BoxMaskList holding M boxes where M <= max_output_size\n\n  Raises:\n    ValueError: if 'scores' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  \"\"\"", "\n", "if", "not", "box_mask_list", ".", "has_field", "(", "\"scores\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Field scores does not exist\"", ")", "\n", "", "if", "iou_threshold", "<", "0.0", "or", "iou_threshold", ">", "1.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"IOU threshold must be in [0, 1]\"", ")", "\n", "", "if", "max_output_size", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"max_output_size must be bigger than 0.\"", ")", "\n", "\n", "", "box_mask_list", "=", "filter_scores_greater_than", "(", "box_mask_list", ",", "score_threshold", ")", "\n", "if", "box_mask_list", ".", "num_boxes", "(", ")", "==", "0", ":", "\n", "        ", "return", "box_mask_list", "\n", "\n", "", "box_mask_list", "=", "sort_by_field", "(", "box_mask_list", ",", "\"scores\"", ")", "\n", "\n", "# Prevent further computation if NMS is disabled.", "\n", "if", "iou_threshold", "==", "1.0", ":", "\n", "        ", "if", "box_mask_list", ".", "num_boxes", "(", ")", ">", "max_output_size", ":", "\n", "            ", "selected_indices", "=", "np", ".", "arange", "(", "max_output_size", ")", "\n", "return", "gather", "(", "box_mask_list", ",", "selected_indices", ")", "\n", "", "else", ":", "\n", "            ", "return", "box_mask_list", "\n", "\n", "", "", "masks", "=", "box_mask_list", ".", "get_masks", "(", ")", "\n", "num_masks", "=", "box_mask_list", ".", "num_boxes", "(", ")", "\n", "\n", "# is_index_valid is True only for all remaining valid boxes,", "\n", "is_index_valid", "=", "np", ".", "full", "(", "num_masks", ",", "1", ",", "dtype", "=", "bool", ")", "\n", "selected_indices", "=", "[", "]", "\n", "num_output", "=", "0", "\n", "for", "i", "in", "range", "(", "num_masks", ")", ":", "\n", "        ", "if", "num_output", "<", "max_output_size", ":", "\n", "            ", "if", "is_index_valid", "[", "i", "]", ":", "\n", "                ", "num_output", "+=", "1", "\n", "selected_indices", ".", "append", "(", "i", ")", "\n", "is_index_valid", "[", "i", "]", "=", "False", "\n", "valid_indices", "=", "np", ".", "where", "(", "is_index_valid", ")", "[", "0", "]", "\n", "if", "valid_indices", ".", "size", "==", "0", ":", "\n", "                    ", "break", "\n", "\n", "", "intersect_over_union", "=", "np_mask_ops", ".", "iou", "(", "\n", "np", ".", "expand_dims", "(", "masks", "[", "i", "]", ",", "axis", "=", "0", ")", ",", "masks", "[", "valid_indices", "]", "\n", ")", "\n", "intersect_over_union", "=", "np", ".", "squeeze", "(", "intersect_over_union", ",", "axis", "=", "0", ")", "\n", "is_index_valid", "[", "valid_indices", "]", "=", "np", ".", "logical_and", "(", "\n", "is_index_valid", "[", "valid_indices", "]", ",", "\n", "intersect_over_union", "<=", "iou_threshold", ",", "\n", ")", "\n", "", "", "", "return", "gather", "(", "box_mask_list", ",", "np", ".", "array", "(", "selected_indices", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.multi_class_non_max_suppression": [[255, 335], ["box_mask_list.get_field", "box_mask_list.num_boxes", "range", "np_box_list_ops.concatenate", "np_box_list_ops.sort_by_field", "np_box_mask_list_ops.box_list_to_box_mask_list", "ValueError", "isinstance", "ValueError", "box_mask_list.has_field", "ValueError", "len", "numpy.reshape", "ValueError", "np_box_mask_list.BoxMaskList", "numpy.reshape", "np_box_mask_list.BoxMaskList.add_field", "np_box_mask_list_ops.filter_scores_greater_than", "np_box_mask_list_ops.non_max_suppression", "non_max_suppression.add_field", "selected_boxes_list.append", "len", "ValueError", "ValueError", "box_mask_list.get", "box_mask_list.get_masks", "numpy.zeros_like", "non_max_suppression.get_field"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.sort_by_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.box_list_to_box_mask_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.filter_scores_greater_than", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.non_max_suppression", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list.BoxMaskList.get_masks", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "multi_class_non_max_suppression", "(", "\n", "box_mask_list", ",", "score_thresh", ",", "iou_thresh", ",", "max_output_size", "\n", ")", ":", "\n", "    ", "\"\"\"Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain a\n      'scores' field representing detection scores.  This scores field is a\n      tensor that can be 1 dimensional (in the case of a single class) or\n      2-dimensional, in which case we assume that it takes the\n      shape [num_boxes, num_classes]. We further assume that this rank is known\n      statically and that scores.shape[1] is also known (i.e., the number of\n      classes is fixed and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n\n  Returns:\n    a box_mask_list holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input box_mask_list does\n      not have a valid scores field.\n  \"\"\"", "\n", "if", "not", "0", "<=", "iou_thresh", "<=", "1.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"thresh must be between 0 and 1\"", ")", "\n", "", "if", "not", "isinstance", "(", "box_mask_list", ",", "np_box_mask_list", ".", "BoxMaskList", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"box_mask_list must be a box_mask_list\"", ")", "\n", "", "if", "not", "box_mask_list", ".", "has_field", "(", "\"scores\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"input box_mask_list must have 'scores' field\"", ")", "\n", "", "scores", "=", "box_mask_list", ".", "get_field", "(", "\"scores\"", ")", "\n", "if", "len", "(", "scores", ".", "shape", ")", "==", "1", ":", "\n", "        ", "scores", "=", "np", ".", "reshape", "(", "scores", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "", "elif", "len", "(", "scores", ".", "shape", ")", "==", "2", ":", "\n", "        ", "if", "scores", ".", "shape", "[", "1", "]", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"scores field must have statically defined second \"", "\"dimension\"", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"scores field must be of rank 1 or 2\"", ")", "\n", "\n", "", "num_boxes", "=", "box_mask_list", ".", "num_boxes", "(", ")", "\n", "num_scores", "=", "scores", ".", "shape", "[", "0", "]", "\n", "num_classes", "=", "scores", ".", "shape", "[", "1", "]", "\n", "\n", "if", "num_boxes", "!=", "num_scores", ":", "\n", "        ", "raise", "ValueError", "(", "\"Incorrect scores field length: actual vs expected.\"", ")", "\n", "\n", "", "selected_boxes_list", "=", "[", "]", "\n", "for", "class_idx", "in", "range", "(", "num_classes", ")", ":", "\n", "        ", "box_mask_list_and_class_scores", "=", "np_box_mask_list", ".", "BoxMaskList", "(", "\n", "box_data", "=", "box_mask_list", ".", "get", "(", ")", ",", "mask_data", "=", "box_mask_list", ".", "get_masks", "(", ")", "\n", ")", "\n", "class_scores", "=", "np", ".", "reshape", "(", "scores", "[", "0", ":", "num_scores", ",", "class_idx", "]", ",", "[", "-", "1", "]", ")", "\n", "box_mask_list_and_class_scores", ".", "add_field", "(", "\"scores\"", ",", "class_scores", ")", "\n", "box_mask_list_filt", "=", "filter_scores_greater_than", "(", "\n", "box_mask_list_and_class_scores", ",", "score_thresh", "\n", ")", "\n", "nms_result", "=", "non_max_suppression", "(", "\n", "box_mask_list_filt", ",", "\n", "max_output_size", "=", "max_output_size", ",", "\n", "iou_threshold", "=", "iou_thresh", ",", "\n", "score_threshold", "=", "score_thresh", ",", "\n", ")", "\n", "nms_result", ".", "add_field", "(", "\n", "\"classes\"", ",", "np", ".", "zeros_like", "(", "nms_result", ".", "get_field", "(", "\"scores\"", ")", ")", "+", "class_idx", "\n", ")", "\n", "selected_boxes_list", ".", "append", "(", "nms_result", ")", "\n", "", "selected_boxes", "=", "np_box_list_ops", ".", "concatenate", "(", "selected_boxes_list", ")", "\n", "sorted_boxes", "=", "np_box_list_ops", ".", "sort_by_field", "(", "selected_boxes", ",", "\"scores\"", ")", "\n", "return", "box_list_to_box_mask_list", "(", "boxlist", "=", "sorted_boxes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.prune_non_overlapping_masks": [[337, 363], ["np_box_mask_list_ops.ioa", "numpy.amax", "numpy.greater_equal", "np_box_mask_list_ops.gather", "numpy.array", "numpy.nonzero"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.ioa", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather"], ["", "def", "prune_non_overlapping_masks", "(", "box_mask_list1", ",", "box_mask_list2", ",", "minoverlap", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"Prunes the boxes in list1 that overlap less than thresh with list2.\n\n  For each mask in box_mask_list1, we want its IOA to be more than minoverlap\n  with at least one of the masks in box_mask_list2. If it does not, we remove\n  it. If the masks are not full size image, we do the pruning based on boxes.\n\n  Args:\n    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks.\n    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n\n  Returns:\n    A pruned box_mask_list with size [N', 4].\n  \"\"\"", "\n", "intersection_over_area", "=", "ioa", "(", "\n", "box_mask_list2", ",", "box_mask_list1", "\n", ")", "# [M, N] tensor", "\n", "intersection_over_area", "=", "np", ".", "amax", "(", "\n", "intersection_over_area", ",", "axis", "=", "0", "\n", ")", "# [N] tensor", "\n", "keep_bool", "=", "np", ".", "greater_equal", "(", "intersection_over_area", ",", "np", ".", "array", "(", "minoverlap", ")", ")", "\n", "keep_inds", "=", "np", ".", "nonzero", "(", "keep_bool", ")", "[", "0", "]", "\n", "new_box_mask_list1", "=", "gather", "(", "box_mask_list1", ",", "keep_inds", ")", "\n", "return", "new_box_mask_list1", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate": [[365, 392], ["np_box_mask_list_ops.box_list_to_box_mask_list", "np_box_list_ops.concatenate", "fields.append"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.box_list_to_box_mask_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate"], ["", "def", "concatenate", "(", "box_mask_lists", ",", "fields", "=", "None", ")", ":", "\n", "    ", "\"\"\"Concatenate list of box_mask_lists.\n\n  This op concatenates a list of input box_mask_lists into a larger\n  box_mask_list.  It also\n  handles concatenation of box_mask_list fields as long as the field tensor\n  shapes are equal except for the first dimension.\n\n  Args:\n    box_mask_lists: list of np_box_mask_list.BoxMaskList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxMaskList in the list are included in the\n      concatenation.\n\n  Returns:\n    a box_mask_list with number of boxes equal to\n      sum([box_mask_list.num_boxes() for box_mask_list in box_mask_list])\n  Raises:\n    ValueError: if box_mask_lists is invalid (i.e., is not a list, is empty, or\n      contains non box_mask_list objects), or if requested fields are not\n      contained in all box_mask_lists\n  \"\"\"", "\n", "if", "fields", "is", "not", "None", ":", "\n", "        ", "if", "\"masks\"", "not", "in", "fields", ":", "\n", "            ", "fields", ".", "append", "(", "\"masks\"", ")", "\n", "", "", "return", "box_list_to_box_mask_list", "(", "\n", "np_box_list_ops", ".", "concatenate", "(", "boxlists", "=", "box_mask_lists", ",", "fields", "=", "fields", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.filter_scores_greater_than": [[395, 429], ["box_mask_list.get_field", "numpy.reshape().astype", "np_box_mask_list_ops.gather", "isinstance", "ValueError", "box_mask_list.has_field", "ValueError", "len", "ValueError", "ValueError", "len", "numpy.reshape", "numpy.where", "numpy.greater"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.has_field"], ["", "def", "filter_scores_greater_than", "(", "box_mask_list", ",", "thresh", ")", ":", "\n", "    ", "\"\"\"Filter to keep only boxes and masks with score exceeding a given threshold.\n\n  This op keeps the collection of boxes and masks whose corresponding scores are\n  greater than the input threshold.\n\n  Args:\n    box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a\n      'scores' field representing detection scores.\n    thresh: scalar threshold\n\n  Returns:\n    a BoxMaskList holding M boxes and masks where M <= N\n\n  Raises:\n    ValueError: if box_mask_list not a np_box_mask_list.BoxMaskList object or\n      if it does not have a scores field\n  \"\"\"", "\n", "if", "not", "isinstance", "(", "box_mask_list", ",", "np_box_mask_list", ".", "BoxMaskList", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"box_mask_list must be a BoxMaskList\"", ")", "\n", "", "if", "not", "box_mask_list", ".", "has_field", "(", "\"scores\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"input box_mask_list must have 'scores' field\"", ")", "\n", "", "scores", "=", "box_mask_list", ".", "get_field", "(", "\"scores\"", ")", "\n", "if", "len", "(", "scores", ".", "shape", ")", ">", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Scores should have rank 1 or 2\"", ")", "\n", "", "if", "len", "(", "scores", ".", "shape", ")", "==", "2", "and", "scores", ".", "shape", "[", "1", "]", "!=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Scores should have rank 1 or have shape \"", "\n", "\"consistent with [None, 1]\"", "\n", ")", "\n", "", "high_score_indices", "=", "np", ".", "reshape", "(", "\n", "np", ".", "where", "(", "np", ".", "greater", "(", "scores", ",", "thresh", ")", ")", ",", "[", "-", "1", "]", "\n", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "return", "gather", "(", "box_mask_list", ",", "high_score_indices", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.DetectionEvaluator.__init__": [[64, 73], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "categories", ")", ":", "\n", "        ", "\"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n    \"\"\"", "\n", "self", ".", "_categories", "=", "categories", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.DetectionEvaluator.add_single_ground_truth_image_info": [[74, 84], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "add_single_ground_truth_image_info", "(", "self", ",", "image_id", ",", "groundtruth_dict", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary of groundtruth numpy arrays required\n        for evaluations.\n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.DetectionEvaluator.add_single_detected_image_info": [[85, 95], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "add_single_detected_image_info", "(", "self", ",", "image_id", ",", "detections_dict", ")", ":", "\n", "        ", "\"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary of detection numpy arrays required\n        for evaluation.\n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.DetectionEvaluator.evaluate": [[96, 100], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Evaluates detections and returns a dictionary of metrics.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.DetectionEvaluator.clear": [[101, 105], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "clear", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.__init__": [[110, 157], ["object_detection_evaluation.DetectionEvaluator.__init__", "max", "object_detection_evaluation.ObjectDetectionEvaluation", "set", "min", "ValueError"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "metric_prefix", "=", "None", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", "evaluate_masks", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n      matching_iou_threshold: IOU threshold to use for matching groundtruth\n        boxes to detection boxes.\n      evaluate_corlocs: (optional) boolean which determines if corloc scores\n        are to be returned or not.\n      metric_prefix: (optional) string prefix for metric name; if None, no\n        prefix is used.\n      use_weighted_mean_ap: (optional) boolean which determines if the mean\n        average precision is computed directly from the scores and tp_fp_labels\n        of all classes.\n      evaluate_masks: If False, evaluation will be performed based on boxes.\n        If True, mask evaluation will be performed instead.\n\n    Raises:\n      ValueError: If the category ids are not 1-indexed.\n    \"\"\"", "\n", "super", "(", "ObjectDetectionEvaluator", ",", "self", ")", ".", "__init__", "(", "categories", ")", "\n", "self", ".", "_num_classes", "=", "max", "(", "[", "cat", "[", "\"id\"", "]", "for", "cat", "in", "categories", "]", ")", "\n", "if", "min", "(", "cat", "[", "\"id\"", "]", "for", "cat", "in", "categories", ")", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Classes should be 1-indexed.\"", ")", "\n", "", "self", ".", "_matching_iou_threshold", "=", "matching_iou_threshold", "\n", "self", ".", "_use_weighted_mean_ap", "=", "use_weighted_mean_ap", "\n", "self", ".", "_label_id_offset", "=", "1", "\n", "self", ".", "_evaluate_masks", "=", "evaluate_masks", "\n", "self", ".", "_evaluation", "=", "ObjectDetectionEvaluation", "(", "\n", "num_groundtruth_classes", "=", "self", ".", "_num_classes", ",", "\n", "matching_iou_threshold", "=", "self", ".", "_matching_iou_threshold", ",", "\n", "use_weighted_mean_ap", "=", "self", ".", "_use_weighted_mean_ap", ",", "\n", "label_id_offset", "=", "self", ".", "_label_id_offset", ",", "\n", ")", "\n", "self", ".", "_image_ids", "=", "set", "(", "[", "]", ")", "\n", "self", ".", "_evaluate_corlocs", "=", "evaluate_corlocs", "\n", "self", ".", "_metric_prefix", "=", "(", "metric_prefix", "+", "\"_\"", ")", "if", "metric_prefix", "else", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.add_single_ground_truth_image_info": [[158, 231], ["object_detection_evaluation.ObjectDetectionEvaluator._evaluation.add_single_ground_truth_image_info", "object_detection_evaluation.ObjectDetectionEvaluator._image_ids.update", "ValueError", "groundtruth_dict.keys", "logging.warn", "ValueError", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info"], ["", "def", "add_single_ground_truth_image_info", "(", "self", ",", "image_id", ",", "groundtruth_dict", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array\n          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of\n          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.InputDataFields.groundtruth_classes: integer numpy array\n          of shape [num_boxes] containing 1-indexed groundtruth classes for the\n          boxes.\n        standard_fields.InputDataFields.groundtruth_difficult: Optional length\n          M numpy boolean array denoting whether a ground truth box is a\n          difficult instance or not. This field is optional to support the case\n          that no boxes are difficult.\n        standard_fields.InputDataFields.groundtruth_instance_masks: Optional\n          numpy array of shape [num_boxes, height, width] with values in {0, 1}.\n\n    Raises:\n      ValueError: On adding groundtruth for an image more than once. Will also\n        raise error if instance masks are not in groundtruth dictionary.\n    \"\"\"", "\n", "if", "image_id", "in", "self", ".", "_image_ids", ":", "\n", "            ", "raise", "ValueError", "(", "\"Image with id {} already added.\"", ".", "format", "(", "image_id", ")", ")", "\n", "\n", "", "groundtruth_classes", "=", "(", "\n", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_classes", "\n", "]", "\n", "-", "self", ".", "_label_id_offset", "\n", ")", "\n", "# If the key is not present in the groundtruth_dict or the array is empty", "\n", "# (unless there are no annotations for the groundtruth on this image)", "\n", "# use values from the dictionary or insert None otherwise.", "\n", "if", "standard_fields", ".", "InputDataFields", ".", "groundtruth_difficult", "in", "groundtruth_dict", ".", "keys", "(", ")", "and", "(", "\n", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_difficult", "\n", "]", ".", "size", "\n", "or", "not", "groundtruth_classes", ".", "size", "\n", ")", ":", "\n", "            ", "groundtruth_difficult", "=", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_difficult", "\n", "]", "\n", "", "else", ":", "\n", "            ", "groundtruth_difficult", "=", "None", "\n", "if", "not", "len", "(", "self", ".", "_image_ids", ")", "%", "1000", ":", "\n", "                ", "logging", ".", "warn", "(", "\n", "\"image %s does not have groundtruth difficult flag specified\"", ",", "\n", "image_id", ",", "\n", ")", "\n", "", "", "groundtruth_masks", "=", "None", "\n", "if", "self", ".", "_evaluate_masks", ":", "\n", "            ", "if", "(", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_instance_masks", "\n", "not", "in", "groundtruth_dict", "\n", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Instance masks not in groundtruth dictionary.\"", "\n", ")", "\n", "", "groundtruth_masks", "=", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_instance_masks", "\n", "]", "\n", "", "self", ".", "_evaluation", ".", "add_single_ground_truth_image_info", "(", "\n", "image_key", "=", "image_id", ",", "\n", "groundtruth_boxes", "=", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_boxes", "\n", "]", ",", "\n", "groundtruth_class_labels", "=", "groundtruth_classes", ",", "\n", "groundtruth_is_difficult_list", "=", "groundtruth_difficult", ",", "\n", "groundtruth_masks", "=", "groundtruth_masks", ",", "\n", ")", "\n", "self", ".", "_image_ids", ".", "update", "(", "[", "image_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.add_single_detected_image_info": [[232, 281], ["object_detection_evaluation.ObjectDetectionEvaluator._evaluation.add_single_detected_image_info", "ValueError"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_detected_image_info"], ["", "def", "add_single_detected_image_info", "(", "self", ",", "image_id", ",", "detections_dict", ")", ":", "\n", "        ", "\"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary containing -\n        standard_fields.DetectionResultFields.detection_boxes: float32 numpy\n          array of shape [num_boxes, 4] containing `num_boxes` detection boxes\n          of the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.DetectionResultFields.detection_scores: float32 numpy\n          array of shape [num_boxes] containing detection scores for the boxes.\n        standard_fields.DetectionResultFields.detection_classes: integer numpy\n          array of shape [num_boxes] containing 1-indexed detection classes for\n          the boxes.\n        standard_fields.DetectionResultFields.detection_masks: uint8 numpy\n          array of shape [num_boxes, height, width] containing `num_boxes` masks\n          of values ranging between 0 and 1.\n\n    Raises:\n      ValueError: If detection masks are not in detections dictionary.\n    \"\"\"", "\n", "detection_classes", "=", "(", "\n", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_classes", "\n", "]", "\n", "-", "self", ".", "_label_id_offset", "\n", ")", "\n", "detection_masks", "=", "None", "\n", "if", "self", ".", "_evaluate_masks", ":", "\n", "            ", "if", "(", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_masks", "\n", "not", "in", "detections_dict", "\n", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Detection masks not in detections dictionary.\"", "\n", ")", "\n", "", "detection_masks", "=", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_masks", "\n", "]", "\n", "", "self", ".", "_evaluation", ".", "add_single_detected_image_info", "(", "\n", "image_key", "=", "image_id", ",", "\n", "detected_boxes", "=", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_boxes", "\n", "]", ",", "\n", "detected_scores", "=", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_scores", "\n", "]", ",", "\n", "detected_class_labels", "=", "detection_classes", ",", "\n", "detected_masks", "=", "detection_masks", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.evaluate": [[283, 341], ["object_detection_evaluation.ObjectDetectionEvaluator._evaluation.evaluate", "label_map_util.create_category_index", "range"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.label_map_util.create_category_index"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Compute evaluation result.\n\n    Returns:\n      A dictionary of metrics with the following fields -\n\n      1. summary_metrics:\n        'Precision/mAP@<matching_iou_threshold>IOU': mean average precision at\n        the specified IOU threshold.\n\n      2. per_category_ap: category specific results with keys of the form\n        'PerformanceByCategory/mAP@<matching_iou_threshold>IOU/category'.\n    \"\"\"", "\n", "(", "\n", "per_class_ap", ",", "\n", "mean_ap", ",", "\n", "_", ",", "\n", "_", ",", "\n", "per_class_corloc", ",", "\n", "mean_corloc", ",", "\n", ")", "=", "self", ".", "_evaluation", ".", "evaluate", "(", ")", "\n", "pascal_metrics", "=", "{", "\n", "self", ".", "_metric_prefix", "\n", "+", "\"Precision/mAP@{}IOU\"", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", "\n", ")", ":", "mean_ap", "\n", "}", "\n", "if", "self", ".", "_evaluate_corlocs", ":", "\n", "            ", "pascal_metrics", "[", "\n", "self", ".", "_metric_prefix", "\n", "+", "\"Precision/meanCorLoc@{}IOU\"", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", "\n", ")", "\n", "]", "=", "mean_corloc", "\n", "", "category_index", "=", "label_map_util", ".", "create_category_index", "(", "self", ".", "_categories", ")", "\n", "for", "idx", "in", "range", "(", "per_class_ap", ".", "size", ")", ":", "\n", "            ", "if", "idx", "+", "self", ".", "_label_id_offset", "in", "category_index", ":", "\n", "                ", "display_name", "=", "(", "\n", "self", ".", "_metric_prefix", "\n", "+", "\"PerformanceByCategory/AP@{}IOU/{}\"", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", ",", "\n", "category_index", "[", "idx", "+", "self", ".", "_label_id_offset", "]", "[", "\"name\"", "]", ",", "\n", ")", "\n", ")", "\n", "pascal_metrics", "[", "display_name", "]", "=", "per_class_ap", "[", "idx", "]", "\n", "\n", "# Optionally add CorLoc metrics.classes", "\n", "if", "self", ".", "_evaluate_corlocs", ":", "\n", "                    ", "display_name", "=", "(", "\n", "self", ".", "_metric_prefix", "\n", "+", "\"PerformanceByCategory/CorLoc@{}IOU/{}\"", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", ",", "\n", "category_index", "[", "idx", "+", "self", ".", "_label_id_offset", "]", "[", "\"name\"", "]", ",", "\n", ")", "\n", ")", "\n", "pascal_metrics", "[", "display_name", "]", "=", "per_class_corloc", "[", "idx", "]", "\n", "\n", "", "", "", "return", "pascal_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.clear": [[342, 351], ["object_detection_evaluation.ObjectDetectionEvaluation", "object_detection_evaluation.ObjectDetectionEvaluator._image_ids.clear"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"", "\n", "self", ".", "_evaluation", "=", "ObjectDetectionEvaluation", "(", "\n", "num_groundtruth_classes", "=", "self", ".", "_num_classes", ",", "\n", "matching_iou_threshold", "=", "self", ".", "_matching_iou_threshold", ",", "\n", "use_weighted_mean_ap", "=", "self", ".", "_use_weighted_mean_ap", ",", "\n", "label_id_offset", "=", "self", ".", "_label_id_offset", ",", "\n", ")", "\n", "self", ".", "_image_ids", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.PascalDetectionEvaluator.__init__": [[356, 363], ["object_detection_evaluation.ObjectDetectionEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "self", ",", "categories", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "PascalDetectionEvaluator", ",", "self", ")", ".", "__init__", "(", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "metric_prefix", "=", "\"PascalBoxes\"", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.WeightedPascalDetectionEvaluator.__init__": [[380, 387], ["object_detection_evaluation.ObjectDetectionEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "self", ",", "categories", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "WeightedPascalDetectionEvaluator", ",", "self", ")", ".", "__init__", "(", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "metric_prefix", "=", "\"WeightedPascalBoxes\"", ",", "\n", "use_weighted_mean_ap", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.PascalInstanceSegmentationEvaluator.__init__": [[393, 401], ["object_detection_evaluation.ObjectDetectionEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "self", ",", "categories", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "PascalInstanceSegmentationEvaluator", ",", "self", ")", ".", "__init__", "(", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "metric_prefix", "=", "\"PascalMasks\"", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", "evaluate_masks", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator.__init__": [[418, 426], ["object_detection_evaluation.ObjectDetectionEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "self", ",", "categories", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "WeightedPascalInstanceSegmentationEvaluator", ",", "self", ")", ".", "__init__", "(", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "metric_prefix", "=", "\"WeightedPascalMasks\"", ",", "\n", "use_weighted_mean_ap", "=", "True", ",", "\n", "evaluate_masks", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.OpenImagesDetectionEvaluator.__init__": [[436, 454], ["object_detection_evaluation.ObjectDetectionEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "categories", ",", "matching_iou_threshold", "=", "0.5", ",", "evaluate_corlocs", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n      matching_iou_threshold: IOU threshold to use for matching groundtruth\n        boxes to detection boxes.\n      evaluate_corlocs: if True, additionally evaluates and returns CorLoc.\n    \"\"\"", "\n", "super", "(", "OpenImagesDetectionEvaluator", ",", "self", ")", ".", "__init__", "(", "\n", "categories", ",", "\n", "matching_iou_threshold", ",", "\n", "evaluate_corlocs", ",", "\n", "metric_prefix", "=", "\"OpenImagesV2\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.OpenImagesDetectionEvaluator.add_single_ground_truth_image_info": [[456, 511], ["object_detection_evaluation.OpenImagesDetectionEvaluator._evaluation.add_single_ground_truth_image_info", "object_detection_evaluation.OpenImagesDetectionEvaluator._image_ids.update", "ValueError", "groundtruth_dict.keys", "logging.warn", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info"], ["", "def", "add_single_ground_truth_image_info", "(", "self", ",", "image_id", ",", "groundtruth_dict", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array\n          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of\n          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.InputDataFields.groundtruth_classes: integer numpy array\n          of shape [num_boxes] containing 1-indexed groundtruth classes for the\n          boxes.\n        standard_fields.InputDataFields.groundtruth_group_of: Optional length\n          M numpy boolean array denoting whether a groundtruth box contains a\n          group of instances.\n\n    Raises:\n      ValueError: On adding groundtruth for an image more than once.\n    \"\"\"", "\n", "if", "image_id", "in", "self", ".", "_image_ids", ":", "\n", "            ", "raise", "ValueError", "(", "\"Image with id {} already added.\"", ".", "format", "(", "image_id", ")", ")", "\n", "\n", "", "groundtruth_classes", "=", "(", "\n", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_classes", "\n", "]", "\n", "-", "self", ".", "_label_id_offset", "\n", ")", "\n", "# If the key is not present in the groundtruth_dict or the array is empty", "\n", "# (unless there are no annotations for the groundtruth on this image)", "\n", "# use values from the dictionary or insert None otherwise.", "\n", "if", "standard_fields", ".", "InputDataFields", ".", "groundtruth_group_of", "in", "groundtruth_dict", ".", "keys", "(", ")", "and", "(", "\n", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_group_of", "\n", "]", ".", "size", "\n", "or", "not", "groundtruth_classes", ".", "size", "\n", ")", ":", "\n", "            ", "groundtruth_group_of", "=", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_group_of", "\n", "]", "\n", "", "else", ":", "\n", "            ", "groundtruth_group_of", "=", "None", "\n", "if", "not", "len", "(", "self", ".", "_image_ids", ")", "%", "1000", ":", "\n", "                ", "logging", ".", "warn", "(", "\n", "\"image %s does not have groundtruth group_of flag specified\"", ",", "\n", "image_id", ",", "\n", ")", "\n", "", "", "self", ".", "_evaluation", ".", "add_single_ground_truth_image_info", "(", "\n", "image_id", ",", "\n", "groundtruth_dict", "[", "standard_fields", ".", "InputDataFields", ".", "groundtruth_boxes", "]", ",", "\n", "groundtruth_classes", ",", "\n", "groundtruth_is_difficult_list", "=", "None", ",", "\n", "groundtruth_is_group_of_list", "=", "groundtruth_group_of", ",", "\n", ")", "\n", "self", ".", "_image_ids", ".", "update", "(", "[", "image_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.__init__": [[529, 560], ["per_image_evaluation.PerImageEvaluation", "numpy.zeros", "numpy.zeros", "object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections", "ValueError"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_groundtruth_classes", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "nms_iou_threshold", "=", "1.0", ",", "\n", "nms_max_output_boxes", "=", "10000", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", "label_id_offset", "=", "0", ",", "\n", ")", ":", "\n", "        ", "if", "num_groundtruth_classes", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Need at least 1 groundtruth class for evaluation.\"", "\n", ")", "\n", "\n", "", "self", ".", "per_image_eval", "=", "per_image_evaluation", ".", "PerImageEvaluation", "(", "\n", "num_groundtruth_classes", "=", "num_groundtruth_classes", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", ")", "\n", "self", ".", "num_class", "=", "num_groundtruth_classes", "\n", "self", ".", "use_weighted_mean_ap", "=", "use_weighted_mean_ap", "\n", "self", ".", "label_id_offset", "=", "label_id_offset", "\n", "\n", "self", ".", "groundtruth_boxes", "=", "{", "}", "\n", "self", ".", "groundtruth_class_labels", "=", "{", "}", "\n", "self", ".", "groundtruth_masks", "=", "{", "}", "\n", "self", ".", "groundtruth_is_difficult_list", "=", "{", "}", "\n", "self", ".", "groundtruth_is_group_of_list", "=", "{", "}", "\n", "self", ".", "num_gt_instances_per_class", "=", "np", ".", "zeros", "(", "self", ".", "num_class", ",", "dtype", "=", "int", ")", "\n", "self", ".", "num_gt_imgs_per_class", "=", "np", ".", "zeros", "(", "self", ".", "num_class", ",", "dtype", "=", "int", ")", "\n", "\n", "self", ".", "_initialize_detections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections": [[561, 571], ["set", "numpy.zeros", "numpy.empty", "object_detection_evaluation.ObjectDetectionEvaluation.average_precision_per_class.fill", "numpy.ones", "range", "range"], "methods", ["None"], ["", "def", "_initialize_detections", "(", "self", ")", ":", "\n", "        ", "self", ".", "detection_keys", "=", "set", "(", ")", "\n", "self", ".", "scores_per_class", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "num_class", ")", "]", "\n", "self", ".", "tp_fp_labels_per_class", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "num_class", ")", "]", "\n", "self", ".", "num_images_correctly_detected_per_class", "=", "np", ".", "zeros", "(", "self", ".", "num_class", ")", "\n", "self", ".", "average_precision_per_class", "=", "np", ".", "empty", "(", "self", ".", "num_class", ",", "dtype", "=", "float", ")", "\n", "self", ".", "average_precision_per_class", ".", "fill", "(", "np", ".", "nan", ")", "\n", "self", ".", "precisions_per_class", "=", "[", "]", "\n", "self", ".", "recalls_per_class", "=", "[", "]", "\n", "self", ".", "corloc_per_class", "=", "np", ".", "ones", "(", "self", ".", "num_class", ",", "dtype", "=", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.clear_detections": [[572, 574], ["object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections"], ["", "def", "clear_detections", "(", "self", ")", ":", "\n", "        ", "self", ".", "_initialize_detections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info": [[575, 630], ["numpy.zeros.astype", "numpy.zeros.astype", "object_detection_evaluation.ObjectDetectionEvaluation._update_ground_truth_statistics", "logging.warn", "numpy.zeros", "numpy.zeros", "numpy.zeros.astype", "numpy.zeros.astype"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._update_ground_truth_statistics"], ["", "def", "add_single_ground_truth_image_info", "(", "\n", "self", ",", "\n", "image_key", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", "=", "None", ",", "\n", "groundtruth_is_group_of_list", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      groundtruth_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` groundtruth boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      groundtruth_class_labels: integer numpy array of shape [num_boxes]\n        containing 0-indexed groundtruth classes for the boxes.\n      groundtruth_is_difficult_list: A length M numpy boolean array denoting\n        whether a ground truth box is a difficult instance or not. To support\n        the case that no boxes are difficult, it is by default set as None.\n      groundtruth_is_group_of_list: A length M numpy boolean array denoting\n          whether a ground truth box is a group-of box or not. To support\n          the case that no boxes are groups-of, it is by default set as None.\n      groundtruth_masks: uint8 numpy array of shape\n        [num_boxes, height, width] containing `num_boxes` groundtruth masks.\n        The mask values range from 0 to 1.\n    \"\"\"", "\n", "if", "image_key", "in", "self", ".", "groundtruth_boxes", ":", "\n", "            ", "logging", ".", "warn", "(", "\n", "\"image %s has already been added to the ground truth database.\"", ",", "\n", "image_key", ",", "\n", ")", "\n", "return", "\n", "\n", "", "self", ".", "groundtruth_boxes", "[", "image_key", "]", "=", "groundtruth_boxes", "\n", "self", ".", "groundtruth_class_labels", "[", "image_key", "]", "=", "groundtruth_class_labels", "\n", "self", ".", "groundtruth_masks", "[", "image_key", "]", "=", "groundtruth_masks", "\n", "if", "groundtruth_is_difficult_list", "is", "None", ":", "\n", "            ", "num_boxes", "=", "groundtruth_boxes", ".", "shape", "[", "0", "]", "\n", "groundtruth_is_difficult_list", "=", "np", ".", "zeros", "(", "num_boxes", ",", "dtype", "=", "bool", ")", "\n", "", "self", ".", "groundtruth_is_difficult_list", "[", "\n", "image_key", "\n", "]", "=", "groundtruth_is_difficult_list", ".", "astype", "(", "dtype", "=", "bool", ")", "\n", "if", "groundtruth_is_group_of_list", "is", "None", ":", "\n", "            ", "num_boxes", "=", "groundtruth_boxes", ".", "shape", "[", "0", "]", "\n", "groundtruth_is_group_of_list", "=", "np", ".", "zeros", "(", "num_boxes", ",", "dtype", "=", "bool", ")", "\n", "", "self", ".", "groundtruth_is_group_of_list", "[", "\n", "image_key", "\n", "]", "=", "groundtruth_is_group_of_list", ".", "astype", "(", "dtype", "=", "bool", ")", "\n", "\n", "self", ".", "_update_ground_truth_statistics", "(", "\n", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", ".", "astype", "(", "dtype", "=", "bool", ")", ",", "\n", "groundtruth_is_group_of_list", ".", "astype", "(", "dtype", "=", "bool", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_detected_image_info": [[632, 718], ["object_detection_evaluation.ObjectDetectionEvaluation.detection_keys.add", "object_detection_evaluation.ObjectDetectionEvaluation.per_image_eval.compute_object_detection_metrics", "range", "ValueError", "logging.warn", "object_detection_evaluation.ObjectDetectionEvaluation.groundtruth_masks.pop", "numpy.empty", "numpy.array", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "len", "numpy.empty", "object_detection_evaluation.ObjectDetectionEvaluation.scores_per_class[].append", "object_detection_evaluation.ObjectDetectionEvaluation.tp_fp_labels_per_class[].append", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.per_image_evaluation.PerImageEvaluation.compute_object_detection_metrics"], ["", "def", "add_single_detected_image_info", "(", "\n", "self", ",", "\n", "image_key", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      detected_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` detection boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      detected_scores: float32 numpy array of shape [num_boxes] containing\n        detection scores for the boxes.\n      detected_class_labels: integer numpy array of shape [num_boxes] containing\n        0-indexed detection classes for the boxes.\n      detected_masks: np.uint8 numpy array of shape [num_boxes, height, width]\n        containing `num_boxes` detection masks with values ranging\n        between 0 and 1.\n\n    Raises:\n      ValueError: if the number of boxes, scores and class labels differ in\n        length.\n    \"\"\"", "\n", "if", "len", "(", "detected_boxes", ")", "!=", "len", "(", "detected_scores", ")", "or", "len", "(", "\n", "detected_boxes", "\n", ")", "!=", "len", "(", "detected_class_labels", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"detected_boxes, detected_scores and \"", "\n", "\"detected_class_labels should all have same lengths. Got\"", "\n", "\"[%d, %d, %d]\"", "%", "len", "(", "detected_boxes", ")", ",", "\n", "len", "(", "detected_scores", ")", ",", "\n", "len", "(", "detected_class_labels", ")", ",", "\n", ")", "\n", "\n", "", "if", "image_key", "in", "self", ".", "detection_keys", ":", "\n", "            ", "logging", ".", "warn", "(", "\n", "\"image %s has already been added to the detection result database\"", ",", "\n", "image_key", ",", "\n", ")", "\n", "return", "\n", "\n", "", "self", ".", "detection_keys", ".", "add", "(", "image_key", ")", "\n", "if", "image_key", "in", "self", ".", "groundtruth_boxes", ":", "\n", "            ", "groundtruth_boxes", "=", "self", ".", "groundtruth_boxes", "[", "image_key", "]", "\n", "groundtruth_class_labels", "=", "self", ".", "groundtruth_class_labels", "[", "image_key", "]", "\n", "# Masks are popped instead of look up. The reason is that we do not want", "\n", "# to keep all masks in memory which can cause memory overflow.", "\n", "groundtruth_masks", "=", "self", ".", "groundtruth_masks", ".", "pop", "(", "image_key", ")", "\n", "groundtruth_is_difficult_list", "=", "self", ".", "groundtruth_is_difficult_list", "[", "\n", "image_key", "\n", "]", "\n", "groundtruth_is_group_of_list", "=", "self", ".", "groundtruth_is_group_of_list", "[", "\n", "image_key", "\n", "]", "\n", "", "else", ":", "\n", "            ", "groundtruth_boxes", "=", "np", ".", "empty", "(", "shape", "=", "[", "0", ",", "4", "]", ",", "dtype", "=", "float", ")", "\n", "groundtruth_class_labels", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "int", ")", "\n", "if", "detected_masks", "is", "None", ":", "\n", "                ", "groundtruth_masks", "=", "None", "\n", "", "else", ":", "\n", "                ", "groundtruth_masks", "=", "np", ".", "empty", "(", "shape", "=", "[", "0", ",", "1", ",", "1", "]", ",", "dtype", "=", "float", ")", "\n", "", "groundtruth_is_difficult_list", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "groundtruth_is_group_of_list", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "", "(", "\n", "scores", ",", "\n", "tp_fp_labels", ",", "\n", ")", "=", "self", ".", "per_image_eval", ".", "compute_object_detection_metrics", "(", "\n", "detected_boxes", "=", "detected_boxes", ",", "\n", "detected_scores", "=", "detected_scores", ",", "\n", "detected_class_labels", "=", "detected_class_labels", ",", "\n", "groundtruth_boxes", "=", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", "=", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", "=", "groundtruth_is_difficult_list", ",", "\n", "groundtruth_is_group_of_list", "=", "groundtruth_is_group_of_list", ",", "\n", "detected_masks", "=", "detected_masks", ",", "\n", "groundtruth_masks", "=", "groundtruth_masks", ",", "\n", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_class", ")", ":", "\n", "            ", "if", "scores", "[", "i", "]", ".", "shape", "[", "0", "]", ">", "0", ":", "\n", "                ", "self", ".", "scores_per_class", "[", "i", "]", ".", "append", "(", "scores", "[", "i", "]", ")", "\n", "self", ".", "tp_fp_labels_per_class", "[", "i", "]", ".", "append", "(", "tp_fp_labels", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._update_ground_truth_statistics": [[719, 751], ["range", "numpy.sum", "numpy.any"], "methods", ["None"], ["", "", "", "def", "_update_ground_truth_statistics", "(", "\n", "self", ",", "\n", "groundtruth_class_labels", ",", "\n", "groundtruth_is_difficult_list", ",", "\n", "groundtruth_is_group_of_list", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Update grouth truth statitistics.\n\n    1. Difficult boxes are ignored when counting the number of ground truth\n    instances as done in Pascal VOC devkit.\n    2. Difficult boxes are treated as normal boxes when computing CorLoc related\n    statitistics.\n\n    Args:\n      groundtruth_class_labels: An integer numpy array of length M,\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a group-of box or not\n    \"\"\"", "\n", "for", "class_index", "in", "range", "(", "self", ".", "num_class", ")", ":", "\n", "            ", "num_gt_instances", "=", "np", ".", "sum", "(", "\n", "groundtruth_class_labels", "[", "\n", "~", "groundtruth_is_difficult_list", "\n", "&", "~", "groundtruth_is_group_of_list", "\n", "]", "\n", "==", "class_index", "\n", ")", "\n", "self", ".", "num_gt_instances_per_class", "[", "class_index", "]", "+=", "num_gt_instances", "\n", "if", "np", ".", "any", "(", "groundtruth_class_labels", "==", "class_index", ")", ":", "\n", "                ", "self", ".", "num_gt_imgs_per_class", "[", "class_index", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate": [[752, 824], ["range", "metrics.compute_cor_loc", "numpy.nanmean", "ObjectDetectionEvalMetrics", "logging.info", "numpy.array", "numpy.array", "metrics.compute_precision_recall", "object_detection_evaluation.ObjectDetectionEvaluation.precisions_per_class.append", "object_detection_evaluation.ObjectDetectionEvaluation.recalls_per_class.append", "metrics.compute_average_precision", "numpy.sum", "metrics.compute_precision_recall", "metrics.compute_average_precision", "numpy.nanmean", "numpy.array", "numpy.array", "numpy.concatenate", "numpy.concatenate", "numpy.append", "numpy.append", "numpy.squeeze", "numpy.argwhere"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_cor_loc", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_precision_recall", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_average_precision", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_precision_recall", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.metrics.compute_average_precision", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate"], ["", "", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Compute evaluation result.\n\n    Returns:\n      A named tuple with the following fields -\n        average_precision: float numpy array of average precision for\n            each class.\n        mean_ap: mean average precision of all classes, float scalar\n        precisions: List of precisions, each precision is a float numpy\n            array\n        recalls: List of recalls, each recall is a float numpy array\n        corloc: numpy float array\n        mean_corloc: Mean CorLoc score for each class, float scalar\n    \"\"\"", "\n", "if", "(", "self", ".", "num_gt_instances_per_class", "==", "0", ")", ".", "any", "(", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "\"The following classes have no ground truth examples: %s\"", ",", "\n", "np", ".", "squeeze", "(", "np", ".", "argwhere", "(", "self", ".", "num_gt_instances_per_class", "==", "0", ")", ")", "\n", "+", "self", ".", "label_id_offset", ",", "\n", ")", "\n", "\n", "", "if", "self", ".", "use_weighted_mean_ap", ":", "\n", "            ", "all_scores", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "float", ")", "\n", "all_tp_fp_labels", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "\n", "", "for", "class_index", "in", "range", "(", "self", ".", "num_class", ")", ":", "\n", "            ", "if", "self", ".", "num_gt_instances_per_class", "[", "class_index", "]", "==", "0", ":", "\n", "                ", "continue", "\n", "", "if", "not", "self", ".", "scores_per_class", "[", "class_index", "]", ":", "\n", "                ", "scores", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "float", ")", "\n", "tp_fp_labels", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "", "else", ":", "\n", "                ", "scores", "=", "np", ".", "concatenate", "(", "self", ".", "scores_per_class", "[", "class_index", "]", ")", "\n", "tp_fp_labels", "=", "np", ".", "concatenate", "(", "\n", "self", ".", "tp_fp_labels_per_class", "[", "class_index", "]", "\n", ")", "\n", "", "if", "self", ".", "use_weighted_mean_ap", ":", "\n", "                ", "all_scores", "=", "np", ".", "append", "(", "all_scores", ",", "scores", ")", "\n", "all_tp_fp_labels", "=", "np", ".", "append", "(", "all_tp_fp_labels", ",", "tp_fp_labels", ")", "\n", "", "precision", ",", "recall", "=", "metrics", ".", "compute_precision_recall", "(", "\n", "scores", ",", "\n", "tp_fp_labels", ",", "\n", "self", ".", "num_gt_instances_per_class", "[", "class_index", "]", ",", "\n", ")", "\n", "self", ".", "precisions_per_class", ".", "append", "(", "precision", ")", "\n", "self", ".", "recalls_per_class", ".", "append", "(", "recall", ")", "\n", "average_precision", "=", "metrics", ".", "compute_average_precision", "(", "\n", "precision", ",", "recall", "\n", ")", "\n", "self", ".", "average_precision_per_class", "[", "class_index", "]", "=", "average_precision", "\n", "\n", "", "self", ".", "corloc_per_class", "=", "metrics", ".", "compute_cor_loc", "(", "\n", "self", ".", "num_gt_imgs_per_class", ",", "\n", "self", ".", "num_images_correctly_detected_per_class", ",", "\n", ")", "\n", "\n", "if", "self", ".", "use_weighted_mean_ap", ":", "\n", "            ", "num_gt_instances", "=", "np", ".", "sum", "(", "self", ".", "num_gt_instances_per_class", ")", "\n", "precision", ",", "recall", "=", "metrics", ".", "compute_precision_recall", "(", "\n", "all_scores", ",", "all_tp_fp_labels", ",", "num_gt_instances", "\n", ")", "\n", "mean_ap", "=", "metrics", ".", "compute_average_precision", "(", "precision", ",", "recall", ")", "\n", "", "else", ":", "\n", "            ", "mean_ap", "=", "np", ".", "nanmean", "(", "self", ".", "average_precision_per_class", ")", "\n", "", "mean_corloc", "=", "np", ".", "nanmean", "(", "self", ".", "corloc_per_class", ")", "\n", "return", "ObjectDetectionEvalMetrics", "(", "\n", "self", ".", "average_precision_per_class", ",", "\n", "mean_ap", ",", "\n", "self", ".", "precisions_per_class", ",", "\n", "self", ".", "recalls_per_class", ",", "\n", "self", ".", "corloc_per_class", ",", "\n", "mean_corloc", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area": [[33, 49], ["numpy.sum", "ValueError"], "function", ["None"], ["def", "area", "(", "masks", ")", ":", "\n", "    ", "\"\"\"Computes area of masks.\n\n  Args:\n    masks: Numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas.\n\n  Raises:\n    ValueError: If masks.dtype is not np.uint8\n  \"\"\"", "\n", "if", "masks", ".", "dtype", "!=", "np", ".", "uint8", ":", "\n", "        ", "raise", "ValueError", "(", "\"Masks type should be np.uint8\"", ")", "\n", "", "return", "np", ".", "sum", "(", "masks", ",", "axis", "=", "(", "1", ",", "2", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection": [[51, 77], ["numpy.zeros", "numpy.arange", "ValueError", "numpy.arange", "numpy.sum", "numpy.minimum"], "function", ["None"], ["", "def", "intersection", "(", "masks1", ",", "masks2", ")", ":", "\n", "    ", "\"\"\"Compute pairwise intersection areas between masks.\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding M masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  \"\"\"", "\n", "if", "masks1", ".", "dtype", "!=", "np", ".", "uint8", "or", "masks2", ".", "dtype", "!=", "np", ".", "uint8", ":", "\n", "        ", "raise", "ValueError", "(", "\"masks1 and masks2 should be of type np.uint8\"", ")", "\n", "", "n", "=", "masks1", ".", "shape", "[", "0", "]", "\n", "m", "=", "masks2", ".", "shape", "[", "0", "]", "\n", "answer", "=", "np", ".", "zeros", "(", "[", "n", ",", "m", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", "in", "np", ".", "arange", "(", "n", ")", ":", "\n", "        ", "for", "j", "in", "np", ".", "arange", "(", "m", ")", ":", "\n", "            ", "answer", "[", "i", ",", "j", "]", "=", "np", ".", "sum", "(", "\n", "np", ".", "minimum", "(", "masks1", "[", "i", "]", ",", "masks2", "[", "j", "]", ")", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "", "", "return", "answer", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.iou": [[79, 105], ["np_mask_ops.intersection", "np_mask_ops.area", "np_mask_ops.area", "ValueError", "numpy.maximum", "numpy.expand_dims", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area"], ["", "def", "iou", "(", "masks1", ",", "masks2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-union between mask collections.\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  \"\"\"", "\n", "if", "masks1", ".", "dtype", "!=", "np", ".", "uint8", "or", "masks2", ".", "dtype", "!=", "np", ".", "uint8", ":", "\n", "        ", "raise", "ValueError", "(", "\"masks1 and masks2 should be of type np.uint8\"", ")", "\n", "", "intersect", "=", "intersection", "(", "masks1", ",", "masks2", ")", "\n", "area1", "=", "area", "(", "masks1", ")", "\n", "area2", "=", "area", "(", "masks2", ")", "\n", "union", "=", "(", "\n", "np", ".", "expand_dims", "(", "area1", ",", "axis", "=", "1", ")", "\n", "+", "np", ".", "expand_dims", "(", "area2", ",", "axis", "=", "0", ")", "\n", "-", "intersect", "\n", ")", "\n", "return", "intersect", "/", "np", ".", "maximum", "(", "union", ",", "EPSILON", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.ioa": [[107, 131], ["np_mask_ops.intersection", "numpy.expand_dims", "ValueError", "np_mask_ops.area"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.intersection", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_mask_ops.area"], ["", "def", "ioa", "(", "masks1", ",", "masks2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as\n  their intersection area over mask2's area. Note that ioa is not symmetric,\n  that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  \"\"\"", "\n", "if", "masks1", ".", "dtype", "!=", "np", ".", "uint8", "or", "masks2", ".", "dtype", "!=", "np", ".", "uint8", ":", "\n", "        ", "raise", "ValueError", "(", "\"masks1 and masks2 should be of type np.uint8\"", ")", "\n", "", "intersect", "=", "intersection", "(", "masks1", ",", "masks2", ")", "\n", "areas", "=", "np", ".", "expand_dims", "(", "area", "(", "masks2", ")", ",", "axis", "=", "0", ")", "\n", "return", "intersect", "/", "(", "areas", "+", "EPSILON", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.predictor.ActionPredictor.__init__": [[24, 39], ["slowfast.models.build_model", "predictor.ActionPredictor.model.eval", "logger.info", "slowfast.utils.misc.log_model_info", "logger.info", "slowfast.load_test_checkpoint", "logger.info"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n        \"\"\"", "\n", "# Build the video model and print model statistics.", "\n", "self", ".", "model", "=", "build_model", "(", "cfg", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "cfg", "=", "cfg", "\n", "logger", ".", "info", "(", "\"Start loading model info\"", ")", "\n", "misc", ".", "log_model_info", "(", "self", ".", "model", ",", "cfg", ",", "use_train_input", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"Start loading model weights\"", ")", "\n", "cu", ".", "load_test_checkpoint", "(", "cfg", ",", "self", ".", "model", ")", "\n", "logger", ".", "info", "(", "\"Finish loading model weights\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.predictor.ActionPredictor.__call__": [[40, 102], ["slowfast.visualization.utils.process_cv2_inputs", "preds.cpu.cpu.detach", "task.add_action_preds", "slowfast.datasets.cv2_transform.scale_boxes", "slowfast.datasets.cv2_transform.scale", "torch.full", "torch.cat", "isinstance", "torch.tensor", "predictor.ActionPredictor.model", "preds.cpu.cpu.cpu", "task.add_bboxes", "cv2.cvtColor", "range", "inputs.cuda.cuda.cuda", "bboxes.cpu.cpu.cpu", "float", "len", "inputs[].cuda"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.process_cv2_inputs", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_action_preds", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_bboxes"], ["", "def", "__call__", "(", "self", ",", "task", ")", ":", "\n", "        ", "\"\"\"\n        Returns the prediction results for the current task.\n        Args:\n            task (TaskInfo object): task object that contain\n                the necessary information for action prediction. (e.g. frames, boxes)\n        Returns:\n            task (TaskInfo object): the same task info object but filled with\n                prediction values (a tensor) and the corresponding boxes for\n                action detection task.\n        \"\"\"", "\n", "frames", ",", "bboxes", "=", "task", ".", "frames", ",", "task", ".", "bboxes", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "            ", "bboxes", "=", "cv2_transform", ".", "scale_boxes", "(", "\n", "self", ".", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "\n", "bboxes", ",", "\n", "task", ".", "img_height", ",", "\n", "task", ".", "img_width", ",", "\n", ")", "\n", "", "if", "self", ".", "cfg", ".", "DEMO", ".", "INPUT_FORMAT", "==", "\"BGR\"", ":", "\n", "            ", "frames", "=", "[", "\n", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2RGB", ")", "for", "frame", "in", "frames", "\n", "]", "\n", "\n", "", "frames", "=", "[", "\n", "cv2_transform", ".", "scale", "(", "self", ".", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "frame", ")", "\n", "for", "frame", "in", "frames", "\n", "]", "\n", "inputs", "=", "process_cv2_inputs", "(", "frames", ",", "self", ".", "cfg", ")", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "            ", "index_pad", "=", "torch", ".", "full", "(", "\n", "size", "=", "(", "bboxes", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "\n", "fill_value", "=", "float", "(", "0", ")", ",", "\n", "device", "=", "bboxes", ".", "device", ",", "\n", ")", "\n", "\n", "# Pad frame index for each box.", "\n", "bboxes", "=", "torch", ".", "cat", "(", "[", "index_pad", ",", "bboxes", "]", ",", "axis", "=", "1", ")", "\n", "", "if", "self", ".", "cfg", ".", "NUM_GPUS", ">", "0", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "            ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                    ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "if", "self", ".", "cfg", ".", "DETECTION", ".", "ENABLE", "and", "not", "bboxes", ".", "shape", "[", "0", "]", ":", "\n", "            ", "preds", "=", "torch", ".", "tensor", "(", "[", "]", ")", "\n", "", "else", ":", "\n", "            ", "preds", "=", "self", ".", "model", "(", "inputs", ",", "bboxes", ")", "\n", "\n", "", "if", "self", ".", "cfg", ".", "NUM_GPUS", ":", "\n", "            ", "preds", "=", "preds", ".", "cpu", "(", ")", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "                ", "bboxes", "=", "bboxes", ".", "cpu", "(", ")", "\n", "\n", "", "", "preds", "=", "preds", ".", "detach", "(", ")", "\n", "\n", "task", ".", "add_action_preds", "(", "preds", ")", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "            ", "task", ".", "add_bboxes", "(", "bboxes", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "", "return", "task", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.predictor.Detectron2Predictor.__init__": [[110, 129], ["detectron2.config.get_cfg", "predictor.Detectron2Predictor.cfg.merge_from_file", "logger.info", "detectron2.engine.DefaultPredictor", "detectron2.model_zoo.get_config_file"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.config.defaults.get_cfg"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n        \"\"\"", "\n", "\n", "self", ".", "cfg", "=", "get_cfg", "(", ")", "\n", "self", ".", "cfg", ".", "merge_from_file", "(", "\n", "model_zoo", ".", "get_config_file", "(", "cfg", ".", "DEMO", ".", "DETECTRON2_CFG", ")", "\n", ")", "\n", "self", ".", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SCORE_THRESH_TEST", "=", "cfg", ".", "DEMO", ".", "DETECTRON2_THRESH", "\n", "self", ".", "cfg", ".", "MODEL", ".", "WEIGHTS", "=", "cfg", ".", "DEMO", ".", "DETECTRON2_WEIGHTS", "\n", "self", ".", "cfg", ".", "INPUT", ".", "FORMAT", "=", "cfg", ".", "DEMO", ".", "INPUT_FORMAT", "\n", "self", ".", "cfg", ".", "MODEL", ".", "DEVICE", "=", "\"cuda:0\"", "if", "cfg", ".", "NUM_GPUS", ">", "0", "else", "\"cpu\"", "\n", "\n", "logger", ".", "info", "(", "\"Initialized Detectron2 Object Detection Model.\"", ")", "\n", "\n", "self", ".", "predictor", "=", "DefaultPredictor", "(", "self", ".", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.predictor.Detectron2Predictor.__call__": [[130, 149], ["predictor.Detectron2Predictor.predictor", "task.add_bboxes", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_bboxes"], ["", "def", "__call__", "(", "self", ",", "task", ")", ":", "\n", "        ", "\"\"\"\n        Return bounding boxes predictions as a tensor.\n        Args:\n            task (TaskInfo object): task object that contain\n                the necessary information for action prediction. (e.g. frames, boxes)\n        Returns:\n            task (TaskInfo object): the same task info object but filled with\n                prediction values (a tensor) and the corresponding boxes for\n                action detection task.\n        \"\"\"", "\n", "middle_frame", "=", "task", ".", "frames", "[", "len", "(", "task", ".", "frames", ")", "//", "2", "]", "\n", "outputs", "=", "self", ".", "predictor", "(", "middle_frame", ")", "\n", "# Get only human instances", "\n", "mask", "=", "outputs", "[", "\"instances\"", "]", ".", "pred_classes", "==", "0", "\n", "pred_boxes", "=", "outputs", "[", "\"instances\"", "]", ".", "pred_boxes", ".", "tensor", "[", "mask", "]", "\n", "task", ".", "add_bboxes", "(", "pred_boxes", ")", "\n", "\n", "return", "task", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.predictor.draw_predictions": [[151, 194], ["slowfast.datasets.cv2_transform.revert_scaled_boxes", "video_vis.draw_clip_range", "len", "len", "video_vis.draw_clip_range"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.revert_scaled_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip_range", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip_range"], ["", "", "def", "draw_predictions", "(", "task", ",", "video_vis", ")", ":", "\n", "    ", "\"\"\"\n    Draw prediction for the given task.\n    Args:\n        task (TaskInfo object): task object that contain\n            the necessary information for visualization. (e.g. frames, preds)\n            All attributes must lie on CPU devices.\n        video_vis (VideoVisualizer object): the video visualizer object.\n    Returns:\n        frames (list of ndarray): visualized frames in the clip.\n    \"\"\"", "\n", "boxes", "=", "task", ".", "bboxes", "\n", "frames", "=", "task", ".", "frames", "\n", "preds", "=", "task", ".", "action_preds", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "        ", "img_width", "=", "task", ".", "img_width", "\n", "img_height", "=", "task", ".", "img_height", "\n", "boxes", "=", "cv2_transform", ".", "revert_scaled_boxes", "(", "\n", "task", ".", "crop_size", ",", "boxes", ",", "img_height", ",", "img_width", "\n", ")", "\n", "\n", "", "keyframe_idx", "=", "len", "(", "frames", ")", "//", "2", "-", "task", ".", "num_buffer_frames", "\n", "draw_range", "=", "[", "\n", "keyframe_idx", "-", "task", ".", "clip_vis_size", ",", "\n", "keyframe_idx", "+", "task", ".", "clip_vis_size", ",", "\n", "]", "\n", "frames", "=", "frames", "[", "task", ".", "num_buffer_frames", ":", "]", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "        ", "if", "len", "(", "boxes", ")", "!=", "0", ":", "\n", "            ", "frames", "=", "video_vis", ".", "draw_clip_range", "(", "\n", "frames", ",", "\n", "preds", ",", "\n", "boxes", ",", "\n", "keyframe_idx", "=", "keyframe_idx", ",", "\n", "draw_range", "=", "draw_range", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "frames", "=", "video_vis", ".", "draw_clip_range", "(", "\n", "frames", ",", "preds", ",", "keyframe_idx", "=", "keyframe_idx", ",", "draw_range", "=", "draw_range", "\n", ")", "\n", "", "del", "task", "\n", "\n", "return", "frames", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.__init__": [[48, 60], ["detectron2.utils.visualizer.Visualizer.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "img_rgb", ",", "meta", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            img_rgb: a tensor or numpy array of shape (H, W, C), where H and W correspond to\n                the height and width of the image respectively. C is the number of\n                color channels. The image is required to be in RGB format since that\n                is a requirement of the Matplotlib library. The image is also expected\n                to be in the range [0, 255].\n            meta (MetadataCatalog): image metadata.\n        \"\"\"", "\n", "super", "(", "ImgVisualizer", ",", "self", ")", ".", "__init__", "(", "img_rgb", ",", "meta", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_text": [[61, 107], ["video_visualizer.ImgVisualizer.output.ax.text"], "methods", ["None"], ["", "def", "draw_text", "(", "\n", "self", ",", "\n", "text", ",", "\n", "position", ",", "\n", "*", ",", "\n", "font_size", "=", "None", ",", "\n", "color", "=", "\"w\"", ",", "\n", "horizontal_alignment", "=", "\"center\"", ",", "\n", "vertical_alignment", "=", "\"bottom\"", ",", "\n", "box_facecolor", "=", "\"black\"", ",", "\n", "alpha", "=", "0.5", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Draw text at the specified position.\n        Args:\n            text (str): the text to draw on image.\n            position (list of 2 ints): the x,y coordinate to place the text.\n            font_size (Optional[int]): font of the text. If not provided, a font size\n                proportional to the image width is calculated and used.\n            color (str): color of the text. Refer to `matplotlib.colors` for full list\n                of formats that are accepted.\n            horizontal_alignment (str): see `matplotlib.text.Text`.\n            vertical_alignment (str): see `matplotlib.text.Text`.\n            box_facecolor (str): color of the box wrapped around the text. Refer to\n                `matplotlib.colors` for full list of formats that are accepted.\n            alpha (float): transparency level of the box.\n        \"\"\"", "\n", "if", "not", "font_size", ":", "\n", "            ", "font_size", "=", "self", ".", "_default_font_size", "\n", "", "x", ",", "y", "=", "position", "\n", "self", ".", "output", ".", "ax", ".", "text", "(", "\n", "x", ",", "\n", "y", ",", "\n", "text", ",", "\n", "size", "=", "font_size", "*", "self", ".", "output", ".", "scale", ",", "\n", "family", "=", "\"monospace\"", ",", "\n", "bbox", "=", "{", "\n", "\"facecolor\"", ":", "box_facecolor", ",", "\n", "\"alpha\"", ":", "alpha", ",", "\n", "\"pad\"", ":", "0.7", ",", "\n", "\"edgecolor\"", ":", "\"none\"", ",", "\n", "}", ",", "\n", "verticalalignment", "=", "vertical_alignment", ",", "\n", "horizontalalignment", "=", "horizontal_alignment", ",", "\n", "color", "=", "color", ",", "\n", "zorder", "=", "10", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text": [[109, 175], ["int", "video_visualizer.ImgVisualizer.draw_multiple_text_upward", "video_visualizer.ImgVisualizer.draw_multiple_text_downward", "isinstance", "len", "len", "video_visualizer.ImgVisualizer._align_y_top", "len", "len", "len", "video_visualizer.ImgVisualizer._align_y_bottom", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text_upward", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text_downward", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_y_top", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_y_bottom"], ["", "def", "draw_multiple_text", "(", "\n", "self", ",", "\n", "text_ls", ",", "\n", "box_coordinate", ",", "\n", "*", ",", "\n", "top_corner", "=", "True", ",", "\n", "font_size", "=", "None", ",", "\n", "color", "=", "\"w\"", ",", "\n", "box_facecolors", "=", "\"black\"", ",", "\n", "alpha", "=", "0.5", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Draw a list of text labels for some bounding box on the image.\n        Args:\n            text_ls (list of strings): a list of text labels.\n            box_coordinate (tensor): shape (4,). The (x_left, y_top, x_right, y_bottom)\n                coordinates of the box.\n            top_corner (bool): If True, draw the text labels at (x_left, y_top) of the box.\n                Else, draw labels at (x_left, y_bottom).\n            font_size (Optional[int]): font of the text. If not provided, a font size\n                proportional to the image width is calculated and used.\n            color (str): color of the text. Refer to `matplotlib.colors` for full list\n                of formats that are accepted.\n            box_facecolors (str): colors of the box wrapped around the text. Refer to\n                `matplotlib.colors` for full list of formats that are accepted.\n            alpha (float): transparency level of the box.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "box_facecolors", ",", "list", ")", ":", "\n", "            ", "box_facecolors", "=", "[", "box_facecolors", "]", "*", "len", "(", "text_ls", ")", "\n", "", "assert", "len", "(", "box_facecolors", ")", "==", "len", "(", "\n", "text_ls", "\n", ")", ",", "\"Number of colors provided is not equal to the number of text labels.\"", "\n", "if", "not", "font_size", ":", "\n", "            ", "font_size", "=", "self", ".", "_default_font_size", "\n", "", "text_box_width", "=", "font_size", "+", "font_size", "//", "2", "\n", "# If the texts does not fit in the assigned location,", "\n", "# we split the text and draw it in another place.", "\n", "if", "top_corner", ":", "\n", "            ", "num_text_split", "=", "self", ".", "_align_y_top", "(", "\n", "box_coordinate", ",", "len", "(", "text_ls", ")", ",", "text_box_width", "\n", ")", "\n", "y_corner", "=", "1", "\n", "", "else", ":", "\n", "            ", "num_text_split", "=", "len", "(", "text_ls", ")", "-", "self", ".", "_align_y_bottom", "(", "\n", "box_coordinate", ",", "len", "(", "text_ls", ")", ",", "text_box_width", "\n", ")", "\n", "y_corner", "=", "3", "\n", "\n", "", "num_text_split", "=", "int", "(", "num_text_split", ")", "\n", "self", ".", "draw_multiple_text_upward", "(", "\n", "text_ls", "[", ":", "num_text_split", "]", ",", "\n", "box_coordinate", ",", "\n", "y_corner", "=", "y_corner", ",", "\n", "font_size", "=", "font_size", ",", "\n", "color", "=", "color", ",", "\n", "box_facecolors", "=", "box_facecolors", "[", ":", "num_text_split", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n", "self", ".", "draw_multiple_text_downward", "(", "\n", "text_ls", "[", "num_text_split", ":", "]", ",", "\n", "box_coordinate", ",", "\n", "y_corner", "=", "y_corner", ",", "\n", "font_size", "=", "font_size", ",", "\n", "color", "=", "color", ",", "\n", "box_facecolors", "=", "box_facecolors", "[", "num_text_split", ":", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text_upward": [[177, 229], ["video_visualizer.ImgVisualizer._align_x_coordinate", "box_coordinate[].item", "enumerate", "isinstance", "len", "len", "video_visualizer.ImgVisualizer.draw_text", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_x_coordinate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_text"], ["", "def", "draw_multiple_text_upward", "(", "\n", "self", ",", "\n", "text_ls", ",", "\n", "box_coordinate", ",", "\n", "*", ",", "\n", "y_corner", "=", "1", ",", "\n", "font_size", "=", "None", ",", "\n", "color", "=", "\"w\"", ",", "\n", "box_facecolors", "=", "\"black\"", ",", "\n", "alpha", "=", "0.5", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Draw a list of text labels for some bounding box on the image in upward direction.\n        The next text label will be on top of the previous one.\n        Args:\n            text_ls (list of strings): a list of text labels.\n            box_coordinate (tensor): shape (4,). The (x_left, y_top, x_right, y_bottom)\n                coordinates of the box.\n            y_corner (int): Value of either 1 or 3. Indicate the index of the y-coordinate of\n                the box to draw labels around.\n            font_size (Optional[int]): font of the text. If not provided, a font size\n                proportional to the image width is calculated and used.\n            color (str): color of the text. Refer to `matplotlib.colors` for full list\n                of formats that are accepted.\n            box_facecolors (str or list of strs): colors of the box wrapped around the text. Refer to\n                `matplotlib.colors` for full list of formats that are accepted.\n            alpha (float): transparency level of the box.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "box_facecolors", ",", "list", ")", ":", "\n", "            ", "box_facecolors", "=", "[", "box_facecolors", "]", "*", "len", "(", "text_ls", ")", "\n", "", "assert", "len", "(", "box_facecolors", ")", "==", "len", "(", "\n", "text_ls", "\n", ")", ",", "\"Number of colors provided is not equal to the number of text labels.\"", "\n", "\n", "assert", "y_corner", "in", "[", "1", ",", "3", "]", ",", "\"Y_corner must be either 1 or 3\"", "\n", "if", "not", "font_size", ":", "\n", "            ", "font_size", "=", "self", ".", "_default_font_size", "\n", "\n", "", "x", ",", "horizontal_alignment", "=", "self", ".", "_align_x_coordinate", "(", "box_coordinate", ")", "\n", "y", "=", "box_coordinate", "[", "y_corner", "]", ".", "item", "(", ")", "\n", "for", "i", ",", "text", "in", "enumerate", "(", "text_ls", ")", ":", "\n", "            ", "self", ".", "draw_text", "(", "\n", "text", ",", "\n", "(", "x", ",", "y", ")", ",", "\n", "font_size", "=", "font_size", ",", "\n", "color", "=", "color", ",", "\n", "horizontal_alignment", "=", "horizontal_alignment", ",", "\n", "vertical_alignment", "=", "\"bottom\"", ",", "\n", "box_facecolor", "=", "box_facecolors", "[", "i", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n", "y", "-=", "font_size", "+", "font_size", "//", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text_downward": [[230, 282], ["video_visualizer.ImgVisualizer._align_x_coordinate", "box_coordinate[].item", "enumerate", "isinstance", "len", "len", "video_visualizer.ImgVisualizer.draw_text", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_x_coordinate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_text"], ["", "", "def", "draw_multiple_text_downward", "(", "\n", "self", ",", "\n", "text_ls", ",", "\n", "box_coordinate", ",", "\n", "*", ",", "\n", "y_corner", "=", "1", ",", "\n", "font_size", "=", "None", ",", "\n", "color", "=", "\"w\"", ",", "\n", "box_facecolors", "=", "\"black\"", ",", "\n", "alpha", "=", "0.5", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Draw a list of text labels for some bounding box on the image in downward direction.\n        The next text label will be below the previous one.\n        Args:\n            text_ls (list of strings): a list of text labels.\n            box_coordinate (tensor): shape (4,). The (x_left, y_top, x_right, y_bottom)\n                coordinates of the box.\n            y_corner (int): Value of either 1 or 3. Indicate the index of the y-coordinate of\n                the box to draw labels around.\n            font_size (Optional[int]): font of the text. If not provided, a font size\n                proportional to the image width is calculated and used.\n            color (str): color of the text. Refer to `matplotlib.colors` for full list\n                of formats that are accepted.\n            box_facecolors (str): colors of the box wrapped around the text. Refer to\n                `matplotlib.colors` for full list of formats that are accepted.\n            alpha (float): transparency level of the box.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "box_facecolors", ",", "list", ")", ":", "\n", "            ", "box_facecolors", "=", "[", "box_facecolors", "]", "*", "len", "(", "text_ls", ")", "\n", "", "assert", "len", "(", "box_facecolors", ")", "==", "len", "(", "\n", "text_ls", "\n", ")", ",", "\"Number of colors provided is not equal to the number of text labels.\"", "\n", "\n", "assert", "y_corner", "in", "[", "1", ",", "3", "]", ",", "\"Y_corner must be either 1 or 3\"", "\n", "if", "not", "font_size", ":", "\n", "            ", "font_size", "=", "self", ".", "_default_font_size", "\n", "\n", "", "x", ",", "horizontal_alignment", "=", "self", ".", "_align_x_coordinate", "(", "box_coordinate", ")", "\n", "y", "=", "box_coordinate", "[", "y_corner", "]", ".", "item", "(", ")", "\n", "for", "i", ",", "text", "in", "enumerate", "(", "text_ls", ")", ":", "\n", "            ", "self", ".", "draw_text", "(", "\n", "text", ",", "\n", "(", "x", ",", "y", ")", ",", "\n", "font_size", "=", "font_size", ",", "\n", "color", "=", "color", ",", "\n", "horizontal_alignment", "=", "horizontal_alignment", ",", "\n", "vertical_alignment", "=", "\"top\"", ",", "\n", "box_facecolor", "=", "box_facecolors", "[", "i", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n", "y", "+=", "font_size", "+", "font_size", "//", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_x_coordinate": [[283, 304], ["None"], "methods", ["None"], ["", "", "def", "_align_x_coordinate", "(", "self", ",", "box_coordinate", ")", ":", "\n", "        ", "\"\"\"\n            Choose an x-coordinate from the box to make sure the text label\n            does not go out of frames. By default, the left x-coordinate is\n            chosen and text is aligned left. If the box is too close to the\n            right side of the image, then the right x-coordinate is chosen\n            instead and the text is aligned right.\n            Args:\n                box_coordinate (array-like): shape (4,). The (x_left, y_top, x_right, y_bottom)\n                coordinates of the box.\n            Returns:\n                x_coordinate (float): the chosen x-coordinate.\n                alignment (str): whether to align left or right.\n        \"\"\"", "\n", "# If the x-coordinate is greater than 5/6 of the image width,", "\n", "# then we align test to the right of the box. This is", "\n", "# chosen by heuristics.", "\n", "if", "box_coordinate", "[", "0", "]", ">", "(", "self", ".", "output", ".", "width", "*", "5", ")", "//", "6", ":", "\n", "            ", "return", "box_coordinate", "[", "2", "]", ",", "\"right\"", "\n", "\n", "", "return", "box_coordinate", "[", "0", "]", ",", "\"left\"", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_y_top": [[305, 322], ["isinstance", "min", "int", "int.item"], "methods", ["None"], ["", "def", "_align_y_top", "(", "self", ",", "box_coordinate", ",", "num_text", ",", "textbox_width", ")", ":", "\n", "        ", "\"\"\"\n            Calculate the number of text labels to plot on top of the box\n            without going out of frames.\n            Args:\n                box_coordinate (array-like): shape (4,). The (x_left, y_top, x_right, y_bottom)\n                coordinates of the box.\n                num_text (int): the number of text labels to plot.\n                textbox_width (float): the width of the box wrapped around text label.\n        \"\"\"", "\n", "dist_to_top", "=", "box_coordinate", "[", "1", "]", "\n", "num_text_top", "=", "dist_to_top", "//", "textbox_width", "\n", "\n", "if", "isinstance", "(", "num_text_top", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "num_text_top", "=", "int", "(", "num_text_top", ".", "item", "(", ")", ")", "\n", "\n", "", "return", "min", "(", "num_text", ",", "num_text_top", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer._align_y_bottom": [[323, 340], ["isinstance", "min", "int", "int.item"], "methods", ["None"], ["", "def", "_align_y_bottom", "(", "self", ",", "box_coordinate", ",", "num_text", ",", "textbox_width", ")", ":", "\n", "        ", "\"\"\"\n            Calculate the number of text labels to plot at the bottom of the box\n            without going out of frames.\n            Args:\n                box_coordinate (array-like): shape (4,). The (x_left, y_top, x_right, y_bottom)\n                coordinates of the box.\n                num_text (int): the number of text labels to plot.\n                textbox_width (float): the width of the box wrapped around text label.\n        \"\"\"", "\n", "dist_to_bottom", "=", "self", ".", "output", ".", "height", "-", "box_coordinate", "[", "3", "]", "\n", "num_text_bottom", "=", "dist_to_bottom", "//", "textbox_width", "\n", "\n", "if", "isinstance", "(", "num_text_bottom", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "num_text_bottom", "=", "int", "(", "num_text_bottom", ".", "item", "(", ")", ")", "\n", "\n", "", "return", "min", "(", "num_text", ",", "num_text_bottom", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.__init__": [[343, 359], ["slowfast.utils.misc.get_class_names", "matplotlib.get_cmap"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_class_names"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_classes", ",", "class_names_path", ",", "top_k", "=", "1", ",", "colormap", "=", "\"rainbow\"", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            num_classes (int): total number of classes.\n            class_names_path (str): path to json file that maps class names to ids.\n                Must be in the format {classname: id}.\n            top_k (int): number of top predicted classes to plot.\n            colormap (str): the colormap to choose color for class labels from.\n                See https://matplotlib.org/tutorials/colors/colormaps.html\n        \"\"\"", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "class_names", ",", "_", ",", "_", "=", "get_class_names", "(", "class_names_path", ",", "None", ",", "None", ")", "\n", "self", ".", "top_k", "=", "top_k", "\n", "self", ".", "color_map", "=", "plt", ".", "get_cmap", "(", "colormap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer._get_color": [[360, 367], ["video_visualizer.VideoVisualizer.color_map"], "methods", ["None"], ["", "def", "_get_color", "(", "self", ",", "class_id", ")", ":", "\n", "        ", "\"\"\"\n        Get color for a class id.\n        Args:\n            class_id (int): class id.\n        \"\"\"", "\n", "return", "self", ".", "color_map", "(", "class_id", "/", "self", ".", "num_classes", ")", "[", ":", "3", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_one_frame": [[368, 471], ["isinstance", "range", "video_visualizer.ImgVisualizer", "min", "ImgVisualizer.output.get_image", "isinstance", "torch.topk", "text_labels.append", "max", "enumerate", "video_visualizer.ImgVisualizer.draw_multiple_text", "preds.unsqueeze.unsqueeze.unsqueeze", "len", "logger.error", "top_scores.tolist", "top_classes.tolist", "video_visualizer._create_text_labels", "len", "len", "len", "len", "ImgVisualizer.draw_box", "video_visualizer.ImgVisualizer.draw_multiple_text", "video_visualizer.VideoVisualizer._get_color", "torch.Tensor", "numpy.sqrt", "video_visualizer.VideoVisualizer._get_color"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer._create_text_labels", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.ImgVisualizer.draw_multiple_text", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer._get_color", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer._get_color"], ["", "def", "draw_one_frame", "(", "\n", "self", ",", "\n", "frame", ",", "\n", "preds", ",", "\n", "bboxes", "=", "None", ",", "\n", "alpha", "=", "0.5", ",", "\n", "text_alpha", "=", "0.7", ",", "\n", "ground_truth", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n            Draw labels and bouding boxes for one image. By default, predicted labels are drawn in\n            the top left corner of the image or corresponding bounding boxes. For ground truth labels\n            (setting True for ground_truth flag), labels will be drawn in the bottom left corner.\n            Args:\n                frame (array-like): a tensor or numpy array of shape (H, W, C), where H and W correspond to\n                    the height and width of the image respectively. C is the number of\n                    color channels. The image is required to be in RGB format since that\n                    is a requirement of the Matplotlib library. The image is also expected\n                    to be in the range [0, 255].\n                preds (tensor or list): If ground_truth is False, provide a float\n                    tensor of shape (num_boxes, num_classes)\n                    that contains all of the confidence scores of the model.\n                    For recognition task, input shape can be (num_classes,). To plot true label (ground_truth is True),\n                    preds is a list contains int32 of the shape (num_boxes, true_class_ids) or (true_class_ids,).\n                bboxes (Optional[tensor]): shape (num_boxes, 4) that contains the coordinates of the bounding boxes.\n                alpha (Optional[float]): transparency level of the bounding boxes.\n                text_alpha (Optional[float]): transparency level of the box wrapped around text labels.\n                ground_truth (bool): whether the prodived bounding boxes are ground-truth.\n        \"\"\"", "\n", "if", "isinstance", "(", "preds", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "if", "preds", ".", "ndim", "==", "1", ":", "\n", "                ", "preds", "=", "preds", ".", "unsqueeze", "(", "0", ")", "\n", "", "n_instances", "=", "preds", ".", "shape", "[", "0", "]", "\n", "", "elif", "isinstance", "(", "preds", ",", "list", ")", ":", "\n", "            ", "n_instances", "=", "len", "(", "preds", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "error", "(", "\"Unsupported type of prediction input.\"", ")", "\n", "return", "\n", "\n", "", "if", "ground_truth", ":", "\n", "            ", "top_scores", ",", "top_classes", "=", "[", "None", "]", "*", "n_instances", ",", "preds", "\n", "\n", "", "else", ":", "\n", "            ", "top_scores", ",", "top_classes", "=", "torch", ".", "topk", "(", "preds", ",", "k", "=", "self", ".", "top_k", ")", "\n", "top_scores", ",", "top_classes", "=", "top_scores", ".", "tolist", "(", ")", ",", "top_classes", ".", "tolist", "(", ")", "\n", "\n", "# Create labels top k predicted classes with their scores.", "\n", "", "text_labels", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_instances", ")", ":", "\n", "            ", "text_labels", ".", "append", "(", "\n", "_create_text_labels", "(", "\n", "top_classes", "[", "i", "]", ",", "\n", "top_scores", "[", "i", "]", ",", "\n", "self", ".", "class_names", ",", "\n", "ground_truth", "=", "ground_truth", ",", "\n", ")", "\n", ")", "\n", "", "frame_visualizer", "=", "ImgVisualizer", "(", "frame", ",", "meta", "=", "None", ")", "\n", "font_size", "=", "min", "(", "\n", "max", "(", "np", ".", "sqrt", "(", "frame", ".", "shape", "[", "0", "]", "*", "frame", ".", "shape", "[", "1", "]", ")", "//", "35", ",", "5", ")", ",", "9", "\n", ")", "\n", "top_corner", "=", "not", "ground_truth", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "preds", ")", "==", "len", "(", "\n", "bboxes", "\n", ")", ",", "\"Encounter {} predictions and {} bounding boxes\"", ".", "format", "(", "\n", "len", "(", "preds", ")", ",", "len", "(", "bboxes", ")", "\n", ")", "\n", "for", "i", ",", "box", "in", "enumerate", "(", "bboxes", ")", ":", "\n", "                ", "text", "=", "text_labels", "[", "i", "]", "\n", "pred_class", "=", "top_classes", "[", "i", "]", "\n", "colors", "=", "[", "self", ".", "_get_color", "(", "pred", ")", "for", "pred", "in", "pred_class", "]", "\n", "\n", "box_color", "=", "\"r\"", "if", "ground_truth", "else", "\"g\"", "\n", "line_style", "=", "\"--\"", "if", "ground_truth", "else", "\"-.\"", "\n", "frame_visualizer", ".", "draw_box", "(", "\n", "box", ",", "\n", "alpha", "=", "alpha", ",", "\n", "edge_color", "=", "box_color", ",", "\n", "line_style", "=", "line_style", ",", "\n", ")", "\n", "frame_visualizer", ".", "draw_multiple_text", "(", "\n", "text", ",", "\n", "box", ",", "\n", "top_corner", "=", "top_corner", ",", "\n", "font_size", "=", "font_size", ",", "\n", "box_facecolors", "=", "colors", ",", "\n", "alpha", "=", "text_alpha", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "text", "=", "text_labels", "[", "0", "]", "\n", "pred_class", "=", "top_classes", "[", "0", "]", "\n", "colors", "=", "[", "self", ".", "_get_color", "(", "pred", ")", "for", "pred", "in", "pred_class", "]", "\n", "frame_visualizer", ".", "draw_multiple_text", "(", "\n", "text", ",", "\n", "torch", ".", "Tensor", "(", "[", "0", ",", "5", ",", "frame", ".", "shape", "[", "1", "]", ",", "frame", ".", "shape", "[", "0", "]", "-", "5", "]", ")", ",", "\n", "top_corner", "=", "top_corner", ",", "\n", "font_size", "=", "font_size", ",", "\n", "box_facecolors", "=", "colors", ",", "\n", "alpha", "=", "text_alpha", ",", "\n", ")", "\n", "\n", "", "return", "frame_visualizer", ".", "output", ".", "get_image", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip_range": [[472, 526], ["max", "list", "len", "list", "video_visualizer.VideoVisualizer.draw_clip", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip"], ["", "def", "draw_clip_range", "(", "\n", "self", ",", "\n", "frames", ",", "\n", "preds", ",", "\n", "bboxes", "=", "None", ",", "\n", "text_alpha", "=", "0.5", ",", "\n", "ground_truth", "=", "False", ",", "\n", "keyframe_idx", "=", "None", ",", "\n", "draw_range", "=", "None", ",", "\n", "repeat_frame", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n            Draw predicted labels or ground truth classes to clip. Draw bouding boxes to clip\n            if bboxes is provided. Boxes will gradually fade in and out the clip, centered around\n            the clip's central frame, within the provided `draw_range`.\n            Args:\n                frames (array-like): video data in the shape (T, H, W, C).\n                preds (tensor): a tensor of shape (num_boxes, num_classes) that contains all of the confidence scores\n                    of the model. For recognition task or for ground_truth labels, input shape can be (num_classes,).\n                bboxes (Optional[tensor]): shape (num_boxes, 4) that contains the coordinates of the bounding boxes.\n                text_alpha (float): transparency label of the box wrapped around text labels.\n                ground_truth (bool): whether the prodived bounding boxes are ground-truth.\n                keyframe_idx (int): the index of keyframe in the clip.\n                draw_range (Optional[list[ints]): only draw frames\n                    in range [start_idx, end_idx] inclusively in the clip.\n                    If None, draw on the entire clip.\n                repeat_frame (int): repeat each frame in draw_range for `repeat_frame` time for slow-motion effect.\n        \"\"\"", "\n", "if", "draw_range", "is", "None", ":", "\n", "            ", "draw_range", "=", "[", "0", ",", "len", "(", "frames", ")", "-", "1", "]", "\n", "", "if", "draw_range", "is", "not", "None", ":", "\n", "            ", "draw_range", "[", "0", "]", "=", "max", "(", "0", ",", "draw_range", "[", "0", "]", ")", "\n", "left_frames", "=", "frames", "[", ":", "draw_range", "[", "0", "]", "]", "\n", "right_frames", "=", "frames", "[", "draw_range", "[", "1", "]", "+", "1", ":", "]", "\n", "\n", "", "draw_frames", "=", "frames", "[", "draw_range", "[", "0", "]", ":", "draw_range", "[", "1", "]", "+", "1", "]", "\n", "if", "keyframe_idx", "is", "None", ":", "\n", "            ", "keyframe_idx", "=", "len", "(", "frames", ")", "//", "2", "\n", "\n", "", "img_ls", "=", "(", "\n", "list", "(", "left_frames", ")", "\n", "+", "self", ".", "draw_clip", "(", "\n", "draw_frames", ",", "\n", "preds", ",", "\n", "bboxes", "=", "bboxes", ",", "\n", "text_alpha", "=", "text_alpha", ",", "\n", "ground_truth", "=", "ground_truth", ",", "\n", "keyframe_idx", "=", "keyframe_idx", "-", "draw_range", "[", "0", "]", ",", "\n", "repeat_frame", "=", "repeat_frame", ",", "\n", ")", "\n", "+", "list", "(", "right_frames", ")", "\n", ")", "\n", "\n", "return", "img_ls", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip": [[527, 593], ["range", "list", "video_visualizer.VideoVisualizer._adjust_frames_type", "numpy.concatenate", "zip", "len", "itertools.chain.from_iterable", "int", "video_visualizer.VideoVisualizer.draw_one_frame", "img_ls.append", "len", "len", "numpy.linspace", "numpy.linspace", "itertools.repeat", "len", "len", "video_visualizer.VideoVisualizer.astype", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer._adjust_frames_type", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_one_frame"], ["", "def", "draw_clip", "(", "\n", "self", ",", "\n", "frames", ",", "\n", "preds", ",", "\n", "bboxes", "=", "None", ",", "\n", "text_alpha", "=", "0.5", ",", "\n", "ground_truth", "=", "False", ",", "\n", "keyframe_idx", "=", "None", ",", "\n", "repeat_frame", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n            Draw predicted labels or ground truth classes to clip. Draw bouding boxes to clip\n            if bboxes is provided. Boxes will gradually fade in and out the clip, centered around\n            the clip's central frame.\n            Args:\n                frames (array-like): video data in the shape (T, H, W, C).\n                preds (tensor): a tensor of shape (num_boxes, num_classes) that contains all of the confidence scores\n                    of the model. For recognition task or for ground_truth labels, input shape can be (num_classes,).\n                bboxes (Optional[tensor]): shape (num_boxes, 4) that contains the coordinates of the bounding boxes.\n                text_alpha (float): transparency label of the box wrapped around text labels.\n                ground_truth (bool): whether the prodived bounding boxes are ground-truth.\n                keyframe_idx (int): the index of keyframe in the clip.\n                repeat_frame (int): repeat each frame in draw_range for `repeat_frame` time for slow-motion effect.\n        \"\"\"", "\n", "assert", "repeat_frame", ">=", "1", ",", "\"`repeat_frame` must be a positive integer.\"", "\n", "\n", "repeated_seq", "=", "range", "(", "0", ",", "len", "(", "frames", ")", ")", "\n", "repeated_seq", "=", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "itertools", ".", "repeat", "(", "x", ",", "repeat_frame", ")", "for", "x", "in", "repeated_seq", "\n", ")", "\n", ")", "\n", "\n", "frames", ",", "adjusted", "=", "self", ".", "_adjust_frames_type", "(", "frames", ")", "\n", "if", "keyframe_idx", "is", "None", ":", "\n", "            ", "half_left", "=", "len", "(", "repeated_seq", ")", "//", "2", "\n", "half_right", "=", "(", "len", "(", "repeated_seq", ")", "+", "1", ")", "//", "2", "\n", "", "else", ":", "\n", "            ", "mid", "=", "int", "(", "(", "keyframe_idx", "/", "len", "(", "frames", ")", ")", "*", "len", "(", "repeated_seq", ")", ")", "\n", "half_left", "=", "mid", "\n", "half_right", "=", "len", "(", "repeated_seq", ")", "-", "mid", "\n", "\n", "", "alpha_ls", "=", "np", ".", "concatenate", "(", "\n", "[", "\n", "np", ".", "linspace", "(", "0", ",", "1", ",", "num", "=", "half_left", ")", ",", "\n", "np", ".", "linspace", "(", "1", ",", "0", ",", "num", "=", "half_right", ")", ",", "\n", "]", "\n", ")", "\n", "text_alpha", "=", "text_alpha", "\n", "frames", "=", "frames", "[", "repeated_seq", "]", "\n", "img_ls", "=", "[", "]", "\n", "for", "alpha", ",", "frame", "in", "zip", "(", "alpha_ls", ",", "frames", ")", ":", "\n", "            ", "draw_img", "=", "self", ".", "draw_one_frame", "(", "\n", "frame", ",", "\n", "preds", ",", "\n", "bboxes", ",", "\n", "alpha", "=", "alpha", ",", "\n", "text_alpha", "=", "text_alpha", ",", "\n", "ground_truth", "=", "ground_truth", ",", "\n", ")", "\n", "if", "adjusted", ":", "\n", "                ", "draw_img", "=", "draw_img", ".", "astype", "(", "\"float32\"", ")", "/", "255", "\n", "\n", "", "img_ls", ".", "append", "(", "draw_img", ")", "\n", "\n", "", "return", "img_ls", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer._adjust_frames_type": [[594, 615], ["numpy.array", "frames.astype.astype.astype", "len", "numpy.array"], "methods", ["None"], ["", "def", "_adjust_frames_type", "(", "self", ",", "frames", ")", ":", "\n", "        ", "\"\"\"\n            Modify video data to have dtype of uint8 and values range in [0, 255].\n            Args:\n                frames (array-like): 4D array of shape (T, H, W, C).\n            Returns:\n                frames (list of frames): list of frames in range [0, 1].\n                adjusted (bool): whether the original frames need adjusted.\n        \"\"\"", "\n", "assert", "(", "\n", "frames", "is", "not", "None", "and", "len", "(", "frames", ")", "!=", "0", "\n", ")", ",", "\"Frames does not contain any values\"", "\n", "frames", "=", "np", ".", "array", "(", "frames", ")", "\n", "assert", "np", ".", "array", "(", "frames", ")", ".", "ndim", "==", "4", ",", "\"Frames must have 4 dimensions\"", "\n", "adjusted", "=", "False", "\n", "if", "frames", ".", "dtype", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ":", "\n", "            ", "frames", "*=", "255", "\n", "frames", "=", "frames", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "adjusted", "=", "True", "\n", "\n", "", "return", "frames", ",", "adjusted", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer._create_text_labels": [[18, 45], ["logger.error", "len", "len", "zip"], "function", ["None"], ["def", "_create_text_labels", "(", "classes", ",", "scores", ",", "class_names", ",", "ground_truth", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Create text labels.\n    Args:\n        classes (list[int]): a list of class ids for each example.\n        scores (list[float] or None): list of scores for each example.\n        class_names (list[str]): a list of class names, ordered by their ids.\n        ground_truth (bool): whether the labels are ground truth.\n    Returns:\n        labels (list[str]): formatted text labels.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "labels", "=", "[", "class_names", "[", "i", "]", "for", "i", "in", "classes", "]", "\n", "", "except", "IndexError", ":", "\n", "        ", "logger", ".", "error", "(", "\"Class indices get out of range: {}\"", ".", "format", "(", "classes", ")", ")", "\n", "return", "None", "\n", "\n", "", "if", "ground_truth", ":", "\n", "        ", "labels", "=", "[", "\"[{}] {}\"", ".", "format", "(", "\"GT\"", ",", "label", ")", "for", "label", "in", "labels", "]", "\n", "", "elif", "scores", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "classes", ")", "==", "len", "(", "scores", ")", "\n", "labels", "=", "[", "\n", "\"[{:.0f}] {}\"", ".", "format", "(", "s", "*", "100", ",", "label", ")", "\n", "for", "s", ",", "label", "in", "zip", "(", "scores", ",", "labels", ")", "\n", "]", "\n", "\n", "", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.__init__": [[14, 56], ["cv2.VideoCapture", "demo_loader.VideoReader.cap.set", "demo_loader.VideoReader.cap.set", "int", "int", "demo_loader.VideoReader.cap.isOpened", "IOError", "demo_loader.VideoReader.get_output_file", "demo_loader.VideoReader.cap.get", "demo_loader.VideoReader.cap.get", "demo_loader.VideoReader.cap.get"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_output_file", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        \"\"\"", "\n", "assert", "(", "\n", "cfg", ".", "DEMO", ".", "WEBCAM", ">", "-", "1", "or", "cfg", ".", "DEMO", ".", "INPUT_VIDEO", "!=", "\"\"", "\n", ")", ",", "\"Must specify a data source as input.\"", "\n", "\n", "self", ".", "source", "=", "(", "\n", "cfg", ".", "DEMO", ".", "WEBCAM", "if", "cfg", ".", "DEMO", ".", "WEBCAM", ">", "-", "1", "else", "cfg", ".", "DEMO", ".", "INPUT_VIDEO", "\n", ")", "\n", "\n", "self", ".", "display_width", "=", "cfg", ".", "DEMO", ".", "DISPLAY_WIDTH", "\n", "self", ".", "display_height", "=", "cfg", ".", "DEMO", ".", "DISPLAY_HEIGHT", "\n", "\n", "self", ".", "cap", "=", "cv2", ".", "VideoCapture", "(", "self", ".", "source", ")", "\n", "\n", "if", "self", ".", "display_width", ">", "0", "and", "self", ".", "display_height", ">", "0", ":", "\n", "            ", "self", ".", "cap", ".", "set", "(", "cv2", ".", "CAP_PROP_FRAME_WIDTH", ",", "self", ".", "display_width", ")", "\n", "self", ".", "cap", ".", "set", "(", "cv2", ".", "CAP_PROP_FRAME_HEIGHT", ",", "self", ".", "display_height", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "display_width", "=", "int", "(", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_WIDTH", ")", ")", "\n", "self", ".", "display_height", "=", "int", "(", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_HEIGHT", ")", ")", "\n", "\n", "", "if", "not", "self", ".", "cap", ".", "isOpened", "(", ")", ":", "\n", "            ", "raise", "IOError", "(", "\"Video {} cannot be opened\"", ".", "format", "(", "self", ".", "source", ")", ")", "\n", "\n", "", "self", ".", "output_file", "=", "None", "\n", "if", "cfg", ".", "DEMO", ".", "OUTPUT_FILE", "!=", "\"\"", ":", "\n", "            ", "if", "cfg", ".", "DEMO", ".", "OUTPUT_FPS", "==", "-", "1", ":", "\n", "                ", "output_fps", "=", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FPS", ")", "\n", "", "else", ":", "\n", "                ", "output_fps", "=", "cfg", ".", "DEMO", ".", "OUTPUT_FPS", "\n", "", "self", ".", "output_file", "=", "self", ".", "get_output_file", "(", "\n", "cfg", ".", "DEMO", ".", "OUTPUT_FILE", ",", "fps", "=", "output_fps", "\n", ")", "\n", "", "self", ".", "id", "=", "-", "1", "\n", "self", ".", "buffer", "=", "[", "]", "\n", "self", ".", "buffer_size", "=", "cfg", ".", "DEMO", ".", "BUFFER_SIZE", "\n", "self", ".", "seq_length", "=", "cfg", ".", "DATA", ".", "NUM_FRAMES", "*", "cfg", ".", "DATA", ".", "SAMPLING_RATE", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.__iter__": [[57, 59], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.__next__": [[60, 84], ["slowfast.visualization.utils.TaskInfo", "slowfast.visualization.utils.TaskInfo.add_frames", "len", "demo_loader.VideoReader.cap.read", "frames.append", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_frames"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Read and return the required number of frames for 1 clip.\n        Returns:\n            was_read (bool): False if not enough frames to return.\n            task (TaskInfo object): object contains metadata for the current clips.\n        \"\"\"", "\n", "self", ".", "id", "+=", "1", "\n", "task", "=", "TaskInfo", "(", ")", "\n", "\n", "frames", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "buffer", ")", "!=", "0", ":", "\n", "            ", "frames", "=", "self", ".", "buffer", "\n", "", "was_read", "=", "True", "\n", "while", "was_read", "and", "len", "(", "frames", ")", "<", "self", ".", "seq_length", ":", "\n", "            ", "was_read", ",", "frame", "=", "self", ".", "cap", ".", "read", "(", ")", "\n", "frames", ".", "append", "(", "frame", ")", "\n", "", "if", "was_read", ":", "\n", "            ", "self", ".", "buffer", "=", "frames", "[", "-", "self", ".", "buffer_size", ":", "]", "\n", "\n", "", "task", ".", "add_frames", "(", "self", ".", "id", ",", "frames", ")", "\n", "task", ".", "num_buffer_frames", "=", "0", "if", "self", ".", "id", "==", "0", "else", "self", ".", "buffer_size", "\n", "\n", "return", "was_read", ",", "task", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.get_output_file": [[85, 98], ["cv2.VideoWriter", "cv2.VideoWriter_fourcc", "float"], "methods", ["None"], ["", "def", "get_output_file", "(", "self", ",", "path", ",", "fps", "=", "30", ")", ":", "\n", "        ", "\"\"\"\n        Return a video writer object.\n        Args:\n            path (str): path to the output video file.\n            fps (int or float): frames per second.\n        \"\"\"", "\n", "return", "cv2", ".", "VideoWriter", "(", "\n", "filename", "=", "path", ",", "\n", "fourcc", "=", "cv2", ".", "VideoWriter_fourcc", "(", "*", "\"mp4v\"", ")", ",", "\n", "fps", "=", "float", "(", "fps", ")", ",", "\n", "frameSize", "=", "(", "self", ".", "display_width", ",", "self", ".", "display_height", ")", ",", "\n", "isColor", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.display": [[100, 109], ["cv2.imshow", "demo_loader.VideoReader.output_file.write"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.write"], ["", "def", "display", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"\n        Either display a single frame (BGR image) to a window or write to\n        an output file if output path is provided.\n        \"\"\"", "\n", "if", "self", ".", "output_file", "is", "None", ":", "\n", "            ", "cv2", ".", "imshow", "(", "\"SlowFast\"", ",", "frame", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output_file", ".", "write", "(", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.demo_loader.VideoReader.clean": [[110, 119], ["demo_loader.VideoReader.cap.release", "cv2.destroyAllWindows", "demo_loader.VideoReader.output_file.release"], "methods", ["None"], ["", "", "def", "clean", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Clean up open video files and windows.\n        \"\"\"", "\n", "self", ".", "cap", ".", "release", "(", ")", "\n", "if", "self", ".", "output_file", "is", "None", ":", "\n", "            ", "cv2", ".", "destroyAllWindows", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output_file", ".", "release", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.__init__": [[25, 88], ["torch.utils.tensorboard.SummaryWriter", "logger.info", "os.path.join", "os.path.join", "slowfast.utils.misc.get_class_names", "slowfast.utils.misc.get_class_names", "logger.info", "logger.info", "slowfast.utils.misc.get_class_names", "slowfast.utils.misc.get_class_names"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_class_names", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_class_names", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_class_names", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.get_class_names"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n        \"\"\"", "\n", "# class_names: list of class names.", "\n", "# cm_subset_classes: a list of class ids -- a user-specified subset.", "\n", "# parent_map: dictionary where key is the parent class name and", "\n", "#   value is a list of ids of its children classes.", "\n", "# hist_subset_classes: a list of class ids -- user-specified to plot histograms.", "\n", "(", "\n", "self", ".", "class_names", ",", "\n", "self", ".", "cm_subset_classes", ",", "\n", "self", ".", "parent_map", ",", "\n", "self", ".", "hist_subset_classes", ",", "\n", ")", "=", "(", "None", ",", "None", ",", "None", ",", "None", ")", "\n", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "cm_figsize", "=", "cfg", ".", "TENSORBOARD", ".", "CONFUSION_MATRIX", ".", "FIGSIZE", "\n", "self", ".", "hist_figsize", "=", "cfg", ".", "TENSORBOARD", ".", "HISTOGRAM", ".", "FIGSIZE", "\n", "\n", "if", "cfg", ".", "TENSORBOARD", ".", "LOG_DIR", "==", "\"\"", ":", "\n", "            ", "log_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "cfg", ".", "OUTPUT_DIR", ",", "\"runs-{}\"", ".", "format", "(", "cfg", ".", "TRAIN", ".", "DATASET", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "log_dir", "=", "os", ".", "path", ".", "join", "(", "cfg", ".", "OUTPUT_DIR", ",", "cfg", ".", "TENSORBOARD", ".", "LOG_DIR", ")", "\n", "\n", "", "self", ".", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "log_dir", ")", "\n", "logger", ".", "info", "(", "\n", "\"To see logged results in Tensorboard, please launch using the command \\\n            `tensorboard  --port=<port-number> --logdir {}`\"", ".", "format", "(", "\n", "log_dir", "\n", ")", "\n", ")", "\n", "\n", "if", "cfg", ".", "TENSORBOARD", ".", "CLASS_NAMES_PATH", "!=", "\"\"", ":", "\n", "            ", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Plotting confusion matrix is currently \\\n                    not supported for detection.\"", "\n", ")", "\n", "", "(", "\n", "self", ".", "class_names", ",", "\n", "self", ".", "parent_map", ",", "\n", "self", ".", "cm_subset_classes", ",", "\n", ")", "=", "get_class_names", "(", "\n", "cfg", ".", "TENSORBOARD", ".", "CLASS_NAMES_PATH", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "CATEGORIES_PATH", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "CONFUSION_MATRIX", ".", "SUBSET_PATH", ",", "\n", ")", "\n", "\n", "if", "cfg", ".", "TENSORBOARD", ".", "HISTOGRAM", ".", "ENABLE", ":", "\n", "                ", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "                    ", "logger", ".", "info", "(", "\n", "\"Plotting histogram is not currently \\\n                    supported for detection tasks.\"", "\n", ")", "\n", "", "if", "cfg", ".", "TENSORBOARD", ".", "HISTOGRAM", ".", "SUBSET_PATH", "!=", "\"\"", ":", "\n", "                    ", "_", ",", "_", ",", "self", ".", "hist_subset_classes", "=", "get_class_names", "(", "\n", "cfg", ".", "TENSORBOARD", ".", "CLASS_NAMES_PATH", ",", "\n", "None", ",", "\n", "cfg", ".", "TENSORBOARD", ".", "HISTOGRAM", ".", "SUBSET_PATH", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_scalars": [[90, 100], ["data_dict.items", "tensorboard_vis.TensorboardWriter.writer.add_scalar"], "methods", ["None"], ["", "", "", "", "def", "add_scalars", "(", "self", ",", "data_dict", ",", "global_step", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Add multiple scalars to Tensorboard logs.\n        Args:\n            data_dict (dict): key is a string specifying the tag of value.\n            global_step (Optinal[int]): Global step value to record.\n        \"\"\"", "\n", "if", "self", ".", "writer", "is", "not", "None", ":", "\n", "            ", "for", "key", ",", "item", "in", "data_dict", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "writer", ".", "add_scalar", "(", "key", ",", "item", ",", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.plot_eval": [[101, 169], ["slowfast.get_confusion_matrix", "slowfast.get_confusion_matrix", "tensorboard_vis.add_confusion_matrix", "tensorboard_vis.plot_hist", "tensorboard_vis.add_confusion_matrix", "tensorboard_vis.TensorboardWriter.parent_map.items", "slowfast.get_confusion_matrix", "slowfast.get_confusion_matrix", "tensorboard_vis.add_confusion_matrix"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_confusion_matrix", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_confusion_matrix", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_confusion_matrix", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.plot_hist", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_confusion_matrix", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_confusion_matrix", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_confusion_matrix", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_confusion_matrix"], ["", "", "", "def", "plot_eval", "(", "self", ",", "preds", ",", "labels", ",", "global_step", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Plot confusion matrices and histograms for eval/test set.\n        Args:\n            preds (tensor or list of tensors): list of predictions.\n            labels (tensor or list of tensors): list of labels.\n            global step (Optional[int]): current step in eval/test.\n        \"\"\"", "\n", "if", "not", "self", ".", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "            ", "cmtx", "=", "None", "\n", "if", "self", ".", "cfg", ".", "TENSORBOARD", ".", "CONFUSION_MATRIX", ".", "ENABLE", ":", "\n", "                ", "cmtx", "=", "vis_utils", ".", "get_confusion_matrix", "(", "\n", "preds", ",", "labels", ",", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", "\n", ")", "\n", "# Add full confusion matrix.", "\n", "add_confusion_matrix", "(", "\n", "self", ".", "writer", ",", "\n", "cmtx", ",", "\n", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "global_step", "=", "global_step", ",", "\n", "class_names", "=", "self", ".", "class_names", ",", "\n", "figsize", "=", "self", ".", "cm_figsize", ",", "\n", ")", "\n", "# If a list of subset is provided, plot confusion matrix subset.", "\n", "if", "self", ".", "cm_subset_classes", "is", "not", "None", ":", "\n", "                    ", "add_confusion_matrix", "(", "\n", "self", ".", "writer", ",", "\n", "cmtx", ",", "\n", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "global_step", "=", "global_step", ",", "\n", "subset_ids", "=", "self", ".", "cm_subset_classes", ",", "\n", "class_names", "=", "self", ".", "class_names", ",", "\n", "tag", "=", "\"Confusion Matrix Subset\"", ",", "\n", "figsize", "=", "self", ".", "cm_figsize", ",", "\n", ")", "\n", "# If a parent-child classes mapping is provided, plot confusion", "\n", "# matrices grouped by parent classes.", "\n", "", "if", "self", ".", "parent_map", "is", "not", "None", ":", "\n", "# Get list of tags (parent categories names) and their children.", "\n", "                    ", "for", "parent_class", ",", "children_ls", "in", "self", ".", "parent_map", ".", "items", "(", ")", ":", "\n", "                        ", "tag", "=", "(", "\n", "\"Confusion Matrices Grouped by Parent Classes/\"", "\n", "+", "parent_class", "\n", ")", "\n", "add_confusion_matrix", "(", "\n", "self", ".", "writer", ",", "\n", "cmtx", ",", "\n", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "global_step", "=", "global_step", ",", "\n", "subset_ids", "=", "children_ls", ",", "\n", "class_names", "=", "self", ".", "class_names", ",", "\n", "tag", "=", "tag", ",", "\n", "figsize", "=", "self", ".", "cm_figsize", ",", "\n", ")", "\n", "", "", "", "if", "self", ".", "cfg", ".", "TENSORBOARD", ".", "HISTOGRAM", ".", "ENABLE", ":", "\n", "                ", "if", "cmtx", "is", "None", ":", "\n", "                    ", "cmtx", "=", "vis_utils", ".", "get_confusion_matrix", "(", "\n", "preds", ",", "labels", ",", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", "\n", ")", "\n", "", "plot_hist", "(", "\n", "self", ".", "writer", ",", "\n", "cmtx", ",", "\n", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "self", ".", "cfg", ".", "TENSORBOARD", ".", "HISTOGRAM", ".", "TOPK", ",", "\n", "global_step", "=", "global_step", ",", "\n", "subset_ids", "=", "self", ".", "hist_subset_classes", ",", "\n", "class_names", "=", "self", ".", "class_names", ",", "\n", "figsize", "=", "self", ".", "hist_figsize", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_video": [[171, 182], ["tensorboard_vis.TensorboardWriter.writer.add_video"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.add_video"], ["", "", "", "def", "add_video", "(", "self", ",", "vid_tensor", ",", "tag", "=", "\"Video Input\"", ",", "global_step", "=", "None", ",", "fps", "=", "4", ")", ":", "\n", "        ", "\"\"\"\n        Add input to tensorboard SummaryWriter as a video.\n        Args:\n            vid_tensor (tensor): shape of (B, T, C, H, W). Values should lie\n                [0, 255] for type uint8 or [0, 1] for type float.\n            tag (Optional[str]): name of the video.\n            global_step(Optional[int]): current step.\n            fps (int): frames per second.\n        \"\"\"", "\n", "self", ".", "writer", ".", "add_video", "(", "tag", ",", "vid_tensor", ",", "global_step", "=", "global_step", ",", "fps", "=", "fps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.plot_weights_and_activations": [[183, 225], ["weight_activation_dict.items", "tensorboard_vis.add_ndim_array", "list", "range"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_ndim_array"], ["", "def", "plot_weights_and_activations", "(", "\n", "self", ",", "\n", "weight_activation_dict", ",", "\n", "tag", "=", "\"\"", ",", "\n", "normalize", "=", "False", ",", "\n", "global_step", "=", "None", ",", "\n", "batch_idx", "=", "None", ",", "\n", "indexing_dict", "=", "None", ",", "\n", "heat_map", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Visualize weights/ activations tensors to Tensorboard.\n        Args:\n            weight_activation_dict (dict[str, tensor]): a dictionary of the pair {layer_name: tensor},\n                where layer_name is a string and tensor is the weights/activations of\n                the layer we want to visualize.\n            tag (Optional[str]): name of the video.\n            normalize (bool): If True, the tensor is normalized. (Default to False)\n            global_step(Optional[int]): current step.\n            batch_idx (Optional[int]): current batch index to visualize. If None,\n                visualize the entire batch.\n            indexing_dict (Optional[dict]): a dictionary of the {layer_name: indexing}.\n                where indexing is numpy-like fancy indexing.\n            heatmap (bool): whether to add heatmap to the weights/ activations.\n        \"\"\"", "\n", "for", "name", ",", "array", "in", "weight_activation_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "batch_idx", "is", "None", ":", "\n", "# Select all items in the batch if batch_idx is not provided.", "\n", "                ", "batch_idx", "=", "list", "(", "range", "(", "array", ".", "shape", "[", "0", "]", ")", ")", "\n", "", "if", "indexing_dict", "is", "not", "None", ":", "\n", "                ", "fancy_indexing", "=", "indexing_dict", "[", "name", "]", "\n", "fancy_indexing", "=", "(", "batch_idx", ",", ")", "+", "fancy_indexing", "\n", "array", "=", "array", "[", "fancy_indexing", "]", "\n", "", "else", ":", "\n", "                ", "array", "=", "array", "[", "batch_idx", "]", "\n", "", "add_ndim_array", "(", "\n", "self", ".", "writer", ",", "\n", "array", ",", "\n", "tag", "+", "name", ",", "\n", "normalize", "=", "normalize", ",", "\n", "global_step", "=", "global_step", ",", "\n", "heat_map", "=", "heat_map", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.flush": [[227, 229], ["tensorboard_vis.TensorboardWriter.writer.flush"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.flush"], ["", "", "def", "flush", "(", "self", ")", ":", "\n", "        ", "self", ".", "writer", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.close": [[230, 233], ["tensorboard_vis.TensorboardWriter.writer.flush", "tensorboard_vis.TensorboardWriter.writer.close"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.TensorboardWriter.flush", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "writer", ".", "flush", "(", ")", "\n", "self", ".", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_confusion_matrix": [[235, 278], ["slowfast.plot_confusion_matrix", "writer.add_figure", "len", "list", "str", "range", "len", "range"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.plot_confusion_matrix"], ["", "", "def", "add_confusion_matrix", "(", "\n", "writer", ",", "\n", "cmtx", ",", "\n", "num_classes", ",", "\n", "global_step", "=", "None", ",", "\n", "subset_ids", "=", "None", ",", "\n", "class_names", "=", "None", ",", "\n", "tag", "=", "\"Confusion Matrix\"", ",", "\n", "figsize", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Calculate and plot confusion matrix to a SummaryWriter.\n    Args:\n        writer (SummaryWriter): the SummaryWriter to write the matrix to.\n        cmtx (ndarray): confusion matrix.\n        num_classes (int): total number of classes.\n        global_step (Optional[int]): current step.\n        subset_ids (list of ints): a list of label indices to keep.\n        class_names (list of strs, optional): a list of all class names.\n        tag (str or list of strs): name(s) of the confusion matrix image.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].\n\n    \"\"\"", "\n", "if", "subset_ids", "is", "None", "or", "len", "(", "subset_ids", ")", "!=", "0", ":", "\n", "# If class names are not provided, use class indices as class names.", "\n", "        ", "if", "class_names", "is", "None", ":", "\n", "            ", "class_names", "=", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_classes", ")", "]", "\n", "# If subset is not provided, take every classes.", "\n", "", "if", "subset_ids", "is", "None", ":", "\n", "            ", "subset_ids", "=", "list", "(", "range", "(", "num_classes", ")", ")", "\n", "\n", "", "sub_cmtx", "=", "cmtx", "[", "subset_ids", ",", ":", "]", "[", ":", ",", "subset_ids", "]", "\n", "sub_names", "=", "[", "class_names", "[", "j", "]", "for", "j", "in", "subset_ids", "]", "\n", "\n", "sub_cmtx", "=", "vis_utils", ".", "plot_confusion_matrix", "(", "\n", "sub_cmtx", ",", "\n", "num_classes", "=", "len", "(", "subset_ids", ")", ",", "\n", "class_names", "=", "sub_names", ",", "\n", "figsize", "=", "figsize", ",", "\n", ")", "\n", "# Add the confusion matrix image to writer.", "\n", "writer", ".", "add_figure", "(", "tag", "=", "tag", ",", "figure", "=", "sub_cmtx", ",", "global_step", "=", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.plot_hist": [[280, 329], ["len", "set", "set", "list", "slowfast.plot_topk_histogram", "writer.add_figure", "range", "range", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.plot_topk_histogram"], ["", "", "def", "plot_hist", "(", "\n", "writer", ",", "\n", "cmtx", ",", "\n", "num_classes", ",", "\n", "k", "=", "10", ",", "\n", "global_step", "=", "None", ",", "\n", "subset_ids", "=", "None", ",", "\n", "class_names", "=", "None", ",", "\n", "figsize", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Given all predictions and all true labels, plot histograms of top-k most\n    frequently predicted classes for each true class.\n\n    Args:\n        writer (SummaryWriter object): a tensorboard SummaryWriter object.\n        cmtx (ndarray): confusion matrix.\n        num_classes (int): total number of classes.\n        k (int): top k to plot histograms.\n        global_step (Optional[int]): current step.\n        subset_ids (list of ints, optional): class indices to plot histogram.\n        mapping (list of strings): names of all classes.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].\n    \"\"\"", "\n", "if", "subset_ids", "is", "None", "or", "len", "(", "subset_ids", ")", "!=", "0", ":", "\n", "        ", "if", "subset_ids", "is", "None", ":", "\n", "            ", "subset_ids", "=", "set", "(", "range", "(", "num_classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "subset_ids", "=", "set", "(", "subset_ids", ")", "\n", "# If class names are not provided, use their indices as names.", "\n", "", "if", "class_names", "is", "None", ":", "\n", "            ", "class_names", "=", "list", "(", "range", "(", "num_classes", ")", ")", "\n", "\n", "", "for", "i", "in", "subset_ids", ":", "\n", "            ", "pred", "=", "cmtx", "[", "i", "]", "\n", "hist", "=", "vis_utils", ".", "plot_topk_histogram", "(", "\n", "class_names", "[", "i", "]", ",", "\n", "torch", ".", "Tensor", "(", "pred", ")", ",", "\n", "k", ",", "\n", "class_names", ",", "\n", "figsize", "=", "figsize", ",", "\n", ")", "\n", "writer", ".", "add_figure", "(", "\n", "tag", "=", "\"Top {} predictions by classes/{}\"", ".", "format", "(", "\n", "k", ",", "class_names", "[", "i", "]", "\n", ")", ",", "\n", "figure", "=", "hist", ",", "\n", "global_step", "=", "global_step", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_ndim_array": [[332, 406], ["array.unsqueeze", "reshaped_array.unsqueeze.view", "int", "tensorboard_vis.add_heatmap", "writer.add_image", "writer.add_image", "array.view", "torchvision.utils.make_grid", "writer.add_image", "math.sqrt", "tensorboard_vis.add_heatmap", "writer.add_image", "writer.add_image", "array.size", "torch.cat", "reshaped_array.unsqueeze.unsqueeze", "int", "add_heatmap().unsqueeze", "math.sqrt", "reshaped_array.unsqueeze.size", "tensorboard_vis.add_heatmap", "reshaped_array.unsqueeze.size"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_heatmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_heatmap", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_heatmap"], ["", "", "", "def", "add_ndim_array", "(", "\n", "writer", ",", "\n", "array", ",", "\n", "name", ",", "\n", "nrow", "=", "None", ",", "\n", "normalize", "=", "False", ",", "\n", "global_step", "=", "None", ",", "\n", "heat_map", "=", "True", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Visualize and add tensors of n-dimentionals to a Tensorboard SummaryWriter. Tensors\n    will be visualized as a 2D grid image.\n    Args:\n        writer (SummaryWriter): Tensorboard SummaryWriter.\n        array (tensor): tensor to visualize.\n        name (str): name of the tensor.\n        nrow (Optional[int]): number of 2D filters in each row in the grid image.\n        normalize (bool): whether to normalize when we have multiple 2D filters.\n            Default to False.\n        global_step (Optional[int]): current step.\n        heat_map (bool): whether to add heat map to 2D each 2D filters in array.\n    \"\"\"", "\n", "if", "array", "is", "not", "None", "and", "array", ".", "ndim", "!=", "0", ":", "\n", "        ", "if", "array", ".", "ndim", "==", "1", ":", "\n", "            ", "reshaped_array", "=", "array", ".", "unsqueeze", "(", "0", ")", "\n", "if", "nrow", "is", "None", ":", "\n", "                ", "nrow", "=", "int", "(", "math", ".", "sqrt", "(", "reshaped_array", ".", "size", "(", ")", "[", "1", "]", ")", ")", "\n", "", "reshaped_array", "=", "reshaped_array", ".", "view", "(", "-", "1", ",", "nrow", ")", "\n", "if", "heat_map", ":", "\n", "                ", "reshaped_array", "=", "add_heatmap", "(", "reshaped_array", ")", "\n", "writer", ".", "add_image", "(", "\n", "name", ",", "\n", "reshaped_array", ",", "\n", "global_step", "=", "global_step", ",", "\n", "dataformats", "=", "\"CHW\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "writer", ".", "add_image", "(", "\n", "name", ",", "\n", "reshaped_array", ",", "\n", "global_step", "=", "global_step", ",", "\n", "dataformats", "=", "\"HW\"", ",", "\n", ")", "\n", "", "", "elif", "array", ".", "ndim", "==", "2", ":", "\n", "            ", "reshaped_array", "=", "array", "\n", "if", "heat_map", ":", "\n", "                ", "heatmap", "=", "add_heatmap", "(", "reshaped_array", ")", "\n", "writer", ".", "add_image", "(", "\n", "name", ",", "heatmap", ",", "global_step", "=", "global_step", ",", "dataformats", "=", "\"CHW\"", "\n", ")", "\n", "", "else", ":", "\n", "                ", "writer", ".", "add_image", "(", "\n", "name", ",", "\n", "reshaped_array", ",", "\n", "global_step", "=", "global_step", ",", "\n", "dataformats", "=", "\"HW\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "last2_dims", "=", "array", ".", "size", "(", ")", "[", "-", "2", ":", "]", "\n", "reshaped_array", "=", "array", ".", "view", "(", "-", "1", ",", "*", "last2_dims", ")", "\n", "if", "heat_map", ":", "\n", "                ", "reshaped_array", "=", "[", "\n", "add_heatmap", "(", "array_2d", ")", ".", "unsqueeze", "(", "0", ")", "\n", "for", "array_2d", "in", "reshaped_array", "\n", "]", "\n", "reshaped_array", "=", "torch", ".", "cat", "(", "reshaped_array", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "reshaped_array", "=", "reshaped_array", ".", "unsqueeze", "(", "1", ")", "\n", "", "if", "nrow", "is", "None", ":", "\n", "                ", "nrow", "=", "int", "(", "math", ".", "sqrt", "(", "reshaped_array", ".", "size", "(", ")", "[", "0", "]", ")", ")", "\n", "", "img_grid", "=", "make_grid", "(", "\n", "reshaped_array", ",", "nrow", ",", "padding", "=", "1", ",", "normalize", "=", "normalize", "\n", ")", "\n", "writer", ".", "add_image", "(", "name", ",", "img_grid", ",", "global_step", "=", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.tensorboard_vis.add_heatmap": [[408, 430], ["tensor.cpu.numpy", "matplotlib.get_cmap", "plt.get_cmap.", "torch.Tensor().permute", "torch.device", "tensor.cpu", "torch.Tensor"], "function", ["None"], ["", "", "", "def", "add_heatmap", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"\n    Add heatmap to 2D tensor.\n    Args:\n        tensor (tensor): a 2D tensor. Tensor value must be in [0..1] range.\n    Returns:\n        heatmap (tensor): a 3D tensor. Result of applying heatmap to the 2D tensor.\n    \"\"\"", "\n", "assert", "tensor", ".", "ndim", "==", "2", ",", "\"Only support 2D tensors.\"", "\n", "# Move tensor to cpu if necessary.", "\n", "if", "tensor", ".", "device", "!=", "torch", ".", "device", "(", "\"cpu\"", ")", ":", "\n", "        ", "arr", "=", "tensor", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "        ", "arr", "=", "tensor", "\n", "", "arr", "=", "arr", ".", "numpy", "(", ")", "\n", "# Get the color map by name.", "\n", "cm", "=", "plt", ".", "get_cmap", "(", "\"viridis\"", ")", "\n", "heatmap", "=", "cm", "(", "arr", ")", "\n", "heatmap", "=", "heatmap", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "# Convert (H, W, C) to (C, H, W)", "\n", "heatmap", "=", "torch", ".", "Tensor", "(", "heatmap", ")", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "\n", "return", "heatmap", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.__init__": [[30, 75], ["fvcore.common.file_io.PathManager.get_local_path", "fvcore.common.file_io.PathManager.isdir", "cv2.VideoCapture", "int", "int", "int", "ava_demo_precomputed_boxes.load_boxes_labels", "os.path.join", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.get", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.get", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.get", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.get", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.isOpened", "IOError", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_output_file", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.source.split", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.source.split", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.video_name.split"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.load_boxes_labels", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_output_file"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n        \"\"\"", "\n", "self", ".", "source", "=", "PathManager", ".", "get_local_path", "(", "path", "=", "cfg", ".", "DEMO", ".", "INPUT_VIDEO", ")", "\n", "self", ".", "fps", "=", "None", "\n", "if", "PathManager", ".", "isdir", "(", "self", ".", "source", ")", ":", "\n", "            ", "self", ".", "fps", "=", "cfg", ".", "DEMO", ".", "FPS", "\n", "self", ".", "video_name", "=", "self", ".", "source", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "self", ".", "source", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "source", ",", "\"{}_%06d.jpg\"", ".", "format", "(", "self", ".", "video_name", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "video_name", "=", "self", ".", "source", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "self", ".", "video_name", "=", "self", ".", "video_name", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "\n", "", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "cap", "=", "cv2", ".", "VideoCapture", "(", "self", ".", "source", ")", "\n", "if", "self", ".", "fps", "is", "None", ":", "\n", "            ", "self", ".", "fps", "=", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FPS", ")", "\n", "\n", "", "self", ".", "total_frames", "=", "int", "(", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_COUNT", ")", ")", "\n", "\n", "self", ".", "display_width", "=", "int", "(", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_WIDTH", ")", ")", "\n", "self", ".", "display_height", "=", "int", "(", "self", ".", "cap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_HEIGHT", ")", ")", "\n", "\n", "if", "not", "self", ".", "cap", ".", "isOpened", "(", ")", ":", "\n", "            ", "raise", "IOError", "(", "\"Video {} cannot be opened\"", ".", "format", "(", "self", ".", "source", ")", ")", "\n", "\n", "", "self", ".", "output_file", "=", "None", "\n", "\n", "if", "cfg", ".", "DEMO", ".", "OUTPUT_FILE", "!=", "\"\"", ":", "\n", "            ", "self", ".", "output_file", "=", "self", ".", "get_output_file", "(", "cfg", ".", "DEMO", ".", "OUTPUT_FILE", ")", "\n", "\n", "", "self", ".", "pred_boxes", ",", "self", ".", "gt_boxes", "=", "load_boxes_labels", "(", "\n", "cfg", ",", "\n", "self", ".", "video_name", ",", "\n", "self", ".", "fps", ",", "\n", "self", ".", "display_width", ",", "\n", "self", ".", "display_height", ",", "\n", ")", "\n", "\n", "self", ".", "seq_length", "=", "cfg", ".", "DATA", ".", "NUM_FRAMES", "*", "cfg", ".", "DATA", ".", "SAMPLING_RATE", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_output_file": [[76, 88], ["cv2.VideoWriter", "cv2.VideoWriter_fourcc", "float"], "methods", ["None"], ["", "def", "get_output_file", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        Return a video writer object.\n        Args:\n            path (str): path to the output video file.\n        \"\"\"", "\n", "return", "cv2", ".", "VideoWriter", "(", "\n", "filename", "=", "path", ",", "\n", "fourcc", "=", "cv2", ".", "VideoWriter_fourcc", "(", "*", "\"mp4v\"", ")", ",", "\n", "fps", "=", "float", "(", "30", ")", ",", "\n", "frameSize", "=", "(", "self", ".", "display_width", ",", "self", ".", "display_height", ")", ",", "\n", "isColor", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_input_clip": [[90, 122], ["slowfast.datasets.utils.get_sequence", "slowfast.datasets.utils.get_sequence", "slowfast.visualization.utils.process_cv2_inputs", "slowfast.visualization.utils.process_cv2_inputs", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.set", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.read", "cv2.cvtColor", "slowfast.datasets.cv2_transform.scale", "slowfast.datasets.cv2_transform.scale", "slowfast.visualization.utils.process_cv2_inputs.append", "slowfast.visualization.utils.process_cv2_inputs.append", "logger.error", "slowfast.visualization.utils.process_cv2_inputs.append", "slowfast.visualization.utils.process_cv2_inputs.append"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.get_sequence", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.get_sequence", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.process_cv2_inputs", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.process_cv2_inputs", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale"], ["", "def", "get_input_clip", "(", "self", ",", "keyframe_idx", ")", ":", "\n", "        ", "\"\"\"\n        Get input clip from the video/folder of images for a given\n        keyframe index.\n        Args:\n            keyframe_idx (int): index of the current keyframe.\n        Returns:\n            clip (list of tensors): formatted input clip(s) corresponding to\n                the current keyframe.\n        \"\"\"", "\n", "seq", "=", "get_sequence", "(", "\n", "keyframe_idx", ",", "\n", "self", ".", "seq_length", "//", "2", ",", "\n", "self", ".", "cfg", ".", "DATA", ".", "SAMPLING_RATE", ",", "\n", "self", ".", "total_frames", ",", "\n", ")", "\n", "clip", "=", "[", "]", "\n", "for", "frame_idx", "in", "seq", ":", "\n", "            ", "self", ".", "cap", ".", "set", "(", "cv2", ".", "CAP_PROP_POS_FRAMES", ",", "frame_idx", ")", "\n", "was_read", ",", "frame", "=", "self", ".", "cap", ".", "read", "(", ")", "\n", "if", "was_read", ":", "\n", "                ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2RGB", ")", "\n", "frame", "=", "scale", "(", "self", ".", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "frame", ")", "\n", "clip", ".", "append", "(", "frame", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Unable to read frame. Duplicating previous frame.\"", "\n", ")", "\n", "clip", ".", "append", "(", "clip", "[", "-", "1", "]", ")", "\n", "\n", "", "", "clip", "=", "process_cv2_inputs", "(", "clip", ",", "self", ".", "cfg", ")", "\n", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_predictions": [[123, 189], ["numpy.random.seed", "torch.manual_seed", "slowfast.setup_logging", "slowfast.setup_logging", "logger.info", "logger.info", "slowfast.models.build_model", "slowfast.models.build_model", "slowfast.models.build_model.eval", "slowfast.models.build_model.eval", "logger.info", "slowfast.utils.misc.log_model_info", "slowfast.utils.misc.log_model_info", "logger.info", "slowfast.load_test_checkpoint", "slowfast.load_test_checkpoint", "logger.info", "logger.info", "tqdm.tqdm", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.pred_boxes.items", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_input_clip", "torch.from_numpy().float", "slowfast.datasets.cv2_transform.scale_boxes", "slowfast.datasets.cv2_transform.scale_boxes", "torch.cat", "slowfast.models.build_model.", "slowfast.models.build_model.", "preds.cpu.cpu.detach", "isinstance", "box_inputs.cuda.cuda.cuda", "preds.cpu.cpu.cpu", "torch.from_numpy", "torch.full", "range", "inputs.cuda.cuda.cuda", "numpy.array", "float", "len", "inputs[].cuda"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.logging.setup_logging", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.misc.log_model_info", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.checkpoint.load_test_checkpoint", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_input_clip", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale_boxes"], ["", "def", "get_predictions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Predict and append prediction results to each box in each keyframe in\n        `self.pred_boxes` dictionary.\n        \"\"\"", "\n", "# Set random seed from configs.", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "cfg", ".", "RNG_SEED", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "cfg", ".", "RNG_SEED", ")", "\n", "\n", "# Setup logging format.", "\n", "logging", ".", "setup_logging", "(", "self", ".", "cfg", ".", "OUTPUT_DIR", ")", "\n", "\n", "# Print config.", "\n", "logger", ".", "info", "(", "\"Run demo with config:\"", ")", "\n", "logger", ".", "info", "(", "self", ".", "cfg", ")", "\n", "assert", "(", "\n", "self", ".", "cfg", ".", "NUM_GPUS", "<=", "1", "\n", ")", ",", "\"Cannot run demo visualization on multiple GPUs.\"", "\n", "\n", "# Build the video model and print model statistics.", "\n", "model", "=", "build_model", "(", "self", ".", "cfg", ")", "\n", "model", ".", "eval", "(", ")", "\n", "logger", ".", "info", "(", "\"Start loading model info\"", ")", "\n", "misc", ".", "log_model_info", "(", "model", ",", "self", ".", "cfg", ",", "use_train_input", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"Start loading model weights\"", ")", "\n", "cu", ".", "load_test_checkpoint", "(", "self", ".", "cfg", ",", "model", ")", "\n", "logger", ".", "info", "(", "\"Finish loading model weights\"", ")", "\n", "logger", ".", "info", "(", "\"Start making predictions for precomputed boxes.\"", ")", "\n", "for", "keyframe_idx", ",", "boxes_and_labels", "in", "tqdm", ".", "tqdm", "(", "self", ".", "pred_boxes", ".", "items", "(", ")", ")", ":", "\n", "            ", "inputs", "=", "self", ".", "get_input_clip", "(", "keyframe_idx", ")", "\n", "boxes", "=", "boxes_and_labels", "[", "0", "]", "\n", "boxes", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "boxes", ")", ")", ".", "float", "(", ")", "\n", "\n", "box_transformed", "=", "scale_boxes", "(", "\n", "self", ".", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", ",", "\n", "boxes", ",", "\n", "self", ".", "display_height", ",", "\n", "self", ".", "display_width", ",", "\n", ")", "\n", "\n", "# Pad frame index for each box.", "\n", "box_inputs", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "torch", ".", "full", "(", "(", "box_transformed", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "float", "(", "0", ")", ")", ",", "\n", "box_transformed", ",", "\n", "]", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "if", "self", ".", "cfg", ".", "NUM_GPUS", ":", "\n", "# Transfer the data to the current GPU device.", "\n", "                ", "if", "isinstance", "(", "inputs", ",", "(", "list", ",", ")", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "                        ", "inputs", "[", "i", "]", "=", "inputs", "[", "i", "]", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                    ", "inputs", "=", "inputs", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "", "box_inputs", "=", "box_inputs", ".", "cuda", "(", ")", "\n", "\n", "", "preds", "=", "model", "(", "inputs", ",", "box_inputs", ")", "\n", "\n", "preds", "=", "preds", ".", "detach", "(", ")", "\n", "\n", "if", "self", ".", "cfg", ".", "NUM_GPUS", ":", "\n", "                ", "preds", "=", "preds", ".", "cpu", "(", ")", "\n", "\n", "", "boxes_and_labels", "[", "1", "]", "=", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.draw_video": [[190, 300], ["ava_demo_precomputed_boxes.merge_pred_gt_boxes", "slowfast.visualization.video_visualizer.VideoVisualizer", "slowfast.visualization.video_visualizer.VideoVisualizer", "sorted", "logger.info", "tqdm.tqdm", "merge_pred_gt_boxes.keys", "max", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range", "enumerate", "cv2.cvtColor", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range", "max", "cv2.cvtColor", "max", "len", "min", "slowfast.visualization.video_visualizer.VideoVisualizer.draw_clip_range", "slowfast.visualization.video_visualizer.VideoVisualizer.draw_clip_range", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "cv2.cvtColor", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "cv2.cvtColor", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "len", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.merge_pred_gt_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip_range", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.video_visualizer.VideoVisualizer.draw_clip_range", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display"], ["", "", "def", "draw_video", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Draw predicted and ground-truth (if provided) results on the video/folder of images.\n        Write the visualized result to a video output file.\n        \"\"\"", "\n", "all_boxes", "=", "merge_pred_gt_boxes", "(", "self", ".", "pred_boxes", ",", "self", ".", "gt_boxes", ")", "\n", "\n", "video_vis", "=", "VideoVisualizer", "(", "\n", "self", ".", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "self", ".", "cfg", ".", "DEMO", ".", "LABEL_FILE_PATH", ",", "\n", "self", ".", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "TOPK_PREDS", ",", "\n", "self", ".", "cfg", ".", "TENSORBOARD", ".", "MODEL_VIS", ".", "COLORMAP", ",", "\n", ")", "\n", "all_keys", "=", "sorted", "(", "all_boxes", ".", "keys", "(", ")", ")", "\n", "no_frames_repeat", "=", "3", "\n", "# Draw around the keyframe for 2/8 of the sequence length.", "\n", "# This is chosen using heuristics.", "\n", "draw_range", "=", "[", "\n", "self", ".", "seq_length", "//", "2", "-", "self", ".", "seq_length", "//", "8", ",", "\n", "self", ".", "seq_length", "//", "2", "+", "self", ".", "seq_length", "//", "8", ",", "\n", "]", "\n", "draw_range_repeat", "=", "[", "\n", "draw_range", "[", "0", "]", ",", "\n", "(", "draw_range", "[", "1", "]", "-", "draw_range", "[", "0", "]", ")", "*", "no_frames_repeat", "+", "draw_range", "[", "0", "]", ",", "\n", "]", "\n", "prev_buffer", "=", "[", "]", "\n", "prev_end_idx", "=", "0", "\n", "\n", "logger", ".", "info", "(", "\"Start Visualization...\"", ")", "\n", "for", "keyframe_idx", "in", "tqdm", ".", "tqdm", "(", "all_keys", ")", ":", "\n", "            ", "pred_gt_boxes", "=", "all_boxes", "[", "keyframe_idx", "]", "\n", "# Find the starting index of the clip. If start_idx exceeds the beginning", "\n", "# of the video, we only choose valid frame from index 0.", "\n", "start_idx", "=", "max", "(", "0", ",", "keyframe_idx", "-", "self", ".", "seq_length", "//", "2", ")", "\n", "# Number of frames from the start of the current clip and the", "\n", "# end of the previous clip.", "\n", "dist", "=", "start_idx", "-", "prev_end_idx", "\n", "# If there are unwritten frames in between clips.", "\n", "if", "dist", ">=", "0", ":", "\n", "# Get the frames in between previous clip and current clip.", "\n", "                ", "frames", "=", "self", ".", "_get_frame_range", "(", "prev_end_idx", ",", "dist", ")", "\n", "# We keep a buffer of frames for overlapping visualization.", "\n", "# Write these to the output file.", "\n", "for", "frame", "in", "prev_buffer", ":", "\n", "                    ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2BGR", ")", "\n", "self", ".", "display", "(", "frame", ")", "\n", "# Write them to output file without any visualization", "\n", "# since they don't have any corresponding keyframes.", "\n", "", "for", "frame", "in", "frames", ":", "\n", "                    ", "self", ".", "display", "(", "frame", ")", "\n", "", "prev_buffer", "=", "[", "]", "\n", "num_new_frames", "=", "self", ".", "seq_length", "\n", "\n", "# If there are overlapping frames in between clips.", "\n", "", "elif", "dist", "<", "0", ":", "\n", "# Flush all ready frames.", "\n", "                ", "for", "frame", "in", "prev_buffer", "[", ":", "dist", "]", ":", "\n", "                    ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2BGR", ")", "\n", "self", ".", "display", "(", "frame", ")", "\n", "", "prev_buffer", "=", "prev_buffer", "[", "dist", ":", "]", "\n", "num_new_frames", "=", "self", ".", "seq_length", "+", "dist", "\n", "# Obtain new frames for the current clip from the input video file.", "\n", "", "new_frames", "=", "self", ".", "_get_frame_range", "(", "\n", "max", "(", "start_idx", ",", "prev_end_idx", ")", ",", "num_new_frames", "\n", ")", "\n", "new_frames", "=", "[", "\n", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2RGB", ")", "for", "frame", "in", "new_frames", "\n", "]", "\n", "clip", "=", "prev_buffer", "+", "new_frames", "\n", "# Calculate the end of this clip. This will be `prev_end_idx` for the", "\n", "# next iteration.", "\n", "prev_end_idx", "=", "max", "(", "start_idx", ",", "prev_end_idx", ")", "+", "len", "(", "new_frames", ")", "\n", "# For each precomputed or gt boxes.", "\n", "for", "i", ",", "boxes", "in", "enumerate", "(", "pred_gt_boxes", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "repeat", "=", "no_frames_repeat", "\n", "current_draw_range", "=", "draw_range", "\n", "", "else", ":", "\n", "                    ", "repeat", "=", "1", "\n", "current_draw_range", "=", "draw_range_repeat", "\n", "# Make sure draw range does not fall out of end of clip.", "\n", "", "current_draw_range", "[", "1", "]", "=", "min", "(", "\n", "current_draw_range", "[", "1", "]", ",", "len", "(", "clip", ")", "-", "1", "\n", ")", "\n", "ground_truth", "=", "boxes", "[", "0", "]", "\n", "bboxes", "=", "boxes", "[", "1", "]", "\n", "label", "=", "boxes", "[", "2", "]", "\n", "# Draw predictions.", "\n", "clip", "=", "video_vis", ".", "draw_clip_range", "(", "\n", "clip", ",", "\n", "label", ",", "\n", "bboxes", "=", "torch", ".", "Tensor", "(", "bboxes", ")", ",", "\n", "ground_truth", "=", "ground_truth", ",", "\n", "draw_range", "=", "current_draw_range", ",", "\n", "repeat_frame", "=", "repeat", ",", "\n", ")", "\n", "# Store the current clip as buffer.", "\n", "", "prev_buffer", "=", "clip", "\n", "\n", "# Write the remaining buffer to output file.", "\n", "", "for", "frame", "in", "prev_buffer", ":", "\n", "            ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2BGR", ")", "\n", "self", ".", "display", "(", "frame", ")", "\n", "# If we still have some remaining frames in the input file,", "\n", "# write those to the output file as well.", "\n", "", "if", "prev_end_idx", "<", "self", ".", "total_frames", ":", "\n", "            ", "dist", "=", "self", ".", "total_frames", "-", "prev_end_idx", "\n", "remaining_clip", "=", "self", ".", "_get_frame_range", "(", "prev_end_idx", ",", "dist", ")", "\n", "for", "frame", "in", "remaining_clip", ":", "\n", "                ", "self", ".", "display", "(", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.__call__": [[301, 304], ["ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_predictions", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.draw_video"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.get_predictions", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.draw_video"], ["", "", "", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "self", ".", "get_predictions", "(", ")", "\n", "self", ".", "draw_video", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.display": [[305, 314], ["cv2.imshow", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.output_file.write"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.tools.extract_feature.write"], ["", "def", "display", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"\n        Either display a single frame (BGR image) to a window or write to\n        an output file if output path is provided.\n        \"\"\"", "\n", "if", "self", ".", "output_file", "is", "None", ":", "\n", "            ", "cv2", ".", "imshow", "(", "\"SlowFast\"", ",", "frame", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output_file", ".", "write", "(", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_keyframe_clip": [[315, 326], ["max", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range"], ["", "", "def", "_get_keyframe_clip", "(", "self", ",", "keyframe_idx", ")", ":", "\n", "        ", "\"\"\"\n        Return a clip corresponding to a keyframe index for visualization.\n        Args:\n            keyframe_idx (int): keyframe index.\n        \"\"\"", "\n", "start_idx", "=", "max", "(", "0", ",", "keyframe_idx", "-", "self", ".", "seq_length", "//", "2", ")", "\n", "\n", "clip", "=", "self", ".", "_get_frame_range", "(", "start_idx", ",", "self", ".", "seq_length", ")", "\n", "\n", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox._get_frame_range": [[327, 348], ["ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.set", "range", "ava_demo_precomputed_boxes.AVAVisualizerWithPrecomputedBox.cap.read", "all_frames.append"], "methods", ["None"], ["", "def", "_get_frame_range", "(", "self", ",", "start_idx", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"\n        Return a clip of `num_frames` frames starting from `start_idx`. If not enough frames\n        from `start_idx`, return the remaining frames from `start_idx`.\n        Args:\n            start_idx (int): starting idx.\n            num_frames (int): number of frames in the returned clip.\n        \"\"\"", "\n", "was_read", "=", "True", "\n", "assert", "start_idx", "<", "self", ".", "total_frames", ",", "\"Start index out of range.\"", "\n", "\n", "self", ".", "cap", ".", "set", "(", "cv2", ".", "CAP_PROP_POS_FRAMES", ",", "start_idx", ")", "\n", "all_frames", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_frames", ")", ":", "\n", "            ", "was_read", ",", "frame", "=", "self", ".", "cap", ".", "read", "(", ")", "\n", "if", "was_read", ":", "\n", "                ", "all_frames", ".", "append", "(", "frame", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "all_frames", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.merge_pred_gt_boxes": [[350, 376], ["pred_dict.items", "gt_dict.items", "merged_dict.get", "merged_dict[].append"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "", "def", "merge_pred_gt_boxes", "(", "pred_dict", ",", "gt_dict", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Merge data from precomputed and ground-truth boxes dictionaries.\n    Args:\n        pred_dict (dict): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`.\n        gt_dict (Optional[dict]): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`. Note that label is -1 for predicted boxes.\n    Returns:\n        merged_dict (dict): merged dictionary from `pred_dict` and `gt_dict` if given.\n            It is a dict which maps from `frame_idx` to a list of [`is_gt`, `boxes`, `labels`],\n            where `is_gt` is a boolean indicate whether the `boxes` and `labels` are ground-truth.\n    \"\"\"", "\n", "merged_dict", "=", "{", "}", "\n", "for", "key", ",", "item", "in", "pred_dict", ".", "items", "(", ")", ":", "\n", "        ", "merged_dict", "[", "key", "]", "=", "[", "[", "False", ",", "item", "[", "0", "]", ",", "item", "[", "1", "]", "]", "]", "\n", "\n", "", "if", "gt_dict", "is", "not", "None", ":", "\n", "        ", "for", "key", ",", "item", "in", "gt_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "merged_dict", ".", "get", "(", "key", ")", "is", "None", ":", "\n", "                ", "merged_dict", "[", "key", "]", "=", "[", "[", "True", ",", "item", "[", "0", "]", ",", "item", "[", "1", "]", "]", "]", "\n", "", "else", ":", "\n", "                ", "merged_dict", "[", "key", "]", ".", "append", "(", "[", "True", ",", "item", "[", "0", "]", ",", "item", "[", "1", "]", "]", ")", "\n", "", "", "", "return", "merged_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.ava_demo_precomputed_boxes.load_boxes_labels": [[378, 456], ["slowfast.datasets.ava_helper.parse_bboxes_file", "ava_demo_precomputed_boxes.load_boxes_labels.process_bboxes_dict"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.parse_bboxes_file"], ["", "def", "load_boxes_labels", "(", "cfg", ",", "video_name", ",", "fps", ",", "img_width", ",", "img_height", ")", ":", "\n", "    ", "\"\"\"\n    Loading boxes and labels from AVA bounding boxes csv files.\n    Args:\n        cfg (CfgNode): config.\n        video_name (str): name of the given video.\n        fps (int or float): frames per second of the input video/images folder.\n        img_width (int): width of images in input video/images folder.\n        img_height (int): height of images in input video/images folder.\n    Returns:\n        preds_boxes (dict): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`. Note that label is -1 for predicted boxes.\n        gt_boxes (dict): if cfg.DEMO.GT_BOXES is given, return similar dict as\n            all_pred_boxes but for ground-truth boxes.\n    \"\"\"", "\n", "starting_second", "=", "cfg", ".", "DEMO", ".", "STARTING_SECOND", "\n", "\n", "def", "sec_to_frameidx", "(", "sec", ")", ":", "\n", "        ", "return", "(", "sec", "-", "starting_second", ")", "*", "fps", "\n", "\n", "", "def", "process_bboxes_dict", "(", "dictionary", ")", ":", "\n", "        ", "\"\"\"\n        Replace all `keyframe_sec` in `dictionary` with `keyframe_idx` and\n        merge all [`box_coordinate`, `box_labels`] pairs into\n        [`all_boxes_coordinates`, `all_boxes_labels`] for each `keyframe_idx`.\n        Args:\n            dictionary (dict): a dictionary which maps `frame_sec` to a list of `box`.\n                Each `box` is a [`box_coord`, `box_labels`] where `box_coord` is the\n                coordinates of box and 'box_labels` are the corresponding\n                labels for the box.\n        Returns:\n            new_dict (dict): a dict which maps from `frame_idx` to a list of `boxes`\n                and `labels`. Each `box` in `boxes` is a list of 4 box coordinates. `labels[i]`\n                is a list of labels for `boxes[i]`. Note that label is -1 for predicted boxes.\n        \"\"\"", "\n", "# Replace all keyframe_sec with keyframe_idx.", "\n", "new_dict", "=", "{", "}", "\n", "for", "keyframe_sec", ",", "boxes_and_labels", "in", "dictionary", ".", "items", "(", ")", ":", "\n", "# Ignore keyframes with no boxes", "\n", "            ", "if", "len", "(", "boxes_and_labels", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "keyframe_idx", "=", "sec_to_frameidx", "(", "keyframe_sec", ")", "\n", "boxes", ",", "labels", "=", "list", "(", "zip", "(", "*", "boxes_and_labels", ")", ")", "\n", "# Shift labels from [1, n_classes] to [0, n_classes - 1].", "\n", "labels", "=", "[", "[", "i", "-", "1", "for", "i", "in", "box_label", "]", "for", "box_label", "in", "labels", "]", "\n", "boxes", "=", "np", ".", "array", "(", "boxes", ")", "\n", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "*=", "img_width", "\n", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "*=", "img_height", "\n", "new_dict", "[", "keyframe_idx", "]", "=", "[", "boxes", ".", "tolist", "(", ")", ",", "list", "(", "labels", ")", "]", "\n", "", "return", "new_dict", "\n", "\n", "", "preds_boxes_path", "=", "cfg", ".", "DEMO", ".", "PREDS_BOXES", "\n", "gt_boxes_path", "=", "cfg", ".", "DEMO", ".", "GT_BOXES", "\n", "\n", "preds_boxes", ",", "_", ",", "_", "=", "parse_bboxes_file", "(", "\n", "ann_filenames", "=", "[", "preds_boxes_path", "]", ",", "\n", "ann_is_gt_box", "=", "[", "False", "]", ",", "\n", "detect_thresh", "=", "cfg", ".", "AVA", ".", "DETECTION_SCORE_THRESH", ",", "\n", "boxes_sample_rate", "=", "1", ",", "\n", ")", "\n", "preds_boxes", "=", "preds_boxes", "[", "video_name", "]", "\n", "if", "gt_boxes_path", "==", "\"\"", ":", "\n", "        ", "gt_boxes", "=", "None", "\n", "", "else", ":", "\n", "        ", "gt_boxes", ",", "_", ",", "_", "=", "parse_bboxes_file", "(", "\n", "ann_filenames", "=", "[", "gt_boxes_path", "]", ",", "\n", "ann_is_gt_box", "=", "[", "True", "]", ",", "\n", "detect_thresh", "=", "cfg", ".", "AVA", ".", "DETECTION_SCORE_THRESH", ",", "\n", "boxes_sample_rate", "=", "1", ",", "\n", ")", "\n", "gt_boxes", "=", "gt_boxes", "[", "video_name", "]", "\n", "\n", "", "preds_boxes", "=", "process_bboxes_dict", "(", "preds_boxes", ")", "\n", "if", "gt_boxes", "is", "not", "None", ":", "\n", "        ", "gt_boxes", "=", "process_bboxes_dict", "(", "gt_boxes", ")", "\n", "\n", "", "return", "preds_boxes", ",", "gt_boxes", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation.__init__": [[163, 177], ["utils.GetWeightAndActivation.model.eval", "utils.GetWeightAndActivation._register_hooks"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._register_hooks"], ["def", "__init__", "(", "self", ",", "model", ",", "layers", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model (nn.Module): the model containing layers to obtain weights and activations from.\n            layers (list of strings): a list of layer names to obtain weights and activations from.\n                Names are hierarchical, separated by /. For example, If a layer follow a path\n                \"s1\" ---> \"pathway0_stem\" ---> \"conv\", the layer path is \"s1/pathway0_stem/conv\".\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "hooks", "=", "{", "}", "\n", "self", ".", "layers_names", "=", "layers", "\n", "# eval mode", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "_register_hooks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._get_layer": [[178, 190], ["layer_name.split"], "methods", ["None"], ["", "def", "_get_layer", "(", "self", ",", "layer_name", ")", ":", "\n", "        ", "\"\"\"\n        Return a layer (nn.Module Object) given a hierarchical layer name, separated by /.\n        Args:\n            layer_name (str): the name of the layer.\n        \"\"\"", "\n", "layer_ls", "=", "layer_name", ".", "split", "(", "\"/\"", ")", "\n", "prev_module", "=", "self", ".", "model", "\n", "for", "layer", "in", "layer_ls", ":", "\n", "            ", "prev_module", "=", "prev_module", ".", "_modules", "[", "layer", "]", "\n", "\n", "", "return", "prev_module", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._register_single_hook": [[191, 203], ["utils.GetWeightAndActivation._get_layer", "utils.GetWeightAndActivation.register_forward_hook", "output.clone().detach", "output.clone"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._get_layer"], ["", "def", "_register_single_hook", "(", "self", ",", "layer_name", ")", ":", "\n", "        ", "\"\"\"\n        Register hook to a layer, given layer_name, to obtain activations.\n        Args:\n            layer_name (str): name of the layer.\n        \"\"\"", "\n", "\n", "def", "hook_fn", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "            ", "self", ".", "hooks", "[", "layer_name", "]", "=", "output", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n", "", "layer", "=", "self", ".", "_get_layer", "(", "layer_name", ")", "\n", "layer", ".", "register_forward_hook", "(", "hook_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._register_hooks": [[204, 210], ["utils.GetWeightAndActivation._register_single_hook"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._register_single_hook"], ["", "def", "_register_hooks", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Register hooks to layers in `self.layers_names`.\n        \"\"\"", "\n", "for", "layer_name", "in", "self", ".", "layers_names", ":", "\n", "            ", "self", ".", "_register_single_hook", "(", "layer_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation.get_activations": [[211, 235], ["utils.GetWeightAndActivation.hooks.items", "inp.clone", "utils.GetWeightAndActivation.model", "utils.GetWeightAndActivation.model"], "methods", ["None"], ["", "", "def", "get_activations", "(", "self", ",", "input", ",", "bboxes", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Obtain all activations from layers that we register hooks for.\n        Args:\n            input (tensors, list of tensors): the model input.\n            bboxes (Optional): Bouding boxes data that might be required\n                by the model.\n        Returns:\n            activation_dict (Python dictionary): a dictionary of the pair\n                {layer_name: list of activations}, where activations are outputs returned\n                by the layer.\n        \"\"\"", "\n", "input_clone", "=", "[", "inp", ".", "clone", "(", ")", "for", "inp", "in", "input", "]", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "            ", "preds", "=", "self", ".", "model", "(", "input_clone", ",", "bboxes", ")", "\n", "", "else", ":", "\n", "            ", "preds", "=", "self", ".", "model", "(", "input_clone", ")", "\n", "\n", "", "activation_dict", "=", "{", "}", "\n", "for", "layer_name", ",", "hook", "in", "self", ".", "hooks", ".", "items", "(", ")", ":", "\n", "# list of activations for each instance.", "\n", "            ", "activation_dict", "[", "layer_name", "]", "=", "hook", "\n", "\n", "", "return", "activation_dict", ",", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation.get_weights": [[236, 253], ["utils.GetWeightAndActivation._get_layer", "hasattr", "utils.GetWeightAndActivation.weight.clone().detach", "logger.error", "utils.GetWeightAndActivation.weight.clone"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.GetWeightAndActivation._get_layer"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns weights from registered layers.\n        Returns:\n            weights (Python dictionary): a dictionary of the pair\n            {layer_name: weight}, where weight is the weight tensor.\n        \"\"\"", "\n", "weights", "=", "{", "}", "\n", "for", "layer", "in", "self", ".", "layers_names", ":", "\n", "            ", "cur_layer", "=", "self", ".", "_get_layer", "(", "layer", ")", "\n", "if", "hasattr", "(", "cur_layer", ",", "\"weight\"", ")", ":", "\n", "                ", "weights", "[", "layer", "]", "=", "cur_layer", ".", "weight", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Layer {} does not have weight attribute.\"", ".", "format", "(", "layer", ")", "\n", ")", "\n", "", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.__init__": [[331, 337], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "frames", "=", "None", "\n", "self", ".", "id", "=", "-", "1", "\n", "self", ".", "bboxes", "=", "None", "\n", "self", ".", "action_preds", "=", "None", "\n", "self", ".", "num_buffer_frames", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_frames": [[338, 347], ["None"], "methods", ["None"], ["", "def", "add_frames", "(", "self", ",", "idx", ",", "frames", ")", ":", "\n", "        ", "\"\"\"\n        Add the clip and corresponding id.\n        Args:\n            idx (int): the current index of the clip.\n            frames (list[ndarray]): list of images in \"BGR\" format.\n        \"\"\"", "\n", "self", ".", "frames", "=", "frames", "\n", "self", ".", "id", "=", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_bboxes": [[348, 353], ["None"], "methods", ["None"], ["", "def", "add_bboxes", "(", "self", ",", "bboxes", ")", ":", "\n", "        ", "\"\"\"\n        Add correspondding bounding boxes.\n        \"\"\"", "\n", "self", ".", "bboxes", "=", "bboxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.TaskInfo.add_action_preds": [[354, 359], ["None"], "methods", ["None"], ["", "def", "add_action_preds", "(", "self", ",", "preds", ")", ":", "\n", "        ", "\"\"\"\n        Add the corresponding action predictions.\n        \"\"\"", "\n", "self", ".", "action_preds", "=", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_confusion_matrix": [[16, 46], ["isinstance", "isinstance", "torch.flatten", "torch.flatten", "sklearn.metrics.confusion_matrix", "torch.cat", "torch.cat", "torch.argmax", "torch.argmax", "list", "range"], "function", ["None"], ["def", "get_confusion_matrix", "(", "preds", ",", "labels", ",", "num_classes", ",", "normalize", "=", "\"true\"", ")", ":", "\n", "    ", "\"\"\"\n    Calculate confusion matrix on the provided preds and labels.\n    Args:\n        preds (tensor or lists of tensors): predictions. Each tensor is in\n            in the shape of (n_batch, num_classes). Tensor(s) must be on CPU.\n        labels (tensor or lists of tensors): corresponding labels. Each tensor is\n            in the shape of either (n_batch,) or (n_batch, num_classes).\n        num_classes (int): number of classes. Tensor(s) must be on CPU.\n        normalize (Optional[str]) : {\u2018true\u2019, \u2018pred\u2019, \u2018all\u2019}, default=\"true\"\n            Normalizes confusion matrix over the true (rows), predicted (columns)\n            conditions or all the population. If None, confusion matrix\n            will not be normalized.\n    Returns:\n        cmtx (ndarray): confusion matrix of size (num_classes x num_classes)\n    \"\"\"", "\n", "if", "isinstance", "(", "preds", ",", "list", ")", ":", "\n", "        ", "preds", "=", "torch", ".", "cat", "(", "preds", ",", "dim", "=", "0", ")", "\n", "", "if", "isinstance", "(", "labels", ",", "list", ")", ":", "\n", "        ", "labels", "=", "torch", ".", "cat", "(", "labels", ",", "dim", "=", "0", ")", "\n", "# If labels are one-hot encoded, get their indices.", "\n", "", "if", "labels", ".", "ndim", "==", "preds", ".", "ndim", ":", "\n", "        ", "labels", "=", "torch", ".", "argmax", "(", "labels", ",", "dim", "=", "-", "1", ")", "\n", "# Get the predicted class indices for examples.", "\n", "", "preds", "=", "torch", ".", "flatten", "(", "torch", ".", "argmax", "(", "preds", ",", "dim", "=", "-", "1", ")", ")", "\n", "labels", "=", "torch", ".", "flatten", "(", "labels", ")", "\n", "cmtx", "=", "confusion_matrix", "(", "\n", "labels", ",", "preds", ",", "labels", "=", "list", "(", "range", "(", "num_classes", ")", ")", ",", "normalize", "=", "normalize", "\n", ")", "\n", "return", "cmtx", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.plot_confusion_matrix": [[48, 90], ["matplotlib.figure", "matplotlib.imshow", "matplotlib.title", "matplotlib.colorbar", "numpy.arange", "matplotlib.xticks", "matplotlib.yticks", "itertools.product", "matplotlib.tight_layout", "matplotlib.ylabel", "matplotlib.xlabel", "len", "cmtx.max", "range", "range", "matplotlib.text", "type", "str", "range", "format"], "function", ["None"], ["", "def", "plot_confusion_matrix", "(", "cmtx", ",", "num_classes", ",", "class_names", "=", "None", ",", "figsize", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    A function to create a colored and labeled confusion matrix matplotlib figure\n    given true labels and preds.\n    Args:\n        cmtx (ndarray): confusion matrix.\n        num_classes (int): total number of classes.\n        class_names (Optional[list of strs]): a list of class names.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].\n\n    Returns:\n        img (figure): matplotlib figure.\n    \"\"\"", "\n", "if", "class_names", "is", "None", "or", "type", "(", "class_names", ")", "!=", "list", ":", "\n", "        ", "class_names", "=", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_classes", ")", "]", "\n", "\n", "", "figure", "=", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "imshow", "(", "cmtx", ",", "interpolation", "=", "\"nearest\"", ",", "cmap", "=", "plt", ".", "cm", ".", "Blues", ")", "\n", "plt", ".", "title", "(", "\"Confusion matrix\"", ")", "\n", "plt", ".", "colorbar", "(", ")", "\n", "tick_marks", "=", "np", ".", "arange", "(", "len", "(", "class_names", ")", ")", "\n", "plt", ".", "xticks", "(", "tick_marks", ",", "class_names", ",", "rotation", "=", "45", ")", "\n", "plt", ".", "yticks", "(", "tick_marks", ",", "class_names", ")", "\n", "\n", "# Use white text if squares are dark; otherwise black.", "\n", "threshold", "=", "cmtx", ".", "max", "(", ")", "/", "2.0", "\n", "for", "i", ",", "j", "in", "itertools", ".", "product", "(", "range", "(", "cmtx", ".", "shape", "[", "0", "]", ")", ",", "range", "(", "cmtx", ".", "shape", "[", "1", "]", ")", ")", ":", "\n", "        ", "color", "=", "\"white\"", "if", "cmtx", "[", "i", ",", "j", "]", ">", "threshold", "else", "\"black\"", "\n", "plt", ".", "text", "(", "\n", "j", ",", "\n", "i", ",", "\n", "format", "(", "cmtx", "[", "i", ",", "j", "]", ",", "\".2f\"", ")", "if", "cmtx", "[", "i", ",", "j", "]", "!=", "0", "else", "\".\"", ",", "\n", "horizontalalignment", "=", "\"center\"", ",", "\n", "color", "=", "color", ",", "\n", ")", "\n", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "ylabel", "(", "\"True label\"", ")", "\n", "plt", ".", "xlabel", "(", "\"Predicted label\"", ")", "\n", "\n", "return", "figure", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.plot_topk_histogram": [[92, 156], ["torch.topk", "matplotlib.Figure", "plt.Figure.add_subplot", "numpy.arange", "fig.add_subplot.bar", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_xticks", "fig.add_subplot.set_xticklabels", "fig.add_subplot.xaxis.set_label_position", "fig.add_subplot.xaxis.tick_bottom", "numpy.linspace", "fig.add_subplot.set_ylabel", "fig.add_subplot.set_yticks", "fig.add_subplot.set_yticklabels", "enumerate", "fig.add_subplot.set_title", "plt.Figure.set_tight_layout", "format", "val.numpy", "fig.add_subplot.text", "str", "format"], "function", ["None"], ["", "def", "plot_topk_histogram", "(", "tag", ",", "array", ",", "k", "=", "10", ",", "class_names", "=", "None", ",", "figsize", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Plot histogram of top-k value from the given array.\n    Args:\n        tag (str): histogram title.\n        array (tensor): a tensor to draw top k value from.\n        k (int): number of top values to draw from array.\n            Defaut to 10.\n        class_names (list of strings, optional):\n            a list of names for values in array.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].\n    Returns:\n        fig (matplotlib figure): a matplotlib figure of the histogram.\n    \"\"\"", "\n", "val", ",", "ind", "=", "torch", ".", "topk", "(", "array", ",", "k", ")", "\n", "\n", "fig", "=", "plt", ".", "Figure", "(", "figsize", "=", "figsize", ",", "facecolor", "=", "\"w\"", ",", "edgecolor", "=", "\"k\"", ")", "\n", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "\n", "if", "class_names", "is", "None", ":", "\n", "        ", "class_names", "=", "[", "str", "(", "i", ")", "for", "i", "in", "ind", "]", "\n", "", "else", ":", "\n", "        ", "class_names", "=", "[", "class_names", "[", "i", "]", "for", "i", "in", "ind", "]", "\n", "\n", "", "tick_marks", "=", "np", ".", "arange", "(", "k", ")", "\n", "width", "=", "0.75", "\n", "ax", ".", "bar", "(", "\n", "tick_marks", ",", "\n", "val", ",", "\n", "width", ",", "\n", "color", "=", "\"orange\"", ",", "\n", "tick_label", "=", "class_names", ",", "\n", "edgecolor", "=", "\"w\"", ",", "\n", "linewidth", "=", "1", ",", "\n", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"Candidates\"", ")", "\n", "ax", ".", "set_xticks", "(", "tick_marks", ")", "\n", "ax", ".", "set_xticklabels", "(", "class_names", ",", "rotation", "=", "-", "45", ",", "ha", "=", "\"center\"", ")", "\n", "ax", ".", "xaxis", ".", "set_label_position", "(", "\"bottom\"", ")", "\n", "ax", ".", "xaxis", ".", "tick_bottom", "(", ")", "\n", "\n", "y_tick", "=", "np", ".", "linspace", "(", "0", ",", "1", ",", "num", "=", "10", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Frequency\"", ")", "\n", "ax", ".", "set_yticks", "(", "y_tick", ")", "\n", "y_labels", "=", "[", "format", "(", "i", ",", "\".1f\"", ")", "for", "i", "in", "y_tick", "]", "\n", "ax", ".", "set_yticklabels", "(", "y_labels", ",", "ha", "=", "\"center\"", ")", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "val", ".", "numpy", "(", ")", ")", ":", "\n", "        ", "ax", ".", "text", "(", "\n", "i", "-", "0.1", ",", "\n", "v", "+", "0.03", ",", "\n", "format", "(", "v", ",", "\".2f\"", ")", ",", "\n", "color", "=", "\"orange\"", ",", "\n", "fontweight", "=", "\"bold\"", ",", "\n", ")", "\n", "\n", "", "ax", ".", "set_title", "(", "tag", ")", "\n", "\n", "fig", ".", "set_tight_layout", "(", "True", ")", "\n", "\n", "return", "fig", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_indexing": [[255, 276], ["string.strip().split", "tuple", "index.split", "final_indexing.append", "string.strip", "int"], "function", ["None"], ["", "", "def", "get_indexing", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Parse numpy-like fancy indexing from a string.\n    Args:\n        string (str): string represent the indices to take\n            a subset of from array. Indices for each dimension\n            are separated by `,`; indices for different dimensions\n            are separated by `;`.\n            e.g.: For a numpy array `arr` of shape (3,3,3), the string \"1,2;1,2\"\n            means taking the sub-array `arr[[1,2], [1,2]]\n    Returns:\n        final_indexing (tuple): the parsed indexing.\n    \"\"\"", "\n", "index_ls", "=", "string", ".", "strip", "(", ")", ".", "split", "(", "\";\"", ")", "\n", "final_indexing", "=", "[", "]", "\n", "for", "index", "in", "index_ls", ":", "\n", "        ", "index_single_dim", "=", "index", ".", "split", "(", "\",\"", ")", "\n", "index_single_dim", "=", "[", "int", "(", "i", ")", "for", "i", "in", "index_single_dim", "]", "\n", "final_indexing", ".", "append", "(", "index_single_dim", ")", "\n", "\n", "", "return", "tuple", "(", "final_indexing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.process_layer_index_data": [[278, 302], ["layer.split", "layer_name.append", "len", "utils.get_indexing"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.get_indexing"], ["", "def", "process_layer_index_data", "(", "layer_ls", ",", "layer_name_prefix", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"\n    Extract layer names and numpy-like fancy indexing from a string.\n    Args:\n        layer_ls (list of strs): list of strings containing data about layer names\n            and their indexing. For each string, layer name and indexing is separated by whitespaces.\n            e.g.: [layer1 1,2;2, layer2, layer3 150;3,4]\n        layer_name_prefix (Optional[str]): prefix to be added to each layer name.\n    Returns:\n        layer_name (list of strings): a list of layer names.\n        indexing_dict (Python dict): a dictionary of the pair\n            {one_layer_name: indexing_for_that_layer}\n    \"\"\"", "\n", "\n", "layer_name", ",", "indexing_dict", "=", "[", "]", ",", "{", "}", "\n", "for", "layer", "in", "layer_ls", ":", "\n", "        ", "ls", "=", "layer", ".", "split", "(", ")", "\n", "name", "=", "layer_name_prefix", "+", "ls", "[", "0", "]", "\n", "layer_name", ".", "append", "(", "name", ")", "\n", "if", "len", "(", "ls", ")", "==", "2", ":", "\n", "            ", "indexing_dict", "[", "name", "]", "=", "get_indexing", "(", "ls", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "indexing_dict", "[", "name", "]", "=", "(", ")", "\n", "", "", "return", "layer_name", ",", "indexing_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.process_cv2_inputs": [[304, 323], ["slowfast.datasets.utils.tensor_normalize", "slowfast.datasets.utils.pack_pathway_output.permute", "torch.linspace().long", "torch.index_select", "slowfast.datasets.utils.pack_pathway_output", "torch.from_numpy().float", "inp.unsqueeze", "torch.linspace", "torch.from_numpy", "numpy.array"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.tensor_normalize", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.pack_pathway_output"], ["", "def", "process_cv2_inputs", "(", "frames", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Normalize and prepare inputs as a list of tensors. Each tensor\n    correspond to a unique pathway.\n    Args:\n        frames (list of array): list of input images (correspond to one clip) in range [0, 255].\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"", "\n", "inputs", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "frames", ")", ")", ".", "float", "(", ")", "/", "255", "\n", "inputs", "=", "tensor_normalize", "(", "inputs", ",", "cfg", ".", "DATA", ".", "MEAN", ",", "cfg", ".", "DATA", ".", "STD", ")", "\n", "# T H W C -> C T H W.", "\n", "inputs", "=", "inputs", ".", "permute", "(", "3", ",", "0", ",", "1", ",", "2", ")", "\n", "# Sample frames for num_frames specified.", "\n", "index", "=", "torch", ".", "linspace", "(", "0", ",", "inputs", ".", "shape", "[", "1", "]", "-", "1", ",", "cfg", ".", "DATA", ".", "NUM_FRAMES", ")", ".", "long", "(", ")", "\n", "inputs", "=", "torch", ".", "index_select", "(", "inputs", ",", "1", ",", "index", ")", "\n", "inputs", "=", "pack_pathway_output", "(", "cfg", ",", "inputs", ")", "\n", "inputs", "=", "[", "inp", ".", "unsqueeze", "(", "0", ")", "for", "inp", "in", "inputs", "]", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.visualization.utils.init_task_info": [[361, 374], ["None"], "function", ["None"], ["", "", "def", "init_task_info", "(", "img_height", ",", "img_width", ",", "crop_size", ",", "clip_vis_size", ")", ":", "\n", "    ", "\"\"\"\n    Initialze global attributes for TaskInfo object.\n    Args:\n        img_height (int): image/video height.\n        img_width (int): image/video width.\n        crop_size (int): crop size for input to the model.\n        clip_vis_size (int): size for scaling the video for visualization.\n    \"\"\"", "\n", "TaskInfo", ".", "img_height", "=", "img_height", "\n", "TaskInfo", ".", "img_width", "=", "img_width", "\n", "TaskInfo", ".", "crop_size", "=", "crop_size", "\n", "TaskInfo", ".", "clip_vis_size", "=", "clip_vis_size", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.clip_boxes_to_image": [[9, 27], ["numpy.minimum", "numpy.minimum", "numpy.maximum", "numpy.maximum"], "function", ["None"], ["def", "clip_boxes_to_image", "(", "boxes", ",", "height", ",", "width", ")", ":", "\n", "    ", "\"\"\"\n    Clip the boxes with the height and width of the image size.\n    Args:\n        boxes (ndarray): bounding boxes to peform crop. The dimension is\n        `num boxes` x 4.\n        height (int): the height of the image.\n        width (int): the width of the image.\n    Returns:\n        boxes (ndarray): cropped bounding boxes.\n    \"\"\"", "\n", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "np", ".", "minimum", "(", "\n", "width", "-", "1.0", ",", "np", ".", "maximum", "(", "0.0", ",", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", ")", "\n", ")", "\n", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "=", "np", ".", "minimum", "(", "\n", "height", "-", "1.0", ",", "np", ".", "maximum", "(", "0.0", ",", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", ")", "\n", ")", "\n", "return", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_short_side_scale_jitter_list": [[29, 74], ["int", "round", "int", "int", "math.floor", "math.floor", "cv2.resize().astype", "numpy.random.uniform", "cv2.resize", "float", "float", "float", "float"], "function", ["None"], ["", "def", "random_short_side_scale_jitter_list", "(", "images", ",", "min_size", ",", "max_size", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (list): list of images to perform scale jitter. Dimension is\n            `height` x `width` x `channel`.\n        min_size (int): the minimal size to scale the frames.\n        max_size (int): the maximal size to scale the frames.\n        boxes (list): optional. Corresponding boxes to images. Dimension is\n            `num boxes` x 4.\n    Returns:\n        (list): the list of scaled images with dimension of\n            `new height` x `new width` x `channel`.\n        (ndarray or None): the scaled boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "size", "=", "int", "(", "round", "(", "1.0", "/", "np", ".", "random", ".", "uniform", "(", "1.0", "/", "max_size", ",", "1.0", "/", "min_size", ")", ")", ")", "\n", "\n", "height", "=", "images", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "width", "=", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "if", "(", "width", "<=", "height", "and", "width", "==", "size", ")", "or", "(", "\n", "height", "<=", "width", "and", "height", "==", "size", "\n", ")", ":", "\n", "        ", "return", "images", ",", "boxes", "\n", "", "new_width", "=", "size", "\n", "new_height", "=", "size", "\n", "if", "width", "<", "height", ":", "\n", "        ", "new_height", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "height", ")", "/", "width", ")", "*", "size", ")", ")", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "boxes", "=", "[", "\n", "proposal", "*", "float", "(", "new_height", ")", "/", "height", "for", "proposal", "in", "boxes", "\n", "]", "\n", "", "", "else", ":", "\n", "        ", "new_width", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "width", ")", "/", "height", ")", "*", "size", ")", ")", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "boxes", "=", "[", "proposal", "*", "float", "(", "new_width", ")", "/", "width", "for", "proposal", "in", "boxes", "]", "\n", "", "", "return", "(", "\n", "[", "\n", "cv2", ".", "resize", "(", "\n", "image", ",", "(", "new_width", ",", "new_height", ")", ",", "interpolation", "=", "cv2", ".", "INTER_LINEAR", "\n", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "for", "image", "in", "images", "\n", "]", ",", "\n", "boxes", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale": [[77, 104], ["cv2.resize", "cv2.resize.astype", "int", "int", "math.floor", "math.floor", "float", "float"], "function", ["None"], ["", "def", "scale", "(", "size", ",", "image", ")", ":", "\n", "    ", "\"\"\"\n    Scale the short side of the image to size.\n    Args:\n        size (int): size to scale the image.\n        image (array): image to perform short side scale. Dimension is\n            `height` x `width` x `channel`.\n    Returns:\n        (ndarray): the scaled image with dimension of\n            `height` x `width` x `channel`.\n    \"\"\"", "\n", "height", "=", "image", ".", "shape", "[", "0", "]", "\n", "width", "=", "image", ".", "shape", "[", "1", "]", "\n", "if", "(", "width", "<=", "height", "and", "width", "==", "size", ")", "or", "(", "\n", "height", "<=", "width", "and", "height", "==", "size", "\n", ")", ":", "\n", "        ", "return", "image", "\n", "", "new_width", "=", "size", "\n", "new_height", "=", "size", "\n", "if", "width", "<", "height", ":", "\n", "        ", "new_height", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "height", ")", "/", "width", ")", "*", "size", ")", ")", "\n", "", "else", ":", "\n", "        ", "new_width", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "width", ")", "/", "height", ")", "*", "size", ")", ")", "\n", "", "img", "=", "cv2", ".", "resize", "(", "\n", "image", ",", "(", "new_width", ",", "new_height", ")", ",", "interpolation", "=", "cv2", ".", "INTER_LINEAR", "\n", ")", "\n", "return", "img", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale_boxes": [[106, 132], ["int", "int", "math.floor", "float", "math.floor", "float", "float", "float"], "function", ["None"], ["", "def", "scale_boxes", "(", "size", ",", "boxes", ",", "height", ",", "width", ")", ":", "\n", "    ", "\"\"\"\n    Scale the short side of the box to size.\n    Args:\n        size (int): size to scale the image.\n        boxes (ndarray): bounding boxes to peform scale. The dimension is\n        `num boxes` x 4.\n        height (int): the height of the image.\n        width (int): the width of the image.\n    Returns:\n        boxes (ndarray): scaled bounding boxes.\n    \"\"\"", "\n", "if", "(", "width", "<=", "height", "and", "width", "==", "size", ")", "or", "(", "\n", "height", "<=", "width", "and", "height", "==", "size", "\n", ")", ":", "\n", "        ", "return", "boxes", "\n", "\n", "", "new_width", "=", "size", "\n", "new_height", "=", "size", "\n", "if", "width", "<", "height", ":", "\n", "        ", "new_height", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "height", ")", "/", "width", ")", "*", "size", ")", ")", "\n", "boxes", "*=", "float", "(", "new_height", ")", "/", "height", "\n", "", "else", ":", "\n", "        ", "new_width", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "width", ")", "/", "height", ")", "*", "size", ")", ")", "\n", "boxes", "*=", "float", "(", "new_width", ")", "/", "width", "\n", "", "return", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.horizontal_flip_list": [[134, 164], ["numpy.random.uniform", "cv2_transform.flip_boxes", "numpy.asarray().swapaxes", "out_images.append", "np.asarray().swapaxes.swapaxes", "numpy.asarray", "cv2.flip"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.flip_boxes"], ["", "def", "horizontal_flip_list", "(", "prob", ",", "images", ",", "order", "=", "\"CHW\"", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Horizontally flip the list of image and optional boxes.\n    Args:\n        prob (float): probability to flip.\n        image (list): ilist of images to perform short side scale. Dimension is\n            `height` x `width` x `channel` or `channel` x `height` x `width`.\n        order (str): order of the `height`, `channel` and `width`.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        (ndarray): the scaled image with dimension of\n            `height` x `width` x `channel`.\n        (list): optional. Corresponding boxes to images. Dimension is\n            `num boxes` x 4.\n    \"\"\"", "\n", "_", ",", "width", ",", "_", "=", "images", "[", "0", "]", ".", "shape", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "prob", ":", "\n", "        ", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "boxes", "=", "[", "flip_boxes", "(", "proposal", ",", "width", ")", "for", "proposal", "in", "boxes", "]", "\n", "", "if", "order", "==", "\"CHW\"", ":", "\n", "            ", "out_images", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "                ", "image", "=", "np", ".", "asarray", "(", "image", ")", ".", "swapaxes", "(", "2", ",", "0", ")", "\n", "image", "=", "image", "[", ":", ":", "-", "1", "]", "\n", "out_images", ".", "append", "(", "image", ".", "swapaxes", "(", "0", ",", "2", ")", ")", "\n", "", "return", "out_images", ",", "boxes", "\n", "", "elif", "order", "==", "\"HWC\"", ":", "\n", "            ", "return", "[", "cv2", ".", "flip", "(", "image", ",", "1", ")", "for", "image", "in", "images", "]", ",", "boxes", "\n", "", "", "return", "images", ",", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.spatial_shift_crop_list": [[166, 214], ["int", "int", "math.ceil", "math.ceil", "range", "len"], "function", ["None"], ["", "def", "spatial_shift_crop_list", "(", "size", ",", "images", ",", "spatial_shift_pos", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform left, center, or right crop of the given list of images.\n    Args:\n        size (int): size to crop.\n        image (list): ilist of images to perform short side scale. Dimension is\n            `height` x `width` x `channel` or `channel` x `height` x `width`.\n        spatial_shift_pos (int): option includes 0 (left), 1 (middle), and\n            2 (right) crop.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        cropped (ndarray): the cropped list of images with dimension of\n            `height` x `width` x `channel`.\n        boxes (list): optional. Corresponding boxes to images. Dimension is\n            `num boxes` x 4.\n    \"\"\"", "\n", "\n", "assert", "spatial_shift_pos", "in", "[", "0", ",", "1", ",", "2", "]", "\n", "\n", "height", "=", "images", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "width", "=", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "y_offset", "=", "int", "(", "math", ".", "ceil", "(", "(", "height", "-", "size", ")", "/", "2", ")", ")", "\n", "x_offset", "=", "int", "(", "math", ".", "ceil", "(", "(", "width", "-", "size", ")", "/", "2", ")", ")", "\n", "\n", "if", "height", ">", "width", ":", "\n", "        ", "if", "spatial_shift_pos", "==", "0", ":", "\n", "            ", "y_offset", "=", "0", "\n", "", "elif", "spatial_shift_pos", "==", "2", ":", "\n", "            ", "y_offset", "=", "height", "-", "size", "\n", "", "", "else", ":", "\n", "        ", "if", "spatial_shift_pos", "==", "0", ":", "\n", "            ", "x_offset", "=", "0", "\n", "", "elif", "spatial_shift_pos", "==", "2", ":", "\n", "            ", "x_offset", "=", "width", "-", "size", "\n", "\n", "", "", "cropped", "=", "[", "\n", "image", "[", "y_offset", ":", "y_offset", "+", "size", ",", "x_offset", ":", "x_offset", "+", "size", ",", ":", "]", "\n", "for", "image", "in", "images", "\n", "]", "\n", "assert", "cropped", "[", "0", "]", ".", "shape", "[", "0", "]", "==", "size", ",", "\"Image height not cropped properly\"", "\n", "assert", "cropped", "[", "0", "]", ".", "shape", "[", "1", "]", "==", "size", ",", "\"Image width not cropped properly\"", "\n", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "boxes", ")", ")", ":", "\n", "            ", "boxes", "[", "i", "]", "[", ":", ",", "[", "0", ",", "2", "]", "]", "-=", "x_offset", "\n", "boxes", "[", "i", "]", "[", ":", ",", "[", "1", ",", "3", "]", "]", "-=", "y_offset", "\n", "", "", "return", "cropped", ",", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.drop_boxes": [[216, 235], ["enumerate", "numpy.logical_and"], "function", ["None"], ["", "def", "drop_boxes", "(", "boxes", ",", "min_scale", "=", "8", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Drop out invalid boxed (width <=  or height <= 1)\n    Args:\n        boxes: list[array]\n        min_scale: int\n    Returns:\n\n    \"\"\"", "\n", "\n", "for", "i", ",", "box", "in", "enumerate", "(", "boxes", ")", ":", "\n", "        ", "width", "=", "box", "[", ":", ",", "2", "]", "-", "box", "[", ":", ",", "0", "]", "\n", "height", "=", "box", "[", ":", ",", "3", "]", "-", "box", "[", ":", ",", "1", "]", "\n", "\n", "valid", "=", "np", ".", "logical_and", "(", "width", ">", "min_scale", ",", "height", ">", "min_scale", ")", "\n", "boxes", "[", "i", "]", "=", "box", "[", "valid", "]", "\n", "\n", "", "return", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.CHW2HWC": [[237, 247], ["image.transpose"], "function", ["None"], ["", "def", "CHW2HWC", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Transpose the dimension from `channel` x `height` x `width` to\n        `height` x `width` x `channel`.\n    Args:\n        image (array): image to transpose.\n    Returns\n        (array): transposed image.\n    \"\"\"", "\n", "return", "image", ".", "transpose", "(", "[", "1", ",", "2", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.HWC2CHW": [[249, 259], ["image.transpose"], "function", ["None"], ["", "def", "HWC2CHW", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Transpose the dimension from `height` x `width` x `channel` to\n        `channel` x `height` x `width`.\n    Args:\n        image (array): image to transpose.\n    Returns\n        (array): transposed image.\n    \"\"\"", "\n", "return", "image", ".", "transpose", "(", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.color_jitter_list": [[261, 292], ["jitter.append", "jitter.append", "jitter.append", "len", "numpy.random.permutation", "range", "numpy.arange", "len", "len", "cv2_transform.brightness_list", "cv2_transform.contrast_list", "cv2_transform.saturation_list"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.brightness_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.contrast_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.saturation_list"], ["", "def", "color_jitter_list", "(", "\n", "images", ",", "img_brightness", "=", "0", ",", "img_contrast", "=", "0", ",", "img_saturation", "=", "0", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Perform color jitter on the list of images.\n    Args:\n        images (list): list of images to perform color jitter.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.\n    Returns:\n        images (list): the jittered list of images.\n    \"\"\"", "\n", "jitter", "=", "[", "]", "\n", "if", "img_brightness", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"brightness\"", ")", "\n", "", "if", "img_contrast", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"contrast\"", ")", "\n", "", "if", "img_saturation", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"saturation\"", ")", "\n", "\n", "", "if", "len", "(", "jitter", ")", ">", "0", ":", "\n", "        ", "order", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "len", "(", "jitter", ")", ")", ")", "\n", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "jitter", ")", ")", ":", "\n", "            ", "if", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"brightness\"", ":", "\n", "                ", "images", "=", "brightness_list", "(", "img_brightness", ",", "images", ")", "\n", "", "elif", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"contrast\"", ":", "\n", "                ", "images", "=", "contrast_list", "(", "img_contrast", ",", "images", ")", "\n", "", "elif", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"saturation\"", ":", "\n", "                ", "images", "=", "saturation_list", "(", "img_saturation", ",", "images", ")", "\n", "", "", "", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.lighting_list": [[294, 321], ["numpy.random.normal", "numpy.array", "numpy.reshape", "numpy.sum", "range", "out_images.append", "numpy.repeat", "numpy.repeat"], "function", ["None"], ["", "def", "lighting_list", "(", "imgs", ",", "alphastd", ",", "eigval", ",", "eigvec", ",", "alpha", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform AlexNet-style PCA jitter on the given list of images.\n    Args:\n        images (list): list of images to perform lighting jitter.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:\n        out_images (list): the list of jittered images.\n    \"\"\"", "\n", "if", "alphastd", "==", "0", ":", "\n", "        ", "return", "imgs", "\n", "# generate alpha1, alpha2, alpha3", "\n", "", "alpha", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "alphastd", ",", "size", "=", "(", "1", ",", "3", ")", ")", "\n", "eig_vec", "=", "np", ".", "array", "(", "eigvec", ")", "\n", "eig_val", "=", "np", ".", "reshape", "(", "eigval", ",", "(", "1", ",", "3", ")", ")", "\n", "rgb", "=", "np", ".", "sum", "(", "\n", "eig_vec", "*", "np", ".", "repeat", "(", "alpha", ",", "3", ",", "axis", "=", "0", ")", "*", "np", ".", "repeat", "(", "eig_val", ",", "3", ",", "axis", "=", "0", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "out_images", "=", "[", "]", "\n", "for", "img", "in", "imgs", ":", "\n", "        ", "for", "idx", "in", "range", "(", "img", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "img", "[", "idx", "]", "=", "img", "[", "idx", "]", "+", "rgb", "[", "2", "-", "idx", "]", "\n", "", "out_images", ".", "append", "(", "img", ")", "\n", "", "return", "out_images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.color_normalization": [[323, 338], ["range", "len", "len"], "function", ["None"], ["", "def", "color_normalization", "(", "image", ",", "mean", ",", "stddev", ")", ":", "\n", "    ", "\"\"\"\n    Perform color normalization on the image with the given mean and stddev.\n    Args:\n        image (array): image to perform color normalization.\n        mean (float): mean value to subtract.\n        stddev (float): stddev to devide.\n    \"\"\"", "\n", "# Input image should in format of CHW", "\n", "assert", "len", "(", "mean", ")", "==", "image", ".", "shape", "[", "0", "]", ",", "\"channel mean not computed properly\"", "\n", "assert", "len", "(", "stddev", ")", "==", "image", ".", "shape", "[", "0", "]", ",", "\"channel stddev not computed properly\"", "\n", "for", "idx", "in", "range", "(", "image", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "image", "[", "idx", "]", "=", "image", "[", "idx", "]", "-", "mean", "[", "idx", "]", "\n", "image", "[", "idx", "]", "=", "image", "[", "idx", "]", "/", "stddev", "[", "idx", "]", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.pad_image": [[340, 363], ["numpy.pad", "numpy.pad", "str", "str"], "function", ["None"], ["", "def", "pad_image", "(", "image", ",", "pad_size", ",", "order", "=", "\"CHW\"", ")", ":", "\n", "    ", "\"\"\"\n    Pad the given image with the size of pad_size.\n    Args:\n        image (array): image to pad.\n        pad_size (int): size to pad.\n        order (str): order of the `height`, `channel` and `width`.\n    Returns:\n        img (array): padded image.\n    \"\"\"", "\n", "if", "order", "==", "\"CHW\"", ":", "\n", "        ", "img", "=", "np", ".", "pad", "(", "\n", "image", ",", "\n", "(", "(", "0", ",", "0", ")", ",", "(", "pad_size", ",", "pad_size", ")", ",", "(", "pad_size", ",", "pad_size", ")", ")", ",", "\n", "mode", "=", "str", "(", "\"constant\"", ")", ",", "\n", ")", "\n", "", "elif", "order", "==", "\"HWC\"", ":", "\n", "        ", "img", "=", "np", ".", "pad", "(", "\n", "image", ",", "\n", "(", "(", "pad_size", ",", "pad_size", ")", ",", "(", "pad_size", ",", "pad_size", ")", ",", "(", "0", ",", "0", ")", ")", ",", "\n", "mode", "=", "str", "(", "\"constant\"", ")", ",", "\n", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.horizontal_flip": [[365, 384], ["numpy.random.uniform", "NotImplementedError"], "function", ["None"], ["", "def", "horizontal_flip", "(", "prob", ",", "image", ",", "order", "=", "\"CHW\"", ")", ":", "\n", "    ", "\"\"\"\n    Horizontally flip the image.\n    Args:\n        prob (float): probability to flip.\n        image (array): image to pad.\n        order (str): order of the `height`, `channel` and `width`.\n    Returns:\n        img (array): flipped image.\n    \"\"\"", "\n", "assert", "order", "in", "[", "\"CHW\"", ",", "\"HWC\"", "]", ",", "\"order {} is not supported\"", ".", "format", "(", "order", ")", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "prob", ":", "\n", "        ", "if", "order", "==", "\"CHW\"", ":", "\n", "            ", "image", "=", "image", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "", "elif", "order", "==", "\"HWC\"", ":", "\n", "            ", "image", "=", "image", "[", ":", ",", ":", ":", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Unknown order {}\"", ".", "format", "(", "order", ")", ")", "\n", "", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.flip_boxes": [[386, 400], ["boxes.copy"], "function", ["None"], ["", "def", "flip_boxes", "(", "boxes", ",", "im_width", ")", ":", "\n", "    ", "\"\"\"\n    Horizontally flip the boxes.\n    Args:\n        boxes (array): box to flip.\n        im_width (int): width of the image.\n    Returns:\n        boxes_flipped (array): flipped box.\n    \"\"\"", "\n", "\n", "boxes_flipped", "=", "boxes", ".", "copy", "(", ")", "\n", "boxes_flipped", "[", ":", ",", "0", ":", ":", "4", "]", "=", "im_width", "-", "boxes", "[", ":", ",", "2", ":", ":", "4", "]", "-", "1", "\n", "boxes_flipped", "[", ":", ",", "2", ":", ":", "4", "]", "=", "im_width", "-", "boxes", "[", ":", ",", "0", ":", ":", "4", "]", "-", "1", "\n", "return", "boxes_flipped", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.crop_boxes": [[402, 413], ["None"], "function", ["None"], ["", "def", "crop_boxes", "(", "boxes", ",", "x_offset", ",", "y_offset", ")", ":", "\n", "    ", "\"\"\"\n    Crop the boxes given the offsets.\n    Args:\n        boxes (array): boxes to crop.\n        x_offset (int): offset on x.\n        y_offset (int): offset on y.\n    \"\"\"", "\n", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "-", "x_offset", "\n", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "=", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "-", "y_offset", "\n", "return", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_crop_list": [[415, 478], ["cv2_transform.pad_image", "int", "int", "cv2_transform.crop_boxes", "numpy.random.randint", "numpy.random.randint", "int", "int", "numpy.random.randint", "numpy.random.randint"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.pad_image", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.crop_boxes"], ["", "def", "random_crop_list", "(", "images", ",", "size", ",", "pad_size", "=", "0", ",", "order", "=", "\"CHW\"", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform random crop on a list of images.\n    Args:\n        images (list): list of images to perform random crop.\n        size (int): size to crop.\n        pad_size (int): padding size.\n        order (str): order of the `height`, `channel` and `width`.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        cropped (ndarray): the cropped list of images with dimension of\n            `height` x `width` x `channel`.\n        boxes (list): optional. Corresponding boxes to images. Dimension is\n            `num boxes` x 4.\n    \"\"\"", "\n", "# explicitly dealing processing per image order to avoid flipping images.", "\n", "if", "pad_size", ">", "0", ":", "\n", "        ", "images", "=", "[", "\n", "pad_image", "(", "pad_size", "=", "pad_size", ",", "image", "=", "image", ",", "order", "=", "order", ")", "\n", "for", "image", "in", "images", "\n", "]", "\n", "\n", "# image format should be CHW.", "\n", "", "if", "order", "==", "\"CHW\"", ":", "\n", "        ", "if", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "==", "size", "and", "images", "[", "0", "]", ".", "shape", "[", "2", "]", "==", "size", ":", "\n", "            ", "return", "images", ",", "boxes", "\n", "", "height", "=", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "width", "=", "images", "[", "0", "]", ".", "shape", "[", "2", "]", "\n", "y_offset", "=", "0", "\n", "if", "height", ">", "size", ":", "\n", "            ", "y_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "height", "-", "size", ")", ")", "\n", "", "x_offset", "=", "0", "\n", "if", "width", ">", "size", ":", "\n", "            ", "x_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "width", "-", "size", ")", ")", "\n", "", "cropped", "=", "[", "\n", "image", "[", ":", ",", "y_offset", ":", "y_offset", "+", "size", ",", "x_offset", ":", "x_offset", "+", "size", "]", "\n", "for", "image", "in", "images", "\n", "]", "\n", "assert", "cropped", "[", "0", "]", ".", "shape", "[", "1", "]", "==", "size", ",", "\"Image not cropped properly\"", "\n", "assert", "cropped", "[", "0", "]", ".", "shape", "[", "2", "]", "==", "size", ",", "\"Image not cropped properly\"", "\n", "", "elif", "order", "==", "\"HWC\"", ":", "\n", "        ", "if", "images", "[", "0", "]", ".", "shape", "[", "0", "]", "==", "size", "and", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "==", "size", ":", "\n", "            ", "return", "images", ",", "boxes", "\n", "", "height", "=", "images", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "width", "=", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "y_offset", "=", "0", "\n", "if", "height", ">", "size", ":", "\n", "            ", "y_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "height", "-", "size", ")", ")", "\n", "", "x_offset", "=", "0", "\n", "if", "width", ">", "size", ":", "\n", "            ", "x_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "width", "-", "size", ")", ")", "\n", "", "cropped", "=", "[", "\n", "image", "[", "y_offset", ":", "y_offset", "+", "size", ",", "x_offset", ":", "x_offset", "+", "size", ",", ":", "]", "\n", "for", "image", "in", "images", "\n", "]", "\n", "assert", "cropped", "[", "0", "]", ".", "shape", "[", "0", "]", "==", "size", ",", "\"Image not cropped properly\"", "\n", "assert", "cropped", "[", "0", "]", ".", "shape", "[", "1", "]", "==", "size", ",", "\"Image not cropped properly\"", "\n", "\n", "", "if", "boxes", "is", "not", "None", ":", "\n", "        ", "boxes", "=", "[", "crop_boxes", "(", "proposal", ",", "x_offset", ",", "y_offset", ")", "for", "proposal", "in", "boxes", "]", "\n", "\n", "", "return", "cropped", ",", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.center_crop": [[480, 495], ["int", "int", "math.ceil", "math.ceil"], "function", ["None"], ["", "def", "center_crop", "(", "size", ",", "image", ")", ":", "\n", "    ", "\"\"\"\n    Perform center crop on input images.\n    Args:\n        size (int): size of the cropped height and width.\n        image (array): the image to perform center crop.\n    \"\"\"", "\n", "height", "=", "image", ".", "shape", "[", "0", "]", "\n", "width", "=", "image", ".", "shape", "[", "1", "]", "\n", "y_offset", "=", "int", "(", "math", ".", "ceil", "(", "(", "height", "-", "size", ")", "/", "2", ")", ")", "\n", "x_offset", "=", "int", "(", "math", ".", "ceil", "(", "(", "width", "-", "size", ")", "/", "2", ")", ")", "\n", "cropped", "=", "image", "[", "y_offset", ":", "y_offset", "+", "size", ",", "x_offset", ":", "x_offset", "+", "size", ",", ":", "]", "\n", "assert", "cropped", ".", "shape", "[", "0", "]", "==", "size", ",", "\"Image height not cropped properly\"", "\n", "assert", "cropped", ".", "shape", "[", "1", "]", "==", "size", ",", "\"Image width not cropped properly\"", "\n", "return", "cropped", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_scale_jitter": [[499, 515], ["int", "cv2_transform.scale", "round", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale"], ["", "def", "random_scale_jitter", "(", "image", ",", "min_size", ",", "max_size", ")", ":", "\n", "    ", "\"\"\"\n    Perform ResNet style random scale jittering: randomly select the scale from\n        [1/max_size, 1/min_size].\n    Args:\n        image (array): image to perform random scale.\n        min_size (int): min size to scale.\n        max_size (int) max size to scale.\n    Returns:\n        image (array): scaled image.\n    \"\"\"", "\n", "img_scale", "=", "int", "(", "\n", "round", "(", "1.0", "/", "np", ".", "random", ".", "uniform", "(", "1.0", "/", "max_size", ",", "1.0", "/", "min_size", ")", ")", "\n", ")", "\n", "image", "=", "scale", "(", "img_scale", ",", "image", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_scale_jitter_list": [[517, 533], ["int", "round", "cv2_transform.scale", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale"], ["", "def", "random_scale_jitter_list", "(", "images", ",", "min_size", ",", "max_size", ")", ":", "\n", "    ", "\"\"\"\n    Perform ResNet style random scale jittering on a list of image: randomly\n        select the scale from [1/max_size, 1/min_size]. Note that all the image\n        will share the same scale.\n    Args:\n        images (list): list of images to perform random scale.\n        min_size (int): min size to scale.\n        max_size (int) max size to scale.\n    Returns:\n        images (list): list of scaled image.\n    \"\"\"", "\n", "img_scale", "=", "int", "(", "\n", "round", "(", "1.0", "/", "np", ".", "random", ".", "uniform", "(", "1.0", "/", "max_size", ",", "1.0", "/", "min_size", ")", ")", "\n", ")", "\n", "return", "[", "scale", "(", "img_scale", ",", "image", ")", "for", "image", "in", "images", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_sized_crop": [[535, 576], ["range", "cv2_transform.center_crop", "numpy.random.uniform", "int", "int", "cv2_transform.scale", "numpy.random.uniform", "round", "round", "numpy.random.uniform", "int", "int", "cv2.resize", "cv2.resize.astype", "math.sqrt", "math.sqrt", "numpy.random.randint", "numpy.random.randint", "float", "float"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.center_crop", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale"], ["", "def", "random_sized_crop", "(", "image", ",", "size", ",", "area_frac", "=", "0.08", ")", ":", "\n", "    ", "\"\"\"\n    Perform random sized cropping on the given image. Random crop with size\n        8% - 100% image area and aspect ratio in [3/4, 4/3].\n    Args:\n        image (array): image to crop.\n        size (int): size to crop.\n        area_frac (float): area of fraction.\n    Returns:\n        (array): cropped image.\n    \"\"\"", "\n", "for", "_", "in", "range", "(", "0", ",", "10", ")", ":", "\n", "        ", "height", "=", "image", ".", "shape", "[", "0", "]", "\n", "width", "=", "image", ".", "shape", "[", "1", "]", "\n", "area", "=", "height", "*", "width", "\n", "target_area", "=", "np", ".", "random", ".", "uniform", "(", "area_frac", ",", "1.0", ")", "*", "area", "\n", "aspect_ratio", "=", "np", ".", "random", ".", "uniform", "(", "3.0", "/", "4.0", ",", "4.0", "/", "3.0", ")", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "float", "(", "target_area", ")", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "float", "(", "target_area", ")", "/", "aspect_ratio", ")", ")", ")", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "0.5", ":", "\n", "            ", "w", ",", "h", "=", "h", ",", "w", "\n", "", "if", "h", "<=", "height", "and", "w", "<=", "width", ":", "\n", "            ", "if", "height", "==", "h", ":", "\n", "                ", "y_offset", "=", "0", "\n", "", "else", ":", "\n", "                ", "y_offset", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "height", "-", "h", ")", "\n", "", "if", "width", "==", "w", ":", "\n", "                ", "x_offset", "=", "0", "\n", "", "else", ":", "\n", "                ", "x_offset", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "width", "-", "w", ")", "\n", "", "y_offset", "=", "int", "(", "y_offset", ")", "\n", "x_offset", "=", "int", "(", "x_offset", ")", "\n", "cropped", "=", "image", "[", "y_offset", ":", "y_offset", "+", "h", ",", "x_offset", ":", "x_offset", "+", "w", ",", ":", "]", "\n", "assert", "(", "\n", "cropped", ".", "shape", "[", "0", "]", "==", "h", "and", "cropped", ".", "shape", "[", "1", "]", "==", "w", "\n", ")", ",", "\"Wrong crop size\"", "\n", "cropped", "=", "cv2", ".", "resize", "(", "\n", "cropped", ",", "(", "size", ",", "size", ")", ",", "interpolation", "=", "cv2", ".", "INTER_LINEAR", "\n", ")", "\n", "return", "cropped", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "", "return", "center_crop", "(", "size", ",", "scale", "(", "size", ",", "image", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.lighting": [[578, 602], ["numpy.random.normal", "numpy.array", "numpy.reshape", "numpy.sum", "range", "numpy.repeat", "numpy.repeat"], "function", ["None"], ["", "def", "lighting", "(", "img", ",", "alphastd", ",", "eigval", ",", "eigvec", ")", ":", "\n", "    ", "\"\"\"\n    Perform AlexNet-style PCA jitter on the given image.\n    Args:\n        image (array): list of images to perform lighting jitter.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (array): eigenvalues for PCA jitter.\n        eigvec (list): eigenvectors for PCA jitter.\n    Returns:\n        img (tensor): the jittered image.\n    \"\"\"", "\n", "if", "alphastd", "==", "0", ":", "\n", "        ", "return", "img", "\n", "# generate alpha1, alpha2, alpha3.", "\n", "", "alpha", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "alphastd", ",", "size", "=", "(", "1", ",", "3", ")", ")", "\n", "eig_vec", "=", "np", ".", "array", "(", "eigvec", ")", "\n", "eig_val", "=", "np", ".", "reshape", "(", "eigval", ",", "(", "1", ",", "3", ")", ")", "\n", "rgb", "=", "np", ".", "sum", "(", "\n", "eig_vec", "*", "np", ".", "repeat", "(", "alpha", ",", "3", ",", "axis", "=", "0", ")", "*", "np", ".", "repeat", "(", "eig_val", ",", "3", ",", "axis", "=", "0", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "for", "idx", "in", "range", "(", "img", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "img", "[", "idx", "]", "=", "img", "[", "idx", "]", "+", "rgb", "[", "2", "-", "idx", "]", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_sized_crop_list": [[604, 652], ["range", "numpy.random.uniform", "int", "int", "cv2_transform.center_crop", "numpy.random.uniform", "round", "round", "numpy.random.uniform", "int", "int", "cv2_transform.scale", "math.sqrt", "math.sqrt", "numpy.random.randint", "numpy.random.randint", "cv2.resize", "croppsed_images.append", "cv2.resize.astype", "float", "float"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.center_crop", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale"], ["", "def", "random_sized_crop_list", "(", "images", ",", "size", ",", "crop_area_fraction", "=", "0.08", ")", ":", "\n", "    ", "\"\"\"\n    Perform random sized cropping on the given list of images. Random crop with\n        size 8% - 100% image area and aspect ratio in [3/4, 4/3].\n    Args:\n        images (list): image to crop.\n        size (int): size to crop.\n        area_frac (float): area of fraction.\n    Returns:\n        (list): list of cropped image.\n    \"\"\"", "\n", "for", "_", "in", "range", "(", "0", ",", "10", ")", ":", "\n", "        ", "height", "=", "images", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "width", "=", "images", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "area", "=", "height", "*", "width", "\n", "target_area", "=", "np", ".", "random", ".", "uniform", "(", "crop_area_fraction", ",", "1.0", ")", "*", "area", "\n", "aspect_ratio", "=", "np", ".", "random", ".", "uniform", "(", "3.0", "/", "4.0", ",", "4.0", "/", "3.0", ")", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "float", "(", "target_area", ")", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "float", "(", "target_area", ")", "/", "aspect_ratio", ")", ")", ")", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "0.5", ":", "\n", "            ", "w", ",", "h", "=", "h", ",", "w", "\n", "", "if", "h", "<=", "height", "and", "w", "<=", "width", ":", "\n", "            ", "if", "height", "==", "h", ":", "\n", "                ", "y_offset", "=", "0", "\n", "", "else", ":", "\n", "                ", "y_offset", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "height", "-", "h", ")", "\n", "", "if", "width", "==", "w", ":", "\n", "                ", "x_offset", "=", "0", "\n", "", "else", ":", "\n", "                ", "x_offset", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "width", "-", "w", ")", "\n", "", "y_offset", "=", "int", "(", "y_offset", ")", "\n", "x_offset", "=", "int", "(", "x_offset", ")", "\n", "\n", "croppsed_images", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "                ", "cropped", "=", "image", "[", "\n", "y_offset", ":", "y_offset", "+", "h", ",", "x_offset", ":", "x_offset", "+", "w", ",", ":", "\n", "]", "\n", "assert", "(", "\n", "cropped", ".", "shape", "[", "0", "]", "==", "h", "and", "cropped", ".", "shape", "[", "1", "]", "==", "w", "\n", ")", ",", "\"Wrong crop size\"", "\n", "cropped", "=", "cv2", ".", "resize", "(", "\n", "cropped", ",", "(", "size", ",", "size", ")", ",", "interpolation", "=", "cv2", ".", "INTER_LINEAR", "\n", ")", "\n", "croppsed_images", ".", "append", "(", "cropped", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "", "return", "croppsed_images", "\n", "\n", "", "", "return", "[", "center_crop", "(", "size", ",", "scale", "(", "size", ",", "image", ")", ")", "for", "image", "in", "images", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.blend": [[654, 656], ["None"], "function", ["None"], ["", "def", "blend", "(", "image1", ",", "image2", ",", "alpha", ")", ":", "\n", "    ", "return", "image1", "*", "alpha", "+", "image2", "*", "(", "1", "-", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.grayscale": [[658, 674], ["numpy.copy"], "function", ["None"], ["", "def", "grayscale", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Convert the image to gray scale.\n    Args:\n        image (tensor): image to convert to gray scale. Dimension is\n            `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): image in gray scale.\n    \"\"\"", "\n", "# R -> 0.299, G -> 0.587, B -> 0.114.", "\n", "img_gray", "=", "np", ".", "copy", "(", "image", ")", "\n", "gray_channel", "=", "0.299", "*", "image", "[", "2", "]", "+", "0.587", "*", "image", "[", "1", "]", "+", "0.114", "*", "image", "[", "0", "]", "\n", "img_gray", "[", "0", "]", "=", "gray_channel", "\n", "img_gray", "[", "1", "]", "=", "gray_channel", "\n", "img_gray", "[", "2", "]", "=", "gray_channel", "\n", "return", "img_gray", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.saturation": [[676, 688], ["cv2_transform.grayscale", "cv2_transform.blend", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "saturation", "(", "var", ",", "image", ")", ":", "\n", "    ", "\"\"\"\n    Perform color saturation on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color saturation.\n    Returns:\n        (array): image that performed color saturation.\n    \"\"\"", "\n", "img_gray", "=", "grayscale", "(", "image", ")", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "return", "blend", "(", "image", ",", "img_gray", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.brightness": [[690, 702], ["numpy.zeros().astype", "cv2_transform.blend", "numpy.random.uniform", "numpy.zeros"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "brightness", "(", "var", ",", "image", ")", ":", "\n", "    ", "\"\"\"\n    Perform color brightness on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color brightness.\n    Returns:\n        (array): image that performed color brightness.\n    \"\"\"", "\n", "img_bright", "=", "np", ".", "zeros", "(", "image", ".", "shape", ")", ".", "astype", "(", "image", ".", "dtype", ")", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "return", "blend", "(", "image", ",", "img_bright", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.contrast": [[704, 717], ["cv2_transform.grayscale", "grayscale.fill", "cv2_transform.blend", "numpy.mean", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "contrast", "(", "var", ",", "image", ")", ":", "\n", "    ", "\"\"\"\n    Perform color contrast on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color contrast.\n    Returns:\n        (array): image that performed color contrast.\n    \"\"\"", "\n", "img_gray", "=", "grayscale", "(", "image", ")", "\n", "img_gray", ".", "fill", "(", "np", ".", "mean", "(", "img_gray", "[", "0", "]", ")", ")", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "return", "blend", "(", "image", ",", "img_gray", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.saturation_list": [[719, 735], ["numpy.random.uniform", "cv2_transform.grayscale", "out_images.append", "cv2_transform.blend"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "saturation_list", "(", "var", ",", "images", ")", ":", "\n", "    ", "\"\"\"\n    Perform color saturation on the list of given images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color saturation.\n    Returns:\n        (list): list of images that performed color saturation.\n    \"\"\"", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "\n", "out_images", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "        ", "img_gray", "=", "grayscale", "(", "image", ")", "\n", "out_images", ".", "append", "(", "blend", "(", "image", ",", "img_gray", ",", "alpha", ")", ")", "\n", "", "return", "out_images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.brightness_list": [[737, 753], ["numpy.random.uniform", "numpy.zeros().astype", "out_images.append", "cv2_transform.blend", "numpy.zeros"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "brightness_list", "(", "var", ",", "images", ")", ":", "\n", "    ", "\"\"\"\n    Perform color brightness on the given list of images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color brightness.\n    Returns:\n        (array): list of images that performed color brightness.\n    \"\"\"", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "\n", "out_images", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "        ", "img_bright", "=", "np", ".", "zeros", "(", "image", ".", "shape", ")", ".", "astype", "(", "image", ".", "dtype", ")", "\n", "out_images", ".", "append", "(", "blend", "(", "image", ",", "img_bright", ",", "alpha", ")", ")", "\n", "", "return", "out_images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.contrast_list": [[755, 772], ["numpy.random.uniform", "cv2_transform.grayscale", "grayscale.fill", "out_images.append", "numpy.mean", "cv2_transform.blend"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "contrast_list", "(", "var", ",", "images", ")", ":", "\n", "    ", "\"\"\"\n    Perform color contrast on the given list of images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color contrast.\n    Returns:\n        (array): image that performed color contrast.\n    \"\"\"", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "\n", "out_images", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "        ", "img_gray", "=", "grayscale", "(", "image", ")", "\n", "img_gray", ".", "fill", "(", "np", ".", "mean", "(", "img_gray", "[", "0", "]", ")", ")", "\n", "out_images", ".", "append", "(", "blend", "(", "image", ",", "img_gray", ",", "alpha", ")", ")", "\n", "", "return", "out_images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.color_jitter": [[774, 803], ["jitter.append", "jitter.append", "jitter.append", "len", "numpy.random.permutation", "range", "numpy.arange", "len", "len", "cv2_transform.brightness", "cv2_transform.contrast", "cv2_transform.saturation"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.brightness", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.contrast", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.saturation"], ["", "def", "color_jitter", "(", "image", ",", "img_brightness", "=", "0", ",", "img_contrast", "=", "0", ",", "img_saturation", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Perform color jitter on the given image.\n    Args:\n        image (array): image to perform color jitter.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.\n    Returns:\n        image (array): the jittered image.\n    \"\"\"", "\n", "jitter", "=", "[", "]", "\n", "if", "img_brightness", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"brightness\"", ")", "\n", "", "if", "img_contrast", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"contrast\"", ")", "\n", "", "if", "img_saturation", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"saturation\"", ")", "\n", "\n", "", "if", "len", "(", "jitter", ")", ">", "0", ":", "\n", "        ", "order", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "len", "(", "jitter", ")", ")", ")", "\n", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "jitter", ")", ")", ":", "\n", "            ", "if", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"brightness\"", ":", "\n", "                ", "image", "=", "brightness", "(", "img_brightness", ",", "image", ")", "\n", "", "elif", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"contrast\"", ":", "\n", "                ", "image", "=", "contrast", "(", "img_contrast", ",", "image", ")", "\n", "", "elif", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"saturation\"", ":", "\n", "                ", "image", "=", "saturation", "(", "img_saturation", ",", "image", ")", "\n", "", "", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.revert_scaled_boxes": [[805, 820], ["numpy.min"], "function", ["None"], ["", "def", "revert_scaled_boxes", "(", "size", ",", "boxes", ",", "img_height", ",", "img_width", ")", ":", "\n", "    ", "\"\"\"\n    Revert scaled input boxes to match the original image size.\n    Args:\n        size (int): size of the cropped image.\n        boxes (array): shape (num_boxes, 4).\n        img_height (int): height of original image.\n        img_width (int): width of original image.\n    Returns:\n        reverted_boxes (array): boxes scaled back to the original image size.\n    \"\"\"", "\n", "scaled_aspect", "=", "np", ".", "min", "(", "[", "img_height", ",", "img_width", "]", ")", "\n", "scale_ratio", "=", "scaled_aspect", "/", "size", "\n", "reverted_boxes", "=", "boxes", "*", "scale_ratio", "\n", "return", "reverted_boxes", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.video_container.get_video_container": [[7, 30], ["open", "fp.read", "av.open", "NotImplementedError"], "function", ["None"], ["def", "get_video_container", "(", "path_to_vid", ",", "multi_thread_decode", "=", "False", ",", "backend", "=", "\"pyav\"", ")", ":", "\n", "    ", "\"\"\"\n    Given the path to the video, return the pyav video container.\n    Args:\n        path_to_vid (str): path to the video.\n        multi_thread_decode (bool): if True, perform multi-thread decoding.\n        backend (str): decoder backend, options include `pyav` and\n            `torchvision`, default is `pyav`.\n    Returns:\n        container (container): video container.\n    \"\"\"", "\n", "if", "backend", "==", "\"torchvision\"", ":", "\n", "        ", "with", "open", "(", "path_to_vid", ",", "\"rb\"", ")", "as", "fp", ":", "\n", "            ", "container", "=", "fp", ".", "read", "(", ")", "\n", "", "return", "container", "\n", "", "elif", "backend", "==", "\"pyav\"", ":", "\n", "        ", "container", "=", "av", ".", "open", "(", "path_to_vid", ")", "\n", "if", "multi_thread_decode", ":", "\n", "# Enable multiple threads for decoding.", "\n", "            ", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "thread_type", "=", "\"AUTO\"", "\n", "", "return", "container", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Unknown backend {}\"", ".", "format", "(", "backend", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.build.build_dataset": [[15, 32], ["dataset_name.capitalize", "DATASET_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["def", "build_dataset", "(", "dataset_name", ",", "cfg", ",", "split", ")", ":", "\n", "    ", "\"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:\n        dataset_name (str): the name of the dataset to be constructed.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        split (str): the split of the data loader. Options include `train`,\n            `val`, and `test`.\n    Returns:\n        Dataset: a constructed dataset specified by dataset_name.\n    \"\"\"", "\n", "# Capitalize the the first letter of the dataset_name since the dataset_name", "\n", "# in configs may be in lowercase but the name of dataset class should always", "\n", "# start with an uppercase letter.", "\n", "name", "=", "dataset_name", ".", "capitalize", "(", ")", "\n", "return", "DATASET_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "split", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.temporal_sampling": [[11, 29], ["torch.linspace", "torch.clamp().long", "torch.index_select", "torch.clamp"], "function", ["None"], ["def", "temporal_sampling", "(", "frames", ",", "start_idx", ",", "end_idx", ",", "num_samples", ")", ":", "\n", "    ", "\"\"\"\n    Given the start and end frame index, sample num_samples frames between\n    the start and end with equal interval.\n    Args:\n        frames (tensor): a tensor of video frames, dimension is\n            `num video frames` x `channel` x `height` x `width`.\n        start_idx (int): the index of the start frame.\n        end_idx (int): the index of the end frame.\n        num_samples (int): number of frames to sample.\n    Returns:\n        frames (tersor): a tensor of temporal sampled video frames, dimension is\n            `num clip frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "index", "=", "torch", ".", "linspace", "(", "start_idx", ",", "end_idx", ",", "num_samples", ")", "\n", "index", "=", "torch", ".", "clamp", "(", "index", ",", "0", ",", "frames", ".", "shape", "[", "0", "]", "-", "1", ")", ".", "long", "(", ")", "\n", "frames", "=", "torch", ".", "index_select", "(", "frames", ",", "0", ",", "index", ")", "\n", "return", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.get_start_end_idx": [[31, 60], ["max", "random.uniform"], "function", ["None"], ["", "def", "get_start_end_idx", "(", "video_size", ",", "clip_size", ",", "clip_idx", ",", "num_clips", ")", ":", "\n", "    ", "\"\"\"\n    Sample a clip of size clip_size from a video of size video_size and\n    return the indices of the first and last frame of the clip. If clip_idx is\n    -1, the clip is randomly sampled, otherwise uniformly split the video to\n    num_clips clips, and select the start and end index of clip_idx-th video\n    clip.\n    Args:\n        video_size (int): number of overall frames.\n        clip_size (int): size of the clip to sample from the frames.\n        clip_idx (int): if clip_idx is -1, perform random jitter sampling. If\n            clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the start and end index of the clip_idx-th video\n            clip.\n        num_clips (int): overall number of clips to uniformly sample from the\n            given video for testing.\n    Returns:\n        start_idx (int): the start frame index.\n        end_idx (int): the end frame index.\n    \"\"\"", "\n", "delta", "=", "max", "(", "video_size", "-", "clip_size", ",", "0", ")", "\n", "if", "clip_idx", "==", "-", "1", ":", "\n", "# Random temporal sampling.", "\n", "        ", "start_idx", "=", "random", ".", "uniform", "(", "0", ",", "delta", ")", "\n", "", "else", ":", "\n", "# Uniformly sample the clip with the given index.", "\n", "        ", "start_idx", "=", "delta", "*", "clip_idx", "/", "num_clips", "\n", "", "end_idx", "=", "start_idx", "+", "clip_size", "-", "1", "\n", "return", "start_idx", ",", "end_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.pyav_decode_stream": [[62, 102], ["max", "container.seek", "container.decode", "max", "sorted"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.decode"], ["", "def", "pyav_decode_stream", "(", "\n", "container", ",", "start_pts", ",", "end_pts", ",", "stream", ",", "stream_name", ",", "buffer_size", "=", "0", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Decode the video with PyAV decoder.\n    Args:\n        container (container): PyAV container.\n        start_pts (int): the starting Presentation TimeStamp to fetch the\n            video frames.\n        end_pts (int): the ending Presentation TimeStamp of the decoded frames.\n        stream (stream): PyAV stream.\n        stream_name (dict): a dictionary of streams. For example, {\"video\": 0}\n            means video stream at stream index 0.\n        buffer_size (int): number of additional frames to decode beyond end_pts.\n    Returns:\n        result (list): list of frames decoded.\n        max_pts (int): max Presentation TimeStamp of the video sequence.\n    \"\"\"", "\n", "# Seeking in the stream is imprecise. Thus, seek to an ealier PTS by a", "\n", "# margin pts.", "\n", "margin", "=", "1024", "\n", "seek_offset", "=", "max", "(", "start_pts", "-", "margin", ",", "0", ")", "\n", "\n", "container", ".", "seek", "(", "seek_offset", ",", "any_frame", "=", "False", ",", "backward", "=", "True", ",", "stream", "=", "stream", ")", "\n", "frames", "=", "{", "}", "\n", "buffer_count", "=", "0", "\n", "max_pts", "=", "0", "\n", "for", "frame", "in", "container", ".", "decode", "(", "**", "stream_name", ")", ":", "\n", "        ", "max_pts", "=", "max", "(", "max_pts", ",", "frame", ".", "pts", ")", "\n", "if", "frame", ".", "pts", "<", "start_pts", ":", "\n", "            ", "continue", "\n", "", "if", "frame", ".", "pts", "<=", "end_pts", ":", "\n", "            ", "frames", "[", "frame", ".", "pts", "]", "=", "frame", "\n", "", "else", ":", "\n", "            ", "buffer_count", "+=", "1", "\n", "frames", "[", "frame", ".", "pts", "]", "=", "frame", "\n", "if", "buffer_count", ">=", "buffer_size", ":", "\n", "                ", "break", "\n", "", "", "", "result", "=", "[", "frames", "[", "pts", "]", "for", "pts", "in", "sorted", "(", "frames", ")", "]", "\n", "return", "result", ",", "max_pts", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.torchvision_decode": [[104, 197], ["torch.from_numpy", "torchvision._read_video_from_memory", "numpy.frombuffer", "len", "torchvision._probe_video_from_memory", "decoder.get_start_end_idx", "int", "int"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.get_start_end_idx"], ["", "def", "torchvision_decode", "(", "\n", "video_handle", ",", "\n", "sampling_rate", ",", "\n", "num_frames", ",", "\n", "clip_idx", ",", "\n", "video_meta", ",", "\n", "num_clips", "=", "10", ",", "\n", "target_fps", "=", "30", ",", "\n", "modalities", "=", "(", "\"visual\"", ",", ")", ",", "\n", "max_spatial_scale", "=", "0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    If video_meta is not empty, perform temporal selective decoding to sample a\n    clip from the video with TorchVision decoder. If video_meta is empty, decode\n    the entire video and update the video_meta.\n    Args:\n        video_handle (bytes): raw bytes of the video file.\n        sampling_rate (int): frame sampling rate (interval between two sampled\n            frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal\n            sampling. If clip_idx is larger than -1, uniformly split the\n            video to num_clips clips, and select the clip_idx-th video clip.\n        video_meta (dict): a dict contains VideoMetaData. Details can be found\n            at `pytorch/vision/torchvision/io/_video_opt.py`.\n        num_clips (int): overall number of clips to uniformly sample from the\n            given video.\n        target_fps (int): the input video may has different fps, convert it to\n            the target video fps.\n        modalities (tuple): tuple of modalities to decode. Currently only\n            support `visual`, planning to support `acoustic` soon.\n        max_spatial_scale (int): the maximal resolution of the spatial shorter\n            edge size during decoding.\n    Returns:\n        frames (tensor): decoded frames from the video.\n        fps (float): the number of frames per second of the video.\n        decode_all_video (bool): if True, the entire video was decoded.\n    \"\"\"", "\n", "# Convert the bytes to a tensor.", "\n", "video_tensor", "=", "torch", ".", "from_numpy", "(", "np", ".", "frombuffer", "(", "video_handle", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "\n", "decode_all_video", "=", "True", "\n", "video_start_pts", ",", "video_end_pts", "=", "0", ",", "-", "1", "\n", "# The video_meta is empty, fetch the meta data from the raw video.", "\n", "if", "len", "(", "video_meta", ")", "==", "0", ":", "\n", "# Tracking the meta info for selective decoding in the future.", "\n", "        ", "meta", "=", "io", ".", "_probe_video_from_memory", "(", "video_tensor", ")", "\n", "# Using the information from video_meta to perform selective decoding.", "\n", "video_meta", "[", "\"video_timebase\"", "]", "=", "meta", ".", "video_timebase", "\n", "video_meta", "[", "\"video_numerator\"", "]", "=", "meta", ".", "video_timebase", ".", "numerator", "\n", "video_meta", "[", "\"video_denominator\"", "]", "=", "meta", ".", "video_timebase", ".", "denominator", "\n", "video_meta", "[", "\"has_video\"", "]", "=", "meta", ".", "has_video", "\n", "video_meta", "[", "\"video_duration\"", "]", "=", "meta", ".", "video_duration", "\n", "video_meta", "[", "\"video_fps\"", "]", "=", "meta", ".", "video_fps", "\n", "video_meta", "[", "\"audio_timebas\"", "]", "=", "meta", ".", "audio_timebase", "\n", "video_meta", "[", "\"audio_numerator\"", "]", "=", "meta", ".", "audio_timebase", ".", "numerator", "\n", "video_meta", "[", "\"audio_denominator\"", "]", "=", "meta", ".", "audio_timebase", ".", "denominator", "\n", "video_meta", "[", "\"has_audio\"", "]", "=", "meta", ".", "has_audio", "\n", "video_meta", "[", "\"audio_duration\"", "]", "=", "meta", ".", "audio_duration", "\n", "video_meta", "[", "\"audio_sample_rate\"", "]", "=", "meta", ".", "audio_sample_rate", "\n", "\n", "", "if", "(", "\n", "video_meta", "[", "\"has_video\"", "]", "\n", "and", "video_meta", "[", "\"video_denominator\"", "]", ">", "0", "\n", "and", "video_meta", "[", "\"video_duration\"", "]", ">", "0", "\n", ")", ":", "\n", "        ", "decode_all_video", "=", "False", "\n", "start_idx", ",", "end_idx", "=", "get_start_end_idx", "(", "\n", "video_meta", "[", "\"video_fps\"", "]", "*", "video_meta", "[", "\"video_duration\"", "]", ",", "\n", "sampling_rate", "*", "num_frames", "/", "target_fps", "*", "video_meta", "[", "\"video_fps\"", "]", ",", "\n", "clip_idx", ",", "\n", "num_clips", ",", "\n", ")", "\n", "# Convert frame index to pts.", "\n", "pts_per_frame", "=", "(", "\n", "video_meta", "[", "\"video_denominator\"", "]", "/", "video_meta", "[", "\"video_fps\"", "]", "\n", ")", "\n", "video_start_pts", "=", "int", "(", "start_idx", "*", "pts_per_frame", ")", "\n", "video_end_pts", "=", "int", "(", "end_idx", "*", "pts_per_frame", ")", "\n", "\n", "# Decode the raw video with the tv decoder.", "\n", "", "v_frames", ",", "_", "=", "io", ".", "_read_video_from_memory", "(", "\n", "video_tensor", ",", "\n", "seek_frame_margin", "=", "1.0", ",", "\n", "read_video_stream", "=", "\"visual\"", "in", "modalities", ",", "\n", "video_width", "=", "0", ",", "\n", "video_height", "=", "0", ",", "\n", "video_min_dimension", "=", "max_spatial_scale", ",", "\n", "video_pts_range", "=", "(", "video_start_pts", ",", "video_end_pts", ")", ",", "\n", "video_timebase_numerator", "=", "video_meta", "[", "\"video_numerator\"", "]", ",", "\n", "video_timebase_denominator", "=", "video_meta", "[", "\"video_denominator\"", "]", ",", "\n", ")", "\n", "return", "v_frames", ",", "video_meta", "[", "\"video_fps\"", "]", ",", "decode_all_video", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.pyav_decode": [[199, 266], ["float", "decoder.get_start_end_idx", "int", "int", "decoder.pyav_decode_stream", "container.close", "torch.as_tensor", "frame.to_rgb().to_ndarray", "numpy.stack", "frame.to_rgb"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.get_start_end_idx", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.pyav_decode_stream", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close"], ["", "def", "pyav_decode", "(", "\n", "container", ",", "sampling_rate", ",", "num_frames", ",", "clip_idx", ",", "num_clips", "=", "10", ",", "target_fps", "=", "30", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Convert the video from its original fps to the target_fps. If the video\n    support selective decoding (contain decoding information in the video head),\n    the perform temporal selective decoding and sample a clip from the video\n    with the PyAV decoder. If the video does not support selective decoding,\n    decode the entire video.\n\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled\n            frames.\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal sampling. If\n            clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly sample from the\n            given video.\n        target_fps (int): the input video may has different fps, convert it to\n            the target video fps before frame sampling.\n    Returns:\n        frames (tensor): decoded frames from the video. Return None if the no\n            video stream was found.\n        fps (float): the number of frames per second of the video.\n        decode_all_video (bool): If True, the entire video was decoded.\n    \"\"\"", "\n", "# Try to fetch the decoding information from the video head. Some of the", "\n", "# videos does not support fetching the decoding information, for that case", "\n", "# it will get None duration.", "\n", "fps", "=", "float", "(", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "average_rate", ")", "\n", "frames_length", "=", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "frames", "\n", "duration", "=", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "duration", "\n", "\n", "if", "duration", "is", "None", ":", "\n", "# If failed to fetch the decoding information, decode the entire video.", "\n", "        ", "decode_all_video", "=", "True", "\n", "video_start_pts", ",", "video_end_pts", "=", "0", ",", "math", ".", "inf", "\n", "", "else", ":", "\n", "# Perform selective decoding.", "\n", "        ", "decode_all_video", "=", "False", "\n", "start_idx", ",", "end_idx", "=", "get_start_end_idx", "(", "\n", "frames_length", ",", "\n", "sampling_rate", "*", "num_frames", "/", "target_fps", "*", "fps", ",", "\n", "clip_idx", ",", "\n", "num_clips", ",", "\n", ")", "\n", "timebase", "=", "duration", "/", "frames_length", "\n", "video_start_pts", "=", "int", "(", "start_idx", "*", "timebase", ")", "\n", "video_end_pts", "=", "int", "(", "end_idx", "*", "timebase", ")", "\n", "\n", "", "frames", "=", "None", "\n", "# If video stream was found, fetch video frames from the video.", "\n", "if", "container", ".", "streams", ".", "video", ":", "\n", "        ", "video_frames", ",", "max_pts", "=", "pyav_decode_stream", "(", "\n", "container", ",", "\n", "video_start_pts", ",", "\n", "video_end_pts", ",", "\n", "container", ".", "streams", ".", "video", "[", "0", "]", ",", "\n", "{", "\"video\"", ":", "0", "}", ",", "\n", ")", "\n", "container", ".", "close", "(", ")", "\n", "\n", "frames", "=", "[", "frame", ".", "to_rgb", "(", ")", ".", "to_ndarray", "(", ")", "for", "frame", "in", "video_frames", "]", "\n", "frames", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "frames", ")", ")", "\n", "", "return", "frames", ",", "fps", ",", "decode_all_video", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.decode": [[268, 349], ["decoder.get_start_end_idx", "decoder.temporal_sampling", "decoder.pyav_decode", "print", "temporal_sampling.size", "decoder.torchvision_decode", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.get_start_end_idx", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.temporal_sampling", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.pyav_decode", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.decoder.torchvision_decode"], ["", "def", "decode", "(", "\n", "container", ",", "\n", "sampling_rate", ",", "\n", "num_frames", ",", "\n", "clip_idx", "=", "-", "1", ",", "\n", "num_clips", "=", "10", ",", "\n", "video_meta", "=", "None", ",", "\n", "target_fps", "=", "30", ",", "\n", "backend", "=", "\"pyav\"", ",", "\n", "max_spatial_scale", "=", "0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Decode the video and perform temporal sampling.\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled\n            frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal\n            sampling. If clip_idx is larger than -1, uniformly split the\n            video to num_clips clips, and select the\n            clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly\n            sample from the given video.\n        video_meta (dict): a dict contains VideoMetaData. Details can be find\n            at `pytorch/vision/torchvision/io/_video_opt.py`.\n        target_fps (int): the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n        backend (str): decoding backend includes `pyav` and `torchvision`. The\n            default one is `pyav`.\n        max_spatial_scale (int): keep the aspect ratio and resize the frame so\n            that shorter edge size is max_spatial_scale. Only used in\n            `torchvision` backend.\n    Returns:\n        frames (tensor): decoded frames from the video.\n    \"\"\"", "\n", "# Currently support two decoders: 1) PyAV, and 2) TorchVision.", "\n", "assert", "clip_idx", ">=", "-", "1", ",", "\"Not valied clip_idx {}\"", ".", "format", "(", "clip_idx", ")", "\n", "try", ":", "\n", "        ", "if", "backend", "==", "\"pyav\"", ":", "\n", "            ", "frames", ",", "fps", ",", "decode_all_video", "=", "pyav_decode", "(", "\n", "container", ",", "\n", "sampling_rate", ",", "\n", "num_frames", ",", "\n", "clip_idx", ",", "\n", "num_clips", ",", "\n", "target_fps", ",", "\n", ")", "\n", "", "elif", "backend", "==", "\"torchvision\"", ":", "\n", "            ", "frames", ",", "fps", ",", "decode_all_video", "=", "torchvision_decode", "(", "\n", "container", ",", "\n", "sampling_rate", ",", "\n", "num_frames", ",", "\n", "clip_idx", ",", "\n", "video_meta", ",", "\n", "num_clips", ",", "\n", "target_fps", ",", "\n", "(", "\"visual\"", ",", ")", ",", "\n", "max_spatial_scale", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"Unknown decoding backend {}\"", ".", "format", "(", "backend", ")", "\n", ")", "\n", "", "", "except", "NotImplementedError", "as", "e", ":", "\n", "        ", "print", "(", "\"Failed to decode by {} with exception: {}\"", ".", "format", "(", "backend", ",", "e", ")", ")", "\n", "return", "None", "\n", "\n", "# Return None if the frames was not decoded successfully.", "\n", "", "if", "frames", "is", "None", "or", "frames", ".", "size", "(", "0", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "\n", "", "start_idx", ",", "end_idx", "=", "get_start_end_idx", "(", "\n", "frames", ".", "shape", "[", "0", "]", ",", "\n", "num_frames", "*", "sampling_rate", "*", "fps", "/", "target_fps", ",", "\n", "clip_idx", "if", "decode_all_video", "else", "0", ",", "\n", "num_clips", "if", "decode_all_video", "else", "1", ",", "\n", ")", "\n", "# Perform temporal sampling from the decoded video.", "\n", "frames", "=", "temporal_sampling", "(", "frames", ",", "start_idx", ",", "end_idx", ",", "num_frames", ")", "\n", "return", "frames", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.load_image_lists": [[18, 69], ["collections.defaultdict", "logger.info", "os.path.join", "fvcore.common.file_io.PathManager.open", "f.readline", "range", "line.split", "image_paths[].append", "len", "len", "len", "video_idx_to_name.append", "os.path.join"], "function", ["None"], ["def", "load_image_lists", "(", "cfg", ",", "is_train", ")", ":", "\n", "    ", "\"\"\"\n    Loading image paths from corresponding files.\n\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)\n            corresponds to one video and contains the paths of images for\n            this video.\n        video_idx_to_name (list): a list which stores video names.\n    \"\"\"", "\n", "list_filenames", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "cfg", ".", "AVA", ".", "FRAME_LIST_DIR", ",", "filename", ")", "\n", "for", "filename", "in", "(", "\n", "cfg", ".", "AVA", ".", "TRAIN_LISTS", "if", "is_train", "else", "cfg", ".", "AVA", ".", "TEST_LISTS", "\n", ")", "\n", "]", "\n", "image_paths", "=", "defaultdict", "(", "list", ")", "\n", "video_name_to_idx", "=", "{", "}", "\n", "video_idx_to_name", "=", "[", "]", "\n", "for", "list_filename", "in", "list_filenames", ":", "\n", "        ", "with", "PathManager", ".", "open", "(", "list_filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "readline", "(", ")", "\n", "for", "line", "in", "f", ":", "\n", "                ", "row", "=", "line", ".", "split", "(", ")", "\n", "# The format of each row should follow:", "\n", "# original_vido_id video_id frame_id path labels.", "\n", "assert", "len", "(", "row", ")", ">=", "4", "\n", "video_name", "=", "row", "[", "0", "]", "\n", "\n", "if", "video_name", "not", "in", "video_name_to_idx", ":", "\n", "                    ", "idx", "=", "len", "(", "video_name_to_idx", ")", "\n", "video_name_to_idx", "[", "video_name", "]", "=", "idx", "\n", "video_idx_to_name", ".", "append", "(", "video_name", ")", "\n", "\n", "", "data_key", "=", "video_name_to_idx", "[", "video_name", "]", "\n", "\n", "image_paths", "[", "data_key", "]", ".", "append", "(", "\n", "os", ".", "path", ".", "join", "(", "cfg", ".", "AVA", ".", "FRAME_DIR", ",", "row", "[", "3", "]", ")", "\n", ")", "\n", "\n", "", "", "", "image_paths", "=", "[", "image_paths", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "image_paths", ")", ")", "]", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Finished loading image paths from: %s\"", "%", "\", \"", ".", "join", "(", "list_filenames", ")", "\n", ")", "\n", "\n", "return", "image_paths", ",", "video_idx_to_name", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.load_feature_bank": [[71, 84], ["bank.items", "os.path.exists", "torch.load", "len"], "function", ["None"], ["", "def", "load_feature_bank", "(", "cfg", ",", "mode", ")", ":", "\n", "\n", "    ", "bank_path", "=", "cfg", ".", "AVA", ".", "FEATURE_BANK_PATH", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "bank_path", ")", ":", "\n", "        ", "return", "None", ",", "0", "\n", "\n", "", "bank", "=", "torch", ".", "load", "(", "bank_path", ")", "[", "mode", "]", "\n", "num", "=", "0", "\n", "for", "_", ",", "saved", "in", "bank", ".", "items", "(", ")", ":", "\n", "        ", "for", "sec", "in", "saved", ":", "\n", "            ", "num", "+=", "len", "(", "saved", "[", "sec", "]", "[", "'feature'", "]", ")", "\n", "\n", "", "", "return", "bank", ",", "num", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.draw_feature_from_sliding_window": [[86, 136], ["list", "max", "min", "db.begin", "range", "len", "txn.get", "torch.cat", "os.path.join().encode", "pickle.loads", "torch.cat.append", "os.path.join"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "draw_feature_from_sliding_window", "(", "cfg", ",", "vid", ",", "sec", ",", "split", ",", "db", "=", "None", ")", ":", "\n", "\n", "    ", "\"\"\"\n\n    Args:\n        cfg: (CfgNode) global configuration\n        vid: (int) video instance number\n        sec: (int) second number\n        split: (str) dataset split, 'train', 'val' or 'test'\n\n    Returns:\n        output_feat: (torch.Tensor or None)\n        output_context: (torch.Tensor or None)\n    \"\"\"", "\n", "\n", "window_size", "=", "cfg", ".", "AVA", ".", "SLIDING_WINDOW_SIZE", "\n", "valid_range_list", "=", "list", "(", "AVA_VALID_FRAMES", ")", "\n", "min_f", ",", "max_f", "=", "valid_range_list", "[", "0", "]", ",", "valid_range_list", "[", "-", "1", "]", "\n", "start", "=", "max", "(", "sec", "-", "window_size", ",", "min_f", ")", "\n", "end", "=", "min", "(", "sec", "+", "window_size", ",", "max_f", ")", "\n", "\n", "bank_mode", "=", "\"train\"", "if", "split", "==", "\"train\"", "else", "\"test\"", "\n", "bank_path", "=", "\"{}/{}\"", ".", "format", "(", "bank_mode", ",", "vid", ")", "\n", "\n", "output_feat", "=", "[", "]", "\n", "time_stamp", "=", "[", "]", "\n", "\n", "assert", "db", "is", "not", "None", ",", "'empty feature bank'", "\n", "with", "db", ".", "begin", "(", "write", "=", "False", ")", "as", "txn", ":", "\n", "\n", "        ", "for", "s", "in", "range", "(", "start", ",", "end", "+", "1", ")", ":", "\n", "            ", "if", "s", "==", "sec", ":", "\n", "                ", "continue", "\n", "\n", "", "key_path", "=", "'{}/{}'", ".", "format", "(", "bank_path", ",", "s", ")", "\n", "# logger.info(key_path)", "\n", "feat_db_val", "=", "txn", ".", "get", "(", "os", ".", "path", ".", "join", "(", "key_path", ",", "'feature'", ")", ".", "encode", "(", ")", ")", "\n", "if", "feat_db_val", "is", "not", "None", ":", "\n", "                ", "fb", "=", "pickle", ".", "loads", "(", "feat_db_val", ")", "\n", "output_feat", ".", "append", "(", "fb", ")", "\n", "time_stamp", "+=", "[", "s", "]", "*", "fb", ".", "shape", "[", "0", "]", "\n", "\n", "", "", "", "if", "len", "(", "output_feat", ")", "==", "0", ":", "\n", "        ", "output_feat", "=", "None", "\n", "", "elif", "cfg", ".", "AVA", ".", "GATHER_BANK", ":", "\n", "        ", "output_feat", "=", "torch", ".", "cat", "(", "output_feat", ",", "dim", "=", "0", ")", "\n", "\n", "# if output_feat is not None:", "\n", "#     print(start, end, output_feat.shape[0], output_context.shape[0])", "\n", "", "return", "output_feat", ",", "time_stamp", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.load_boxes_and_labels": [[138, 184], ["ava_helper.parse_bboxes_file", "logger.info", "logger.info", "logger.info", "logger.info", "os.path.join", "len", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.parse_bboxes_file"], ["", "def", "load_boxes_and_labels", "(", "cfg", ",", "mode", ")", ":", "\n", "    ", "\"\"\"\n    Loading boxes and labels from csv files.\n\n    Args:\n        cfg (CfgNode): config.\n        mode (str): 'train', 'val', or 'test' mode.\n    Returns:\n        all_boxes (dict): a dict which maps from `video_name` and\n            `frame_sec` to a list of `box`. Each `box` is a\n            [`box_coord`, `box_labels`] where `box_coord` is the\n            coordinates of box and 'box_labels` are the corresponding\n            labels for the box.\n    \"\"\"", "\n", "gt_lists", "=", "cfg", ".", "AVA", ".", "TRAIN_GT_BOX_LISTS", "if", "mode", "==", "\"train\"", "else", "cfg", ".", "AVA", ".", "TEST_GT_BOX_LISTS", "\n", "pred_lists", "=", "(", "\n", "cfg", ".", "AVA", ".", "TRAIN_PREDICT_BOX_LISTS", "\n", "if", "mode", "==", "\"train\"", "\n", "else", "cfg", ".", "AVA", ".", "TEST_PREDICT_BOX_LISTS", "\n", ")", "\n", "ann_filenames", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "cfg", ".", "AVA", ".", "ANNOTATION_DIR", ",", "filename", ")", "\n", "for", "filename", "in", "gt_lists", "+", "pred_lists", "\n", "]", "\n", "ann_is_gt_box", "=", "[", "True", "]", "*", "len", "(", "gt_lists", ")", "+", "[", "False", "]", "*", "len", "(", "pred_lists", ")", "\n", "\n", "detect_thresh", "=", "cfg", ".", "AVA", ".", "DETECTION_SCORE_THRESH", "\n", "# Only select frame_sec % 4 = 0 samples for validation if not", "\n", "# set FULL_TEST_ON_VAL.", "\n", "boxes_sample_rate", "=", "(", "\n", "4", "if", "mode", "==", "\"val\"", "and", "not", "cfg", ".", "AVA", ".", "FULL_TEST_ON_VAL", "else", "1", "\n", ")", "\n", "all_boxes", ",", "count", ",", "unique_box_count", "=", "parse_bboxes_file", "(", "\n", "ann_filenames", "=", "ann_filenames", ",", "\n", "ann_is_gt_box", "=", "ann_is_gt_box", ",", "\n", "detect_thresh", "=", "detect_thresh", ",", "\n", "boxes_sample_rate", "=", "boxes_sample_rate", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Finished loading annotations from: %s\"", "%", "\", \"", ".", "join", "(", "ann_filenames", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"Detection threshold: {}\"", ".", "format", "(", "detect_thresh", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of unique boxes: %d\"", "%", "unique_box_count", ")", "\n", "logger", ".", "info", "(", "\"Number of annotations: %d\"", "%", "count", ")", "\n", "\n", "return", "all_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.get_keyframe_data": [[186, 230], ["range", "logger.info", "len", "keyframe_boxes_and_labels.append", "boxes_and_labels[].keys", "len", "keyframe_indices.append", "keyframe_boxes_and_labels[].append", "ava_helper.get_keyframe_data.sec_to_frame"], "function", ["None"], ["", "def", "get_keyframe_data", "(", "boxes_and_labels", ")", ":", "\n", "    ", "\"\"\"\n    Getting keyframe indices, boxes and labels in the dataset.\n\n    Args:\n        boxes_and_labels (list[dict]): a list which maps from video_idx to a dict.\n            Each dict `frame_sec` to a list of boxes and corresponding labels.\n\n    Returns:\n        keyframe_indices (list): a list of indices of the keyframes.\n        keyframe_boxes_and_labels (list[list[list]]): a list of list which maps from\n            video_idx and sec_idx to a list of boxes and corresponding labels.\n    \"\"\"", "\n", "\n", "def", "sec_to_frame", "(", "sec", ")", ":", "\n", "        ", "\"\"\"\n        Convert time index (in second) to frame index.\n        0: 900\n        30: 901\n        \"\"\"", "\n", "return", "(", "sec", "-", "900", ")", "*", "FPS", "\n", "\n", "", "keyframe_indices", "=", "[", "]", "\n", "keyframe_boxes_and_labels", "=", "[", "]", "\n", "count", "=", "0", "\n", "for", "video_idx", "in", "range", "(", "len", "(", "boxes_and_labels", ")", ")", ":", "\n", "        ", "sec_idx", "=", "0", "\n", "keyframe_boxes_and_labels", ".", "append", "(", "[", "]", ")", "\n", "for", "sec", "in", "boxes_and_labels", "[", "video_idx", "]", ".", "keys", "(", ")", ":", "\n", "            ", "if", "sec", "not", "in", "AVA_VALID_FRAMES", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "len", "(", "boxes_and_labels", "[", "video_idx", "]", "[", "sec", "]", ")", ">", "0", ":", "\n", "                ", "keyframe_indices", ".", "append", "(", "\n", "(", "video_idx", ",", "sec_idx", ",", "sec", ",", "sec_to_frame", "(", "sec", ")", ")", "\n", ")", "\n", "keyframe_boxes_and_labels", "[", "video_idx", "]", ".", "append", "(", "\n", "boxes_and_labels", "[", "video_idx", "]", "[", "sec", "]", "\n", ")", "\n", "sec_idx", "+=", "1", "\n", "count", "+=", "1", "\n", "", "", "", "logger", ".", "info", "(", "\"%d keyframes used.\"", "%", "count", ")", "\n", "\n", "return", "keyframe_indices", ",", "keyframe_boxes_and_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.get_num_boxes_used": [[232, 249], ["len"], "function", ["None"], ["", "def", "get_num_boxes_used", "(", "keyframe_indices", ",", "keyframe_boxes_and_labels", ")", ":", "\n", "    ", "\"\"\"\n    Get total number of used boxes.\n\n    Args:\n        keyframe_indices (list): a list of indices of the keyframes.\n        keyframe_boxes_and_labels (list[list[list]]): a list of list which maps from\n            video_idx and sec_idx to a list of boxes and corresponding labels.\n\n    Returns:\n        count (int): total number of used boxes.\n    \"\"\"", "\n", "\n", "count", "=", "0", "\n", "for", "video_idx", ",", "sec_idx", ",", "_", ",", "_", "in", "keyframe_indices", ":", "\n", "        ", "count", "+=", "len", "(", "keyframe_boxes_and_labels", "[", "video_idx", "]", "[", "sec_idx", "]", ")", "\n", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.parse_bboxes_file": [[251, 307], ["zip", "all_boxes.keys", "all_boxes[].keys", "fvcore.common.file_io.PathManager.open", "list", "line.strip().split", "list", "[].append", "[].values", "float", "int", "map", "int", "line.strip"], "function", ["None"], ["", "def", "parse_bboxes_file", "(", "\n", "ann_filenames", ",", "ann_is_gt_box", ",", "detect_thresh", ",", "boxes_sample_rate", "=", "1", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Parse AVA bounding boxes files.\n    Args:\n        ann_filenames (list of str(s)): a list of AVA bounding boxes annotation files.\n        ann_is_gt_box (list of bools): a list of boolean to indicate whether the corresponding\n            ann_file is ground-truth. `ann_is_gt_box[i]` correspond to `ann_filenames[i]`.\n        detect_thresh (float): threshold for accepting predicted boxes, range [0, 1].\n        boxes_sample_rate (int): sample rate for test bounding boxes. Get 1 every `boxes_sample_rate`.\n    \"\"\"", "\n", "all_boxes", "=", "{", "}", "\n", "count", "=", "0", "\n", "unique_box_count", "=", "0", "\n", "for", "filename", ",", "is_gt_box", "in", "zip", "(", "ann_filenames", ",", "ann_is_gt_box", ")", ":", "\n", "        ", "with", "PathManager", ".", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "row", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "# When we use predicted boxes to train/eval, we need to", "\n", "# ignore the boxes whose scores are below the threshold.", "\n", "if", "not", "is_gt_box", ":", "\n", "                    ", "score", "=", "float", "(", "row", "[", "7", "]", ")", "\n", "if", "score", "<", "detect_thresh", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "video_name", ",", "frame_sec", "=", "row", "[", "0", "]", ",", "int", "(", "row", "[", "1", "]", ")", "\n", "if", "frame_sec", "%", "boxes_sample_rate", "!=", "0", ":", "\n", "                    ", "continue", "\n", "\n", "# Box with format [x1, y1, x2, y2] with a range of [0, 1] as float.", "\n", "", "box_key", "=", "\",\"", ".", "join", "(", "row", "[", "2", ":", "6", "]", ")", "\n", "box", "=", "list", "(", "map", "(", "float", ",", "row", "[", "2", ":", "6", "]", ")", ")", "\n", "label", "=", "-", "1", "if", "row", "[", "6", "]", "==", "\"\"", "else", "int", "(", "row", "[", "6", "]", ")", "\n", "\n", "if", "video_name", "not", "in", "all_boxes", ":", "\n", "                    ", "all_boxes", "[", "video_name", "]", "=", "{", "}", "\n", "for", "sec", "in", "AVA_VALID_FRAMES", ":", "\n", "                        ", "all_boxes", "[", "video_name", "]", "[", "sec", "]", "=", "{", "}", "\n", "\n", "", "", "if", "box_key", "not", "in", "all_boxes", "[", "video_name", "]", "[", "frame_sec", "]", ":", "\n", "                    ", "all_boxes", "[", "video_name", "]", "[", "frame_sec", "]", "[", "box_key", "]", "=", "[", "box", ",", "[", "]", "]", "\n", "unique_box_count", "+=", "1", "\n", "\n", "", "all_boxes", "[", "video_name", "]", "[", "frame_sec", "]", "[", "box_key", "]", "[", "1", "]", ".", "append", "(", "label", ")", "\n", "if", "label", "!=", "-", "1", ":", "\n", "                    ", "count", "+=", "1", "\n", "\n", "", "", "", "", "for", "video_name", "in", "all_boxes", ".", "keys", "(", ")", ":", "\n", "        ", "for", "frame_sec", "in", "all_boxes", "[", "video_name", "]", ".", "keys", "(", ")", ":", "\n", "# Save in format of a list of [box_i, box_i_labels].", "\n", "            ", "all_boxes", "[", "video_name", "]", "[", "frame_sec", "]", "=", "list", "(", "\n", "all_boxes", "[", "video_name", "]", "[", "frame_sec", "]", ".", "values", "(", ")", "\n", ")", "\n", "\n", "", "", "return", "all_boxes", ",", "count", ",", "unique_box_count", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_short_side_scale_jitter": [[9, 63], ["int", "int", "int", "int", "torch.nn.functional.interpolate", "round", "round", "math.floor", "math.floor", "numpy.random.uniform", "numpy.random.uniform", "float", "float", "float", "float"], "function", ["None"], ["def", "random_short_side_scale_jitter", "(", "\n", "images", ",", "min_size", ",", "max_size", ",", "boxes", "=", "None", ",", "inverse_uniform_sampling", "=", "False", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (tensor): images to perform scale jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        min_size (int): the minimal size to scale the frames.\n        max_size (int): the maximal size to scale the frames.\n        boxes (ndarray): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        inverse_uniform_sampling (bool): if True, sample uniformly in\n            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the\n            scale. If False, take a uniform sample from [min_scale, max_scale].\n    Returns:\n        (tensor): the scaled images with dimension of\n            `num frames` x `channel` x `new height` x `new width`.\n        (ndarray or None): the scaled boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "if", "inverse_uniform_sampling", ":", "\n", "        ", "size", "=", "int", "(", "\n", "round", "(", "1.0", "/", "np", ".", "random", ".", "uniform", "(", "1.0", "/", "max_size", ",", "1.0", "/", "min_size", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "size", "=", "int", "(", "round", "(", "np", ".", "random", ".", "uniform", "(", "min_size", ",", "max_size", ")", ")", ")", "\n", "\n", "", "height", "=", "images", ".", "shape", "[", "2", "]", "\n", "width", "=", "images", ".", "shape", "[", "3", "]", "\n", "if", "(", "width", "<=", "height", "and", "width", "==", "size", ")", "or", "(", "\n", "height", "<=", "width", "and", "height", "==", "size", "\n", ")", ":", "\n", "        ", "return", "images", ",", "boxes", "\n", "", "new_width", "=", "size", "\n", "new_height", "=", "size", "\n", "if", "width", "<", "height", ":", "\n", "        ", "new_height", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "height", ")", "/", "width", ")", "*", "size", ")", ")", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "boxes", "=", "boxes", "*", "float", "(", "new_height", ")", "/", "height", "\n", "", "", "else", ":", "\n", "        ", "new_width", "=", "int", "(", "math", ".", "floor", "(", "(", "float", "(", "width", ")", "/", "height", ")", "*", "size", ")", ")", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "boxes", "=", "boxes", "*", "float", "(", "new_width", ")", "/", "width", "\n", "\n", "", "", "return", "(", "\n", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "\n", "images", ",", "\n", "size", "=", "(", "new_height", ",", "new_width", ")", ",", "\n", "mode", "=", "\"bilinear\"", ",", "\n", "align_corners", "=", "False", ",", "\n", ")", ",", "\n", "boxes", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.crop_boxes": [[66, 83], ["boxes.copy"], "function", ["None"], ["", "def", "crop_boxes", "(", "boxes", ",", "x_offset", ",", "y_offset", ")", ":", "\n", "    ", "\"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "cropped_boxes", "=", "boxes", ".", "copy", "(", ")", "\n", "cropped_boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "-", "x_offset", "\n", "cropped_boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "=", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "-", "y_offset", "\n", "\n", "return", "cropped_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_crop": [[85, 119], ["int", "int", "transform.crop_boxes", "numpy.random.randint", "numpy.random.randint"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.crop_boxes"], ["", "def", "random_crop", "(", "images", ",", "size", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform random spatial crop on the given images and corresponding boxes.\n    Args:\n        images (tensor): images to perform random crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): the size of height and width to crop on the image.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        cropped (tensor): cropped images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "if", "images", ".", "shape", "[", "2", "]", "==", "size", "and", "images", ".", "shape", "[", "3", "]", "==", "size", ":", "\n", "        ", "return", "images", "\n", "", "height", "=", "images", ".", "shape", "[", "2", "]", "\n", "width", "=", "images", ".", "shape", "[", "3", "]", "\n", "y_offset", "=", "0", "\n", "if", "height", ">", "size", ":", "\n", "        ", "y_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "height", "-", "size", ")", ")", "\n", "", "x_offset", "=", "0", "\n", "if", "width", ">", "size", ":", "\n", "        ", "x_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "width", "-", "size", ")", ")", "\n", "", "cropped", "=", "images", "[", "\n", ":", ",", ":", ",", "y_offset", ":", "y_offset", "+", "size", ",", "x_offset", ":", "x_offset", "+", "size", "\n", "]", "\n", "\n", "cropped_boxes", "=", "(", "\n", "crop_boxes", "(", "boxes", ",", "x_offset", ",", "y_offset", ")", "if", "boxes", "is", "not", "None", "else", "None", "\n", ")", "\n", "\n", "return", "cropped", ",", "cropped_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.horizontal_flip": [[121, 149], ["boxes.copy", "numpy.random.uniform", "images.flip.flip"], "function", ["None"], ["", "def", "horizontal_flip", "(", "prob", ",", "images", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform horizontal flip on the given images and corresponding boxes.\n    Args:\n        prob (float): probility to flip the images.\n        images (tensor): images to perform horizontal flip, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        images (tensor): images with dimension of\n            `num frames` x `channel` x `height` x `width`.\n        flipped_boxes (ndarray or None): the flipped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "if", "boxes", "is", "None", ":", "\n", "        ", "flipped_boxes", "=", "None", "\n", "", "else", ":", "\n", "        ", "flipped_boxes", "=", "boxes", ".", "copy", "(", ")", "\n", "\n", "", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "prob", ":", "\n", "        ", "images", "=", "images", ".", "flip", "(", "(", "-", "1", ")", ")", "\n", "\n", "width", "=", "images", ".", "shape", "[", "3", "]", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "flipped_boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "width", "-", "boxes", "[", ":", ",", "[", "2", ",", "0", "]", "]", "-", "1", "\n", "\n", "", "", "return", "images", ",", "flipped_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.uniform_crop": [[151, 195], ["int", "int", "math.ceil", "math.ceil", "transform.crop_boxes"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.crop_boxes"], ["", "def", "uniform_crop", "(", "images", ",", "size", ",", "spatial_idx", ",", "boxes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "assert", "spatial_idx", "in", "[", "0", ",", "1", ",", "2", "]", "\n", "height", "=", "images", ".", "shape", "[", "2", "]", "\n", "width", "=", "images", ".", "shape", "[", "3", "]", "\n", "\n", "y_offset", "=", "int", "(", "math", ".", "ceil", "(", "(", "height", "-", "size", ")", "/", "2", ")", ")", "\n", "x_offset", "=", "int", "(", "math", ".", "ceil", "(", "(", "width", "-", "size", ")", "/", "2", ")", ")", "\n", "\n", "if", "height", ">", "width", ":", "\n", "        ", "if", "spatial_idx", "==", "0", ":", "\n", "            ", "y_offset", "=", "0", "\n", "", "elif", "spatial_idx", "==", "2", ":", "\n", "            ", "y_offset", "=", "height", "-", "size", "\n", "", "", "else", ":", "\n", "        ", "if", "spatial_idx", "==", "0", ":", "\n", "            ", "x_offset", "=", "0", "\n", "", "elif", "spatial_idx", "==", "2", ":", "\n", "            ", "x_offset", "=", "width", "-", "size", "\n", "", "", "cropped", "=", "images", "[", "\n", ":", ",", ":", ",", "y_offset", ":", "y_offset", "+", "size", ",", "x_offset", ":", "x_offset", "+", "size", "\n", "]", "\n", "\n", "cropped_boxes", "=", "(", "\n", "crop_boxes", "(", "boxes", ",", "x_offset", ",", "y_offset", ")", "if", "boxes", "is", "not", "None", "else", "None", "\n", ")", "\n", "\n", "return", "cropped", ",", "cropped_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.clip_boxes_to_image": [[197, 217], ["boxes.copy", "numpy.minimum", "numpy.minimum", "numpy.maximum", "numpy.maximum"], "function", ["None"], ["", "def", "clip_boxes_to_image", "(", "boxes", ",", "height", ",", "width", ")", ":", "\n", "    ", "\"\"\"\n    Clip an array of boxes to an image with the given height and width.\n    Args:\n        boxes (ndarray): bounding boxes to perform clipping.\n            Dimension is `num boxes` x 4.\n        height (int): given image height.\n        width (int): given image width.\n    Returns:\n        clipped_boxes (ndarray): the clipped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "\n", "clipped_boxes", "=", "boxes", ".", "copy", "(", ")", "\n", "clipped_boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "=", "np", ".", "minimum", "(", "\n", "width", "-", "1.0", ",", "np", ".", "maximum", "(", "0.0", ",", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", ")", "\n", ")", "\n", "clipped_boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "=", "np", ".", "minimum", "(", "\n", "height", "-", "1.0", ",", "np", ".", "maximum", "(", "0.0", ",", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", ")", "\n", ")", "\n", "return", "clipped_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend": [[219, 233], ["None"], "function", ["None"], ["", "def", "blend", "(", "images1", ",", "images2", ",", "alpha", ")", ":", "\n", "    ", "\"\"\"\n    Blend two images with a given weight alpha.\n    Args:\n        images1 (tensor): the first images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        images2 (tensor): the second images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alpha (float): the blending weight.\n    Returns:\n        (tensor): blended images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "return", "images1", "*", "alpha", "+", "images2", "*", "(", "1", "-", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale": [[235, 255], ["torch.tensor"], "function", ["None"], ["", "def", "grayscale", "(", "images", ")", ":", "\n", "    ", "\"\"\"\n    Get the grayscale for the input images. The channels of images should be\n    in order BGR.\n    Args:\n        images (tensor): the input images for getting grayscale. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): blended images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "# R -> 0.299, G -> 0.587, B -> 0.114.", "\n", "img_gray", "=", "torch", ".", "tensor", "(", "images", ")", "\n", "gray_channel", "=", "(", "\n", "0.299", "*", "images", "[", ":", ",", "2", "]", "+", "0.587", "*", "images", "[", ":", ",", "1", "]", "+", "0.114", "*", "images", "[", ":", ",", "0", "]", "\n", ")", "\n", "img_gray", "[", ":", ",", "0", "]", "=", "gray_channel", "\n", "img_gray", "[", ":", ",", "1", "]", "=", "gray_channel", "\n", "img_gray", "[", ":", ",", "2", "]", "=", "gray_channel", "\n", "return", "img_gray", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.color_jitter": [[257, 290], ["jitter.append", "jitter.append", "jitter.append", "len", "numpy.random.permutation", "range", "numpy.arange", "len", "len", "transform.brightness_jitter", "transform.contrast_jitter", "transform.saturation_jitter"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.brightness_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.contrast_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.saturation_jitter"], ["", "def", "color_jitter", "(", "images", ",", "img_brightness", "=", "0", ",", "img_contrast", "=", "0", ",", "img_saturation", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Perfrom a color jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "\n", "jitter", "=", "[", "]", "\n", "if", "img_brightness", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"brightness\"", ")", "\n", "", "if", "img_contrast", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"contrast\"", ")", "\n", "", "if", "img_saturation", "!=", "0", ":", "\n", "        ", "jitter", ".", "append", "(", "\"saturation\"", ")", "\n", "\n", "", "if", "len", "(", "jitter", ")", ">", "0", ":", "\n", "        ", "order", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "len", "(", "jitter", ")", ")", ")", "\n", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "jitter", ")", ")", ":", "\n", "            ", "if", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"brightness\"", ":", "\n", "                ", "images", "=", "brightness_jitter", "(", "img_brightness", ",", "images", ")", "\n", "", "elif", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"contrast\"", ":", "\n", "                ", "images", "=", "contrast_jitter", "(", "img_contrast", ",", "images", ")", "\n", "", "elif", "jitter", "[", "order", "[", "idx", "]", "]", "==", "\"saturation\"", ":", "\n", "                ", "images", "=", "saturation_jitter", "(", "img_saturation", ",", "images", ")", "\n", "", "", "", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.brightness_jitter": [[292, 309], ["torch.zeros", "transform.blend", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "brightness_jitter", "(", "var", ",", "images", ")", ":", "\n", "    ", "\"\"\"\n    Perfrom brightness jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for brightness.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "\n", "img_bright", "=", "torch", ".", "zeros", "(", "images", ".", "shape", ")", "\n", "images", "=", "blend", "(", "images", ",", "img_bright", ",", "alpha", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.contrast_jitter": [[311, 329], ["transform.grayscale", "torch.mean", "transform.blend", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "contrast_jitter", "(", "var", ",", "images", ")", ":", "\n", "    ", "\"\"\"\n    Perfrom contrast jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for contrast.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "\n", "img_gray", "=", "grayscale", "(", "images", ")", "\n", "img_gray", "[", ":", "]", "=", "torch", ".", "mean", "(", "img_gray", ",", "dim", "=", "(", "1", ",", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "images", "=", "blend", "(", "images", ",", "img_gray", ",", "alpha", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.saturation_jitter": [[331, 348], ["transform.grayscale", "transform.blend", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.grayscale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.blend"], ["", "def", "saturation_jitter", "(", "var", ",", "images", ")", ":", "\n", "    ", "\"\"\"\n    Perfrom saturation jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for saturation.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "alpha", "=", "1.0", "+", "np", ".", "random", ".", "uniform", "(", "-", "var", ",", "var", ")", "\n", "img_gray", "=", "grayscale", "(", "images", ")", "\n", "images", "=", "blend", "(", "images", ",", "img_gray", ",", "alpha", ")", "\n", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.lighting_jitter": [[350, 378], ["numpy.random.normal", "numpy.array", "numpy.reshape", "numpy.sum", "torch.zeros_like", "range", "numpy.repeat", "numpy.repeat"], "function", ["None"], ["", "def", "lighting_jitter", "(", "images", ",", "alphastd", ",", "eigval", ",", "eigvec", ")", ":", "\n", "    ", "\"\"\"\n    Perform AlexNet-style PCA jitter on the given images.\n    Args:\n        images (tensor): images to perform lighting jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:\n        out_images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "if", "alphastd", "==", "0", ":", "\n", "        ", "return", "images", "\n", "# generate alpha1, alpha2, alpha3.", "\n", "", "alpha", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "alphastd", ",", "size", "=", "(", "1", ",", "3", ")", ")", "\n", "eig_vec", "=", "np", ".", "array", "(", "eigvec", ")", "\n", "eig_val", "=", "np", ".", "reshape", "(", "eigval", ",", "(", "1", ",", "3", ")", ")", "\n", "rgb", "=", "np", ".", "sum", "(", "\n", "eig_vec", "*", "np", ".", "repeat", "(", "alpha", ",", "3", ",", "axis", "=", "0", ")", "*", "np", ".", "repeat", "(", "eig_val", ",", "3", ",", "axis", "=", "0", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "out_images", "=", "torch", ".", "zeros_like", "(", "images", ")", "\n", "for", "idx", "in", "range", "(", "images", ".", "shape", "[", "1", "]", ")", ":", "\n", "        ", "out_images", "[", ":", ",", "idx", "]", "=", "images", "[", ":", ",", "idx", "]", "+", "rgb", "[", "2", "-", "idx", "]", "\n", "\n", "", "return", "out_images", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.color_normalization": [[380, 403], ["torch.zeros_like", "range", "len", "len", "len"], "function", ["None"], ["", "def", "color_normalization", "(", "images", ",", "mean", ",", "stddev", ")", ":", "\n", "    ", "\"\"\"\n    Perform color nomration on the given images.\n    Args:\n        images (tensor): images to perform color normalization. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        mean (list): mean values for normalization.\n        stddev (list): standard deviations for normalization.\n\n    Returns:\n        out_images (tensor): the noramlized images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"", "\n", "assert", "len", "(", "mean", ")", "==", "images", ".", "shape", "[", "1", "]", ",", "\"channel mean not computed properly\"", "\n", "assert", "(", "\n", "len", "(", "stddev", ")", "==", "images", ".", "shape", "[", "1", "]", "\n", ")", ",", "\"channel stddev not computed properly\"", "\n", "\n", "out_images", "=", "torch", ".", "zeros_like", "(", "images", ")", "\n", "for", "idx", "in", "range", "(", "len", "(", "mean", ")", ")", ":", "\n", "        ", "out_images", "[", ":", ",", "idx", "]", "=", "(", "images", "[", ":", ",", "idx", "]", "-", "mean", "[", "idx", "]", ")", "/", "stddev", "[", "idx", "]", "\n", "\n", "", "return", "out_images", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.retry_load_images": [[18, 47], ["range", "all", "torch.as_tensor.append", "logger.warn", "time.sleep", "Exception", "fvcore.common.file_io.PathManager.open", "numpy.frombuffer", "cv2.imdecode", "torch.as_tensor", "f.read", "numpy.stack"], "function", ["None"], ["\n", "if", "isinstance", "(", "preds", ",", "list", ")", ":", "\n", "        ", "preds", "=", "torch", ".", "cat", "(", "preds", ",", "dim", "=", "0", ")", "\n", "", "if", "isinstance", "(", "labels", ",", "list", ")", ":", "\n", "        ", "labels", "=", "torch", ".", "cat", "(", "labels", ",", "dim", "=", "0", ")", "\n", "# If labels are one-hot encoded, get their indices.", "\n", "", "if", "labels", ".", "ndim", "==", "preds", ".", "ndim", ":", "\n", "        ", "labels", "=", "torch", ".", "argmax", "(", "labels", ",", "dim", "=", "-", "1", ")", "\n", "# Get the predicted class indices for examples.", "\n", "", "preds", "=", "torch", ".", "flatten", "(", "torch", ".", "argmax", "(", "preds", ",", "dim", "=", "-", "1", ")", ")", "\n", "labels", "=", "torch", ".", "flatten", "(", "labels", ")", "\n", "cmtx", "=", "confusion_matrix", "(", "\n", "labels", ",", "preds", ",", "labels", "=", "list", "(", "range", "(", "num_classes", ")", ")", ",", "normalize", "=", "normalize", "\n", ")", "\n", "return", "cmtx", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.get_sequence": [[49, 70], ["list", "range", "range", "len"], "function", ["None"], ["    ", "\"\"\"\n    A function to create a colored and labeled confusion matrix matplotlib figure\n    given true labels and preds.\n    Args:\n        cmtx (ndarray): confusion matrix.\n        num_classes (int): total number of classes.\n        class_names (Optional[list of strs]): a list of class names.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].\n\n    Returns:\n        img (figure): matplotlib figure.\n    \"\"\"", "\n", "if", "class_names", "is", "None", "or", "type", "(", "class_names", ")", "!=", "list", ":", "\n", "        ", "class_names", "=", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_classes", ")", "]", "\n", "\n", "", "figure", "=", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "imshow", "(", "cmtx", ",", "interpolation", "=", "\"nearest\"", ",", "cmap", "=", "plt", ".", "cm", ".", "Blues", ")", "\n", "plt", ".", "title", "(", "\"Confusion matrix\"", ")", "\n", "plt", ".", "colorbar", "(", ")", "\n", "tick_marks", "=", "np", ".", "arange", "(", "len", "(", "class_names", ")", ")", "\n", "plt", ".", "xticks", "(", "tick_marks", ",", "class_names", ",", "rotation", "=", "45", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.pack_pathway_output": [[72, 106], ["torch.index_select", "NotImplementedError", "torch.linspace().long", "torch.linspace"], "function", ["None"], ["\n", "# Use white text if squares are dark; otherwise black.", "\n", "threshold", "=", "cmtx", ".", "max", "(", ")", "/", "2.0", "\n", "for", "i", ",", "j", "in", "itertools", ".", "product", "(", "range", "(", "cmtx", ".", "shape", "[", "0", "]", ")", ",", "range", "(", "cmtx", ".", "shape", "[", "1", "]", ")", ")", ":", "\n", "        ", "color", "=", "\"white\"", "if", "cmtx", "[", "i", ",", "j", "]", ">", "threshold", "else", "\"black\"", "\n", "plt", ".", "text", "(", "\n", "j", ",", "\n", "i", ",", "\n", "format", "(", "cmtx", "[", "i", ",", "j", "]", ",", "\".2f\"", ")", "if", "cmtx", "[", "i", ",", "j", "]", "!=", "0", "else", "\".\"", ",", "\n", "horizontalalignment", "=", "\"center\"", ",", "\n", "color", "=", "color", ",", "\n", ")", "\n", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "ylabel", "(", "\"True label\"", ")", "\n", "plt", ".", "xlabel", "(", "\"Predicted label\"", ")", "\n", "\n", "return", "figure", "\n", "\n", "\n", "", "def", "plot_topk_histogram", "(", "tag", ",", "array", ",", "k", "=", "10", ",", "class_names", "=", "None", ",", "figsize", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Plot histogram of top-k value from the given array.\n    Args:\n        tag (str): histogram title.\n        array (tensor): a tensor to draw top k value from.\n        k (int): number of top values to draw from array.\n            Defaut to 10.\n        class_names (list of strings, optional):\n            a list of names for values in array.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].\n    Returns:\n        fig (matplotlib figure): a matplotlib figure of the histogram.\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.spatial_sampling": [[108, 160], ["transform.random_short_side_scale_jitter", "transform.random_crop", "transform.random_short_side_scale_jitter", "transform.uniform_crop", "transform.horizontal_flip", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_short_side_scale_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_crop", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_short_side_scale_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.uniform_crop", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.horizontal_flip"], ["\n", "fig", "=", "plt", ".", "Figure", "(", "figsize", "=", "figsize", ",", "facecolor", "=", "\"w\"", ",", "edgecolor", "=", "\"k\"", ")", "\n", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "\n", "if", "class_names", "is", "None", ":", "\n", "        ", "class_names", "=", "[", "str", "(", "i", ")", "for", "i", "in", "ind", "]", "\n", "", "else", ":", "\n", "        ", "class_names", "=", "[", "class_names", "[", "i", "]", "for", "i", "in", "ind", "]", "\n", "\n", "", "tick_marks", "=", "np", ".", "arange", "(", "k", ")", "\n", "width", "=", "0.75", "\n", "ax", ".", "bar", "(", "\n", "tick_marks", ",", "\n", "val", ",", "\n", "width", ",", "\n", "color", "=", "\"orange\"", ",", "\n", "tick_label", "=", "class_names", ",", "\n", "edgecolor", "=", "\"w\"", ",", "\n", "linewidth", "=", "1", ",", "\n", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"Candidates\"", ")", "\n", "ax", ".", "set_xticks", "(", "tick_marks", ")", "\n", "ax", ".", "set_xticklabels", "(", "class_names", ",", "rotation", "=", "-", "45", ",", "ha", "=", "\"center\"", ")", "\n", "ax", ".", "xaxis", ".", "set_label_position", "(", "\"bottom\"", ")", "\n", "ax", ".", "xaxis", ".", "tick_bottom", "(", ")", "\n", "\n", "y_tick", "=", "np", ".", "linspace", "(", "0", ",", "1", ",", "num", "=", "10", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Frequency\"", ")", "\n", "ax", ".", "set_yticks", "(", "y_tick", ")", "\n", "y_labels", "=", "[", "format", "(", "i", ",", "\".1f\"", ")", "for", "i", "in", "y_tick", "]", "\n", "ax", ".", "set_yticklabels", "(", "y_labels", ",", "ha", "=", "\"center\"", ")", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "val", ".", "numpy", "(", ")", ")", ":", "\n", "        ", "ax", ".", "text", "(", "\n", "i", "-", "0.1", ",", "\n", "v", "+", "0.03", ",", "\n", "format", "(", "v", ",", "\".2f\"", ")", ",", "\n", "color", "=", "\"orange\"", ",", "\n", "fontweight", "=", "\"bold\"", ",", "\n", ")", "\n", "\n", "", "ax", ".", "set_title", "(", "tag", ")", "\n", "\n", "fig", ".", "set_tight_layout", "(", "True", ")", "\n", "\n", "return", "fig", "\n", "\n", "\n", "", "class", "GetWeightAndActivation", ":", "\n", "    "]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.as_binary_vector": [[162, 176], ["numpy.zeros", "set"], "function", ["None"], ["\n", "def", "__init__", "(", "self", ",", "model", ",", "layers", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model (nn.Module): the model containing layers to obtain weights and activations from.\n            layers (list of strings): a list of layer names to obtain weights and activations from.\n                Names are hierarchical, separated by /. For example, If a layer follow a path\n                \"s1\" ---> \"pathway0_stem\" ---> \"conv\", the layer path is \"s1/pathway0_stem/conv\".\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "hooks", "=", "{", "}", "\n", "self", ".", "layers_names", "=", "layers", "\n", "# eval mode", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "_register_hooks", "(", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.aggregate_labels": [[178, 191], ["list", "set", "all_labels.append"], "function", ["None"], ["", "def", "_get_layer", "(", "self", ",", "layer_name", ")", ":", "\n", "        ", "\"\"\"\n        Return a layer (nn.Module Object) given a hierarchical layer name, separated by /.\n        Args:\n            layer_name (str): the name of the layer.\n        \"\"\"", "\n", "layer_ls", "=", "layer_name", ".", "split", "(", "\"/\"", ")", "\n", "prev_module", "=", "self", ".", "model", "\n", "for", "layer", "in", "layer_ls", ":", "\n", "            ", "prev_module", "=", "prev_module", ".", "_modules", "[", "layer", "]", "\n", "\n", "", "return", "prev_module", "\n", "\n", "", "def", "_register_single_hook", "(", "self", ",", "layer_name", ")", ":", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.convert_to_video_level_labels": [[193, 207], ["range", "len", "utils.aggregate_labels", "range", "len"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.aggregate_labels"], ["\n", "\n", "def", "hook_fn", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "            ", "self", ".", "hooks", "[", "layer_name", "]", "=", "output", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n", "", "layer", "=", "self", ".", "_get_layer", "(", "layer_name", ")", "\n", "layer", ".", "register_forward_hook", "(", "hook_fn", ")", "\n", "\n", "", "def", "_register_hooks", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Register hooks to layers in `self.layers_names`.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists": [[209, 252], ["collections.defaultdict", "collections.defaultdict", "fvcore.common.file_io.PathManager.open", "f.readline().startswith", "collections.defaultdict.keys", "dict", "dict", "line.split", "image_paths[].append", "row[].replace", "f.readline", "len", "os.path.join", "labels[].append", "labels[].append", "int", "row[].replace.split"], "function", ["None"], ["            ", "self", ".", "_register_single_hook", "(", "layer_name", ")", "\n", "\n", "", "", "def", "get_activations", "(", "self", ",", "input", ",", "bboxes", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Obtain all activations from layers that we register hooks for.\n        Args:\n            input (tensors, list of tensors): the model input.\n            bboxes (Optional): Bouding boxes data that might be required\n                by the model.\n        Returns:\n            activation_dict (Python dictionary): a dictionary of the pair\n                {layer_name: list of activations}, where activations are outputs returned\n                by the layer.\n        \"\"\"", "\n", "input_clone", "=", "[", "inp", ".", "clone", "(", ")", "for", "inp", "in", "input", "]", "\n", "if", "bboxes", "is", "not", "None", ":", "\n", "            ", "preds", "=", "self", ".", "model", "(", "input_clone", ",", "bboxes", ")", "\n", "", "else", ":", "\n", "            ", "preds", "=", "self", ".", "model", "(", "input_clone", ")", "\n", "\n", "", "activation_dict", "=", "{", "}", "\n", "for", "layer_name", ",", "hook", "in", "self", ".", "hooks", ".", "items", "(", ")", ":", "\n", "# list of activations for each instance.", "\n", "            ", "activation_dict", "[", "layer_name", "]", "=", "hook", "\n", "\n", "", "return", "activation_dict", ",", "preds", "\n", "\n", "", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns weights from registered layers.\n        Returns:\n            weights (Python dictionary): a dictionary of the pair\n            {layer_name: weight}, where weight is the weight tensor.\n        \"\"\"", "\n", "weights", "=", "{", "}", "\n", "for", "layer", "in", "self", ".", "layers_names", ":", "\n", "            ", "cur_layer", "=", "self", ".", "_get_layer", "(", "layer", ")", "\n", "if", "hasattr", "(", "cur_layer", ",", "\"weight\"", ")", ":", "\n", "                ", "weights", "[", "layer", "]", "=", "cur_layer", ".", "weight", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Layer {} does not have weight attribute.\"", ".", "format", "(", "layer", ")", "\n", ")", "\n", "", "", "return", "weights", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.tensor_normalize": [[254, 272], ["tensor.float.float", "type", "torch.tensor", "type", "torch.tensor"], "function", ["None"], ["\n", "", "", "def", "get_indexing", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Parse numpy-like fancy indexing from a string.\n    Args:\n        string (str): string represent the indices to take\n            a subset of from array. Indices for each dimension\n            are separated by `,`; indices for different dimensions\n            are separated by `;`.\n            e.g.: For a numpy array `arr` of shape (3,3,3), the string \"1,2;1,2\"\n            means taking the sub-array `arr[[1,2], [1,2]]\n    Returns:\n        final_indexing (tuple): the parsed indexing.\n    \"\"\"", "\n", "index_ls", "=", "string", ".", "strip", "(", ")", ".", "split", "(", "\";\"", ")", "\n", "final_indexing", "=", "[", "]", "\n", "for", "index", "in", "index_ls", ":", "\n", "        ", "index_single_dim", "=", "index", ".", "split", "(", "\",\"", ")", "\n", "index_single_dim", "=", "[", "int", "(", "i", ")", "for", "i", "in", "index_single_dim", "]", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.get_random_sampling_rate": [[274, 284], ["random.randint"], "function", ["None"], ["\n", "", "return", "tuple", "(", "final_indexing", ")", "\n", "\n", "\n", "", "def", "process_layer_index_data", "(", "layer_ls", ",", "layer_name_prefix", "=", "\"\"", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.revert_tensor_normalize": [[286, 301], ["type", "torch.tensor", "type", "torch.tensor"], "function", ["None"], ["\n", "\n", "layer_name", ",", "indexing_dict", "=", "[", "]", ",", "{", "}", "\n", "for", "layer", "in", "layer_ls", ":", "\n", "        ", "ls", "=", "layer", ".", "split", "(", ")", "\n", "name", "=", "layer_name_prefix", "+", "ls", "[", "0", "]", "\n", "layer_name", ".", "append", "(", "name", ")", "\n", "if", "len", "(", "ls", ")", "==", "2", ":", "\n", "            ", "indexing_dict", "[", "name", "]", "=", "get_indexing", "(", "ls", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "indexing_dict", "[", "name", "]", "=", "(", ")", "\n", "", "", "return", "layer_name", ",", "indexing_dict", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.SequentialDistributedSampler.__init__": [[32, 68], ["torch.utils.data.distributed.DistributedSampler.__init__", "dict", "range", "list", "loader.SequentialDistributedSampler.shot_keys.sort", "len", "logger.info", "dataset.num_videos", "dataset.get_idx_sequence_from_video", "loader.SequentialDistributedSampler.shot_to_clip.update", "loader.SequentialDistributedSampler.shot_to_clip.keys", "int", "math.ceil", "len", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.num_videos", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.get_idx_sequence_from_video"], ["def", "__init__", "(", "self", ",", "cfg", ",", "train", ",", "dataset", ")", ":", "\n", "        ", "super", "(", "SequentialDistributedSampler", ",", "self", ")", ".", "__init__", "(", "dataset", ")", "\n", "# make video samplers divisible", "\n", "\n", "if", "train", ":", "\n", "            ", "batch_size", "=", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", "//", "self", ".", "num_replicas", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "cfg", ".", "TEST", ".", "BATCH_SIZE", "//", "self", ".", "num_replicas", "\n", "", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "# reorganize the clip index, so that each shot can be", "\n", "# indexed via a specific clip id", "\n", "self", ".", "shot_to_clip", "=", "dict", "(", ")", "\n", "for", "vid", "in", "range", "(", "dataset", ".", "num_videos", "(", ")", ")", ":", "\n", "            ", "indexes", "=", "dataset", ".", "get_idx_sequence_from_video", "(", "vid", ")", "\n", "if", "len", "(", "indexes", ")", "%", "self", ".", "batch_size", ">", "0", ":", "\n", "                ", "left", "=", "self", ".", "batch_size", "-", "len", "(", "indexes", ")", "%", "self", ".", "batch_size", "\n", "indexes", "+=", "indexes", "[", "-", "left", ":", "]", "\n", "\n", "", "self", ".", "shot_to_clip", ".", "update", "(", "\n", "{", "\n", "indexes", "[", "idx", "]", ":", "indexes", "[", "idx", ":", "idx", "+", "self", ".", "batch_size", "]", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "indexes", ")", ",", "self", ".", "batch_size", ")", "\n", "}", "\n", ")", "\n", "\n", "", "self", ".", "shot_keys", "=", "list", "(", "self", ".", "shot_to_clip", ".", "keys", "(", ")", ")", "\n", "self", ".", "shot_keys", ".", "sort", "(", ")", "\n", "self", ".", "num_shot", "=", "len", "(", "self", ".", "shot_to_clip", ")", "\n", "self", ".", "total_shot", "=", "int", "(", "math", ".", "ceil", "(", "self", ".", "num_shot", "*", "1.0", "/", "self", ".", "num_replicas", ")", ")", "*", "self", ".", "num_replicas", "\n", "self", ".", "num_clips", "=", "self", ".", "total_shot", "*", "self", ".", "batch_size", "\n", "self", ".", "num_samples", "=", "self", ".", "num_clips", "//", "self", ".", "num_replicas", "\n", "\n", "self", ".", "seed", "=", "0", "\n", "logger", ".", "info", "(", "'{} shots, {} clips'", ".", "format", "(", "self", ".", "num_shot", ",", "self", ".", "num_clips", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.SequentialDistributedSampler.__iter__": [[69, 99], ["logger.info", "iter", "torch.Generator", "torch.Generator.manual_seed", "torch.randperm().tolist", "functools.reduce", "functools.reduce", "functools.reduce", "functools.reduce", "list", "len", "len", "range", "len", "len", "torch.randperm", "len", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "shuffle", ":", "\n", "# deterministically shuffle based on epoch and seed", "\n", "            ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "seed", "+", "self", ".", "epoch", ")", "\n", "random_indices", "=", "torch", ".", "randperm", "(", "len", "(", "self", ".", "shot_keys", ")", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "\n", "video_indices", "=", "[", "self", ".", "shot_keys", "[", "idx", "]", "for", "idx", "in", "random_indices", "]", "\n", "video_indices", "+=", "video_indices", "[", ":", "(", "self", ".", "total_shot", "-", "self", ".", "num_shot", ")", "]", "\n", "shot_indices", "=", "video_indices", "[", "self", ".", "rank", ":", "self", ".", "total_shot", ":", "self", ".", "num_replicas", "]", "\n", "\n", "indices", "=", "functools", ".", "reduce", "(", "\n", "lambda", "x", ",", "y", ":", "x", "+", "y", ",", "\n", "[", "self", ".", "shot_to_clip", "[", "s", "]", "for", "s", "in", "shot_indices", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# add extra samples to make it evenly divisible", "\n", "indices", "+=", "indices", "[", ":", "(", "self", ".", "total_size", "-", "len", "(", "indices", ")", ")", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "\n", "", "logger", ".", "info", "(", "'num samples {} {}'", ".", "format", "(", "len", "(", "indices", ")", ",", "self", ".", "num_samples", ")", ")", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", ",", "\"{} vs. {}\"", ".", "format", "(", "len", "(", "indices", ")", ",", "self", ".", "num_samples", ")", "\n", "\n", "return", "iter", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.detection_collate": [[100, 157], ["zip", "torch.tensor().float", "list", "list", "extra_data[].keys", "torch.utils.data._utils.collate.default_collate", "torch.utils.data._utils.collate.default_collate", "torch.tensor().float", "torch.tensor", "torch.Tensor().bool", "numpy.concatenate", "torch.tensor().float", "torch.tensor", "new_labels.append", "new_labels.append", "numpy.concatenate", "numpy.concatenate", "torch.tensor().view", "torch.utils.data._utils.collate.default_collate", "numpy.concatenate", "numpy.max", "torch.Tensor", "range", "torch.tensor", "numpy.full", "len", "torch.tensor", "float", "list", "itertools.chain"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate"], ["", "", "def", "detection_collate", "(", "batch", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Collate function for detection task. Concatanate bboxes, labels and\n    metadata from different samples in the first dimension instead of\n    stacking them to have a batch-size dimension.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated detection data batch.\n    \"\"\"", "\n", "inputs", ",", "labels", ",", "video_idx", ",", "extra_data", ",", "feat_banks", ",", "bank_times", "=", "zip", "(", "*", "batch", ")", "\n", "inputs", ",", "video_idx", "=", "default_collate", "(", "inputs", ")", ",", "default_collate", "(", "video_idx", ")", "\n", "\n", "collated_extra_data", "=", "{", "\n", "\"raw_labels\"", ":", "torch", ".", "tensor", "(", "np", ".", "concatenate", "(", "labels", ",", "axis", "=", "0", ")", ")", ".", "float", "(", ")", "\n", "}", "\n", "\n", "if", "not", "cfg", ".", "AVA", ".", "GATHER_BANK", ":", "\n", "        ", "new_labels", "=", "[", "]", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "if", "label", ".", "shape", "[", "0", "]", ">", "0", ":", "\n", "                ", "new_labels", ".", "append", "(", "np", ".", "max", "(", "label", ",", "axis", "=", "0", ",", "keepdims", "=", "True", ")", ")", "\n", "", "else", ":", "\n", "                ", "new_labels", ".", "append", "(", "label", ")", "\n", "\n", "", "", "labels", "=", "new_labels", "\n", "\n", "", "labels", "=", "torch", ".", "tensor", "(", "np", ".", "concatenate", "(", "labels", ",", "axis", "=", "0", ")", ")", ".", "float", "(", ")", "\n", "\n", "feat_banks", "=", "list", "(", "feat_banks", ")", "\n", "bank_times", "=", "list", "(", "bank_times", ")", "\n", "\n", "for", "key", "in", "extra_data", "[", "0", "]", ".", "keys", "(", ")", ":", "\n", "        ", "data", "=", "[", "d", "[", "key", "]", "for", "d", "in", "extra_data", "]", "\n", "\n", "if", "key", "==", "\"boxes\"", ":", "\n", "            ", "has_box", "=", "[", "d", ".", "shape", "[", "0", "]", ">", "0", "for", "d", "in", "data", "]", "\n", "collated_extra_data", "[", "\"has_box\"", "]", "=", "torch", ".", "Tensor", "(", "has_box", ")", ".", "bool", "(", ")", "\n", "\n", "", "if", "key", "==", "\"boxes\"", "or", "key", "==", "\"ori_boxes\"", ":", "\n", "# Append idx info to the bboxes before concatenating them.", "\n", "            ", "bboxes", "=", "[", "\n", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "full", "(", "(", "data", "[", "i", "]", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "float", "(", "i", ")", ")", ",", "data", "[", "i", "]", "]", ",", "axis", "=", "1", "\n", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", "\n", "]", "\n", "bboxes", "=", "np", ".", "concatenate", "(", "bboxes", ",", "axis", "=", "0", ")", "\n", "collated_extra_data", "[", "key", "]", "=", "torch", ".", "tensor", "(", "bboxes", ")", ".", "float", "(", ")", "\n", "", "elif", "key", "==", "\"metadata\"", ":", "\n", "            ", "collated_extra_data", "[", "key", "]", "=", "torch", ".", "tensor", "(", "\n", "list", "(", "itertools", ".", "chain", "(", "*", "data", ")", ")", "\n", ")", ".", "view", "(", "-", "1", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "collated_extra_data", "[", "key", "]", "=", "default_collate", "(", "data", ")", "\n", "\n", "", "", "return", "inputs", ",", "labels", ",", "video_idx", ",", "collated_extra_data", ",", "feat_banks", ",", "bank_times", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.construct_loader": [[158, 223], ["build.build_dataset", "int", "slowfast.datasets.multigrid_helper.ShortCycleBatchSampler", "torch.utils.data.DataLoader", "functools.partial", "functools.partial", "torch.utils.data.DataLoader", "int", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.sampler.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "max", "int", "max", "max"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.build.build_dataset"], ["", "def", "construct_loader", "(", "cfg", ",", "split", ",", "is_precise_bn", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Constructs the data loader for the given dataset.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        split (str): the split of the data loader. Options include `train`,\n            `val`, and `test`.\n    \"\"\"", "\n", "assert", "split", "in", "[", "\"train\"", ",", "\"val\"", ",", "\"test\"", "]", "\n", "if", "split", "in", "[", "\"train\"", "]", ":", "\n", "        ", "dataset_name", "=", "cfg", ".", "TRAIN", ".", "DATASET", "\n", "batch_size", "=", "int", "(", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", "/", "max", "(", "1", ",", "cfg", ".", "NUM_GPUS", ")", ")", "\n", "shuffle", "=", "True", "\n", "drop_last", "=", "True", "\n", "", "elif", "split", "in", "[", "\"val\"", "]", ":", "\n", "        ", "dataset_name", "=", "cfg", ".", "TRAIN", ".", "DATASET", "\n", "batch_size", "=", "int", "(", "cfg", ".", "TEST", ".", "BATCH_SIZE", "/", "max", "(", "1", ",", "cfg", ".", "NUM_GPUS", ")", ")", "\n", "shuffle", "=", "False", "\n", "drop_last", "=", "False", "\n", "", "elif", "split", "in", "[", "\"test\"", "]", ":", "\n", "        ", "dataset_name", "=", "cfg", ".", "TEST", ".", "DATASET", "\n", "batch_size", "=", "int", "(", "cfg", ".", "TEST", ".", "BATCH_SIZE", "/", "max", "(", "1", ",", "cfg", ".", "NUM_GPUS", ")", ")", "\n", "shuffle", "=", "False", "\n", "drop_last", "=", "False", "\n", "\n", "# Construct the dataset", "\n", "", "dataset", "=", "build_dataset", "(", "dataset_name", ",", "cfg", ",", "split", ")", "\n", "\n", "if", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE", "and", "split", "in", "[", "\"train\"", "]", "and", "not", "is_precise_bn", ":", "\n", "# Create a sampler for multi-process training", "\n", "        ", "sampler", "=", "(", "\n", "DistributedSampler", "(", "dataset", ")", "\n", "if", "cfg", ".", "NUM_GPUS", ">", "1", "\n", "else", "RandomSampler", "(", "dataset", ")", "\n", ")", "\n", "batch_sampler", "=", "ShortCycleBatchSampler", "(", "\n", "sampler", ",", "batch_size", "=", "batch_size", ",", "drop_last", "=", "drop_last", ",", "cfg", "=", "cfg", "\n", ")", "\n", "# Create a loader", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "num_workers", "=", "cfg", ".", "DATA_LOADER", ".", "NUM_WORKERS", ",", "\n", "pin_memory", "=", "cfg", ".", "DATA_LOADER", ".", "PIN_MEMORY", ",", "\n", ")", "\n", "", "else", ":", "\n", "# Create a sampler for multi-process training", "\n", "        ", "if", "cfg", ".", "NUM_GPUS", ">", "1", ":", "\n", "            ", "sampler", "=", "DistributedSampler", "(", "dataset", ",", "shuffle", "=", "shuffle", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "None", "\n", "# Create a loader", "\n", "", "collate_fn", "=", "functools", ".", "partial", "(", "detection_collate", ",", "cfg", "=", "cfg", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "(", "False", "if", "sampler", "else", "shuffle", ")", ",", "\n", "sampler", "=", "sampler", ",", "\n", "num_workers", "=", "cfg", ".", "DATA_LOADER", ".", "NUM_WORKERS", ",", "\n", "pin_memory", "=", "cfg", ".", "DATA_LOADER", ".", "PIN_MEMORY", ",", "\n", "drop_last", "=", "drop_last", ",", "\n", "collate_fn", "=", "collate_fn", "if", "cfg", ".", "DETECTION", ".", "ENABLE", "else", "None", ",", "\n", ")", "\n", "", "return", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.loader.shuffle_dataset": [[225, 244], ["isinstance", "isinstance", "type", "isinstance", "issubclass", "sampler.set_epoch"], "function", ["None"], ["", "def", "shuffle_dataset", "(", "loader", ",", "cur_epoch", ")", ":", "\n", "    ", "\"\"\"\"\n    Shuffles the data.\n    Args:\n        loader (loader): data loader to perform shuffle.\n        cur_epoch (int): number of the current epoch.\n    \"\"\"", "\n", "sampler", "=", "(", "\n", "loader", ".", "batch_sampler", ".", "sampler", "\n", "if", "isinstance", "(", "loader", ".", "batch_sampler", ",", "ShortCycleBatchSampler", ")", "\n", "else", "loader", ".", "sampler", "\n", ")", "\n", "assert", "isinstance", "(", "\n", "sampler", ",", "(", "RandomSampler", ",", "DistributedSampler", ",", "SequentialDistributedSampler", ")", "\n", ")", ",", "\"Sampler type '{}' not supported\"", ".", "format", "(", "type", "(", "sampler", ")", ")", "\n", "# RandomSampler handles shuffling automatically", "\n", "if", "isinstance", "(", "sampler", ",", "DistributedSampler", ")", "or", "issubclass", "(", "sampler", ",", "DistributedSampler", ")", ":", "\n", "# DistributedSampler shuffles data based on epoch", "\n", "        ", "sampler", ".", "set_epoch", "(", "cur_epoch", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.__init__": [[24, 50], ["ava_dataset.Ava._load_data"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava._load_data"], ["def", "__init__", "(", "self", ",", "cfg", ",", "split", ")", ":", "\n", "        ", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "_split", "=", "split", "\n", "self", ".", "_sample_rate", "=", "cfg", ".", "DATA", ".", "SAMPLING_RATE", "\n", "self", ".", "_video_length", "=", "cfg", ".", "DATA", ".", "NUM_FRAMES", "\n", "self", ".", "_seq_len", "=", "self", ".", "_video_length", "*", "self", ".", "_sample_rate", "\n", "self", ".", "_num_classes", "=", "cfg", ".", "MODEL", ".", "NUM_CLASSES", "\n", "# Augmentation params.", "\n", "self", ".", "_data_mean", "=", "cfg", ".", "DATA", ".", "MEAN", "\n", "self", ".", "_data_std", "=", "cfg", ".", "DATA", ".", "STD", "\n", "self", ".", "_use_bgr", "=", "cfg", ".", "AVA", ".", "BGR", "\n", "self", ".", "random_horizontal_flip", "=", "cfg", ".", "DATA", ".", "RANDOM_FLIP", "\n", "if", "self", ".", "_split", "==", "\"train\"", "and", "not", "cfg", ".", "AVA", ".", "FEATURE_EXTRACTION", ":", "\n", "            ", "self", ".", "_crop_size", "=", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", "\n", "self", ".", "_jitter_min_scale", "=", "cfg", ".", "DATA", ".", "TRAIN_JITTER_SCALES", "[", "0", "]", "\n", "self", ".", "_jitter_max_scale", "=", "cfg", ".", "DATA", ".", "TRAIN_JITTER_SCALES", "[", "1", "]", "\n", "self", ".", "_use_color_augmentation", "=", "cfg", ".", "AVA", ".", "TRAIN_USE_COLOR_AUGMENTATION", "\n", "self", ".", "_pca_jitter_only", "=", "cfg", ".", "AVA", ".", "TRAIN_PCA_JITTER_ONLY", "\n", "self", ".", "_pca_eigval", "=", "cfg", ".", "AVA", ".", "TRAIN_PCA_EIGVAL", "\n", "self", ".", "_pca_eigvec", "=", "cfg", ".", "AVA", ".", "TRAIN_PCA_EIGVEC", "\n", "", "else", ":", "\n", "            ", "self", ".", "_crop_size", "=", "cfg", ".", "DATA", ".", "TEST_CROP_SIZE", "\n", "self", ".", "_test_force_flip", "=", "cfg", ".", "AVA", ".", "TEST_FORCE_FLIP", "\n", "\n", "", "self", ".", "_load_data", "(", "cfg", ")", "\n", "self", ".", "db_handler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close": [[51, 54], ["isinstance", "ava_dataset.Ava.db_handler.close"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "db_handler", ",", "lmdb", ".", "Environment", ")", ":", "\n", "            ", "self", ".", "db_handler", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava._load_data": [[55, 96], ["ava_helper.load_image_lists", "ava_helper.load_boxes_and_labels", "ava_helper.get_keyframe_data", "ava_helper.get_num_boxes_used", "enumerate", "ava_dataset.Ava.print_summary", "len", "len", "ava_dataset.Ava.video_to_indexes[].append", "range", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.load_image_lists", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.load_boxes_and_labels", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.get_keyframe_data", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.get_num_boxes_used", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.print_summary"], ["", "", "def", "_load_data", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Load frame paths and annotations from files\n\n        Args:\n            cfg (CfgNode): config\n        \"\"\"", "\n", "# Loading frame paths.", "\n", "(", "\n", "self", ".", "_image_paths", ",", "\n", "self", ".", "_video_idx_to_name", ",", "\n", ")", "=", "ava_helper", ".", "load_image_lists", "(", "cfg", ",", "is_train", "=", "(", "self", ".", "_split", "==", "\"train\"", ")", ")", "\n", "\n", "# Loading annotations for boxes and labels.", "\n", "boxes_and_labels", "=", "ava_helper", ".", "load_boxes_and_labels", "(", "\n", "cfg", ",", "mode", "=", "self", ".", "_split", "\n", ")", "\n", "\n", "assert", "len", "(", "boxes_and_labels", ")", "==", "len", "(", "self", ".", "_image_paths", ")", "\n", "\n", "boxes_and_labels", "=", "[", "\n", "boxes_and_labels", "[", "self", ".", "_video_idx_to_name", "[", "i", "]", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_image_paths", ")", ")", "\n", "]", "\n", "\n", "# Get indices of keyframes and corresponding boxes and labels.", "\n", "(", "\n", "self", ".", "_keyframe_indices", ",", "\n", "self", ".", "_keyframe_boxes_and_labels", ",", "\n", ")", "=", "ava_helper", ".", "get_keyframe_data", "(", "boxes_and_labels", ")", "\n", "\n", "# Calculate the number of used boxes.", "\n", "self", ".", "_num_boxes_used", "=", "ava_helper", ".", "get_num_boxes_used", "(", "\n", "self", ".", "_keyframe_indices", ",", "self", ".", "_keyframe_boxes_and_labels", "\n", ")", "\n", "\n", "self", ".", "video_to_indexes", "=", "[", "[", "]", "for", "_", "in", "self", ".", "_image_paths", "]", "\n", "for", "idx", ",", "(", "video_idx", ",", "_", ",", "_", ",", "_", ")", "in", "enumerate", "(", "self", ".", "_keyframe_indices", ")", ":", "\n", "            ", "self", ".", "video_to_indexes", "[", "video_idx", "]", ".", "append", "(", "idx", ")", "\n", "\n", "", "self", ".", "print_summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.get_idx_sequence_from_video": [[97, 99], ["None"], "methods", ["None"], ["", "def", "get_idx_sequence_from_video", "(", "self", ",", "video_idx", ")", ":", "\n", "        ", "return", "self", ".", "video_to_indexes", "[", "video_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.num_videos": [[100, 102], ["len"], "methods", ["None"], ["", "def", "num_videos", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.print_summary": [[103, 113], ["logger.info", "logger.info", "logger.info", "sum", "logger.info", "logger.info", "logger.info", "len", "len", "len"], "methods", ["None"], ["", "def", "print_summary", "(", "self", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"=== AVA dataset summary ===\"", ")", "\n", "logger", ".", "info", "(", "\"Split: {}\"", ".", "format", "(", "self", ".", "_split", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of videos: {}\"", ".", "format", "(", "len", "(", "self", ".", "_image_paths", ")", ")", ")", "\n", "total_frames", "=", "sum", "(", "\n", "len", "(", "video_img_paths", ")", "for", "video_img_paths", "in", "self", ".", "_image_paths", "\n", ")", "\n", "logger", ".", "info", "(", "\"Number of frames: {}\"", ".", "format", "(", "total_frames", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of key frames: {}\"", ".", "format", "(", "len", "(", "self", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of boxes: {}.\"", ".", "format", "(", "self", ".", "_num_boxes_used", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.__len__": [[114, 116], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_keyframe_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava._images_and_boxes_preprocessing_cv2": [[117, 251], ["cv2_transform.clip_boxes_to_image", "numpy.concatenate", "numpy.ascontiguousarray", "torch.from_numpy", "cv2_transform.clip_boxes_to_image", "cv2_transform.random_short_side_scale_jitter_list", "cv2_transform.random_crop_list", "cv2_transform.HWC2CHW", "numpy.ascontiguousarray().astype", "cv2_transform.lighting_list", "cv2_transform.color_normalization", "cv2_transform.horizontal_flip_list", "cv2_transform.spatial_shift_crop_list", "cv2_transform.color_jitter_list", "numpy.array", "numpy.array", "numpy.expand_dims", "cv2_transform.scale", "cv2_transform.scale_boxes", "cv2_transform.horizontal_flip_list", "NotImplementedError", "numpy.ascontiguousarray", "numpy.array().astype", "numpy.array().astype", "cv2_transform.scale", "cv2_transform.scale_boxes", "cv2_transform.horizontal_flip_list", "img.reshape", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.clip_boxes_to_image", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_mask_list_ops.concatenate", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.clip_boxes_to_image", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_short_side_scale_jitter_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.random_crop_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.HWC2CHW", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.lighting_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.color_normalization", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.horizontal_flip_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.spatial_shift_crop_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.color_jitter_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.horizontal_flip_list", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.scale_boxes", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.cv2_transform.horizontal_flip_list"], ["", "def", "_images_and_boxes_preprocessing_cv2", "(", "self", ",", "imgs", ",", "boxes", ")", ":", "\n", "        ", "\"\"\"\n        This function performs preprocessing for the input images and\n        corresponding boxes for one clip with opencv as backend.\n\n        Args:\n            imgs (tensor): the images.\n            boxes (ndarray): the boxes for the current clip.\n\n        Returns:\n            imgs (tensor): list of preprocessed images.\n            boxes (ndarray): preprocessed boxes.\n        \"\"\"", "\n", "\n", "height", ",", "width", ",", "_", "=", "imgs", "[", "0", "]", ".", "shape", "\n", "\n", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "*=", "width", "\n", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "*=", "height", "\n", "boxes", "=", "cv2_transform", ".", "clip_boxes_to_image", "(", "boxes", ",", "height", ",", "width", ")", "\n", "\n", "# `transform.py` is list of np.array. However, for AVA, we only have", "\n", "# one np.array.", "\n", "boxes", "=", "[", "boxes", "]", "\n", "\n", "split_mode", "=", "\"test\"", "if", "self", ".", "cfg", ".", "AVA", ".", "FEATURE_EXTRACTION", "else", "self", ".", "_split", "\n", "\n", "# The image now is in HWC, BGR format.", "\n", "if", "split_mode", "==", "\"train\"", ":", "# \"train\"", "\n", "            ", "imgs", ",", "boxes", "=", "cv2_transform", ".", "random_short_side_scale_jitter_list", "(", "\n", "imgs", ",", "\n", "min_size", "=", "self", ".", "_jitter_min_scale", ",", "\n", "max_size", "=", "self", ".", "_jitter_max_scale", ",", "\n", "boxes", "=", "boxes", ",", "\n", ")", "\n", "imgs", ",", "boxes", "=", "cv2_transform", ".", "random_crop_list", "(", "\n", "imgs", ",", "self", ".", "_crop_size", ",", "order", "=", "\"HWC\"", ",", "boxes", "=", "boxes", "\n", ")", "\n", "\n", "if", "self", ".", "random_horizontal_flip", ":", "\n", "# random flip", "\n", "                ", "imgs", ",", "boxes", "=", "cv2_transform", ".", "horizontal_flip_list", "(", "\n", "0.5", ",", "imgs", ",", "order", "=", "\"HWC\"", ",", "boxes", "=", "boxes", "\n", ")", "\n", "", "", "elif", "split_mode", "==", "\"val\"", ":", "\n", "# Short side to test_scale. Non-local and STRG uses 256.", "\n", "            ", "imgs", "=", "[", "cv2_transform", ".", "scale", "(", "self", ".", "_crop_size", ",", "img", ")", "for", "img", "in", "imgs", "]", "\n", "boxes", "=", "[", "\n", "cv2_transform", ".", "scale_boxes", "(", "\n", "self", ".", "_crop_size", ",", "boxes", "[", "0", "]", ",", "height", ",", "width", "\n", ")", "\n", "]", "\n", "imgs", ",", "boxes", "=", "cv2_transform", ".", "spatial_shift_crop_list", "(", "\n", "self", ".", "_crop_size", ",", "imgs", ",", "1", ",", "boxes", "=", "boxes", "\n", ")", "\n", "\n", "if", "self", ".", "_test_force_flip", ":", "\n", "                ", "imgs", ",", "boxes", "=", "cv2_transform", ".", "horizontal_flip_list", "(", "\n", "1", ",", "imgs", ",", "order", "=", "\"HWC\"", ",", "boxes", "=", "boxes", "\n", ")", "\n", "", "", "elif", "split_mode", "==", "\"test\"", ":", "\n", "# Short side to test_scale. Non-local and STRG uses 256.", "\n", "            ", "imgs", "=", "[", "cv2_transform", ".", "scale", "(", "self", ".", "_crop_size", ",", "img", ")", "for", "img", "in", "imgs", "]", "\n", "boxes", "=", "[", "\n", "cv2_transform", ".", "scale_boxes", "(", "\n", "self", ".", "_crop_size", ",", "boxes", "[", "0", "]", ",", "height", ",", "width", "\n", ")", "\n", "]", "\n", "\n", "if", "self", ".", "_test_force_flip", ":", "\n", "                ", "imgs", ",", "boxes", "=", "cv2_transform", ".", "horizontal_flip_list", "(", "\n", "1", ",", "imgs", ",", "order", "=", "\"HWC\"", ",", "boxes", "=", "boxes", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"Unsupported split mode {}\"", ".", "format", "(", "self", ".", "_split", ")", "\n", ")", "\n", "\n", "# Convert image to CHW keeping BGR order.", "\n", "", "imgs", "=", "[", "cv2_transform", ".", "HWC2CHW", "(", "img", ")", "for", "img", "in", "imgs", "]", "\n", "\n", "# Image [0, 255] -> [0, 1].", "\n", "imgs", "=", "[", "img", "/", "255.0", "for", "img", "in", "imgs", "]", "\n", "\n", "imgs", "=", "[", "\n", "np", ".", "ascontiguousarray", "(", "\n", "# img.reshape((3, self._crop_size, self._crop_size))", "\n", "img", ".", "reshape", "(", "(", "3", ",", "imgs", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "imgs", "[", "0", "]", ".", "shape", "[", "2", "]", ")", ")", "\n", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "for", "img", "in", "imgs", "\n", "]", "\n", "\n", "# Do color augmentation (after divided by 255.0).", "\n", "if", "split_mode", "==", "\"train\"", "and", "self", ".", "_use_color_augmentation", ":", "\n", "            ", "if", "not", "self", ".", "_pca_jitter_only", ":", "\n", "                ", "imgs", "=", "cv2_transform", ".", "color_jitter_list", "(", "\n", "imgs", ",", "\n", "img_brightness", "=", "0.4", ",", "\n", "img_contrast", "=", "0.4", ",", "\n", "img_saturation", "=", "0.4", ",", "\n", ")", "\n", "\n", "", "imgs", "=", "cv2_transform", ".", "lighting_list", "(", "\n", "imgs", ",", "\n", "alphastd", "=", "0.1", ",", "\n", "eigval", "=", "np", ".", "array", "(", "self", ".", "_pca_eigval", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "\n", "eigvec", "=", "np", ".", "array", "(", "self", ".", "_pca_eigvec", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "\n", ")", "\n", "\n", "# Normalize images by mean and std.", "\n", "", "imgs", "=", "[", "\n", "cv2_transform", ".", "color_normalization", "(", "\n", "img", ",", "\n", "np", ".", "array", "(", "self", ".", "_data_mean", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", "np", ".", "array", "(", "self", ".", "_data_std", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", ")", "\n", "for", "img", "in", "imgs", "\n", "]", "\n", "\n", "# Concat list of images to single ndarray.", "\n", "imgs", "=", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "expand_dims", "(", "img", ",", "axis", "=", "1", ")", "for", "img", "in", "imgs", "]", ",", "axis", "=", "1", "\n", ")", "\n", "\n", "if", "not", "self", ".", "_use_bgr", ":", "\n", "# Convert image format from BGR to RGB.", "\n", "            ", "imgs", "=", "imgs", "[", ":", ":", "-", "1", ",", "...", "]", "\n", "\n", "", "imgs", "=", "np", ".", "ascontiguousarray", "(", "imgs", ")", "\n", "imgs", "=", "torch", ".", "from_numpy", "(", "imgs", ")", "\n", "boxes", "=", "cv2_transform", ".", "clip_boxes_to_image", "(", "\n", "boxes", "[", "0", "]", ",", "imgs", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "imgs", "[", "0", "]", ".", "shape", "[", "2", "]", "\n", ")", "\n", "\n", "return", "imgs", ",", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava._images_and_boxes_preprocessing": [[252, 358], ["transform.color_jitter.float", "transform.clip_boxes_to_image", "transform.color_normalization", "transform.clip_boxes_to_image", "transform.random_short_side_scale_jitter", "transform.random_crop", "transform.horizontal_flip", "transform.lighting_jitter", "numpy.array", "numpy.array", "transform.random_short_side_scale_jitter", "transform.uniform_crop", "transform.color_jitter", "transform.horizontal_flip", "transform.random_short_side_scale_jitter", "NotImplementedError", "numpy.array().astype", "numpy.array().astype", "transform.horizontal_flip", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.clip_boxes_to_image", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.color_normalization", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.clip_boxes_to_image", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_short_side_scale_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_crop", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.horizontal_flip", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.lighting_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_short_side_scale_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.uniform_crop", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.color_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.horizontal_flip", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.random_short_side_scale_jitter", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.transform.horizontal_flip"], ["", "def", "_images_and_boxes_preprocessing", "(", "self", ",", "imgs", ",", "boxes", ")", ":", "\n", "        ", "\"\"\"\n        This function performs preprocessing for the input images and\n        corresponding boxes for one clip.\n\n        Args:\n            imgs (tensor): the images.\n            boxes (ndarray): the boxes for the current clip.\n\n        Returns:\n            imgs (tensor): list of preprocessed images.\n            boxes (ndarray): preprocessed boxes.\n        \"\"\"", "\n", "# Image [0, 255] -> [0, 1].", "\n", "imgs", "=", "imgs", ".", "float", "(", ")", "\n", "imgs", "=", "imgs", "/", "255.0", "\n", "\n", "height", ",", "width", "=", "imgs", ".", "shape", "[", "2", "]", ",", "imgs", ".", "shape", "[", "3", "]", "\n", "# The format of boxes is [x1, y1, x2, y2]. The input boxes are in the", "\n", "# range of [0, 1].", "\n", "boxes", "[", ":", ",", "[", "0", ",", "2", "]", "]", "*=", "width", "\n", "boxes", "[", ":", ",", "[", "1", ",", "3", "]", "]", "*=", "height", "\n", "boxes", "=", "transform", ".", "clip_boxes_to_image", "(", "boxes", ",", "height", ",", "width", ")", "\n", "\n", "if", "self", ".", "_split", "==", "\"train\"", ":", "\n", "# Train split", "\n", "            ", "imgs", ",", "boxes", "=", "transform", ".", "random_short_side_scale_jitter", "(", "\n", "imgs", ",", "\n", "min_size", "=", "self", ".", "_jitter_min_scale", ",", "\n", "max_size", "=", "self", ".", "_jitter_max_scale", ",", "\n", "boxes", "=", "boxes", ",", "\n", ")", "\n", "imgs", ",", "boxes", "=", "transform", ".", "random_crop", "(", "\n", "imgs", ",", "self", ".", "_crop_size", ",", "boxes", "=", "boxes", "\n", ")", "\n", "\n", "# Random flip.", "\n", "imgs", ",", "boxes", "=", "transform", ".", "horizontal_flip", "(", "0.5", ",", "imgs", ",", "boxes", "=", "boxes", ")", "\n", "", "elif", "self", ".", "_split", "==", "\"val\"", ":", "\n", "# Val split", "\n", "# Resize short side to crop_size. Non-local and STRG uses 256.", "\n", "            ", "imgs", ",", "boxes", "=", "transform", ".", "random_short_side_scale_jitter", "(", "\n", "imgs", ",", "\n", "min_size", "=", "self", ".", "_crop_size", ",", "\n", "max_size", "=", "self", ".", "_crop_size", ",", "\n", "boxes", "=", "boxes", ",", "\n", ")", "\n", "\n", "# Apply center crop for val split", "\n", "imgs", ",", "boxes", "=", "transform", ".", "uniform_crop", "(", "\n", "imgs", ",", "size", "=", "self", ".", "_crop_size", ",", "spatial_idx", "=", "1", ",", "boxes", "=", "boxes", "\n", ")", "\n", "\n", "if", "self", ".", "_test_force_flip", ":", "\n", "                ", "imgs", ",", "boxes", "=", "transform", ".", "horizontal_flip", "(", "1", ",", "imgs", ",", "boxes", "=", "boxes", ")", "\n", "", "", "elif", "self", ".", "_split", "==", "\"test\"", ":", "\n", "# Test split", "\n", "# Resize short side to crop_size. Non-local and STRG uses 256.", "\n", "            ", "imgs", ",", "boxes", "=", "transform", ".", "random_short_side_scale_jitter", "(", "\n", "imgs", ",", "\n", "min_size", "=", "self", ".", "_crop_size", ",", "\n", "max_size", "=", "self", ".", "_crop_size", ",", "\n", "boxes", "=", "boxes", ",", "\n", ")", "\n", "\n", "if", "self", ".", "_test_force_flip", ":", "\n", "                ", "imgs", ",", "boxes", "=", "transform", ".", "horizontal_flip", "(", "1", ",", "imgs", ",", "boxes", "=", "boxes", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"{} split not supported yet!\"", ".", "format", "(", "self", ".", "_split", ")", "\n", ")", "\n", "\n", "# Do color augmentation (after divided by 255.0).", "\n", "", "if", "self", ".", "_split", "==", "\"train\"", "and", "self", ".", "_use_color_augmentation", ":", "\n", "            ", "if", "not", "self", ".", "_pca_jitter_only", ":", "\n", "                ", "imgs", "=", "transform", ".", "color_jitter", "(", "\n", "imgs", ",", "\n", "img_brightness", "=", "0.4", ",", "\n", "img_contrast", "=", "0.4", ",", "\n", "img_saturation", "=", "0.4", ",", "\n", ")", "\n", "\n", "", "imgs", "=", "transform", ".", "lighting_jitter", "(", "\n", "imgs", ",", "\n", "alphastd", "=", "0.1", ",", "\n", "eigval", "=", "np", ".", "array", "(", "self", ".", "_pca_eigval", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "\n", "eigvec", "=", "np", ".", "array", "(", "self", ".", "_pca_eigvec", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "\n", ")", "\n", "\n", "# Normalize images by mean and std.", "\n", "", "imgs", "=", "transform", ".", "color_normalization", "(", "\n", "imgs", ",", "\n", "np", ".", "array", "(", "self", ".", "_data_mean", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", "np", ".", "array", "(", "self", ".", "_data_std", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", ")", "\n", "\n", "if", "not", "self", ".", "_use_bgr", ":", "\n", "# Convert image format from BGR to RGB.", "\n", "# Note that Kinetics pre-training uses RGB!", "\n", "            ", "imgs", "=", "imgs", "[", ":", ",", "[", "2", ",", "1", ",", "0", "]", ",", "...", "]", "\n", "\n", "", "boxes", "=", "transform", ".", "clip_boxes_to_image", "(", "\n", "boxes", ",", "self", ".", "_crop_size", ",", "self", ".", "_crop_size", "\n", ")", "\n", "\n", "return", "imgs", ",", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava.__getitem__": [[359, 454], ["utils.get_sequence", "numpy.array", "boxes[].copy", "boxes[].copy.copy", "utils.retry_load_images", "numpy.zeros", "enumerate", "utils.pack_pathway_output", "len", "boxes[].copy.append", "labels.append", "imgs.permute.permute.permute", "ava_dataset.Ava._images_and_boxes_preprocessing", "imgs.permute.permute.permute", "ava_dataset.Ava._images_and_boxes_preprocessing_cv2", "len", "ava_helper.draw_feature_from_sliding_window", "len", "len", "lmdb.open", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.get_sequence", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.retry_load_images", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.utils.pack_pathway_output", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava._images_and_boxes_preprocessing", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_dataset.Ava._images_and_boxes_preprocessing_cv2", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.ava_helper.draw_feature_from_sliding_window"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"\n        Generate corresponding clips, boxes, labels and metadata for given idx.\n\n        Args:\n            idx (int): the video index provided by the pytorch sampler.\n        Returns:\n            frames (tensor): the frames of sampled from the video. The dimension\n                is `channel` x `num frames` x `height` x `width`.\n            label (ndarray): the label for correspond boxes for the current video.\n            idx (int): the video index provided by the pytorch sampler.\n            extra_data (dict): a dict containing extra data fields, like \"boxes\",\n                \"ori_boxes\" and \"metadata\".\n        \"\"\"", "\n", "video_idx", ",", "sec_idx", ",", "sec", ",", "center_idx", "=", "self", ".", "_keyframe_indices", "[", "idx", "]", "\n", "# Get the frame idxs for current clip.", "\n", "\n", "seq", "=", "utils", ".", "get_sequence", "(", "\n", "center_idx", ",", "\n", "self", ".", "_seq_len", "//", "2", ",", "\n", "self", ".", "_sample_rate", ",", "\n", "num_frames", "=", "len", "(", "self", ".", "_image_paths", "[", "video_idx", "]", ")", ",", "\n", ")", "\n", "\n", "clip_label_list", "=", "self", ".", "_keyframe_boxes_and_labels", "[", "video_idx", "]", "[", "sec_idx", "]", "\n", "assert", "len", "(", "clip_label_list", ")", ">", "0", "\n", "\n", "# Get boxes and labels for current clip.", "\n", "boxes", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "box_labels", "in", "clip_label_list", ":", "\n", "            ", "boxes", ".", "append", "(", "box_labels", "[", "0", "]", ")", "\n", "labels", ".", "append", "(", "box_labels", "[", "1", "]", ")", "\n", "", "boxes", "=", "np", ".", "array", "(", "boxes", ")", "\n", "# Score is not used.", "\n", "boxes", "=", "boxes", "[", ":", ",", ":", "4", "]", ".", "copy", "(", ")", "\n", "ori_boxes", "=", "boxes", ".", "copy", "(", ")", "\n", "\n", "# Load images of current clip.", "\n", "image_paths", "=", "[", "self", ".", "_image_paths", "[", "video_idx", "]", "[", "frame", "]", "for", "frame", "in", "seq", "]", "\n", "imgs", "=", "utils", ".", "retry_load_images", "(", "\n", "image_paths", ",", "backend", "=", "self", ".", "cfg", ".", "AVA", ".", "IMG_PROC_BACKEND", "\n", ")", "\n", "if", "self", ".", "cfg", ".", "AVA", ".", "IMG_PROC_BACKEND", "==", "\"pytorch\"", ":", "\n", "# T H W C -> T C H W.", "\n", "            ", "imgs", "=", "imgs", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "# Preprocess images and boxes.", "\n", "imgs", ",", "boxes", "=", "self", ".", "_images_and_boxes_preprocessing", "(", "\n", "imgs", ",", "boxes", "=", "boxes", "\n", ")", "\n", "# T C H W -> C T H W.", "\n", "imgs", "=", "imgs", ".", "permute", "(", "1", ",", "0", ",", "2", ",", "3", ")", "\n", "", "else", ":", "\n", "# Preprocess images and boxes", "\n", "            ", "imgs", ",", "boxes", "=", "self", ".", "_images_and_boxes_preprocessing_cv2", "(", "\n", "imgs", ",", "boxes", "=", "boxes", "\n", ")", "\n", "\n", "# Construct label arrays.", "\n", "", "label_arrs", "=", "np", ".", "zeros", "(", "(", "len", "(", "labels", ")", ",", "self", ".", "_num_classes", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", ",", "box_labels", "in", "enumerate", "(", "labels", ")", ":", "\n", "# AVA label index starts from 1.", "\n", "            ", "for", "label", "in", "box_labels", ":", "\n", "                ", "if", "label", "==", "-", "1", ":", "\n", "                    ", "continue", "\n", "", "assert", "label", ">=", "1", "and", "label", "<=", "80", "\n", "label_arrs", "[", "i", "]", "[", "label", "-", "1", "]", "=", "1", "\n", "\n", "", "", "imgs", "=", "utils", ".", "pack_pathway_output", "(", "self", ".", "cfg", ",", "imgs", ")", "\n", "metadata", "=", "[", "[", "video_idx", ",", "sec", "]", "]", "*", "len", "(", "boxes", ")", "\n", "\n", "extra_data", "=", "{", "\n", "\"boxes\"", ":", "boxes", ",", "\n", "\"ori_boxes\"", ":", "ori_boxes", ",", "\n", "\"metadata\"", ":", "metadata", ",", "\n", "}", "\n", "\n", "if", "self", ".", "cfg", ".", "AVA", ".", "FEATURE_BANK_PATH", "!=", "\"\"", "and", "not", "self", ".", "cfg", ".", "AVA", ".", "FEATURE_EXTRACTION", ":", "\n", "# lazy initialization", "\n", "            ", "if", "self", ".", "db_handler", "is", "None", ":", "\n", "                ", "self", ".", "db_handler", "=", "lmdb", ".", "open", "(", "\n", "self", ".", "cfg", ".", "AVA", ".", "FEATURE_BANK_PATH", ",", "\n", "readonly", "=", "True", ",", "\n", "lock", "=", "False", "\n", ")", "\n", "\n", "", "feat_bank", ",", "time_stamp", "=", "ava_helper", ".", "draw_feature_from_sliding_window", "(", "\n", "cfg", "=", "self", ".", "cfg", ",", "vid", "=", "video_idx", ",", "sec", "=", "sec", ",", "split", "=", "self", ".", "_split", ",", "db", "=", "self", ".", "db_handler", "\n", ")", "\n", "time_stamp", "+=", "[", "sec", "]", "*", "len", "(", "boxes", ")", "\n", "\n", "", "else", ":", "\n", "            ", "feat_bank", ",", "time_stamp", "=", "None", ",", "None", "\n", "\n", "", "return", "imgs", ",", "label_arrs", ",", "idx", ",", "extra_data", ",", "feat_bank", ",", "time_stamp", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.multigrid_helper.ShortCycleBatchSampler.__init__": [[18, 58], ["isinstance", "ValueError", "isinstance", "ValueError", "isinstance", "ValueError", "int", "isinstance", "round", "float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sampler", ",", "batch_size", ",", "drop_last", ",", "cfg", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "sampler", ",", "Sampler", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"sampler should be an instance of \"", "\n", "\"torch.utils.data.Sampler, but got sampler={}\"", ".", "format", "(", "sampler", ")", "\n", ")", "\n", "", "if", "(", "\n", "not", "isinstance", "(", "batch_size", ",", "_int_classes", ")", "\n", "or", "isinstance", "(", "batch_size", ",", "bool", ")", "\n", "or", "batch_size", "<=", "0", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"batch_size should be a positive integer value, \"", "\n", "\"but got batch_size={}\"", ".", "format", "(", "batch_size", ")", "\n", ")", "\n", "", "if", "not", "isinstance", "(", "drop_last", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"drop_last should be a boolean value, but got \"", "\n", "\"drop_last={}\"", ".", "format", "(", "drop_last", ")", "\n", ")", "\n", "", "self", ".", "sampler", "=", "sampler", "\n", "self", ".", "drop_last", "=", "drop_last", "\n", "\n", "bs_factor", "=", "[", "\n", "int", "(", "\n", "round", "(", "\n", "(", "\n", "float", "(", "cfg", ".", "DATA", ".", "TRAIN_CROP_SIZE", ")", "\n", "/", "(", "s", "*", "cfg", ".", "MULTIGRID", ".", "DEFAULT_S", ")", "\n", ")", "\n", "**", "2", "\n", ")", "\n", ")", "\n", "for", "s", "in", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE_FACTORS", "\n", "]", "\n", "\n", "self", ".", "batch_sizes", "=", "[", "\n", "batch_size", "*", "bs_factor", "[", "0", "]", ",", "\n", "batch_size", "*", "bs_factor", "[", "1", "]", ",", "\n", "batch_size", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.multigrid_helper.ShortCycleBatchSampler.__iter__": [[60, 73], ["batch.append", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "counter", "=", "0", "\n", "batch_size", "=", "self", ".", "batch_sizes", "[", "0", "]", "\n", "batch", "=", "[", "]", "\n", "for", "idx", "in", "self", ".", "sampler", ":", "\n", "            ", "batch", ".", "append", "(", "(", "idx", ",", "counter", "%", "3", ")", ")", "\n", "if", "len", "(", "batch", ")", "==", "batch_size", ":", "\n", "                ", "yield", "batch", "\n", "counter", "+=", "1", "\n", "batch_size", "=", "self", ".", "batch_sizes", "[", "counter", "%", "3", "]", "\n", "batch", "=", "[", "]", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", "and", "not", "self", ".", "drop_last", ":", "\n", "            ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.datasets.multigrid_helper.ShortCycleBatchSampler.__len__": [[74, 80], ["sum", "int", "int", "numpy.floor", "numpy.ceil", "len", "len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "avg_batch_size", "=", "sum", "(", "self", ".", "batch_sizes", ")", "/", "3.0", "\n", "if", "self", ".", "drop_last", ":", "\n", "            ", "return", "int", "(", "np", ".", "floor", "(", "len", "(", "self", ".", "sampler", ")", "/", "avg_batch_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "np", ".", "ceil", "(", "len", "(", "self", ".", "sampler", ")", "/", "avg_batch_size", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.config.defaults._assert_and_infer_cfg": [[705, 726], ["None"], "function", ["None"], ["def", "_assert_and_infer_cfg", "(", "cfg", ")", ":", "\n", "# BN assertions.", "\n", "    ", "if", "cfg", ".", "BN", ".", "USE_PRECISE_STATS", ":", "\n", "        ", "assert", "cfg", ".", "BN", ".", "NUM_BATCHES_PRECISE", ">=", "0", "\n", "# TRAIN assertions.", "\n", "", "assert", "cfg", ".", "TRAIN", ".", "CHECKPOINT_TYPE", "in", "[", "\"pytorch\"", ",", "\"caffe2\"", "]", "\n", "assert", "cfg", ".", "TRAIN", ".", "BATCH_SIZE", "%", "cfg", ".", "NUM_GPUS", "==", "0", "\n", "\n", "# TEST assertions.", "\n", "assert", "cfg", ".", "TEST", ".", "CHECKPOINT_TYPE", "in", "[", "\"pytorch\"", ",", "\"caffe2\"", "]", "\n", "assert", "cfg", ".", "TEST", ".", "BATCH_SIZE", "%", "cfg", ".", "NUM_GPUS", "==", "0", "\n", "assert", "cfg", ".", "TEST", ".", "NUM_SPATIAL_CROPS", "==", "3", "\n", "\n", "# RESNET assertions.", "\n", "assert", "cfg", ".", "RESNET", ".", "NUM_GROUPS", ">", "0", "\n", "assert", "cfg", ".", "RESNET", ".", "WIDTH_PER_GROUP", ">", "0", "\n", "assert", "cfg", ".", "RESNET", ".", "WIDTH_PER_GROUP", "%", "cfg", ".", "RESNET", ".", "NUM_GROUPS", "==", "0", "\n", "\n", "# General assertions.", "\n", "assert", "cfg", ".", "SHARD_ID", "<", "cfg", ".", "NUM_SHARDS", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.config.defaults.get_cfg": [[728, 733], ["defaults._assert_and_infer_cfg", "_C.clone"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.config.defaults._assert_and_infer_cfg"], ["", "def", "get_cfg", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get a copy of the default config.\n    \"\"\"", "\n", "return", "_assert_and_infer_cfg", "(", "_C", ".", "clone", "(", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.config.custom_config.add_custom_config": [[7, 10], ["None"], "function", ["None"], ["def", "add_custom_config", "(", "_C", ")", ":", "\n", "# Add your own customized configs.", "\n", "    ", "pass", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.SubBatchNorm3d.__init__": [[48, 68], ["torch.Module.__init__", "args.get", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["def", "__init__", "(", "self", ",", "num_splits", ",", "**", "args", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            num_splits (int): number of splits.\n            args (list): other arguments.\n        \"\"\"", "\n", "super", "(", "SubBatchNorm3d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "num_features", "=", "args", "[", "\"num_features\"", "]", "\n", "# Keep only one set of weight and bias.", "\n", "if", "args", ".", "get", "(", "\"affine\"", ",", "True", ")", ":", "\n", "            ", "self", ".", "affine", "=", "True", "\n", "args", "[", "\"affine\"", "]", "=", "False", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "affine", "=", "False", "\n", "", "self", ".", "bn", "=", "nn", ".", "BatchNorm3d", "(", "**", "args", ")", "\n", "args", "[", "\"num_features\"", "]", "=", "num_features", "*", "num_splits", "\n", "self", ".", "split_bn", "=", "nn", ".", "BatchNorm3d", "(", "**", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.SubBatchNorm3d._get_aggregated_mean_std": [[69, 83], ["means.view().sum", "mean.detach", "std.detach", "stds.view().sum", "means.view", "stds.view", "means.view"], "methods", ["None"], ["", "def", "_get_aggregated_mean_std", "(", "self", ",", "means", ",", "stds", ",", "n", ")", ":", "\n", "        ", "\"\"\"\n        Calculate the aggregated mean and stds.\n        Args:\n            means (tensor): mean values.\n            stds (tensor): standard deviations.\n            n (int): number of sets of means and stds.\n        \"\"\"", "\n", "mean", "=", "means", ".", "view", "(", "n", ",", "-", "1", ")", ".", "sum", "(", "0", ")", "/", "n", "\n", "std", "=", "(", "\n", "stds", ".", "view", "(", "n", ",", "-", "1", ")", ".", "sum", "(", "0", ")", "/", "n", "\n", "+", "(", "(", "means", ".", "view", "(", "n", ",", "-", "1", ")", "-", "mean", ")", "**", "2", ")", ".", "view", "(", "n", ",", "-", "1", ")", ".", "sum", "(", "0", ")", "/", "n", "\n", ")", "\n", "return", "mean", ".", "detach", "(", ")", ",", "std", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.SubBatchNorm3d.aggregate_stats": [[84, 96], ["batchnorm_helper.SubBatchNorm3d._get_aggregated_mean_std"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.SubBatchNorm3d._get_aggregated_mean_std"], ["", "def", "aggregate_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Synchronize running_mean, and running_var. Call this before eval.\n        \"\"\"", "\n", "if", "self", ".", "split_bn", ".", "track_running_stats", ":", "\n", "            ", "(", "\n", "self", ".", "bn", ".", "running_mean", ".", "data", ",", "\n", "self", ".", "bn", ".", "running_var", ".", "data", ",", "\n", ")", "=", "self", ".", "_get_aggregated_mean_std", "(", "\n", "self", ".", "split_bn", ".", "running_mean", ",", "\n", "self", ".", "split_bn", ".", "running_var", ",", "\n", "self", ".", "num_splits", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.SubBatchNorm3d.forward": [[98, 110], ["batchnorm_helper.SubBatchNorm3d.view", "batchnorm_helper.SubBatchNorm3d.split_bn", "batchnorm_helper.SubBatchNorm3d.view", "batchnorm_helper.SubBatchNorm3d.bn", "batchnorm_helper.SubBatchNorm3d.weight.view", "batchnorm_helper.SubBatchNorm3d.bias.view"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "training", ":", "\n", "            ", "n", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "n", "//", "self", ".", "num_splits", ",", "c", "*", "self", ".", "num_splits", ",", "t", ",", "h", ",", "w", ")", "\n", "x", "=", "self", ".", "split_bn", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "c", ",", "t", ",", "h", ",", "w", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "", "if", "self", ".", "affine", ":", "\n", "            ", "x", "=", "x", "*", "self", ".", "weight", ".", "view", "(", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "bias", ".", "view", "(", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.GroupGather.forward": [[117, 144], ["torch.all_gather", "torch.all_gather", "torch.all_gather", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "slowfast.get_local_rank", "range", "slowfast.get_local_size"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_rank", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "num_sync_devices", ",", "num_groups", ")", ":", "\n", "        ", "\"\"\"\n        Perform forwarding, gathering the stats across different process/ GPU\n        group.\n        \"\"\"", "\n", "ctx", ".", "num_sync_devices", "=", "num_sync_devices", "\n", "ctx", ".", "num_groups", "=", "num_groups", "\n", "\n", "input_list", "=", "[", "\n", "torch", ".", "zeros_like", "(", "input", ")", "for", "k", "in", "range", "(", "du", ".", "get_local_size", "(", ")", ")", "\n", "]", "\n", "dist", ".", "all_gather", "(", "\n", "input_list", ",", "input", ",", "async_op", "=", "False", ",", "group", "=", "du", ".", "_LOCAL_PROCESS_GROUP", "\n", ")", "\n", "\n", "inputs", "=", "torch", ".", "stack", "(", "input_list", ",", "dim", "=", "0", ")", "\n", "if", "num_groups", ">", "1", ":", "\n", "            ", "rank", "=", "du", ".", "get_local_rank", "(", ")", "\n", "group_idx", "=", "rank", "//", "num_sync_devices", "\n", "inputs", "=", "inputs", "[", "\n", "group_idx", "\n", "*", "num_sync_devices", ":", "(", "group_idx", "+", "1", ")", "\n", "*", "num_sync_devices", "\n", "]", "\n", "", "inputs", "=", "torch", ".", "sum", "(", "inputs", ",", "dim", "=", "0", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.GroupGather.backward": [[145, 172], ["torch.all_gather", "torch.all_gather", "torch.all_gather", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "slowfast.get_local_rank", "range", "slowfast.get_local_size"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.all_gather", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_rank", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "\"\"\"\n        Perform backwarding, gathering the gradients across different process/ GPU\n        group.\n        \"\"\"", "\n", "grad_output_list", "=", "[", "\n", "torch", ".", "zeros_like", "(", "grad_output", ")", "for", "k", "in", "range", "(", "du", ".", "get_local_size", "(", ")", ")", "\n", "]", "\n", "dist", ".", "all_gather", "(", "\n", "grad_output_list", ",", "\n", "grad_output", ",", "\n", "async_op", "=", "False", ",", "\n", "group", "=", "du", ".", "_LOCAL_PROCESS_GROUP", ",", "\n", ")", "\n", "\n", "grads", "=", "torch", ".", "stack", "(", "grad_output_list", ",", "dim", "=", "0", ")", "\n", "if", "ctx", ".", "num_groups", ">", "1", ":", "\n", "            ", "rank", "=", "du", ".", "get_local_rank", "(", ")", "\n", "group_idx", "=", "rank", "//", "ctx", ".", "num_sync_devices", "\n", "grads", "=", "grads", "[", "\n", "group_idx", "\n", "*", "ctx", ".", "num_sync_devices", ":", "(", "group_idx", "+", "1", ")", "\n", "*", "ctx", ".", "num_sync_devices", "\n", "]", "\n", "", "grads", "=", "torch", ".", "sum", "(", "grads", ",", "dim", "=", "0", ")", "\n", "return", "grads", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.NaiveSyncBatchNorm3d.__init__": [[175, 193], ["torch.BatchNorm3d.__init__", "slowfast.get_local_size", "slowfast.get_local_size", "slowfast.get_local_size", "slowfast.get_local_size"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size"], ["    ", "def", "__init__", "(", "self", ",", "num_sync_devices", ",", "**", "args", ")", ":", "\n", "        ", "\"\"\"\n        Naive version of Synchronized 3D BatchNorm.\n        Args:\n            num_sync_devices (int): number of device to sync.\n            args (list): other arguments.\n        \"\"\"", "\n", "self", ".", "num_sync_devices", "=", "num_sync_devices", "\n", "if", "self", ".", "num_sync_devices", ">", "0", ":", "\n", "            ", "assert", "du", ".", "get_local_size", "(", ")", "%", "self", ".", "num_sync_devices", "==", "0", ",", "(", "\n", "du", ".", "get_local_size", "(", ")", ",", "\n", "self", ".", "num_sync_devices", ",", "\n", ")", "\n", "self", ".", "num_groups", "=", "du", ".", "get_local_size", "(", ")", "//", "self", ".", "num_sync_devices", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_sync_devices", "=", "du", ".", "get_local_size", "(", ")", "\n", "self", ".", "num_groups", "=", "1", "\n", "", "super", "(", "NaiveSyncBatchNorm3d", ",", "self", ")", ".", "__init__", "(", "**", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.NaiveSyncBatchNorm3d.forward": [[194, 219], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "scale.reshape.reshape.reshape", "bias.reshape.reshape.reshape", "super().forward", "GroupGather.apply", "slowfast.get_local_size", "torch.mean.detach", "torch.mean.detach", "torch.mean.detach", "var.detach"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.forward", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.distributed.get_local_size"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "du", ".", "get_local_size", "(", ")", "==", "1", "or", "not", "self", ".", "training", ":", "\n", "            ", "return", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "\n", "", "assert", "input", ".", "shape", "[", "0", "]", ">", "0", ",", "\"SyncBatchNorm does not support empty inputs\"", "\n", "C", "=", "input", ".", "shape", "[", "1", "]", "\n", "mean", "=", "torch", ".", "mean", "(", "input", ",", "dim", "=", "[", "0", ",", "2", ",", "3", ",", "4", "]", ")", "\n", "meansqr", "=", "torch", ".", "mean", "(", "input", "*", "input", ",", "dim", "=", "[", "0", ",", "2", ",", "3", ",", "4", "]", ")", "\n", "\n", "vec", "=", "torch", ".", "cat", "(", "[", "mean", ",", "meansqr", "]", ",", "dim", "=", "0", ")", "\n", "vec", "=", "GroupGather", ".", "apply", "(", "vec", ",", "self", ".", "num_sync_devices", ",", "self", ".", "num_groups", ")", "*", "(", "\n", "1.0", "/", "self", ".", "num_sync_devices", "\n", ")", "\n", "\n", "mean", ",", "meansqr", "=", "torch", ".", "split", "(", "vec", ",", "C", ")", "\n", "var", "=", "meansqr", "-", "mean", "*", "mean", "\n", "self", ".", "running_mean", "+=", "self", ".", "momentum", "*", "(", "mean", ".", "detach", "(", ")", "-", "self", ".", "running_mean", ")", "\n", "self", ".", "running_var", "+=", "self", ".", "momentum", "*", "(", "var", ".", "detach", "(", ")", "-", "self", ".", "running_var", ")", "\n", "\n", "invstd", "=", "torch", ".", "rsqrt", "(", "var", "+", "self", ".", "eps", ")", "\n", "scale", "=", "self", ".", "weight", "*", "invstd", "\n", "bias", "=", "self", ".", "bias", "-", "mean", "*", "scale", "\n", "scale", "=", "scale", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "bias", "=", "bias", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "return", "input", "*", "scale", "+", "bias", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.get_norm": [[15, 34], ["functools.partial", "functools.partial", "NotImplementedError"], "function", ["None"], ["def", "get_norm", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        cfg (CfgNode): model building configs, details are in the comments of\n            the config file.\n    Returns:\n        nn.Module: the normalization layer.\n    \"\"\"", "\n", "if", "cfg", ".", "BN", ".", "NORM_TYPE", "==", "\"batchnorm\"", ":", "\n", "        ", "return", "nn", ".", "BatchNorm3d", "\n", "", "elif", "cfg", ".", "BN", ".", "NORM_TYPE", "==", "\"sub_batchnorm\"", ":", "\n", "        ", "return", "partial", "(", "SubBatchNorm3d", ",", "num_splits", "=", "cfg", ".", "BN", ".", "NUM_SPLITS", ")", "\n", "", "elif", "cfg", ".", "BN", ".", "NORM_TYPE", "==", "\"sync_batchnorm\"", ":", "\n", "        ", "return", "partial", "(", "\n", "NaiveSyncBatchNorm3d", ",", "num_sync_devices", "=", "cfg", ".", "BN", ".", "NUM_SYNC_DEVICES", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"Norm type {} is not supported\"", ".", "format", "(", "cfg", ".", "BN", ".", "NORM_TYPE", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.BasicTransform.__init__": [[29, 69], ["torch.Module.__init__", "resnet_helper.BasicTransform._construct"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage._construct"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "temp_kernel_size", ",", "\n", "stride", ",", "\n", "dim_inner", "=", "None", ",", "\n", "num_groups", "=", "1", ",", "\n", "stride_1x1", "=", "None", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "eps", "=", "1e-5", ",", "\n", "bn_mmt", "=", "0.1", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temp_kernel_size (int): the temporal kernel sizes of the middle\n                convolution in the bottleneck.\n            stride (int): the stride of the bottleneck.\n            dim_inner (None): the inner dimension would not be used in\n                BasicTransform.\n            num_groups (int): number of groups for the convolution. Number of\n                group is always 1 for BasicTransform.\n            stride_1x1 (None): stride_1x1 will not be used in BasicTransform.\n            inplace_relu (bool): if True, calculate the relu on the original\n                input without allocating new memory.\n            eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "BasicTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temp_kernel_size", "=", "temp_kernel_size", "\n", "self", ".", "_inplace_relu", "=", "inplace_relu", "\n", "self", ".", "_eps", "=", "eps", "\n", "self", ".", "_bn_mmt", "=", "bn_mmt", "\n", "self", ".", "_construct", "(", "dim_in", ",", "dim_out", ",", "stride", ",", "norm_module", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.BasicTransform._construct": [[70, 98], ["torch.Conv3d", "norm_module", "torch.ReLU", "torch.Conv3d", "norm_module", "int"], "methods", ["None"], ["", "def", "_construct", "(", "self", ",", "dim_in", ",", "dim_out", ",", "stride", ",", "norm_module", ")", ":", "\n", "# Tx3x3, BN, ReLU.", "\n", "        ", "self", ".", "a", "=", "nn", ".", "Conv3d", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "kernel_size", "=", "[", "self", ".", "temp_kernel_size", ",", "3", ",", "3", "]", ",", "\n", "stride", "=", "[", "1", ",", "stride", ",", "stride", "]", ",", "\n", "padding", "=", "[", "int", "(", "self", ".", "temp_kernel_size", "//", "2", ")", ",", "1", ",", "1", "]", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "a_bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_out", ",", "eps", "=", "self", ".", "_eps", ",", "momentum", "=", "self", ".", "_bn_mmt", "\n", ")", "\n", "self", ".", "a_relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "self", ".", "_inplace_relu", ")", "\n", "# 1x3x3, BN.", "\n", "self", ".", "b", "=", "nn", ".", "Conv3d", "(", "\n", "dim_out", ",", "\n", "dim_out", ",", "\n", "kernel_size", "=", "[", "1", ",", "3", ",", "3", "]", ",", "\n", "stride", "=", "[", "1", ",", "1", ",", "1", "]", ",", "\n", "padding", "=", "[", "0", ",", "1", ",", "1", "]", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "b_bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_out", ",", "eps", "=", "self", ".", "_eps", ",", "momentum", "=", "self", ".", "_bn_mmt", "\n", ")", "\n", "\n", "self", ".", "b_bn", ".", "transform_final_bn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.BasicTransform.forward": [[99, 107], ["resnet_helper.BasicTransform.a", "resnet_helper.BasicTransform.a_bn", "resnet_helper.BasicTransform.a_relu", "resnet_helper.BasicTransform.b", "resnet_helper.BasicTransform.b_bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "a", "(", "x", ")", "\n", "x", "=", "self", ".", "a_bn", "(", "x", ")", "\n", "x", "=", "self", ".", "a_relu", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "b", "(", "x", ")", "\n", "x", "=", "self", ".", "b_bn", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.BottleneckTransform.__init__": [[115, 166], ["torch.Module.__init__", "resnet_helper.BottleneckTransform._construct"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage._construct"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "temp_kernel_size", ",", "\n", "stride", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "stride_1x1", "=", "False", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "eps", "=", "1e-5", ",", "\n", "bn_mmt", "=", "0.1", ",", "\n", "dilation", "=", "1", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temp_kernel_size (int): the temporal kernel sizes of the middle\n                convolution in the bottleneck.\n            stride (int): the stride of the bottleneck.\n            dim_inner (int): the inner dimension of the block.\n            num_groups (int): number of groups for the convolution. num_groups=1\n                is for standard ResNet like networks, and num_groups>1 is for\n                ResNeXt like networks.\n            stride_1x1 (bool): if True, apply stride to 1x1 conv, otherwise\n                apply stride to the 3x3 conv.\n            inplace_relu (bool): if True, calculate the relu on the original\n                input without allocating new memory.\n            eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            dilation (int): size of dilation.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "BottleneckTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temp_kernel_size", "=", "temp_kernel_size", "\n", "self", ".", "_inplace_relu", "=", "inplace_relu", "\n", "self", ".", "_eps", "=", "eps", "\n", "self", ".", "_bn_mmt", "=", "bn_mmt", "\n", "self", ".", "_stride_1x1", "=", "stride_1x1", "\n", "self", ".", "_construct", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "stride", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "dilation", ",", "\n", "norm_module", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.BottleneckTransform._construct": [[168, 223], ["torch.Conv3d", "norm_module", "torch.ReLU", "torch.Conv3d", "norm_module", "torch.ReLU", "torch.Conv3d", "norm_module", "int"], "methods", ["None"], ["", "def", "_construct", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "stride", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "dilation", ",", "\n", "norm_module", ",", "\n", ")", ":", "\n", "        ", "(", "str1x1", ",", "str3x3", ")", "=", "(", "stride", ",", "1", ")", "if", "self", ".", "_stride_1x1", "else", "(", "1", ",", "stride", ")", "\n", "\n", "# Tx1x1, BN, ReLU.", "\n", "self", ".", "a", "=", "nn", ".", "Conv3d", "(", "\n", "dim_in", ",", "\n", "dim_inner", ",", "\n", "kernel_size", "=", "[", "self", ".", "temp_kernel_size", ",", "1", ",", "1", "]", ",", "\n", "stride", "=", "[", "1", ",", "str1x1", ",", "str1x1", "]", ",", "\n", "padding", "=", "[", "int", "(", "self", ".", "temp_kernel_size", "//", "2", ")", ",", "0", ",", "0", "]", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "a_bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_inner", ",", "eps", "=", "self", ".", "_eps", ",", "momentum", "=", "self", ".", "_bn_mmt", "\n", ")", "\n", "self", ".", "a_relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "self", ".", "_inplace_relu", ")", "\n", "\n", "# 1x3x3, BN, ReLU.", "\n", "self", ".", "b", "=", "nn", ".", "Conv3d", "(", "\n", "dim_inner", ",", "\n", "dim_inner", ",", "\n", "[", "1", ",", "3", ",", "3", "]", ",", "\n", "stride", "=", "[", "1", ",", "str3x3", ",", "str3x3", "]", ",", "\n", "padding", "=", "[", "0", ",", "dilation", ",", "dilation", "]", ",", "\n", "groups", "=", "num_groups", ",", "\n", "bias", "=", "False", ",", "\n", "dilation", "=", "[", "1", ",", "dilation", ",", "dilation", "]", ",", "\n", ")", "\n", "self", ".", "b_bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_inner", ",", "eps", "=", "self", ".", "_eps", ",", "momentum", "=", "self", ".", "_bn_mmt", "\n", ")", "\n", "self", ".", "b_relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "self", ".", "_inplace_relu", ")", "\n", "\n", "# 1x1x1, BN.", "\n", "self", ".", "c", "=", "nn", ".", "Conv3d", "(", "\n", "dim_inner", ",", "\n", "dim_out", ",", "\n", "kernel_size", "=", "[", "1", ",", "1", ",", "1", "]", ",", "\n", "stride", "=", "[", "1", ",", "1", ",", "1", "]", ",", "\n", "padding", "=", "[", "0", ",", "0", ",", "0", "]", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "c_bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_out", ",", "eps", "=", "self", ".", "_eps", ",", "momentum", "=", "self", ".", "_bn_mmt", "\n", ")", "\n", "self", ".", "c_bn", ".", "transform_final_bn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.BottleneckTransform.forward": [[224, 240], ["resnet_helper.BottleneckTransform.a", "resnet_helper.BottleneckTransform.a_bn", "resnet_helper.BottleneckTransform.a_relu", "resnet_helper.BottleneckTransform.b", "resnet_helper.BottleneckTransform.b_bn", "resnet_helper.BottleneckTransform.b_relu", "resnet_helper.BottleneckTransform.c", "resnet_helper.BottleneckTransform.c_bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Explicitly forward every layer.", "\n", "# Branch2a.", "\n", "        ", "x", "=", "self", ".", "a", "(", "x", ")", "\n", "x", "=", "self", ".", "a_bn", "(", "x", ")", "\n", "x", "=", "self", ".", "a_relu", "(", "x", ")", "\n", "\n", "# Branch2b.", "\n", "x", "=", "self", ".", "b", "(", "x", ")", "\n", "x", "=", "self", ".", "b_bn", "(", "x", ")", "\n", "x", "=", "self", ".", "b_relu", "(", "x", ")", "\n", "\n", "# Branch2c", "\n", "x", "=", "self", ".", "c", "(", "x", ")", "\n", "x", "=", "self", ".", "c_bn", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResBlock.__init__": [[247, 307], ["torch.Module.__init__", "resnet_helper.ResBlock._construct"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage._construct"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "temp_kernel_size", ",", "\n", "stride", ",", "\n", "trans_func", ",", "\n", "dim_inner", ",", "\n", "num_groups", "=", "1", ",", "\n", "stride_1x1", "=", "False", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "eps", "=", "1e-5", ",", "\n", "bn_mmt", "=", "0.1", ",", "\n", "dilation", "=", "1", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        ResBlock class constructs redisual blocks. More details can be found in:\n            Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n            \"Deep residual learning for image recognition.\"\n            https://arxiv.org/abs/1512.03385\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temp_kernel_size (int): the temporal kernel sizes of the middle\n                convolution in the bottleneck.\n            stride (int): the stride of the bottleneck.\n            trans_func (string): transform function to be used to construct the\n                bottleneck.\n            dim_inner (int): the inner dimension of the block.\n            num_groups (int): number of groups for the convolution. num_groups=1\n                is for standard ResNet like networks, and num_groups>1 is for\n                ResNeXt like networks.\n            stride_1x1 (bool): if True, apply stride to 1x1 conv, otherwise\n                apply stride to the 3x3 conv.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            dilation (int): size of dilation.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "ResBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_inplace_relu", "=", "inplace_relu", "\n", "self", ".", "_eps", "=", "eps", "\n", "self", ".", "_bn_mmt", "=", "bn_mmt", "\n", "self", ".", "_construct", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "temp_kernel_size", ",", "\n", "stride", ",", "\n", "trans_func", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "stride_1x1", ",", "\n", "inplace_relu", ",", "\n", "dilation", ",", "\n", "norm_module", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResBlock._construct": [[309, 350], ["trans_func", "torch.ReLU", "torch.Conv3d", "norm_module"], "methods", ["None"], ["", "def", "_construct", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "temp_kernel_size", ",", "\n", "stride", ",", "\n", "trans_func", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "stride_1x1", ",", "\n", "inplace_relu", ",", "\n", "dilation", ",", "\n", "norm_module", ",", "\n", ")", ":", "\n", "# Use skip connection with projection if dim or res change.", "\n", "        ", "if", "(", "dim_in", "!=", "dim_out", ")", "or", "(", "stride", "!=", "1", ")", ":", "\n", "            ", "self", ".", "branch1", "=", "nn", ".", "Conv3d", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "[", "1", ",", "stride", ",", "stride", "]", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "dilation", "=", "1", ",", "\n", ")", "\n", "self", ".", "branch1_bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_out", ",", "eps", "=", "self", ".", "_eps", ",", "momentum", "=", "self", ".", "_bn_mmt", "\n", ")", "\n", "", "self", ".", "branch2", "=", "trans_func", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "temp_kernel_size", ",", "\n", "stride", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "stride_1x1", "=", "stride_1x1", ",", "\n", "inplace_relu", "=", "inplace_relu", ",", "\n", "dilation", "=", "dilation", ",", "\n", "norm_module", "=", "norm_module", ",", "\n", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "self", ".", "_inplace_relu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResBlock.forward": [[351, 358], ["hasattr", "resnet_helper.ResBlock.relu", "resnet_helper.ResBlock.branch1_bn", "resnet_helper.ResBlock.branch2", "resnet_helper.ResBlock.branch2", "resnet_helper.ResBlock.branch1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ",", "\"branch1\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "branch1_bn", "(", "self", ".", "branch1", "(", "x", ")", ")", "+", "self", ".", "branch2", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "branch2", "(", "x", ")", "\n", "", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage.__init__": [[371, 480], ["torch.Module.__init__", "all", "len", "resnet_helper.ResStage._construct", "len", "range", "range", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage._construct"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "stride", ",", "\n", "temp_kernel_sizes", ",", "\n", "num_blocks", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "num_block_temp_kernel", ",", "\n", "nonlocal_inds", ",", "\n", "nonlocal_group", ",", "\n", "nonlocal_pool", ",", "\n", "dilation", ",", "\n", "instantiation", "=", "\"softmax\"", ",", "\n", "trans_func_name", "=", "\"bottleneck_transform\"", ",", "\n", "stride_1x1", "=", "False", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these arguments.\n        ResStage builds p streams, where p can be greater or equal to one.\n        Args:\n            dim_in (list): list of p the channel dimensions of the input.\n                Different channel dimensions control the input dimension of\n                different pathways.\n            dim_out (list): list of p the channel dimensions of the output.\n                Different channel dimensions control the input dimension of\n                different pathways.\n            temp_kernel_sizes (list): list of the p temporal kernel sizes of the\n                convolution in the bottleneck. Different temp_kernel_sizes\n                control different pathway.\n            stride (list): list of the p strides of the bottleneck. Different\n                stride control different pathway.\n            num_blocks (list): list of p numbers of blocks for each of the\n                pathway.\n            dim_inner (list): list of the p inner channel dimensions of the\n                input. Different channel dimensions control the input dimension\n                of different pathways.\n            num_groups (list): list of number of p groups for the convolution.\n                num_groups=1 is for standard ResNet like networks, and\n                num_groups>1 is for ResNeXt like networks.\n            num_block_temp_kernel (list): extent the temp_kernel_sizes to\n                num_block_temp_kernel blocks, then fill temporal kernel size\n                of 1 for the rest of the layers.\n            nonlocal_inds (list): If the tuple is empty, no nonlocal layer will\n                be added. If the tuple is not empty, add nonlocal layers after\n                the index-th block.\n            dilation (list): size of dilation for each pathway.\n            nonlocal_group (list): list of number of p nonlocal groups. Each\n                number controls how to fold temporal dimension to batch\n                dimension before applying nonlocal transformation.\n                https://github.com/facebookresearch/video-nonlocal-net.\n            instantiation (string): different instantiation for nonlocal layer.\n                Supports two different instantiation method:\n                    \"dot_product\": normalizing correlation matrix with L2.\n                    \"softmax\": normalizing correlation matrix with Softmax.\n            trans_func_name (string): name of the the transformation function apply\n                on the network.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "ResStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "all", "(", "\n", "(", "\n", "num_block_temp_kernel", "[", "i", "]", "<=", "num_blocks", "[", "i", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "temp_kernel_sizes", ")", ")", "\n", ")", "\n", ")", "\n", "self", ".", "num_blocks", "=", "num_blocks", "\n", "self", ".", "nonlocal_group", "=", "nonlocal_group", "\n", "self", ".", "temp_kernel_sizes", "=", "[", "\n", "(", "temp_kernel_sizes", "[", "i", "]", "*", "num_blocks", "[", "i", "]", ")", "[", ":", "num_block_temp_kernel", "[", "i", "]", "]", "\n", "+", "[", "1", "]", "*", "(", "num_blocks", "[", "i", "]", "-", "num_block_temp_kernel", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "temp_kernel_sizes", ")", ")", "\n", "]", "\n", "assert", "(", "\n", "len", "(", "\n", "{", "\n", "len", "(", "dim_in", ")", ",", "\n", "len", "(", "dim_out", ")", ",", "\n", "len", "(", "temp_kernel_sizes", ")", ",", "\n", "len", "(", "stride", ")", ",", "\n", "len", "(", "num_blocks", ")", ",", "\n", "len", "(", "dim_inner", ")", ",", "\n", "len", "(", "num_groups", ")", ",", "\n", "len", "(", "num_block_temp_kernel", ")", ",", "\n", "len", "(", "nonlocal_inds", ")", ",", "\n", "len", "(", "nonlocal_group", ")", ",", "\n", "}", "\n", ")", "\n", "==", "1", "\n", ")", "\n", "self", ".", "num_pathways", "=", "len", "(", "self", ".", "num_blocks", ")", "\n", "self", ".", "_construct", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "stride", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "trans_func_name", ",", "\n", "stride_1x1", ",", "\n", "inplace_relu", ",", "\n", "nonlocal_inds", ",", "\n", "nonlocal_pool", ",", "\n", "instantiation", ",", "\n", "dilation", ",", "\n", "norm_module", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage._construct": [[482, 527], ["range", "range", "resnet_helper.get_trans_func", "resnet_helper.ResBlock", "resnet_helper.ResStage.add_module", "slowfast.models.nonlocal_helper.Nonlocal", "resnet_helper.ResStage.add_module"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.get_trans_func"], ["", "def", "_construct", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "stride", ",", "\n", "dim_inner", ",", "\n", "num_groups", ",", "\n", "trans_func_name", ",", "\n", "stride_1x1", ",", "\n", "inplace_relu", ",", "\n", "nonlocal_inds", ",", "\n", "nonlocal_pool", ",", "\n", "instantiation", ",", "\n", "dilation", ",", "\n", "norm_module", ",", "\n", ")", ":", "\n", "        ", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "num_blocks", "[", "pathway", "]", ")", ":", "\n", "# Retrieve the transformation function.", "\n", "                ", "trans_func", "=", "get_trans_func", "(", "trans_func_name", ")", "\n", "# Construct the block.", "\n", "res_block", "=", "ResBlock", "(", "\n", "dim_in", "[", "pathway", "]", "if", "i", "==", "0", "else", "dim_out", "[", "pathway", "]", ",", "\n", "dim_out", "[", "pathway", "]", ",", "\n", "self", ".", "temp_kernel_sizes", "[", "pathway", "]", "[", "i", "]", ",", "\n", "stride", "[", "pathway", "]", "if", "i", "==", "0", "else", "1", ",", "\n", "trans_func", ",", "\n", "dim_inner", "[", "pathway", "]", ",", "\n", "num_groups", "[", "pathway", "]", ",", "\n", "stride_1x1", "=", "stride_1x1", ",", "\n", "inplace_relu", "=", "inplace_relu", ",", "\n", "dilation", "=", "dilation", "[", "pathway", "]", ",", "\n", "norm_module", "=", "norm_module", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"pathway{}_res{}\"", ".", "format", "(", "pathway", ",", "i", ")", ",", "res_block", ")", "\n", "if", "i", "in", "nonlocal_inds", "[", "pathway", "]", ":", "\n", "                    ", "nln", "=", "Nonlocal", "(", "\n", "dim_out", "[", "pathway", "]", ",", "\n", "dim_out", "[", "pathway", "]", "//", "2", ",", "\n", "nonlocal_pool", "[", "pathway", "]", ",", "\n", "instantiation", "=", "instantiation", ",", "\n", "norm_module", "=", "norm_module", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\n", "\"pathway{}_nonlocal{}\"", ".", "format", "(", "pathway", ",", "i", ")", ",", "nln", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.ResStage.forward": [[529, 561], ["range", "range", "output.append", "getattr", "getattr.", "hasattr", "getattr", "getattr.", "x.permute.permute.permute", "x.permute.permute.reshape", "x.permute.permute.permute", "x.permute.permute.permute", "x.permute.permute.reshape", "x.permute.permute.permute"], "methods", ["None"], ["", "", "", "", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "output", "=", "[", "]", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "x", "=", "inputs", "[", "pathway", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_blocks", "[", "pathway", "]", ")", ":", "\n", "                ", "m", "=", "getattr", "(", "self", ",", "\"pathway{}_res{}\"", ".", "format", "(", "pathway", ",", "i", ")", ")", "\n", "x", "=", "m", "(", "x", ")", "\n", "if", "hasattr", "(", "self", ",", "\"pathway{}_nonlocal{}\"", ".", "format", "(", "pathway", ",", "i", ")", ")", ":", "\n", "                    ", "nln", "=", "getattr", "(", "\n", "self", ",", "\"pathway{}_nonlocal{}\"", ".", "format", "(", "pathway", ",", "i", ")", "\n", ")", "\n", "b", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "shape", "\n", "if", "self", ".", "nonlocal_group", "[", "pathway", "]", ">", "1", ":", "\n", "# Fold temporal dimension into batch dimension.", "\n", "                        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "x", "=", "x", ".", "reshape", "(", "\n", "b", "*", "self", ".", "nonlocal_group", "[", "pathway", "]", ",", "\n", "t", "//", "self", ".", "nonlocal_group", "[", "pathway", "]", ",", "\n", "c", ",", "\n", "h", ",", "\n", "w", ",", "\n", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "", "x", "=", "nln", "(", "x", ")", "\n", "if", "self", ".", "nonlocal_group", "[", "pathway", "]", ">", "1", ":", "\n", "# Fold back to temporal dimension.", "\n", "                        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "x", "=", "x", ".", "reshape", "(", "b", ",", "t", ",", "c", ",", "h", ",", "w", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "", "", "", "output", ".", "append", "(", "x", ")", "\n", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.resnet_helper.get_trans_func": [[10, 22], ["trans_funcs.keys"], "function", ["None"], ["def", "get_trans_func", "(", "name", ")", ":", "\n", "    ", "\"\"\"\n    Retrieves the transformation module by name.\n    \"\"\"", "\n", "trans_funcs", "=", "{", "\n", "\"bottleneck_transform\"", ":", "BottleneckTransform", ",", "\n", "\"basic_transform\"", ":", "BasicTransform", ",", "\n", "}", "\n", "assert", "(", "\n", "name", "in", "trans_funcs", ".", "keys", "(", ")", "\n", ")", ",", "\"Transformation function '{}' not supported\"", ".", "format", "(", "name", ")", "\n", "return", "trans_funcs", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.FuseFastToSlow.__init__": [[85, 127], ["torch.Module.__init__", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "norm_module", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "fusion_conv_channel_ratio", ",", "\n", "fusion_kernel", ",", "\n", "alpha", ",", "\n", "eps", "=", "1e-5", ",", "\n", "bn_mmt", "=", "0.1", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dim_in (int): the channel dimension of the input.\n            fusion_conv_channel_ratio (int): channel ratio for the convolution\n                used to fuse from Fast pathway to Slow pathway.\n            fusion_kernel (int): kernel size of the convolution used to fuse\n                from Fast pathway to Slow pathway.\n            alpha (int): the frame rate ratio between the Fast and Slow pathway.\n            eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            inplace_relu (bool): if True, calculate the relu on the original\n                input without allocating new memory.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "FuseFastToSlow", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_f2s", "=", "nn", ".", "Conv3d", "(", "\n", "dim_in", ",", "\n", "dim_in", "*", "fusion_conv_channel_ratio", ",", "\n", "kernel_size", "=", "[", "fusion_kernel", ",", "1", ",", "1", "]", ",", "\n", "stride", "=", "[", "alpha", ",", "1", ",", "1", "]", ",", "\n", "padding", "=", "[", "fusion_kernel", "//", "2", ",", "0", ",", "0", "]", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_in", "*", "fusion_conv_channel_ratio", ",", "\n", "eps", "=", "eps", ",", "\n", "momentum", "=", "bn_mmt", ",", "\n", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace_relu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.FuseFastToSlow.forward": [[128, 136], ["video_model_builder.FuseFastToSlow.conv_f2s", "video_model_builder.FuseFastToSlow.bn", "video_model_builder.FuseFastToSlow.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_s", "=", "x", "[", "0", "]", "\n", "x_f", "=", "x", "[", "1", "]", "\n", "fuse", "=", "self", ".", "conv_f2s", "(", "x_f", ")", "\n", "fuse", "=", "self", ".", "bn", "(", "fuse", ")", "\n", "fuse", "=", "self", ".", "relu", "(", "fuse", ")", "\n", "x_s_fuse", "=", "torch", ".", "cat", "(", "[", "x_s", ",", "fuse", "]", ",", "1", ")", "\n", "return", "[", "x_s_fuse", ",", "x_f", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.SlowFast.__init__": [[148, 164], ["torch.Module.__init__", "slowfast.models.batchnorm_helper.get_norm", "video_model_builder.SlowFast._construct_network", "slowfast.init_weights"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.get_norm", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.ResNet._construct_network", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.weight_init_helper.init_weights"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"", "\n", "super", "(", "SlowFast", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm_module", "=", "get_norm", "(", "cfg", ")", "\n", "self", ".", "enable_detection", "=", "cfg", ".", "DETECTION", ".", "ENABLE", "\n", "self", ".", "enable_lstc", "=", "cfg", ".", "LSTC", ".", "ENABLE", "\n", "self", ".", "num_pathways", "=", "2", "\n", "self", ".", "_construct_network", "(", "cfg", ")", "\n", "init_helper", ".", "init_weights", "(", "\n", "self", ",", "cfg", ".", "MODEL", ".", "FC_INIT_STD", ",", "cfg", ".", "RESNET", ".", "ZERO_INIT_FINAL_BN", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.SlowFast._construct_network": [[166, 417], ["stem_helper.VideoModelStem", "video_model_builder.FuseFastToSlow", "resnet_helper.ResStage", "video_model_builder.FuseFastToSlow", "range", "resnet_helper.ResStage", "video_model_builder.FuseFastToSlow", "resnet_helper.ResStage", "video_model_builder.FuseFastToSlow", "resnet_helper.ResStage", "_POOL1.keys", "len", "_MODEL_STAGE_DEPTH.keys", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "video_model_builder.SlowFast.add_module", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "head_helper.ResNetBasicHead", "head_helper.ResNetPoolHead", "head_helper.ResNetRoIHead", "len"], "methods", ["None"], ["", "def", "_construct_network", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Builds a SlowFast model. The first pathway is the Slow pathway and the\n            second pathway is the Fast pathway.\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"", "\n", "assert", "cfg", ".", "MODEL", ".", "ARCH", "in", "_POOL1", ".", "keys", "(", ")", "\n", "pool_size", "=", "_POOL1", "[", "cfg", ".", "MODEL", ".", "ARCH", "]", "\n", "assert", "len", "(", "{", "len", "(", "pool_size", ")", ",", "self", ".", "num_pathways", "}", ")", "==", "1", "\n", "assert", "cfg", ".", "RESNET", ".", "DEPTH", "in", "_MODEL_STAGE_DEPTH", ".", "keys", "(", ")", "\n", "\n", "(", "d2", ",", "d3", ",", "d4", ",", "d5", ")", "=", "_MODEL_STAGE_DEPTH", "[", "cfg", ".", "RESNET", ".", "DEPTH", "]", "\n", "\n", "num_groups", "=", "cfg", ".", "RESNET", ".", "NUM_GROUPS", "\n", "width_per_group", "=", "cfg", ".", "RESNET", ".", "WIDTH_PER_GROUP", "\n", "dim_inner", "=", "num_groups", "*", "width_per_group", "\n", "out_dim_ratio", "=", "(", "\n", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "//", "cfg", ".", "SLOWFAST", ".", "FUSION_CONV_CHANNEL_RATIO", "\n", ")", "\n", "\n", "temp_kernel", "=", "_TEMPORAL_KERNEL_BASIS", "[", "cfg", ".", "MODEL", ".", "ARCH", "]", "\n", "\n", "self", ".", "s1", "=", "stem_helper", ".", "VideoModelStem", "(", "\n", "dim_in", "=", "cfg", ".", "DATA", ".", "INPUT_CHANNEL_NUM", ",", "\n", "dim_out", "=", "[", "width_per_group", ",", "width_per_group", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "]", ",", "\n", "kernel", "=", "[", "temp_kernel", "[", "0", "]", "[", "0", "]", "+", "[", "7", ",", "7", "]", ",", "temp_kernel", "[", "0", "]", "[", "1", "]", "+", "[", "7", ",", "7", "]", "]", ",", "\n", "stride", "=", "[", "[", "1", ",", "2", ",", "2", "]", "]", "*", "2", ",", "\n", "padding", "=", "[", "\n", "[", "temp_kernel", "[", "0", "]", "[", "0", "]", "[", "0", "]", "//", "2", ",", "3", ",", "3", "]", ",", "\n", "[", "temp_kernel", "[", "0", "]", "[", "1", "]", "[", "0", "]", "//", "2", ",", "3", ",", "3", "]", ",", "\n", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "self", ".", "s1_fuse", "=", "FuseFastToSlow", "(", "\n", "width_per_group", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_CONV_CHANNEL_RATIO", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_KERNEL_SZ", ",", "\n", "cfg", ".", "SLOWFAST", ".", "ALPHA", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "self", ".", "s2", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "\n", "width_per_group", "+", "width_per_group", "//", "out_dim_ratio", ",", "\n", "width_per_group", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_out", "=", "[", "\n", "width_per_group", "*", "4", ",", "\n", "width_per_group", "*", "4", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", ",", "dim_inner", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "1", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "0", "]", ",", "\n", "num_blocks", "=", "[", "d2", "]", "*", "2", ",", "\n", "num_groups", "=", "[", "num_groups", "]", "*", "2", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "0", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "0", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "0", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "0", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "0", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "self", ".", "s2_fuse", "=", "FuseFastToSlow", "(", "\n", "width_per_group", "*", "4", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_CONV_CHANNEL_RATIO", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_KERNEL_SZ", ",", "\n", "cfg", ".", "SLOWFAST", ".", "ALPHA", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "pool", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "pool_size", "[", "pathway", "]", ",", "\n", "stride", "=", "pool_size", "[", "pathway", "]", ",", "\n", "padding", "=", "[", "0", ",", "0", ",", "0", "]", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"pathway{}_pool\"", ".", "format", "(", "pathway", ")", ",", "pool", ")", "\n", "\n", "", "self", ".", "s3", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "\n", "width_per_group", "*", "4", "+", "width_per_group", "*", "4", "//", "out_dim_ratio", ",", "\n", "width_per_group", "*", "4", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_out", "=", "[", "\n", "width_per_group", "*", "8", ",", "\n", "width_per_group", "*", "8", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "*", "2", ",", "dim_inner", "*", "2", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "2", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "1", "]", ",", "\n", "num_blocks", "=", "[", "d3", "]", "*", "2", ",", "\n", "num_groups", "=", "[", "num_groups", "]", "*", "2", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "1", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "1", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "1", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "1", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "1", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "self", ".", "s3_fuse", "=", "FuseFastToSlow", "(", "\n", "width_per_group", "*", "8", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_CONV_CHANNEL_RATIO", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_KERNEL_SZ", ",", "\n", "cfg", ".", "SLOWFAST", ".", "ALPHA", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "self", ".", "s4", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "\n", "width_per_group", "*", "8", "+", "width_per_group", "*", "8", "//", "out_dim_ratio", ",", "\n", "width_per_group", "*", "8", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_out", "=", "[", "\n", "width_per_group", "*", "16", ",", "\n", "width_per_group", "*", "16", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "*", "4", ",", "dim_inner", "*", "4", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "3", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "2", "]", ",", "\n", "num_blocks", "=", "[", "d4", "]", "*", "2", ",", "\n", "num_groups", "=", "[", "num_groups", "]", "*", "2", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "2", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "2", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "2", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "2", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "2", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "self", ".", "s4_fuse", "=", "FuseFastToSlow", "(", "\n", "width_per_group", "*", "16", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_CONV_CHANNEL_RATIO", ",", "\n", "cfg", ".", "SLOWFAST", ".", "FUSION_KERNEL_SZ", ",", "\n", "cfg", ".", "SLOWFAST", ".", "ALPHA", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "self", ".", "s5", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "\n", "width_per_group", "*", "16", "+", "width_per_group", "*", "16", "//", "out_dim_ratio", ",", "\n", "width_per_group", "*", "16", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_out", "=", "[", "\n", "width_per_group", "*", "32", ",", "\n", "width_per_group", "*", "32", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "*", "8", ",", "dim_inner", "*", "8", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "4", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "3", "]", ",", "\n", "num_blocks", "=", "[", "d5", "]", "*", "2", ",", "\n", "num_groups", "=", "[", "num_groups", "]", "*", "2", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "3", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "3", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "3", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "3", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "3", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "if", "cfg", ".", "LSTC", ".", "ENABLE", ":", "\n", "# compress the output channel", "\n", "            ", "self", ".", "slow_out", "=", "nn", ".", "Conv3d", "(", "\n", "width_per_group", "*", "32", ",", "\n", "cfg", ".", "SLOWFAST", ".", "OUTPUT_CHANNEL", "[", "0", "]", ",", "\n", "1", ",", "1", ",", "0", "\n", ")", "\n", "\n", "self", ".", "fast_out", "=", "nn", ".", "Conv3d", "(", "\n", "width_per_group", "*", "32", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", ",", "\n", "cfg", ".", "SLOWFAST", ".", "OUTPUT_CHANNEL", "[", "1", "]", ",", "\n", "1", ",", "1", ",", "0", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "DETECTION", ".", "ENABLE", ":", "\n", "            ", "if", "cfg", ".", "LSTC", ".", "ENABLE", ":", "\n", "                ", "self", ".", "head", "=", "head_helper", ".", "ResNetPoolHead", "(", "\n", "cfg", "=", "cfg", ",", "\n", "dim_in", "=", "cfg", ".", "SLOWFAST", ".", "OUTPUT_CHANNEL", ",", "\n", "num_classes", "=", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "pool_size", "=", "[", "\n", "[", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", "\n", "//", "cfg", ".", "SLOWFAST", ".", "ALPHA", "\n", "//", "pool_size", "[", "0", "]", "[", "0", "]", ",", "\n", "1", ",", "\n", "1", ",", "\n", "]", ",", "\n", "[", "cfg", ".", "DATA", ".", "NUM_FRAMES", "//", "pool_size", "[", "1", "]", "[", "0", "]", ",", "1", ",", "1", "]", ",", "\n", "]", ",", "\n", "resolution", "=", "[", "[", "cfg", ".", "DETECTION", ".", "ROI_XFORM_RESOLUTION", "]", "*", "2", "]", "*", "2", ",", "\n", "scale_factor", "=", "[", "cfg", ".", "DETECTION", ".", "SPATIAL_SCALE_FACTOR", "]", "*", "2", ",", "\n", "dropout_rate", "=", "cfg", ".", "MODEL", ".", "DROPOUT_RATE", ",", "\n", "act_func", "=", "cfg", ".", "MODEL", ".", "HEAD_ACT", ",", "\n", "aligned", "=", "cfg", ".", "DETECTION", ".", "ALIGNED", "\n", ")", "\n", "", "else", ":", "\n", "                ", "dim_in", "=", "[", "\n", "width_per_group", "*", "32", ",", "\n", "width_per_group", "*", "32", "//", "cfg", ".", "SLOWFAST", ".", "BETA_INV", "\n", "]", "\n", "self", ".", "head", "=", "head_helper", ".", "ResNetRoIHead", "(", "\n", "cfg", "=", "cfg", ",", "\n", "dim_in", "=", "dim_in", ",", "\n", "num_classes", "=", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "pool_size", "=", "[", "\n", "[", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", "\n", "//", "cfg", ".", "SLOWFAST", ".", "ALPHA", "\n", "//", "pool_size", "[", "0", "]", "[", "0", "]", ",", "\n", "1", ",", "\n", "1", ",", "\n", "]", ",", "\n", "[", "cfg", ".", "DATA", ".", "NUM_FRAMES", "//", "pool_size", "[", "1", "]", "[", "0", "]", ",", "1", ",", "1", "]", ",", "\n", "]", ",", "\n", "resolution", "=", "[", "[", "cfg", ".", "DETECTION", ".", "ROI_XFORM_RESOLUTION", "]", "*", "2", "]", "*", "2", ",", "\n", "scale_factor", "=", "[", "cfg", ".", "DETECTION", ".", "SPATIAL_SCALE_FACTOR", "]", "*", "2", ",", "\n", "dropout_rate", "=", "cfg", ".", "MODEL", ".", "DROPOUT_RATE", ",", "\n", "act_func", "=", "cfg", ".", "MODEL", ".", "HEAD_ACT", ",", "\n", "aligned", "=", "cfg", ".", "DETECTION", ".", "ALIGNED", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "head", "=", "head_helper", ".", "ResNetBasicHead", "(", "\n", "dim_in", "=", "cfg", ".", "SLOWFAST", ".", "OUTPUT_CHANNEL", ",", "\n", "num_classes", "=", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "pool_size", "=", "[", "None", ",", "None", "]", "\n", "if", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE", "\n", "else", "[", "\n", "[", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", "\n", "//", "cfg", ".", "SLOWFAST", ".", "ALPHA", "\n", "//", "pool_size", "[", "0", "]", "[", "0", "]", ",", "\n", "cfg", ".", "DATA", ".", "CROP_SIZE", "//", "32", "//", "pool_size", "[", "0", "]", "[", "1", "]", ",", "\n", "cfg", ".", "DATA", ".", "CROP_SIZE", "//", "32", "//", "pool_size", "[", "0", "]", "[", "2", "]", ",", "\n", "]", ",", "\n", "[", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", "//", "pool_size", "[", "1", "]", "[", "0", "]", ",", "\n", "cfg", ".", "DATA", ".", "CROP_SIZE", "//", "32", "//", "pool_size", "[", "1", "]", "[", "1", "]", ",", "\n", "cfg", ".", "DATA", ".", "CROP_SIZE", "//", "32", "//", "pool_size", "[", "1", "]", "[", "2", "]", ",", "\n", "]", ",", "\n", "]", ",", "# None for AdaptiveAvgPool3d((1, 1, 1))", "\n", "dropout_rate", "=", "cfg", ".", "MODEL", ".", "DROPOUT_RATE", ",", "\n", "act_func", "=", "cfg", ".", "MODEL", ".", "HEAD_ACT", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.SlowFast.forward": [[419, 442], ["video_model_builder.SlowFast.s1", "video_model_builder.SlowFast.s1_fuse", "video_model_builder.SlowFast.s2", "video_model_builder.SlowFast.s2_fuse", "range", "video_model_builder.SlowFast.s3", "video_model_builder.SlowFast.s3_fuse", "video_model_builder.SlowFast.s4", "video_model_builder.SlowFast.s4_fuse", "video_model_builder.SlowFast.s5", "getattr", "getattr.", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "video_model_builder.SlowFast.head", "video_model_builder.SlowFast.slow_out", "video_model_builder.SlowFast.fast_out", "video_model_builder.SlowFast.head", "video_model_builder.SlowFast.head"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "*", ",", "bboxes", "=", "None", ",", "extract", "=", "False", ",", "FBs", "=", "None", ",", "BTs", "=", "None", ")", ":", "\n", "        ", "x", "=", "self", ".", "s1", "(", "x", ")", "\n", "x", "=", "self", ".", "s1_fuse", "(", "x", ")", "\n", "x", "=", "self", ".", "s2", "(", "x", ")", "\n", "x", "=", "self", ".", "s2_fuse", "(", "x", ")", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "pool", "=", "getattr", "(", "self", ",", "\"pathway{}_pool\"", ".", "format", "(", "pathway", ")", ")", "\n", "x", "[", "pathway", "]", "=", "pool", "(", "x", "[", "pathway", "]", ")", "\n", "", "x", "=", "self", ".", "s3", "(", "x", ")", "\n", "x", "=", "self", ".", "s3_fuse", "(", "x", ")", "\n", "x", "=", "self", ".", "s4", "(", "x", ")", "\n", "x", "=", "self", ".", "s4_fuse", "(", "x", ")", "\n", "x", "=", "self", ".", "s5", "(", "x", ")", "\n", "\n", "if", "self", ".", "enable_detection", "and", "self", ".", "enable_lstc", ":", "\n", "            ", "x", "[", "0", "]", "=", "F", ".", "relu", "(", "self", ".", "slow_out", "(", "x", "[", "0", "]", ")", ")", "\n", "x", "[", "1", "]", "=", "F", ".", "relu", "(", "self", ".", "fast_out", "(", "x", "[", "1", "]", ")", ")", "\n", "x", "=", "self", ".", "head", "(", "x", ",", "bboxes", ",", "extract", ",", "FBs", ",", "BTs", ")", "\n", "", "elif", "self", ".", "enable_detection", ":", "\n", "            ", "x", "=", "self", ".", "head", "(", "x", ",", "bboxes", ",", "extract", ",", "FBs", ",", "BTs", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.ResNet.__init__": [[459, 476], ["torch.Module.__init__", "slowfast.models.batchnorm_helper.get_norm", "video_model_builder.ResNet._construct_network", "slowfast.init_weights"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.batchnorm_helper.get_norm", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.ResNet._construct_network", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.weight_init_helper.init_weights"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"", "\n", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm_module", "=", "get_norm", "(", "cfg", ")", "\n", "self", ".", "enable_detection", "=", "cfg", ".", "DETECTION", ".", "ENABLE", "\n", "self", ".", "enable_lstc", "=", "cfg", ".", "LSTC", ".", "ENABLE", "\n", "self", ".", "num_pathways", "=", "1", "\n", "self", ".", "_construct_network", "(", "cfg", ")", "\n", "init_helper", ".", "init_weights", "(", "\n", "self", ",", "cfg", ".", "MODEL", ".", "FC_INIT_STD", ",", "cfg", ".", "RESNET", ".", "ZERO_INIT_FINAL_BN", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.ResNet._construct_network": [[478, 623], ["stem_helper.VideoModelStem", "resnet_helper.ResStage", "range", "resnet_helper.ResStage", "resnet_helper.ResStage", "resnet_helper.ResStage", "_POOL1.keys", "len", "_MODEL_STAGE_DEPTH.keys", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "video_model_builder.ResNet.add_module", "head_helper.ResNetPoolHead", "head_helper.ResNetBasicHead", "len"], "methods", ["None"], ["", "def", "_construct_network", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"\n        Builds a single pathway ResNet model.\n\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"", "\n", "assert", "cfg", ".", "MODEL", ".", "ARCH", "in", "_POOL1", ".", "keys", "(", ")", "\n", "pool_size", "=", "_POOL1", "[", "cfg", ".", "MODEL", ".", "ARCH", "]", "\n", "assert", "len", "(", "{", "len", "(", "pool_size", ")", ",", "self", ".", "num_pathways", "}", ")", "==", "1", "\n", "assert", "cfg", ".", "RESNET", ".", "DEPTH", "in", "_MODEL_STAGE_DEPTH", ".", "keys", "(", ")", "\n", "\n", "(", "d2", ",", "d3", ",", "d4", ",", "d5", ")", "=", "_MODEL_STAGE_DEPTH", "[", "cfg", ".", "RESNET", ".", "DEPTH", "]", "\n", "\n", "num_groups", "=", "cfg", ".", "RESNET", ".", "NUM_GROUPS", "\n", "width_per_group", "=", "cfg", ".", "RESNET", ".", "WIDTH_PER_GROUP", "\n", "dim_inner", "=", "num_groups", "*", "width_per_group", "\n", "\n", "temp_kernel", "=", "_TEMPORAL_KERNEL_BASIS", "[", "cfg", ".", "MODEL", ".", "ARCH", "]", "\n", "\n", "self", ".", "s1", "=", "stem_helper", ".", "VideoModelStem", "(", "\n", "dim_in", "=", "cfg", ".", "DATA", ".", "INPUT_CHANNEL_NUM", ",", "\n", "dim_out", "=", "[", "width_per_group", "]", ",", "\n", "kernel", "=", "[", "temp_kernel", "[", "0", "]", "[", "0", "]", "+", "[", "7", ",", "7", "]", "]", ",", "\n", "stride", "=", "[", "[", "1", ",", "2", ",", "2", "]", "]", ",", "\n", "padding", "=", "[", "[", "temp_kernel", "[", "0", "]", "[", "0", "]", "[", "0", "]", "//", "2", ",", "3", ",", "3", "]", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "self", ".", "s2", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "width_per_group", "]", ",", "\n", "dim_out", "=", "[", "width_per_group", "*", "4", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "1", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "0", "]", ",", "\n", "num_blocks", "=", "[", "d2", "]", ",", "\n", "num_groups", "=", "[", "num_groups", "]", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "0", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "0", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "0", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "0", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "stride_1x1", "=", "cfg", ".", "RESNET", ".", "STRIDE_1X1", ",", "\n", "inplace_relu", "=", "cfg", ".", "RESNET", ".", "INPLACE_RELU", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "0", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "pool", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "pool_size", "[", "pathway", "]", ",", "\n", "stride", "=", "pool_size", "[", "pathway", "]", ",", "\n", "padding", "=", "[", "0", ",", "0", ",", "0", "]", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"pathway{}_pool\"", ".", "format", "(", "pathway", ")", ",", "pool", ")", "\n", "\n", "", "self", ".", "s3", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "width_per_group", "*", "4", "]", ",", "\n", "dim_out", "=", "[", "width_per_group", "*", "8", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "*", "2", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "2", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "1", "]", ",", "\n", "num_blocks", "=", "[", "d3", "]", ",", "\n", "num_groups", "=", "[", "num_groups", "]", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "1", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "1", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "1", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "1", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "stride_1x1", "=", "cfg", ".", "RESNET", ".", "STRIDE_1X1", ",", "\n", "inplace_relu", "=", "cfg", ".", "RESNET", ".", "INPLACE_RELU", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "1", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "self", ".", "s4", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "width_per_group", "*", "8", "]", ",", "\n", "dim_out", "=", "[", "width_per_group", "*", "16", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "*", "4", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "3", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "2", "]", ",", "\n", "num_blocks", "=", "[", "d4", "]", ",", "\n", "num_groups", "=", "[", "num_groups", "]", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "2", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "2", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "2", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "2", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "stride_1x1", "=", "cfg", ".", "RESNET", ".", "STRIDE_1X1", ",", "\n", "inplace_relu", "=", "cfg", ".", "RESNET", ".", "INPLACE_RELU", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "2", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "self", ".", "s5", "=", "resnet_helper", ".", "ResStage", "(", "\n", "dim_in", "=", "[", "width_per_group", "*", "16", "]", ",", "\n", "dim_out", "=", "[", "width_per_group", "*", "32", "]", ",", "\n", "dim_inner", "=", "[", "dim_inner", "*", "8", "]", ",", "\n", "temp_kernel_sizes", "=", "temp_kernel", "[", "4", "]", ",", "\n", "stride", "=", "cfg", ".", "RESNET", ".", "SPATIAL_STRIDES", "[", "3", "]", ",", "\n", "num_blocks", "=", "[", "d5", "]", ",", "\n", "num_groups", "=", "[", "num_groups", "]", ",", "\n", "num_block_temp_kernel", "=", "cfg", ".", "RESNET", ".", "NUM_BLOCK_TEMP_KERNEL", "[", "3", "]", ",", "\n", "nonlocal_inds", "=", "cfg", ".", "NONLOCAL", ".", "LOCATION", "[", "3", "]", ",", "\n", "nonlocal_group", "=", "cfg", ".", "NONLOCAL", ".", "GROUP", "[", "3", "]", ",", "\n", "nonlocal_pool", "=", "cfg", ".", "NONLOCAL", ".", "POOL", "[", "3", "]", ",", "\n", "instantiation", "=", "cfg", ".", "NONLOCAL", ".", "INSTANTIATION", ",", "\n", "trans_func_name", "=", "cfg", ".", "RESNET", ".", "TRANS_FUNC", ",", "\n", "stride_1x1", "=", "cfg", ".", "RESNET", ".", "STRIDE_1X1", ",", "\n", "inplace_relu", "=", "cfg", ".", "RESNET", ".", "INPLACE_RELU", ",", "\n", "dilation", "=", "cfg", ".", "RESNET", ".", "SPATIAL_DILATIONS", "[", "3", "]", ",", "\n", "norm_module", "=", "self", ".", "norm_module", ",", "\n", ")", "\n", "\n", "if", "self", ".", "enable_detection", ":", "\n", "            ", "self", ".", "head", "=", "head_helper", ".", "ResNetPoolHead", "(", "\n", "cfg", "=", "cfg", ",", "\n", "dim_in", "=", "[", "width_per_group", "*", "32", "]", ",", "\n", "num_classes", "=", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "pool_size", "=", "[", "[", "cfg", ".", "DATA", ".", "NUM_FRAMES", "//", "pool_size", "[", "0", "]", "[", "0", "]", ",", "1", ",", "1", "]", "]", ",", "\n", "resolution", "=", "[", "[", "cfg", ".", "DETECTION", ".", "ROI_XFORM_RESOLUTION", "]", "*", "2", "]", ",", "\n", "scale_factor", "=", "[", "cfg", ".", "DETECTION", ".", "SPATIAL_SCALE_FACTOR", "]", ",", "\n", "dropout_rate", "=", "cfg", ".", "MODEL", ".", "DROPOUT_RATE", ",", "\n", "act_func", "=", "cfg", ".", "MODEL", ".", "HEAD_ACT", ",", "\n", "aligned", "=", "cfg", ".", "DETECTION", ".", "ALIGNED", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "head", "=", "head_helper", ".", "ResNetBasicHead", "(", "\n", "dim_in", "=", "[", "width_per_group", "*", "32", "]", ",", "\n", "num_classes", "=", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ",", "\n", "pool_size", "=", "[", "None", ",", "None", "]", "\n", "if", "cfg", ".", "MULTIGRID", ".", "SHORT_CYCLE", "\n", "else", "[", "\n", "[", "\n", "cfg", ".", "DATA", ".", "NUM_FRAMES", "//", "pool_size", "[", "0", "]", "[", "0", "]", ",", "\n", "cfg", ".", "DATA", ".", "CROP_SIZE", "//", "32", "//", "pool_size", "[", "0", "]", "[", "1", "]", ",", "\n", "cfg", ".", "DATA", ".", "CROP_SIZE", "//", "32", "//", "pool_size", "[", "0", "]", "[", "2", "]", ",", "\n", "]", "\n", "]", ",", "# None for AdaptiveAvgPool3d((1, 1, 1))", "\n", "dropout_rate", "=", "cfg", ".", "MODEL", ".", "DROPOUT_RATE", ",", "\n", "act_func", "=", "cfg", ".", "MODEL", ".", "HEAD_ACT", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.video_model_builder.ResNet.forward": [[625, 641], ["video_model_builder.ResNet.s1", "video_model_builder.ResNet.s2", "range", "video_model_builder.ResNet.s3", "video_model_builder.ResNet.s4", "video_model_builder.ResNet.s5", "getattr", "getattr.", "video_model_builder.ResNet.head", "video_model_builder.ResNet.head"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "*", ",", "bboxes", "=", "None", ",", "extract", "=", "False", ",", "FBs", "=", "None", ",", "BTs", "=", "None", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "s1", "(", "x", ")", "\n", "x", "=", "self", ".", "s2", "(", "x", ")", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "pool", "=", "getattr", "(", "self", ",", "\"pathway{}_pool\"", ".", "format", "(", "pathway", ")", ")", "\n", "x", "[", "pathway", "]", "=", "pool", "(", "x", "[", "pathway", "]", ")", "\n", "", "x", "=", "self", ".", "s3", "(", "x", ")", "\n", "x", "=", "self", ".", "s4", "(", "x", ")", "\n", "x", "=", "self", ".", "s5", "(", "x", ")", "\n", "\n", "if", "self", ".", "enable_detection", ":", "\n", "            ", "x", "=", "self", ".", "head", "(", "x", ",", "bboxes", ",", "extract", ",", "FBs", ",", "BTs", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.bank_model_builder.BankContext.__init__": [[24, 34], ["torch.Module.__init__", "sum", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "bank_model_builder.BankContext._build_aggregators"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.bank_model_builder.BankContext._build_aggregators"], ["def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "super", "(", "BankContext", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "window_size", "=", "cfg", ".", "AVA", ".", "SLIDING_WINDOW_SIZE", "*", "2", "+", "1", "\n", "self", ".", "feature_size", "=", "sum", "(", "cfg", ".", "SLOWFAST", ".", "OUTPUT_CHANNEL", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "feature_size", ",", "cfg", ".", "MODEL", ".", "NUM_CLASSES", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "cfg", ".", "MODEL", ".", "DROPOUT_RATE", ")", "\n", "self", ".", "act_func", "=", "self", ".", "act", "[", "cfg", ".", "MODEL", ".", "HEAD_ACT", "]", "(", ")", "\n", "\n", "self", ".", "_build_aggregators", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.bank_model_builder.BankContext._build_aggregators": [[35, 50], ["torch.LayerNorm", "torch.LayerNorm", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "_build_aggregators", "(", "self", ",", "ratio", "=", "2", ")", ":", "\n", "\n", "        ", "inter_size", "=", "self", ".", "feature_size", "//", "ratio", "\n", "self", ".", "norm1", "=", "nn", ".", "LayerNorm", "(", "self", ".", "feature_size", ")", "\n", "self", ".", "ffn1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", ",", "inter_size", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "inter_size", ",", "self", ".", "feature_size", ")", "\n", ")", "\n", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "self", ".", "feature_size", ")", "\n", "self", ".", "ffn2", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "feature_size", ",", "inter_size", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "inter_size", ",", "self", ".", "feature_size", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.bank_model_builder.BankContext.forward": [[52, 79], ["len", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bank_model_builder.BankContext.dropout", "bank_model_builder.BankContext.act_func", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "bank_model_builder.BankContext.ffn1", "torch.mean().unsqueeze", "torch.mean().unsqueeze", "torch.mean().unsqueeze", "torch.mean().unsqueeze", "output.append", "bank_model_builder.BankContext.classifier", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "bank_model_builder.BankContext.norm1", "bank_model_builder.BankContext.ffn2", "enumerate", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "bank_model_builder.BankContext.norm2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "FBs", ")", ":", "\n", "        ", "\"\"\"\n        aggregate context information from banks\n        Args:\n            FBs: list[torch.Tensor]\n\n        Returns:\n            torch.Tensor\n        \"\"\"", "\n", "num_batch", "=", "len", "(", "FBs", ")", "\n", "output", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "num_batch", ")", ":", "\n", "            ", "feature_bank", "=", "FBs", "[", "b", "]", "\n", "clip_feat", "=", "[", "torch", ".", "mean", "(", "val", ",", "dim", "=", "0", ")", "\n", "for", "k", ",", "val", "in", "enumerate", "(", "feature_bank", ")", "\n", "if", "val", "is", "not", "None", "and", "k", "!=", "self", ".", "window_size", "//", "2", "]", "\n", "\n", "clip_feat", "=", "torch", ".", "stack", "(", "clip_feat", ",", "dim", "=", "0", ")", "\n", "clip_feat", "=", "self", ".", "ffn1", "(", "self", ".", "norm1", "(", "clip_feat", ")", ")", "\n", "\n", "feat", "=", "torch", ".", "mean", "(", "clip_feat", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "output", ".", "append", "(", "self", ".", "ffn2", "(", "self", ".", "norm2", "(", "feat", ")", ")", ")", "\n", "\n", "", "x", "=", "torch", ".", "cat", "(", "output", ",", "dim", "=", "0", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "return", "self", ".", "act_func", "(", "self", ".", "classifier", "(", "x", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.build.build_model": [[18, 51], ["torch.cuda.is_available", "MODEL_REGISTRY.get", "torch.cuda.current_device", "torch.nn.parallel.DistributedDataParallel.cuda", "torch.nn.parallel.DistributedDataParallel", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.ava_evaluation.np_box_list.BoxList.get"], ["\n", "# Capitalize the the first letter of the dataset_name since the dataset_name", "\n", "# in configs may be in lowercase but the name of dataset class should always", "\n", "# start with an uppercase letter.", "\n", "name", "=", "dataset_name", ".", "capitalize", "(", ")", "\n", "return", "DATASET_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "split", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.nonlocal_helper.Nonlocal.__init__": [[20, 65], ["torch.Module.__init__", "nonlocal_helper.Nonlocal._construct_nonlocal", "any"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.nonlocal_helper.Nonlocal._construct_nonlocal"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim", ",", "\n", "dim_inner", ",", "\n", "pool_size", "=", "None", ",", "\n", "instantiation", "=", "\"softmax\"", ",", "\n", "zero_init_final_conv", "=", "False", ",", "\n", "zero_init_final_norm", "=", "True", ",", "\n", "norm_eps", "=", "1e-5", ",", "\n", "norm_momentum", "=", "0.1", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dim (int): number of dimension for the input.\n            dim_inner (int): number of dimension inside of the Non-local block.\n            pool_size (list): the kernel size of spatial temporal pooling,\n                temporal pool kernel size, spatial pool kernel size, spatial\n                pool kernel size in order. By default pool_size is None,\n                then there would be no pooling used.\n            instantiation (string): supports two different instantiation method:\n                \"dot_product\": normalizing correlation matrix with L2.\n                \"softmax\": normalizing correlation matrix with Softmax.\n            zero_init_final_conv (bool): If true, zero initializing the final\n                convolution of the Non-local block.\n            zero_init_final_norm (bool):\n                If true, zero initializing the final batch norm of the Non-local\n                block.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "Nonlocal", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "dim_inner", "=", "dim_inner", "\n", "self", ".", "pool_size", "=", "pool_size", "\n", "self", ".", "instantiation", "=", "instantiation", "\n", "self", ".", "use_pool", "=", "(", "\n", "False", "\n", "if", "pool_size", "is", "None", "\n", "else", "any", "(", "(", "size", ">", "1", "for", "size", "in", "pool_size", ")", ")", "\n", ")", "\n", "self", ".", "norm_eps", "=", "norm_eps", "\n", "self", ".", "norm_momentum", "=", "norm_momentum", "\n", "self", ".", "_construct_nonlocal", "(", "\n", "zero_init_final_conv", ",", "zero_init_final_norm", ",", "norm_module", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.nonlocal_helper.Nonlocal._construct_nonlocal": [[67, 103], ["torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "norm_module", "torch.MaxPool3d", "torch.MaxPool3d"], "methods", ["None"], ["", "def", "_construct_nonlocal", "(", "\n", "self", ",", "zero_init_final_conv", ",", "zero_init_final_norm", ",", "norm_module", "\n", ")", ":", "\n", "# Three convolution heads: theta, phi, and g.", "\n", "        ", "self", ".", "conv_theta", "=", "nn", ".", "Conv3d", "(", "\n", "self", ".", "dim", ",", "self", ".", "dim_inner", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", "\n", ")", "\n", "self", ".", "conv_phi", "=", "nn", ".", "Conv3d", "(", "\n", "self", ".", "dim", ",", "self", ".", "dim_inner", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", "\n", ")", "\n", "self", ".", "conv_g", "=", "nn", ".", "Conv3d", "(", "\n", "self", ".", "dim", ",", "self", ".", "dim_inner", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", "\n", ")", "\n", "\n", "# Final convolution output.", "\n", "self", ".", "conv_out", "=", "nn", ".", "Conv3d", "(", "\n", "self", ".", "dim_inner", ",", "self", ".", "dim", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", "\n", ")", "\n", "# Zero initializing the final convolution output.", "\n", "self", ".", "conv_out", ".", "zero_init", "=", "zero_init_final_conv", "\n", "\n", "# TODO: change the name to `norm`", "\n", "self", ".", "bn", "=", "norm_module", "(", "\n", "num_features", "=", "self", ".", "dim", ",", "\n", "eps", "=", "self", ".", "norm_eps", ",", "\n", "momentum", "=", "self", ".", "norm_momentum", ",", "\n", ")", "\n", "# Zero initializing the final bn.", "\n", "self", ".", "bn", ".", "transform_final_bn", "=", "zero_init_final_norm", "\n", "\n", "# Optional to add the spatial-temporal pooling.", "\n", "if", "self", ".", "use_pool", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "self", ".", "pool_size", ",", "\n", "stride", "=", "self", ".", "pool_size", ",", "\n", "padding", "=", "[", "0", ",", "0", ",", "0", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.nonlocal_helper.Nonlocal.forward": [[105, 149], ["nonlocal_helper.Nonlocal.size", "nonlocal_helper.Nonlocal.conv_theta", "nonlocal_helper.Nonlocal.conv_phi", "nonlocal_helper.Nonlocal.conv_g", "theta.view.view.view", "phi.view.view.view", "g.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "theta_phi_g.view.view.view", "nonlocal_helper.Nonlocal.conv_out", "nonlocal_helper.Nonlocal.bn", "nonlocal_helper.Nonlocal.pool", "torch.functional.softmax", "torch.functional.softmax", "NotImplementedError"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_identity", "=", "x", "\n", "N", ",", "C", ",", "T", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "\n", "theta", "=", "self", ".", "conv_theta", "(", "x", ")", "\n", "\n", "# Perform temporal-spatial pooling to reduce the computation.", "\n", "if", "self", ".", "use_pool", ":", "\n", "            ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "\n", "", "phi", "=", "self", ".", "conv_phi", "(", "x", ")", "\n", "g", "=", "self", ".", "conv_g", "(", "x", ")", "\n", "\n", "theta", "=", "theta", ".", "view", "(", "N", ",", "self", ".", "dim_inner", ",", "-", "1", ")", "\n", "phi", "=", "phi", ".", "view", "(", "N", ",", "self", ".", "dim_inner", ",", "-", "1", ")", "\n", "g", "=", "g", ".", "view", "(", "N", ",", "self", ".", "dim_inner", ",", "-", "1", ")", "\n", "\n", "# (N, C, TxHxW) * (N, C, TxHxW) => (N, TxHxW, TxHxW).", "\n", "theta_phi", "=", "torch", ".", "einsum", "(", "\"nct,ncp->ntp\"", ",", "(", "theta", ",", "phi", ")", ")", "\n", "# For original Non-local paper, there are two main ways to normalize", "\n", "# the affinity tensor:", "\n", "#   1) Softmax normalization (norm on exp).", "\n", "#   2) dot_product normalization.", "\n", "if", "self", ".", "instantiation", "==", "\"softmax\"", ":", "\n", "# Normalizing the affinity tensor theta_phi before softmax.", "\n", "            ", "theta_phi", "=", "theta_phi", "*", "(", "self", ".", "dim_inner", "**", "-", "0.5", ")", "\n", "theta_phi", "=", "nn", ".", "functional", ".", "softmax", "(", "theta_phi", ",", "dim", "=", "2", ")", "\n", "", "elif", "self", ".", "instantiation", "==", "\"dot_product\"", ":", "\n", "            ", "spatial_temporal_dim", "=", "theta_phi", ".", "shape", "[", "2", "]", "\n", "theta_phi", "=", "theta_phi", "/", "spatial_temporal_dim", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"Unknown norm type {}\"", ".", "format", "(", "self", ".", "instantiation", ")", "\n", ")", "\n", "\n", "# (N, TxHxW, TxHxW) * (N, C, TxHxW) => (N, C, TxHxW).", "\n", "", "theta_phi_g", "=", "torch", ".", "einsum", "(", "\"ntg,ncg->nct\"", ",", "(", "theta_phi", ",", "g", ")", ")", "\n", "\n", "# (N, C, TxHxW) => (N, C, T, H, W).", "\n", "theta_phi_g", "=", "theta_phi_g", ".", "view", "(", "N", ",", "self", ".", "dim_inner", ",", "T", ",", "H", ",", "W", ")", "\n", "\n", "p", "=", "self", ".", "conv_out", "(", "theta_phi_g", ")", "\n", "p", "=", "self", ".", "bn", "(", "p", ")", "\n", "return", "x_identity", "+", "p", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.head_helper.ResNetPoolHead.__init__": [[18, 158], ["torch.Module.__init__", "len", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Linear", "torch.Linear", "torch.Linear", "len", "sum", "sum", "torch.AvgPool3d", "torch.AvgPool3d", "torch.AvgPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "head_helper.ResNetPoolHead.add_module", "head_helper.ResNetPoolHead.add_module", "detectron2.layers.ROIAlign", "head_helper.ResNetPoolHead.add_module", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "head_helper.ResNetPoolHead.add_module", "head_helper.ResNetPoolHead.add_module", "slowfast.models.context_helper.ContextModule", "head_helper.ResNetPoolHead.add_module", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Softmax", "torch.Softmax", "torch.Softmax", "slowfast.models.context_helper.ReaderUnit", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "NotImplementedError", "len", "len", "slowfast.models.context_helper.FFN", "slowfast.models.context_helper.FFN", "range", "range", "range", "sum", "sum", "sum", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "cfg", ",", "\n", "dim_in", ",", "\n", "num_classes", ",", "\n", "pool_size", ",", "\n", "resolution", ",", "\n", "scale_factor", ",", "\n", "dropout_rate", "=", "0.0", ",", "\n", "act_func", "=", "\"softmax\"", ",", "\n", "aligned", "=", "True", "\n", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        ResNetRoIHead takes p pathways as input where p in [1, infty].\n\n        Args:\n            dim_in (list): the list of channel dimensions of the p inputs to the\n                ResNetHead.\n            num_classes (int): the channel dimensions of the p outputs to the\n                ResNetHead.\n            pool_size (list): the list of kernel sizes of p spatial temporal\n                poolings, temporal pool kernel size, spatial pool kernel size,\n                spatial pool kernel size in order.\n            resolution (list): the list of spatial output size from the ROIAlign.\n            scale_factor (list): the list of ratio to the input boxes by this\n                number.\n            dropout_rate (float): dropout rate. If equal to 0.0, perform no\n                dropout.\n            act_func (string): activation function to use. 'softmax': applies\n                softmax on the output. 'sigmoid': applies sigmoid on the output.\n            aligned (bool): if False, use the legacy implementation. If True,\n                align the results more perfectly.\n        Note:\n            Given a continuous coordinate c, its two neighboring pixel indices\n            (in our pixel model) are computed by floor (c - 0.5) and ceil\n            (c - 0.5). For example, c=1.3 has pixel neighbors with discrete\n            indices [0] and [1] (which are sampled from the underlying signal at\n            continuous coordinates 0.5 and 1.5). But the original roi_align\n            (aligned=False) does not subtract the 0.5 when computing neighboring\n            pixel indices and therefore it uses pixels with a slightly incorrect\n            alignment (relative to our pixel model) when performing bilinear\n            interpolation.\n            With `aligned=True`, we first appropriately scale the ROI and then\n            shift it by -0.5 prior to calling roi_align. This produces the\n            correct neighbors; It makes negligible differences to the model's\n            performance if ROIAlign is used together with conv layers.\n        \"\"\"", "\n", "super", "(", "ResNetPoolHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "\n", "len", "(", "{", "len", "(", "pool_size", ")", ",", "len", "(", "dim_in", ")", "}", ")", "==", "1", "\n", ")", ",", "\"pathway dimensions are not consistent.\"", "\n", "self", ".", "num_pathways", "=", "len", "(", "pool_size", ")", "\n", "output_dim", "=", "sum", "(", "dim_in", ")", "+", "sum", "(", "cfg", ".", "LSTC", ".", "SHORT_CONTEXT_VAL", ")", "\n", "\n", "self", ".", "num_readers", "=", "cfg", ".", "LSTC", ".", "NUM_READERS", "\n", "self", ".", "ff_feat", "=", "nn", ".", "Sequential", "(", "\n", "*", "[", "\n", "FFN", "(", "\n", "mem_dim", "=", "sum", "(", "dim_in", ")", ",", "\n", "key_dim", "=", "sum", "(", "cfg", ".", "LSTC", ".", "SHORT_CONTEXT_VAL", ")", "\n", ")", "for", "_", "in", "range", "(", "self", ".", "num_readers", ")", "\n", "]", "\n", ")", "\n", "self", ".", "ff_ctx", "=", "nn", ".", "Sequential", "(", "\n", "*", "[", "\n", "FFN", "(", "\n", "mem_dim", "=", "sum", "(", "cfg", ".", "LSTC", ".", "SHORT_CONTEXT_VAL", ")", ",", "\n", "key_dim", "=", "sum", "(", "dim_in", ")", "\n", ")", "for", "_", "in", "range", "(", "self", ".", "num_readers", ")", "\n", "]", "\n", ")", "\n", "\n", "# build necessary readers for feature banks", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "bank_dim", "=", "cfg", ".", "AVA", ".", "FEATURE_BANK_DIM", "\n", "\n", "self", ".", "bank_classifier", "=", "nn", ".", "Linear", "(", "bank_dim", ",", "num_classes", ")", "\n", "\n", "self", ".", "ff_feat_bank", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "ReaderUnit", "(", "\n", "query_dim_in", "=", "sum", "(", "dim_in", ")", "if", "i", "==", "0", "else", "bank_dim", ",", "\n", "dim_in", "=", "bank_dim", ",", "\n", "window_size", "=", "cfg", ".", "AVA", ".", "SLIDING_WINDOW_SIZE", ",", "\n", "embed_size", "=", "cfg", ".", "AVA", ".", "TEMPORAL_EMBED", ",", "\n", "num_pairs", "=", "cfg", ".", "LSTC", ".", "NUM_PAIRS", "\n", ")", "for", "i", "in", "range", "(", "self", ".", "num_readers", ")", "\n", "]", "\n", ")", "\n", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "temporal_pool", "=", "nn", ".", "AvgPool3d", "(", "\n", "[", "pool_size", "[", "pathway", "]", "[", "0", "]", ",", "1", ",", "1", "]", ",", "stride", "=", "1", "\n", ")", "\n", "temporal_pool_max", "=", "nn", ".", "MaxPool3d", "(", "\n", "[", "pool_size", "[", "pathway", "]", "[", "0", "]", ",", "1", ",", "1", "]", ",", "stride", "=", "1", "\n", ")", "\n", "self", ".", "add_module", "(", "\"s{}_tpool\"", ".", "format", "(", "pathway", ")", ",", "temporal_pool", ")", "\n", "self", ".", "add_module", "(", "\"s{}_tpool_max\"", ".", "format", "(", "pathway", ")", ",", "temporal_pool_max", ")", "\n", "\n", "roi_align", "=", "ROIAlign", "(", "\n", "resolution", "[", "pathway", "]", ",", "\n", "spatial_scale", "=", "1.0", "/", "scale_factor", "[", "pathway", "]", ",", "\n", "sampling_ratio", "=", "0", ",", "\n", "aligned", "=", "aligned", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"s{}_roi\"", ".", "format", "(", "pathway", ")", ",", "roi_align", ")", "\n", "spatial_pool", "=", "nn", ".", "MaxPool2d", "(", "resolution", "[", "pathway", "]", ",", "stride", "=", "1", ")", "\n", "spatial_pool_avg", "=", "nn", ".", "AvgPool2d", "(", "resolution", "[", "pathway", "]", ",", "stride", "=", "1", ")", "\n", "self", ".", "add_module", "(", "\"s{}_spool\"", ".", "format", "(", "pathway", ")", ",", "spatial_pool", ")", "\n", "self", ".", "add_module", "(", "\"s{}_spool_avg\"", ".", "format", "(", "pathway", ")", ",", "spatial_pool_avg", ")", "\n", "\n", "context", "=", "ContextModule", "(", "\n", "dim_in", "=", "dim_in", "[", "pathway", "]", ",", "\n", "dim_ctx_in", "=", "dim_in", "[", "pathway", "]", ",", "\n", "dim_key", "=", "cfg", ".", "LSTC", ".", "SHORT_CONTEXT_KEY", "[", "pathway", "]", ",", "\n", "dim_val", "=", "cfg", ".", "LSTC", ".", "SHORT_CONTEXT_VAL", "[", "pathway", "]", "\n", ")", "\n", "self", ".", "add_module", "(", "'s{}_context'", ".", "format", "(", "pathway", ")", ",", "context", ")", "\n", "\n", "", "if", "dropout_rate", ">", "0.0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "\n", "output_dim", ",", "\n", "num_classes", ",", "\n", "bias", "=", "False", "\n", ")", "\n", "\n", "# Softmax for evaluation and testing.", "\n", "if", "act_func", "==", "\"softmax\"", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "", "elif", "act_func", "==", "\"sigmoid\"", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"{} is not supported as an activation\"", "\n", "\"function.\"", ".", "format", "(", "act_func", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.head_helper.ResNetPoolHead.forward": [[160, 237], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "head_helper.ResNetPoolHead.clone", "getattr.clone", "head_helper.ResNetPoolHead.ff_feat", "head_helper.ResNetPoolHead.ff_ctx", "enumerate", "hasattr", "head_helper.ResNetPoolHead.bank_classifier", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "head_helper.ResNetPoolHead.view", "head_helper.ResNetPoolHead.classifier", "head_helper.ResNetPoolHead.act", "head_helper.ResNetPoolHead.act", "head_helper.ResNetPoolHead.act", "len", "getattr", "getattr.", "getattr", "getattr.", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "getattr", "getattr.", "getattr.", "getattr", "getattr.", "getattr", "getattr.", "out.view.view.view", "pool_out.append", "getattr", "getattr.", "ctx_out.append", "m", "head_helper.ResNetPoolHead.dropout", "head_helper.ResNetPoolHead.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "bboxes", ",", "extract", "=", "False", ",", "FBs", "=", "None", ",", "BTs", "=", "None", ")", ":", "\n", "        ", "assert", "(", "\n", "len", "(", "inputs", ")", "==", "self", ".", "num_pathways", "\n", ")", ",", "\"Input tensor does not contain {} pathway\"", ".", "format", "(", "self", ".", "num_pathways", ")", "\n", "pool_out", "=", "[", "]", "\n", "ctx_out", "=", "[", "]", "\n", "box_idx", "=", "bboxes", "[", ":", ",", "0", "]", "\n", "\n", "# attns = []", "\n", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "\n", "# local feature", "\n", "            ", "t_pool", "=", "getattr", "(", "self", ",", "\"s{}_tpool\"", ".", "format", "(", "pathway", ")", ")", "\n", "out_t_avg", "=", "t_pool", "(", "inputs", "[", "pathway", "]", ")", "\n", "t_pool", "=", "getattr", "(", "self", ",", "\"s{}_tpool_max\"", ".", "format", "(", "pathway", ")", ")", "\n", "out_t_max", "=", "t_pool", "(", "inputs", "[", "pathway", "]", ")", "\n", "assert", "out_t_avg", ".", "shape", "[", "2", "]", "==", "1", "\n", "out_t_avg", "=", "torch", ".", "squeeze", "(", "out_t_avg", ",", "2", ")", "\n", "out_t_max", "=", "torch", ".", "squeeze", "(", "out_t_max", ",", "2", ")", "\n", "\n", "roi_align", "=", "getattr", "(", "self", ",", "\"s{}_roi\"", ".", "format", "(", "pathway", ")", ")", "\n", "out_t_avg", "=", "roi_align", "(", "out_t_avg", ",", "bboxes", ")", "\n", "out_t_max", "=", "roi_align", "(", "out_t_max", ",", "bboxes", ")", "\n", "s_pool", "=", "getattr", "(", "self", ",", "\"s{}_spool\"", ".", "format", "(", "pathway", ")", ")", "\n", "out_t_avg", "=", "s_pool", "(", "out_t_avg", ")", "\n", "s_pool", "=", "getattr", "(", "self", ",", "\"s{}_spool_avg\"", ".", "format", "(", "pathway", ")", ")", "\n", "out_t_max", "=", "s_pool", "(", "out_t_max", ")", "\n", "out", "=", "(", "out_t_max", "+", "out_t_avg", ")", "/", "2", "\n", "out", "=", "out", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "pool_out", ".", "append", "(", "out", ")", "\n", "\n", "# context feature", "\n", "ctx", "=", "getattr", "(", "self", ",", "\"s{}_context\"", ".", "format", "(", "pathway", ")", ")", "\n", "path_context", ",", "attn", "=", "ctx", "(", "out", ",", "inputs", "[", "pathway", "]", ",", "box_idx", ")", "\n", "ctx_out", ".", "append", "(", "path_context", ")", "\n", "# attns.append(attn)", "\n", "\n", "# B C H W.", "\n", "", "feat", "=", "torch", ".", "cat", "(", "pool_out", ",", "dim", "=", "1", ")", "\n", "ctx", "=", "torch", ".", "cat", "(", "ctx_out", ",", "dim", "=", "1", ")", "\n", "\n", "feat_out", "=", "feat", ".", "clone", "(", ")", "\n", "context_out", "=", "ctx", ".", "clone", "(", ")", "\n", "\n", "feat", "=", "self", ".", "ff_feat", "(", "feat", ")", "\n", "ctx", "=", "self", ".", "ff_ctx", "(", "ctx", ")", "\n", "\n", "bank_feat_out", "=", "feat", "\n", "for", "i", ",", "m", "in", "enumerate", "(", "self", ".", "ff_feat_bank", ")", ":", "\n", "            ", "bank_feat_out", "=", "m", "(", "bank_feat_out", ",", "FBs", ",", "BTs", ",", "box_idx", ",", "residual", "=", "False", "if", "i", "==", "0", "else", "True", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ",", "'dropout'", ")", ":", "\n", "            ", "bank_feat_out", "=", "self", ".", "dropout", "(", "bank_feat_out", ")", "\n", "", "Zl", "=", "self", ".", "bank_classifier", "(", "bank_feat_out", ")", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "feat", ",", "ctx", "]", ",", "dim", "=", "1", ")", "\n", "# Perform dropout.", "\n", "if", "hasattr", "(", "self", ",", "\"dropout\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x_out", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x_out", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# x = self.projection(x)", "\n", "Zs", "=", "self", ".", "classifier", "(", "x", ")", "\n", "\n", "sub_pred", "=", "self", ".", "act", "(", "Zs", ")", "\n", "bank_pred", "=", "self", ".", "act", "(", "Zl", ")", "\n", "x", "=", "Zl", "+", "Zs", "\n", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "\n", "if", "extract", ":", "\n", "            ", "return", "feat_out", ",", "context_out", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "sub_pred", ",", "bank_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.head_helper.ResNetRoIHead.__init__": [[243, 330], ["torch.Module.__init__", "len", "range", "torch.Linear", "torch.Linear", "torch.Linear", "len", "torch.AvgPool3d", "torch.AvgPool3d", "torch.AvgPool3d", "head_helper.ResNetRoIHead.add_module", "detectron2.layers.ROIAlign", "head_helper.ResNetRoIHead.add_module", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "head_helper.ResNetRoIHead.add_module", "torch.Dropout", "torch.Dropout", "torch.Dropout", "sum", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "NotImplementedError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "cfg", ",", "\n", "dim_in", ",", "\n", "num_classes", ",", "\n", "pool_size", ",", "\n", "resolution", ",", "\n", "scale_factor", ",", "\n", "dropout_rate", "=", "0.0", ",", "\n", "act_func", "=", "\"softmax\"", ",", "\n", "aligned", "=", "True", "\n", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        ResNetRoIHead takes p pathways as input where p in [1, infty].\n\n        Args:\n            dim_in (list): the list of channel dimensions of the p inputs to the\n                ResNetHead.\n            num_classes (int): the channel dimensions of the p outputs to the\n                ResNetHead.\n            pool_size (list): the list of kernel sizes of p spatial temporal\n                poolings, temporal pool kernel size, spatial pool kernel size,\n                spatial pool kernel size in order.\n            resolution (list): the list of spatial output size from the ROIAlign.\n            scale_factor (list): the list of ratio to the input boxes by this\n                number.\n            dropout_rate (float): dropout rate. If equal to 0.0, perform no\n                dropout.\n            act_func (string): activation function to use. 'softmax': applies\n                softmax on the output. 'sigmoid': applies sigmoid on the output.\n            aligned (bool): if False, use the legacy implementation. If True,\n                align the results more perfectly.\n        Note:\n            Given a continuous coordinate c, its two neighboring pixel indices\n            (in our pixel model) are computed by floor (c - 0.5) and ceil\n            (c - 0.5). For example, c=1.3 has pixel neighbors with discrete\n            indices [0] and [1] (which are sampled from the underlying signal at\n            continuous coordinates 0.5 and 1.5). But the original roi_align\n            (aligned=False) does not subtract the 0.5 when computing neighboring\n            pixel indices and therefore it uses pixels with a slightly incorrect\n            alignment (relative to our pixel model) when performing bilinear\n            interpolation.\n            With `aligned=True`, we first appropriately scale the ROI and then\n            shift it by -0.5 prior to calling roi_align. This produces the\n            correct neighbors; It makes negligible differences to the model's\n            performance if ROIAlign is used together with conv layers.\n        \"\"\"", "\n", "super", "(", "ResNetRoIHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "\n", "len", "(", "{", "len", "(", "pool_size", ")", ",", "len", "(", "dim_in", ")", "}", ")", "==", "1", "\n", ")", ",", "\"pathway dimensions are not consistent.\"", "\n", "self", ".", "num_pathways", "=", "len", "(", "pool_size", ")", "\n", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "temporal_pool", "=", "nn", ".", "AvgPool3d", "(", "\n", "[", "pool_size", "[", "pathway", "]", "[", "0", "]", ",", "1", ",", "1", "]", ",", "stride", "=", "1", "\n", ")", "\n", "self", ".", "add_module", "(", "\"s{}_tpool\"", ".", "format", "(", "pathway", ")", ",", "temporal_pool", ")", "\n", "\n", "roi_align", "=", "ROIAlign", "(", "\n", "resolution", "[", "pathway", "]", ",", "\n", "spatial_scale", "=", "1.0", "/", "scale_factor", "[", "pathway", "]", ",", "\n", "sampling_ratio", "=", "0", ",", "\n", "aligned", "=", "aligned", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"s{}_roi\"", ".", "format", "(", "pathway", ")", ",", "roi_align", ")", "\n", "spatial_pool", "=", "nn", ".", "MaxPool2d", "(", "resolution", "[", "pathway", "]", ",", "stride", "=", "1", ")", "\n", "self", ".", "add_module", "(", "\"s{}_spool\"", ".", "format", "(", "pathway", ")", ",", "spatial_pool", ")", "\n", "\n", "", "if", "dropout_rate", ">", "0.0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "# Perform FC in a fully convolutional manner. The FC layer will be", "\n", "# initialized with a different std comparing to convolutional layers.", "\n", "", "self", ".", "projection", "=", "nn", ".", "Linear", "(", "sum", "(", "dim_in", ")", ",", "num_classes", ",", "bias", "=", "True", ")", "\n", "\n", "# Softmax for evaluation and testing.", "\n", "if", "act_func", "==", "\"softmax\"", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "", "elif", "act_func", "==", "\"sigmoid\"", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"{} is not supported as an activation\"", "\n", "\"function.\"", ".", "format", "(", "act_func", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.head_helper.ResNetRoIHead.forward": [[332, 363], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "head_helper.ResNetRoIHead.view", "head_helper.ResNetRoIHead.projection", "head_helper.ResNetRoIHead.act", "len", "getattr", "getattr.", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "getattr", "getattr.", "getattr", "pool_out.append", "head_helper.ResNetRoIHead.dropout", "getattr."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "bboxes", ",", "extract", ",", "FBs", "=", "None", ",", "BTs", "=", "None", ")", ":", "\n", "        ", "assert", "(", "\n", "len", "(", "inputs", ")", "==", "self", ".", "num_pathways", "\n", ")", ",", "\"Input tensor does not contain {} pathway\"", ".", "format", "(", "self", ".", "num_pathways", ")", "\n", "pool_out", "=", "[", "]", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "t_pool", "=", "getattr", "(", "self", ",", "\"s{}_tpool\"", ".", "format", "(", "pathway", ")", ")", "\n", "out", "=", "t_pool", "(", "inputs", "[", "pathway", "]", ")", "\n", "assert", "out", ".", "shape", "[", "2", "]", "==", "1", "\n", "out", "=", "torch", ".", "squeeze", "(", "out", ",", "2", ")", "\n", "\n", "roi_align", "=", "getattr", "(", "self", ",", "\"s{}_roi\"", ".", "format", "(", "pathway", ")", ")", "\n", "out", "=", "roi_align", "(", "out", ",", "bboxes", ")", "\n", "\n", "s_pool", "=", "getattr", "(", "self", ",", "\"s{}_spool\"", ".", "format", "(", "pathway", ")", ")", "\n", "pool_out", ".", "append", "(", "s_pool", "(", "out", ")", ")", "\n", "\n", "# B C H W.", "\n", "", "x", "=", "torch", ".", "cat", "(", "pool_out", ",", "1", ")", "\n", "if", "extract", ":", "\n", "            ", "return", "x", "\n", "\n", "# Perform dropout.", "\n", "", "if", "hasattr", "(", "self", ",", "\"dropout\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "projection", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.head_helper.ResNetBasicHead.__init__": [[374, 428], ["torch.Module.__init__", "len", "range", "torch.Linear", "torch.Linear", "torch.Linear", "len", "head_helper.ResNetBasicHead.add_module", "torch.Dropout", "torch.Dropout", "torch.Dropout", "sum", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AvgPool3d", "torch.AvgPool3d", "torch.AvgPool3d", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "NotImplementedError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "num_classes", ",", "\n", "pool_size", ",", "\n", "dropout_rate", "=", "0.0", ",", "\n", "act_func", "=", "\"softmax\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        ResNetBasicHead takes p pathways as input where p in [1, infty].\n\n        Args:\n            dim_in (list): the list of channel dimensions of the p inputs to the\n                ResNetHead.\n            num_classes (int): the channel dimensions of the p outputs to the\n                ResNetHead.\n            pool_size (list): the list of kernel sizes of p spatial temporal\n                poolings, temporal pool kernel size, spatial pool kernel size,\n                spatial pool kernel size in order.\n            dropout_rate (float): dropout rate. If equal to 0.0, perform no\n                dropout.\n            act_func (string): activation function to use. 'softmax': applies\n                softmax on the output. 'sigmoid': applies sigmoid on the output.\n        \"\"\"", "\n", "super", "(", "ResNetBasicHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "\n", "len", "(", "{", "len", "(", "pool_size", ")", ",", "len", "(", "dim_in", ")", "}", ")", "==", "1", "\n", ")", ",", "\"pathway dimensions are not consistent.\"", "\n", "self", ".", "num_pathways", "=", "len", "(", "pool_size", ")", "\n", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "if", "pool_size", "[", "pathway", "]", "is", "None", ":", "\n", "                ", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "avg_pool", "=", "nn", ".", "AvgPool3d", "(", "pool_size", "[", "pathway", "]", ",", "stride", "=", "1", ")", "\n", "", "self", ".", "add_module", "(", "\"pathway{}_avgpool\"", ".", "format", "(", "pathway", ")", ",", "avg_pool", ")", "\n", "\n", "", "if", "dropout_rate", ">", "0.0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "# Perform FC in a fully convolutional manner. The FC layer will be", "\n", "# initialized with a different std comparing to convolutional layers.", "\n", "", "self", ".", "projection", "=", "nn", ".", "Linear", "(", "sum", "(", "dim_in", ")", ",", "num_classes", ",", "bias", "=", "True", ")", "\n", "\n", "# Softmax for evaluation and testing.", "\n", "if", "act_func", "==", "\"softmax\"", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Softmax", "(", "dim", "=", "4", ")", "\n", "", "elif", "act_func", "==", "\"sigmoid\"", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"{} is not supported as an activation\"", "\n", "\"function.\"", ".", "format", "(", "act_func", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.head_helper.ResNetBasicHead.forward": [[430, 453], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x.mean.mean.permute", "hasattr", "head_helper.ResNetBasicHead.projection", "x.mean.mean.view", "len", "getattr", "pool_out.append", "head_helper.ResNetBasicHead.dropout", "head_helper.ResNetBasicHead.act", "x.mean.mean.mean", "getattr."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "assert", "(", "\n", "len", "(", "inputs", ")", "==", "self", ".", "num_pathways", "\n", ")", ",", "\"Input tensor does not contain {} pathway\"", ".", "format", "(", "self", ".", "num_pathways", ")", "\n", "pool_out", "=", "[", "]", "\n", "for", "pathway", "in", "range", "(", "self", ".", "num_pathways", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "\"pathway{}_avgpool\"", ".", "format", "(", "pathway", ")", ")", "\n", "pool_out", ".", "append", "(", "m", "(", "inputs", "[", "pathway", "]", ")", ")", "\n", "", "x", "=", "torch", ".", "cat", "(", "pool_out", ",", "1", ")", "\n", "# (N, C, T, H, W) -> (N, T, H, W, C).", "\n", "x", "=", "x", ".", "permute", "(", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", ")", "\n", "# Perform dropout.", "\n", "if", "hasattr", "(", "self", ",", "\"dropout\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "x", "=", "self", ".", "projection", "(", "x", ")", "\n", "\n", "# Performs fully convlutional inference.", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "x", ".", "mean", "(", "[", "1", ",", "2", ",", "3", "]", ")", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.construct_optimizer": [[11, 69], ["model.named_parameters", "len", "len", "len", "len", "torch.optim.SGD", "bn_params.append", "non_bn_parameters.append", "list", "len", "len", "list", "torch.optim.Adam", "NotImplementedError", "model.parameters", "model.parameters"], "function", ["None"], ["def", "construct_optimizer", "(", "model", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Construct a stochastic gradient descent or ADAM optimizer with momentum.\n    Details can be found in:\n    Herbert Robbins, and Sutton Monro. \"A stochastic approximation method.\"\n    and\n    Diederik P.Kingma, and Jimmy Ba.\n    \"Adam: A Method for Stochastic Optimization.\"\n\n    Args:\n        model (model): model to perform stochastic gradient descent\n        optimization or ADAM optimization.\n        cfg (config): configs of hyper-parameters of SGD or ADAM, includes base\n        learning rate,  momentum, weight_decay, dampening, and etc.\n    \"\"\"", "\n", "# Batchnorm parameters.", "\n", "bn_params", "=", "[", "]", "\n", "# Non-batchnorm parameters.", "\n", "non_bn_parameters", "=", "[", "]", "\n", "for", "name", ",", "p", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "\"bn\"", "in", "name", ":", "\n", "            ", "bn_params", ".", "append", "(", "p", ")", "\n", "", "else", ":", "\n", "            ", "non_bn_parameters", ".", "append", "(", "p", ")", "\n", "# Apply different weight decay to Batchnorm and non-batchnorm parameters.", "\n", "# In Caffe2 classification codebase the weight decay for batchnorm is 0.0.", "\n", "# Having a different weight decay on batchnorm might cause a performance", "\n", "# drop.", "\n", "", "", "optim_params", "=", "[", "\n", "{", "\"params\"", ":", "bn_params", ",", "\"weight_decay\"", ":", "cfg", ".", "BN", ".", "WEIGHT_DECAY", "}", ",", "\n", "{", "\"params\"", ":", "non_bn_parameters", ",", "\"weight_decay\"", ":", "cfg", ".", "SOLVER", ".", "WEIGHT_DECAY", "}", ",", "\n", "]", "\n", "# Check all parameters will be passed into optimizer.", "\n", "assert", "len", "(", "list", "(", "model", ".", "parameters", "(", ")", ")", ")", "==", "len", "(", "non_bn_parameters", ")", "+", "len", "(", "\n", "bn_params", "\n", ")", ",", "\"parameter size does not match: {} + {} != {}\"", ".", "format", "(", "\n", "len", "(", "non_bn_parameters", ")", ",", "len", "(", "bn_params", ")", ",", "len", "(", "list", "(", "model", ".", "parameters", "(", ")", ")", ")", "\n", ")", "\n", "\n", "if", "cfg", ".", "SOLVER", ".", "OPTIMIZING_METHOD", "==", "\"sgd\"", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "SGD", "(", "\n", "optim_params", ",", "\n", "lr", "=", "cfg", ".", "SOLVER", ".", "BASE_LR", ",", "\n", "momentum", "=", "cfg", ".", "SOLVER", ".", "MOMENTUM", ",", "\n", "weight_decay", "=", "cfg", ".", "SOLVER", ".", "WEIGHT_DECAY", ",", "\n", "dampening", "=", "cfg", ".", "SOLVER", ".", "DAMPENING", ",", "\n", "nesterov", "=", "cfg", ".", "SOLVER", ".", "NESTEROV", ",", "\n", ")", "\n", "", "elif", "cfg", ".", "SOLVER", ".", "OPTIMIZING_METHOD", "==", "\"adam\"", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "Adam", "(", "\n", "optim_params", ",", "\n", "lr", "=", "cfg", ".", "SOLVER", ".", "BASE_LR", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "weight_decay", "=", "cfg", ".", "SOLVER", ".", "WEIGHT_DECAY", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"Does not support {} optimizer\"", ".", "format", "(", "cfg", ".", "SOLVER", ".", "OPTIMIZING_METHOD", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.get_epoch_lr": [[72, 81], ["slowfast.get_lr_at_epoch"], "function", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.utils.lr_policy.get_lr_at_epoch"], ["", "", "def", "get_epoch_lr", "(", "cur_epoch", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Retrieves the lr for the given epoch (as specified by the lr policy).\n    Args:\n        cfg (config): configs of hyper-parameters of ADAM, includes base\n        learning rate, betas, and weight decays.\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"", "\n", "return", "lr_policy", ".", "get_lr_at_epoch", "(", "cfg", ",", "cur_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.optimizer.set_lr": [[83, 92], ["None"], "function", ["None"], ["", "def", "set_lr", "(", "optimizer", ",", "new_lr", ")", ":", "\n", "    ", "\"\"\"\n    Sets the optimizer lr to the specified value.\n    Args:\n        optimizer (optim): the optimizer using to optimize the current network.\n        new_lr (float): the new learning rate to set.\n    \"\"\"", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "\"lr\"", "]", "=", "new_lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.VideoModelStem.__init__": [[15, 76], ["torch.Module.__init__", "len", "stem_helper.VideoModelStem._construct_stem", "len", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.ResNetBasicStem._construct_stem"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "kernel", ",", "\n", "stride", ",", "\n", "padding", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "eps", "=", "1e-5", ",", "\n", "bn_mmt", "=", "0.1", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these\n        arguments. List size of 1 for single pathway models (C2D, I3D, Slow\n        and etc), list size of 2 for two pathway models (SlowFast).\n\n        Args:\n            dim_in (list): the list of channel dimensions of the inputs.\n            dim_out (list): the output dimension of the convolution in the stem\n                layer.\n            kernel (list): the kernels' size of the convolutions in the stem\n                layers. Temporal kernel size, height kernel size, width kernel\n                size in order.\n            stride (list): the stride sizes of the convolutions in the stem\n                layer. Temporal kernel stride, height kernel size, width kernel\n                size in order.\n            padding (list): the paddings' sizes of the convolutions in the stem\n                layer. Temporal padding size, height padding size, width padding\n                size in order.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "VideoModelStem", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "(", "\n", "len", "(", "\n", "{", "\n", "len", "(", "dim_in", ")", ",", "\n", "len", "(", "dim_out", ")", ",", "\n", "len", "(", "kernel", ")", ",", "\n", "len", "(", "stride", ")", ",", "\n", "len", "(", "padding", ")", ",", "\n", "}", "\n", ")", "\n", "==", "1", "\n", ")", ",", "\"Input pathway dimensions are not consistent.\"", "\n", "self", ".", "num_pathways", "=", "len", "(", "dim_in", ")", "\n", "self", ".", "kernel", "=", "kernel", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "inplace_relu", "=", "inplace_relu", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "bn_mmt", "=", "bn_mmt", "\n", "# Construct the stem layer.", "\n", "self", ".", "_construct_stem", "(", "dim_in", ",", "dim_out", ",", "norm_module", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.VideoModelStem._construct_stem": [[77, 91], ["range", "len", "stem_helper.ResNetBasicStem", "stem_helper.VideoModelStem.add_module"], "methods", ["None"], ["", "def", "_construct_stem", "(", "self", ",", "dim_in", ",", "dim_out", ",", "norm_module", ")", ":", "\n", "        ", "for", "pathway", "in", "range", "(", "len", "(", "dim_in", ")", ")", ":", "\n", "            ", "stem", "=", "ResNetBasicStem", "(", "\n", "dim_in", "[", "pathway", "]", ",", "\n", "dim_out", "[", "pathway", "]", ",", "\n", "self", ".", "kernel", "[", "pathway", "]", ",", "\n", "self", ".", "stride", "[", "pathway", "]", ",", "\n", "self", ".", "padding", "[", "pathway", "]", ",", "\n", "self", ".", "inplace_relu", ",", "\n", "self", ".", "eps", ",", "\n", "self", ".", "bn_mmt", ",", "\n", "norm_module", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"pathway{}_stem\"", ".", "format", "(", "pathway", ")", ",", "stem", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.VideoModelStem.forward": [[92, 100], ["range", "len", "len", "getattr", "getattr."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "(", "\n", "len", "(", "x", ")", "==", "self", ".", "num_pathways", "\n", ")", ",", "\"Input tensor does not contain {} pathway\"", ".", "format", "(", "self", ".", "num_pathways", ")", "\n", "for", "pathway", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "\"pathway{}_stem\"", ".", "format", "(", "pathway", ")", ")", "\n", "x", "[", "pathway", "]", "=", "m", "(", "x", "[", "pathway", "]", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.ResNetBasicStem.__init__": [[109, 155], ["torch.Module.__init__", "stem_helper.ResNetBasicStem._construct_stem"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__", "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.ResNetBasicStem._construct_stem"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "kernel", ",", "\n", "stride", ",", "\n", "padding", ",", "\n", "inplace_relu", "=", "True", ",", "\n", "eps", "=", "1e-5", ",", "\n", "bn_mmt", "=", "0.1", ",", "\n", "norm_module", "=", "nn", ".", "BatchNorm3d", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these arguments.\n\n        Args:\n            dim_in (int): the channel dimension of the input. Normally 3 is used\n                for rgb input, and 2 or 3 is used for optical flow input.\n            dim_out (int): the output dimension of the convolution in the stem\n                layer.\n            kernel (list): the kernel size of the convolution in the stem layer.\n                temporal kernel size, height kernel size, width kernel size in\n                order.\n            stride (list): the stride size of the convolution in the stem layer.\n                temporal kernel stride, height kernel size, width kernel size in\n                order.\n            padding (int): the padding size of the convolution in the stem\n                layer, temporal padding size, height padding size, width\n                padding size in order.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            norm_module (nn.Module): nn.Module for the normalization layer. The\n                default is nn.BatchNorm3d.\n        \"\"\"", "\n", "super", "(", "ResNetBasicStem", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "kernel", "=", "kernel", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "inplace_relu", "=", "inplace_relu", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "bn_mmt", "=", "bn_mmt", "\n", "# Construct the stem layer.", "\n", "self", ".", "_construct_stem", "(", "dim_in", ",", "dim_out", ",", "norm_module", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.ResNetBasicStem._construct_stem": [[156, 171], ["torch.Conv3d", "norm_module", "torch.ReLU", "torch.MaxPool3d"], "methods", ["None"], ["", "def", "_construct_stem", "(", "self", ",", "dim_in", ",", "dim_out", ",", "norm_module", ")", ":", "\n", "        ", "self", ".", "conv", "=", "nn", ".", "Conv3d", "(", "\n", "dim_in", ",", "\n", "dim_out", ",", "\n", "self", ".", "kernel", ",", "\n", "stride", "=", "self", ".", "stride", ",", "\n", "padding", "=", "self", ".", "padding", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "bn", "=", "norm_module", "(", "\n", "num_features", "=", "dim_out", ",", "eps", "=", "self", ".", "eps", ",", "momentum", "=", "self", ".", "bn_mmt", "\n", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "self", ".", "inplace_relu", ")", "\n", "self", ".", "pool_layer", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "[", "1", ",", "3", ",", "3", "]", ",", "stride", "=", "[", "1", ",", "2", ",", "2", "]", ",", "padding", "=", "[", "0", ",", "1", ",", "1", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.stem_helper.ResNetBasicStem.forward": [[173, 179], ["stem_helper.ResNetBasicStem.conv", "stem_helper.ResNetBasicStem.bn", "stem_helper.ResNetBasicStem.relu", "stem_helper.ResNetBasicStem.pool_layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "pool_layer", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.losses.CustomizeCrossEntropy.__init__": [[11, 14], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "reduction", "=", "\"mean\"", ")", ":", "\n", "        ", "super", "(", "CustomizeCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.losses.CustomizeCrossEntropy.forward": [[15, 24], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pred", ",", "labels", ")", ":", "\n", "\n", "        ", "pred", "=", "-", "1", "*", "torch", ".", "log", "(", "pred", "+", "1e-5", ")", "\n", "loss", "=", "torch", ".", "sum", "(", "pred", "*", "labels", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "reduction", "==", "\"mean\"", ":", "\n", "            ", "return", "torch", ".", "mean", "(", "loss", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "sum", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.losses.CrossScopeFocalLoss.__init__": [[27, 31], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "reduction", "=", "\"mean\"", ",", "gamma", "=", "1.0", ")", ":", "\n", "        ", "super", "(", "CrossScopeFocalLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "reduction", "=", "reduction", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.losses.CrossScopeFocalLoss.forward": [[32, 49], ["pred1.clone", "pred2.clone", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pred1", ",", "pred2", ",", "labels", ")", ":", "\n", "\n", "        ", "prob1", "=", "pred1", ".", "clone", "(", ")", "\n", "prob2", "=", "pred2", ".", "clone", "(", ")", "\n", "\n", "pred1", "=", "-", "1", "*", "torch", ".", "log", "(", "prob1", "+", "1e-5", ")", "\n", "pred2", "=", "-", "1", "*", "torch", ".", "log", "(", "prob2", "+", "1e-5", ")", "\n", "\n", "loss1", "=", "torch", ".", "sum", "(", "(", "(", "1", "-", "prob2", ")", "**", "self", ".", "gamma", ")", "*", "pred1", "*", "labels", ",", "dim", "=", "-", "1", ")", "\n", "loss2", "=", "torch", ".", "sum", "(", "(", "(", "1", "-", "prob1", ")", "**", "self", ".", "gamma", ")", "*", "pred2", "*", "labels", ",", "dim", "=", "-", "1", ")", "\n", "\n", "loss", "=", "loss1", "+", "loss2", "\n", "\n", "if", "self", ".", "reduction", "==", "\"mean\"", ":", "\n", "            ", "return", "torch", ".", "mean", "(", "loss", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "sum", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.losses.get_loss_func": [[50, 59], ["_LOSSES.keys", "NotImplementedError"], "function", ["None"], ["", "", "", "def", "get_loss_func", "(", "loss_name", ")", ":", "\n", "    ", "\"\"\"\n    Retrieve the loss given the loss name.\n    Args (int):\n        loss_name: the name of the loss to use.\n    \"\"\"", "\n", "if", "loss_name", "not", "in", "_LOSSES", ".", "keys", "(", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Loss {} is not supported\"", ".", "format", "(", "loss_name", ")", ")", "\n", "", "return", "_LOSSES", "[", "loss_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.Pair.__init__": [[12, 37], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.PReLU", "torch.PReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "query_dim_in", ",", "\n", "ratio", "=", "2", ",", "\n", "dropout", "=", "0.2", "\n", ")", ":", "\n", "        ", "super", "(", "Pair", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# part for construct context pair", "\n", "\n", "self", ".", "projectc1", "=", "nn", ".", "Linear", "(", "query_dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "projectc2", "=", "nn", ".", "Linear", "(", "dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "projectc3", "=", "nn", ".", "Linear", "(", "dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "\n", "self", ".", "projectd1", "=", "nn", ".", "Linear", "(", "query_dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "projectd2", "=", "nn", ".", "Linear", "(", "dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "projectd3", "=", "nn", ".", "Linear", "(", "dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "latent", "=", "dim_in", "//", "ratio", "\n", "\n", "self", ".", "cffn", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "LayerNorm", "(", "dim_in", "//", "ratio", ")", ",", "\n", "nn", ".", "PReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "dim_in", "//", "ratio", ",", "dim_in", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.Pair.make_context_pair": [[39, 52], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_helper.Pair.cffn", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "math.sqrt", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "math.sqrt", "key_pair1.permute", "key_pair2.permute"], "methods", ["None"], ["", "def", "make_context_pair", "(", "self", ",", "rois_pair1", ",", "key_pair1", ",", "val_pair1", ",", "rois_pair2", ",", "key_pair2", ",", "val_pair2", ")", ":", "\n", "\n", "        ", "p1", "=", "torch", ".", "matmul", "(", "rois_pair1", ",", "key_pair1", ".", "permute", "(", "1", ",", "0", ")", ")", "/", "math", ".", "sqrt", "(", "self", ".", "latent", ")", "\n", "p1", "=", "torch", ".", "softmax", "(", "p1", ",", "dim", "=", "-", "1", ")", "\n", "f1", "=", "torch", ".", "matmul", "(", "p1", ",", "val_pair1", ")", "\n", "p2", "=", "torch", ".", "matmul", "(", "rois_pair2", ",", "key_pair2", ".", "permute", "(", "1", ",", "0", ")", ")", "/", "math", ".", "sqrt", "(", "self", ".", "latent", ")", "\n", "p2", "=", "torch", ".", "softmax", "(", "p2", ",", "dim", "=", "-", "1", ")", "\n", "f2", "=", "torch", ".", "matmul", "(", "p2", ",", "val_pair2", ")", "\n", "\n", "cpair", "=", "f1", "*", "f2", "\n", "cpair", "=", "self", ".", "cffn", "(", "cpair", ")", "\n", "\n", "return", "cpair", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.Pair.forward": [[53, 68], ["context_helper.Pair.projectc2", "context_helper.Pair.projectc3", "context_helper.Pair.projectc1", "context_helper.Pair.projectd2", "context_helper.Pair.projectd3", "context_helper.Pair.projectd1", "context_helper.Pair.make_context_pair"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.Pair.make_context_pair"], ["", "def", "forward", "(", "self", ",", "feat", ",", "key_bank", ")", ":", "\n", "\n", "# one-to-pair", "\n", "        ", "pair_kb1", "=", "self", ".", "projectc2", "(", "key_bank", ")", "\n", "pair_fb1", "=", "self", ".", "projectc3", "(", "key_bank", ")", "\n", "rois_pair_key1", "=", "self", ".", "projectc1", "(", "feat", ")", "\n", "\n", "pair_kb2", "=", "self", ".", "projectd2", "(", "key_bank", ")", "\n", "pair_fb2", "=", "self", ".", "projectd3", "(", "key_bank", ")", "\n", "rois_pair_key2", "=", "self", ".", "projectd1", "(", "feat", ")", "\n", "\n", "pair_ctx", "=", "self", ".", "make_context_pair", "(", "rois_pair_key1", ",", "pair_kb1", ",", "pair_fb1", ",", "\n", "rois_pair_key2", ",", "pair_kb2", ",", "pair_fb2", ")", "\n", "\n", "return", "pair_ctx", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.ReaderUnit.__init__": [[72, 112], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Parameter", "torch.Parameter", "torch.ModuleList", "torch.ModuleList", "context_helper.FFN", "context_helper.FFN", "torch.LayerNorm", "torch.LayerNorm", "torch.PReLU", "torch.PReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "context_helper.Pair", "range"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "query_dim_in", ",", "\n", "ratio", "=", "2", ",", "\n", "dropout", "=", "0.2", ",", "\n", "window_size", "=", "0", ",", "\n", "embed_size", "=", "0", ",", "\n", "num_pairs", "=", "2", "\n", ")", ":", "\n", "        ", "super", "(", "ReaderUnit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "codec", "=", "nn", ".", "Sequential", "(", "\n", "FFN", "(", "mem_dim", "=", "dim_in", "+", "embed_size", ",", "key_dim", "=", "dim_in", "+", "embed_size", ")", ",", "\n", "FFN", "(", "mem_dim", "=", "dim_in", "+", "embed_size", ",", "key_dim", "=", "dim_in", "+", "embed_size", ")", "\n", ")", "\n", "self", ".", "project1", "=", "nn", ".", "Linear", "(", "query_dim_in", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "project2", "=", "nn", ".", "Linear", "(", "dim_in", "+", "embed_size", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "project3", "=", "nn", ".", "Linear", "(", "dim_in", "+", "embed_size", ",", "dim_in", "//", "ratio", ")", "\n", "self", ".", "ffn", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "LayerNorm", "(", "dim_in", "//", "ratio", ")", ",", "\n", "nn", ".", "PReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "dim_in", "//", "ratio", ",", "dim_in", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", "\n", ")", "\n", "\n", "self", ".", "dim_in", "=", "dim_in", "\n", "self", ".", "latent", "=", "dim_in", "//", "ratio", "\n", "self", ".", "window_size", "=", "window_size", "+", "1", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "\n", "# pair unit", "\n", "self", ".", "num_pairs", "=", "num_pairs", "\n", "self", ".", "aggregator", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "1", ",", "self", ".", "num_pairs", ")", "/", "num_pairs", ")", "\n", "self", ".", "pairs", "=", "nn", ".", "ModuleList", "(", "[", "\n", "Pair", "(", "dim_in", ",", "query_dim_in", ",", "ratio", ")", "\n", "for", "_", "in", "range", "(", "num_pairs", ")", "\n", "]", ")", "\n", "\n", "if", "window_size", ">", "0", "and", "embed_size", ">", "0", ":", "\n", "            ", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "self", ".", "window_size", ",", "self", ".", "embed_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.ReaderUnit.forward": [[113, 164], ["context_helper.ReaderUnit.project1", "range", "int", "feat.new_zeros", "feat.clone", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.max().item", "torch.max().item", "torch.max().item", "torch.max().item", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "key_banks[].unsqueeze().expand", "feat_key[].unsqueeze", "context_helper.ReaderUnit.codec", "context_helper.ReaderUnit.project2", "context_helper.ReaderUnit.permute().contiguous", "context_helper.ReaderUnit.project3", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul().squeeze", "torch.matmul().squeeze", "torch.matmul().squeeze", "torch.matmul().squeeze", "context_helper.ReaderUnit.ffn", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.abs().long", "torch.abs().long", "torch.abs().long", "torch.abs().long", "context_helper.ReaderUnit.embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "math.sqrt", "torch.max", "torch.max", "torch.max", "torch.max", "m", "key_banks[].unsqueeze", "context_helper.ReaderUnit.permute", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "feat_time.unsqueeze", "bank_time.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "feat", ",", "key_banks", ",", "time_stamps", ",", "batch_idx", ",", "residual", "=", "True", ")", ":", "\n", "\n", "        ", "num_batch", "=", "int", "(", "torch", ".", "max", "(", "batch_idx", ")", ".", "item", "(", ")", ")", "+", "1", "\n", "output", "=", "feat", ".", "new_zeros", "(", "feat", ".", "shape", "[", "0", "]", ",", "self", ".", "dim_in", ")", "if", "not", "residual", "else", "feat", ".", "clone", "(", ")", "\n", "feat_key", "=", "self", ".", "project1", "(", "feat", ")", "\n", "\n", "for", "b", "in", "range", "(", "num_batch", ")", ":", "\n", "            ", "clip_idx", "=", "torch", ".", "nonzero", "(", "batch_idx", "==", "b", ")", ".", "squeeze", "(", "1", ")", "\n", "if", "clip_idx", ".", "shape", "[", "0", "]", ">", "0", "and", "key_banks", "[", "b", "]", "is", "not", "None", ":", "\n", "\n", "# one-to-pair", "\n", "                ", "pairs", "=", "torch", ".", "stack", "(", "\n", "[", "m", "(", "feat", "[", "clip_idx", "]", ",", "key_banks", "[", "b", "]", ")", "for", "m", "in", "self", ".", "pairs", "]", ",", "\n", "dim", "=", "2", "\n", ")", "\n", "pair_ctx", "=", "torch", ".", "sum", "(", "self", ".", "aggregator", "*", "pairs", ",", "dim", "=", "2", ")", "\n", "\n", "# one-to-one", "\n", "read_bank", "=", "key_banks", "[", "b", "]", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "clip_idx", ".", "shape", "[", "0", "]", ",", "key_banks", "[", "b", "]", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "rois_keys", "=", "feat_key", "[", "clip_idx", "]", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "if", "self", ".", "window_size", ">", "0", "and", "self", ".", "embed_size", ">", "0", ":", "\n", "                    ", "times", "=", "torch", ".", "Tensor", "(", "time_stamps", "[", "b", "]", ")", ".", "to", "(", "feat", ".", "device", ")", "\n", "nbanks", "=", "times", ".", "shape", "[", "0", "]", "-", "clip_idx", ".", "shape", "[", "0", "]", "\n", "nfeat", "=", "clip_idx", ".", "shape", "[", "0", "]", "\n", "bank_time", ",", "feat_time", "=", "times", "[", ":", "nbanks", "]", ",", "times", "[", "-", "nfeat", ":", "]", "\n", "temporal_dist", "=", "torch", ".", "abs", "(", "feat_time", ".", "unsqueeze", "(", "1", ")", "-", "bank_time", ".", "unsqueeze", "(", "0", ")", ")", ".", "long", "(", ")", "\n", "\n", "temp_embed", "=", "self", ".", "embed", "(", "temporal_dist", ")", "\n", "read_bank", "=", "torch", ".", "cat", "(", "[", "read_bank", ",", "temp_embed", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "read_bank", "=", "self", ".", "codec", "(", "read_bank", ")", "\n", "\n", "kb", "=", "self", ".", "project2", "(", "read_bank", ")", "\n", "key_bank", "=", "kb", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "fb", "=", "self", ".", "project3", "(", "read_bank", ")", "\n", "\n", "coeff", "=", "torch", ".", "matmul", "(", "rois_keys", ",", "key_bank", ")", "/", "math", ".", "sqrt", "(", "self", ".", "latent", ")", "\n", "coeff", "=", "torch", ".", "softmax", "(", "coeff", ",", "dim", "=", "-", "1", ")", "\n", "\n", "feat_out", "=", "torch", ".", "matmul", "(", "coeff", ",", "fb", ")", ".", "squeeze", "(", "1", ")", "\n", "feat_out", "=", "self", ".", "ffn", "(", "feat_out", ")", "\n", "\n", "feat_out", "+=", "pair_ctx", "\n", "\n", "if", "residual", ":", "\n", "                    ", "output", "[", "clip_idx", "]", "=", "output", "[", "clip_idx", "]", "+", "feat_out", "\n", "", "else", ":", "\n", "                    ", "output", "[", "clip_idx", "]", "=", "feat_out", "\n", "\n", "", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.ContextModule.__init__": [[168, 182], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in", ",", "\n", "dim_ctx_in", ",", "\n", "dim_key", ",", "\n", "dim_val", "\n", ")", ":", "\n", "        ", "super", "(", "ContextModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "project", "=", "nn", ".", "Linear", "(", "dim_in", ",", "dim_key", ")", "\n", "self", ".", "keys", "=", "nn", ".", "Conv3d", "(", "dim_ctx_in", ",", "dim_key", ",", "1", ",", "1", ",", "0", ")", "\n", "self", ".", "vals", "=", "nn", ".", "Conv3d", "(", "dim_ctx_in", ",", "dim_val", ",", "1", ",", "1", ",", "0", ")", "\n", "self", ".", "dim_in", "=", "dim_in", "\n", "self", ".", "dim_key", "=", "dim_key", "\n", "self", ".", "dim_val", "=", "dim_val", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.ContextModule.forward": [[183, 217], ["context_helper.ContextModule.project", "context_helper.ContextModule.keys().view", "context_helper.ContextModule.vals().view().permute().contiguous", "rois.new_zeros", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "context_helper.ContextModule.keys", "context_helper.ContextModule.vals().view().permute", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.cat.append", "torch.cat.append", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "math.sqrt", "context_helper.ContextModule.vals().view", "torch.softmax.view", "torch.softmax.view", "context_helper.ContextModule.vals"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "rois", ",", "context", ",", "batch_idx", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Args:\n            rois: [N_box x dim_in] torch.Tensor\n            context: [N x dim_in x T x H x W] torch.Tensor\n            batch_idx: [N_box] torch.Tensor\n\n        Returns:\n            [N_box x dim_val] torch.Tensor\n\n        \"\"\"", "\n", "num_batch", ",", "_", ",", "ctx_time", ",", "ctx_height", ",", "ctx_width", "=", "context", ".", "shape", "\n", "rois_keys", "=", "self", ".", "project", "(", "rois", ")", "\n", "context_key", "=", "self", ".", "keys", "(", "context", ")", ".", "view", "(", "num_batch", ",", "self", ".", "dim_key", ",", "-", "1", ")", "\n", "context_val", "=", "self", ".", "vals", "(", "context", ")", ".", "view", "(", "num_batch", ",", "\n", "self", ".", "dim_val", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "ctx", "=", "rois", ".", "new_zeros", "(", "(", "rois", ".", "shape", "[", "0", "]", ",", "self", ".", "dim_val", ")", ")", "\n", "attn", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_batch", ")", ":", "\n", "            ", "box_idx", "=", "torch", ".", "nonzero", "(", "batch_idx", "==", "i", ")", ".", "squeeze", "(", "1", ")", "\n", "nbox", "=", "box_idx", ".", "shape", "[", "0", "]", "\n", "if", "nbox", ">", "0", ":", "\n", "                ", "query", "=", "rois_keys", "[", "box_idx", "]", "\n", "coeff", "=", "torch", ".", "mm", "(", "query", ",", "context_key", "[", "i", "]", ")", "/", "math", ".", "sqrt", "(", "self", ".", "dim_key", ")", "\n", "coeff", "=", "torch", ".", "softmax", "(", "coeff", ",", "dim", "=", "-", "1", ")", "\n", "attn", ".", "append", "(", "coeff", ".", "view", "(", "nbox", ",", "ctx_time", ",", "ctx_height", ",", "ctx_width", ")", "*", "255.0", ")", "\n", "batch_ctx", "=", "torch", ".", "mm", "(", "coeff", ",", "context_val", "[", "i", "]", ")", "\n", "ctx", "[", "box_idx", "]", "=", "batch_ctx", "\n", "\n", "", "", "attn", "=", "torch", ".", "cat", "(", "attn", ",", "dim", "=", "0", ")", "\n", "\n", "return", "ctx", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.FFN.__init__": [[221, 244], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.PReLU", "torch.PReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "mem_dim", ",", "\n", "key_dim", ",", "\n", "ratio", "=", "4", ",", "\n", "dropout", "=", "0.2", ",", "\n", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "FFN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mem_dim", "=", "mem_dim", "\n", "self", ".", "key_dim", "=", "key_dim", "\n", "\n", "self", ".", "output_ffn", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "mem_dim", ",", "mem_dim", "//", "ratio", ")", ",", "\n", "nn", ".", "PReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "mem_dim", "//", "ratio", ",", "mem_dim", ")", "\n", ")", "\n", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "mem_dim", ")", "\n", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "self", ".", "drop_out", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.FFN.forward": [[245, 263], ["context_helper.FFN.output_ffn", "context_helper.FFN.norm2", "hasattr", "context_helper.FFN.drop_out"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "query", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Args:\n            query: torch.Tensor [Nd x mem_dim] input query value\n            key: torch.Tensor [Nd x key_dim] input key value\n\n        Returns:\n\n        \"\"\"", "\n", "\n", "codec", "=", "query", "\n", "codec", "=", "self", ".", "output_ffn", "(", "codec", ")", "\n", "codec", "=", "self", ".", "norm2", "(", "codec", ")", "\n", "if", "hasattr", "(", "self", ",", "'drop_out'", ")", ":", "\n", "            ", "codec", "=", "self", ".", "drop_out", "(", "codec", ")", "\n", "\n", "", "return", "codec", "+", "query", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.BasicTransformer.__init__": [[267, 297], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_query_in", ",", "\n", "dim_keyval_in", ",", "\n", "dim_key", ",", "\n", "dim_inner", ",", "\n", "num_head", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", "BasicTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dim_query_in", "%", "num_head", "==", "0", "\n", "self", ".", "dim_key", "=", "dim_key", "\n", "self", ".", "num_head", "=", "num_head", "\n", "dim_val", "=", "dim_query_in", "//", "num_head", "\n", "self", ".", "dim_val", "=", "dim_val", "\n", "\n", "self", ".", "query_transform", "=", "nn", ".", "Linear", "(", "dim_query_in", ",", "dim_key", ")", "\n", "\n", "self", ".", "keys", "=", "nn", ".", "Linear", "(", "dim_keyval_in", ",", "dim_key", ")", "\n", "self", ".", "vals", "=", "nn", ".", "Linear", "(", "dim_keyval_in", ",", "dim_val", ")", "\n", "\n", "self", ".", "ffn", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "dim_query_in", ",", "dim_inner", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "dim_inner", ",", "dim_query_in", ")", "\n", ")", "\n", "\n", "self", ".", "norm1", "=", "nn", ".", "LayerNorm", "(", "dim_query_in", ")", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "dim_query_in", ")", "\n", "self", ".", "dropout1", "=", "nn", ".", "Dropout", "(", "0.2", ")", "\n", "self", ".", "dropout2", "=", "nn", ".", "Dropout", "(", "0.2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.BasicTransformer.forward": [[298, 328], ["context_helper.BasicTransformer.query_transform", "context_helper.BasicTransformer.keys", "context_helper.BasicTransformer.vals", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "context_helper.BasicTransformer.norm1", "context_helper.BasicTransformer.norm2", "torch.softmax.detach", "torch.softmax.detach", "context_helper.BasicTransformer.dropout1", "context_helper.BasicTransformer.dropout2", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "math.sqrt", "context_helper.BasicTransformer.ffn", "context_helper.BasicTransformer.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "keyval", ")", ":", "\n", "\n", "        ", "\"\"\"\n\n        Args:\n            query: [Nq x dim_query_in] torch.Tensor\n            keyval: [Nd x dim_keyval_in] torch.Tensor\n\n        \"\"\"", "\n", "heat", "=", "0.0", "\n", "\n", "query_key", "=", "self", ".", "query_transform", "(", "query", ")", "\n", "key", "=", "self", ".", "keys", "(", "keyval", ")", "\n", "val", "=", "self", ".", "vals", "(", "keyval", ")", "\n", "\n", "attention", "=", "torch", ".", "softmax", "(", "\n", "torch", ".", "mm", "(", "query_key", ",", "key", ".", "permute", "(", "1", ",", "0", ")", ")", "/", "math", ".", "sqrt", "(", "self", ".", "dim_key", ")", ",", "\n", "dim", "=", "1", ")", "\n", "heat", "+=", "torch", ".", "sum", "(", "attention", ".", "detach", "(", ")", ",", "dim", "=", "0", ")", "\n", "\n", "out", "=", "torch", ".", "mm", "(", "attention", ",", "val", ")", "\n", "\n", "heat", "/=", "self", ".", "num_head", "\n", "out1", "=", "query", "+", "self", ".", "dropout1", "(", "out", ")", "\n", "out1", "=", "self", ".", "norm1", "(", "out1", ")", "\n", "\n", "out2", "=", "out1", "+", "self", ".", "dropout2", "(", "self", ".", "ffn", "(", "out1", ")", ")", "\n", "out2", "=", "self", ".", "norm2", "(", "out2", ")", "\n", "\n", "return", "out2", ",", "heat", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiHeadTransformer.__init__": [[332, 349], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "range", "context_helper.MultiHeadTransformer.transformers.append", "context_helper.BasicTransformer"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_query_in", ",", "\n", "dim_keyval_in", ",", "\n", "dim_key", ",", "\n", "dim_inner", ",", "\n", "num_head", ",", "\n", ")", ":", "\n", "        ", "super", "(", "MultiHeadTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transformers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "self", ".", "transformers", ".", "append", "(", "\n", "BasicTransformer", "(", "\n", "dim_query_in", ",", "\n", "dim_keyval_in", ",", "\n", "dim_key", ",", "\n", "dim_inner", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiHeadTransformer.forward": [[352, 354], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "m"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "keyval", ")", ":", "\n", "        ", "return", "torch", ".", "cat", "(", "[", "m", "(", "query", ",", "keyval", ")", "for", "m", "in", "self", ".", "transformers", "]", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__": [[358, 371], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "range", "context_helper.MultiLayerTransformer.transformers.append", "transforemer_type"], "methods", ["home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "transforemer_type", ",", "\n", "num_layers", ",", "\n", "*", "arg", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", "MultiLayerTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transformers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "self", ".", "transformers", ".", "append", "(", "\n", "transforemer_type", "(", "\n", "*", "arg", ",", "**", "kwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.tencentyouturesearch_actiondetection-lstc.models.context_helper.MultiLayerTransformer.forward": [[374, 380], ["m"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "keyval", ")", ":", "\n", "\n", "        ", "for", "m", "in", "self", ".", "transformers", ":", "\n", "            ", "query", "=", "m", "(", "query", ",", "keyval", ")", "\n", "\n", "", "return", "query", "\n", "", "", ""]]}