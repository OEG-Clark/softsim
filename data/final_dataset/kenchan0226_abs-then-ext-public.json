{"home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_lens_info._count_data": [[15, 22], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["", "def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_lens_info.main": [[24, 73], ["os.path.join", "make_lens_info._count_data", "range", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "np.array.dump", "np.array.dump", "np.array.dump", "np.array.dump", "np.array.dump", "np.array.dump", "json.load", "enumerate", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "open", "len", "len", "article[].strip().split", "abstract[].strip().split", "len", "len", "np.array.append", "np.array.append", "np.array.append", "np.array.append", "os.path.join", "article[].strip", "abstract[].strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "data_path", "=", "join", "(", "DATA_DIR", ",", "args", ".", "split", ")", "\n", "\n", "n_data", "=", "_count_data", "(", "data_path", ")", "\n", "compress_ratios", "=", "[", "]", "\n", "compress_sents_lens", "=", "[", "]", "\n", "original_sents_lens", "=", "[", "]", "\n", "ext_label_rouge_l_scores", "=", "[", "]", "\n", "distances_two_to_one", "=", "[", "]", "\n", "improvements_all", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "js_obj", "=", "json", ".", "load", "(", "open", "(", "join", "(", "data_path", ",", "\"{}.json\"", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "abstract", "=", "js_obj", "[", "'abstract'", "]", "\n", "article", "=", "js_obj", "[", "'article'", "]", "\n", "ext_ids", "=", "js_obj", "[", "'extracted'", "]", "\n", "rouge_l_scores", "=", "js_obj", "[", "'score'", "]", "\n", "#computed_compress_ratios = js_obj['compression_ratios']", "\n", "assert", "len", "(", "rouge_l_scores", ")", "==", "len", "(", "ext_ids", ")", "\n", "distances_two_to_one", "+=", "[", "d", "for", "d", "in", "js_obj", "[", "'distances_two_to_one_{}'", ".", "format", "(", "args", ".", "threshold", ")", "]", "if", "d", "is", "not", "None", "]", "# filter out all none distance", "\n", "\n", "for", "j", ",", "ext_id", "in", "enumerate", "(", "ext_ids", ")", ":", "\n", "            ", "original_sent", "=", "article", "[", "ext_id", "]", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "compressed_sent", "=", "abstract", "[", "j", "]", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "original_sent_len", "=", "len", "(", "original_sent", ")", "\n", "compressed_sent_len", "=", "len", "(", "compressed_sent", ")", "\n", "compress_ratio", "=", "(", "original_sent_len", "-", "compressed_sent_len", ")", "/", "original_sent_len", "\n", "#if abs(compress_ratio-computed_compress_ratios[j]) > 1e-4:", "\n", "#    raise ValueError", "\n", "compress_ratios", ".", "append", "(", "compress_ratio", ")", "\n", "compress_sents_lens", ".", "append", "(", "compressed_sent_len", ")", "\n", "original_sents_lens", ".", "append", "(", "original_sent_len", ")", "\n", "ext_label_rouge_l_scores", ".", "append", "(", "rouge_l_scores", "[", "j", "]", ")", "\n", "\n", "", "improvements_all", "+=", "js_obj", "[", "'improvements'", "]", "\n", "\n", "", "compress_ratios", "=", "np", ".", "array", "(", "compress_ratios", ")", "\n", "compress_sents_lens", "=", "np", ".", "array", "(", "compress_sents_lens", ")", "\n", "original_sents_lens", "=", "np", ".", "array", "(", "original_sents_lens", ")", "\n", "ext_label_rouge_l_scores", "=", "np", ".", "array", "(", "ext_label_rouge_l_scores", ")", "\n", "distances_two_to_one", "=", "np", ".", "array", "(", "distances_two_to_one", ")", "\n", "improvements_all", "=", "np", ".", "array", "(", "improvements_all", ")", "\n", "\n", "compress_ratios", ".", "dump", "(", "join", "(", "data_path", ",", "\"compress_ratios.dat\"", ")", ")", "\n", "compress_sents_lens", ".", "dump", "(", "join", "(", "data_path", ",", "\"compress_sents_lens.dat\"", ")", ")", "\n", "original_sents_lens", ".", "dump", "(", "join", "(", "data_path", ",", "\"original_sents_lens.dat\"", ")", ")", "\n", "ext_label_rouge_l_scores", ".", "dump", "(", "join", "(", "data_path", ",", "\"ext_label_rouge_l_scores.dat\"", ")", ")", "\n", "distances_two_to_one", ".", "dump", "(", "join", "(", "data_path", ",", "\"distances_two_to_one_{}.dat\"", ".", "format", "(", "args", ".", "threshold", ")", ")", ")", "\n", "improvements_all", ".", "dump", "(", "join", "(", "data_path", ",", "\"improvements.dat\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references.dump": [[18, 33], ["time.time", "print", "os.path.join", "os.path.join", "utils.count_data", "range", "print", "print", "open", "json.loads", "open", "f.write", "datetime.timedelta", "os.path.join", "f.read", "os.path.join", "decoding.make_html_safe", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "dump", "(", "split", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "dump_dir", "=", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "print", "(", "'processing {}/{} ({:.2f}%%)\\r'", ".", "format", "(", "i", ",", "n_data", ",", "100", "*", "i", "/", "n_data", ")", ",", "\n", "end", "=", "''", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "abs_sents", "=", "data", "[", "'abstract'", "]", "\n", "with", "open", "(", "join", "(", "dump_dir", ",", "'{}.ref'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "abs_sents", ")", ")", ")", "\n", "", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references.main": [[34, 44], ["make_eval_references.dump", "make_eval_references.dump", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "split", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'test'", "]", ":", "# evaluation of train data takes too long", "\n", "            ", "if", "not", "exists", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", "\n", "", "dump", "(", "split", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "exists", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", "\n", "", "dump", "(", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.RLDataset.__init__": [[46, 48], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.RLDataset.__getitem__": [[49, 54], ["data.data.CnnDmDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", "=", "js_data", "[", "'article'", "]", "\n", "abs_sents", "=", "js_data", "[", "'abstract'", "]", "\n", "return", "art_sents", ",", "abs_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.RLDataset_backup.__init__": [[57, 79], ["os.path.join", "os.path.join", "os.path.exists", "print", "print", "utils.count_data", "range", "print", "open", "pickle.load", "pickle.load", "pickle.load", "pickle.load", "json.load", "open", "pickle.dump", "pickle.dump", "pickle.dump", "pickle.dump", "open", "train_full_rl.RLDataset_backup.examples.append", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["    ", "def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "        ", "split_dir", "=", "os", ".", "path", ".", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "DATA_DIR", ",", "'cached_'", "+", "split", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", ":", "\n", "            ", "print", "(", "\"Loading features from cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "self", ".", "examples", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Creating features from dataset file at %s\"", ",", "DATA_DIR", ")", "\n", "self", ".", "examples", "=", "[", "]", "\n", "n_data", "=", "count_data", "(", "split_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "                ", "js", "=", "json", ".", "load", "(", "open", "(", "join", "(", "split_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "if", "js", "[", "'article'", "]", "and", "js", "[", "'abstract'", "]", ":", "\n", "                    ", "doc_sent_list", "=", "js", "[", "'article'", "]", "\n", "summary_sent_list", "=", "js", "[", "'abstract'", "]", "\n", "self", ".", "examples", ".", "append", "(", "(", "doc_sent_list", ",", "summary_sent_list", ")", ")", "\n", "\n", "", "", "print", "(", "\"Saving features into cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "'wb'", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "examples", ",", "handle", ",", "protocol", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.RLDataset_backup.__len__": [[80, 82], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.RLDataset_backup.__getitem__": [[83, 85], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "examples", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.load_ext_net": [[87, 111], ["json.load", "decoding.load_best_ckpt", "print", "pickle.load", "print", "model.extract.PtrExtractSumm.load_state_dict", "print", "open", "open", "model.extract.PtrExtractRewrittenSumm", "os.path.join", "os.path.join", "model.extract.PtrExtractRewrittenBertSumm", "model.extract.PtrExtractRewrittenSentBertSumm", "model.extract.PtrExtractRewrittenSentWordBertSumm", "model.extract.PtrExtractSumm"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "def", "load_ext_net", "(", "ext_dir", ",", "ext_type", ")", ":", "\n", "    ", "ext_meta", "=", "json", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'meta.json'", ")", ")", ")", "\n", "#assert ext_meta['net'] == 'ml_rnn_extractor'", "\n", "assert", "'ml_{}_extractor'", ".", "format", "(", "ext_type", ")", "==", "ext_meta", "[", "'net'", "]", "\n", "ext_ckpt", "=", "load_best_ckpt", "(", "ext_dir", ")", "\n", "print", "(", "\"finish load chkpt\"", ")", "\n", "#if '_extractor._stop' not in ext_ckpt:", "\n", "#    ext_ckpt['_extractor._stop'] = torch.zeros_like(ext_ckpt['_extractor._init_i'])", "\n", "ext_args", "=", "ext_meta", "[", "'net_args'", "]", "\n", "vocab", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "print", "(", "\"finish load vocab\"", ")", "\n", "if", "ext_type", "==", "\"rewritten_rnn\"", ":", "\n", "        ", "ext", "=", "PtrExtractRewrittenSumm", "(", "**", "ext_args", ")", "\n", "", "elif", "ext_type", "==", "\"rewritten_bert_rnn\"", ":", "\n", "        ", "ext", "=", "PtrExtractRewrittenBertSumm", "(", "**", "ext_args", ")", "\n", "", "elif", "ext_type", "==", "\"rewritten_sent_bert_rnn\"", ":", "\n", "        ", "ext", "=", "PtrExtractRewrittenSentBertSumm", "(", "**", "ext_args", ")", "\n", "", "elif", "ext_type", "==", "\"rewritten_sent_word_bert_rnn\"", ":", "\n", "        ", "ext", "=", "PtrExtractRewrittenSentWordBertSumm", "(", "**", "ext_args", ")", "\n", "", "else", ":", "\n", "        ", "ext", "=", "PtrExtractSumm", "(", "**", "ext_args", ")", "\n", "", "ext", ".", "load_state_dict", "(", "ext_ckpt", ")", "\n", "print", "(", "\"loaded extractor\"", ")", "\n", "return", "ext", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.configure_net": [[113, 175], ["train_full_rl.load_ext_net", "json.load", "model.rl.ActorCriticCand", "model.rl.ActorCritic.cuda", "json.load", "open", "decoding.ConditionalAbstractor", "decoding.Abstractor", "decoding.ArticleBatcher", "open", "os.path.join", "model.rl.ActorCriticSentBertCand", "os.path.join", "decoding.ArticleBatcher", "model.rl.ActorCriticSentWordBertCand", "model.rl.ActorCritic", "decoding.ArticleBatcher", "decoding.ArticleBatcher"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.load_ext_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "configure_net", "(", "abs_dir", ",", "ext_dir", ",", "ext_type", ",", "emb_type", ",", "cuda", ",", "num_candidates", "=", "1", ",", "is_conditional_abs", "=", "False", ")", ":", "\n", "    ", "\"\"\" load pretrained sub-modules and build the actor-critic network\"\"\"", "\n", "# load pretrained abstractor model", "\n", "\"\"\"\n    if abs_dir is not None:\n        abstractor = Abstractor(abs_dir, MAX_ABS_LEN, cuda)\n    else:\n        abstractor = identity\n    \"\"\"", "\n", "if", "abs_dir", "is", "None", "or", "\"rewritten\"", "in", "ext_type", ":", "\n", "        ", "abstractor", "=", "identity", "\n", "", "elif", "is_conditional_abs", ":", "\n", "        ", "abstractor", "=", "ConditionalAbstractor", "(", "abs_dir", ",", "MAX_ABS_LEN", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "MAX_ABS_LEN", ",", "cuda", ")", "\n", "\n", "# load ML trained extractor net and buiild RL agent", "\n", "", "extractor", ",", "agent_vocab", "=", "load_ext_net", "(", "ext_dir", ",", "ext_type", ")", "\n", "\n", "if", "ext_type", "==", "\"rewritten_rnn\"", ":", "\n", "        ", "assert", "num_candidates", "==", "extractor", ".", "num_candidates", "\n", "agent", "=", "ActorCriticCand", "(", "extractor", ".", "_candidate_sent_enc", ",", "\n", "extractor", ".", "_candidate_agg", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "agent_vocab", ",", "emb_type", ",", "cuda", ")", ",", "\n", "num_candidates", ")", "\n", "", "elif", "ext_type", "==", "\"rewritten_bert_rnn\"", ":", "\n", "        ", "raise", "ValueError", "\n", "", "elif", "ext_type", "==", "\"rewritten_sent_bert_rnn\"", ":", "\n", "        ", "assert", "num_candidates", "==", "extractor", ".", "num_candidates", "\n", "agent", "=", "ActorCriticSentBertCand", "(", "extractor", ".", "_sentence_encoder", ",", "\n", "extractor", ".", "_bert_w", ",", "\n", "extractor", ".", "_candidate_agg", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "agent_vocab", ",", "emb_type", ",", "cuda", ")", ",", "\n", "num_candidates", ")", "\n", "", "elif", "ext_type", "==", "\"rewritten_sent_word_bert_rnn\"", ":", "\n", "        ", "assert", "num_candidates", "==", "extractor", ".", "num_candidates", "\n", "agent", "=", "ActorCriticSentWordBertCand", "(", "extractor", ".", "_sentence_encoder", ",", "\n", "extractor", ".", "_bert_w", ",", "\n", "extractor", ".", "_candidate_sent_enc", ",", "\n", "extractor", ".", "_candidate_agg", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "agent_vocab", ",", "emb_type", ",", "cuda", ")", ",", "\n", "num_candidates", ")", "\n", "", "else", ":", "\n", "        ", "agent", "=", "ActorCritic", "(", "extractor", ".", "_sent_enc", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "agent_vocab", ",", "emb_type", ",", "cuda", ")", ")", "\n", "", "if", "cuda", ":", "\n", "        ", "agent", "=", "agent", ".", "cuda", "(", ")", "\n", "\n", "", "net_args", "=", "{", "}", "\n", "net_args", "[", "'abstractor'", "]", "=", "(", "None", "if", "abs_dir", "is", "None", "\n", "else", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "net_args", "[", "'extractor'", "]", "=", "json", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'meta.json'", ")", ")", ")", "\n", "\n", "return", "agent", ",", "agent_vocab", ",", "abstractor", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.configure_training": [[177, 194], ["None"], "function", ["None"], ["", "def", "configure_training", "(", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ",", "\n", "gamma", ",", "reward", ",", "stop_coeff", ",", "stop_reward", ")", ":", "\n", "    ", "assert", "opt", "in", "[", "'adam'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "train_params", "[", "'gamma'", "]", "=", "gamma", "\n", "train_params", "[", "'reward'", "]", "=", "reward", "\n", "train_params", "[", "'stop_coeff'", "]", "=", "stop_coeff", "\n", "train_params", "[", "'stop_reward'", "]", "=", "stop_reward", "\n", "\n", "return", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.build_batchers": [[195, 237], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "toolz.sandbox.core.unzip", "zip", "train_full_rl.RLDataset", "train_full_rl.RLDataset", "itertools.cycle", "data.batcher.tokenize", "art_sents.append", "raw_art_sents.append", "raw_abs_sents.append", "sent.split", "sent.split"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "build_batchers", "(", "batch_size", ",", "train_set_folder", ",", "valid_set_folder", ",", "emb_type", ",", "num_candidates", ",", "max_word", ")", ":", "\n", "    ", "def", "coll", "(", "batch", ")", ":", "\n", "        ", "art_batch", ",", "abs_batch", "=", "unzip", "(", "batch", ")", "\n", "\"\"\"\n        # debug\n        art_batch = list(art_batch)\n        print(\"raw batch:\")\n        print(list(art_batch)[0][0])\n        print(list(art_batch)[0][1])\n        print(list(art_batch)[0][2])\n        print(list(art_batch)[0][3])\n        \"\"\"", "\n", "#art_sents = list(filter(bool, map(tokenize(max_word, emb_type, num_candidates), art_batch)))", "\n", "#abs_sents = list(filter(bool, map(tokenize(max_word, emb_type, num_candidates), abs_batch)))", "\n", "\n", "art_sents", "=", "[", "]", "\n", "#abs_sents = []", "\n", "raw_art_sents", "=", "[", "]", "\n", "raw_abs_sents", "=", "[", "]", "\n", "for", "art", ",", "abs", "in", "zip", "(", "art_batch", ",", "abs_batch", ")", ":", "\n", "            ", "tokenized_art", "=", "tokenize", "(", "max_word", ",", "emb_type", ",", "num_candidates", ",", "art", ")", "[", ":", "args", ".", "max_sent", "]", "\n", "# tokenized_abs = tokenize(max_word, emb_type, num_candidates, abs)", "\n", "raw_art", "=", "[", "sent", ".", "split", "(", "\" \"", ")", "[", ":", "max_word", "]", "for", "sent", "in", "art", "]", "\n", "raw_abs", "=", "[", "sent", ".", "split", "(", "\" \"", ")", "[", ":", "max_word", "]", "for", "sent", "in", "abs", "]", "\n", "if", "tokenized_art", "and", "raw_abs", ":", "\n", "                ", "art_sents", ".", "append", "(", "tokenized_art", ")", "\n", "#abs_sents.append(tokenized_abs)", "\n", "raw_art_sents", ".", "append", "(", "raw_art", ")", "\n", "raw_abs_sents", ".", "append", "(", "raw_abs", ")", "\n", "\n", "", "", "return", "art_sents", ",", "raw_art_sents", ",", "raw_abs_sents", "\n", "", "loader", "=", "DataLoader", "(", "\n", "RLDataset", "(", "train_set_folder", ")", ",", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "num_workers", "=", "4", ",", "pin_memory", "=", "True", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "val_loader", "=", "DataLoader", "(", "\n", "RLDataset", "(", "valid_set_folder", ")", ",", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "pin_memory", "=", "True", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "return", "cycle", "(", "loader", ")", ",", "val_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.weighted_sum_rouge_12l": [[239, 247], ["list", "list", "metric.compute_rouge_n", "metric.compute_rouge_n", "metric.compute_rouge_l_summ", "cytoolz.concat", "cytoolz.concat"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l_summ"], ["", "def", "weighted_sum_rouge_12l", "(", "prediction_sent_list", ",", "abstract_sent_list", ")", ":", "\n", "    ", "concated_prediction", "=", "list", "(", "concat", "(", "prediction_sent_list", ")", ")", "\n", "concated_abstract", "=", "list", "(", "concat", "(", "abstract_sent_list", ")", ")", "\n", "rouge_1", "=", "compute_rouge_n", "(", "concated_prediction", ",", "concated_abstract", ",", "n", "=", "1", ",", "mode", "=", "'f'", ")", "\n", "rouge_2", "=", "compute_rouge_n", "(", "concated_prediction", ",", "concated_abstract", ",", "n", "=", "2", ",", "mode", "=", "'f'", ")", "\n", "rouge_l", "=", "compute_rouge_l_summ", "(", "prediction_sent_list", ",", "abstract_sent_list", ",", "mode", "=", "'f'", ")", "\n", "weight", "=", "1.0", "/", "3", "\n", "return", "weight", "*", "rouge_1", "+", "weight", "*", "rouge_2", "+", "weight", "*", "rouge_l", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_full_rl.train": [[249, 336], ["print", "print", "train_full_rl.configure_net", "print", "train_full_rl.configure_training", "print", "train_full_rl.build_batchers", "print", "rl.get_grad_fn", "torch.optim.Adam", "rl.A2CPipeline", "training.BasicTrainer", "print", "print", "sys.stdout.flush", "training.BasicTrainer.train", "os.path.exists", "os.makedirs", "metric.compute_rouge_n", "decoding.load_best_ckpt", "pickle.load", "os.path.join", "os.makedirs", "torch.save", "open", "json.dump", "open", "pickle.dump", "agent.parameters", "torch.optim.lr_scheduler.ReduceLROnPlateau", "metric.compute_rouge_n", "open", "os.path.join", "open", "json.dump", "os.path.join", "open", "pickle.dump", "os.path.join", "os.path.join", "metric.compute_rouge_l_summ", "metric.compute_rouge_n", "os.path.join", "os.path.join", "os.path.join", "metric.compute_weighted_rouge_1_2"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.get_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l_summ", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_weighted_rouge_1_2"], ["", "def", "train", "(", "args", ")", ":", "\n", "    ", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "\n", "", "if", "\"bert\"", "in", "args", ".", "ext_type", ":", "\n", "        ", "args", ".", "emb_type", "=", "\"bert\"", "\n", "", "else", ":", "\n", "        ", "args", ".", "emb_type", "=", "\"w2v\"", "\n", "\n", "", "print", "(", "\"emb_type\"", ")", "\n", "print", "(", "args", ".", "emb_type", ")", "\n", "\n", "# make net", "\n", "agent", ",", "agent_vocab", ",", "abstractor", ",", "net_args", "=", "configure_net", "(", "\n", "args", ".", "abs_dir", ",", "args", ".", "ext_dir", ",", "args", ".", "ext_type", ",", "args", ".", "emb_type", ",", "args", ".", "cuda", ",", "args", ".", "num_candidates", ",", "args", ".", "is_conditional_abs", ")", "\n", "\n", "print", "(", "\"network config\"", ")", "\n", "\n", "# configure training setting", "\n", "assert", "args", ".", "stop", ">", "0", "\n", "train_params", "=", "configure_training", "(", "\n", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", ",", "\n", "args", ".", "gamma", ",", "args", ".", "reward", ",", "args", ".", "stop", ",", "'rouge-1'", "\n", ")", "\n", "print", "(", "\"train config\"", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "args", ".", "batch", ",", "args", ".", "train_set_folder", ",", "args", ".", "valid_set_folder", ",", "args", ".", "emb_type", ",", "args", ".", "num_candidates", ",", "args", ".", "max_word", ")", "\n", "print", "(", "\"build batcher\"", ")", "\n", "if", "args", ".", "reward_type", "==", "0", ":", "\n", "        ", "reward_fn", "=", "compute_rouge_l", "\n", "stop_reward_fn", "=", "compute_rouge_n", "(", "n", "=", "1", ")", "\n", "", "elif", "args", ".", "reward_type", "==", "1", "or", "args", ".", "reward_type", "==", "2", ":", "\n", "        ", "reward_fn", "=", "compute_rouge_l_summ", "\n", "stop_reward_fn", "=", "compute_rouge_n", "(", "n", "=", "1", ")", "\n", "", "elif", "args", ".", "reward_type", "==", "3", ":", "\n", "        ", "reward_fn", "=", "compute_rouge_l_summ", "(", "mode", "=", "'r'", ")", "\n", "stop_reward_fn", "=", "compute_rouge_n", "(", "n", "=", "1", ")", "\n", "", "elif", "args", ".", "reward_type", "==", "4", ":", "\n", "        ", "reward_fn", "=", "compute_rouge_l_summ", "\n", "stop_reward_fn", "=", "compute_weighted_rouge_1_2", "(", "rouge_1_weight", "=", "0.5", ",", "mode", "=", "'f'", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "\n", "\n", "# save abstractor binary", "\n", "", "if", "args", ".", "abs_dir", "is", "not", "None", ":", "\n", "        ", "abs_ckpt", "=", "{", "}", "\n", "abs_ckpt", "[", "'state_dict'", "]", "=", "load_best_ckpt", "(", "args", ".", "abs_dir", ")", "\n", "abs_vocab", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "args", ".", "abs_dir", ",", "'vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "abs_dir", "=", "join", "(", "args", ".", "path", ",", "'abstractor'", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "abs_dir", ",", "'ckpt'", ")", ")", "\n", "with", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "net_args", "[", "'abstractor'", "]", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "torch", ".", "save", "(", "abs_ckpt", ",", "join", "(", "abs_dir", ",", "'ckpt/ckpt-0-0'", ")", ")", "\n", "with", "open", "(", "join", "(", "abs_dir", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pkl", ".", "dump", "(", "abs_vocab", ",", "f", ",", "protocol", "=", "4", ")", "\n", "# save configuration", "\n", "", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'{}-ext_abs_rl'", ".", "format", "(", "args", ".", "ext_type", ")", "# 'ml_{}_extractor'", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'train_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'agent_vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "agent_vocab", ",", "f", ",", "protocol", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "grad_fn", "=", "get_grad_fn", "(", "agent", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "agent", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "if", "args", ".", "no_lr_decay", ":", "\n", "        ", "scheduler", "=", "None", "\n", "", "else", ":", "\n", "        ", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'max'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "args", ".", "min_lr", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "", "pipeline", "=", "A2CPipeline", "(", "meta", "[", "'net'", "]", ",", "agent", ",", "abstractor", ",", "\n", "train_batcher", ",", "val_batcher", ",", "\n", "optimizer", ",", "grad_fn", ",", "\n", "reward_fn", ",", "args", ".", "gamma", ",", "\n", "stop_reward_fn", ",", "args", ".", "stop", ",", "args", ".", "reward_type", ",", "args", ".", "disable_selected_mask", ",", "args", ".", "is_conditional_abs", ",", "args", ".", "debug", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ",", "\n", "val_mode", "=", "'score'", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.TwoToOneMatchDataset.__init__": [[38, 41], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "threshold", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.TwoToOneMatchDataset.__getitem__": [[42, 64], ["data.data.CnnDmDataset.__getitem__", "zip", "matched_abss.append", "len", "matched_arts.append", "len", "matched_arts.append", "ValueError", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted_two_to_one_{}'", ".", "format", "(", "self", ".", "threshold", ")", "]", ")", "\n", "# only keep the sentences with two ext labels", "\n", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "for", "ext", ",", "abst", "in", "zip", "(", "extracts", ",", "abs_sents", ")", ":", "\n", "            ", "if", "len", "(", "ext", ")", "==", "2", ":", "\n", "                ", "primary_idx", "=", "ext", "[", "0", "]", "\n", "if", "primary_idx", "<", "len", "(", "art_sents", ")", "-", "1", ":", "\n", "                    ", "secondary_idx", "=", "primary_idx", "+", "1", "\n", "", "else", ":", "\n", "                    ", "secondary_idx", "=", "0", "\n", "", "matched_arts", ".", "append", "(", "art_sents", "[", "primary_idx", "]", "+", "' '", "+", "art_sents", "[", "secondary_idx", "]", ")", "\n", "", "elif", "len", "(", "ext", ")", "==", "1", ":", "\n", "                ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Bug!\"", ")", "\n", "", "matched_abss", ".", "append", "(", "abst", ")", "\n", "# return matched_arts, abs_sents[:len(extracts)]", "\n", "", "return", "matched_arts", ",", "matched_abss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.coll": [[66, 73], ["enumerate", "filtered_art_batch.append", "filtered_abs_batch.append"], "function", ["None"], ["", "", "def", "coll", "(", "batch", ")", ":", "\n", "    ", "filtered_art_batch", "=", "[", "]", "\n", "filtered_abs_batch", "=", "[", "]", "\n", "for", "batch_i", ",", "(", "art_sents", ",", "abs_sents", ")", "in", "enumerate", "(", "batch", ")", ":", "\n", "        ", "filtered_art_batch", ".", "append", "(", "art_sents", ")", "\n", "filtered_abs_batch", ".", "append", "(", "abs_sents", ")", "\n", "", "return", "filtered_art_batch", ",", "filtered_abs_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.decode": [[75, 240], ["time.time", "decode_two_to_one_next_neighbor.TwoToOneMatchDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "os.makedirs", "json.load", "print", "decoding.Abstractor", "decoding.BeamAbstractor", "list", "os.path.join", "open", "open", "json.dump", "torch.no_grad", "enumerate", "range", "os.path.join", "os.path.join", "list", "len", "print", "print", "print", "print", "print", "map", "print", "print", "print", "extractor", "list", "print", "print", "decoding.BeamAbstractor.", "decode_two_to_one_next_neighbor.rerank_mp", "decoding.BeamAbstractor.", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "data.batcher.tokenize", "map", "len", "print", "enumerate", "print", "range", "print", "print", "open", "f.write", "len", "len", "len", "print", "len", "os.path.join", "decoding.make_html_safe", "datetime.timedelta", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "decode", "(", "save_path", ",", "abs_dir", ",", "split", ",", "batch_size", ",", "beam_size", ",", "diverse", ",", "max_len", ",", "final_rerank", ",", "cuda", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "topk", "=", "1", "\n", "\n", "# setup model", "\n", "assert", "abs_dir", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "        ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "# a dummy extractor that extract all the sentences", "\n", "", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "\n", "dataset", "=", "TwoToOneMatchDataset", "(", "split", ",", "threshold", "=", "0.15", ")", "# only need json['article'] and json['abstract']", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "save_path", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output'", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "None", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "(", "raw_article_batch", ",", "raw_abs_batch", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "debug", ":", "\n", "                ", "print", "(", "\"raw article batch\"", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "print", "(", "\"article lengths\"", ")", "\n", "print", "(", "[", "len", "(", "art", ")", "for", "art", "in", "raw_article_batch", "]", ")", "\n", "# pick out the original sentence", "\n", "# raw_article_batch a list of list of sentences, article, then sentence in article", "\n", "#raw_original_article_batch = []", "\n", "#for raw_article_sents in raw_article_batch:", "\n", "#    original_article_sents = [article_sent for cand_i, article_sent in enumerate(raw_article_sents) if cand_i % exist_candidates == 0]", "\n", "#    raw_original_article_batch.append(original_article_sents)", "\n", "\n", "", "tokenized_original_article_batch", "=", "list", "(", "map", "(", "tokenize", "(", "None", ")", ",", "raw_article_batch", ")", ")", "\n", "if", "debug", ":", "\n", "                ", "print", "(", "\"tokenized_original_article_batch\"", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "\n", "", "ext_arts", "=", "[", "]", "\n", "ext_inds", "=", "[", "]", "\n", "for", "raw_art_sents", "in", "tokenized_original_article_batch", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "len", "(", "ext_arts", ")", ",", "len", "(", "ext", ")", ")", "]", "\n", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"ext_inds\"", ")", "\n", "print", "(", "ext_inds", ")", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "topk", ",", "final_rerank", ")", "\n", "# dec_outs: a list of list of token list", "\n", "", "else", ":", "\n", "                ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "\n", "#assert i == batch_size * i_debug", "\n", "", "if", "i", "!=", "batch_size", "*", "i_debug", ":", "\n", "                ", "print", "(", "\"i: {}\"", ".", "format", "(", "i", ")", ")", "\n", "print", "(", "\"batch_size: {}, i_debug: {}, batch_size * i_debug: {}\"", ".", "format", "(", "batch_size", ",", "i_debug", ",", "batch_size", "*", "i_debug", ")", ")", "\n", "raise", "ValueError", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"dec_outs[2]\"", ")", "\n", "print", "(", "dec_outs", "[", "2", "]", ")", "\n", "print", "(", "\"dec_outs[3]\"", ")", "\n", "print", "(", "dec_outs", "[", "3", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "\"\"\"\n            if i_debug == 18:\n                print(\"Length of ext_ids: {}\".format(len(ext_inds)))\n                print(\"Length of raw_rticle_batch: {}\".format(len(raw_article_batch)))\n                print(\"Length of tokenized_article_batch: {}\".format(len(list(tokenized_article_batch))))\n                print(\"i: {}\".format(i))\n            \"\"\"", "\n", "\n", "batch_i", "=", "0", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "\n", "                ", "if", "debug", ":", "\n", "                    ", "print", "(", "\"j: {}, n: {}\"", ".", "format", "(", "j", ",", "n", ")", ")", "\n", "\n", "# one article", "\n", "", "article_decoded_sents", "=", "[", "]", "# a list of all candidate sentences for one article", "\n", "\n", "if", "j", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# if the input article is not empty", "\n", "\n", "# construct a list of all candidate sentences in one article, a list of str.", "\n", "                    ", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "                        ", "candidate_list", "=", "[", "]", "\n", "\n", "# one sent", "\n", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "candidate", ")", "for", "candidate", "in", "sent", "]", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "sent", ")", "]", "\n", "\n", "#if keep_original_sent:", "\n", "#    candidate_list.insert(0, raw_article_batch[batch_i][sent_i])", "\n", "\n", "", "article_decoded_sents", "+=", "candidate_list", "\n", "# fetch the abstract of the original sample", "\n", "", "raw_abstract", "=", "raw_abs_batch", "[", "batch_i", "]", "\n", "batch_i", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raw_abstract", "=", "[", "]", "\n", "\n", "", "if", "debug", ":", "\n", "                    ", "print", "(", "\"article_decoded_sents[0]\"", ")", "\n", "for", "z", "in", "range", "(", "9", ")", ":", "\n", "                        ", "print", "(", "article_decoded_sents", "[", "z", "]", ")", "\n", "", "print", "(", "\"article_decoded_sents len\"", ")", "\n", "print", "(", "len", "(", "article_decoded_sents", ")", ")", "\n", "\n", "", "with", "open", "(", "join", "(", "save_path", ",", "'output'", ",", "'{}.dec'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "article_decoded_sents", ")", ")", ")", "\n", "\n", "", "i", "+=", "1", "\n", "\n", "\"\"\"\n                if i_debug == 18:\n                    art_len = len(json_dict['abstract'])\n                    print(\"length of current article: {}\".format(art_len))\n                    if art_len > 0:\n                        print(json_dict['abstract'][0])\n                    print(\"i increases to: {}\\n\".format(i))\n                \"\"\"", "\n", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "\"\"\"\n            if i_debug == 18:\n                raise ValueError\n            \"\"\"", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.rerank": [[248, 256], ["list", "map", "map", "cytoolz.concat", "decode_two_to_one_next_neighbor.rereank_topk_one", "decode_two_to_one_next_neighbor.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "final_rerank", ":", "\n", "        ", "topked", "=", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.rerank_mp": [[258, 267], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map", "decode_two_to_one_next_neighbor.rereank_topk_one", "decode_two_to_one_next_neighbor.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.rereank_topk_one": [[269, 287], ["map", "decode_two_to_one_next_neighbor.rereank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rereank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor.topk_one": [[289, 297], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor._make_n_gram": [[299, 301], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_next_neighbor._compute_score": [[303, 307], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.DecodeCandidateDataset.__init__": [[35, 37], ["data.data.CnnDmDatasetFromIdx.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "start_idx", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ",", "start_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.DecodeCandidateDataset.__getitem__": [[38, 43], ["data.data.CnnDmDatasetFromIdx.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", "=", "js_data", "[", "'article'", "]", "\n", "abs_sents", "=", "js_data", "[", "'abstract'", "]", "\n", "return", "art_sents", ",", "abs_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.coll": [[45, 57], ["enumerate", "filtered_art_batch.append", "filtered_abs_batch.append", "empty_data_indices.append"], "function", ["None"], ["", "", "def", "coll", "(", "batch", ")", ":", "\n", "    ", "filtered_art_batch", "=", "[", "]", "\n", "filtered_abs_batch", "=", "[", "]", "\n", "empty_data_indices", "=", "[", "]", "\n", "# filter out all empty articles", "\n", "for", "batch_i", ",", "(", "art_sents", ",", "abs_sents", ")", "in", "enumerate", "(", "batch", ")", ":", "\n", "        ", "if", "art_sents", ":", "# only keep non empty articles", "\n", "            ", "filtered_art_batch", ".", "append", "(", "art_sents", ")", "\n", "filtered_abs_batch", ".", "append", "(", "abs_sents", ")", "\n", "", "else", ":", "# log the empty idx", "\n", "            ", "empty_data_indices", ".", "append", "(", "batch_i", ")", "\n", "", "", "return", "filtered_art_batch", ",", "filtered_abs_batch", ",", "empty_data_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.decode": [[59, 243], ["time.time", "decode_compression.DecodeCandidateDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "json.load", "print", "decoding.CompressionControlAbstractor", "decoding.BeamCompressionControlAbstractor", "list", "os.path.join", "open", "open", "json.dump", "torch.no_grad", "enumerate", "range", "os.path.join", "os.path.join", "map", "zip", "len", "print", "print", "print", "print", "print", "data.batcher.tokenize", "tokenized_compress_labeled_article_batch.append", "compress_label_batch.append", "len", "len", "extractor", "list", "list", "print", "print", "decoding.BeamCompressionControlAbstractor.", "decode_compression.rerank_mp", "decoding.BeamCompressionControlAbstractor.", "print", "print", "print", "print", "print", "print", "print", "print", "print", "ext_inds.insert", "print", "range", "map", "map", "print", "enumerate", "print", "len", "print", "enumerate", "open", "f.write", "len", "tokenized_compress_labeled_sents.append", "compress_labels_in_article.append", "len", "len", "print", "print", "print", "print", "print", "os.path.join", "json.dumps", "datetime.timedelta", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp"], ["", "def", "decode", "(", "save_path", ",", "abs_dir", ",", "split", ",", "batch_size", ",", "beam_size", ",", "diverse", ",", "max_len", ",", "topk", ",", "rerank_mode", ",", "keep_original_sent", ",", "start_idx", ",", "cuda", ",", "n_compression_levels", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "assert", "abs_dir", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "        ", "abstractor", "=", "CompressionControlAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "BeamCompressionControlAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "# a dummy extractor that extract all the sentences", "\n", "", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "\n", "dataset", "=", "DecodeCandidateDataset", "(", "split", ",", "start_idx", ")", "# only need json['article'] and json['abstract']", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'{}_candidate'", ".", "format", "(", "split", ")", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "None", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "(", "raw_article_batch", ",", "raw_abs_batch", ",", "empty_data_indices", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "debug", ":", "\n", "                ", "print", "(", "\"raw article batch\"", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "print", "(", "\"article lengths\"", ")", "\n", "print", "(", "[", "len", "(", "art", ")", "for", "art", "in", "raw_article_batch", "]", ")", "\n", "", "tokenized_article_batch", "=", "map", "(", "tokenize", "(", "100", ",", "\"w2v\"", ",", "None", ")", ",", "raw_article_batch", ")", "\n", "\n", "# repeat each input article sentence for n_compression_levels times", "\n", "num_new_candidates", "=", "n_compression_levels", "\n", "tokenized_compress_labeled_article_batch", "=", "[", "]", "\n", "compress_label_batch", "=", "[", "]", "\n", "for", "tokenized_article_sents", "in", "tokenized_article_batch", ":", "\n", "# sents in one article", "\n", "                ", "tokenized_compress_labeled_sents", "=", "[", "]", "\n", "compress_labels_in_article", "=", "[", "]", "\n", "for", "tokenized_article_sent", "in", "tokenized_article_sents", ":", "\n", "                    ", "for", "level", "in", "range", "(", "n_compression_levels", ")", ":", "\n", "                        ", "tokenized_compress_labeled_sents", ".", "append", "(", "tokenized_article_sent", ")", "\n", "compress_labels_in_article", ".", "append", "(", "level", ")", "\n", "", "", "tokenized_compress_labeled_article_batch", ".", "append", "(", "tokenized_compress_labeled_sents", ")", "\n", "compress_label_batch", ".", "append", "(", "compress_labels_in_article", ")", "\n", "\n", "", "assert", "len", "(", "tokenized_compress_labeled_article_batch", ")", "==", "len", "(", "compress_label_batch", ")", "\n", "\n", "\n", "\n", "ext_arts", "=", "[", "]", "\n", "ext_inds", "=", "[", "]", "\n", "ext_compress_labels", "=", "[", "]", "\n", "for", "raw_art_sents", ",", "compress_labels", "in", "zip", "(", "tokenized_compress_labeled_article_batch", ",", "compress_label_batch", ")", ":", "\n", "# process sents in each article", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "len", "(", "ext_arts", ")", ",", "len", "(", "ext", ")", ")", "]", "\n", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "ext_compress_labels", "+=", "list", "(", "map", "(", "lambda", "i", ":", "compress_labels", "[", "i", "]", ",", "ext", ")", ")", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"ext_inds\"", ")", "\n", "print", "(", "ext_inds", ")", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "ext_compress_labels", ",", "beam_size", ",", "diverse", ")", "# all_beams: a list of beam for the whole batch", "\n", "\n", "if", "debug", ":", "\n", "                    ", "print", "(", "\"Beams:\"", ")", "\n", "for", "beam_i", ",", "beam", "in", "enumerate", "(", "all_beams", ")", ":", "\n", "                        ", "print", "(", "[", "hyp", ".", "sequence", "for", "hyp", "in", "beam", "]", ")", "\n", "", "print", "(", "\"======\"", ")", "\n", "", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "topk", ",", "rerank_mode", ")", "\n", "#dec_outs = rerank(all_beams, ext_inds, topk, rerank_mode)", "\n", "# dec_outs: a list of list of token list", "\n", "", "else", ":", "\n", "                ", "dec_outs", "=", "abstractor", "(", "ext_arts", ",", "ext_compress_labels", ")", "\n", "\n", "#assert i == batch_size * i_debug", "\n", "", "if", "i", "!=", "batch_size", "*", "i_debug", ":", "\n", "                ", "print", "(", "\"i: {}\"", ".", "format", "(", "i", ")", ")", "\n", "print", "(", "\"batch_size: {}, i_debug: {}, batch_size * i_debug: {}\"", ".", "format", "(", "batch_size", ",", "i_debug", ",", "batch_size", "*", "i_debug", ")", ")", "\n", "raise", "ValueError", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "\"\"\"\n            if i_debug == 18:\n                print(\"Length of ext_ids: {}\".format(len(ext_inds)))\n                print(\"Length of raw_rticle_batch: {}\".format(len(raw_article_batch)))\n                print(\"Length of tokenized_article_batch: {}\".format(len(list(tokenized_article_batch))))\n                print(\"i: {}\".format(i))\n            \"\"\"", "\n", "\n", "# insert place holders for samples with empty article", "\n", "for", "empty_idx", "in", "empty_data_indices", ":", "\n", "                ", "ext_inds", ".", "insert", "(", "empty_idx", ",", "(", "None", ",", "None", ")", ")", "\n", "\n", "", "batch_i", "=", "0", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "\n", "                ", "if", "debug", ":", "\n", "                    ", "print", "(", "\"j: {}, n: {}\"", ".", "format", "(", "j", ",", "n", ")", ")", "\n", "\n", "# one article", "\n", "", "article_decoded_sents", "=", "[", "]", "# a list of all candidates sentences for one article", "\n", "\n", "if", "j", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# if the input article is not empty", "\n", "# construct a list of all candidate sentences in one article, a list of str.", "\n", "                    ", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "\n", "                        ", "candidate_list", "=", "[", "]", "\n", "if", "sent_i", "%", "num_new_candidates", "==", "0", ":", "\n", "                            ", "original_sent_idx", "=", "sent_i", "//", "num_new_candidates", "\n", "candidate_list", "+=", "[", "raw_article_batch", "[", "batch_i", "]", "[", "original_sent_idx", "]", "]", "\n", "\n", "# one sent", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "candidate", ")", "for", "candidate", "in", "sent", "]", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "sent", ")", "]", "\n", "\n", "#if keep_original_sent:", "\n", "#    candidate_list.insert(0, raw_article_batch[batch_i][sent_i])", "\n", "\n", "", "article_decoded_sents", "+=", "candidate_list", "\n", "# fetch the abstract of the original sample", "\n", "", "raw_abstract", "=", "raw_abs_batch", "[", "batch_i", "]", "\n", "batch_i", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raw_abstract", "=", "[", "]", "\n", "\n", "if", "debug", ":", "\n", "                        ", "print", "(", "article_decoded_sents", "[", "0", "]", ")", "\n", "print", "(", "article_decoded_sents", "[", "1", "]", ")", "\n", "print", "(", "article_decoded_sents", "[", "2", "]", ")", "\n", "print", "(", "article_decoded_sents", "[", "3", "]", ")", "\n", "\n", "", "", "json_dict", "=", "{", "\"article\"", ":", "article_decoded_sents", ",", "\"abstract\"", ":", "raw_abstract", "}", "\n", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'{}_candidate/{}.json'", ".", "format", "(", "split", ",", "i", "+", "start_idx", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "json", ".", "dumps", "(", "json_dict", ")", ")", "\n", "", "i", "+=", "1", "\n", "\n", "\"\"\"\n                if i_debug == 18:\n                    art_len = len(json_dict['abstract'])\n                    print(\"length of current article: {}\".format(art_len))\n                    if art_len > 0:\n                        print(json_dict['abstract'][0])\n                    print(\"i increases to: {}\\n\".format(i))\n                \"\"\"", "\n", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "\"\"\"\n            if i_debug == 18:\n                raise ValueError\n            \"\"\"", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.rerank": [[251, 261], ["list", "map", "cytoolz.concat", "decode_compression.topk_one", "map", "map", "decode_compression.rerank_topk_one", "decode_compression.rerank_by_length"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_by_length"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "rerank_mode", "=", "0", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "rerank_mode", "==", "0", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "elif", "rerank_mode", "==", "1", ":", "\n", "        ", "topked", "=", "map", "(", "rerank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "rerank_by_length", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.rerank_mp": [[263, 276], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "decode_compression.topk_one", "pool.map", "decode_compression.rerank_topk_one", "pool.map", "decode_compression.rerank_by_length"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_by_length"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "rerank_mode", "=", "0", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beams for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "rerank_mode", "==", "0", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "elif", "rerank_mode", "==", "1", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rerank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "elif", "rerank_mode", "==", "2", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rerank_by_length", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.rerank_by_length": [[278, 298], ["sorted", "len", "art_dec_outs.append", "art_dec_outs.append", "len"], "function", ["None"], ["", "@", "curry", "\n", "def", "rerank_by_length", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :return:\n    \"\"\"", "\n", "# sort it according to lengths, longest sequence first.", "\n", "#beams = [beam.sort(key=lambda h: -len(h.sequence)) for beam in beams]", "\n", "beams", "=", "[", "sorted", "(", "beam", ",", "key", "=", "lambda", "h", ":", "-", "len", "(", "h", ".", "sequence", ")", ")", "for", "beam", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "beam", "in", "beams", ":", "\n", "# append the candidates for each beam", "\n", "        ", "beam_size", "=", "len", "(", "beam", ")", "\n", "if", "k", "==", "2", ":", "\n", "            ", "art_dec_outs", ".", "append", "(", "[", "beam", "[", "0", "]", ".", "sequence", ",", "beam", "[", "-", "1", "]", ".", "sequence", "]", ")", "\n", "", "elif", "k", "==", "3", ":", "\n", "            ", "art_dec_outs", ".", "append", "(", "[", "beam", "[", "0", "]", ".", "sequence", ",", "beam", "[", "beam_size", "//", "2", "]", ".", "sequence", ",", "beam", "[", "-", "1", "]", ".", "sequence", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression._compute_len_score": [[300, 302], ["len"], "function", ["None"], ["", "def", "_compute_len_score", "(", "hyp", ")", ":", "\n", "    ", "return", "len", "(", "hyp", ".", "sequence", ")", ",", "hyp", ".", "logprob", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.rerank_topk_one": [[304, 322], ["map", "decode_compression.rerank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rerank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression.topk_one": [[324, 332], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression._make_n_gram": [[334, 336], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_compression._compute_score": [[338, 342], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.A2CPipeline.__init__": [[320, 342], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "\n", "net", ",", "abstractor", ",", "\n", "train_batcher", ",", "val_batcher", ",", "\n", "optim", ",", "grad_fn", ",", "\n", "reward_fn", ",", "gamma", ",", "\n", "stop_reward_fn", ",", "stop_coeff", ",", "reward_type", ",", "disable_selected_mask", "=", "False", ",", "is_conditional_abs", "=", "False", ",", "debug", "=", "False", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "_net", "=", "net", "\n", "self", ".", "_train_batcher", "=", "train_batcher", "\n", "self", ".", "_val_batcher", "=", "val_batcher", "\n", "self", ".", "_opt", "=", "optim", "\n", "self", ".", "_grad_fn", "=", "grad_fn", "\n", "self", ".", "_abstractor", "=", "abstractor", "\n", "self", ".", "_gamma", "=", "gamma", "\n", "self", ".", "_reward_fn", "=", "reward_fn", "\n", "self", ".", "_stop_reward_fn", "=", "stop_reward_fn", "\n", "self", ".", "_stop_coeff", "=", "stop_coeff", "\n", "self", ".", "_disable_selected_mask", "=", "disable_selected_mask", "\n", "self", ".", "_n_epoch", "=", "0", "# epoch not very useful?", "\n", "self", ".", "_reward_type", "=", "reward_type", "\n", "self", ".", "debug", "=", "debug", "\n", "self", ".", "_is_conditional_abs", "=", "is_conditional_abs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.A2CPipeline.batches": [[343, 345], ["NotImplementedError"], "methods", ["None"], ["", "def", "batches", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'A2C does not use batcher'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.A2CPipeline.train_step": [[346, 359], ["rl.A2CPipeline._net.train", "rl.a2c_train_step"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.a2c_train_step"], ["", "def", "train_step", "(", "self", ")", ":", "\n", "# forward pass of model", "\n", "        ", "self", ".", "_net", ".", "train", "(", ")", "\n", "#print('train step')", "\n", "#sys.stdout.flush()", "\n", "log_dict", "=", "a2c_train_step", "(", "\n", "self", ".", "_net", ",", "self", ".", "_abstractor", ",", "\n", "self", ".", "_train_batcher", ",", "\n", "self", ".", "_opt", ",", "self", ".", "_grad_fn", ",", "\n", "self", ".", "_gamma", ",", "self", ".", "_reward_fn", ",", "\n", "self", ".", "_stop_reward_fn", ",", "self", ".", "_stop_coeff", ",", "self", ".", "_reward_type", ",", "self", ".", "_disable_selected_mask", ",", "self", ".", "_is_conditional_abs", ",", "self", ".", "debug", "\n", ")", "\n", "return", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.A2CPipeline.validate": [[360, 362], ["rl.a2c_validate"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.a2c_validate"], ["", "def", "validate", "(", "self", ")", ":", "\n", "        ", "return", "a2c_validate", "(", "self", ".", "_net", ",", "self", ".", "_abstractor", ",", "self", ".", "_val_batcher", ",", "self", ".", "_disable_selected_mask", ",", "self", ".", "_is_conditional_abs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.A2CPipeline.checkpoint": [[363, 366], ["super().checkpoint"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.checkpoint"], ["", "def", "checkpoint", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# explicitly use inherited function in case I forgot :)", "\n", "        ", "return", "super", "(", ")", ".", "checkpoint", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.A2CPipeline.terminate": [[367, 369], ["None"], "methods", ["None"], ["", "def", "terminate", "(", "self", ")", ":", "\n", "        ", "pass", "# No extra processs so do nothing", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.a2c_validate": [[23, 106], ["agent.eval", "time.time", "print", "print", "torch.no_grad", "len", "enumerate", "zip", "datetime.timedelta", "agent", "abstractor", "abstractor", "metric.compute_rouge_n", "len", "enumerate", "list", "list", "int", "len", "len", "cytoolz.concat", "cytoolz.concat", "len", "sequential_ext_sents.append", "sequential_article_ids.append", "idx.item", "len", "sequential_ext_sents[].append", "sequential_article_ids[].append", "time.time", "range", "range", "idx.item", "idx.item", "len", "len", "len", "idx.item"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n"], ["def", "a2c_validate", "(", "agent", ",", "abstractor", ",", "loader", ",", "disable_selected_mask", "=", "False", ",", "is_conditional_abs", "=", "False", ")", ":", "\n", "    ", "agent", ".", "eval", "(", ")", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start running validation...'", ",", "end", "=", "''", ")", "\n", "avg_reward", "=", "0", "\n", "i", "=", "0", "\n", "\n", "# debug", "\n", "#extracted_local_idx_2dlist = []", "\n", "#num_batches = 0", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "art_batch", ",", "raw_art_batch", ",", "raw_abs_batch", "in", "loader", ":", "\n", "\n", "# debug", "\n", "#num_batches += 1", "\n", "\n", "            ", "num_articles", "=", "len", "(", "art_batch", ")", "\n", "num_ext_sents", "=", "0", "\n", "if", "is_conditional_abs", ":", "\n", "                ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "ext_sents", "=", "[", "]", "\n", "", "ext_inds", "=", "[", "]", "\n", "for", "article_i", ",", "raw_arts_tokenized", "in", "enumerate", "(", "art_batch", ")", ":", "\n", "\n", "# debug", "\n", "                ", "\"\"\"\n                if num_batches == 2 and article_i == 27:\n                    print(\"disable_selected_mask\")\n                    print(disable_selected_mask)\n                    print(\"raw_arts_tokenized\")\n                    print(raw_arts_tokenized)\n                    exit()\n                \"\"\"", "\n", "\n", "indices", "=", "agent", "(", "raw_arts_tokenized", ",", "disable_selected_mask", ")", "\n", "ext_inds", "+=", "[", "(", "num_ext_sents", ",", "len", "(", "indices", ")", "-", "1", ")", "]", "\n", "num_ext_sents", "+=", "len", "(", "indices", ")", "-", "1", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "                    ", "num_selected_sents_excluded_eos", "=", "len", "(", "indices", ")", "-", "1", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                        ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "idx_i", ",", "idx", "in", "enumerate", "(", "indices", ")", ":", "\n", "                        ", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", ":", "\n", "# ext_sents.append(raw_arts_tokenized[idx.item()])", "\n", "                            ", "sequential_ext_sents", "[", "idx_i", "]", ".", "append", "(", "raw_arts_tokenized", "[", "idx", ".", "item", "(", ")", "]", ")", "\n", "sequential_article_ids", "[", "idx_i", "]", ".", "append", "(", "article_i", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "ext_sents", "+=", "[", "raw_art_batch", "[", "article_i", "]", "[", "idx", ".", "item", "(", ")", "]", "for", "idx", "in", "indices", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", "]", "\n", "\n", "# debug", "\n", "#extracted_local_idx_2dlist.append([idx.item() for idx in indices if idx.item() < len(raw_arts_tokenized)])", "\n", "\n", "# abstract", "\n", "", "", "if", "is_conditional_abs", ":", "\n", "                ", "all_summs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "                ", "all_summs", "=", "abstractor", "(", "ext_sents", ")", "\n", "\n", "", "for", "(", "j", ",", "n", ")", ",", "abs_sents", "in", "zip", "(", "ext_inds", ",", "raw_abs_batch", ")", ":", "\n", "                ", "summs", "=", "all_summs", "[", "j", ":", "j", "+", "n", "]", "\n", "# python ROUGE-1 (not official evaluation)", "\n", "avg_reward", "+=", "compute_rouge_n", "(", "list", "(", "concat", "(", "summs", ")", ")", ",", "\n", "list", "(", "concat", "(", "abs_sents", ")", ")", ",", "n", "=", "1", ")", "\n", "\n", "i", "+=", "1", "\n", "", "", "", "avg_reward", "/=", "(", "i", "/", "100", ")", "\n", "print", "(", "'finished in {}! avg reward: {:.2f}'", ".", "format", "(", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", ",", "avg_reward", ")", ")", "\n", "\n", "# debug", "\n", "#extracted_local_idx_2darray = np.array(extracted_local_idx_2dlist)", "\n", "#extracted_local_idx_2darray.dump('/home/ubuntu/ken/projects/abstract_then_extract/val_selected_indices_2d.dat')", "\n", "\n", "return", "{", "'reward'", ":", "avg_reward", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.a2c_train_step": [[108, 292], ["metric.compute_rouge_n", "opt.zero_grad", "next", "len", "enumerate", "zip", "list", "list", "list", "torch.Tensor().to", "torch.cat().squeeze", "zip", "torch.nn.functional.mse_loss", "torch.autograd.backward", "grad_fn", "opt.step", "log_dict.update", "F.mse_loss.item", "agent", "list.append", "list.append", "list.append", "torch.no_grad", "cytoolz.concat", "cytoolz.concat", "cytoolz.concat", "losses.append", "len", "avg_advantage.item", "len", "math.isnan", "enumerate", "abstractor", "abstractor", "len", "len", "len", "disc_rs.insert", "torch.Tensor", "torch.Tensor().to.mean", "torch.Tensor().to.std", "float", "torch.cat", "len", "len", "torch.tensor().to", "len", "sequential_ext_sents.append", "sequential_article_ids.append", "idx.item", "len", "sequential_ext_sents[].append", "sequential_article_ids[].append", "reward_fn", "numpy.finfo", "p.log_prob", "len", "torch.ones().to", "range", "range", "idx.item", "idx.item", "len", "reward_fn", "stop_reward_fn", "range", "len", "reward_fn", "torch.tensor", "range", "range", "list", "list", "stop_reward_fn", "range", "len", "torch.ones", "len", "len", "idx.item", "min", "max", "cytoolz.concat", "cytoolz.concat", "len", "range", "list", "list", "min", "stop_reward_fn", "len", "cytoolz.concat", "cytoolz.concat", "len", "range", "range", "list", "list", "len", "len", "len", "len", "max", "cytoolz.concat", "cytoolz.concat", "len", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n"], ["", "def", "a2c_train_step", "(", "agent", ",", "abstractor", ",", "loader", ",", "opt", ",", "grad_fn", ",", "\n", "gamma", "=", "0.99", ",", "reward_fn", "=", "compute_rouge_l", ",", "\n", "stop_reward_fn", "=", "compute_rouge_n", "(", "n", "=", "1", ")", ",", "stop_coeff", "=", "1.0", ",", "reward_type", "=", "0", ",", "disable_selected_mask", "=", "False", ",", "is_conditional_abs", "=", "False", ",", "debug", "=", "False", ")", ":", "\n", "#print('a2c train step')", "\n", "#sys.stdout.flush()", "\n", "    ", "opt", ".", "zero_grad", "(", ")", "\n", "indices", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "baselines", "=", "[", "]", "\n", "if", "is_conditional_abs", ":", "\n", "        ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "        ", "ext_sents", "=", "[", "]", "\n", "", "art_batch", ",", "raw_art_batch", ",", "raw_abs_batch", "=", "next", "(", "loader", ")", "\n", "#print('Loader next')", "\n", "#sys.stdout.flush()", "\n", "num_articles", "=", "len", "(", "art_batch", ")", "\n", "# extract", "\n", "for", "article_i", ",", "raw_arts_tokenized", "in", "enumerate", "(", "art_batch", ")", ":", "# extract sent indices for each article", "\n", "        ", "\"\"\"\n        if debug:\n            print(\"raw_arts_tokenized[0:5]\")\n            print(\" \".join(raw_arts_tokenized[0]))\n            print(\" \".join(raw_arts_tokenized[1]))\n            print(\" \".join(raw_arts_tokenized[2]))\n            print(\" \".join(raw_arts_tokenized[3]))\n        \"\"\"", "\n", "(", "inds", ",", "ms", ")", ",", "bs", "=", "agent", "(", "raw_arts_tokenized", ",", "disable_selected_mask", ",", "debug", "=", "debug", ")", "\n", "\"\"\"\n        if debug:\n            print(\"inds length: {}\".format(len(inds)))\n            print(\"ms shape: {}\".format(len(ms)))\n            print(\"bs shape: {}\".format(len(bs)))\n        \"\"\"", "\n", "\n", "baselines", ".", "append", "(", "bs", ")", "\n", "indices", ".", "append", "(", "inds", ")", "\n", "probs", ".", "append", "(", "ms", ")", "\n", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "            ", "num_selected_sents_excluded_eos", "=", "len", "(", "inds", ")", "-", "1", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "i", ",", "idx", "in", "enumerate", "(", "inds", ")", ":", "\n", "                ", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", ":", "\n", "#ext_sents.append(raw_arts_tokenized[idx.item()])", "\n", "                    ", "sequential_ext_sents", "[", "i", "]", ".", "append", "(", "raw_arts_tokenized", "[", "idx", ".", "item", "(", ")", "]", ")", "\n", "sequential_article_ids", "[", "i", "]", ".", "append", "(", "article_i", ")", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "ext_sents", "+=", "[", "raw_art_batch", "[", "article_i", "]", "[", "idx", ".", "item", "(", ")", "]", "\n", "for", "idx", "in", "inds", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", "]", "\n", "\n", "# abstract", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "if", "is_conditional_abs", ":", "\n", "            ", "summaries", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "            ", "summaries", "=", "abstractor", "(", "ext_sents", ")", "\n", "\n", "", "", "i", "=", "0", "\n", "rewards", "=", "[", "]", "\n", "avg_reward", "=", "0", "\n", "#reward_lens = []", "\n", "#print(\"len indices and batch\")", "\n", "#print(len(indices))", "\n", "#print(len(abs_batch))", "\n", "#print()", "\n", "#print()", "\n", "for", "inds", ",", "abss", "in", "zip", "(", "indices", ",", "raw_abs_batch", ")", ":", "\n", "# process each article", "\n", "        ", "if", "reward_type", "==", "0", ":", "\n", "            ", "rs", "=", "(", "[", "reward_fn", "(", "summaries", "[", "i", "+", "j", "]", ",", "abss", "[", "j", "]", ")", "for", "j", "in", "range", "(", "min", "(", "len", "(", "inds", ")", "-", "1", ",", "len", "(", "abss", ")", ")", ")", "]", "\n", "+", "[", "0", "for", "_", "in", "range", "(", "max", "(", "0", ",", "len", "(", "inds", ")", "-", "1", "-", "len", "(", "abss", ")", ")", ")", "]", "\n", "+", "[", "stop_coeff", "*", "stop_reward_fn", "(", "\n", "list", "(", "concat", "(", "summaries", "[", "i", ":", "i", "+", "len", "(", "inds", ")", "-", "1", "]", ")", ")", ",", "\n", "list", "(", "concat", "(", "abss", ")", ")", ")", "]", ")", "\n", "", "elif", "reward_type", "==", "1", ":", "\n", "            ", "sent_reward", "=", "[", "reward_fn", "(", "summaries", "[", "i", ":", "i", "+", "j", "]", ",", "abss", ")", "for", "j", "in", "range", "(", "len", "(", "inds", ")", "-", "1", ")", "]", "\n", "shaped_reward", "=", "[", "sent_reward", "[", "0", "]", "]", "+", "[", "sent_reward", "[", "i", "+", "1", "]", "-", "sent_reward", "[", "i", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sent_reward", ")", "-", "1", ")", "]", "if", "len", "(", "sent_reward", ")", ">", "0", "else", "[", "]", "\n", "\n", "rs", "=", "shaped_reward", "+", "[", "stop_coeff", "*", "stop_reward_fn", "(", "\n", "list", "(", "concat", "(", "summaries", "[", "i", ":", "i", "+", "len", "(", "inds", ")", "-", "1", "]", ")", ")", ",", "\n", "list", "(", "concat", "(", "abss", ")", ")", ")", "]", "\n", "#print(\"rs: {}\".format(rs))", "\n", "", "elif", "reward_type", "==", "2", "or", "reward_type", "==", "3", "or", "reward_type", "==", "4", ":", "\n", "# debug", "\n", "#print(\"abss\")", "\n", "#print(abss)", "\n", "#print()", "\n", "#print(\"prediction\")", "\n", "#print([ (i,i + j) for j in range(min(len(inds) - 1, len(abss)))])", "\n", "#print( [summaries[i:i + j] for j in range( min(len(inds)-1, len(abss)) )] )", "\n", "#print()", "\n", "\n", "            ", "sent_reward", "=", "[", "reward_fn", "(", "summaries", "[", "i", ":", "i", "+", "j", "+", "1", "]", ",", "abss", ")", "for", "j", "in", "range", "(", "min", "(", "len", "(", "inds", ")", "-", "1", ",", "len", "(", "abss", ")", ")", ")", "]", "\n", "shaped_reward", "=", "[", "sent_reward", "[", "0", "]", "]", "+", "[", "sent_reward", "[", "i", "+", "1", "]", "-", "sent_reward", "[", "i", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sent_reward", ")", "-", "1", ")", "]", "if", "len", "(", "sent_reward", ")", ">", "0", "else", "[", "]", "\n", "\n", "# debug", "\n", "#print(\"sent_reward\")", "\n", "#print(sent_reward)", "\n", "#print(\"shaped_reward\")", "\n", "#print(shaped_reward)", "\n", "#print()", "\n", "\n", "rs", "=", "shaped_reward", "+", "[", "0", "for", "_", "in", "range", "(", "max", "(", "0", ",", "len", "(", "inds", ")", "-", "1", "-", "len", "(", "abss", ")", ")", ")", "]", "+", "[", "stop_coeff", "*", "stop_reward_fn", "(", "\n", "list", "(", "concat", "(", "summaries", "[", "i", ":", "i", "+", "len", "(", "inds", ")", "-", "1", "]", ")", ")", ",", "\n", "list", "(", "concat", "(", "abss", ")", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "#print(\"inds: {}\".format(inds))", "\n", "#print(\"rs: {}\".format(rs))", "\n", "#if len(inds) - 1 == 0:", "\n", "#    print(\"rs: {}\".format(rs))", "\n", "#    print(\"stop reward: {}\".format([stop_coeff*stop_reward_fn(", "\n", "#              list(concat(summaries[i:i+len(inds)-1])),", "\n", "#              list(concat(abss)))]))", "\n", "\n", "", "assert", "len", "(", "rs", ")", "==", "len", "(", "inds", ")", "\n", "#raise ValueError", "\n", "\n", "avg_reward", "+=", "rs", "[", "-", "1", "]", "/", "stop_coeff", "\n", "i", "+=", "len", "(", "inds", ")", "-", "1", "\n", "# compute discounted rewards", "\n", "R", "=", "0", "\n", "disc_rs", "=", "[", "]", "# a list of discounted reward, with len=len(inds)", "\n", "for", "r", "in", "rs", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "R", "=", "r", "+", "gamma", "*", "R", "\n", "disc_rs", ".", "insert", "(", "0", ",", "R", ")", "\n", "#reward_lens.append(len(disc_rs))", "\n", "", "rewards", "+=", "disc_rs", "# a list of all discounted reward in the batch.", "\n", "\n", "# baselines", "\n", "", "\"\"\"\n    print(\"length of reward lens\")\n    print(len(reward_lens))\n    print(reward_lens)\n    print()\n    print(\"baselines\")\n    print(len(baselines))\n    print()\n    for b in baselines:\n        print(len(b))\n    print()\n    print(\"inds\")\n    for inds in indices:\n        print(len(inds))\n    \"\"\"", "\n", "\n", "indices", "=", "list", "(", "concat", "(", "indices", ")", ")", "\n", "probs", "=", "list", "(", "concat", "(", "probs", ")", ")", "\n", "baselines", "=", "list", "(", "concat", "(", "baselines", ")", ")", "\n", "# standardize rewards", "\n", "reward", "=", "torch", ".", "Tensor", "(", "rewards", ")", ".", "to", "(", "baselines", "[", "0", "]", ".", "device", ")", "\n", "reward", "=", "(", "reward", "-", "reward", ".", "mean", "(", ")", ")", "/", "(", "\n", "reward", ".", "std", "(", ")", "+", "float", "(", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ")", ")", "\n", "baseline", "=", "torch", ".", "cat", "(", "baselines", ")", ".", "squeeze", "(", ")", "\n", "avg_advantage", "=", "0", "\n", "losses", "=", "[", "]", "\n", "for", "action", ",", "p", ",", "r", ",", "b", "in", "zip", "(", "indices", ",", "probs", ",", "reward", ",", "baseline", ")", ":", "\n", "        ", "advantage", "=", "r", "-", "b", "\n", "avg_advantage", "+=", "advantage", "\n", "losses", ".", "append", "(", "-", "p", ".", "log_prob", "(", "action", ")", "\n", "*", "(", "advantage", "/", "len", "(", "indices", ")", ")", ")", "# divide by T*B", "\n", "", "critic_loss", "=", "F", ".", "mse_loss", "(", "baseline", ",", "reward", ")", "\n", "# backprop and update", "\n", "autograd", ".", "backward", "(", "\n", "[", "critic_loss", "]", "+", "losses", ",", "\n", "[", "torch", ".", "tensor", "(", "1.0", ")", ".", "to", "(", "critic_loss", ".", "device", ")", "]", "+", "[", "torch", ".", "ones", "(", "1", ")", ".", "to", "(", "critic_loss", ".", "device", ")", "]", "*", "(", "len", "(", "losses", ")", ")", "\n", ")", "\n", "grad_log", "=", "grad_fn", "(", ")", "\n", "opt", ".", "step", "(", ")", "\n", "log_dict", "=", "{", "}", "\n", "log_dict", ".", "update", "(", "grad_log", ")", "\n", "log_dict", "[", "'reward'", "]", "=", "avg_reward", "/", "len", "(", "art_batch", ")", "\n", "log_dict", "[", "'advantage'", "]", "=", "avg_advantage", ".", "item", "(", ")", "/", "len", "(", "indices", ")", "\n", "log_dict", "[", "'mse'", "]", "=", "critic_loss", ".", "item", "(", ")", "\n", "assert", "not", "math", ".", "isnan", "(", "log_dict", "[", "'grad_norm'", "]", ")", "\n", "return", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.rl.get_grad_fn": [[294, 317], ["agent.named_children", "torch.nn.utils.clip_grad_norm_", "agent.parameters", "m.parameters", "print", "p.grad.norm"], "function", ["None"], ["", "def", "get_grad_fn", "(", "agent", ",", "clip_grad", ",", "max_grad", "=", "1e2", ")", ":", "\n", "    ", "\"\"\" monitor gradient for each sub-component\"\"\"", "\n", "params", "=", "[", "p", "for", "p", "in", "agent", ".", "parameters", "(", ")", "]", "\n", "def", "f", "(", ")", ":", "\n", "        ", "grad_log", "=", "{", "}", "\n", "for", "n", ",", "m", "in", "agent", ".", "named_children", "(", ")", ":", "\n", "            ", "tot_grad", "=", "0", "\n", "for", "p", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "tot_grad", "+=", "p", ".", "grad", ".", "norm", "(", "2", ")", "**", "2", "\n", "", "", "tot_grad", "=", "tot_grad", "**", "(", "1", "/", "2", ")", "\n", "#grad_log['grad_norm'+n] = tot_grad.item()", "\n", "grad_log", "[", "'grad_norm'", "+", "n", "]", "=", "tot_grad", "\n", "", "grad_norm", "=", "clip_grad_norm_", "(", "\n", "[", "p", "for", "p", "in", "params", "if", "p", ".", "requires_grad", "]", ",", "clip_grad", ")", "\n", "grad_norm", "=", "grad_norm", "\n", "#grad_norm = grad_norm.item()", "\n", "if", "max_grad", "is", "not", "None", "and", "grad_norm", ">=", "max_grad", ":", "\n", "            ", "print", "(", "'WARNING: Exploding Gradients {:.2f}'", ".", "format", "(", "grad_norm", ")", ")", "\n", "grad_norm", "=", "max_grad", "\n", "", "grad_log", "[", "'grad_norm'", "]", "=", "grad_norm", "\n", "return", "grad_log", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_pubmed_training_data._split_words": [[24, 26], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_pubmed_training_data.check_empty": [[28, 33], ["len"], "function", ["None"], ["", "def", "check_empty", "(", "sents", ")", ":", "\n", "    ", "if", "len", "(", "sents", ")", "==", "1", "and", "sents", "[", "0", "]", "==", "\"\"", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_pubmed_training_data.process": [[35, 50], ["os.path.join", "clean_pubmed_training_data.check_empty", "clean_pubmed_training_data.check_empty", "open", "json.loads", "open", "json.dump", "os.path.join", "f.read", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.check_empty", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.check_empty", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "", "@", "curry", "\n", "def", "process", "(", "split", ",", "i", ")", ":", "\n", "    ", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "art_sents", "=", "data", "[", "'article'", "]", "\n", "abs_sents", "=", "data", "[", "'abstract'", "]", "\n", "if", "check_empty", "(", "art_sents", ")", ":", "\n", "        ", "data", "[", "'article'", "]", "=", "[", "]", "\n", "data", "[", "'extracted'", "]", "=", "[", "]", "\n", "", "if", "check_empty", "(", "abs_sents", ")", ":", "\n", "        ", "data", "[", "'abstract'", "]", "=", "[", "]", "\n", "data", "[", "'extracted'", "]", "=", "[", "]", "\n", "", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_pubmed_training_data.label_mp": [[52, 62], ["time.time", "print", "os.path.join", "utils.count_data", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "clean_pubmed_training_data.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "label_mp", "(", "split", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "split", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_pubmed_training_data.label": [[64, 73], ["time.time", "print", "os.path.join", "utils.count_data", "range", "print", "clean_pubmed_training_data.process", "datetime.timedelta", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "def", "label", "(", "split", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "process", "(", "split", ",", "i", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_pubmed_training_data.main": [[75, 82], ["clean_pubmed_training_data.label_mp", "clean_pubmed_training_data.label_mp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp"], ["", "def", "main", "(", "split", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'train'", ",", "'test'", "]", ":", "\n", "            ", "label_mp", "(", "split", ")", "\n", "#label(split)", "\n", "", "", "else", ":", "\n", "        ", "label_mp", "(", "split", ")", "\n", "#label(split)", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.build_vocab_pubmed.main": [[8, 26], ["collections.Counter", "enumerate", "print", "open", "f_in.readlines", "line.strip().split", "int", "open", "pickle.dump", "os.path.join", "os.path.join", "line.strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["def", "main", "(", "data_dir", ")", ":", "\n", "    ", "with", "open", "(", "join", "(", "data_dir", ",", "\"vocab\"", ")", ")", "as", "f_in", ":", "\n", "        ", "all_lines", "=", "f_in", ".", "readlines", "(", ")", "\n", "\n", "", "vocab_counter", "=", "Counter", "(", ")", "\n", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "all_lines", ")", ":", "\n", "        ", "word", ",", "count", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "vocab_counter", "[", "word", "]", "=", "int", "(", "count", ")", "\n", "#print(word)", "\n", "#print(int(count))", "\n", "#print(vocab_counter)", "\n", "#exit()", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"vocab_cnt.pkl\"", ")", ",", "\n", "'wb'", ")", "as", "vocab_file", ":", "\n", "        ", "pkl", ".", "dump", "(", "vocab_counter", ",", "vocab_file", ",", "protocol", "=", "4", ")", "\n", "", "print", "(", "\"Finished!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.ControllableMatchDataset.__init__": [[58, 62], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "filter_out_low_recall", "=", "False", ",", "n_compression_levels", "=", "2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "filter_out_low_recall", "=", "filter_out_low_recall", "\n", "self", ".", "n_compression_level", "=", "n_compression_levels", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.ControllableMatchDataset._compute_level_from_ratio": [[63, 67], ["None"], "methods", ["None"], ["", "def", "_compute_level_from_ratio", "(", "self", ",", "ratio", ")", ":", "\n", "        ", "if", "self", ".", "n_compression_level", "==", "2", ":", "\n", "            ", "level", "=", "0", "if", "ratio", "<=", "0.5", "else", "1", "\n", "", "return", "level", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.ControllableMatchDataset.__getitem__": [[68, 89], ["data.data.CnnDmDataset.__getitem__", "enumerate", "list", "zip", "map", "matched_arts.append", "matched_abss.append", "list.append", "len", "train_controllable_abstractor.ControllableMatchDataset._compute_level_from_ratio", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.ControllableMatchDataset._compute_level_from_ratio"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", ",", "scores", ",", "compression_ratios", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted'", "]", ",", "js_data", "[", "'score'", "]", ",", "js_data", "[", "'compression_ratios'", "]", ")", "\n", "\n", "if", "self", ".", "filter_out_low_recall", ":", "\n", "            ", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "matched_levels", "=", "[", "]", "\n", "\n", "for", "ext_i", ",", "(", "ext", ",", "abst", ",", "compression_ratio", ")", "in", "enumerate", "(", "zip", "(", "extracts", ",", "abs_sents", ",", "compression_ratios", ")", ")", ":", "\n", "                ", "if", "scores", "[", "ext_i", "]", ">=", "0.2", ":", "\n", "                    ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "]", ")", "\n", "matched_abss", ".", "append", "(", "abst", ")", "\n", "matched_levels", ".", "append", "(", "self", ".", "_compute_level_from_ratio", "(", "compression_ratio", ")", ")", "\n", "", "", "", "else", ":", "\n", "            ", "matched_arts", "=", "[", "art_sents", "[", "i", "]", "for", "i", "in", "extracts", "]", "\n", "matched_abss", "=", "abs_sents", "[", ":", "len", "(", "extracts", ")", "]", "\n", "matched_levels", "=", "list", "(", "map", "(", "self", ".", "_compute_level_from_ratio", ",", "compression_ratios", "[", ":", "len", "(", "extracts", ")", "]", ")", ")", "\n", "\n", "", "return", "matched_arts", ",", "matched_abss", ",", "matched_levels", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.configure_net": [[91, 108], ["model.controllable_abstractor.CompressControlSumm", "sum", "print", "p.numel", "model.controllable_abstractor.CompressControlSumm.parameters"], "function", ["None"], ["", "", "def", "configure_net", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "n_compression_levels", ")", ":", "\n", "    ", "net_args", "=", "{", "}", "\n", "net_args", "[", "'vocab_size'", "]", "=", "vocab_size", "\n", "net_args", "[", "'emb_dim'", "]", "=", "emb_dim", "\n", "net_args", "[", "'n_hidden'", "]", "=", "n_hidden", "\n", "net_args", "[", "'bidirectional'", "]", "=", "bidirectional", "\n", "net_args", "[", "'n_layer'", "]", "=", "n_layer", "\n", "net_args", "[", "'n_compression_levels'", "]", "=", "n_compression_levels", "\n", "\n", "net", "=", "CompressControlSumm", "(", "**", "net_args", ")", "\n", "\n", "# print number of trainable parameters", "\n", "trainable_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "net", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "\"Number of trainable parameters: {}\"", ".", "format", "(", "trainable_params", ")", ")", "\n", "\n", "return", "net", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.configure_training": [[110, 127], ["torch.nn.functional.nll_loss", "model.util.sequence_loss"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss"], ["", "def", "configure_training", "(", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\" supports Adam optimizer only\"\"\"", "\n", "assert", "opt", "in", "[", "'adam'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "\n", "nll", "=", "lambda", "logit", ",", "target", ":", "F", ".", "nll_loss", "(", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "def", "criterion", "(", "logits", ",", "targets", ")", ":", "\n", "        ", "return", "sequence_loss", "(", "logits", ",", "targets", ",", "nll", ",", "pad_idx", "=", "PAD", ")", "\n", "\n", "", "return", "criterion", ",", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.build_batchers": [[129, 156], ["data.batcher.prepro_fn_control", "cytoolz.compose", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "data.batcher.batchify_fn_copy_control", "data.batcher.convert_batch_copy_control", "train_controllable_abstractor.ControllableMatchDataset", "train_controllable_abstractor.ControllableMatchDataset", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_control", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy_control", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy_control"], ["", "def", "build_batchers", "(", "word2id", ",", "cuda", ",", "filter_out_low_recall", ",", "debug", ")", ":", "\n", "    ", "prepro", "=", "prepro_fn_control", "(", "args", ".", "max_art", ",", "args", ".", "max_abs", ")", "# tokenize", "\n", "def", "sort_key", "(", "sample", ")", ":", "\n", "        ", "src", ",", "target", ",", "level", "=", "sample", "\n", "return", "(", "len", "(", "target", ")", ",", "len", "(", "src", ")", ")", "\n", "", "batchify", "=", "compose", "(", "\n", "batchify_fn_copy_control", "(", "PAD", ",", "START", ",", "END", ",", "cuda", "=", "cuda", ")", ",", "# padding", "\n", "convert_batch_copy_control", "(", "UNK", ",", "word2id", ")", "# convert to idx", "\n", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "\n", "ControllableMatchDataset", "(", "'train'", ",", "filter_out_low_recall", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "not", "debug", ",", "\n", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_control_gen", "\n", ")", "\n", "train_batcher", "=", "BucketedGenerater", "(", "train_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "False", ",", "fork", "=", "not", "debug", ")", "\n", "\n", "val_loader", "=", "DataLoader", "(", "\n", "ControllableMatchDataset", "(", "'val'", ",", "filter_out_low_recall", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_control_gen", "\n", ")", "\n", "val_batcher", "=", "BucketedGenerater", "(", "val_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "fork", "=", "not", "debug", ")", "\n", "return", "train_batcher", ",", "val_batcher", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_controllable_abstractor.main": [[158, 213], ["utils.make_vocab", "train_controllable_abstractor.build_batchers", "train_controllable_abstractor.configure_net", "train_controllable_abstractor.configure_training", "training.basic_validate", "training.get_basic_grad_fn", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "training.BasicPipeline", "training.BasicTrainer", "print", "print", "training.BasicTrainer.train", "open", "pickle.load", "len", "utils.make_embedding", "net.cuda.set_embedding", "os.path.exists", "os.makedirs", "open", "pickle.dump", "open", "json.dump", "net.cuda.parameters", "net.cuda.cuda", "os.path.join", "os.path.join", "os.path.join", "utils.make_vocab.items"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "args", ")", ":", "\n", "# create data batcher, vocabulary", "\n", "# batcher", "\n", "    ", "with", "open", "(", "join", "(", "DATA_DIR", ",", "'vocab_cnt.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "wc", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "word2id", "=", "make_vocab", "(", "wc", ",", "args", ".", "vsize", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "word2id", ",", "\n", "args", ".", "cuda", ",", "args", ".", "filter_out_low_recall", ",", "args", ".", "debug", ")", "\n", "\n", "# make net", "\n", "net", ",", "net_args", "=", "configure_net", "(", "len", "(", "word2id", ")", ",", "args", ".", "emb_dim", ",", "\n", "args", ".", "n_hidden", ",", "args", ".", "bi", ",", "args", ".", "n_layer", ",", "args", ".", "n_compression_levels", ")", "\n", "if", "args", ".", "w2v", ":", "\n", "# NOTE: the pretrained embedding having the same dimension", "\n", "#       as args.emb_dim should already be trained", "\n", "        ", "embedding", ",", "_", "=", "make_embedding", "(", "\n", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", ",", "args", ".", "w2v", ")", "\n", "net", ".", "set_embedding", "(", "embedding", ")", "\n", "\n", "# configure training setting", "\n", "", "criterion", ",", "train_params", "=", "configure_training", "(", "\n", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", "\n", ")", "\n", "\n", "# save experiment setting", "\n", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "word2id", ",", "f", ",", "protocol", "=", "4", ")", "\n", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'controllable_abstractor'", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'traing_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "val_fn", "=", "basic_validate", "(", "net", ",", "criterion", ")", "\n", "grad_fn", "=", "get_basic_grad_fn", "(", "net", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "0", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "", "pipeline", "=", "BasicPipeline", "(", "meta", "[", "'net'", "]", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "args", ".", "batch", ",", "val_fn", ",", "\n", "criterion", ",", "optimizer", ",", "grad_fn", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.copy_sentences.process": [[22, 49], ["os.path.join", "os.path.join", "os.path.join", "range", "open", "json.loads", "open", "json.loads", "len", "trg_sent_cands.insert", "open", "json.dump", "os.path.join", "f.read", "os.path.join", "f.read", "len", "len", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "@", "curry", "\n", "def", "process", "(", "src_folder", ",", "trg_folder", ",", "out_folder", ",", "k_src", ",", "k_trg", ",", "copy_i", ",", "insert_i", ",", "i", ")", ":", "\n", "    ", "src_data_dir", "=", "join", "(", "DATA_DIR", ",", "src_folder", ")", "\n", "trg_data_dir", "=", "join", "(", "DATA_DIR", ",", "trg_folder", ")", "\n", "out_data_dir", "=", "join", "(", "DATA_DIR", ",", "out_folder", ")", "\n", "with", "open", "(", "join", "(", "src_data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "src_data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "with", "open", "(", "join", "(", "trg_data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "trg_data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "src_art_sents", "=", "src_data", "[", "'article'", "]", "\n", "trg_art_sents", "=", "trg_data", "[", "'article'", "]", "\n", "trg_abs_sents", "=", "trg_data", "[", "'abstract'", "]", "\n", "assert", "len", "(", "src_art_sents", ")", "%", "k_src", "==", "0", "\n", "assert", "len", "(", "trg_art_sents", ")", "%", "k_trg", "==", "0", "\n", "num_original_sents", "=", "len", "(", "trg_art_sents", ")", "//", "k_trg", "\n", "appended_trg_art_sents", "=", "[", "]", "\n", "for", "sent_i", "in", "range", "(", "num_original_sents", ")", ":", "\n", "        ", "trg_local_sent_i", "=", "sent_i", "*", "k_trg", "\n", "src_local_sent_i", "=", "sent_i", "*", "k_src", "\n", "trg_sent_cands", "=", "trg_art_sents", "[", "trg_local_sent_i", ":", "trg_local_sent_i", "+", "k_trg", "]", "\n", "src_sent_cands", "=", "src_art_sents", "[", "src_local_sent_i", ":", "src_local_sent_i", "+", "k_src", "]", "\n", "trg_sent_cands", ".", "insert", "(", "insert_i", ",", "src_sent_cands", "[", "copy_i", "]", ")", "\n", "appended_trg_art_sents", "+=", "trg_sent_cands", "\n", "\n", "", "new_json_dict", "=", "{", "\"article\"", ":", "appended_trg_art_sents", ",", "\"abstract\"", ":", "trg_abs_sents", "}", "\n", "with", "open", "(", "join", "(", "out_data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "new_json_dict", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.copy_sentences.copy_mp": [[51, 65], ["time.time", "print", "os.path.join", "utils.count_data", "os.path.join", "os.makedirs", "copy_sentences.process", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "copy_sentences.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "copy_mp", "(", "src_folder", ",", "trg_folder", ",", "out_folder", ",", "k_src", ",", "k_trg", ",", "copy_i", ",", "insert_i", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "trg_folder", ")", ")", "\n", "trg_data_dir", "=", "join", "(", "DATA_DIR", ",", "trg_folder", ")", "\n", "n_data", "=", "count_data", "(", "trg_data_dir", ")", "\n", "# make output folder", "\n", "out_data_dir", "=", "join", "(", "DATA_DIR", ",", "out_folder", ")", "\n", "os", ".", "makedirs", "(", "out_data_dir", ")", "\n", "process", "(", "src_folder", ",", "trg_folder", ",", "out_folder", ",", "k_src", ",", "k_trg", ",", "copy_i", ",", "insert_i", ",", "1000", ")", "\n", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "src_folder", ",", "trg_folder", ",", "out_folder", ",", "k_src", ",", "k_trg", ",", "copy_i", ",", "insert_i", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.copy_sentences.main": [[67, 70], ["copy_sentences.copy_mp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.copy_sentences.copy_mp"], ["", "def", "main", "(", "src_folder", ",", "trg_folder", ",", "out_folder", ",", "k_src", ",", "k_trg", ",", "copy_i", ",", "insert_i", ")", ":", "\n", "    ", "copy_mp", "(", "src_folder", ",", "trg_folder", ",", "out_folder", ",", "k_src", ",", "k_trg", ",", "copy_i", ",", "insert_i", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_full_model.main": [[16, 36], ["os.path.join", "os.path.join", "os.path.exists", "print", "open", "evaluate.eval_rouge", "evaluate.eval_meteor", "open", "f.write", "os.path.join", "json.loads", "os.path.join", "f.read"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_rouge", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_meteor"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "dec_dir", "=", "join", "(", "args", ".", "decode_dir", ",", "'output'", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'log.json'", ")", ")", "as", "f", ":", "\n", "        ", "split", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "[", "'split'", "]", "\n", "", "ref_dir", "=", "join", "(", "_DATA_DIR", ",", "'refs'", ",", "split", ")", "\n", "assert", "exists", "(", "ref_dir", ")", "\n", "\n", "if", "args", ".", "rouge", ":", "\n", "        ", "dec_pattern", "=", "r'(\\d+).dec'", "\n", "ref_pattern", "=", "'#ID#.ref'", "\n", "output", "=", "eval_rouge", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "metric", "=", "'rouge'", "\n", "", "else", ":", "\n", "        ", "dec_pattern", "=", "'[0-9]+.dec'", "\n", "ref_pattern", "=", "'[0-9]+.ref'", "\n", "output", "=", "eval_meteor", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "metric", "=", "'meteor'", "\n", "", "print", "(", "output", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'{}.txt'", ".", "format", "(", "metric", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.compute_doc_len_stat.main": [[15, 46], ["os.path.join", "utils.count_data", "range", "numpy.array", "numpy.array", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "json.load", "np.array.append", "np.array.append", "open", "len", "len", "len", "np.array.max", "np.array.mean", "np.array.std", "np.array.max", "np.array.mean", "np.array.std", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "data_path", "=", "join", "(", "DATA_DIR", ",", "args", ".", "split", ")", "\n", "\n", "n_data", "=", "count_data", "(", "data_path", ")", "\n", "doc_sents_numbers", "=", "[", "]", "\n", "sum_sents_numbers", "=", "[", "]", "\n", "num_long_doc", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "print", "(", "'processing {}/{} ({:.2f}%%)\\r'", ".", "format", "(", "i", ",", "n_data", ",", "100", "*", "i", "/", "n_data", ")", ",", "\n", "end", "=", "''", ")", "\n", "js_obj", "=", "json", ".", "load", "(", "open", "(", "join", "(", "data_path", ",", "\"{}.json\"", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "abstract", "=", "js_obj", "[", "'abstract'", "]", "\n", "article", "=", "js_obj", "[", "'article'", "]", "\n", "doc_sents_numbers", ".", "append", "(", "len", "(", "article", ")", ")", "\n", "sum_sents_numbers", ".", "append", "(", "len", "(", "abstract", ")", ")", "\n", "if", "len", "(", "article", ")", ">", "400", ":", "\n", "            ", "num_long_doc", "+=", "1", "\n", "\n", "", "", "doc_sents_numbers", "=", "np", ".", "array", "(", "doc_sents_numbers", ")", "\n", "sum_sents_numbers", "=", "np", ".", "array", "(", "sum_sents_numbers", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"doc max: {}\"", ".", "format", "(", "doc_sents_numbers", ".", "max", "(", ")", ")", ")", "\n", "print", "(", "\"doc mean: {}\"", ".", "format", "(", "doc_sents_numbers", ".", "mean", "(", ")", ")", ")", "\n", "print", "(", "\"doc std: {}\"", ".", "format", "(", "doc_sents_numbers", ".", "std", "(", ")", ")", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"sum max: {}\"", ".", "format", "(", "sum_sents_numbers", ".", "max", "(", ")", ")", ")", "\n", "print", "(", "\"sum mean: {}\"", ".", "format", "(", "sum_sents_numbers", ".", "mean", "(", ")", ")", ")", "\n", "print", "(", "\"sum std: {}\"", ".", "format", "(", "sum_sents_numbers", ".", "std", "(", ")", ")", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"percent of long doc: {}\"", ".", "format", "(", "num_long_doc", "/", "n_data", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.TestDataset.__init__": [[102, 106], ["os.path.join", "BART_summarization_prediction._count_data"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data"], ["    ", "def", "__init__", "(", "self", ",", "data_dir", ",", "split", ")", ":", "\n", "        ", "assert", "split", "in", "[", "'test'", "]", "\n", "self", ".", "_data_path", "=", "join", "(", "data_dir", ",", "split", ")", "\n", "self", ".", "_n_data", "=", "_count_data", "(", "self", ".", "_data_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.TestDataset.__len__": [[107, 109], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.TestDataset.__getitem__": [[110, 114], ["open", "json.loads", "os.path.join", "f.read"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ":", "int", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "_data_path", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "js", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "return", "\" \"", ".", "join", "(", "js", "[", "\"article\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.set_seed": [[80, 85], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction._count_data": [[87, 94], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["", "", "def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.make_html_safe": [[96, 99], ["s.replace().replace", "s.replace"], "function", ["None"], ["", "def", "make_html_safe", "(", "s", ")", ":", "\n", "    ", "\"\"\"Rouge use html, has to make output html safe\"\"\"", "\n", "return", "s", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.load_and_cache_examples": [[116, 119], ["BART_summarization_prediction.TestDataset"], "function", ["None"], ["", "", "def", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "split", ")", ":", "\n", "    ", "dataset", "=", "TestDataset", "(", "data_dir", "=", "args", ".", "data_dir", ",", "split", "=", "split", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.coll": [[121, 129], ["tokenizer.prepare_seq2seq_batch", "src_text.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "coll", "(", "batch", ",", "tokenizer", ",", "input_trunc_len", "=", "512", ")", ":", "\n", "    ", "src_text", "=", "[", "]", "\n", "for", "doc_str", "in", "batch", ":", "\n", "        ", "src_text", ".", "append", "(", "doc_str", ")", "\n", "", "batch", "=", "tokenizer", ".", "prepare_seq2seq_batch", "(", "src_text", ",", "truncation", "=", "True", ",", "padding", "=", "'longest'", ",", "return_tensors", "=", "\"pt\"", ",", "max_length", "=", "input_trunc_len", ")", "\n", "\n", "return", "batch", ".", "input_ids", ",", "batch", ".", "attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.predict": [[131, 188], ["BART_summarization_prediction.load_and_cache_examples", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "model.eval", "enumerate", "tqdm.tqdm", "context.to.to", "attention_mask.to.to", "input_ids.size", "outputs_tensor.squeeze.tolist", "BART_summarization_prediction.coll", "torch.no_grad", "torch.no_grad", "model.generate", "len", "outputs_tensor.squeeze.squeeze", "tokenizer.decode", "tokenizer.decode.split", "len", "open", "f.write", "enumerate", "os.path.join", "BART_summarization_prediction.make_html_safe"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.load_and_cache_examples", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.coll", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.decode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "predict", "(", "args", ",", "model", ",", "max_output_length", ",", "tokenizer", ",", "split", ",", "temperature", "=", "1", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "repetition_penalty", "=", "1.0", ")", ":", "\n", "    ", "predict_dataset", "=", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "split", "=", "split", ")", "\n", "predict_sampler", "=", "SequentialSampler", "(", "predict_dataset", ")", "\n", "# TLDR_id_list = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"<control><summarize>:\"))", "\n", "pad_idx", "=", "tokenizer", ".", "pad_token_id", "\n", "#assert pad_idx == 0", "\n", "eos_idx", "=", "tokenizer", ".", "eos_token_id", "\n", "num_exported_samples", "=", "0", "\n", "predict_dataloader", "=", "DataLoader", "(", "predict_dataset", ",", "sampler", "=", "predict_sampler", ",", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "collate_fn", "=", "coll", "(", "tokenizer", "=", "tokenizer", ",", "\n", "input_trunc_len", "=", "args", ".", "input_trunc_length", ")", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "batch_i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "predict_dataloader", ")", ")", ":", "\n", "        ", "context", ",", "attention_mask", "=", "batch", "\n", "# attn_mask", "\n", "context", "=", "context", ".", "to", "(", "args", ".", "device", ")", "\n", "attention_mask", "=", "attention_mask", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "input_ids", "=", "context", "\n", "input_sequence_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "\n", "#print(\"input_ids size\")", "\n", "#print(input_ids.size())", "\n", "#print(\"mask size\")", "\n", "#print(attention_mask.size())", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "outputs_tensor", "=", "model", ".", "generate", "(", "input_ids", "=", "input_ids", ",", "max_length", "=", "max_output_length", ",", "\n", "do_sample", "=", "args", ".", "do_sample", ",", "\n", "num_beams", "=", "args", ".", "beam_size", ",", "temperature", "=", "args", ".", "temperature", ",", "top_k", "=", "args", ".", "top_k", ",", "\n", "top_p", "=", "args", ".", "top_p", ",", "repetition_penalty", "=", "args", ".", "repetition_penalty", ",", "\n", "pad_token_id", "=", "pad_idx", ",", "num_return_sequences", "=", "1", ",", "\n", "attention_mask", "=", "attention_mask", ")", "# tensor:  [batch, max_out_seq_len]", "\n", "\n", "", "if", "len", "(", "outputs_tensor", ".", "shape", ")", ">", "2", ":", "\n", "            ", "outputs_tensor", "=", "outputs_tensor", ".", "squeeze", "(", ")", "\n", "\n", "#print(\"output_tensor_size\")", "\n", "#print(outputs_tensor.size())", "\n", "\n", "# remove the input ids from output tensor", "\n", "# outputs_tensor = outputs_tensor[:, input_sequence_length:]", "\n", "\n", "", "outputs", "=", "outputs_tensor", ".", "tolist", "(", ")", "\n", "for", "out_ids", "in", "outputs", ":", "\n", "            ", "eos_positions", "=", "[", "position", "for", "position", ",", "word_id", "in", "enumerate", "(", "out_ids", ")", "if", "word_id", "==", "eos_idx", "]", "\n", "if", "len", "(", "eos_positions", ")", ">", "0", ":", "\n", "                ", "end_position", "=", "eos_positions", "[", "0", "]", "\n", "out_ids", "=", "out_ids", "[", ":", "end_position", "]", "\n", "", "out_text", "=", "tokenizer", ".", "decode", "(", "out_ids", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "\n", "decode_out_sent_list", "=", "out_text", ".", "split", "(", "\"<n>\"", ")", "\n", "#decode_out_sent_list = nltk.tokenize.sent_tokenize(out_text)", "\n", "# output the predicted sentences to a file", "\n", "with", "open", "(", "join", "(", "args", ".", "pred_path", ",", "'output/{}.dec'", ".", "format", "(", "num_exported_samples", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "decode_out_sent_list", ")", ")", ")", "\n", "", "num_exported_samples", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.main": [[190, 265], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.makedirs", "os.makedirs", "json.dump", "torch.device", "torch.device", "BART_summarization_prediction.set_seed", "parser.parse_args.model_type.lower", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "model_class.from_pretrained.to", "model_class.from_pretrained.eval", "logger.info", "BART_summarization_prediction.predict", "os.path.join", "vars", "open", "os.path.join", "logger.info", "torch.cuda.is_available", "torch.cuda.is_available", "MODEL_CLASSES.keys"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.set_seed", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.BART_summarization_prediction.predict"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Model type selected in the list: \"", "+", "\", \"", ".", "join", "(", "MODEL_CLASSES", ".", "keys", "(", ")", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name selected in the list\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pred_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The path of output dir.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The path of the directory containing all the splits.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--split\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The split to be predicted.\"", ")", "\n", "# parser.add_argument(\"--prompt\", type=str, default=\"\")", "\n", "# parser.add_argument(\"--padding_text\", type=str, default=\"\")", "\n", "parser", ".", "add_argument", "(", "\"--xlm_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Optional language when used with the XLM model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_output_length\"", ",", "type", "=", "int", ",", "default", "=", "120", ")", "\n", "parser", ".", "add_argument", "(", "'--input_trunc_length'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "'Max length of output.'", ")", "\n", "# parser.add_argument(\"--num_samples\", type=int, default=1)", "\n", "parser", ".", "add_argument", "(", "\"--temperature\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"temperature of 0 implies greedy sampling\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--repetition_penalty\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"primarily useful for CTRL model; in that case, use 1.2\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--beam_size\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_sample\"", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Do sampling or not\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--top_k\"", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "\"--top_p\"", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "8", ")", "\n", "parser", ".", "add_argument", "(", "\"--multiple_reference\"", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "help", "=", "\"for duc2002\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "# parser.add_argument('--stop_token', type=str, default=None,", "\n", "#                    help=\"Token at which text generation is stopped\")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "pred_path", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "args", ".", "pred_path", ",", "'output'", ")", ")", "\n", "\n", "json", ".", "dump", "(", "vars", "(", "args", ")", ",", "open", "(", "join", "(", "args", ".", "pred_path", ",", "'log.json'", ")", ",", "'w'", ")", ")", "\n", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "1", "\n", "args", ".", "overwrite_cache", "=", "False", "\n", "# args.n_gpu = torch.cuda.device_count()", "\n", "\n", "set_seed", "(", "args", ")", "\n", "\n", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "\n", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "tokenizer_name", "if", "args", ".", "tokenizer_name", "else", "args", ".", "model_name_or_path", ")", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "if", "args", ".", "max_output_length", "<", "0", "and", "model", ".", "config", ".", "max_position_embeddings", ">", "0", ":", "\n", "        ", "args", ".", "max_output_length", "=", "model", ".", "config", ".", "max_position_embeddings", "\n", "", "elif", "0", "<", "model", ".", "config", ".", "max_position_embeddings", "<", "args", ".", "max_output_length", ":", "\n", "        ", "args", ".", "max_output_length", "=", "model", ".", "config", ".", "max_position_embeddings", "# No generation bigger than model size", "\n", "", "elif", "args", ".", "max_output_length", "<", "0", ":", "\n", "        ", "args", ".", "max_output_length", "=", "MAX_LENGTH", "# avoid infinite loop", "\n", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "if", "args", ".", "model_type", "in", "[", "\"ctrl\"", "]", ":", "\n", "        ", "if", "args", ".", "temperature", ">", "0.7", ":", "\n", "            ", "logger", ".", "info", "(", "'CTRL typically works better with lower temperatures (and lower top_k).'", ")", "\n", "\n", "", "", "predict", "(", "args", ",", "model", ",", "args", ".", "max_output_length", ",", "tokenizer", ",", "args", ".", "split", ",", "args", ".", "temperature", ",", "args", ".", "top_k", ",", "args", ".", "top_p", ",", "\n", "args", ".", "repetition_penalty", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.TwoToOneMatchDataset.__init__": [[38, 41], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "threshold", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.TwoToOneMatchDataset.__getitem__": [[42, 60], ["data.data.CnnDmDataset.__getitem__", "zip", "matched_abss.append", "len", "matched_arts.append", "len", "matched_arts.append", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted_two_to_one_{}'", ".", "format", "(", "self", ".", "threshold", ")", "]", ")", "\n", "# only keep the sentences with two ext labels", "\n", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "\n", "for", "ext", ",", "abst", "in", "zip", "(", "extracts", ",", "abs_sents", ")", ":", "\n", "            ", "if", "len", "(", "ext", ")", "==", "2", ":", "\n", "                ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "[", "0", "]", "]", "+", "' '", "+", "art_sents", "[", "ext", "[", "1", "]", "]", ")", "\n", "", "elif", "len", "(", "ext", ")", "==", "1", ":", "\n", "                ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Bug!\"", ")", "\n", "", "matched_abss", ".", "append", "(", "abst", ")", "\n", "# return matched_arts, abs_sents[:len(extracts)]", "\n", "", "return", "matched_arts", ",", "matched_abss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.coll": [[62, 69], ["enumerate", "filtered_art_batch.append", "filtered_abs_batch.append"], "function", ["None"], ["", "", "def", "coll", "(", "batch", ")", ":", "\n", "    ", "filtered_art_batch", "=", "[", "]", "\n", "filtered_abs_batch", "=", "[", "]", "\n", "for", "batch_i", ",", "(", "art_sents", ",", "abs_sents", ")", "in", "enumerate", "(", "batch", ")", ":", "\n", "        ", "filtered_art_batch", ".", "append", "(", "art_sents", ")", "\n", "filtered_abs_batch", ".", "append", "(", "abs_sents", ")", "\n", "", "return", "filtered_art_batch", ",", "filtered_abs_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.decode": [[71, 236], ["time.time", "decode_two_to_one_oracle.TwoToOneMatchDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "os.makedirs", "json.load", "print", "decoding.Abstractor", "decoding.BeamAbstractor", "list", "os.path.join", "open", "open", "json.dump", "torch.no_grad", "enumerate", "range", "os.path.join", "os.path.join", "list", "len", "print", "print", "print", "print", "print", "map", "print", "print", "print", "extractor", "list", "print", "print", "decoding.BeamAbstractor.", "decode_two_to_one_oracle.rerank_mp", "decoding.BeamAbstractor.", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "data.batcher.tokenize", "map", "len", "print", "enumerate", "print", "range", "print", "print", "open", "f.write", "len", "len", "len", "print", "len", "os.path.join", "decoding.make_html_safe", "datetime.timedelta", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "decode", "(", "save_path", ",", "abs_dir", ",", "split", ",", "batch_size", ",", "beam_size", ",", "diverse", ",", "max_len", ",", "final_rerank", ",", "cuda", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "topk", "=", "1", "\n", "\n", "# setup model", "\n", "assert", "abs_dir", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "        ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "# a dummy extractor that extract all the sentences", "\n", "", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "\n", "dataset", "=", "TwoToOneMatchDataset", "(", "split", ",", "threshold", "=", "0.15", ")", "# only need json['article'] and json['abstract']", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "save_path", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output'", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "None", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "(", "raw_article_batch", ",", "raw_abs_batch", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "debug", ":", "\n", "                ", "print", "(", "\"raw article batch\"", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "print", "(", "\"article lengths\"", ")", "\n", "print", "(", "[", "len", "(", "art", ")", "for", "art", "in", "raw_article_batch", "]", ")", "\n", "# pick out the original sentence", "\n", "# raw_article_batch a list of list of sentences, article, then sentence in article", "\n", "#raw_original_article_batch = []", "\n", "#for raw_article_sents in raw_article_batch:", "\n", "#    original_article_sents = [article_sent for cand_i, article_sent in enumerate(raw_article_sents) if cand_i % exist_candidates == 0]", "\n", "#    raw_original_article_batch.append(original_article_sents)", "\n", "\n", "", "tokenized_original_article_batch", "=", "list", "(", "map", "(", "tokenize", "(", "None", ")", ",", "raw_article_batch", ")", ")", "\n", "if", "debug", ":", "\n", "                ", "print", "(", "\"tokenized_original_article_batch\"", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "\n", "", "ext_arts", "=", "[", "]", "\n", "ext_inds", "=", "[", "]", "\n", "for", "raw_art_sents", "in", "tokenized_original_article_batch", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "len", "(", "ext_arts", ")", ",", "len", "(", "ext", ")", ")", "]", "\n", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"ext_inds\"", ")", "\n", "print", "(", "ext_inds", ")", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "topk", ",", "final_rerank", ")", "\n", "# dec_outs: a list of list of token list", "\n", "", "else", ":", "\n", "                ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "\n", "#assert i == batch_size * i_debug", "\n", "", "if", "i", "!=", "batch_size", "*", "i_debug", ":", "\n", "                ", "print", "(", "\"i: {}\"", ".", "format", "(", "i", ")", ")", "\n", "print", "(", "\"batch_size: {}, i_debug: {}, batch_size * i_debug: {}\"", ".", "format", "(", "batch_size", ",", "i_debug", ",", "batch_size", "*", "i_debug", ")", ")", "\n", "raise", "ValueError", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"dec_outs[2]\"", ")", "\n", "print", "(", "dec_outs", "[", "2", "]", ")", "\n", "print", "(", "\"dec_outs[3]\"", ")", "\n", "print", "(", "dec_outs", "[", "3", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "\"\"\"\n            if i_debug == 18:\n                print(\"Length of ext_ids: {}\".format(len(ext_inds)))\n                print(\"Length of raw_rticle_batch: {}\".format(len(raw_article_batch)))\n                print(\"Length of tokenized_article_batch: {}\".format(len(list(tokenized_article_batch))))\n                print(\"i: {}\".format(i))\n            \"\"\"", "\n", "\n", "batch_i", "=", "0", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "\n", "                ", "if", "debug", ":", "\n", "                    ", "print", "(", "\"j: {}, n: {}\"", ".", "format", "(", "j", ",", "n", ")", ")", "\n", "\n", "# one article", "\n", "", "article_decoded_sents", "=", "[", "]", "# a list of all candidate sentences for one article", "\n", "\n", "if", "j", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# if the input article is not empty", "\n", "\n", "# construct a list of all candidate sentences in one article, a list of str.", "\n", "                    ", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "                        ", "candidate_list", "=", "[", "]", "\n", "\n", "# one sent", "\n", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "candidate", ")", "for", "candidate", "in", "sent", "]", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "sent", ")", "]", "\n", "\n", "#if keep_original_sent:", "\n", "#    candidate_list.insert(0, raw_article_batch[batch_i][sent_i])", "\n", "\n", "", "article_decoded_sents", "+=", "candidate_list", "\n", "# fetch the abstract of the original sample", "\n", "", "raw_abstract", "=", "raw_abs_batch", "[", "batch_i", "]", "\n", "batch_i", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raw_abstract", "=", "[", "]", "\n", "\n", "", "if", "debug", ":", "\n", "                    ", "print", "(", "\"article_decoded_sents[0]\"", ")", "\n", "for", "z", "in", "range", "(", "9", ")", ":", "\n", "                        ", "print", "(", "article_decoded_sents", "[", "z", "]", ")", "\n", "", "print", "(", "\"article_decoded_sents len\"", ")", "\n", "print", "(", "len", "(", "article_decoded_sents", ")", ")", "\n", "\n", "", "with", "open", "(", "join", "(", "save_path", ",", "'output'", ",", "'{}.dec'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "article_decoded_sents", ")", ")", ")", "\n", "\n", "", "i", "+=", "1", "\n", "\n", "\"\"\"\n                if i_debug == 18:\n                    art_len = len(json_dict['abstract'])\n                    print(\"length of current article: {}\".format(art_len))\n                    if art_len > 0:\n                        print(json_dict['abstract'][0])\n                    print(\"i increases to: {}\\n\".format(i))\n                \"\"\"", "\n", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "\"\"\"\n            if i_debug == 18:\n                raise ValueError\n            \"\"\"", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.rerank": [[244, 252], ["list", "map", "map", "cytoolz.concat", "decode_two_to_one_oracle.rereank_topk_one", "decode_two_to_one_oracle.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "final_rerank", ":", "\n", "        ", "topked", "=", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.rerank_mp": [[254, 263], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map", "decode_two_to_one_oracle.rereank_topk_one", "decode_two_to_one_oracle.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.rereank_topk_one": [[265, 283], ["map", "decode_two_to_one_oracle.rereank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rereank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle.topk_one": [[285, 293], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle._make_n_gram": [[295, 297], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_oracle._compute_score": [[299, 303], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_acl.main": [[11, 23], ["os.path.join", "print", "evaluate.eval_rouge", "evaluate.eval_meteor"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_rouge", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_meteor"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "dec_dir", "=", "args", ".", "decode_dir", "\n", "ref_dir", "=", "join", "(", "_REF_DIR", ",", "'reference'", ")", "\n", "if", "args", ".", "rouge", ":", "\n", "        ", "dec_pattern", "=", "r'(\\d+).dec'", "\n", "ref_pattern", "=", "'#ID#.ref'", "\n", "output", "=", "eval_rouge", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "", "else", ":", "\n", "        ", "dec_pattern", "=", "'[0-9]+.dec'", "\n", "ref_pattern", "=", "'[0-9]+.ref'", "\n", "output", "=", "eval_meteor", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "", "print", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_conditional_abstractor.ConditionalMatchDataset.__init__": [[57, 60], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "filter_out_low_recall", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "filter_out_low_recall", "=", "filter_out_low_recall", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_conditional_abstractor.ConditionalMatchDataset.__getitem__": [[61, 85], ["data.data.CnnDmDataset.__getitem__", "enumerate", "zip", "matched_arts.append", "matched_abss.append", "matched_mems.append", "matched_mems.append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", ",", "scores", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted'", "]", ",", "js_data", "[", "'score'", "]", ")", "\n", "\n", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "matched_mems", "=", "[", "]", "\n", "\n", "for", "ext_i", ",", "(", "ext", ",", "abst", ")", "in", "enumerate", "(", "zip", "(", "extracts", ",", "abs_sents", ")", ")", ":", "\n", "            ", "if", "not", "self", ".", "filter_out_low_recall", "or", "scores", "[", "ext_i", "]", ">=", "0.2", ":", "\n", "                ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "]", ")", "\n", "matched_abss", ".", "append", "(", "abst", ")", "\n", "if", "ext_i", "==", "0", ":", "# the external memory for the first sentence in an article is always empty", "\n", "                    ", "matched_mems", ".", "append", "(", "'<empty_mem>'", ")", "\n", "", "else", ":", "\n", "                    ", "matched_mems", ".", "append", "(", "' '", ".", "join", "(", "abs_sents", "[", ":", "ext_i", "]", ")", ")", "\n", "\n", "#print(\"matched_arts: \")", "\n", "#print(matched_arts)", "\n", "#print(\"matched_abss: \")", "\n", "#print(matched_abss)", "\n", "\n", "", "", "", "return", "matched_arts", ",", "matched_abss", ",", "matched_mems", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_conditional_abstractor.configure_net": [[87, 98], ["model.copy_cond_summ.CopyCondSumm"], "function", ["None"], ["", "", "def", "configure_net", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ")", ":", "\n", "    ", "net_args", "=", "{", "}", "\n", "net_args", "[", "'vocab_size'", "]", "=", "vocab_size", "\n", "net_args", "[", "'emb_dim'", "]", "=", "emb_dim", "\n", "net_args", "[", "'n_hidden'", "]", "=", "n_hidden", "\n", "net_args", "[", "'bidirectional'", "]", "=", "bidirectional", "\n", "net_args", "[", "'n_layer'", "]", "=", "n_layer", "\n", "\n", "net", "=", "CopyCondSumm", "(", "**", "net_args", ")", "\n", "return", "net", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_conditional_abstractor.configure_training": [[100, 117], ["torch.nn.functional.nll_loss", "model.util.sequence_loss"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss"], ["", "def", "configure_training", "(", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\" supports Adam optimizer only\"\"\"", "\n", "assert", "opt", "in", "[", "'adam'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "\n", "nll", "=", "lambda", "logit", ",", "target", ":", "F", ".", "nll_loss", "(", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "def", "criterion", "(", "logits", ",", "targets", ")", ":", "\n", "        ", "return", "sequence_loss", "(", "logits", ",", "targets", ",", "nll", ",", "pad_idx", "=", "PAD", ")", "\n", "\n", "", "return", "criterion", ",", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_conditional_abstractor.build_batchers": [[118, 145], ["data.batcher.prepro_fn_cond", "cytoolz.compose", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "data.batcher.batchify_fn_copy_cond", "data.batcher.convert_batch_copy_cond", "train_conditional_abstractor.ConditionalMatchDataset", "train_conditional_abstractor.ConditionalMatchDataset", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_cond", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy_cond", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy_cond"], ["", "def", "build_batchers", "(", "word2id", ",", "cuda", ",", "filter_out_low_recall", ",", "debug", ")", ":", "\n", "    ", "prepro", "=", "prepro_fn_cond", "(", "args", ".", "max_art", ",", "args", ".", "max_abs", ")", "\n", "def", "sort_key", "(", "sample", ")", ":", "\n", "        ", "src", ",", "target", ",", "mem", "=", "sample", "\n", "return", "(", "len", "(", "target", ")", ",", "len", "(", "src", ")", ",", "len", "(", "mem", ")", ")", "\n", "", "batchify", "=", "compose", "(", "\n", "batchify_fn_copy_cond", "(", "PAD", ",", "START", ",", "END", ",", "cuda", "=", "cuda", ")", ",", "# padding", "\n", "convert_batch_copy_cond", "(", "UNK", ",", "word2id", ")", "# convert to idx", "\n", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "\n", "ConditionalMatchDataset", "(", "'train'", ",", "filter_out_low_recall", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "not", "debug", ",", "\n", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_cond_gen", "\n", ")", "\n", "train_batcher", "=", "BucketedGenerater", "(", "train_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "False", ",", "fork", "=", "not", "debug", ")", "\n", "\n", "val_loader", "=", "DataLoader", "(", "\n", "ConditionalMatchDataset", "(", "'val'", ",", "filter_out_low_recall", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_cond_gen", "\n", ")", "\n", "val_batcher", "=", "BucketedGenerater", "(", "val_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "fork", "=", "not", "debug", ")", "\n", "return", "train_batcher", ",", "val_batcher", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_conditional_abstractor.main": [[146, 201], ["utils.make_vocab_cond", "train_conditional_abstractor.build_batchers", "train_conditional_abstractor.configure_net", "train_conditional_abstractor.configure_training", "training.basic_validate", "training.get_basic_grad_fn", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "training.BasicPipeline", "training.BasicTrainer", "print", "print", "training.BasicTrainer.train", "open", "pickle.load", "len", "utils.make_embedding", "net.cuda.set_embedding", "os.path.exists", "os.makedirs", "open", "pickle.dump", "open", "json.dump", "net.cuda.parameters", "net.cuda.cuda", "os.path.join", "os.path.join", "os.path.join", "utils.make_vocab_cond.items"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab_cond", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "args", ")", ":", "\n", "# create data batcher, vocabulary", "\n", "# batcher", "\n", "    ", "with", "open", "(", "join", "(", "DATA_DIR", ",", "'vocab_cnt.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "wc", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "word2id", "=", "make_vocab_cond", "(", "wc", ",", "args", ".", "vsize", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "word2id", ",", "\n", "args", ".", "cuda", ",", "args", ".", "filter_out_low_recall", ",", "args", ".", "debug", ")", "\n", "\n", "# make net", "\n", "net", ",", "net_args", "=", "configure_net", "(", "len", "(", "word2id", ")", ",", "args", ".", "emb_dim", ",", "\n", "args", ".", "n_hidden", ",", "args", ".", "bi", ",", "args", ".", "n_layer", ")", "\n", "if", "args", ".", "w2v", ":", "\n", "# NOTE: the pretrained embedding having the same dimension", "\n", "#       as args.emb_dim should already be trained", "\n", "        ", "embedding", ",", "_", "=", "make_embedding", "(", "\n", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", ",", "args", ".", "w2v", ")", "\n", "net", ".", "set_embedding", "(", "embedding", ")", "\n", "\n", "# configure training setting", "\n", "", "criterion", ",", "train_params", "=", "configure_training", "(", "\n", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", "\n", ")", "\n", "\n", "# save experiment setting", "\n", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "word2id", ",", "f", ",", "protocol", "=", "4", ")", "\n", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'base_abstractor'", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'traing_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "val_fn", "=", "basic_validate", "(", "net", ",", "criterion", ")", "\n", "grad_fn", "=", "get_basic_grad_fn", "(", "net", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "0", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "", "pipeline", "=", "BasicPipeline", "(", "meta", "[", "'net'", "]", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "args", ".", "batch", ",", "val_fn", ",", "\n", "criterion", ",", "optimizer", ",", "grad_fn", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.output_stat._make_n_gram": [[8, 10], ["tuple", "range", "len"], "function", ["None"], ["def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.output_stat._count_data": [[12, 19], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["", "def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.dec'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.output_stat.count_lines_and_tokens": [[21, 37], ["collections.Counter", "collections.Counter", "collections.Counter", "sum", "sum", "sum", "open", "enumerate", "l.strip().split", "collections.Counter.update", "collections.Counter.update", "collections.Counter.update", "len", "output_stat._make_n_gram", "output_stat._make_n_gram", "output_stat._make_n_gram", "collections.Counter.items", "collections.Counter.items", "collections.Counter.items", "l.strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor._make_n_gram", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor._make_n_gram", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor._make_n_gram"], ["", "def", "count_lines_and_tokens", "(", "fname", ")", ":", "\n", "    ", "num_tokens", "=", "0", "\n", "one_gram_counter", "=", "Counter", "(", ")", "\n", "two_gram_counter", "=", "Counter", "(", ")", "\n", "three_gram_counter", "=", "Counter", "(", ")", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "l", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "l_tokenized", "=", "l", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "one_gram_counter", ".", "update", "(", "_make_n_gram", "(", "l_tokenized", ",", "n", "=", "1", ")", ")", "\n", "two_gram_counter", ".", "update", "(", "_make_n_gram", "(", "l_tokenized", ",", "n", "=", "2", ")", ")", "\n", "three_gram_counter", ".", "update", "(", "_make_n_gram", "(", "l_tokenized", ",", "n", "=", "3", ")", ")", "\n", "num_tokens", "+=", "len", "(", "l_tokenized", ")", "\n", "", "", "one_gram_repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "one_gram_counter", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "two_gram_repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "two_gram_counter", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "three_gram_repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "three_gram_counter", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "return", "i", "+", "1", ",", "num_tokens", ",", "one_gram_repeat", ",", "two_gram_repeat", ",", "three_gram_repeat", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.output_stat.main": [[39, 62], ["os.path.join", "output_stat._count_data", "range", "print", "print", "print", "print", "print", "os.path.join", "output_stat.count_lines_and_tokens"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.output_stat.count_lines_and_tokens"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "output_path", "=", "join", "(", "args", ".", "decode_dir", ",", "\"output\"", ")", "\n", "n_output", "=", "_count_data", "(", "output_path", ")", "\n", "total_num_sentences", "=", "0", "\n", "total_num_tokens", "=", "0", "\n", "one_gram_repeat_sum", "=", "0", "\n", "two_gram_repeat_sum", "=", "0", "\n", "three_gram_repeat_sum", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "n_output", ")", ":", "# iterate every .dec", "\n", "        ", "dec_file_path", "=", "join", "(", "output_path", ",", "\"{}.dec\"", ".", "format", "(", "i", ")", ")", "\n", "num_sentences", ",", "num_tokens", ",", "one_gram_repeat", ",", "two_gram_repeat", ",", "three_gram_repeat", "=", "count_lines_and_tokens", "(", "dec_file_path", ")", "\n", "total_num_sentences", "+=", "num_sentences", "\n", "total_num_tokens", "+=", "num_tokens", "\n", "one_gram_repeat_sum", "+=", "one_gram_repeat", "\n", "two_gram_repeat_sum", "+=", "two_gram_repeat", "\n", "three_gram_repeat_sum", "+=", "three_gram_repeat", "\n", "\n", "", "print", "(", "\"average generated sentences: {:.3f}\"", ".", "format", "(", "total_num_sentences", "/", "n_output", ")", ")", "\n", "print", "(", "\"average tokens per sentence: {:.3f}\"", ".", "format", "(", "total_num_tokens", "/", "total_num_sentences", ")", ")", "\n", "print", "(", "\"average repeat 1-gram: {:.3f}\"", ".", "format", "(", "one_gram_repeat_sum", "/", "n_output", ")", ")", "\n", "print", "(", "\"average repeat 2-gram: {:.3f}\"", ".", "format", "(", "two_gram_repeat_sum", "/", "n_output", ")", ")", "\n", "print", "(", "\"average repeat 3-gram: {:.3f}\"", ".", "format", "(", "three_gram_repeat_sum", "/", "n_output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.DecodeCandidateDataset.__init__": [[35, 37], ["data.data.CnnDmDatasetFromIdx.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "start_idx", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ",", "start_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.DecodeCandidateDataset.__getitem__": [[38, 43], ["data.data.CnnDmDatasetFromIdx.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", "=", "js_data", "[", "'article'", "]", "\n", "abs_sents", "=", "js_data", "[", "'abstract'", "]", "\n", "return", "art_sents", ",", "abs_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.coll": [[45, 57], ["enumerate", "filtered_art_batch.append", "filtered_abs_batch.append", "empty_data_indices.append"], "function", ["None"], ["", "", "def", "coll", "(", "batch", ")", ":", "\n", "    ", "filtered_art_batch", "=", "[", "]", "\n", "filtered_abs_batch", "=", "[", "]", "\n", "empty_data_indices", "=", "[", "]", "\n", "# filter out all empty articles", "\n", "for", "batch_i", ",", "(", "art_sents", ",", "abs_sents", ")", "in", "enumerate", "(", "batch", ")", ":", "\n", "        ", "if", "art_sents", ":", "# only keep non empty articles", "\n", "            ", "filtered_art_batch", ".", "append", "(", "art_sents", ")", "\n", "filtered_abs_batch", ".", "append", "(", "abs_sents", ")", "\n", "", "else", ":", "# log the empty idx", "\n", "            ", "empty_data_indices", ".", "append", "(", "batch_i", ")", "\n", "", "", "return", "filtered_art_batch", ",", "filtered_abs_batch", ",", "empty_data_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.decode": [[59, 214], ["time.time", "decode_candidates.DecodeCandidateDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "json.load", "print", "decoding.Abstractor", "decoding.BeamAbstractor", "list", "os.path.join", "open", "open", "json.dump", "torch.no_grad", "enumerate", "range", "os.path.join", "os.path.join", "map", "len", "print", "print", "print", "print", "print", "data.batcher.tokenize", "extractor", "list", "print", "print", "decoding.BeamAbstractor.", "decode_candidates.rerank_mp", "decoding.BeamAbstractor.", "print", "print", "print", "print", "print", "print", "print", "print", "print", "ext_inds.insert", "print", "map", "print", "enumerate", "print", "len", "print", "enumerate", "open", "f.write", "len", "len", "len", "print", "print", "print", "print", "print", "os.path.join", "json.dumps", "datetime.timedelta", "candidate_list.insert", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp"], ["", "def", "decode", "(", "save_path", ",", "abs_dir", ",", "split", ",", "batch_size", ",", "beam_size", ",", "diverse", ",", "max_len", ",", "topk", ",", "rerank_mode", ",", "keep_original_sent", ",", "start_idx", ",", "cuda", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "assert", "abs_dir", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "        ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "# a dummy extractor that extract all the sentences", "\n", "", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "\n", "dataset", "=", "DecodeCandidateDataset", "(", "split", ",", "start_idx", ")", "# only need json['article'] and json['abstract']", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'{}_candidate'", ".", "format", "(", "split", ")", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "None", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "(", "raw_article_batch", ",", "raw_abs_batch", ",", "empty_data_indices", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "debug", ":", "\n", "                ", "print", "(", "\"raw article batch\"", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "print", "(", "\"article lengths\"", ")", "\n", "print", "(", "[", "len", "(", "art", ")", "for", "art", "in", "raw_article_batch", "]", ")", "\n", "", "tokenized_article_batch", "=", "map", "(", "tokenize", "(", "100", ",", "\"w2v\"", ",", "None", ")", ",", "raw_article_batch", ")", "\n", "ext_arts", "=", "[", "]", "\n", "ext_inds", "=", "[", "]", "\n", "for", "raw_art_sents", "in", "tokenized_article_batch", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "len", "(", "ext_arts", ")", ",", "len", "(", "ext", ")", ")", "]", "\n", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"ext_inds\"", ")", "\n", "print", "(", "ext_inds", ")", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "\n", "if", "debug", ":", "\n", "                    ", "print", "(", "\"Beams:\"", ")", "\n", "for", "beam_i", ",", "beam", "in", "enumerate", "(", "all_beams", ")", ":", "\n", "                        ", "print", "(", "[", "hyp", ".", "sequence", "for", "hyp", "in", "beam", "]", ")", "\n", "", "print", "(", "\"======\"", ")", "\n", "", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "topk", ",", "rerank_mode", ")", "\n", "#dec_outs = rerank(all_beams, ext_inds, topk, rerank_mode)", "\n", "# dec_outs: a list of list of token list", "\n", "", "else", ":", "\n", "                ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "\n", "#assert i == batch_size * i_debug", "\n", "", "if", "i", "!=", "batch_size", "*", "i_debug", ":", "\n", "                ", "print", "(", "\"i: {}\"", ".", "format", "(", "i", ")", ")", "\n", "print", "(", "\"batch_size: {}, i_debug: {}, batch_size * i_debug: {}\"", ".", "format", "(", "batch_size", ",", "i_debug", ",", "batch_size", "*", "i_debug", ")", ")", "\n", "raise", "ValueError", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "\"\"\"\n            if i_debug == 18:\n                print(\"Length of ext_ids: {}\".format(len(ext_inds)))\n                print(\"Length of raw_rticle_batch: {}\".format(len(raw_article_batch)))\n                print(\"Length of tokenized_article_batch: {}\".format(len(list(tokenized_article_batch))))\n                print(\"i: {}\".format(i))\n            \"\"\"", "\n", "\n", "# insert place holders for samples with empty article", "\n", "for", "empty_idx", "in", "empty_data_indices", ":", "\n", "                ", "ext_inds", ".", "insert", "(", "empty_idx", ",", "(", "None", ",", "None", ")", ")", "\n", "\n", "", "batch_i", "=", "0", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "\n", "                ", "if", "debug", ":", "\n", "                    ", "print", "(", "\"j: {}, n: {}\"", ".", "format", "(", "j", ",", "n", ")", ")", "\n", "\n", "# one article", "\n", "", "article_decoded_sents", "=", "[", "]", "# a list of all candidates sentences for one article", "\n", "\n", "if", "j", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# if the input article is not empty", "\n", "# construct a list of all candidate sentences in one article, a list of str.", "\n", "                    ", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "# one sent", "\n", "                        ", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "=", "[", "' '", ".", "join", "(", "candidate", ")", "for", "candidate", "in", "sent", "]", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "=", "[", "' '", ".", "join", "(", "sent", ")", "]", "\n", "\n", "", "if", "keep_original_sent", ":", "\n", "                            ", "candidate_list", ".", "insert", "(", "0", ",", "raw_article_batch", "[", "batch_i", "]", "[", "sent_i", "]", ")", "\n", "\n", "", "article_decoded_sents", "+=", "candidate_list", "\n", "# fetch the abstract of the original sample", "\n", "", "raw_abstract", "=", "raw_abs_batch", "[", "batch_i", "]", "\n", "batch_i", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raw_abstract", "=", "[", "]", "\n", "\n", "if", "debug", ":", "\n", "                        ", "print", "(", "article_decoded_sents", "[", "0", "]", ")", "\n", "print", "(", "article_decoded_sents", "[", "1", "]", ")", "\n", "print", "(", "article_decoded_sents", "[", "2", "]", ")", "\n", "print", "(", "article_decoded_sents", "[", "3", "]", ")", "\n", "\n", "", "", "json_dict", "=", "{", "\"article\"", ":", "article_decoded_sents", ",", "\"abstract\"", ":", "raw_abstract", "}", "\n", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'{}_candidate/{}.json'", ".", "format", "(", "split", ",", "i", "+", "start_idx", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "json", ".", "dumps", "(", "json_dict", ")", ")", "\n", "", "i", "+=", "1", "\n", "\n", "\"\"\"\n                if i_debug == 18:\n                    art_len = len(json_dict['abstract'])\n                    print(\"length of current article: {}\".format(art_len))\n                    if art_len > 0:\n                        print(json_dict['abstract'][0])\n                    print(\"i increases to: {}\\n\".format(i))\n                \"\"\"", "\n", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "\"\"\"\n            if i_debug == 18:\n                raise ValueError\n            \"\"\"", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank": [[222, 232], ["list", "map", "cytoolz.concat", "decode_candidates.topk_one", "map", "map", "decode_candidates.rerank_topk_one", "decode_candidates.rerank_by_length"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_by_length"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "rerank_mode", "=", "0", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "rerank_mode", "==", "0", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "elif", "rerank_mode", "==", "1", ":", "\n", "        ", "topked", "=", "map", "(", "rerank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "rerank_by_length", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_mp": [[234, 247], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "decode_candidates.topk_one", "pool.map", "decode_candidates.rerank_topk_one", "pool.map", "decode_candidates.rerank_by_length"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_by_length"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "rerank_mode", "=", "0", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beams for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "rerank_mode", "==", "0", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "elif", "rerank_mode", "==", "1", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rerank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "elif", "rerank_mode", "==", "2", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rerank_by_length", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_by_length": [[249, 269], ["sorted", "len", "art_dec_outs.append", "art_dec_outs.append", "len"], "function", ["None"], ["", "@", "curry", "\n", "def", "rerank_by_length", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :return:\n    \"\"\"", "\n", "# sort it according to lengths, longest sequence first.", "\n", "#beams = [beam.sort(key=lambda h: -len(h.sequence)) for beam in beams]", "\n", "beams", "=", "[", "sorted", "(", "beam", ",", "key", "=", "lambda", "h", ":", "-", "len", "(", "h", ".", "sequence", ")", ")", "for", "beam", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "beam", "in", "beams", ":", "\n", "# append the candidates for each beam", "\n", "        ", "beam_size", "=", "len", "(", "beam", ")", "\n", "if", "k", "==", "2", ":", "\n", "            ", "art_dec_outs", ".", "append", "(", "[", "beam", "[", "0", "]", ".", "sequence", ",", "beam", "[", "-", "1", "]", ".", "sequence", "]", ")", "\n", "", "elif", "k", "==", "3", ":", "\n", "            ", "art_dec_outs", ".", "append", "(", "[", "beam", "[", "0", "]", ".", "sequence", ",", "beam", "[", "beam_size", "//", "2", "]", ".", "sequence", ",", "beam", "[", "-", "1", "]", ".", "sequence", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates._compute_len_score": [[271, 273], ["len"], "function", ["None"], ["", "def", "_compute_len_score", "(", "hyp", ")", ":", "\n", "    ", "return", "len", "(", "hyp", ".", "sequence", ")", ",", "hyp", ".", "logprob", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.rerank_topk_one": [[275, 293], ["map", "decode_candidates.rerank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rerank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates.topk_one": [[295, 303], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates._make_n_gram": [[305, 307], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_candidates._compute_score": [[309, 313], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_rouge": [[20, 42], ["pyrouge.utils.log.get_global_console_logger().setLevel", "tempfile.TemporaryDirectory", "pyrouge.Rouge155.convert_summaries_to_rouge_format", "pyrouge.Rouge155.convert_summaries_to_rouge_format", "pyrouge.Rouge155.write_config_static", "subprocess.check_output", "pyrouge.utils.log.get_global_console_logger", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "cmd.split", "os.path.join", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "eval_rouge", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ",", "\n", "cmd", "=", "'-c 95 -r 1000 -n 2 -m'", ",", "system_id", "=", "1", ")", ":", "\n", "    ", "\"\"\" evaluate by original Perl implementation\"\"\"", "\n", "# silence pyrouge logging", "\n", "assert", "_ROUGE_PATH", "is", "not", "None", "\n", "log", ".", "get_global_console_logger", "(", ")", ".", "setLevel", "(", "logging", ".", "WARNING", ")", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tmp_dir", ":", "\n", "        ", "Rouge155", ".", "convert_summaries_to_rouge_format", "(", "\n", "dec_dir", ",", "join", "(", "tmp_dir", ",", "'dec'", ")", ")", "\n", "Rouge155", ".", "convert_summaries_to_rouge_format", "(", "\n", "ref_dir", ",", "join", "(", "tmp_dir", ",", "'ref'", ")", ")", "\n", "Rouge155", ".", "write_config_static", "(", "\n", "join", "(", "tmp_dir", ",", "'dec'", ")", ",", "dec_pattern", ",", "\n", "join", "(", "tmp_dir", ",", "'ref'", ")", ",", "ref_pattern", ",", "\n", "join", "(", "tmp_dir", ",", "'settings.xml'", ")", ",", "system_id", "\n", ")", "\n", "cmd", "=", "(", "join", "(", "_ROUGE_PATH", ",", "'ROUGE-1.5.5.pl'", ")", "\n", "+", "' -e {} '", ".", "format", "(", "join", "(", "_ROUGE_PATH", ",", "'data'", ")", ")", "\n", "+", "cmd", "\n", "+", "' -a {}'", ".", "format", "(", "join", "(", "tmp_dir", ",", "'settings.xml'", ")", ")", ")", "\n", "output", "=", "sp", ".", "check_output", "(", "cmd", ".", "split", "(", "' '", ")", ",", "universal_newlines", "=", "True", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_meteor": [[49, 72], ["re.compile", "sorted", "re.compile", "sorted", "tempfile.TemporaryDirectory", "subprocess.check_output", "open", "open", "open", "ref_f.write", "dec_f.write", "os.path.join", "os.path.join", "cmd.split", "os.listdir", "re.compile.match", "int", "os.listdir", "re.compile.match", "int", "os.path.join", "f.read().split", "os.path.join", "os.path.join", "name.split", "name.split", "f.read", "map", "map", "evaluate.eval_meteor.read_file"], "function", ["None"], ["", "def", "eval_meteor", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", ":", "\n", "    ", "\"\"\" METEOR evaluation\"\"\"", "\n", "assert", "_METEOR_PATH", "is", "not", "None", "\n", "ref_matcher", "=", "re", ".", "compile", "(", "ref_pattern", ")", "\n", "refs", "=", "sorted", "(", "[", "r", "for", "r", "in", "os", ".", "listdir", "(", "ref_dir", ")", "if", "ref_matcher", ".", "match", "(", "r", ")", "]", ",", "\n", "key", "=", "lambda", "name", ":", "int", "(", "name", ".", "split", "(", "'.'", ")", "[", "0", "]", ")", ")", "\n", "dec_matcher", "=", "re", ".", "compile", "(", "dec_pattern", ")", "\n", "decs", "=", "sorted", "(", "[", "d", "for", "d", "in", "os", ".", "listdir", "(", "dec_dir", ")", "if", "dec_matcher", ".", "match", "(", "d", ")", "]", ",", "\n", "key", "=", "lambda", "name", ":", "int", "(", "name", ".", "split", "(", "'.'", ")", "[", "0", "]", ")", ")", "\n", "@", "curry", "\n", "def", "read_file", "(", "file_dir", ",", "file_name", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "file_dir", ",", "file_name", ")", ")", "as", "f", ":", "\n", "            ", "return", "' '", ".", "join", "(", "f", ".", "read", "(", ")", ".", "split", "(", ")", ")", "\n", "", "", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tmp_dir", ":", "\n", "        ", "with", "open", "(", "join", "(", "tmp_dir", ",", "'ref.txt'", ")", ",", "'w'", ")", "as", "ref_f", ",", "open", "(", "join", "(", "tmp_dir", ",", "'dec.txt'", ")", ",", "'w'", ")", "as", "dec_f", ":", "\n", "            ", "ref_f", ".", "write", "(", "'\\n'", ".", "join", "(", "map", "(", "read_file", "(", "ref_dir", ")", ",", "refs", ")", ")", "+", "'\\n'", ")", "\n", "dec_f", ".", "write", "(", "'\\n'", ".", "join", "(", "map", "(", "read_file", "(", "dec_dir", ")", ",", "decs", ")", ")", "+", "'\\n'", ")", "\n", "\n", "", "cmd", "=", "'java -Xmx2G -jar {} {} {} -l en -norm'", ".", "format", "(", "\n", "_METEOR_PATH", ",", "join", "(", "tmp_dir", ",", "'dec.txt'", ")", ",", "join", "(", "tmp_dir", ",", "'ref.txt'", ")", ")", "\n", "output", "=", "sp", ".", "check_output", "(", "cmd", ".", "split", "(", "' '", ")", ",", "universal_newlines", "=", "True", ")", "\n", "", "return", "output", "\n", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.oracle_rouge_l._split_words": [[22, 24], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.oracle_rouge_l.process": [[26, 39], ["os.path.join", "os.path.join", "open", "json.loads", "open", "f.writelines", "os.path.join", "f.read", "os.path.join"], "function", ["None"], ["", "@", "curry", "\n", "def", "process", "(", "in_folder", ",", "out_folder", ",", "i", ")", ":", "\n", "#tokenize = compose(list, _split_words)", "\n", "    ", "in_data_dir", "=", "join", "(", "DATA_DIR", ",", "in_folder", ")", "\n", "out_data_dir", "=", "join", "(", "out_folder", ",", "'output'", ")", "\n", "with", "open", "(", "join", "(", "in_data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "art_sents", "=", "data", "[", "'article'", "]", "\n", "abs_sents", "=", "data", "[", "'abstract'", "]", "\n", "exts", "=", "data", "[", "'extracted'", "]", "\n", "oracle_sents", "=", "[", "art_sents", "[", "ext", "]", "for", "ext", "in", "exts", "]", "\n", "with", "open", "(", "join", "(", "out_data_dir", ",", "'{}.dec'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "writelines", "(", "oracle_sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.oracle_rouge_l.remove_mp": [[41, 59], ["time.time", "print", "os.path.join", "utils.count_data", "os.path.join", "os.makedirs", "print", "open", "json.dump", "multiprocessing.Pool", "list", "os.path.join", "pool.imap_unordered", "datetime.timedelta", "oracle_rouge_l.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "remove_mp", "(", "in_folder", ",", "out_folder", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "in_folder", ")", ")", "\n", "in_data_dir", "=", "join", "(", "DATA_DIR", ",", "in_folder", ")", "\n", "n_data", "=", "count_data", "(", "in_data_dir", ")", "\n", "# make output folder", "\n", "#out_data_dir = join(DATA_DIR, out_folder)", "\n", "out_data_dir", "=", "join", "(", "out_folder", ",", "'output'", ")", "\n", "os", ".", "makedirs", "(", "out_data_dir", ")", "\n", "log_dict", "=", "{", "\"split\"", ":", "in_folder", "}", "\n", "with", "open", "(", "join", "(", "out_folder", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "log_dict", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "in_folder", ",", "out_folder", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.oracle_rouge_l.main": [[61, 64], ["oracle_rouge_l.remove_mp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.oracle_rouge_l.remove_mp"], ["", "def", "main", "(", "in_folder", ",", "out_folder", ")", ":", "\n", "    ", "remove_mp", "(", "in_folder", ",", "out_folder", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_compression_label._split_words": [[24, 26], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_compression_label.process": [[28, 56], ["os.path.join", "cytoolz.compose", "cytoolz.compose.", "cytoolz.compose.", "open", "json.loads", "zip", "open", "json.dump", "os.path.join", "f.read", "compression_ratios.append", "os.path.join", "len", "print", "print", "print", "print", "exit", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "@", "curry", "\n", "def", "process", "(", "split", ",", "i", ")", ":", "\n", "    ", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "tokenize", "=", "compose", "(", "list", ",", "_split_words", ")", "\n", "art_sents", "=", "tokenize", "(", "data", "[", "'article'", "]", ")", "\n", "abs_sents", "=", "tokenize", "(", "data", "[", "'abstract'", "]", ")", "\n", "ext_labels", "=", "data", "[", "'extracted'", "]", "\n", "compression_ratios", "=", "[", "]", "\n", "if", "art_sents", "and", "abs_sents", ":", "# some data contains empty article/abstract", "\n", "        ", "for", "ext_label", ",", "abs_sent", "in", "zip", "(", "ext_labels", ",", "abs_sents", ")", ":", "\n", "            ", "original_sent", "=", "art_sents", "[", "ext_label", "]", "\n", "if", "len", "(", "original_sent", ")", "==", "0", ":", "\n", "                ", "print", "(", "i", ")", "\n", "print", "(", "ext_label", ")", "\n", "print", "(", "art_sents", "[", "ext_label", "]", ")", "\n", "print", "(", "abs_sent", ")", "\n", "exit", "(", ")", "\n", "compression_ratio", "=", "0.0", "\n", "", "else", ":", "\n", "                ", "compression_ratio", "=", "(", "len", "(", "original_sent", ")", "-", "len", "(", "abs_sent", ")", ")", "/", "len", "(", "original_sent", ")", "\n", "", "compression_ratios", ".", "append", "(", "compression_ratio", ")", "\n", "", "", "else", ":", "\n", "        ", "compression_ratios", "=", "[", "]", "\n", "", "data", "[", "'compression_ratios'", "]", "=", "compression_ratios", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_compression_label.label_mp": [[58, 68], ["time.time", "print", "os.path.join", "utils.count_data", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "make_compression_label.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "label_mp", "(", "split", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "split", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_compression_label.label": [[70, 79], ["time.time", "print", "os.path.join", "utils.count_data", "range", "print", "make_compression_label.process", "datetime.timedelta", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "def", "label", "(", "split", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "process", "(", "split", ",", "i", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_compression_label.main": [[81, 89], ["make_compression_label.label", "make_compression_label.label"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label"], ["", "def", "main", "(", "split", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'train'", "]", ":", "\n", "#label_mp(split)", "\n", "            ", "label", "(", "split", ")", "\n", "", "", "else", ":", "\n", "#label_mp(split)", "\n", "        ", "label", "(", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.ExtractDataset.__init__": [[45, 47], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.ExtractDataset.__getitem__": [[48, 52], ["data.data.CnnDmDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "extracts", "=", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'extracted'", "]", "\n", "return", "art_sents", ",", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.ExtractDatasetStop.__init__": [[57, 59], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.ExtractDatasetStop.__getitem__": [[60, 65], ["data.data.CnnDmDataset.__getitem__", "extracts.append", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "extracts", "=", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'extracted'", "]", "\n", "extracts", ".", "append", "(", "len", "(", "art_sents", ")", ")", "\n", "return", "art_sents", ",", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.build_batchers": [[66, 101], ["data.batcher.prepro_fn_extract", "cytoolz.compose", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "len", "batchify_fn", "convert_batch", "dataset_class", "dataset_class"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_extract", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch"], ["", "", "def", "build_batchers", "(", "net_type", ",", "word2id", ",", "cuda", ",", "train_set_folder", ",", "valid_set_folder", ",", "debug", ",", "auto_stop", "=", "False", ")", ":", "\n", "    ", "assert", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", "]", "\n", "prepro", "=", "prepro_fn_extract", "(", "args", ".", "max_word", ",", "args", ".", "max_sent", ")", "\n", "def", "sort_key", "(", "sample", ")", ":", "\n", "        ", "src_sents", ",", "_", "=", "sample", "\n", "return", "len", "(", "src_sents", ")", "\n", "", "batchify_fn", "=", "(", "batchify_fn_extract_ff", "if", "net_type", "==", "'ff'", "\n", "else", "batchify_fn_extract_ptr", ")", "\n", "convert_batch", "=", "(", "convert_batch_extract_ff", "if", "net_type", "==", "'ff'", "\n", "else", "convert_batch_extract_ptr", ")", "\n", "batchify", "=", "compose", "(", "batchify_fn", "(", "PAD", ",", "cuda", "=", "cuda", ")", ",", "\n", "convert_batch", "(", "UNK", ",", "word2id", ")", ")", "\n", "\n", "if", "auto_stop", ":", "\n", "        ", "dataset_class", "=", "ExtractDatasetStop", "\n", "", "else", ":", "\n", "        ", "dataset_class", "=", "ExtractDataset", "\n", "\n", "", "train_loader", "=", "DataLoader", "(", "\n", "dataset_class", "(", "train_set_folder", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "not", "debug", ",", "\n", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_extract", "\n", ")", "\n", "train_batcher", "=", "BucketedGenerater", "(", "train_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "False", ",", "fork", "=", "not", "debug", ")", "\n", "\n", "val_loader", "=", "DataLoader", "(", "\n", "dataset_class", "(", "valid_set_folder", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_extract", "\n", ")", "\n", "val_batcher", "=", "BucketedGenerater", "(", "val_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "fork", "=", "not", "debug", ")", "\n", "return", "train_batcher", ",", "val_batcher", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.configure_net": [[103, 127], ["model.extract.ExtractSumm", "model.extract.PtrExtractSumm", "model.extract.PtrExtractRewrittenSumm"], "function", ["None"], ["", "def", "configure_net", "(", "net_type", ",", "vocab_size", ",", "emb_dim", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "num_candidates", "=", "-", "1", ",", "auto_stop", "=", "False", ")", ":", "\n", "    ", "assert", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", "]", "\n", "net_args", "=", "{", "}", "\n", "net_args", "[", "'vocab_size'", "]", "=", "vocab_size", "\n", "net_args", "[", "'emb_dim'", "]", "=", "emb_dim", "\n", "net_args", "[", "'conv_hidden'", "]", "=", "conv_hidden", "\n", "net_args", "[", "'lstm_hidden'", "]", "=", "lstm_hidden", "\n", "net_args", "[", "'lstm_layer'", "]", "=", "lstm_layer", "\n", "net_args", "[", "'bidirectional'", "]", "=", "bidirectional", "\n", "net_args", "[", "'auto_stop'", "]", "=", "auto_stop", "\n", "if", "net_type", "==", "'rewritten_rnn'", ":", "\n", "        ", "assert", "num_candidates", ">", "0", "\n", "net_args", "[", "'num_candidates'", "]", "=", "num_candidates", "\n", "\n", "", "if", "net_type", "==", "'ff'", ":", "\n", "        ", "net", "=", "ExtractSumm", "(", "**", "net_args", ")", "\n", "", "elif", "net_type", "==", "'rnn'", ":", "\n", "        ", "net", "=", "PtrExtractSumm", "(", "**", "net_args", ")", "\n", "", "else", ":", "\n", "        ", "net", "=", "PtrExtractRewrittenSumm", "(", "**", "net_args", ")", "\n", "# net = (ExtractSumm(**net_args) if net_type == 'ff' else PtrExtractSumm(**net_args))", "\n", "\n", "", "return", "net", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.configure_training": [[129, 151], ["torch.nn.functional.binary_cross_entropy_with_logits", "torch.nn.functional.cross_entropy", "model.util.sequence_loss"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss"], ["", "def", "configure_training", "(", "net_type", ",", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\" supports Adam optimizer only\"\"\"", "\n", "assert", "opt", "in", "[", "'adam'", "]", "\n", "assert", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "\n", "if", "net_type", "==", "'ff'", ":", "\n", "        ", "criterion", "=", "lambda", "logit", ",", "target", ":", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "ce", "=", "lambda", "logit", ",", "target", ":", "F", ".", "cross_entropy", "(", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "def", "criterion", "(", "logits", ",", "targets", ")", ":", "\n", "            ", "return", "sequence_loss", "(", "logits", ",", "targets", ",", "ce", ",", "pad_idx", "=", "-", "1", ")", "\n", "\n", "", "", "return", "criterion", ",", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_join_ml.main": [[153, 210], ["utils.make_vocab", "train_extractor_join_ml.build_batchers", "train_extractor_join_ml.configure_net", "train_extractor_join_ml.configure_training", "training.basic_validate", "training.get_basic_grad_fn", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "training.BasicPipeline", "training.BasicTrainer", "print", "print", "training.BasicTrainer.train", "open", "pickle.load", "len", "utils.make_embedding", "net.cuda.set_embedding", "os.path.exists", "os.makedirs", "open", "pickle.dump", "open", "json.dump", "net.cuda.parameters", "net.cuda.cuda", "os.path.join", "os.path.join", "os.path.join", "utils.make_vocab.items"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "assert", "args", ".", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", "]", "\n", "# create data batcher, vocabulary", "\n", "# batcher", "\n", "with", "open", "(", "join", "(", "DATA_DIR", ",", "'vocab_cnt.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "wc", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "word2id", "=", "make_vocab", "(", "wc", ",", "args", ".", "vsize", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "args", ".", "net_type", ",", "word2id", ",", "\n", "args", ".", "cuda", ",", "args", ".", "train_set_folder", ",", "args", ".", "valid_set_folder", ",", "args", ".", "debug", ",", "args", ".", "auto_stop", ")", "\n", "\n", "# make net", "\n", "net", ",", "net_args", "=", "configure_net", "(", "args", ".", "net_type", ",", "\n", "len", "(", "word2id", ")", ",", "args", ".", "emb_dim", ",", "args", ".", "conv_hidden", ",", "\n", "args", ".", "lstm_hidden", ",", "args", ".", "lstm_layer", ",", "args", ".", "bi", ",", "args", ".", "num_candidates", ",", "args", ".", "auto_stop", ")", "\n", "if", "args", ".", "w2v", ":", "\n", "# NOTE: the pretrained embedding having the same dimension", "\n", "#       as args.emb_dim should already be trained", "\n", "        ", "embedding", ",", "_", "=", "make_embedding", "(", "\n", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", ",", "args", ".", "w2v", ")", "\n", "net", ".", "set_embedding", "(", "embedding", ")", "\n", "\n", "# configure training setting", "\n", "", "criterion", ",", "train_params", "=", "configure_training", "(", "\n", "args", ".", "net_type", ",", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", "\n", ")", "\n", "\n", "# save experiment setting", "\n", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "word2id", ",", "f", ",", "protocol", "=", "4", ")", "\n", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'ml_{}_extractor'", ".", "format", "(", "args", ".", "net_type", ")", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'traing_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "val_fn", "=", "basic_validate", "(", "net", ",", "criterion", ")", "\n", "grad_fn", "=", "get_basic_grad_fn", "(", "net", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "0", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "", "pipeline", "=", "BasicPipeline", "(", "meta", "[", "'net'", "]", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "args", ".", "batch", ",", "val_fn", ",", "\n", "criterion", ",", "optimizer", ",", "grad_fn", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.DecodeDataset.__init__": [[31, 35], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "#assert split in ['val', 'test']", "\n", "        ", "assert", "'train'", "not", "in", "split", "\n", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATASET_DIR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.DecodeDataset.__getitem__": [[36, 40], ["data.data.CnnDmDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", "=", "js_data", "[", "'article'", "]", "\n", "return", "art_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.Abstractor.__init__": [[60, 78], ["json.load", "decoding.load_best_ckpt", "pickle.load", "model.copy_summ.CopySumm", "model.copy_summ.CopySumm.load_state_dict", "torch.device", "model.copy_summ.CopySumm.to", "decoding.Abstractor._net.parameters", "open", "open", "os.path.join", "os.path.join", "pickle.load.items"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["    ", "def", "__init__", "(", "self", ",", "abs_dir", ",", "max_len", "=", "30", ",", "cuda", "=", "True", ")", ":", "\n", "        ", "abs_meta", "=", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", "\n", "assert", "abs_meta", "[", "'net'", "]", "==", "'base_abstractor'", "\n", "abs_args", "=", "abs_meta", "[", "'net_args'", "]", "\n", "abs_ckpt", "=", "load_best_ckpt", "(", "abs_dir", ")", "\n", "word2id", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "abstractor", "=", "CopySumm", "(", "**", "abs_args", ")", "\n", "abstractor", ".", "load_state_dict", "(", "abs_ckpt", ")", "\n", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "_net", "=", "abstractor", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_word2id", "=", "word2id", "\n", "self", ".", "_id2word", "=", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", "\n", "self", ".", "_max_len", "=", "max_len", "\n", "self", ".", "n_hidden", "=", "self", ".", "_net", ".", "n_hidden", "\n", "\n", "# set require_grad to false to all parameters", "\n", "for", "param", "in", "self", ".", "_net", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.Abstractor._prepro": [[79, 98], ["dict", "dict", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "len", "len", "data.batcher.pad_batch_tensorize", "data.batcher.pad_batch_tensorize", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "", "def", "_prepro", "(", "self", ",", "raw_article_sents", ")", ":", "\n", "        ", "ext_word2id", "=", "dict", "(", "self", ".", "_word2id", ")", "\n", "ext_id2word", "=", "dict", "(", "self", ".", "_id2word", ")", "\n", "for", "raw_words", "in", "raw_article_sents", ":", "\n", "            ", "for", "w", "in", "raw_words", ":", "\n", "                ", "if", "not", "w", "in", "ext_word2id", ":", "\n", "                    ", "ext_word2id", "[", "w", "]", "=", "len", "(", "ext_word2id", ")", "\n", "ext_id2word", "[", "len", "(", "ext_id2word", ")", "]", "=", "w", "\n", "", "", "", "articles", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_article_sents", ")", "# a list of list of int", "\n", "art_lens", "=", "[", "len", "(", "art", ")", "for", "art", "in", "articles", "]", "\n", "article", "=", "pad_batch_tensorize", "(", "articles", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "extend_arts", "=", "conver2id", "(", "UNK", ",", "ext_word2id", ",", "raw_article_sents", ")", "\n", "extend_art", "=", "pad_batch_tensorize", "(", "extend_arts", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "extend_vsize", "=", "len", "(", "ext_word2id", ")", "\n", "dec_args", "=", "(", "article", ",", "art_lens", ",", "extend_art", ",", "extend_vsize", ",", "\n", "START", ",", "END", ",", "UNK", ",", "self", ".", "_max_len", ")", "\n", "return", "dec_args", ",", "ext_id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.Abstractor.__call__": [[99, 118], ["decoding.Abstractor._net.eval", "decoding.Abstractor._prepro", "decoding.Abstractor._net.batch_decode", "enumerate", "zip", "dec_sents.append", "max", "range", "dec.append", "dec.append", "len", "decoding.Abstractor.__call__.argmax"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batch_decode"], ["", "def", "__call__", "(", "self", ",", "raw_article_sents", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "raw_article_sents", ")", "\n", "decs", ",", "attns", "=", "self", ".", "_net", ".", "batch_decode", "(", "*", "dec_args", ")", "\n", "def", "argmax", "(", "arr", ",", "keys", ")", ":", "\n", "            ", "return", "arr", "[", "max", "(", "range", "(", "len", "(", "arr", ")", ")", ",", "key", "=", "lambda", "i", ":", "keys", "[", "i", "]", ".", "item", "(", ")", ")", "]", "\n", "", "dec_sents", "=", "[", "]", "# a list of token list, len=batch_size", "\n", "# convert idx to words", "\n", "for", "i", ",", "raw_words", "in", "enumerate", "(", "raw_article_sents", ")", ":", "# each input sentence", "\n", "            ", "dec", "=", "[", "]", "\n", "for", "id_", ",", "attn", "in", "zip", "(", "decs", ",", "attns", ")", ":", "# id_: a tensor with size = batch_size", "\n", "                ", "if", "id_", "[", "i", "]", "==", "END", ":", "\n", "                    ", "break", "\n", "", "elif", "id_", "[", "i", "]", "==", "UNK", ":", "# replace unk word with highest attention word", "\n", "                    ", "dec", ".", "append", "(", "argmax", "(", "raw_words", ",", "attn", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "dec", ".", "append", "(", "id2word", "[", "id_", "[", "i", "]", ".", "item", "(", ")", "]", ")", "\n", "", "", "dec_sents", ".", "append", "(", "dec", ")", "\n", "", "return", "dec_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.Abstractor.encode": [[119, 123], ["decoding.Abstractor._net.encode"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "# return the initial decode hidden state", "\n", "        ", "_", ",", "(", "init_dec_states", ",", "_", ")", "=", "self", ".", "_net", ".", "encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "return", "init_dec_states", "[", "0", "]", "# [num_sents, abstractor_n_hidden]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.CompressionControlAbstractor.__init__": [[125, 143], ["json.load", "decoding.load_best_ckpt", "pickle.load", "model.controllable_abstractor.CompressControlSumm", "model.controllable_abstractor.CompressControlSumm.load_state_dict", "torch.device", "model.controllable_abstractor.CompressControlSumm.to", "decoding.CompressionControlAbstractor._net.parameters", "open", "open", "os.path.join", "os.path.join", "pickle.load.items"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["    ", "def", "__init__", "(", "self", ",", "abs_dir", ",", "max_len", "=", "30", ",", "cuda", "=", "True", ")", ":", "\n", "        ", "abs_meta", "=", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", "\n", "assert", "abs_meta", "[", "'net'", "]", "==", "'controllable_abstractor'", "\n", "abs_args", "=", "abs_meta", "[", "'net_args'", "]", "\n", "abs_ckpt", "=", "load_best_ckpt", "(", "abs_dir", ")", "\n", "word2id", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "abstractor", "=", "CompressControlSumm", "(", "**", "abs_args", ")", "\n", "abstractor", ".", "load_state_dict", "(", "abs_ckpt", ")", "\n", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "_net", "=", "abstractor", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_word2id", "=", "word2id", "\n", "self", ".", "_id2word", "=", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", "\n", "self", ".", "_max_len", "=", "max_len", "\n", "self", ".", "n_hidden", "=", "self", ".", "_net", ".", "n_hidden", "\n", "\n", "# set require_grad to false to all parameters", "\n", "for", "param", "in", "self", ".", "_net", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.CompressionControlAbstractor._prepro": [[144, 164], ["dict", "dict", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "torch.LongTensor().to", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "len", "len", "data.batcher.pad_batch_tensorize", "torch.LongTensor", "data.batcher.pad_batch_tensorize", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "", "def", "_prepro", "(", "self", ",", "raw_article_sents", ",", "raw_compression_levels", ")", ":", "\n", "        ", "ext_word2id", "=", "dict", "(", "self", ".", "_word2id", ")", "\n", "ext_id2word", "=", "dict", "(", "self", ".", "_id2word", ")", "\n", "for", "raw_words", "in", "raw_article_sents", ":", "\n", "            ", "for", "w", "in", "raw_words", ":", "\n", "                ", "if", "not", "w", "in", "ext_word2id", ":", "\n", "                    ", "ext_word2id", "[", "w", "]", "=", "len", "(", "ext_word2id", ")", "\n", "ext_id2word", "[", "len", "(", "ext_id2word", ")", "]", "=", "w", "\n", "", "", "", "articles", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_article_sents", ")", "# a list of list of int", "\n", "art_lens", "=", "[", "len", "(", "art", ")", "for", "art", "in", "articles", "]", "\n", "article", "=", "pad_batch_tensorize", "(", "articles", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "compression_levels", "=", "torch", ".", "LongTensor", "(", "raw_compression_levels", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "extend_arts", "=", "conver2id", "(", "UNK", ",", "ext_word2id", ",", "raw_article_sents", ")", "\n", "extend_art", "=", "pad_batch_tensorize", "(", "extend_arts", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "extend_vsize", "=", "len", "(", "ext_word2id", ")", "\n", "dec_args", "=", "(", "article", ",", "art_lens", ",", "compression_levels", ",", "extend_art", ",", "extend_vsize", ",", "\n", "START", ",", "END", ",", "UNK", ",", "self", ".", "_max_len", ")", "\n", "return", "dec_args", ",", "ext_id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.CompressionControlAbstractor.__call__": [[165, 184], ["decoding.CompressionControlAbstractor._net.eval", "decoding.CompressionControlAbstractor._prepro", "decoding.CompressionControlAbstractor._net.batch_decode", "enumerate", "zip", "dec_sents.append", "max", "range", "dec.append", "dec.append", "len", "decoding.CompressionControlAbstractor.__call__.argmax"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batch_decode"], ["", "def", "__call__", "(", "self", ",", "raw_article_sents", ",", "raw_compression_levels", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "raw_article_sents", ",", "raw_compression_levels", ")", "\n", "decs", ",", "attns", "=", "self", ".", "_net", ".", "batch_decode", "(", "*", "dec_args", ")", "\n", "def", "argmax", "(", "arr", ",", "keys", ")", ":", "\n", "            ", "return", "arr", "[", "max", "(", "range", "(", "len", "(", "arr", ")", ")", ",", "key", "=", "lambda", "i", ":", "keys", "[", "i", "]", ".", "item", "(", ")", ")", "]", "\n", "", "dec_sents", "=", "[", "]", "# a list of token list, len=batch_size", "\n", "# convert idx to words", "\n", "for", "i", ",", "raw_words", "in", "enumerate", "(", "raw_article_sents", ")", ":", "# each input sentence", "\n", "            ", "dec", "=", "[", "]", "\n", "for", "id_", ",", "attn", "in", "zip", "(", "decs", ",", "attns", ")", ":", "# id_: a tensor with size = batch_size", "\n", "                ", "if", "id_", "[", "i", "]", "==", "END", ":", "\n", "                    ", "break", "\n", "", "elif", "id_", "[", "i", "]", "==", "UNK", ":", "# replace unk word with highest attention word", "\n", "                    ", "dec", ".", "append", "(", "argmax", "(", "raw_words", ",", "attn", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "dec", ".", "append", "(", "id2word", "[", "id_", "[", "i", "]", ".", "item", "(", ")", "]", ")", "\n", "", "", "dec_sents", ".", "append", "(", "dec", ")", "\n", "", "return", "dec_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ConditionalAbstractor.__init__": [[187, 207], ["json.load", "decoding.load_best_ckpt", "pickle.load", "model.copy_cond_summ.CopyCondSumm", "model.copy_cond_summ.CopyCondSumm.load_state_dict", "torch.device", "model.copy_cond_summ.CopyCondSumm.to", "decoding.ConditionalAbstractor._net.parameters", "open", "open", "os.path.join", "os.path.join", "pickle.load.items"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["    ", "def", "__init__", "(", "self", ",", "abs_dir", ",", "max_len", "=", "30", ",", "cuda", "=", "True", ")", ":", "\n", "        ", "abs_meta", "=", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", "\n", "assert", "abs_meta", "[", "'net'", "]", "==", "'conditional_abstractor'", "\n", "abs_args", "=", "abs_meta", "[", "'net_args'", "]", "\n", "abs_ckpt", "=", "load_best_ckpt", "(", "abs_dir", ")", "\n", "word2id", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "\n", "abstractor", "=", "CopyCondSumm", "(", "**", "abs_args", ")", "\n", "\n", "abstractor", ".", "load_state_dict", "(", "abs_ckpt", ")", "\n", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "_net", "=", "abstractor", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_word2id", "=", "word2id", "\n", "self", ".", "_id2word", "=", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", "\n", "self", ".", "_max_len", "=", "max_len", "\n", "self", ".", "n_hidden", "=", "self", ".", "_net", ".", "n_hidden", "\n", "\n", "# set require_grad to false to all parameters", "\n", "for", "param", "in", "self", ".", "_net", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ConditionalAbstractor._prepro": [[208, 248], ["dict", "dict", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "len", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "articles_sequential.append", "art_lens_sequential.append", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "extend_arts_sequential.append", "len", "data.batcher.pad_batch_tensorize().to.size", "torch.Size", "len", "data.batcher.pad_batch_tensorize", "data.batcher.pad_batch_tensorize", "data.batcher.pad_batch_tensorize", "range", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "", "def", "_prepro", "(", "self", ",", "sequential_raw_article_sents", ")", ":", "\n", "        ", "ext_word2id", "=", "dict", "(", "self", ".", "_word2id", ")", "\n", "ext_id2word", "=", "dict", "(", "self", ".", "_id2word", ")", "\n", "articles_sequential", "=", "[", "]", "\n", "art_lens_sequential", "=", "[", "]", "\n", "extend_arts_sequential", "=", "[", "]", "\n", "\n", "for", "raw_article_sents", "in", "sequential_raw_article_sents", ":", "\n", "            ", "for", "raw_words", "in", "raw_article_sents", ":", "\n", "                ", "for", "w", "in", "raw_words", ":", "\n", "                    ", "if", "not", "w", "in", "ext_word2id", ":", "\n", "                        ", "ext_word2id", "[", "w", "]", "=", "len", "(", "ext_word2id", ")", "\n", "ext_id2word", "[", "len", "(", "ext_id2word", ")", "]", "=", "w", "\n", "# convert article to idx and pad and append to list", "\n", "", "", "", "articles", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_article_sents", ")", "\n", "article", "=", "pad_batch_tensorize", "(", "articles", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "articles_sequential", ".", "append", "(", "article", ")", "\n", "# compute art lens", "\n", "art_lens", "=", "[", "len", "(", "art", ")", "for", "art", "in", "articles", "]", "\n", "art_lens_sequential", ".", "append", "(", "art_lens", ")", "\n", "# convert article to idx with oov and pad and append to list", "\n", "extend_arts", "=", "conver2id", "(", "UNK", ",", "ext_word2id", ",", "raw_article_sents", ")", "\n", "extend_art", "=", "pad_batch_tensorize", "(", "extend_arts", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "extend_arts_sequential", ".", "append", "(", "extend_art", ")", "\n", "\n", "# construct an initial memory sentence with only the <empty_mem> token", "\n", "# repeat it for every article", "\n", "# convert it to idx and pad", "\n", "", "init_mems", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "[", "[", "'<empty_mem>'", "]", "for", "_", "in", "range", "(", "len", "(", "sequential_raw_article_sents", "[", "0", "]", ")", ")", "]", ")", "\n", "init_mem", "=", "pad_batch_tensorize", "(", "init_mems", ",", "PAD", ",", "cuda", "=", "False", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "init_mem_lens", "=", "[", "1", "]", "*", "len", "(", "sequential_raw_article_sents", "[", "0", "]", ")", "\n", "\n", "assert", "init_mem", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "len", "(", "sequential_raw_article_sents", "[", "0", "]", ")", ",", "1", "]", ")", "\n", "\n", "extend_vsize", "=", "len", "(", "ext_word2id", ")", "\n", "dec_args", "=", "(", "articles_sequential", ",", "art_lens_sequential", ",", "init_mem", ",", "init_mem_lens", ",", "extend_arts_sequential", ",", "extend_vsize", ",", "\n", "START", ",", "END", ",", "UNK", ",", "self", ".", "_max_len", ")", "\n", "return", "dec_args", ",", "ext_id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ConditionalAbstractor.__call__": [[249, 279], ["decoding.ConditionalAbstractor._net.eval", "decoding.ConditionalAbstractor._prepro", "enumerate", "list", "zip", "decoding.ConditionalAbstractor._net.batch_decode", "decoding.ConditionalAbstractor._process_dec_out", "zip", "cytoolz.concat", "range", "range", "data.batcher.pad_batch_tensorize().to", "len", "len", "dec_sents_raw_all[].append", "len", "data.batcher.pad_batch_tensorize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batch_decode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ConditionalAbstractor._process_dec_out", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "def", "__call__", "(", "self", ",", "sequential_raw_article_sents", ",", "sequential_article_ids", ",", "num_articles", ")", ":", "\n", "# raw_article_sents: a list of token list", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "sequential_raw_article_sents", ")", "\n", "# dec_args = (article, art_lens, extend_art, extend_vsize, START, END, UNK, self._max_len)", "\n", "articles_sequential", ",", "art_lens_sequential", ",", "external_memory", ",", "external_memory_lens", ",", "extend_arts_sequential", ",", "extend_vsize", ",", "start", ",", "end", ",", "unk", ",", "max_len", "=", "dec_args", "\n", "memory_sents_all", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_articles", ")", "]", "# len=batch_size", "\n", "dec_sents_raw_all", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_articles", ")", "]", "# len=batch_size", "\n", "\n", "for", "i", ",", "(", "articles_sents_i", ",", "art_lens_i", ",", "extend_arts_sents_i", ",", "raw_art_sents_i", ",", "article_ids", ")", "in", "enumerate", "(", "zip", "(", "articles_sequential", ",", "art_lens_sequential", ",", "extend_arts_sequential", ",", "sequential_raw_article_sents", ",", "sequential_article_ids", ")", ")", ":", "\n", "\n", "            ", "if", "i", ">", "0", ":", "\n", "# construct memory and memory_lens for the current step", "\n", "                ", "external_memory", "=", "pad_batch_tensorize", "(", "[", "memory_sents_all", "[", "article_id", "]", "for", "article_id", "in", "article_ids", "]", ",", "PAD", ",", "\n", "cuda", "=", "False", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "external_memory_lens", "=", "[", "len", "(", "memory_sents_all", "[", "article_id", "]", ")", "for", "article_id", "in", "article_ids", "]", "\n", "\n", "# decode", "\n", "", "decs", ",", "attns", "=", "self", ".", "_net", ".", "batch_decode", "(", "articles_sents_i", ",", "art_lens_i", ",", "external_memory", ",", "external_memory_lens", ",", "extend_arts_sents_i", ",", "extend_vsize", ",", "start", ",", "end", ",", "unk", ",", "max_len", ")", "\n", "# decs: a list of tensor with size [batch_size]", "\n", "# preprocess dec. dec_sents: a list of token list, len=batch_size", "\n", "dec_sents_raw", ",", "dec_sents", "=", "self", ".", "_process_dec_out", "(", "decs", ",", "attns", ",", "raw_art_sents_i", ",", "id2word", ",", "self", ".", "_word2id", ")", "\n", "\n", "# update memory_sents_all", "\n", "assert", "len", "(", "dec_sents", ")", "==", "len", "(", "article_ids", ")", "\n", "for", "dec_sent", ",", "article_id", ",", "dec_sent_raw", "in", "zip", "(", "dec_sents", ",", "article_ids", ",", "dec_sents_raw", ")", ":", "\n", "                ", "memory_sents_all", "[", "article_id", "]", "+=", "dec_sent", "\n", "dec_sents_raw_all", "[", "article_id", "]", ".", "append", "(", "dec_sent_raw", ")", "\n", "\n", "", "", "return", "list", "(", "concat", "(", "dec_sents_raw_all", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ConditionalAbstractor._process_dec_out": [[281, 304], ["enumerate", "zip", "dec_sents.append", "dec_sents_idx.append", "id_[].item", "dec_idx.append", "max", "range", "decoding.ConditionalAbstractor._process_dec_out.argmax"], "methods", ["None"], ["", "def", "_process_dec_out", "(", "self", ",", "decs", ",", "attns", ",", "raw_article_sents", ",", "id2word", ",", "word2id", ")", ":", "\n", "        ", "def", "argmax", "(", "arr", ",", "keys", ")", ":", "\n", "            ", "return", "arr", "[", "max", "(", "range", "(", "len", "(", "arr", ")", ")", ",", "key", "=", "lambda", "i", ":", "keys", "[", "i", "]", ".", "item", "(", ")", ")", "]", "\n", "", "dec_sents", "=", "[", "]", "# a list of token list, len=batch_size", "\n", "dec_sents_idx", "=", "[", "]", "# a list of int list, len=batch_size", "\n", "# convert idx to words", "\n", "for", "i", ",", "raw_words", "in", "enumerate", "(", "raw_article_sents", ")", ":", "# each input sentence", "\n", "            ", "dec", "=", "[", "]", "\n", "dec_idx", "=", "[", "]", "\n", "for", "id_", ",", "attn", "in", "zip", "(", "decs", ",", "attns", ")", ":", "# id_: a tensor with size = batch_size", "\n", "                ", "id_i", "=", "id_", "[", "i", "]", ".", "item", "(", ")", "\n", "if", "id_i", "==", "END", ":", "\n", "                    ", "break", "\n", "", "elif", "id_i", "==", "UNK", ":", "# replace unk word with highest attention word", "\n", "                    ", "max_attn_src_word", "=", "argmax", "(", "raw_words", ",", "attn", "[", "i", "]", ")", "\n", "dec", ".", "append", "(", "max_attn_src_word", ")", "\n", "#dec_idx.append(word2id[max_attn_src_word])", "\n", "", "else", ":", "\n", "                    ", "dec", ".", "append", "(", "id2word", "[", "id_i", "]", ")", "\n", "", "dec_idx", ".", "append", "(", "id_i", "if", "id_i", "<", "len", "(", "word2id", ")", "else", "UNK", ")", "# should not include oov, otherwise we cannot compute an embedding", "\n", "", "dec_sents", ".", "append", "(", "dec", ")", "\n", "dec_sents_idx", ".", "append", "(", "dec_idx", ")", "\n", "", "return", "dec_sents", ",", "dec_sents_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.BeamAbstractor.__call__": [[307, 315], ["decoding.BeamAbstractor._net.eval", "decoding.BeamAbstractor._prepro", "decoding.BeamAbstractor._net.batched_beamsearch", "list", "itertools.starmap", "decoding._process_beam", "zip"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batched_beamsearch", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._process_beam"], ["    ", "def", "__call__", "(", "self", ",", "raw_article_sents", ",", "beam_size", "=", "5", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "raw_article_sents", ")", "\n", "dec_args", "=", "(", "*", "dec_args", ",", "beam_size", ",", "diverse", ")", "\n", "all_beams", "=", "self", ".", "_net", ".", "batched_beamsearch", "(", "*", "dec_args", ")", "\n", "all_beams", "=", "list", "(", "starmap", "(", "_process_beam", "(", "id2word", ")", ",", "\n", "zip", "(", "all_beams", ",", "raw_article_sents", ")", ")", ")", "\n", "return", "all_beams", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.BeamCompressionControlAbstractor.__call__": [[318, 326], ["decoding.BeamCompressionControlAbstractor._net.eval", "decoding.BeamCompressionControlAbstractor._prepro", "decoding.BeamCompressionControlAbstractor._net.batched_beamsearch", "list", "itertools.starmap", "decoding._process_beam", "zip"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batched_beamsearch", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._process_beam"], ["    ", "def", "__call__", "(", "self", ",", "raw_article_sents", ",", "raw_compression_levels", ",", "beam_size", "=", "5", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "raw_article_sents", ",", "raw_compression_levels", ")", "\n", "dec_args", "=", "(", "*", "dec_args", ",", "beam_size", ",", "diverse", ")", "\n", "all_beams", "=", "self", ".", "_net", ".", "batched_beamsearch", "(", "*", "dec_args", ")", "\n", "all_beams", "=", "list", "(", "starmap", "(", "_process_beam", "(", "id2word", ")", ",", "\n", "zip", "(", "all_beams", ",", "raw_article_sents", ")", ")", ")", "\n", "return", "all_beams", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.BeamConditionalAbstractor.__call__": [[347, 381], ["decoding.BeamConditionalAbstractor._net.eval", "decoding.BeamConditionalAbstractor._prepro", "enumerate", "list", "zip", "decoding.BeamConditionalAbstractor._net.batched_beamsearch", "list", "decoding.rerank_one", "zip", "cytoolz.concat", "range", "range", "data.batcher.pad_batch_tensorize().to", "itertools.starmap", "len", "len", "dec_sents_raw_all[].append", "len", "decoding._process_cond_beam", "zip", "data.batcher.pad_batch_tensorize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batched_beamsearch", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model.rerank_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._process_cond_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["    ", "def", "__call__", "(", "self", ",", "sequential_raw_article_sents", ",", "sequential_article_ids", ",", "num_articles", ",", "beam_size", "=", "5", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "sequential_raw_article_sents", ")", "\n", "# (articles_sequential, art_lens_sequential, init_mem, init_mem_lens, extend_arts_sequential, extend_vsize,", "\n", "#            START, END, UNK, self._max_len)", "\n", "# dec_args = (*dec_args, beam_size, diverse)", "\n", "articles_sequential", ",", "art_lens_sequential", ",", "external_memory", ",", "external_memory_lens", ",", "extend_arts_sequential", ",", "extend_vsize", ",", "start", ",", "end", ",", "unk", ",", "max_len", "=", "dec_args", "\n", "memory_sents_all", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_articles", ")", "]", "# len=batch_size", "\n", "dec_sents_raw_all", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_articles", ")", "]", "# len=batch_size", "\n", "\n", "for", "i", ",", "(", "articles_sents_i", ",", "art_lens_i", ",", "extend_arts_sents_i", ",", "raw_art_sents_i", ",", "article_ids", ")", "in", "enumerate", "(", "\n", "zip", "(", "articles_sequential", ",", "art_lens_sequential", ",", "extend_arts_sequential", ",", "sequential_raw_article_sents", ",", "\n", "sequential_article_ids", ")", ")", ":", "\n", "            ", "if", "i", ">", "0", ":", "\n", "# construct memory and memory_lens for the current step", "\n", "                ", "external_memory", "=", "pad_batch_tensorize", "(", "[", "memory_sents_all", "[", "article_id", "]", "for", "article_id", "in", "article_ids", "]", ",", "\n", "PAD", ",", "\n", "cuda", "=", "False", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "external_memory_lens", "=", "[", "len", "(", "memory_sents_all", "[", "article_id", "]", ")", "for", "article_id", "in", "article_ids", "]", "\n", "\n", "# beam search", "\n", "", "all_beams", "=", "self", ".", "_net", ".", "batched_beamsearch", "(", "articles_sents_i", ",", "art_lens_i", ",", "external_memory", ",", "external_memory_lens", ",", "extend_arts_sents_i", ",", "extend_vsize", ",", "start", ",", "end", ",", "unk", ",", "max_len", ",", "beam_size", ",", "diverse", ")", "\n", "all_beams", "=", "list", "(", "starmap", "(", "_process_cond_beam", "(", "id2word", ",", "self", ".", "_word2id", ")", ",", "zip", "(", "all_beams", ",", "raw_art_sents_i", ")", ")", ")", "\n", "all_beams", "=", "rerank_one", "(", "all_beams", ")", "\n", "\n", "assert", "len", "(", "all_beams", ")", "==", "len", "(", "article_ids", ")", "\n", "# all_beams: a list of list of hyp", "\n", "for", "beam", ",", "article_id", "in", "zip", "(", "all_beams", ",", "article_ids", ")", ":", "\n", "                ", "memory_sents_all", "[", "article_id", "]", "+=", "beam", ".", "sequence_idx", "\n", "dec_sents_raw_all", "[", "article_id", "]", ".", "append", "(", "beam", ".", "sequence", ")", "\n", "#memory_sents_all[article_id] += beam[0].sequence_idx", "\n", "#dec_sents_raw_all[article_id].append(beam[0].sequence)", "\n", "\n", "", "", "return", "list", "(", "concat", "(", "dec_sents_raw_all", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro": [[405, 429], ["dict", "dict", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "data.batcher.conver2id", "data.batcher.pad_batch_tensorize().to", "len", "len", "len", "data.batcher.pad_batch_tensorize", "data.batcher.pad_batch_tensorize", "data.batcher.pad_batch_tensorize", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["    ", "def", "_prepro", "(", "self", ",", "raw_article_sents", ",", "raw_memory_sents", ")", ":", "\n", "        ", "ext_word2id", "=", "dict", "(", "self", ".", "_word2id", ")", "\n", "ext_id2word", "=", "dict", "(", "self", ".", "_id2word", ")", "\n", "for", "raw_words", "in", "raw_article_sents", ":", "\n", "            ", "for", "w", "in", "raw_words", ":", "\n", "                ", "if", "not", "w", "in", "ext_word2id", ":", "\n", "                    ", "ext_word2id", "[", "w", "]", "=", "len", "(", "ext_word2id", ")", "\n", "ext_id2word", "[", "len", "(", "ext_id2word", ")", "]", "=", "w", "\n", "", "", "", "articles", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_article_sents", ")", "# a list of list of int", "\n", "art_lens", "=", "[", "len", "(", "art", ")", "for", "art", "in", "articles", "]", "\n", "article", "=", "pad_batch_tensorize", "(", "articles", ",", "PAD", ",", "cuda", "=", "False", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "\n", "memories", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_memory_sents", ")", "# a list of list of int", "\n", "mem_lens", "=", "[", "len", "(", "mem", ")", "for", "mem", "in", "memories", "]", "\n", "memory", "=", "pad_batch_tensorize", "(", "memories", ",", "PAD", ",", "cuda", "=", "False", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "\n", "extend_arts", "=", "conver2id", "(", "UNK", ",", "ext_word2id", ",", "raw_article_sents", ")", "\n", "extend_art", "=", "pad_batch_tensorize", "(", "extend_arts", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "\n", "extend_vsize", "=", "len", "(", "ext_word2id", ")", "\n", "dec_args", "=", "(", "article", ",", "art_lens", ",", "memory", ",", "mem_lens", ",", "extend_art", ",", "extend_vsize", ",", "\n", "START", ",", "END", ",", "UNK", ",", "self", ".", "_max_len", ")", "\n", "return", "dec_args", ",", "ext_id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor.__call__": [[430, 437], ["decoding.MemroyBeamConditionalAbstractor._net.eval", "decoding.MemroyBeamConditionalAbstractor._prepro", "decoding.MemroyBeamConditionalAbstractor._net.batched_beamsearch", "list", "itertools.starmap", "decoding._process_beam", "zip"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.MemroyBeamConditionalAbstractor._prepro", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batched_beamsearch", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._process_beam"], ["", "def", "__call__", "(", "self", ",", "raw_article_sents", ",", "raw_memory_sents", ",", "beam_size", "=", "5", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "dec_args", ",", "id2word", "=", "self", ".", "_prepro", "(", "raw_article_sents", ",", "raw_memory_sents", ")", "\n", "dec_args", "=", "(", "*", "dec_args", ",", "beam_size", ",", "diverse", ")", "\n", "all_beams", "=", "self", ".", "_net", ".", "batched_beamsearch", "(", "*", "dec_args", ")", "\n", "all_beams", "=", "list", "(", "starmap", "(", "_process_beam", "(", "id2word", ")", ",", "zip", "(", "all_beams", ",", "raw_article_sents", ")", ")", ")", "\n", "return", "all_beams", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.Extractor.__init__": [[461, 492], ["json.load", "decoding.load_best_ckpt", "ext_cls", "ext_cls.load_state_dict", "pickle.load", "torch.device", "ext_cls.to", "open", "open", "os.path.join", "os.path.join", "pickle.load.items", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["    ", "def", "__init__", "(", "self", ",", "ext_dir", ",", "max_ext", "=", "5", ",", "cuda", "=", "True", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "ext_meta", "=", "json", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'meta.json'", ")", ")", ")", "\n", "if", "ext_meta", "[", "'net'", "]", "==", "'ml_ff_extractor'", ":", "\n", "            ", "ext_cls", "=", "ExtractSumm", "\n", "", "elif", "ext_meta", "[", "'net'", "]", "==", "'ml_rnn_extractor'", ":", "\n", "            ", "ext_cls", "=", "PtrExtractSumm", "\n", "", "elif", "ext_meta", "[", "'net'", "]", "==", "'ml_rewritten_rnn_extractor'", ":", "\n", "            ", "ext_cls", "=", "PtrExtractRewrittenSumm", "\n", "", "elif", "ext_meta", "[", "'net'", "]", "==", "'ml_rewritten_bert_rnn_extractor'", ":", "\n", "            ", "ext_cls", "=", "PtrExtractRewrittenBertSumm", "\n", "", "elif", "ext_meta", "[", "'net'", "]", "==", "'ml_rewritten_sent_bert_rnn_extractor'", ":", "\n", "            ", "ext_cls", "=", "PtrExtractRewrittenSentBertSumm", "\n", "", "elif", "ext_meta", "[", "'net'", "]", "==", "'ml_rewritten_sent_word_bert_rnn_extractor'", ":", "\n", "            ", "ext_cls", "=", "PtrExtractRewrittenSentWordBertSumm", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", ")", "\n", "", "ext_ckpt", "=", "load_best_ckpt", "(", "ext_dir", ")", "\n", "ext_args", "=", "ext_meta", "[", "'net_args'", "]", "\n", "extractor", "=", "ext_cls", "(", "**", "ext_args", ")", "\n", "extractor", ".", "load_state_dict", "(", "ext_ckpt", ")", "\n", "word2id", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "_net", "=", "extractor", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_word2id", "=", "word2id", "\n", "self", ".", "_id2word", "=", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", "\n", "self", ".", "_max_ext", "=", "max_ext", "\n", "self", ".", "_disable_selected_mask", "=", "disable_selected_mask", "\n", "if", "\"bert\"", "in", "ext_meta", "[", "'net'", "]", ":", "\n", "            ", "self", ".", "emb_type", "=", "\"bert\"", "\n", "", "else", ":", "\n", "            ", "self", ".", "emb_type", "=", "\"w2v\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.Extractor.__call__": [[493, 504], ["decoding.Extractor._net.eval", "len", "data.batcher.pad_batch_tensorize().to", "decoding.Extractor._net.extract", "data.batcher.conver2id", "data.batcher.bert_tokenizer.convert_tokens_to_ids", "data.batcher.pad_batch_tensorize", "min"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "", "def", "__call__", "(", "self", ",", "raw_article_sents", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "n_art", "=", "len", "(", "raw_article_sents", ")", "\n", "if", "self", ".", "emb_type", "==", "\"w2v\"", ":", "\n", "            ", "articles", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_article_sents", ")", "\n", "", "else", ":", "\n", "            ", "articles", "=", "[", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "sentence", ")", "for", "sentence", "in", "raw_article_sents", "]", "\n", "", "article", "=", "pad_batch_tensorize", "(", "articles", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "indices", "=", "self", ".", "_net", ".", "extract", "(", "[", "article", "]", ",", "k", "=", "min", "(", "n_art", ",", "self", ".", "_max_ext", ")", ",", "disable_selected_mask", "=", "self", ".", "_disable_selected_mask", ")", "\n", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ArticleBatcher.__init__": [[507, 512], ["torch.device", "torch.device"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["    ", "def", "__init__", "(", "self", ",", "word2id", ",", "emb_type", ",", "cuda", "=", "True", ")", ":", "\n", "        ", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "_word2id", "=", "word2id", "\n", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "emb_type", "=", "emb_type", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.ArticleBatcher.__call__": [[513, 521], ["data.batcher.pad_batch_tensorize().to", "data.batcher.conver2id", "data.batcher.bert_tokenizer.convert_tokens_to_ids", "data.batcher.pad_batch_tensorize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "def", "__call__", "(", "self", ",", "raw_article_sents", ")", ":", "\n", "        ", "if", "self", ".", "emb_type", "==", "\"w2v\"", ":", "\n", "            ", "articles", "=", "conver2id", "(", "UNK", ",", "self", ".", "_word2id", ",", "raw_article_sents", ")", "\n", "", "else", ":", "\n", "            ", "articles", "=", "[", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "sentence", ")", "for", "sentence", "in", "raw_article_sents", "]", "\n", "", "article", "=", "pad_batch_tensorize", "(", "articles", ",", "PAD", ",", "cuda", "=", "False", "\n", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "return", "article", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.RLExtractor.__init__": [[523, 582], ["json.load", "pickle.load", "decoding.load_best_ckpt", "model.rl.ActorCriticSentWordBertCand.load_state_dict", "torch.device", "model.rl.ActorCriticSentWordBertCand.to", "open", "open", "model.extract.PtrExtractSumm", "model.rl.ActorCritic", "os.path.join", "os.path.join", "decoding.ArticleBatcher", "model.extract.PtrExtractRewrittenSumm", "model.rl.ActorCriticCand", "pickle.load.items", "decoding.ArticleBatcher", "model.extract.PtrExtractRewrittenSentBertSumm", "model.rl.ActorCriticSentBertCand", "decoding.ArticleBatcher", "model.extract.PtrExtractRewrittenSentWordBertSumm", "print", "model.rl.ActorCriticSentWordBertCand", "decoding.ArticleBatcher"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["    ", "def", "__init__", "(", "self", ",", "ext_dir", ",", "cuda", "=", "True", ",", "num_candidates", "=", "1", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "ext_meta", "=", "json", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'meta.json'", ")", ")", ")", "\n", "#assert ext_meta['net'] in ['rnn-ext_abs_rl', 'rewritten_rnn-ext_abs_rl']", "\n", "ext_args", "=", "ext_meta", "[", "'net_args'", "]", "[", "'extractor'", "]", "[", "'net_args'", "]", "\n", "word2id", "=", "pkl", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'agent_vocab.pkl'", ")", ",", "'rb'", ")", ")", "\n", "self", ".", "net_type", "=", "ext_meta", "[", "'net'", "]", "\n", "if", "\"bert\"", "in", "self", ".", "net_type", ":", "\n", "            ", "self", ".", "emb_type", "=", "\"bert\"", "\n", "", "else", ":", "\n", "            ", "self", ".", "emb_type", "=", "\"w2v\"", "\n", "", "if", "self", ".", "net_type", "==", "'rnn-ext_abs_rl'", ":", "\n", "            ", "extractor", "=", "PtrExtractSumm", "(", "**", "ext_args", ")", "\n", "assert", "num_candidates", "==", "1", ",", "\"When using PtrExtractSumm, num_candidates should be 1.\"", "\n", "agent", "=", "ActorCritic", "(", "extractor", ".", "_sent_enc", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "word2id", ",", "self", ".", "emb_type", ",", "cuda", ")", ")", "\n", "", "elif", "self", ".", "net_type", "==", "'rewritten_rnn-ext_abs_rl'", ":", "\n", "            ", "extractor", "=", "PtrExtractRewrittenSumm", "(", "**", "ext_args", ")", "\n", "assert", "num_candidates", ">", "1", "and", "num_candidates", "==", "extractor", ".", "num_candidates", "\n", "agent", "=", "ActorCriticCand", "(", "extractor", ".", "_candidate_sent_enc", ",", "\n", "extractor", ".", "_candidate_agg", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "word2id", ",", "self", ".", "emb_type", ",", "cuda", ")", ",", "\n", "num_candidates", ")", "\n", "", "elif", "self", ".", "net_type", "==", "'rewritten_bert_rnn-ext_abs_rl'", ":", "\n", "            ", "raise", "ValueError", "\n", "", "elif", "self", ".", "net_type", "==", "'rewritten_sent_bert_rnn-ext_abs_rl'", ":", "\n", "            ", "extractor", "=", "PtrExtractRewrittenSentBertSumm", "(", "**", "ext_args", ")", "\n", "assert", "num_candidates", ">", "1", "and", "num_candidates", "==", "extractor", ".", "num_candidates", "\n", "agent", "=", "ActorCriticSentBertCand", "(", "extractor", ".", "_candidate_sent_encode", ",", "\n", "extractor", ".", "_bert_w", ",", "\n", "extractor", ".", "_candidate_agg", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "word2id", ",", "self", ".", "emb_type", ",", "cuda", ")", ",", "\n", "num_candidates", ")", "\n", "", "elif", "self", ".", "net_type", "==", "'rewritten_sent_word_bert_rnn-ext_abs_rl'", ":", "\n", "            ", "extractor", "=", "PtrExtractRewrittenSentWordBertSumm", "(", "**", "ext_args", ")", "\n", "print", "(", "extractor", ")", "\n", "assert", "num_candidates", ">", "1", "and", "num_candidates", "==", "extractor", ".", "num_candidates", "\n", "agent", "=", "ActorCriticSentWordBertCand", "(", "extractor", ".", "_sentence_encoder", ",", "\n", "extractor", ".", "_bert_w", ",", "\n", "extractor", ".", "_candidate_sent_enc", ",", "\n", "extractor", ".", "_candidate_agg", ",", "\n", "extractor", ".", "_art_enc", ",", "\n", "extractor", ".", "_extractor", ",", "\n", "ArticleBatcher", "(", "word2id", ",", "self", ".", "emb_type", ",", "cuda", ")", ",", "\n", "num_candidates", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "ext_ckpt", "=", "load_best_ckpt", "(", "ext_dir", ",", "reverse", "=", "True", ")", "\n", "agent", ".", "load_state_dict", "(", "ext_ckpt", ")", "\n", "self", ".", "_device", "=", "torch", ".", "device", "(", "'cuda'", "if", "cuda", "else", "'cpu'", ")", "\n", "self", ".", "_net", "=", "agent", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_word2id", "=", "word2id", "\n", "self", ".", "_id2word", "=", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", "\n", "self", ".", "_disable_selected_mask", "=", "disable_selected_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.RLExtractor.__call__": [[583, 587], ["decoding.RLExtractor._net.eval", "decoding.RLExtractor._net"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "raw_article_sents", ")", ":", "\n", "        ", "self", ".", "_net", ".", "eval", "(", ")", "\n", "indices", "=", "self", ".", "_net", "(", "raw_article_sents", ",", "self", ".", "_disable_selected_mask", ")", "\n", "return", "indices", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe": [[42, 45], ["s.replace().replace", "s.replace"], "function", ["None"], ["", "", "def", "make_html_safe", "(", "s", ")", ":", "\n", "    ", "\"\"\"Rouge use html, has to make output html safe\"\"\"", "\n", "return", "s", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.load_best_ckpt": [[47, 57], ["os.listdir", "re.compile", "sorted", "print", "print", "os.path.join", "torch.load", "os.path.join", "re.compile.match", "float", "c.split"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "load_best_ckpt", "(", "model_dir", ",", "reverse", "=", "False", ")", ":", "\n", "    ", "\"\"\" reverse=False->loss, reverse=True->reward/score\"\"\"", "\n", "ckpts", "=", "os", ".", "listdir", "(", "join", "(", "model_dir", ",", "'ckpt'", ")", ")", "\n", "ckpt_matcher", "=", "re", ".", "compile", "(", "'^ckpt-.*-[0-9]*'", ")", "\n", "ckpts", "=", "sorted", "(", "[", "c", "for", "c", "in", "ckpts", "if", "ckpt_matcher", ".", "match", "(", "c", ")", "]", ",", "\n", "key", "=", "lambda", "c", ":", "float", "(", "c", ".", "split", "(", "'-'", ")", "[", "1", "]", ")", ",", "reverse", "=", "reverse", ")", "\n", "print", "(", "'loading checkpoint {}...'", ".", "format", "(", "ckpts", "[", "0", "]", ")", ")", "\n", "ckpt", "=", "torch", ".", "load", "(", "join", "(", "model_dir", ",", "'ckpt/{}'", ".", "format", "(", "ckpts", "[", "0", "]", ")", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "[", "'state_dict'", "]", "\n", "print", "(", "\"torch.load\"", ")", "\n", "return", "ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._process_beam": [[328, 344], ["list", "zip", "map", "seq.append", "seq.append", "max", "range", "len", "attn[].item"], "function", ["None"], ["", "", "@", "curry", "\n", "def", "_process_beam", "(", "id2word", ",", "beam", ",", "art_sent", ")", ":", "\n", "    ", "def", "process_hyp", "(", "hyp", ")", ":", "\n", "        ", "seq", "=", "[", "]", "\n", "for", "i", ",", "attn", "in", "zip", "(", "hyp", ".", "sequence", "[", "1", ":", "]", ",", "hyp", ".", "attns", "[", ":", "-", "1", "]", ")", ":", "\n", "            ", "if", "i", "==", "UNK", ":", "\n", "                ", "copy_word", "=", "art_sent", "[", "max", "(", "range", "(", "len", "(", "art_sent", ")", ")", ",", "\n", "key", "=", "lambda", "j", ":", "attn", "[", "j", "]", ".", "item", "(", ")", ")", "]", "\n", "seq", ".", "append", "(", "copy_word", ")", "\n", "", "else", ":", "\n", "                ", "seq", ".", "append", "(", "id2word", "[", "i", "]", ")", "\n", "", "", "hyp", ".", "sequence", "=", "seq", "\n", "del", "hyp", ".", "hists", "\n", "del", "hyp", ".", "attns", "\n", "return", "hyp", "\n", "", "return", "list", "(", "map", "(", "process_hyp", ",", "beam", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.rerank_one": [[383, 392], ["map", "max", "collections.Counter", "decoding._make_n_gram"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor._make_n_gram"], ["", "", "def", "rerank_one", "(", "beams", ")", ":", "\n", "    ", "@", "curry", "\n", "def", "process_beam", "(", "beam", ")", ":", "\n", "        ", "for", "b", "in", "beam", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "\n", "", "beams", "=", "map", "(", "process_beam", ",", "beams", ")", "\n", "beams", "=", "[", "max", "(", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "return", "beams", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._make_n_gram": [[394, 396], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._compute_score": [[398, 402], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding._process_cond_beam": [[439, 458], ["list", "zip", "map", "seq_idx.append", "seq.append", "seq.append", "max", "len", "range", "len", "attn[].item"], "function", ["None"], ["", "", "@", "curry", "\n", "def", "_process_cond_beam", "(", "id2word", ",", "word2id", ",", "beam", ",", "art_sent", ")", ":", "\n", "    ", "def", "process_hyp", "(", "hyp", ")", ":", "\n", "        ", "seq", "=", "[", "]", "\n", "seq_idx", "=", "[", "]", "\n", "for", "i", ",", "attn", "in", "zip", "(", "hyp", ".", "sequence", "[", "1", ":", "]", ",", "hyp", ".", "attns", "[", ":", "-", "1", "]", ")", ":", "\n", "            ", "if", "i", "==", "UNK", ":", "\n", "                ", "copy_word", "=", "art_sent", "[", "max", "(", "range", "(", "len", "(", "art_sent", ")", ")", ",", "\n", "key", "=", "lambda", "j", ":", "attn", "[", "j", "]", ".", "item", "(", ")", ")", "]", "\n", "seq", ".", "append", "(", "copy_word", ")", "\n", "", "else", ":", "\n", "                ", "seq", ".", "append", "(", "id2word", "[", "i", "]", ")", "\n", "", "seq_idx", ".", "append", "(", "i", "if", "i", "<", "len", "(", "word2id", ")", "else", "UNK", ")", "\n", "", "hyp", ".", "sequence", "=", "seq", "\n", "hyp", ".", "sequence_idx", "=", "seq_idx", "\n", "del", "hyp", ".", "hists", "\n", "del", "hyp", ".", "attns", "\n", "return", "hyp", "\n", "", "return", "list", "(", "map", "(", "process_hyp", ",", "beam", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand.decode": [[29, 162], ["time.time", "decoding.Extractor", "print", "print", "decoding.DecodeDataset", "len", "torch.utils.data.DataLoader", "range", "json.load", "json.load", "print", "list", "os.makedirs", "open", "open", "open", "json.dump", "torch.no_grad", "enumerate", "decoding.Abstractor", "decoding.BeamAbstractor", "filter", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "list", "enumerate", "map", "print", "print", "print", "print", "print", "print", "print", "decoding.Extractor.", "enumerate", "print", "data.batcher.tokenize", "len", "decoding.BeamAbstractor.", "decode_baseline_cand.rerank_mp", "decoding.BeamAbstractor.", "len", "enumerate", "print", "print", "print", "print", "print", "print", "open", "f.write", "datetime.timedelta", "len", "len", "len", "candidate_list.insert", "os.path.join", "decoding.make_html_safe", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["def", "decode", "(", "save_path", ",", "abs_dir", ",", "ext_dir", ",", "split", ",", "batch_size", ",", "max_len", ",", "num_candidates", ",", "beam_size", ",", "diverse", ",", "\n", "final_rerank", ",", "keep_original_sent", ",", "cuda", ",", "abstracted", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "assert", "beam_size", ">=", "num_candidates", "and", "num_candidates", ">", "1", "\n", "\n", "\n", "if", "abstracted", ":", "\n", "        ", "abstractor", "=", "identity", "\n", "", "else", ":", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "", "", "extractor", "=", "Extractor", "(", "ext_dir", ",", "max_ext", "=", "MAX_ABS_NUM", ",", "cuda", "=", "cuda", ")", "\n", "\n", "emb_type", "=", "extractor", ".", "emb_type", "\n", "print", "(", "\"emb_type\"", ")", "\n", "print", "(", "emb_type", ")", "\n", "\n", "# setup loader", "\n", "def", "coll", "(", "batch", ")", ":", "\n", "        ", "articles", "=", "list", "(", "filter", "(", "bool", ",", "batch", ")", ")", "\n", "return", "articles", "\n", "", "dataset", "=", "DecodeDataset", "(", "split", ")", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "for", "i", "in", "range", "(", "MAX_ABS_NUM", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output_{}'", ".", "format", "(", "i", ")", ")", ")", "\n", "", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "json", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'meta.json'", ")", ")", ")", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "raw_article_batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "#tokenized_article_batch = list(map(tokenize(None), raw_article_batch))", "\n", "            ", "tokenized_article_batch", "=", "list", "(", "map", "(", "tokenize", "(", "None", ",", "emb_type", ",", "num_candidates", ")", ",", "raw_article_batch", ")", ")", "\n", "\n", "art_ids", "=", "[", "]", "\n", "if", "abstracted", ":", "\n", "                ", "num_passed_sents", "=", "0", "\n", "for", "art_sents", "in", "tokenized_article_batch", ":", "\n", "                    ", "art_ids", "+=", "[", "(", "num_passed_sents", ",", "len", "(", "art_sents", ")", ")", "]", "\n", "num_passed_sents", "+=", "len", "(", "art_sents", ")", "\n", "", "", "else", ":", "\n", "                ", "tokenized_article_batch_flattened", "=", "[", "]", "# a list of tokenized sentence for all the articles in the batch", "\n", "for", "art_sents", "in", "tokenized_article_batch", ":", "\n", "                    ", "art_ids", "+=", "[", "(", "len", "(", "tokenized_article_batch_flattened", ")", ",", "len", "(", "art_sents", ")", ")", "]", "\n", "tokenized_article_batch_flattened", "+=", "art_sents", "\n", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                    ", "all_beams", "=", "abstractor", "(", "tokenized_article_batch_flattened", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "art_ids", ",", "num_candidates", "-", "1", ",", "final_rerank", "=", "final_rerank", ")", "\n", "# dec_outs: a list of list of token list [total number of sentences in batch, num_candidates, seq_len]", "\n", "", "else", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "tokenized_article_batch_flattened", ")", "\n", "\n", "", "", "tokenized_article_batch_flattened", "=", "[", "]", "\n", "\n", "assert", "i", "==", "batch_size", "*", "i_debug", "\n", "\n", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "for", "batch_i", ",", "(", "j", ",", "n", ")", "in", "enumerate", "(", "art_ids", ")", ":", "\n", "# one article", "\n", "                ", "raw_article_sents", "=", "raw_article_batch", "[", "batch_i", "]", "\n", "if", "abstracted", ":", "\n", "                    ", "art_sents_with_cands", "=", "tokenized_article_batch", "[", "batch_i", "]", "\n", "", "else", ":", "\n", "                    ", "art_sents_with_cands", "=", "[", "]", "# a list of tokenized sentence candidates for one article", "\n", "\n", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "# one sent", "\n", "                        ", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "=", "sent", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "=", "[", "sent", "]", "\n", "\n", "", "if", "keep_original_sent", ":", "\n", "                            ", "candidate_list", ".", "insert", "(", "0", ",", "tokenized_article_batch", "[", "batch_i", "]", "[", "sent_i", "]", ")", "\n", "\n", "", "art_sents_with_cands", "+=", "candidate_list", "\n", "\n", "", "", "if", "debug", ":", "\n", "                    ", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "0", "]", ")", ")", ")", "\n", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "1", "]", ")", ")", ")", "\n", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "2", "]", ")", ")", ")", "\n", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "3", "]", ")", ")", ")", "\n", "\n", "# extraction", "\n", "", "ext", "=", "extractor", "(", "art_sents_with_cands", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "print", "(", "\"ext: {}\"", ".", "format", "(", "ext", ")", ")", "\n", "\n", "# write to .dec", "\n", "", "for", "k", ",", "ext_i", "in", "enumerate", "(", "ext", ")", ":", "\n", "#ext_str = ' '.join(art_sents_with_cands[ext_i])", "\n", "                    ", "ext_str", "=", "raw_article_sents", "[", "ext_i", "]", "\n", "if", "debug", ":", "\n", "                        ", "print", "(", "\"k: {}, i: {}, sent: {}\"", ".", "format", "(", "k", ",", "i", ",", "ext_str", ")", ")", "\n", "", "with", "open", "(", "join", "(", "save_path", ",", "'output_{}/{}.dec'", ".", "format", "(", "k", ",", "i", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                        ", "f", ".", "write", "(", "make_html_safe", "(", "ext_str", ")", ")", "\n", "", "", "i", "+=", "1", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "\n", "", "", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand.rerank": [[168, 176], ["list", "map", "map", "cytoolz.concat", "decode_baseline_cand.rereank_topk_one", "decode_baseline_cand.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "final_rerank", ":", "\n", "        ", "topked", "=", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand.rerank_mp": [[178, 187], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map", "decode_baseline_cand.rereank_topk_one", "decode_baseline_cand.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand.rereank_topk_one": [[189, 207], ["map", "decode_baseline_cand.rereank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rereank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand.topk_one": [[209, 217], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand._make_n_gram": [[219, 221], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baseline_cand._compute_score": [[223, 227], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump": [[18, 34], ["time.time", "print", "os.path.join", "os.path.join", "utils.count_data", "range", "print", "print", "enumerate", "open", "json.loads", "datetime.timedelta", "os.path.join", "f.read", "open", "f.write", "os.path.join", "decoding.make_html_safe", "time.time", "chr"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "dump", "(", "split", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "dump_dir", "=", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "print", "(", "'processing {}/{} ({:.2f}%%)\\r'", ".", "format", "(", "i", ",", "n_data", ",", "100", "*", "i", "/", "n_data", ")", ",", "\n", "end", "=", "''", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "abs_sents_list", "=", "data", "[", "'abstract'", "]", "\n", "for", "j", ",", "abs_sents", "in", "enumerate", "(", "abs_sents_list", ")", ":", "\n", "            ", "with", "open", "(", "join", "(", "dump_dir", ",", "'{}.{}.ref'", ".", "format", "(", "chr", "(", "65", "+", "j", ")", ",", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "abs_sents", ")", ")", ")", "\n", "", "", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.main": [[35, 45], ["make_eval_references_duc.dump", "make_eval_references_duc.dump", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "split", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'test'", "]", ":", "# evaluation of train data takes too long", "\n", "            ", "if", "not", "exists", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", "\n", "", "dump", "(", "split", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "exists", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "join", "(", "DATA_DIR", ",", "'refs'", ",", "split", ")", ")", "\n", "", "dump", "(", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand.decode": [[27, 215], ["time.time", "decoding.RLExtractor", "print", "print", "decoding.DecodeDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "print", "collections.Counter", "numpy.array", "np.array.dump", "numpy.array", "np.array.dump", "len", "collections.Counter.items", "open", "json.loads", "list", "os.path.join", "open", "json.dump", "torch.no_grad", "enumerate", "os.path.join", "os.path.join", "print", "os.path.join", "f.read", "decoding.Abstractor", "decoding.BeamAbstractor", "filter", "os.path.join", "list", "enumerate", "os.path.join", "os.path.join", "map", "extracted_local_idx_2dlist.append", "print", "data.batcher.tokenize", "len", "decoding.BeamAbstractor.", "decode_full_model_cand.rerank_mp", "decoding.BeamAbstractor.", "print", "print", "print", "enumerate", "print", "print", "print", "print", "decoding.RLExtractor.", "extracted_local_idx_list.append", "print", "print", "open", "f.write", "list", "print", "print", "i.item", "print", "os.path.join", "decoding.make_html_safe", "datetime.timedelta", "len", "len", "len", "candidate_list.insert", "range", "len", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["def", "decode", "(", "save_path", ",", "model_dir", ",", "split", ",", "batch_size", ",", "\n", "beam_size", ",", "diverse", ",", "max_len", ",", "cuda", ",", "num_candidates", ",", "final_rerank", ",", "keep_original_sent", ",", "disable_selected_mask", ",", "\n", "abstracted", ",", "debug", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "#assert beam_size >= num_candidates > 1", "\n", "\n", "# setup model", "\n", "with", "open", "(", "join", "(", "model_dir", ",", "'meta.json'", ")", ")", "as", "f", ":", "\n", "        ", "meta", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "\n", "", "if", "abstracted", ":", "\n", "        ", "abstractor", "=", "identity", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "\n", "assert", "meta", "[", "'net_args'", "]", "[", "'abstractor'", "]", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "Abstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "", "extractor", "=", "RLExtractor", "(", "model_dir", ",", "cuda", "=", "cuda", ",", "num_candidates", "=", "num_candidates", ",", "disable_selected_mask", "=", "disable_selected_mask", ")", "\n", "\n", "emb_type", "=", "extractor", ".", "emb_type", "\n", "\n", "print", "(", "\"emb_type\"", ")", "\n", "print", "(", "emb_type", ")", "\n", "\n", "# setup loaders", "\n", "def", "coll", "(", "batch", ")", ":", "\n", "        ", "articles", "=", "list", "(", "filter", "(", "bool", ",", "batch", ")", ")", "\n", "return", "articles", "\n", "", "dataset", "=", "DecodeDataset", "(", "split", ")", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output'", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "meta", "[", "'net_args'", "]", "[", "'abstractor'", "]", "\n", "dec_log", "[", "'extractor'", "]", "=", "meta", "[", "'net_args'", "]", "[", "'extractor'", "]", "\n", "dec_log", "[", "'rl'", "]", "=", "True", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "", "num_extracted_original_sents", "=", "0", "\n", "num_extracted_abstractions", "=", "0", "\n", "extracted_local_idx_list", "=", "[", "]", "\n", "extracted_local_idx_2dlist", "=", "[", "]", "\n", "\n", "# Decoding", "\n", "i", "=", "0", "# idx for decoded article", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "raw_article_batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "# raw_article_batch: a list of list of sent str", "\n", "            ", "tokenized_article_batch", "=", "list", "(", "map", "(", "tokenize", "(", "100", ",", "emb_type", ",", "args", ".", "num_candidates", ")", ",", "raw_article_batch", ")", ")", "\n", "\n", "art_ids", "=", "[", "]", "\n", "if", "abstracted", ":", "\n", "                ", "num_passed_sents", "=", "0", "\n", "for", "art_sents", "in", "tokenized_article_batch", ":", "\n", "                    ", "art_ids", "+=", "[", "(", "num_passed_sents", ",", "len", "(", "art_sents", ")", ")", "]", "\n", "num_passed_sents", "+=", "len", "(", "art_sents", ")", "\n", "", "", "else", ":", "\n", "                ", "tokenized_article_batch_flattened", "=", "[", "]", "# a list of tokenized sentence for all the articles in the batch", "\n", "for", "art_sents", "in", "tokenized_article_batch", ":", "\n", "                    ", "art_ids", "+=", "[", "(", "len", "(", "tokenized_article_batch_flattened", ")", ",", "len", "(", "art_sents", ")", ")", "]", "\n", "tokenized_article_batch_flattened", "+=", "art_sents", "\n", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                    ", "all_beams", "=", "abstractor", "(", "tokenized_article_batch_flattened", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "art_ids", ",", "num_candidates", "-", "1", ",", "final_rerank", "=", "final_rerank", ")", "\n", "# dec_outs: a list of list of token list [total number of sentences in batch, num_candidates, seq_len]", "\n", "", "else", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "tokenized_article_batch_flattened", ")", "\n", "\n", "", "if", "debug", ":", "\n", "                    ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "# print(\"dec_outs[1]\")", "\n", "# print(dec_outs[1])", "\n", "# print(\"length of dec_out\")", "\n", "# print(len(dec_outs))", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "", "assert", "i", "==", "batch_size", "*", "i_debug", "\n", "\n", "\n", "for", "batch_i", ",", "(", "j", ",", "n", ")", "in", "enumerate", "(", "art_ids", ")", ":", "\n", "# one article", "\n", "                ", "raw_article_sents", "=", "raw_article_batch", "[", "batch_i", "]", "\n", "if", "abstracted", ":", "\n", "                    ", "art_sents_with_cands", "=", "tokenized_article_batch", "[", "batch_i", "]", "\n", "", "else", ":", "\n", "                    ", "art_sents_with_cands", "=", "[", "]", "# a list of tokenized sentence candidates for one article", "\n", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "# one sent", "\n", "                        ", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "=", "sent", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "=", "[", "sent", "]", "\n", "\n", "", "if", "keep_original_sent", ":", "\n", "                            ", "candidate_list", ".", "insert", "(", "0", ",", "tokenized_article_batch", "[", "batch_i", "]", "[", "sent_i", "]", ")", "\n", "\n", "", "art_sents_with_cands", "+=", "candidate_list", "\n", "\n", "# debug", "\n", "", "", "\"\"\"\n                if i_debug == 1 and batch_i == 27:\n                    print()\n                    print(\"article\")\n                    print(art_sents_with_cands)\n                    exit()\n                \"\"\"", "\n", "\n", "if", "debug", ":", "\n", "                    ", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "0", "]", ")", ")", ")", "\n", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "1", "]", ")", ")", ")", "\n", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "2", "]", ")", ")", ")", "\n", "print", "(", "\"art_sents_with_cands: {}\"", ".", "format", "(", "' '", ".", "join", "(", "art_sents_with_cands", "[", "3", "]", ")", ")", ")", "\n", "\n", "# extraction", "\n", "", "ext", "=", "extractor", "(", "art_sents_with_cands", ")", "[", ":", "-", "1", "]", "\n", "if", "not", "ext", ":", "\n", "# use top-5 if nothing is extracted", "\n", "# in some rare cases rnn-ext does not extract at all", "\n", "#ext = list(range(5))[:len(art_sents_with_cands)]", "\n", "                    ", "ext", "=", "list", "(", "range", "(", "0", ",", "5", "*", "num_candidates", ",", "num_candidates", ")", ")", "[", ":", "len", "(", "art_sents_with_cands", ")", "//", "num_candidates", "]", "\n", "if", "debug", ":", "\n", "                        ", "print", "(", "\"extracted nothing, lead-5:\"", ")", "\n", "print", "(", "ext", ")", "\n", "", "", "else", ":", "\n", "                    ", "ext", "=", "[", "i", ".", "item", "(", ")", "for", "i", "in", "ext", "]", "\n", "if", "debug", ":", "\n", "                        ", "print", "(", "\"extracted sth. ext: {}\"", ".", "format", "(", "ext", ")", ")", "\n", "\n", "# log number of extracted original sentences", "\n", "", "", "extracted_local_idx_2dlist", ".", "append", "(", "ext", ")", "\n", "for", "ext_i", "in", "ext", ":", "\n", "                    ", "extracted_local_idx_list", ".", "append", "(", "ext_i", "%", "num_candidates", ")", "\n", "\"\"\"\n                    if ext_i % num_candidates == 0:\n                        num_extracted_original_sents += 1\n                    else:\n                        num_extracted_abstractions += 1\n                    \"\"\"", "\n", "\n", "", "ext_sents", "=", "[", "raw_article_sents", "[", "i", "]", "for", "i", "in", "ext", "]", "\n", "# ext_sents = [' '.join(art_sents_with_cands[i]) for i in ext]", "\n", "if", "debug", ":", "\n", "                    ", "print", "(", "\"ext_sents: \"", ")", "\n", "print", "(", "ext_sents", ")", "\n", "\n", "", "with", "open", "(", "join", "(", "save_path", ",", "'output/{}.dec'", ".", "format", "(", "i", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "ext_sents", ")", ")", ")", "\n", "", "i", "+=", "1", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", "and", "i", "==", "5", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "", "", "print", "(", ")", "\n", "#num_extracted_original_sents_ratio = num_extracted_original_sents/(num_extracted_original_sents + num_extracted_abstractions)", "\n", "#print(\"Number of extracted original sentences: {} ({:.3f})\".format(num_extracted_original_sents, num_extracted_original_sents_ratio))", "\n", "#print(\"Number of extracted abstractions: {} ({:.3f})\".format(num_extracted_abstractions, 1 - num_extracted_original_sents_ratio))", "\n", "\n", "# dump seleected sentenc indices", "\n", "extracted_local_idx_freq_counter", "=", "Counter", "(", "extracted_local_idx_list", ")", "\n", "extracted_local_idx_array", "=", "np", ".", "array", "(", "extracted_local_idx_list", ")", "\n", "extracted_local_idx_array", ".", "dump", "(", "join", "(", "save_path", ",", "'selected_indices.dat'", ")", ")", "\n", "extracted_local_idx_2darray", "=", "np", ".", "array", "(", "extracted_local_idx_2dlist", ")", "\n", "extracted_local_idx_2darray", ".", "dump", "(", "join", "(", "save_path", ",", "'selectd_indices_2d.dat'", ")", ")", "\n", "\n", "# print noramlzied count", "\n", "total_num_selected_idx", "=", "len", "(", "extracted_local_idx_list", ")", "\n", "for", "idx", ",", "cnt", "in", "extracted_local_idx_freq_counter", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "\"{}: {:.3f}\"", ".", "format", "(", "idx", ",", "cnt", "/", "total_num_selected_idx", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand.rerank": [[222, 230], ["list", "map", "map", "cytoolz.concat", "decode_full_model_cand.rereank_topk_one", "decode_full_model_cand.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "final_rerank", ":", "\n", "        ", "topked", "=", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand.rerank_mp": [[232, 241], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map", "decode_full_model_cand.rereank_topk_one", "decode_full_model_cand.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand.rereank_topk_one": [[243, 261], ["map", "decode_full_model_cand.rereank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rereank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand.topk_one": [[263, 271], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand._make_n_gram": [[273, 275], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_cand._compute_score": [[277, 281], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels._split_words": [[24, 26], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels._lcs_dp": [[28, 40], ["range", "range", "range", "len", "range", "len", "max", "len", "len"], "function", ["None"], ["", "def", "_lcs_dp", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the len dp of lcs\"\"\"", "\n", "dp", "=", "[", "[", "0", "for", "_", "in", "range", "(", "0", ",", "len", "(", "b", ")", "+", "1", ")", "]", "\n", "for", "_", "in", "range", "(", "0", ",", "len", "(", "a", ")", "+", "1", ")", "]", "\n", "# dp[i][j]: lcs_len(a[:i], b[:j])", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "a", ")", "+", "1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "b", ")", "+", "1", ")", ":", "\n", "            ", "if", "a", "[", "i", "-", "1", "]", "==", "b", "[", "j", "-", "1", "]", ":", "\n", "                ", "dp", "[", "i", "]", "[", "j", "]", "=", "dp", "[", "i", "-", "1", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "dp", "[", "i", "]", "[", "j", "]", "=", "max", "(", "dp", "[", "i", "-", "1", "]", "[", "j", "]", ",", "dp", "[", "i", "]", "[", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "dp", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels._lcs": [[42, 59], ["make_group_ext_labels._lcs_dp", "len", "len", "collections.deque", "len", "collections.deque.appendleft"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp"], ["", "def", "_lcs", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the longest common subsequence between a and b\"\"\"", "\n", "dp", "=", "_lcs_dp", "(", "a", ",", "b", ")", "\n", "i", "=", "len", "(", "a", ")", "\n", "j", "=", "len", "(", "b", ")", "\n", "lcs", "=", "deque", "(", ")", "\n", "while", "(", "i", ">", "0", "and", "j", ">", "0", ")", ":", "\n", "        ", "if", "a", "[", "i", "-", "1", "]", "==", "b", "[", "j", "-", "1", "]", ":", "\n", "            ", "lcs", ".", "appendleft", "(", "a", "[", "i", "-", "1", "]", ")", "\n", "i", "-=", "1", "\n", "j", "-=", "1", "\n", "", "elif", "dp", "[", "i", "-", "1", "]", "[", "j", "]", ">=", "dp", "[", "i", "]", "[", "j", "-", "1", "]", ":", "\n", "            ", "i", "-=", "1", "\n", "", "else", ":", "\n", "            ", "j", "-=", "1", "\n", "", "", "assert", "len", "(", "lcs", ")", "==", "dp", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "return", "lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.get_extract_label": [[61, 81], ["make_group_ext_labels.get_extract_label_for_one_abstract_sent", "extracted.append", "extracted_major.append", "distances.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_extract_label_for_one_abstract_sent"], ["", "def", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ":", "\n", "    ", "\"\"\" greedily match summary sentences to article sentences\"\"\"", "\n", "extracted", "=", "[", "]", "\n", "#scores = []", "\n", "#indices = list(range(len(art_sents)))", "\n", "extracted_major", "=", "[", "]", "\n", "distances", "=", "[", "]", "\n", "for", "abst", "in", "abs_sents", ":", "\n", "#rouges = list(map(compute_rouge_l(reference=abst, mode='r'), art_sents))", "\n", "#rouges = list(map(compute_rouge_l(reference=abst, mode=ROUGE_mode), art_sents))  # Rouge-L F1", "\n", "        ", "ext", ",", "distance", "=", "get_extract_label_for_one_abstract_sent", "(", "art_sents", ",", "abst", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ",", "extracted_major", ")", "\n", "#ext = max(indices, key=lambda i: rouges[i])", "\n", "#indices.remove(ext)", "\n", "extracted", ".", "append", "(", "ext", ")", "\n", "extracted_major", ".", "append", "(", "ext", "[", "0", "]", ")", "\n", "distances", ".", "append", "(", "distance", ")", "\n", "#scores.append(rouges[ext])", "\n", "if", "len", "(", "extracted_major", ")", "==", "len", "(", "abs_sents", ")", ":", "\n", "            ", "break", "\n", "", "", "return", "extracted", ",", "distances", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.get_extract_label_for_one_abstract_sent": [[83, 115], ["make_group_ext_labels.find_max_val_and_idx", "make_group_ext_labels._lcs_len", "make_group_ext_labels.find_max_val_and_idx", "metric.compute_rouge_l", "enumerate", "ext_labels.append", "abs", "make_group_ext_labels.find_max_val_and_idx", "make_group_ext_labels.get_union_lcs_len", "enumerate", "ext_labels.append", "abs", "metric.compute_rouge_l_summ", "enumerate"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_len", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_union_lcs_len", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l_summ"], ["", "def", "get_extract_label_for_one_abstract_sent", "(", "art_sents", ",", "abs_sent", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ",", "escape_art_sent_indices", "=", "[", "]", ")", ":", "\n", "# pick doc sentences with the highest Rouge-L recall (major sentence)", "\n", "#indices = list(range(len(art_sents)))", "\n", "#rouges = list(map(compute_rouge_l(reference=abs_sent, mode=ROUGE_mode), art_sents))  # Rouge-L F1", "\n", "    ", "rouges", "=", "[", "compute_rouge_l", "(", "output", "=", "art_sent", ",", "reference", "=", "abs_sent", ",", "mode", "=", "ROUGE_mode", ")", "if", "art_sent_idx", "not", "in", "escape_art_sent_indices", "else", "-", "1", "for", "art_sent_idx", ",", "art_sent", "in", "enumerate", "(", "art_sents", ")", "]", "\n", "major_art_sent_rouge", ",", "major_art_sent_idx", "=", "find_max_val_and_idx", "(", "rouges", ")", "\n", "major_art_sent", "=", "art_sents", "[", "major_art_sent_idx", "]", "\n", "ext_labels", "=", "[", "major_art_sent_idx", "]", "\n", "distance", "=", "None", "\n", "\n", "if", "ext_type", "==", "0", ":", "\n", "#threshold = 3", "\n", "        ", "major_lcs_len", "=", "_lcs_len", "(", "major_art_sent", ",", "abs_sent", ")", "\n", "#major_sent_idx = max(indices, key=lambda i: rouges[i])", "\n", "\n", "union_lcs_len_list", "=", "[", "get_union_lcs_len", "(", "[", "major_art_sent", ",", "art_sent", "]", ",", "abs_sent", ")", "if", "art_sent_idx", "!=", "major_art_sent_idx", "else", "-", "1", "for", "art_sent_idx", ",", "art_sent", "in", "enumerate", "(", "art_sents", ")", "]", "\n", "major_minor_union_lcs_len", ",", "minor_art_sent_idx", "=", "find_max_val_and_idx", "(", "union_lcs_len_list", ")", "\n", "if", "major_minor_union_lcs_len", "-", "major_lcs_len", ">=", "threshold", ":", "\n", "            ", "ext_labels", ".", "append", "(", "minor_art_sent_idx", ")", "\n", "distance", "=", "abs", "(", "major_art_sent_idx", "-", "minor_art_sent_idx", ")", "\n", "", "", "elif", "ext_type", "==", "1", ":", "\n", "#threshold = 0.01", "\n", "        ", "minor_sent_candidates_rouges", "=", "[", "\n", "compute_rouge_l_summ", "(", "[", "major_art_sent", ",", "art_sent", "]", ",", "[", "abs_sent", "]", ",", "mode", "=", "ROUGE_mode", ")", "if", "art_sent_idx", "!=", "major_art_sent_idx", "else", "-", "1", "for", "\n", "art_sent_idx", ",", "art_sent", "in", "enumerate", "(", "art_sents", ")", "]", "\n", "major_minor_rouge", ",", "minor_art_sent_idx", "=", "find_max_val_and_idx", "(", "minor_sent_candidates_rouges", ")", "\n", "if", "major_minor_rouge", "-", "major_art_sent_rouge", ">=", "threshold", ":", "\n", "            ", "ext_labels", ".", "append", "(", "minor_art_sent_idx", ")", "\n", "distance", "=", "abs", "(", "major_art_sent_idx", "-", "minor_art_sent_idx", ")", "\n", "\n", "# compute_rouge_l_summ", "\n", "", "", "return", "ext_labels", ",", "distance", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.find_max_val_and_idx": [[117, 125], ["enumerate"], "function", ["None"], ["", "def", "find_max_val_and_idx", "(", "val_list", ")", ":", "\n", "    ", "max_val", "=", "-", "1000000", "\n", "max_idx", "=", "-", "1", "\n", "for", "idx", ",", "val", "in", "enumerate", "(", "val_list", ")", ":", "\n", "        ", "if", "val", ">", "max_val", ":", "\n", "            ", "max_val", "=", "val", "\n", "max_idx", "=", "idx", "\n", "", "", "return", "max_val", ",", "max_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.get_union_lcs_len": [[127, 139], ["collections.Counter", "collections.Counter", "cytoolz.concat", "make_group_ext_labels._lcs"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs"], ["", "def", "get_union_lcs_len", "(", "art_sents", ",", "ref_sent", ")", ":", "\n", "    ", "tot_hit", "=", "0", "\n", "art_cnt", "=", "Counter", "(", "concat", "(", "art_sents", ")", ")", "\n", "ref_cnt", "=", "Counter", "(", "ref_sent", ")", "\n", "for", "art_sent", "in", "art_sents", ":", "\n", "        ", "lcs", "=", "_lcs", "(", "art_sent", ",", "ref_sent", ")", "\n", "for", "gram", "in", "lcs", ":", "\n", "            ", "if", "ref_cnt", "[", "gram", "]", ">", "0", "and", "art_cnt", "[", "gram", "]", ">", "0", ":", "\n", "                ", "tot_hit", "+=", "1", "\n", "", "ref_cnt", "[", "gram", "]", "-=", "1", "\n", "art_cnt", "[", "gram", "]", "-=", "1", "\n", "", "", "return", "tot_hit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels._lcs_len": [[141, 145], ["make_group_ext_labels._lcs_dp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp"], ["", "def", "_lcs_len", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the length of longest common subsequence between a and b\"\"\"", "\n", "dp", "=", "_lcs_dp", "(", "a", ",", "b", ")", "\n", "return", "dp", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.process": [[147, 165], ["os.path.join", "cytoolz.compose", "cytoolz.compose.", "cytoolz.compose.", "open", "json.loads", "make_group_ext_labels.get_extract_label", "open", "json.dump", "os.path.join", "f.read", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.get_extract_label", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "@", "curry", "\n", "def", "process", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ",", "i", ")", ":", "\n", "    ", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "tokenize", "=", "compose", "(", "list", ",", "_split_words", ")", "\n", "art_sents", "=", "tokenize", "(", "data", "[", "'article'", "]", ")", "\n", "abs_sents", "=", "tokenize", "(", "data", "[", "'abstract'", "]", ")", "\n", "if", "art_sents", "and", "abs_sents", ":", "# some data contains empty article/abstract", "\n", "        ", "extracted", ",", "distances", "=", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", "\n", "", "else", ":", "\n", "        ", "extracted", ",", "distances", "=", "[", "]", ",", "[", "]", "\n", "#data.pop('extracted_by_lcs', None)", "\n", "#data.pop('distances_lcs', None)", "\n", "", "data", "[", "'extracted_two_to_one_{}'", ".", "format", "(", "threshold", ")", "]", "=", "extracted", "\n", "data", "[", "'distances_two_to_one_{}'", ".", "format", "(", "threshold", ")", "]", "=", "distances", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.label_mp": [[167, 177], ["time.time", "print", "os.path.join", "utils.count_data", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "make_group_ext_labels.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "label_mp", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels.main": [[179, 185], ["make_group_ext_labels.label_mp", "make_group_ext_labels.label_mp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp"], ["", "def", "main", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'train'", "]", ":", "# no need of extraction label when testing", "\n", "            ", "label_mp", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", "\n", "", "", "else", ":", "\n", "        ", "label_mp", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.__init__": [[13, 17], ["sentence_transformers.SentenceTransformer.__init__", "print"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", "=", "None", ",", "modules", ":", "Iterable", "[", "nn", ".", "Module", "]", "=", "None", ",", "device", ":", "str", "=", "None", ",", "num_bert_layers", ":", "int", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name_or_path", ",", "modules", ",", "device", ")", "\n", "self", ".", "num_bert_layers", "=", "num_bert_layers", "\n", "print", "(", "\"num_Bert_layers: {}\"", ".", "format", "(", "num_bert_layers", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.encode_tensor": [[18, 36], ["sentence_transformer_wrapper.SentenceTransformerWrapper.forward", "torch.cat", "attention_mask_tensor.unsqueeze().expand().float", "attention_mask_tensor.unsqueeze().expand().float.sum", "torch.cat", "torch.sum", "attention_mask_tensor.unsqueeze().expand", "torch.cat.size", "attention_mask_tensor.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.forward"], ["", "def", "encode_tensor", "(", "self", ",", "input_ids_tensor", ",", "attention_mask_tensor", ")", ":", "\n", "        ", "token_type_ids", "=", "None", "\n", "input_dict", "=", "{", "\"input_ids\"", ":", "input_ids_tensor", ",", "\"attention_mask\"", ":", "attention_mask_tensor", ",", "\"token_type_ids\"", ":", "token_type_ids", "}", "\n", "out_features", "=", "self", ".", "forward", "(", "input_dict", ")", "\n", "last_layer_sent_embeddings", "=", "out_features", "[", "\"sentence_embedding\"", "]", "# [num_sents * num_cands, 768]", "\n", "\n", "if", "self", ".", "num_bert_layers", "==", "1", ":", "\n", "            ", "return", "last_layer_sent_embeddings", "\n", "", "else", ":", "\n", "            ", "all_layer_embeddings", "=", "out_features", "[", "'all_layer_embeddings'", "]", "# word embeddings", "\n", "concated_embeddings", "=", "torch", ".", "cat", "(", "all_layer_embeddings", "[", "-", "2", ":", "-", "self", ".", "num_bert_layers", "-", "1", ":", "-", "1", "]", ",", "dim", "=", "-", "1", ")", "\n", "# [num_sents * num_cands, sent_len, 768 * (num_bert_layers-1)]", "\n", "input_mask_expanded", "=", "attention_mask_tensor", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "concated_embeddings", ".", "size", "(", ")", ")", ".", "float", "(", ")", "\n", "sum_mask", "=", "input_mask_expanded", ".", "sum", "(", "1", ")", "\n", "lower_layers_sent_embeddings", "=", "torch", ".", "sum", "(", "concated_embeddings", ",", "dim", "=", "1", ")", "/", "sum_mask", "\n", "# [num_sents * num_cands, 768 * (num_bert_layers-1)]", "\n", "concated_sent_embeddings", "=", "torch", ".", "cat", "(", "[", "last_layer_sent_embeddings", ",", "lower_layers_sent_embeddings", "]", ",", "dim", "=", "-", "1", ")", "\n", "return", "concated_sent_embeddings", "# [num_sents * num_cands, 768 * num_bert_layers]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.encode_word": [[38, 52], ["sentence_transformer_wrapper.SentenceTransformerWrapper.forward", "torch.cat", "attention_mask_tensor.unsqueeze().expand().float", "attention_mask_tensor.unsqueeze().expand", "torch.cat.size", "attention_mask_tensor.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.forward"], ["", "", "def", "encode_word", "(", "self", ",", "input_ids_tensor", ",", "attention_mask_tensor", ")", ":", "\n", "        ", "token_type_ids", "=", "None", "\n", "input_dict", "=", "{", "\"input_ids\"", ":", "input_ids_tensor", ",", "\"attention_mask\"", ":", "attention_mask_tensor", ",", "\"token_type_ids\"", ":", "token_type_ids", "}", "\n", "out_features", "=", "self", ".", "forward", "(", "input_dict", ")", "\n", "#token_embeddings = out_features[\"token_embeddings\"]", "\n", "#input_mask = out_features['attention_mask']", "\n", "all_layer_embeddings", "=", "out_features", "[", "'all_layer_embeddings'", "]", "\n", "concated_embeddings", "=", "torch", ".", "cat", "(", "all_layer_embeddings", "[", "-", "1", ":", "-", "self", ".", "num_bert_layers", "-", "1", ":", "-", "1", "]", ",", "dim", "=", "-", "1", ")", "\n", "# [num_sents * num_cands, sent_len, 768 * num_bert_layers]", "\n", "\n", "input_mask_expanded", "=", "attention_mask_tensor", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "concated_embeddings", ".", "size", "(", ")", ")", ".", "float", "(", ")", "\n", "embeddings", "=", "concated_embeddings", "*", "input_mask_expanded", "\n", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_word2vec.Sentences.__init__": [[23, 26], ["os.path.join", "utils.count_data"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_path", "=", "join", "(", "DATA_DIR", ",", "'train'", ")", "\n", "self", ".", "_n_data", "=", "count_data", "(", "self", ".", "_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_word2vec.Sentences.__iter__": [[27, 33], ["range", "cytoolz.concatv", "open", "json.loads", "os.path.join", "f.read", "s.lower().split", "s.lower"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "_n_data", ")", ":", "\n", "            ", "with", "open", "(", "join", "(", "self", ".", "_path", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "for", "s", "in", "concatv", "(", "data", "[", "'article'", "]", ",", "data", "[", "'abstract'", "]", ")", ":", "\n", "                ", "yield", "[", "'<s>'", "]", "+", "s", ".", "lower", "(", ")", ".", "split", "(", ")", "+", "[", "r'<\\s>'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_word2vec.main": [[35, 59], ["logging.basicConfig", "time.time", "train_word2vec.Sentences", "gensim.models.Word2Vec", "gensim.models.Word2Vec.build_vocab", "print", "gensim.models.Word2Vec.train", "gensim.models.Word2Vec.save", "gensim.models.Word2Vec.wv.save_word2vec_format", "print", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "datetime.timedelta", "datetime.timedelta", "len", "len", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save"], ["", "", "", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "\n", "level", "=", "logging", ".", "INFO", ")", "\n", "start", "=", "time", "(", ")", "\n", "save_dir", "=", "args", ".", "path", "\n", "if", "not", "exists", "(", "save_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "\n", "", "sentences", "=", "Sentences", "(", ")", "\n", "model", "=", "gensim", ".", "models", ".", "Word2Vec", "(", "\n", "size", "=", "args", ".", "dim", ",", "min_count", "=", "5", ",", "workers", "=", "16", ",", "sg", "=", "1", ")", "\n", "model", ".", "build_vocab", "(", "sentences", ")", "\n", "print", "(", "'vocab built in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "model", ".", "train", "(", "sentences", ",", "\n", "total_examples", "=", "model", ".", "corpus_count", ",", "epochs", "=", "model", ".", "iter", ")", "\n", "\n", "model", ".", "save", "(", "join", "(", "save_dir", ",", "'word2vec.{}d.{}k.bin'", ".", "format", "(", "\n", "args", ".", "dim", ",", "len", "(", "model", ".", "wv", ".", "vocab", ")", "//", "1000", ")", ")", ")", "\n", "model", ".", "wv", ".", "save_word2vec_format", "(", "join", "(", "\n", "save_dir", ",", "\n", "'word2vec.{}d.{}k.w2v'", ".", "format", "(", "args", ".", "dim", ",", "len", "(", "model", ".", "wv", ".", "vocab", ")", "//", "1000", ")", "\n", ")", ")", "\n", "\n", "print", "(", "'word2vec trained in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.ExtractDataset.__init__": [[49, 51], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.ExtractDataset.__getitem__": [[52, 56], ["data.data.CnnDmDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "extracts", "=", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'extracted'", "]", "\n", "return", "art_sents", ",", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.ExtractDatasetStop.__init__": [[61, 63], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.ExtractDatasetStop.__getitem__": [[64, 69], ["data.data.CnnDmDataset.__getitem__", "extracts.append", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "extracts", "=", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'extracted'", "]", "\n", "extracts", ".", "append", "(", "len", "(", "art_sents", ")", ")", "\n", "return", "art_sents", ",", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.build_batchers": [[70, 105], ["data.batcher.prepro_fn_extract", "cytoolz.compose", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "len", "batchify_fn", "convert_batch", "dataset_class", "dataset_class"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_extract", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch"], ["", "", "def", "build_batchers", "(", "net_type", ",", "word2id", ",", "cuda", ",", "train_set_folder", ",", "valid_set_folder", ",", "debug", ",", "auto_stop", "=", "False", ",", "emb_type", "=", "\"w2v\"", ",", "num_candidates", "=", "1", ")", ":", "\n", "    ", "assert", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", ",", "'rewritten_bert_rnn'", ",", "'rewritten_sent_bert_rnn'", ",", "'rewritten_sent_word_bert_rnn'", "]", "\n", "prepro", "=", "prepro_fn_extract", "(", "args", ".", "max_word", ",", "args", ".", "max_sent", ",", "emb_type", ",", "num_candidates", ")", "\n", "def", "sort_key", "(", "sample", ")", ":", "\n", "        ", "src_sents", ",", "_", "=", "sample", "\n", "return", "len", "(", "src_sents", ")", "\n", "", "batchify_fn", "=", "(", "batchify_fn_extract_ff", "if", "net_type", "==", "'ff'", "\n", "else", "batchify_fn_extract_ptr", ")", "\n", "convert_batch", "=", "(", "convert_batch_extract_ff", "if", "net_type", "==", "'ff'", "\n", "else", "convert_batch_extract_ptr", ")", "\n", "batchify", "=", "compose", "(", "batchify_fn", "(", "PAD", ",", "cuda", "=", "cuda", ")", ",", "\n", "convert_batch", "(", "UNK", ",", "word2id", ",", "emb_type", ")", ")", "\n", "\n", "if", "auto_stop", ":", "\n", "        ", "dataset_class", "=", "ExtractDatasetStop", "\n", "", "else", ":", "\n", "        ", "dataset_class", "=", "ExtractDataset", "\n", "\n", "", "train_loader", "=", "DataLoader", "(", "\n", "dataset_class", "(", "train_set_folder", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "not", "debug", ",", "\n", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_extract", "\n", ")", "\n", "train_batcher", "=", "BucketedGenerater", "(", "train_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "False", ",", "fork", "=", "not", "debug", ")", "\n", "\n", "val_loader", "=", "DataLoader", "(", "\n", "dataset_class", "(", "valid_set_folder", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn_extract", "\n", ")", "\n", "val_batcher", "=", "BucketedGenerater", "(", "val_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "fork", "=", "not", "debug", ")", "\n", "return", "train_batcher", ",", "val_batcher", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.configure_net": [[107, 147], ["print", "sum", "print", "model.extract.ExtractSumm", "model.extract.PtrExtractSumm", "p.numel", "model.extract.PtrExtractRewrittenSumm", "model.extract.PtrExtractRewrittenSentWordBertSumm.parameters", "model.extract.PtrExtractRewrittenBertSumm", "model.extract.PtrExtractRewrittenSentBertSumm", "model.extract.PtrExtractRewrittenSentWordBertSumm"], "function", ["None"], ["", "def", "configure_net", "(", "net_type", ",", "vocab_size", ",", "emb_dim", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "num_candidates", "=", "-", "1", ",", "candidate_agg_type", "=", "'mean'", ",", "auto_stop", "=", "False", ")", ":", "\n", "    ", "assert", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", ",", "'rewritten_bert_rnn'", ",", "'rewritten_sent_bert_rnn'", ",", "'rewritten_sent_word_bert_rnn'", "]", "\n", "net_args", "=", "{", "}", "\n", "net_args", "[", "'vocab_size'", "]", "=", "vocab_size", "\n", "net_args", "[", "'emb_dim'", "]", "=", "emb_dim", "\n", "net_args", "[", "'conv_hidden'", "]", "=", "conv_hidden", "\n", "net_args", "[", "'lstm_hidden'", "]", "=", "lstm_hidden", "\n", "net_args", "[", "'lstm_layer'", "]", "=", "lstm_layer", "\n", "net_args", "[", "'bidirectional'", "]", "=", "bidirectional", "\n", "net_args", "[", "'auto_stop'", "]", "=", "auto_stop", "\n", "if", "\"rewritten\"", "in", "net_type", ":", "\n", "        ", "assert", "num_candidates", ">", "0", "\n", "net_args", "[", "'num_candidates'", "]", "=", "num_candidates", "\n", "net_args", "[", "'candidate_agg_type'", "]", "=", "candidate_agg_type", "\n", "if", "\"bert\"", "in", "net_type", ":", "\n", "            ", "net_args", "[", "\"cache_dir\"", "]", "=", "CACHE_DIR", "\n", "\n", "", "", "if", "net_type", "==", "'ff'", ":", "\n", "        ", "net", "=", "ExtractSumm", "(", "**", "net_args", ")", "\n", "", "elif", "net_type", "==", "'rnn'", ":", "\n", "        ", "net", "=", "PtrExtractSumm", "(", "**", "net_args", ")", "\n", "", "elif", "net_type", "==", "'rewritten_rnn'", ":", "\n", "        ", "net", "=", "PtrExtractRewrittenSumm", "(", "**", "net_args", ")", "\n", "", "elif", "net_type", "==", "'rewritten_bert_rnn'", ":", "\n", "        ", "net", "=", "PtrExtractRewrittenBertSumm", "(", "**", "net_args", ")", "\n", "", "elif", "net_type", "==", "'rewritten_sent_bert_rnn'", ":", "\n", "        ", "net", "=", "PtrExtractRewrittenSentBertSumm", "(", "**", "net_args", ")", "\n", "", "elif", "net_type", "==", "'rewritten_sent_word_bert_rnn'", ":", "\n", "        ", "net", "=", "PtrExtractRewrittenSentWordBertSumm", "(", "**", "net_args", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "\n", "# net = (ExtractSumm(**net_args) if net_type == 'ff' else PtrExtractSumm(**net_args))", "\n", "", "print", "(", "net", ")", "\n", "\n", "# print number of trainable parameters", "\n", "trainable_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "net", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "\"Number of trainable parameters: {}\"", ".", "format", "(", "trainable_params", ")", ")", "\n", "\n", "return", "net", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.configure_training": [[149, 171], ["torch.nn.functional.binary_cross_entropy_with_logits", "torch.nn.functional.cross_entropy", "model.util.sequence_loss"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss"], ["", "def", "configure_training", "(", "net_type", ",", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\" supports Adam optimizer only\"\"\"", "\n", "assert", "opt", "in", "[", "'adam'", "]", "\n", "assert", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", ",", "'rewritten_bert_rnn'", ",", "'rewritten_sent_bert_rnn'", ",", "'rewritten_sent_word_bert_rnn'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "\n", "if", "net_type", "==", "'ff'", ":", "\n", "        ", "criterion", "=", "lambda", "logit", ",", "target", ":", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "ce", "=", "lambda", "logit", ",", "target", ":", "F", ".", "cross_entropy", "(", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "def", "criterion", "(", "logits", ",", "targets", ")", ":", "\n", "            ", "return", "sequence_loss", "(", "logits", ",", "targets", ",", "ce", ",", "pad_idx", "=", "-", "1", ")", "\n", "\n", "", "", "return", "criterion", ",", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_extractor_ml.main": [[173, 234], ["utils.make_vocab", "train_extractor_ml.build_batchers", "train_extractor_ml.configure_net", "train_extractor_ml.configure_training", "training.basic_validate", "training.get_basic_grad_fn", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "training.BasicPipeline", "training.BasicTrainer", "print", "print", "training.BasicTrainer.train", "open", "pickle.load", "len", "utils.make_embedding", "net.cuda.set_embedding", "os.path.exists", "os.makedirs", "open", "pickle.dump", "open", "json.dump", "net.cuda.parameters", "net.cuda.cuda", "os.path.join", "os.path.join", "os.path.join", "utils.make_vocab.items"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "assert", "args", ".", "net_type", "in", "[", "'ff'", ",", "'rnn'", ",", "'rewritten_rnn'", ",", "'rewritten_bert_rnn'", ",", "'rewritten_sent_bert_rnn'", ",", "'rewritten_sent_word_bert_rnn'", "]", "\n", "if", "\"bert\"", "in", "args", ".", "net_type", ":", "\n", "        ", "args", ".", "emb_type", "=", "\"bert\"", "\n", "", "else", ":", "\n", "        ", "args", ".", "emb_type", "=", "\"w2v\"", "\n", "# create data batcher, vocabulary", "\n", "# batcher", "\n", "", "with", "open", "(", "join", "(", "DATA_DIR", ",", "'vocab_cnt.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "wc", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "word2id", "=", "make_vocab", "(", "wc", ",", "args", ".", "vsize", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "args", ".", "net_type", ",", "word2id", ",", "\n", "args", ".", "cuda", ",", "args", ".", "train_set_folder", ",", "args", ".", "valid_set_folder", ",", "args", ".", "debug", ",", "args", ".", "auto_stop", ",", "args", ".", "emb_type", ",", "args", ".", "num_candidates", ")", "\n", "\n", "# make net", "\n", "net", ",", "net_args", "=", "configure_net", "(", "args", ".", "net_type", ",", "\n", "len", "(", "word2id", ")", ",", "args", ".", "emb_dim", ",", "args", ".", "conv_hidden", ",", "\n", "args", ".", "lstm_hidden", ",", "args", ".", "lstm_layer", ",", "args", ".", "bi", ",", "args", ".", "num_candidates", ",", "args", ".", "candidate_agg_type", ",", "args", ".", "auto_stop", ")", "\n", "if", "args", ".", "w2v", "and", "\"bert\"", "not", "in", "args", ".", "net_type", ":", "\n", "# NOTE: the pretrained embedding having the same dimension", "\n", "#       as args.emb_dim should already be trained", "\n", "        ", "embedding", ",", "_", "=", "make_embedding", "(", "\n", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", ",", "args", ".", "w2v", ")", "\n", "net", ".", "set_embedding", "(", "embedding", ")", "\n", "\n", "# configure training setting", "\n", "", "criterion", ",", "train_params", "=", "configure_training", "(", "\n", "args", ".", "net_type", ",", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", "\n", ")", "\n", "\n", "# save experiment setting", "\n", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "word2id", ",", "f", ",", "protocol", "=", "4", ")", "\n", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'ml_{}_extractor'", ".", "format", "(", "args", ".", "net_type", ")", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'traing_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "val_fn", "=", "basic_validate", "(", "net", ",", "criterion", ")", "\n", "grad_fn", "=", "get_basic_grad_fn", "(", "net", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "args", ".", "min_lr", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "", "pipeline", "=", "BasicPipeline", "(", "meta", "[", "'net'", "]", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "args", ".", "batch", ",", "val_fn", ",", "\n", "criterion", ",", "optimizer", ",", "grad_fn", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.lead_baseline._count_data": [[11, 18], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.lead_baseline.main": [[20, 49], ["lead_baseline._count_data", "range", "os.path.exists", "os.makedirs", "os.makedirs", "os.path.join", "json.load", "json.dump", "os.path.join", "open", "sent_i.split", "open", "open", "f.write", "os.path.join", "sent_i.strip", "lead_sents.append", "len", "lead_sents.append", "os.path.join", "os.path.join", "decoding.make_html_safe", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "main", "(", "pred_path", ",", "data_dir", ",", "split", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "pred_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "pred_path", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "pred_path", ",", "'output'", ")", ")", "\n", "", "n_data", "=", "_count_data", "(", "join", "(", "data_dir", ",", "split", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "js", "=", "json", ".", "load", "(", "open", "(", "join", "(", "data_dir", ",", "split", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "doc_sents", "=", "js", "[", "\"article\"", "]", "\n", "lead_sents", "=", "[", "]", "\n", "#word_limit = 200", "\n", "#word_count = 0", "\n", "word_left", "=", "200", "\n", "for", "sent_i", "in", "doc_sents", ":", "\n", "            ", "if", "sent_i", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "sent_i_words", "=", "sent_i", ".", "split", "(", "\" \"", ")", "\n", "if", "word_left", "-", "len", "(", "sent_i_words", ")", ">=", "0", ":", "\n", "                ", "lead_sents", ".", "append", "(", "sent_i", ")", "\n", "word_left", "-=", "len", "(", "sent_i_words", ")", "\n", "", "else", ":", "\n", "                ", "sent_i_trunc", "=", "\" \"", ".", "join", "(", "sent_i_words", "[", ":", "word_left", "]", ")", "\n", "lead_sents", ".", "append", "(", "sent_i_trunc", ")", "\n", "break", "\n", "\n", "", "", "log", "=", "{", "'split'", ":", "'test'", "}", "\n", "json", ".", "dump", "(", "log", ",", "open", "(", "join", "(", "pred_path", ",", "'log.json'", ")", ",", "'w'", ")", ")", "\n", "with", "open", "(", "join", "(", "pred_path", ",", "'output'", ",", "'{}.dec'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "lead_sents", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.ext_label_rouge_stat.main": [[14, 33], ["os.path.join", "utils.count_data", "range", "print", "json.load", "enumerate", "open", "article[].strip().split", "abstract[].strip().split", "os.path.join", "article[].strip", "abstract[].strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "data_path", "=", "join", "(", "DATA_DIR", ",", "args", ".", "split", ")", "\n", "\n", "n_data", "=", "count_data", "(", "data_path", ")", "\n", "compress_ratios", "=", "[", "]", "\n", "compress_sents_lens", "=", "[", "]", "\n", "original_sents_lens", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "print", "(", "'processing {}/{} ({:.2f}%%)\\r'", ".", "format", "(", "i", ",", "n_data", ",", "100", "*", "i", "/", "n_data", ")", ",", "\n", "end", "=", "''", ")", "\n", "js_obj", "=", "json", ".", "load", "(", "open", "(", "join", "(", "data_path", ",", "\"{}.json\"", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "abstract", "=", "js_obj", "[", "'abstract'", "]", "\n", "article", "=", "js_obj", "[", "'article'", "]", "\n", "ext_ids", "=", "js_obj", "[", "'extracted'", "]", "\n", "\n", "for", "j", ",", "ext_id", "in", "enumerate", "(", "ext_ids", ")", ":", "\n", "            ", "original_sent", "=", "article", "[", "ext_id", "]", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "compressed_sent", "=", "abstract", "[", "j", "]", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.DecodeCandidateDataset.__init__": [[35, 37], ["data.data.CnnDmDatasetFromIdx.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "start_idx", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ",", "start_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.DecodeCandidateDataset.__getitem__": [[38, 43], ["data.data.CnnDmDatasetFromIdx.__getitem__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", "=", "js_data", "[", "'article'", "]", "\n", "abs_sents", "=", "js_data", "[", "'abstract'", "]", "\n", "return", "art_sents", ",", "abs_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.coll": [[45, 57], ["enumerate", "filtered_art_batch.append", "filtered_abs_batch.append", "empty_data_indices.append"], "function", ["None"], ["", "", "def", "coll", "(", "batch", ")", ":", "\n", "    ", "filtered_art_batch", "=", "[", "]", "\n", "filtered_abs_batch", "=", "[", "]", "\n", "empty_data_indices", "=", "[", "]", "\n", "# filter out all empty articles", "\n", "for", "batch_i", ",", "(", "art_sents", ",", "abs_sents", ")", "in", "enumerate", "(", "batch", ")", ":", "\n", "        ", "if", "art_sents", ":", "# only keep non empty articles", "\n", "            ", "filtered_art_batch", ".", "append", "(", "art_sents", ")", "\n", "filtered_abs_batch", ".", "append", "(", "abs_sents", ")", "\n", "", "else", ":", "# log the empty idx", "\n", "            ", "empty_data_indices", ".", "append", "(", "batch_i", ")", "\n", "", "", "return", "filtered_art_batch", ",", "filtered_abs_batch", ",", "empty_data_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.decode": [[59, 249], ["time.time", "decode_two_to_one.DecodeCandidateDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "json.load", "print", "decoding.Abstractor", "decoding.BeamAbstractor", "list", "os.path.join", "open", "open", "json.dump", "torch.no_grad", "enumerate", "range", "os.path.join", "os.path.join", "list", "len", "print", "print", "print", "print", "print", "raw_original_article_batch.append", "map", "print", "print", "print", "len", "range", "tokenized_concated_article_batch.append", "extractor", "list", "print", "print", "decoding.BeamAbstractor.", "decode_two_to_one.rerank_mp", "decoding.BeamAbstractor.", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "ext_inds.insert", "print", "data.batcher.tokenize", "range", "map", "len", "print", "enumerate", "print", "range", "print", "print", "open", "f.write", "len", "enumerate", "len", "len", "print", "len", "os.path.join", "json.dumps", "datetime.timedelta", "concated_sents.append", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "decode", "(", "save_path", ",", "abs_dir", ",", "split", ",", "batch_size", ",", "beam_size", ",", "diverse", ",", "max_len", ",", "exist_candidates", ",", "topk", ",", "window_size", ",", "final_rerank", ",", "start_idx", ",", "cuda", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "assert", "abs_dir", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "        ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "# a dummy extractor that extract all the sentences", "\n", "", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "\n", "dataset", "=", "DecodeCandidateDataset", "(", "split", ",", "start_idx", ")", "# only need json['article'] and json['abstract']", "\n", "\n", "new_candidates", "=", "window_size", "*", "2", "*", "topk", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'{}_candidate'", ".", "format", "(", "split", ")", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "None", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "(", "raw_article_batch", ",", "raw_abs_batch", ",", "empty_data_indices", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "debug", ":", "\n", "                ", "print", "(", "\"raw article batch\"", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "print", "(", "\"article lengths\"", ")", "\n", "print", "(", "[", "len", "(", "art", ")", "for", "art", "in", "raw_article_batch", "]", ")", "\n", "# pick out the original sentence", "\n", "# raw_article_batch a list of list of sentences, article, then sentence in article", "\n", "", "raw_original_article_batch", "=", "[", "]", "\n", "for", "raw_article_sents", "in", "raw_article_batch", ":", "\n", "                ", "original_article_sents", "=", "[", "article_sent", "for", "cand_i", ",", "article_sent", "in", "enumerate", "(", "raw_article_sents", ")", "if", "cand_i", "%", "exist_candidates", "==", "0", "]", "\n", "raw_original_article_batch", ".", "append", "(", "original_article_sents", ")", "\n", "\n", "", "tokenized_original_article_batch", "=", "list", "(", "map", "(", "tokenize", "(", "None", ")", ",", "raw_original_article_batch", ")", ")", "\n", "if", "debug", ":", "\n", "                ", "print", "(", "\"tokenized_original_article_batch\"", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "\n", "", "tokenized_concated_article_batch", "=", "[", "]", "\n", "for", "tokenized_original_article_sents", "in", "tokenized_original_article_batch", ":", "\n", "                ", "concated_article_sents", "=", "[", "]", "\n", "num_original_article_sents", "=", "len", "(", "tokenized_original_article_sents", ")", "\n", "for", "j", "in", "range", "(", "num_original_article_sents", ")", ":", "# for each original sentence j", "\n", "                    ", "concated_sents", "=", "[", "]", "\n", "for", "offset", "in", "range", "(", "-", "window_size", ",", "window_size", "+", "1", ")", ":", "\n", "                        ", "if", "offset", "!=", "0", ":", "\n", "                            ", "minor_sentence_idx", "=", "j", "+", "offset", "\n", "if", "minor_sentence_idx", ">=", "num_original_article_sents", ":", "\n", "                                ", "minor_sentence_idx", "-=", "num_original_article_sents", "\n", "", "concated_sents", ".", "append", "(", "tokenized_original_article_sents", "[", "j", "]", "+", "tokenized_original_article_sents", "[", "minor_sentence_idx", "]", ")", "\n", "", "", "concated_article_sents", "+=", "concated_sents", "\n", "", "tokenized_concated_article_batch", ".", "append", "(", "concated_article_sents", ")", "\n", "\n", "", "ext_arts", "=", "[", "]", "\n", "ext_inds", "=", "[", "]", "\n", "for", "raw_art_sents", "in", "tokenized_concated_article_batch", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "len", "(", "ext_arts", ")", ",", "len", "(", "ext", ")", ")", "]", "\n", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"ext_inds\"", ")", "\n", "print", "(", "ext_inds", ")", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "topk", ",", "final_rerank", ")", "\n", "# dec_outs: a list of list of token list", "\n", "", "else", ":", "\n", "                ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "\n", "#assert i == batch_size * i_debug", "\n", "", "if", "i", "!=", "batch_size", "*", "i_debug", ":", "\n", "                ", "print", "(", "\"i: {}\"", ".", "format", "(", "i", ")", ")", "\n", "print", "(", "\"batch_size: {}, i_debug: {}, batch_size * i_debug: {}\"", ".", "format", "(", "batch_size", ",", "i_debug", ",", "batch_size", "*", "i_debug", ")", ")", "\n", "raise", "ValueError", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"dec_outs[2]\"", ")", "\n", "print", "(", "dec_outs", "[", "2", "]", ")", "\n", "print", "(", "\"dec_outs[3]\"", ")", "\n", "print", "(", "dec_outs", "[", "3", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "\"\"\"\n            if i_debug == 18:\n                print(\"Length of ext_ids: {}\".format(len(ext_inds)))\n                print(\"Length of raw_rticle_batch: {}\".format(len(raw_article_batch)))\n                print(\"Length of tokenized_article_batch: {}\".format(len(list(tokenized_article_batch))))\n                print(\"i: {}\".format(i))\n            \"\"\"", "\n", "\n", "# insert place holders for samples with empty article", "\n", "for", "empty_idx", "in", "empty_data_indices", ":", "\n", "                ", "ext_inds", ".", "insert", "(", "empty_idx", ",", "(", "None", ",", "None", ")", ")", "\n", "\n", "", "batch_i", "=", "0", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "\n", "                ", "if", "debug", ":", "\n", "                    ", "print", "(", "\"j: {}, n: {}\"", ".", "format", "(", "j", ",", "n", ")", ")", "\n", "\n", "# one article", "\n", "", "article_decoded_sents", "=", "[", "]", "# a list of all candidate sentences for one article", "\n", "\n", "if", "j", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# if the input article is not empty", "\n", "\n", "# construct a list of all candidate sentences in one article, a list of str.", "\n", "                    ", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "\n", "                        ", "candidate_list", "=", "[", "]", "\n", "if", "sent_i", "%", "new_candidates", "==", "0", ":", "# the beginning of the merging of an original sentence, add back all the original candidates", "\n", "                            ", "original_sent_idx", "=", "sent_i", "//", "new_candidates", "*", "exist_candidates", "\n", "candidate_list", "+=", "raw_article_batch", "[", "batch_i", "]", "[", "original_sent_idx", ":", "original_sent_idx", "+", "exist_candidates", "]", "\n", "\n", "# one sent", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "candidate", ")", "for", "candidate", "in", "sent", "]", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "sent", ")", "]", "\n", "\n", "#if keep_original_sent:", "\n", "#    candidate_list.insert(0, raw_article_batch[batch_i][sent_i])", "\n", "\n", "", "article_decoded_sents", "+=", "candidate_list", "\n", "# fetch the abstract of the original sample", "\n", "", "raw_abstract", "=", "raw_abs_batch", "[", "batch_i", "]", "\n", "batch_i", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raw_abstract", "=", "[", "]", "\n", "\n", "", "if", "debug", ":", "\n", "                    ", "print", "(", "\"article_decoded_sents[0]\"", ")", "\n", "for", "z", "in", "range", "(", "9", ")", ":", "\n", "                        ", "print", "(", "article_decoded_sents", "[", "z", "]", ")", "\n", "", "print", "(", "\"article_decoded_sents len\"", ")", "\n", "print", "(", "len", "(", "article_decoded_sents", ")", ")", "\n", "\n", "\n", "", "json_dict", "=", "{", "\"article\"", ":", "article_decoded_sents", ",", "\"abstract\"", ":", "raw_abstract", "}", "\n", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'{}_candidate/{}.json'", ".", "format", "(", "split", ",", "i", "+", "start_idx", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "json", ".", "dumps", "(", "json_dict", ")", ")", "\n", "", "i", "+=", "1", "\n", "\n", "\"\"\"\n                if i_debug == 18:\n                    art_len = len(json_dict['abstract'])\n                    print(\"length of current article: {}\".format(art_len))\n                    if art_len > 0:\n                        print(json_dict['abstract'][0])\n                    print(\"i increases to: {}\\n\".format(i))\n                \"\"\"", "\n", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "\"\"\"\n            if i_debug == 18:\n                raise ValueError\n            \"\"\"", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.rerank": [[257, 265], ["list", "map", "map", "cytoolz.concat", "decode_two_to_one.rereank_topk_one", "decode_two_to_one.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "final_rerank", ":", "\n", "        ", "topked", "=", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.rerank_mp": [[267, 276], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map", "decode_two_to_one.rereank_topk_one", "decode_two_to_one.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.rereank_topk_one": [[278, 296], ["map", "decode_two_to_one.rereank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rereank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one.topk_one": [[298, 306], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one._make_n_gram": [[308, 310], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one._compute_score": [[312, 316], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_other.decode": [[25, 139], ["time.time", "decoding.RLExtractor", "decoding.DecodeDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "print", "open", "json.loads", "list", "os.path.join", "open", "json.dump", "torch.no_grad", "enumerate", "os.path.join", "f.read", "filter", "os.path.join", "map", "len", "enumerate", "decoding.ConditionalAbstractor", "decoding.BeamConditionalAbstractor", "decoding.Abstractor", "decoding.BeamAbstractor", "data.batcher.tokenize", "len", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "decoding.RLExtractor.", "len", "enumerate", "decoding.BeamAbstractor.", "decoding.BeamAbstractor.", "decode_full_model_other.rerank_mp", "decoding.BeamAbstractor.", "decoding.BeamAbstractor.", "open", "f.write", "list", "i.item", "len", "len", "sequential_ext_sents[].append", "sequential_article_ids[].append", "os.path.join", "decoding.make_html_safe", "datetime.timedelta", "range", "len", "sequential_ext_sents.append", "sequential_article_ids.append", "range", "range", "int", "len", "len", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["def", "decode", "(", "save_path", ",", "model_dir", ",", "split", ",", "batch_size", ",", "\n", "beam_size", ",", "diverse", ",", "max_len", ",", "cuda", ",", "is_conditional_abs", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "with", "open", "(", "join", "(", "model_dir", ",", "'meta.json'", ")", ")", "as", "f", ":", "\n", "        ", "meta", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "if", "meta", "[", "'net_args'", "]", "[", "'abstractor'", "]", "is", "None", ":", "\n", "# NOTE: if no abstractor is provided then", "\n", "#       the whole model would be extractive summarization", "\n", "        ", "assert", "beam_size", "==", "1", "\n", "abstractor", "=", "identity", "\n", "", "elif", "is_conditional_abs", ":", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "ConditionalAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamConditionalAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "Abstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "", "extractor", "=", "RLExtractor", "(", "model_dir", ",", "cuda", "=", "cuda", ")", "\n", "\n", "# setup loaders", "\n", "def", "coll", "(", "batch", ")", ":", "\n", "        ", "articles", "=", "list", "(", "filter", "(", "bool", ",", "batch", ")", ")", "\n", "return", "articles", "\n", "", "dataset", "=", "DecodeDataset", "(", "split", ")", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output'", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "meta", "[", "'net_args'", "]", "[", "'abstractor'", "]", "\n", "dec_log", "[", "'extractor'", "]", "=", "meta", "[", "'net_args'", "]", "[", "'extractor'", "]", "\n", "dec_log", "[", "'rl'", "]", "=", "True", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "# idx for decoded article", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "raw_article_batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "tokenized_article_batch", "=", "map", "(", "tokenize", "(", "None", ")", ",", "raw_article_batch", ")", "\n", "num_articles", "=", "len", "(", "raw_article_batch", ")", "\n", "num_ext_sents", "=", "0", "\n", "if", "is_conditional_abs", ":", "\n", "                ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "ext_arts", "=", "[", "]", "\n", "", "ext_inds", "=", "[", "]", "\n", "for", "article_i", ",", "raw_art_sents", "in", "enumerate", "(", "tokenized_article_batch", ")", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "[", ":", "-", "1", "]", "# exclude EOE", "\n", "if", "not", "ext", ":", "\n", "# use top-5 if nothing is extracted", "\n", "# in some rare cases rnn-ext does not extract at all", "\n", "                    ", "ext", "=", "list", "(", "range", "(", "5", ")", ")", "[", ":", "len", "(", "raw_art_sents", ")", "]", "\n", "", "else", ":", "\n", "                    ", "ext", "=", "[", "i", ".", "item", "(", ")", "for", "i", "in", "ext", "]", "\n", "", "ext_inds", "+=", "[", "(", "num_ext_sents", ",", "len", "(", "ext", ")", ")", "]", "\n", "num_ext_sents", "+=", "len", "(", "ext", ")", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "                    ", "num_selected_sents_excluded_eos", "=", "len", "(", "ext", ")", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                        ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "idx_i", ",", "idx", "in", "enumerate", "(", "ext", ")", ":", "\n", "                        ", "sequential_ext_sents", "[", "idx_i", "]", ".", "append", "(", "raw_art_sents", "[", "idx", "]", ")", "\n", "sequential_article_ids", "[", "idx_i", "]", ".", "append", "(", "article_i", ")", "\n", "", "", "else", ":", "\n", "                    ", "ext_arts", "+=", "[", "raw_art_sents", "[", "i", "]", "for", "i", "in", "ext", "]", "\n", "\n", "", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "if", "is_conditional_abs", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ",", "beam_size", ",", "diverse", ")", "\n", "", "else", ":", "\n", "                    ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ")", "\n", "#dec_outs = rerank(all_beams, ext_inds)", "\n", "", "", "else", ":", "\n", "                ", "if", "is_conditional_abs", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "", "", "assert", "i", "==", "batch_size", "*", "i_debug", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "                ", "decoded_sents", "=", "[", "' '", ".", "join", "(", "dec", ")", "for", "dec", "in", "dec_outs", "[", "j", ":", "j", "+", "n", "]", "]", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'output/{}.dec'", ".", "format", "(", "i", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "decoded_sents", ")", ")", ")", "\n", "", "i", "+=", "1", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_other.rerank": [[145, 148], ["list", "cytoolz.concat", "map"], "function", ["None"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "return", "list", "(", "concat", "(", "map", "(", "rerank_one", ",", "beam_lists", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_other.rerank_mp": [[149, 155], ["list", "torch.multiprocessing.Pool", "pool.map", "cytoolz.concat"], "function", ["None"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "reranked", "=", "pool", ".", "map", "(", "rerank_one", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "reranked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_other.rerank_one": [[156, 166], ["map", "max", "decode_full_model_other.rerank_one.process_beam"], "function", ["None"], ["", "def", "rerank_one", "(", "beams", ")", ":", "\n", "    ", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "best_hyps", "=", "max", "(", "product", "(", "*", "beams", ")", ",", "key", "=", "_compute_score", ")", "# a tuple", "\n", "dec_outs", "=", "[", "h", ".", "sequence", "for", "h", "in", "best_hyps", "]", "\n", "return", "dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_other._make_n_gram": [[167, 169], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model_other._compute_score": [[170, 175], ["functools.reduce", "sum", "collections.Counter", "sum", "sum", "functools.reduce.items", "len"], "function", ["None"], ["", "def", "_compute_score", "(", "hyps", ")", ":", "\n", "    ", "all_cnt", "=", "reduce", "(", "op", ".", "iadd", ",", "(", "h", ".", "gram_cnt", "for", "h", "in", "hyps", ")", ",", "Counter", "(", ")", ")", "\n", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "all_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "sum", "(", "h", ".", "logprob", "for", "h", "in", "hyps", ")", "/", "sum", "(", "len", "(", "h", ".", "sequence", ")", "for", "h", "in", "hyps", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.matched_labels._count_data": [[15, 22], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["", "def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.matched_labels.main": [[24, 43], ["os.path.join", "matched_labels._count_data", "range", "print", "json.load", "len", "enumerate", "open", "open", "f.readlines", "[].strip", "os.path.join", "os.path.join", "len", "dec_lines[].strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "data_path", "=", "join", "(", "DATA_DIR", ",", "args", ".", "split", ")", "\n", "n_data", "=", "_count_data", "(", "data_path", ")", "\n", "num_match", "=", "0", "\n", "num_ref_sents", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "js_obj", "=", "json", ".", "load", "(", "open", "(", "join", "(", "data_path", ",", "\"{}.json\"", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "dec_folder", ",", "\"output/{}.dec\"", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "dec_lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "num_ref_sents", "+=", "len", "(", "js_obj", "[", "'extracted'", "]", ")", "\n", "for", "sent_i", ",", "ext_label", "in", "enumerate", "(", "js_obj", "[", "'extracted'", "]", ")", ":", "\n", "            ", "ref_sent", "=", "js_obj", "[", "'article'", "]", "[", "ext_label", "]", ".", "strip", "(", ")", "\n", "if", "sent_i", "<", "len", "(", "dec_lines", ")", ":", "\n", "                ", "dec_sent", "=", "dec_lines", "[", "sent_i", "]", ".", "strip", "(", ")", "\n", "if", "ref_sent", "==", "dec_sent", ":", "\n", "                    ", "num_match", "+=", "1", "\n", "\n", "", "", "", "", "print", "(", "\"percentage of matched: {:.3f}\"", ".", "format", "(", "num_match", "/", "num_ref_sents", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data._split_words": [[24, 26], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.check_empty": [[28, 33], ["len"], "function", ["None"], ["", "def", "check_empty", "(", "sents", ")", ":", "\n", "    ", "if", "len", "(", "sents", ")", "==", "1", "and", "sents", "[", "0", "]", "==", "\"\"", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.process": [[35, 49], ["os.path.join", "open", "json.loads", "sent.strip.strip", "open", "json.dump", "os.path.join", "f.read", "art_sents_preprocessed.append", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "", "@", "curry", "\n", "def", "process", "(", "split", ",", "i", ")", ":", "\n", "    ", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "art_sents", "=", "data", "[", "'article'", "]", "\n", "art_sents_preprocessed", "=", "[", "]", "\n", "for", "sent", "in", "art_sents", ":", "\n", "        ", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "if", "sent", "!=", "\"\"", ":", "\n", "            ", "art_sents_preprocessed", ".", "append", "(", "sent", ")", "\n", "", "", "data", "[", "'article'", "]", "=", "art_sents_preprocessed", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.label_mp": [[51, 61], ["time.time", "print", "os.path.join", "utils.count_data", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "clean_empty_sentence_pubmed_training_data.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "label_mp", "(", "split", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "split", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.label": [[63, 72], ["time.time", "print", "os.path.join", "utils.count_data", "range", "print", "clean_empty_sentence_pubmed_training_data.process", "datetime.timedelta", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "def", "label", "(", "split", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "process", "(", "split", ",", "i", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.clean_empty_sentence_pubmed_training_data.main": [[74, 82], ["clean_empty_sentence_pubmed_training_data.label", "clean_empty_sentence_pubmed_training_data.label"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label"], ["", "def", "main", "(", "split", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'train'", ",", "'test'", "]", ":", "\n", "#label_mp(split)", "\n", "            ", "label", "(", "split", ")", "\n", "", "", "else", ":", "\n", "#label_mp(split)", "\n", "        ", "label", "(", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.build_vocab._count_data": [[10, 17], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.build_vocab.main": [[19, 41], ["os.path.join", "build_vocab._count_data", "collections.Counter", "range", "print", "json.load", "summary_text.strip().split", "document_text.strip().split", "collections.Counter.update", "open", "pickle.dump", "open", "os.path.join", "os.path.join", "summary_text.strip", "document_text.strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "data_dir", ")", ":", "\n", "    ", "split_dir", "=", "join", "(", "data_dir", ",", "\"train\"", ")", "\n", "n_data", "=", "_count_data", "(", "split_dir", ")", "\n", "vocab_counter", "=", "Counter", "(", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "js", "=", "json", ".", "load", "(", "open", "(", "join", "(", "split_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "\n", "summary", "=", "js", "[", "'abstract'", "]", "\n", "summary_text", "=", "' '", ".", "join", "(", "summary", ")", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "summary_word_list", "=", "summary_text", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "\n", "document", "=", "js", "[", "'article'", "]", "\n", "document_text", "=", "' '", ".", "join", "(", "document", ")", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "document_word_list", "=", "document_text", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "\n", "all_tokens", "=", "summary_word_list", "+", "document_word_list", "\n", "vocab_counter", ".", "update", "(", "[", "t", "for", "t", "in", "all_tokens", "if", "t", "!=", "\"\"", "]", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"vocab_cnt.pkl\"", ")", ",", "\n", "'wb'", ")", "as", "vocab_file", ":", "\n", "        ", "pkl", ".", "dump", "(", "vocab_counter", ",", "vocab_file", ",", "protocol", "=", "4", ")", "\n", "", "print", "(", "\"Finished!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.compress_stat.main": [[16, 101], ["os.path.join", "numpy.load", "numpy.load", "numpy.load", "numpy.load", "numpy.load", "numpy.load", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "numpy.histogram", "matplotlib.pyplot.figure", "matplotlib.pyplot.hist", "matplotlib.pyplot.title", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.savefig", "matplotlib.pyplot.savefig", "print", "print", "print", "numpy.histogram", "matplotlib.pyplot.figure", "matplotlib.pyplot.hist", "matplotlib.pyplot.title", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.savefig", "matplotlib.pyplot.savefig", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "numpy.histogram", "print", "print", "print", "numpy.histogram", "print", "print", "print", "print", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "numpy.ones_like", "os.path.join", "os.path.join", "numpy.ones_like", "os.path.join", "os.path.join", "max", "np.load.mean", "min", "max", "np.load.mean", "min", "max", "np.load.mean", "min", "max", "np.load.mean", "min", "max", "min", "sum"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "data_path", "=", "join", "(", "DATA_DIR", ",", "args", ".", "split", ")", "\n", "compress_ratios", "=", "np", ".", "load", "(", "join", "(", "data_path", ",", "\"compress_ratios.dat\"", ")", ")", "\n", "# less train/compress_ratios.dat", "\n", "\n", "compress_sents_lens", "=", "np", ".", "load", "(", "join", "(", "data_path", ",", "\"compress_sents_lens.dat\"", ")", ")", "\n", "original_sents_lens", "=", "np", ".", "load", "(", "join", "(", "data_path", ",", "\"original_sents_lens.dat\"", ")", ")", "\n", "ext_label_rouge_l_scores", "=", "np", ".", "load", "(", "join", "(", "data_path", ",", "\"ext_label_rouge_l_scores.dat\"", ")", ")", "\n", "distances_one_to_many", "=", "np", ".", "load", "(", "join", "(", "data_path", ",", "\"distances_two_to_one_{}.dat\"", ".", "format", "(", "args", ".", "threshold", ")", ")", ")", "\n", "improvements", "=", "np", ".", "load", "(", "join", "(", "data_path", ",", "\"improvements.dat\"", ")", ")", "\n", "\n", "#print(compress_ratios.shape)", "\n", "#print(original_sents_lens.shape)", "\n", "#print(compress_sents_lens.shape)", "\n", "#print(ext_label_rouge_l_scores.shape)", "\n", "num_ext_label", "=", "compress_ratios", ".", "shape", "[", "0", "]", "\n", "\n", "print", "(", "\"max compression lengths:\\t{}\"", ".", "format", "(", "max", "(", "compress_sents_lens", ")", ")", ")", "\n", "print", "(", "\"mean compression lengths:\\t{:.3f}\"", ".", "format", "(", "(", "compress_sents_lens", ".", "mean", "(", ")", ")", ")", ")", "\n", "print", "(", "\"min compression lengths:\\t{}\"", ".", "format", "(", "min", "(", "compress_sents_lens", ")", ")", ")", "\n", "print", "(", "\"max original lengths:\\t{}\"", ".", "format", "(", "max", "(", "original_sents_lens", ")", ")", ")", "\n", "print", "(", "\"mean original lengths:\\t{:.3f}\"", ".", "format", "(", "(", "original_sents_lens", ".", "mean", "(", ")", ")", ")", ")", "\n", "print", "(", "\"min original lengths:\\t{}\"", ".", "format", "(", "min", "(", "original_sents_lens", ")", ")", ")", "\n", "print", "(", "\"max compression ratio:\\t{:.3f}\"", ".", "format", "(", "max", "(", "compress_ratios", ")", ")", ")", "\n", "print", "(", "\"mean compression ratio:\\t{:.3f}\"", ".", "format", "(", "(", "compress_ratios", ".", "mean", "(", ")", ")", ")", ")", "\n", "print", "(", "\"min compression ratio:\\t{:.3f}\"", ".", "format", "(", "min", "(", "compress_ratios", ")", ")", ")", "\n", "print", "(", "\"max Rouge-L score:\\t{}\"", ".", "format", "(", "max", "(", "ext_label_rouge_l_scores", ")", ")", ")", "\n", "print", "(", "\"mean Rouge-L score:\\t{:.3f}\"", ".", "format", "(", "(", "ext_label_rouge_l_scores", ".", "mean", "(", ")", ")", ")", ")", "\n", "print", "(", "\"min Rouge-L score:\\t{}\"", ".", "format", "(", "min", "(", "ext_label_rouge_l_scores", ")", ")", ")", "\n", "\n", "print", "(", "\"histogram of compression ratio:\"", ")", "\n", "# [0, 0.2, 0.4, 0.6, 0.8, 1]", "\n", "compress_ratios_weights", "=", "np", ".", "ones_like", "(", "compress_ratios", ")", "/", "compress_ratios", ".", "shape", "[", "0", "]", "\n", "#hist, bins = np.histogram(compress_ratios, bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], density=False)", "\n", "hist", ",", "bins", "=", "np", ".", "histogram", "(", "compress_ratios", ",", "bins", "=", "[", "0", ",", "0.4", ",", "0.65", ",", "1", "]", ",", "density", "=", "False", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "hist", "(", "compress_ratios", ",", "bins", "=", "[", "0", ",", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", ",", "0.6", ",", "0.7", ",", "0.8", ",", "0.9", ",", "1", "]", ",", "density", "=", "False", ",", "weights", "=", "compress_ratios_weights", ")", "\n", "plt", ".", "title", "(", "\"Histogram of compression ratio in NYT\"", ")", "\n", "plt", ".", "xlabel", "(", "'Compression ratio'", ")", "\n", "plt", ".", "ylabel", "(", "'Proportion of samples'", ")", "\n", "#plt.show()", "\n", "plt", ".", "savefig", "(", "join", "(", "data_path", ",", "\"compress_ratio_hist.pdf\"", ")", ")", "\n", "plt", ".", "savefig", "(", "join", "(", "data_path", ",", "\"compress_ratio_hist.png\"", ")", ")", "\n", "print", "(", "hist", ")", "\n", "print", "(", "bins", ")", "\n", "#print(sum(hist))", "\n", "\n", "print", "(", "\"histogram of Rouge-L scores of extraction label:\"", ")", "\n", "ext_label_rouge_l_scores_weights", "=", "np", ".", "ones_like", "(", "ext_label_rouge_l_scores", ")", "/", "ext_label_rouge_l_scores", ".", "shape", "[", "0", "]", "\n", "hist", ",", "bins", "=", "np", ".", "histogram", "(", "ext_label_rouge_l_scores", ",", "bins", "=", "[", "0", ",", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", ",", "0.6", ",", "0.7", ",", "0.8", ",", "0.9", ",", "1", "]", ",", "density", "=", "False", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "hist", "(", "ext_label_rouge_l_scores", ",", "bins", "=", "[", "0", ",", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", ",", "0.6", ",", "0.7", ",", "0.8", ",", "0.9", ",", "1", "]", ",", "density", "=", "False", ",", "weights", "=", "ext_label_rouge_l_scores_weights", ")", "\n", "plt", ".", "title", "(", "\"Histogram of Rouge-l scores of extraction label in CNN/DM\"", ")", "\n", "plt", ".", "xlabel", "(", "'Rouge-l score'", ")", "\n", "plt", ".", "ylabel", "(", "'Proportion of samples'", ")", "\n", "#plt.show()", "\n", "plt", ".", "savefig", "(", "join", "(", "data_path", ",", "\"ext_label_rouge_l.pdf\"", ")", ")", "\n", "plt", ".", "savefig", "(", "join", "(", "data_path", ",", "\"ext_label_rouge_l.png\"", ")", ")", "\n", "print", "(", "hist", ")", "\n", "print", "(", "bins", ")", "\n", "#print(sum(hist))", "\n", "\n", "print", "(", "\"Ext label with Rouge_l less than 0.1:\\t{:.3f}\"", ".", "format", "(", "hist", "[", "0", "]", "/", "num_ext_label", ")", ")", "\n", "print", "(", "\"Ext label with Rouge_l less than 0.2:\\t{:.3f}\"", ".", "format", "(", "(", "hist", "[", "0", "]", "+", "hist", "[", "1", "]", ")", "/", "num_ext_label", ")", ")", "\n", "print", "(", "\"Ext label with Rouge_l less than 0.3:\\t{:.3f}\"", ".", "format", "(", "(", "hist", "[", "0", "]", "+", "hist", "[", "1", "]", "+", "hist", "[", "2", "]", ")", "/", "num_ext_label", ")", ")", "\n", "\n", "print", "(", "\"number of two to one extraciton labels:\\t{}\"", ".", "format", "(", "distances_one_to_many", ".", "shape", "[", "0", "]", ")", ")", "\n", "print", "(", "\"number of summary sents:\\t{}\"", ".", "format", "(", "num_ext_label", ")", ")", "\n", "print", "(", "\"max two to one distance:\\t{}\"", ".", "format", "(", "max", "(", "distances_one_to_many", ")", ")", ")", "\n", "print", "(", "\"average two to one distance:\\t{:.3f}\"", ".", "format", "(", "sum", "(", "distances_one_to_many", ")", "/", "distances_one_to_many", ".", "shape", "[", "0", "]", ")", ")", "\n", "print", "(", "\"min two to one distance:\\t{}\"", ".", "format", "(", "min", "(", "distances_one_to_many", ")", ")", ")", "\n", "print", "(", "\"histogram of two to one distance:\"", ")", "\n", "hist", ",", "bins", "=", "np", ".", "histogram", "(", "distances_one_to_many", ",", "bins", "=", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "9", ",", "10", ",", "20", "]", ",", "\n", "density", "=", "False", ")", "\n", "print", "(", "hist", ")", "\n", "print", "(", "bins", ")", "\n", "\n", "print", "(", "\"hist. of improvements\"", ")", "\n", "hist", ",", "bins", "=", "np", ".", "histogram", "(", "improvements", ",", "bins", "=", "[", "0", ",", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", ",", "0.6", ",", "0.7", ",", "0.8", ",", "0.9", ",", "1", "]", ",", "density", "=", "False", ")", "\n", "print", "(", "hist", ")", "\n", "print", "(", "bins", ")", "\n", "\n", "print", "(", "hist", "[", "0", "]", "/", "num_ext_label", ")", "\n", "print", "(", "(", "hist", "[", "0", "]", "+", "hist", "[", "1", "]", ")", "/", "num_ext_label", ")", "\n", "print", "(", "(", "hist", "[", "0", "]", "+", "hist", "[", "1", "]", "+", "hist", "[", "2", "]", ")", "/", "num_ext_label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_duc.sorted_nicely": [[13, 23], ["sorted", "text.isdigit", "int", "convert", "re.split"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert"], ["def", "sorted_nicely", "(", "l", ")", ":", "\n", "    ", "\"\"\" From https://arcpy.wordpress.com/2012/05/11/sorting-alphanumeric-strings-in-python/\n    Sorts the given iterable in the way that is expected.\n    Required arguments:\n    l -- The iterable to be sorted.\n\n    \"\"\"", "\n", "convert", "=", "lambda", "text", ":", "int", "(", "text", ")", "if", "text", ".", "isdigit", "(", ")", "else", "text", "\n", "alphanum_key", "=", "lambda", "key", ":", "[", "convert", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "key", ")", "]", "\n", "return", "sorted", "(", "l", ",", "key", "=", "alphanum_key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_duc.main": [[25, 70], ["os.makedirs", "os.path.join", "os.makedirs", "os.path.join", "os.path.join", "os.listdir", "preprocess_duc.sorted_nicely", "os.listdir", "print", "print", "re.compile", "os.listdir", "preprocess_duc.sorted_nicely", "collections.defaultdict", "os.path.join", "os.path.exists", "bs4.BeautifulSoup", "bs4.BeautifulSoup.find().find_all", "re.match", "os.path.join", "bs4.BeautifulSoup", "bs4.BeautifulSoup.find_all", "open", "f.read", "s.text.strip", "open", "json.dump", "open", "f.read", "doc_set_summary_dict[].append", "os.path.join", "bs4.BeautifulSoup.find", "os.path.join", "os.path.join", "nltk.tokenize.sent_tokenize", "doc_summary.text.strip().replace", "doc_summary.text.strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_duc.sorted_nicely", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_duc.sorted_nicely", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "data_dir", ",", "out_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "out_test_dir", "=", "join", "(", "out_dir", ",", "'test'", ")", "\n", "os", ".", "makedirs", "(", "out_test_dir", ")", "\n", "summary_dir", "=", "join", "(", "data_dir", ",", "\"summaries\"", ")", "\n", "doc_dir", "=", "join", "(", "data_dir", ",", "\"docs.with.sentence.breaks\"", ")", "\n", "doc_set_id_list", "=", "os", ".", "listdir", "(", "doc_dir", ")", "\n", "doc_set_id_list", "=", "sorted_nicely", "(", "doc_set_id_list", ")", "\n", "ref_doc_set_id_list", "=", "os", ".", "listdir", "(", "summary_dir", ")", "\n", "num_doc", "=", "0", "\n", "num_ref", "=", "0", "\n", "for", "doc_set_id", "in", "doc_set_id_list", ":", "\n", "        ", "regex", "=", "re", ".", "compile", "(", "'{}.'", ".", "format", "(", "doc_set_id", ")", ")", "\n", "ref_dirs", "=", "[", "ref_dir", "for", "ref_dir", "in", "ref_doc_set_id_list", "if", "re", ".", "match", "(", "regex", ",", "ref_dir", ")", "]", "\n", "doc_id_list", "=", "os", ".", "listdir", "(", "join", "(", "doc_dir", ",", "doc_set_id", ")", ")", "\n", "doc_id_list", "=", "sorted_nicely", "(", "doc_id_list", ")", "\n", "doc_set_summary_dict", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "ref_dir", "in", "ref_dirs", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "join", "(", "summary_dir", ",", "ref_dir", ",", "'perdocs'", ")", ")", ":", "\n", "                ", "with", "open", "(", "join", "(", "summary_dir", ",", "ref_dir", ",", "'perdocs'", ")", ")", "as", "f", ":", "\n", "                    ", "ref_html_txt", "=", "f", ".", "read", "(", ")", "\n", "", "ref_soup", "=", "BeautifulSoup", "(", "ref_html_txt", ",", "'lxml'", ")", "\n", "all_doc_summary_list", "=", "ref_soup", ".", "find_all", "(", "'sum'", ")", "\n", "\n", "for", "doc_summary", "in", "all_doc_summary_list", ":", "\n", "                    ", "num_ref", "+=", "1", "\n", "doc_set_summary_dict", "[", "doc_summary", ".", "attrs", "[", "'docref'", "]", "]", ".", "append", "(", "nltk", ".", "tokenize", ".", "sent_tokenize", "(", "doc_summary", ".", "text", ".", "strip", "(", ")", ".", "replace", "(", "'\\n'", ",", "' '", ")", ")", ")", "\n", "\n", "", "", "", "for", "doc_id", "in", "doc_id_list", ":", "\n", "# doc_id with \".s\"", "\n", "            ", "with", "open", "(", "join", "(", "doc_dir", ",", "doc_set_id", ",", "doc_id", ")", ")", "as", "f", ":", "\n", "                ", "html_txt", "=", "f", ".", "read", "(", ")", "\n", "", "doc_soup", "=", "BeautifulSoup", "(", "html_txt", ",", "'lxml'", ")", "\n", "raw_s_list", "=", "doc_soup", ".", "find", "(", "'text'", ")", ".", "find_all", "(", "'s'", ")", "\n", "doc_sentence_list", "=", "[", "s", ".", "text", ".", "strip", "(", ")", "for", "s", "in", "raw_s_list", "]", "\n", "doc_summary_list", "=", "doc_set_summary_dict", "[", "doc_id", "[", ":", "-", "2", "]", "]", "\n", "\n", "json_out", "=", "{", "\"article\"", ":", "doc_sentence_list", ",", "\"abstract\"", ":", "doc_summary_list", ",", "\"doc_ref\"", ":", "doc_id", "[", ":", "-", "2", "]", ",", "\"doc_set\"", ":", "doc_set_id", "}", "\n", "with", "open", "(", "join", "(", "out_test_dir", ",", "'{}.json'", ".", "format", "(", "num_doc", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "json_out", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "num_doc", "+=", "1", "\n", "\n", "", "", "print", "(", "\"Total no. of doc: {}\"", ".", "format", "(", "num_doc", ")", ")", "\n", "print", "(", "\"Total no. of references: {}\"", ".", "format", "(", "num_ref", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.Meteor.__init__": [[142, 151], ["subprocess.Popen", "threading.Lock", "cmd.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "assert", "_METEOR_PATH", "is", "not", "None", "\n", "cmd", "=", "'java -Xmx2G -jar {} - - -l en -norm -stdio'", ".", "format", "(", "_METEOR_PATH", ")", "\n", "self", ".", "_meteor_proc", "=", "sp", ".", "Popen", "(", "\n", "cmd", ".", "split", "(", ")", ",", "\n", "stdin", "=", "sp", ".", "PIPE", ",", "stdout", "=", "sp", ".", "PIPE", ",", "stderr", "=", "sp", ".", "PIPE", ",", "\n", "universal_newlines", "=", "True", ",", "encoding", "=", "'utf-8'", ",", "bufsize", "=", "1", "\n", ")", "\n", "self", ".", "_lock", "=", "threading", ".", "Lock", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.Meteor.__call__": [[152, 163], ["metric.Meteor._lock.acquire", "metric.Meteor._meteor_proc.stdin.write", "metric.Meteor._meteor_proc.stdout.readline().strip", "metric.Meteor._meteor_proc.stdin.write", "float", "metric.Meteor._lock.release", "metric.Meteor._meteor_proc.stdout.readline().strip", "metric.Meteor._meteor_proc.stdout.readline", "metric.Meteor._meteor_proc.stdout.readline"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "summ", ",", "ref", ")", ":", "\n", "        ", "self", ".", "_lock", ".", "acquire", "(", ")", "\n", "score_line", "=", "'SCORE ||| {} ||| {}\\n'", ".", "format", "(", "\n", "' '", ".", "join", "(", "ref", ")", ",", "' '", ".", "join", "(", "summ", ")", ")", "\n", "self", ".", "_meteor_proc", ".", "stdin", ".", "write", "(", "score_line", ")", "\n", "stats", "=", "self", ".", "_meteor_proc", ".", "stdout", ".", "readline", "(", ")", ".", "strip", "(", ")", "\n", "eval_line", "=", "'EVAL ||| {}\\n'", ".", "format", "(", "stats", ")", "\n", "self", ".", "_meteor_proc", ".", "stdin", ".", "write", "(", "eval_line", ")", "\n", "score", "=", "float", "(", "self", ".", "_meteor_proc", ".", "stdout", ".", "readline", "(", ")", ".", "strip", "(", ")", ")", "\n", "self", ".", "_lock", ".", "release", "(", ")", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.Meteor.__del__": [[164, 170], ["metric.Meteor._lock.acquire", "metric.Meteor._meteor_proc.stdin.close", "metric.Meteor._meteor_proc.kill", "metric.Meteor._meteor_proc.wait", "metric.Meteor._lock.release"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_lock", ".", "acquire", "(", ")", "\n", "self", ".", "_meteor_proc", ".", "stdin", ".", "close", "(", ")", "\n", "self", ".", "_meteor_proc", ".", "kill", "(", ")", "\n", "self", ".", "_meteor_proc", ".", "wait", "(", ")", "\n", "self", ".", "_lock", ".", "release", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.make_n_grams": [[10, 14], ["tuple", "range", "len"], "function", ["None"], ["def", "make_n_grams", "(", "seq", ",", "n", ")", ":", "\n", "    ", "\"\"\" return iterator \"\"\"", "\n", "ngrams", "=", "(", "tuple", "(", "seq", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "n", "+", "1", ")", ")", "\n", "return", "ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric._n_gram_match": [[15, 21], ["collections.Counter", "collections.Counter", "min", "sum", "metric.make_n_grams", "metric.make_n_grams", "min"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.make_n_grams", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.make_n_grams"], ["", "def", "_n_gram_match", "(", "summ", ",", "ref", ",", "n", ")", ":", "\n", "    ", "summ_grams", "=", "Counter", "(", "make_n_grams", "(", "summ", ",", "n", ")", ")", "\n", "ref_grams", "=", "Counter", "(", "make_n_grams", "(", "ref", ",", "n", ")", ")", "\n", "grams", "=", "min", "(", "summ_grams", ",", "ref_grams", ",", "key", "=", "len", ")", "\n", "count", "=", "sum", "(", "min", "(", "summ_grams", "[", "g", "]", ",", "ref_grams", "[", "g", "]", ")", "for", "g", "in", "grams", ")", "\n", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_weighted_rouge_1_2": [[22, 27], ["metric.compute_rouge_n", "metric.compute_rouge_n"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n"], ["", "@", "curry", "\n", "def", "compute_weighted_rouge_1_2", "(", "output", ",", "reference", ",", "rouge_1_weight", "=", "0.5", ",", "mode", "=", "'f'", ")", ":", "\n", "    ", "rouge_1", "=", "compute_rouge_n", "(", "output", ",", "reference", ",", "n", "=", "1", ",", "mode", "=", "mode", ")", "\n", "rouge_2", "=", "compute_rouge_n", "(", "output", ",", "reference", ",", "n", "=", "2", ",", "mode", "=", "mode", ")", "\n", "return", "rouge_1_weight", "*", "rouge_1", "+", "(", "1", "-", "rouge_1_weight", ")", "*", "rouge_2", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_n": [[28, 46], ["metric._n_gram_match", "list", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric._n_gram_match"], ["", "@", "curry", "\n", "def", "compute_rouge_n", "(", "output", ",", "reference", ",", "n", "=", "1", ",", "mode", "=", "'f'", ")", ":", "\n", "    ", "\"\"\" compute ROUGE-N for a single pair of summary and reference\"\"\"", "\n", "assert", "mode", "in", "list", "(", "'fpr'", ")", "# F-1, precision, recall", "\n", "match", "=", "_n_gram_match", "(", "reference", ",", "output", ",", "n", ")", "\n", "if", "match", "==", "0", ":", "\n", "        ", "score", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "precision", "=", "match", "/", "len", "(", "output", ")", "\n", "recall", "=", "match", "/", "len", "(", "reference", ")", "\n", "f_score", "=", "2", "*", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "if", "mode", "==", "'p'", ":", "\n", "            ", "score", "=", "precision", "\n", "", "elif", "mode", "==", "'r'", ":", "\n", "            ", "score", "=", "recall", "\n", "", "else", ":", "\n", "            ", "score", "=", "f_score", "\n", "", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric._lcs_dp": [[48, 60], ["range", "range", "range", "len", "range", "len", "max", "len", "len"], "function", ["None"], ["", "def", "_lcs_dp", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the len dp of lcs\"\"\"", "\n", "dp", "=", "[", "[", "0", "for", "_", "in", "range", "(", "0", ",", "len", "(", "b", ")", "+", "1", ")", "]", "\n", "for", "_", "in", "range", "(", "0", ",", "len", "(", "a", ")", "+", "1", ")", "]", "\n", "# dp[i][j]: lcs_len(a[:i], b[:j])", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "a", ")", "+", "1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "b", ")", "+", "1", ")", ":", "\n", "            ", "if", "a", "[", "i", "-", "1", "]", "==", "b", "[", "j", "-", "1", "]", ":", "\n", "                ", "dp", "[", "i", "]", "[", "j", "]", "=", "dp", "[", "i", "-", "1", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "dp", "[", "i", "]", "[", "j", "]", "=", "max", "(", "dp", "[", "i", "-", "1", "]", "[", "j", "]", ",", "dp", "[", "i", "]", "[", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "dp", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric._lcs_len": [[61, 65], ["metric._lcs_dp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp"], ["", "def", "_lcs_len", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the length of longest common subsequence between a and b\"\"\"", "\n", "dp", "=", "_lcs_dp", "(", "a", ",", "b", ")", "\n", "return", "dp", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l": [[66, 86], ["metric._lcs_len", "list", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_len"], ["", "@", "curry", "\n", "def", "compute_rouge_l", "(", "output", ",", "reference", ",", "mode", "=", "'f'", ")", ":", "\n", "    ", "\"\"\" compute ROUGE-L for a single pair of summary and reference\n    output, reference are list of words\n    \"\"\"", "\n", "assert", "mode", "in", "list", "(", "'fpr'", ")", "# F-1, precision, recall", "\n", "lcs", "=", "_lcs_len", "(", "output", ",", "reference", ")", "\n", "if", "lcs", "==", "0", ":", "\n", "        ", "score", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "precision", "=", "lcs", "/", "len", "(", "output", ")", "\n", "recall", "=", "lcs", "/", "len", "(", "reference", ")", "\n", "f_score", "=", "2", "*", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "if", "mode", "==", "'p'", ":", "\n", "            ", "score", "=", "precision", "\n", "", "elif", "mode", "==", "'r'", ":", "\n", "            ", "score", "=", "recall", "\n", "", "else", ":", "\n", "            ", "score", "=", "f_score", "\n", "", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric._lcs": [[88, 105], ["metric._lcs_dp", "len", "len", "collections.deque", "len", "collections.deque.appendleft"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp"], ["", "def", "_lcs", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the longest common subsequence between a and b\"\"\"", "\n", "dp", "=", "_lcs_dp", "(", "a", ",", "b", ")", "\n", "i", "=", "len", "(", "a", ")", "\n", "j", "=", "len", "(", "b", ")", "\n", "lcs", "=", "deque", "(", ")", "\n", "while", "(", "i", ">", "0", "and", "j", ">", "0", ")", ":", "\n", "        ", "if", "a", "[", "i", "-", "1", "]", "==", "b", "[", "j", "-", "1", "]", ":", "\n", "            ", "lcs", ".", "appendleft", "(", "a", "[", "i", "-", "1", "]", ")", "\n", "i", "-=", "1", "\n", "j", "-=", "1", "\n", "", "elif", "dp", "[", "i", "-", "1", "]", "[", "j", "]", ">=", "dp", "[", "i", "]", "[", "j", "-", "1", "]", ":", "\n", "            ", "i", "-=", "1", "\n", "", "else", ":", "\n", "            ", "j", "-=", "1", "\n", "", "", "assert", "len", "(", "lcs", ")", "==", "dp", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "return", "lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l_summ": [[106, 134], ["collections.Counter", "collections.Counter", "list", "cytoolz.concat", "cytoolz.concat", "metric._lcs", "sum", "sum", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs"], ["", "@", "curry", "\n", "def", "compute_rouge_l_summ", "(", "summs", ",", "refs", ",", "mode", "=", "'f'", ")", ":", "\n", "    ", "\"\"\" summary level ROUGE-L\"\"\"", "\n", "assert", "mode", "in", "list", "(", "'fpr'", ")", "# F-1, precision, recall", "\n", "tot_hit", "=", "0", "\n", "ref_cnt", "=", "Counter", "(", "concat", "(", "refs", ")", ")", "\n", "summ_cnt", "=", "Counter", "(", "concat", "(", "summs", ")", ")", "\n", "for", "ref", "in", "refs", ":", "\n", "        ", "for", "summ", "in", "summs", ":", "\n", "            ", "lcs", "=", "_lcs", "(", "summ", ",", "ref", ")", "\n", "for", "gram", "in", "lcs", ":", "\n", "                ", "if", "ref_cnt", "[", "gram", "]", ">", "0", "and", "summ_cnt", "[", "gram", "]", ">", "0", ":", "\n", "                    ", "tot_hit", "+=", "1", "\n", "", "ref_cnt", "[", "gram", "]", "-=", "1", "\n", "summ_cnt", "[", "gram", "]", "-=", "1", "\n", "", "", "", "if", "tot_hit", "==", "0", ":", "\n", "        ", "score", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "precision", "=", "tot_hit", "/", "sum", "(", "(", "len", "(", "s", ")", "for", "s", "in", "summs", ")", ")", "\n", "recall", "=", "tot_hit", "/", "sum", "(", "(", "len", "(", "r", ")", "for", "r", "in", "refs", ")", ")", "\n", "f_score", "=", "2", "*", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "if", "mode", "==", "'p'", ":", "\n", "            ", "score", "=", "precision", "\n", "", "elif", "mode", "==", "'r'", ":", "\n", "            ", "score", "=", "recall", "\n", "", "else", ":", "\n", "            ", "score", "=", "f_score", "\n", "", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_full_model_duc.main": [[16, 37], ["os.path.join", "os.path.join", "os.path.exists", "print", "open", "evaluate.eval_rouge", "evaluate.eval_meteor", "open", "f.write", "os.path.join", "json.loads", "os.path.join", "f.read"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_rouge", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_meteor"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "dec_dir", "=", "join", "(", "args", ".", "decode_dir", ",", "'output'", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'log.json'", ")", ")", "as", "f", ":", "\n", "        ", "split", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "[", "'split'", "]", "\n", "", "ref_dir", "=", "join", "(", "_DATA_DIR", ",", "'refs'", ",", "split", ")", "\n", "assert", "exists", "(", "ref_dir", ")", "\n", "\n", "if", "args", ".", "rouge", ":", "\n", "        ", "dec_pattern", "=", "r'(\\d+).dec'", "\n", "ref_pattern", "=", "'[A-Z].#ID#.ref'", "\n", "#output = eval_rouge(dec_pattern, dec_dir, ref_pattern, ref_dir)", "\n", "output", "=", "eval_rouge", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ",", "cmd", "=", "\"-c 95 -r 1000 -n 2 -m -f B\"", ")", "\n", "metric", "=", "'rouge'", "\n", "", "else", ":", "\n", "        ", "dec_pattern", "=", "'[0-9]+.dec'", "\n", "ref_pattern", "=", "'[0-9]+.ref'", "\n", "output", "=", "eval_meteor", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "metric", "=", "'meteor'", "\n", "", "print", "(", "output", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'{}.txt'", ".", "format", "(", "metric", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.__init__": [[62, 79], ["training.BasicPipeline.batches"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.batches"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "batch_size", ",", "\n", "val_fn", ",", "criterion", ",", "optim", ",", "grad_fn", "=", "None", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "_net", "=", "net", "\n", "self", ".", "_train_batcher", "=", "train_batcher", "\n", "self", ".", "_val_batcher", "=", "val_batcher", "\n", "self", ".", "_criterion", "=", "criterion", "\n", "self", ".", "_opt", "=", "optim", "\n", "# grad_fn is calleble without input args that modifyies gradient", "\n", "# it should return a dictionary of logging values", "\n", "self", ".", "_grad_fn", "=", "grad_fn", "\n", "self", ".", "_val_fn", "=", "val_fn", "\n", "\n", "self", ".", "_n_epoch", "=", "0", "# epoch not very useful?", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_batches", "=", "self", ".", "batches", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.batches": [[80, 87], ["training.BasicPipeline._train_batcher"], "methods", ["None"], ["", "def", "batches", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "for", "fw_args", ",", "bw_args", "in", "self", ".", "_train_batcher", "(", "self", ".", "_batch_size", ")", ":", "\n", "                ", "yield", "fw_args", ",", "bw_args", "\n", "# debug", "\n", "#print(\"fw_args 1: {}\".format(fw_args))", "\n", "", "self", ".", "_n_epoch", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.get_loss_args": [[88, 94], ["isinstance"], "methods", ["None"], ["", "", "def", "get_loss_args", "(", "self", ",", "net_out", ",", "bw_args", ")", ":", "\n", "        ", "if", "isinstance", "(", "net_out", ",", "tuple", ")", ":", "\n", "            ", "loss_args", "=", "net_out", "+", "bw_args", "\n", "", "else", ":", "\n", "            ", "loss_args", "=", "(", "net_out", ",", ")", "+", "bw_args", "\n", "", "return", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.train_step": [[95, 118], ["training.BasicPipeline._net.train", "next", "training.BasicPipeline._net", "training.BasicPipeline.get_loss_args", "training.BasicPipeline._criterion().mean", "training.BasicPipeline.backward", "training.BasicPipeline.item", "training.BasicPipeline._opt.step", "training.BasicPipeline._net.zero_grad", "log_dict.update", "training.BasicPipeline._criterion", "training.BasicPipeline._grad_fn"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.get_loss_args"], ["", "def", "train_step", "(", "self", ")", ":", "\n", "# forward pass of model", "\n", "        ", "self", ".", "_net", ".", "train", "(", ")", "\n", "fw_args", ",", "bw_args", "=", "next", "(", "self", ".", "_batches", ")", "\n", "net_out", "=", "self", ".", "_net", "(", "*", "fw_args", ")", "\n", "\n", "# debug", "\n", "#print(\"fw_args: {}\".format(fw_args))", "\n", "\n", "# get logs and output for logging, backward", "\n", "log_dict", "=", "{", "}", "\n", "loss_args", "=", "self", ".", "get_loss_args", "(", "net_out", ",", "bw_args", ")", "\n", "\n", "# backward and update ( and optional gradient monitoring )", "\n", "loss", "=", "self", ".", "_criterion", "(", "*", "loss_args", ")", ".", "mean", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "log_dict", "[", "'loss'", "]", "=", "loss", ".", "item", "(", ")", "\n", "if", "self", ".", "_grad_fn", "is", "not", "None", ":", "\n", "            ", "log_dict", ".", "update", "(", "self", ".", "_grad_fn", "(", ")", ")", "\n", "", "self", ".", "_opt", ".", "step", "(", ")", "\n", "self", ".", "_net", ".", "zero_grad", "(", ")", "\n", "\n", "return", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.validate": [[119, 121], ["training.BasicPipeline._val_fn", "training.BasicPipeline._val_batcher"], "methods", ["None"], ["", "def", "validate", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_val_fn", "(", "self", ".", "_val_batcher", "(", "self", ".", "_batch_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.checkpoint": [[122, 133], ["training.BasicPipeline._net.state_dict", "training.BasicPipeline._opt.state_dict", "torch.save", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save"], ["", "def", "checkpoint", "(", "self", ",", "save_path", ",", "step", ",", "val_metric", "=", "None", ")", ":", "\n", "        ", "save_dict", "=", "{", "}", "\n", "if", "val_metric", "is", "not", "None", ":", "\n", "            ", "name", "=", "'ckpt-{:6f}-{}'", ".", "format", "(", "val_metric", ",", "step", ")", "\n", "save_dict", "[", "'val_metric'", "]", "=", "val_metric", "\n", "", "else", ":", "\n", "            ", "name", "=", "'ckpt-{}'", ".", "format", "(", "step", ")", "\n", "\n", "", "save_dict", "[", "'state_dict'", "]", "=", "self", ".", "_net", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'optimizer'", "]", "=", "self", ".", "_opt", ".", "state_dict", "(", ")", "\n", "torch", ".", "save", "(", "save_dict", ",", "join", "(", "save_path", ",", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.terminate": [[134, 137], ["training.BasicPipeline._train_batcher.terminate", "training.BasicPipeline._val_batcher.terminate"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.terminate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.terminate"], ["", "def", "terminate", "(", "self", ")", ":", "\n", "        ", "self", ".", "_train_batcher", ".", "terminate", "(", ")", "\n", "self", ".", "_val_batcher", ".", "terminate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.__init__": [[141, 160], ["isinstance", "tensorboardX.SummaryWriter", "os.makedirs", "os.path.join", "os.path.join"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "pipeline", ",", "save_dir", ",", "ckpt_freq", ",", "patience", ",", "\n", "scheduler", "=", "None", ",", "val_mode", "=", "'loss'", ")", ":", "\n", "        ", "assert", "isinstance", "(", "pipeline", ",", "BasicPipeline", ")", "\n", "assert", "val_mode", "in", "[", "'loss'", ",", "'score'", "]", "\n", "self", ".", "_pipeline", "=", "pipeline", "\n", "self", ".", "_save_dir", "=", "save_dir", "\n", "self", ".", "_logger", "=", "tensorboardX", ".", "SummaryWriter", "(", "join", "(", "save_dir", ",", "'log'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "save_dir", ",", "'ckpt'", ")", ")", "\n", "\n", "self", ".", "_ckpt_freq", "=", "ckpt_freq", "\n", "self", ".", "_patience", "=", "patience", "\n", "self", ".", "_sched", "=", "scheduler", "\n", "self", ".", "_val_mode", "=", "val_mode", "\n", "\n", "self", ".", "_step", "=", "0", "\n", "self", ".", "_running_loss", "=", "None", "\n", "# state vars for early stopping", "\n", "self", ".", "_current_p", "=", "0", "\n", "self", ".", "_best_val", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log": [[161, 175], ["print", "sys.stdout.flush", "log_dict.items", "training.BasicTrainer._logger.add_scalar"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "log_dict", ")", ":", "\n", "        ", "loss", "=", "log_dict", "[", "'loss'", "]", "if", "'loss'", "in", "log_dict", "else", "log_dict", "[", "'reward'", "]", "\n", "if", "self", ".", "_running_loss", "is", "not", "None", ":", "\n", "            ", "self", ".", "_running_loss", "=", "0.99", "*", "self", ".", "_running_loss", "+", "0.01", "*", "loss", "\n", "", "else", ":", "\n", "            ", "self", ".", "_running_loss", "=", "loss", "\n", "", "print", "(", "'train step: {}, {}: {:.4f}\\r'", ".", "format", "(", "\n", "self", ".", "_step", ",", "\n", "'loss'", "if", "'loss'", "in", "log_dict", "else", "'reward'", ",", "\n", "self", ".", "_running_loss", ")", ",", "end", "=", "''", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "for", "key", ",", "value", "in", "log_dict", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "add_scalar", "(", "\n", "'{}_{}'", ".", "format", "(", "key", ",", "self", ".", "_pipeline", ".", "name", ")", ",", "value", ",", "self", ".", "_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.validate": [[176, 191], ["print", "training.BasicTrainer._pipeline.validate", "sys.stdout.flush", "training.BasicTrainer.items", "training.BasicTrainer._logger.add_scalar"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.validate"], ["", "", "def", "validate", "(", "self", ")", ":", "\n", "        ", "print", "(", ")", "\n", "val_log", "=", "self", ".", "_pipeline", ".", "validate", "(", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "for", "key", ",", "value", "in", "val_log", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "add_scalar", "(", "\n", "'val_{}_{}'", ".", "format", "(", "key", ",", "self", ".", "_pipeline", ".", "name", ")", ",", "\n", "value", ",", "self", ".", "_step", "\n", ")", "\n", "", "if", "'reward'", "in", "val_log", ":", "\n", "            ", "val_metric", "=", "val_log", "[", "'reward'", "]", "\n", "", "else", ":", "\n", "            ", "val_metric", "=", "(", "val_log", "[", "'loss'", "]", "if", "self", ".", "_val_mode", "==", "'loss'", "\n", "else", "val_log", "[", "'score'", "]", ")", "\n", "", "return", "val_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.checkpoint": [[192, 204], ["training.BasicTrainer.validate", "training.BasicTrainer._pipeline.checkpoint", "isinstance", "training.BasicTrainer.check_stop", "os.path.join", "training.BasicTrainer._sched.step", "training.BasicTrainer._sched.step"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.checkpoint", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.check_stop"], ["", "def", "checkpoint", "(", "self", ")", ":", "\n", "        ", "val_metric", "=", "self", ".", "validate", "(", ")", "\n", "self", ".", "_pipeline", ".", "checkpoint", "(", "\n", "join", "(", "self", ".", "_save_dir", ",", "'ckpt'", ")", ",", "self", ".", "_step", ",", "val_metric", ")", "\n", "if", "isinstance", "(", "self", ".", "_sched", ",", "ReduceLROnPlateau", ")", ":", "\n", "            ", "self", ".", "_sched", ".", "step", "(", "val_metric", ")", "\n", "", "elif", "self", ".", "_sched", "is", "None", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "self", ".", "_sched", ".", "step", "(", ")", "\n", "", "stop", "=", "self", ".", "check_stop", "(", "val_metric", ")", "\n", "return", "stop", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.check_stop": [[205, 215], ["None"], "methods", ["None"], ["", "def", "check_stop", "(", "self", ",", "val_metric", ")", ":", "\n", "        ", "if", "self", ".", "_best_val", "is", "None", ":", "\n", "            ", "self", ".", "_best_val", "=", "val_metric", "\n", "", "elif", "(", "(", "val_metric", "<", "self", ".", "_best_val", "and", "self", ".", "_val_mode", "==", "'loss'", ")", "\n", "or", "(", "val_metric", ">", "self", ".", "_best_val", "and", "self", ".", "_val_mode", "==", "'score'", ")", ")", ":", "\n", "            ", "self", ".", "_current_p", "=", "0", "\n", "self", ".", "_best_val", "=", "val_metric", "\n", "", "else", ":", "\n", "            ", "self", ".", "_current_p", "+=", "1", "\n", "", "return", "self", ".", "_current_p", ">=", "self", ".", "_patience", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train": [[216, 233], ["time.time.time", "print", "sys.stdout.flush", "print", "training.BasicTrainer._pipeline.terminate", "training.BasicTrainer._pipeline.train_step", "training.BasicTrainer.log", "datetime.timedelta", "training.BasicTrainer.checkpoint", "time.time.time"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.terminate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicPipeline.train_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.checkpoint"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "start", "=", "time", "(", ")", "\n", "print", "(", "'Start training'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "while", "True", ":", "\n", "                ", "log_dict", "=", "self", ".", "_pipeline", ".", "train_step", "(", ")", "\n", "self", ".", "_step", "+=", "1", "\n", "self", ".", "log", "(", "log_dict", ")", "\n", "\n", "if", "self", ".", "_step", "%", "self", ".", "_ckpt_freq", "==", "0", ":", "\n", "                    ", "stop", "=", "self", ".", "checkpoint", "(", ")", "\n", "if", "stop", ":", "\n", "                        ", "break", "\n", "", "", "", "print", "(", "'Training finised in '", ",", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", "\n", "", "finally", ":", "\n", "            ", "self", ".", "_pipeline", ".", "terminate", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn": [[17, 29], ["torch.nn.utils.clip_grad_norm_", "print", "net.parameters"], "function", ["None"], ["def", "get_basic_grad_fn", "(", "net", ",", "clip_grad", ",", "max_grad", "=", "1e2", ")", ":", "\n", "    ", "def", "f", "(", ")", ":", "\n", "        ", "grad_norm", "=", "clip_grad_norm_", "(", "\n", "[", "p", "for", "p", "in", "net", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", ",", "clip_grad", ")", "\n", "#grad_norm = grad_norm.item()", "\n", "if", "max_grad", "is", "not", "None", "and", "grad_norm", ">=", "max_grad", ":", "\n", "            ", "print", "(", "'WARNING: Exploding Gradients {:.2f}'", ".", "format", "(", "grad_norm", ")", ")", "\n", "grad_norm", "=", "max_grad", "\n", "", "grad_log", "=", "{", "}", "\n", "grad_log", "[", "'grad_norm'", "]", "=", "grad_norm", "\n", "return", "grad_log", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.compute_loss": [[30, 34], ["criterion", "net"], "function", ["None"], ["", "@", "curry", "\n", "def", "compute_loss", "(", "net", ",", "criterion", ",", "fw_args", ",", "loss_args", ")", ":", "\n", "    ", "loss", "=", "criterion", "(", "*", "(", "(", "net", "(", "*", "fw_args", ")", ",", ")", "+", "loss_args", ")", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.val_step": [[35, 39], ["loss_step", "loss_step.size", "loss_step.sum().item", "loss_step.sum"], "function", ["None"], ["", "@", "curry", "\n", "def", "val_step", "(", "loss_step", ",", "fw_args", ",", "loss_args", ")", ":", "\n", "    ", "loss", "=", "loss_step", "(", "fw_args", ",", "loss_args", ")", "\n", "return", "loss", ".", "size", "(", "0", ")", ",", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate": [[40, 59], ["print", "net.eval", "time.time", "print", "print", "torch.no_grad", "training.val_step", "cytoolz.reduce", "training.compute_loss", "itertools.starmap", "datetime.timedelta", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.val_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.compute_loss"], ["", "@", "curry", "\n", "def", "basic_validate", "(", "net", ",", "criterion", ",", "val_batches", ")", ":", "\n", "    ", "print", "(", "'running validation ... '", ",", "end", "=", "''", ")", "\n", "net", ".", "eval", "(", ")", "\n", "start", "=", "time", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "validate_fn", "=", "val_step", "(", "compute_loss", "(", "net", ",", "criterion", ")", ")", "\n", "n_data", ",", "tot_loss", "=", "reduce", "(", "\n", "lambda", "a", ",", "b", ":", "(", "a", "[", "0", "]", "+", "b", "[", "0", "]", ",", "a", "[", "1", "]", "+", "b", "[", "1", "]", ")", ",", "\n", "starmap", "(", "validate_fn", ",", "val_batches", ")", ",", "\n", "(", "0", ",", "0", ")", "\n", ")", "\n", "", "val_loss", "=", "tot_loss", "/", "n_data", "\n", "print", "(", "\n", "'validation finished in {}                                    '", ".", "format", "(", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", ")", "\n", ")", "\n", "print", "(", "'validation loss: {:.4f} ... '", ".", "format", "(", "val_loss", ")", ")", "\n", "return", "{", "'loss'", ":", "val_loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_baselines.make_summaries": [[16, 32], ["os.path.join", "os.makedirs", "os.listdir", "re.compile", "sorted", "os.path.join", "range", "os.path.join", "os.path.exists", "open", "f.write", "re.compile.match", "float", "os.path.join", "open", "sents.append", "d.split", "f.read"], "function", ["None"], ["", "def", "make_summaries", "(", "decode_dir", ",", "n_ext", ")", ":", "\n", "    ", "out_dir", "=", "join", "(", "decode_dir", ",", "'output_top{}'", ".", "format", "(", "args", ".", "n_ext", ")", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "decs", "=", "os", ".", "listdir", "(", "join", "(", "decode_dir", ",", "'output_0'", ")", ")", "\n", "dec_matcher", "=", "re", ".", "compile", "(", "'[0-9]*.dec'", ")", "\n", "decs", "=", "sorted", "(", "[", "d", "for", "d", "in", "decs", "if", "dec_matcher", ".", "match", "(", "d", ")", "]", ",", "\n", "key", "=", "lambda", "d", ":", "float", "(", "d", ".", "split", "(", "'.'", ")", "[", "0", "]", ")", ")", "\n", "for", "d", "in", "decs", ":", "\n", "        ", "sents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_ext", ")", ":", "\n", "            ", "fname", "=", "join", "(", "decode_dir", ",", "'output_{}/{}'", ".", "format", "(", "i", ",", "d", ")", ")", "\n", "if", "exists", "(", "fname", ")", ":", "\n", "                ", "with", "open", "(", "fname", ",", "'r'", ")", "as", "f", ":", "\n", "                    ", "sents", ".", "append", "(", "f", ".", "read", "(", ")", ")", "\n", "", "", "", "with", "open", "(", "join", "(", "out_dir", ",", "d", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "sents", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_baselines.main": [[34, 57], ["os.path.join", "os.path.join", "os.path.exists", "print", "os.path.exists", "eval_baselines.make_summaries", "open", "evaluate.eval_rouge", "evaluate.eval_meteor", "open", "f.write", "os.path.join", "json.loads", "os.path.join", "f.read"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_baselines.make_summaries", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_rouge", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_meteor"], ["", "", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "dec_dir", "=", "join", "(", "args", ".", "decode_dir", ",", "'output_top{}'", ".", "format", "(", "args", ".", "n_ext", ")", ")", "\n", "if", "not", "exists", "(", "dec_dir", ")", ":", "\n", "        ", "make_summaries", "(", "args", ".", "decode_dir", ",", "args", ".", "n_ext", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'log.json'", ")", ")", "as", "f", ":", "\n", "        ", "split", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "[", "'split'", "]", "\n", "", "ref_dir", "=", "join", "(", "_DATA_DIR", ",", "'refs'", ",", "split", ")", "\n", "assert", "exists", "(", "ref_dir", ")", "\n", "\n", "if", "args", ".", "rouge", ":", "\n", "        ", "dec_pattern", "=", "r'(\\d+).dec'", "\n", "ref_pattern", "=", "'#ID#.ref'", "\n", "output", "=", "eval_rouge", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "metric", "=", "'rouge'", "\n", "", "else", ":", "\n", "        ", "dec_pattern", "=", "'[0-9]+.dec'", "\n", "ref_pattern", "=", "'[0-9]+.ref'", "\n", "output", "=", "eval_meteor", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "metric", "=", "'meteor'", "\n", "", "print", "(", "output", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "\n", "'top{}_{}.txt'", ".", "format", "(", "args", ".", "n_ext", ",", "metric", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._split_words": [[24, 26], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp": [[28, 40], ["range", "range", "range", "len", "range", "len", "max", "len", "len"], "function", ["None"], ["", "def", "_lcs_dp", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the len dp of lcs\"\"\"", "\n", "dp", "=", "[", "[", "0", "for", "_", "in", "range", "(", "0", ",", "len", "(", "b", ")", "+", "1", ")", "]", "\n", "for", "_", "in", "range", "(", "0", ",", "len", "(", "a", ")", "+", "1", ")", "]", "\n", "# dp[i][j]: lcs_len(a[:i], b[:j])", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "a", ")", "+", "1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "b", ")", "+", "1", ")", ":", "\n", "            ", "if", "a", "[", "i", "-", "1", "]", "==", "b", "[", "j", "-", "1", "]", ":", "\n", "                ", "dp", "[", "i", "]", "[", "j", "]", "=", "dp", "[", "i", "-", "1", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "dp", "[", "i", "]", "[", "j", "]", "=", "max", "(", "dp", "[", "i", "-", "1", "]", "[", "j", "]", ",", "dp", "[", "i", "]", "[", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "dp", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs": [[42, 59], ["make_group_ext_labels_new._lcs_dp", "len", "len", "collections.deque", "len", "collections.deque.appendleft"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp"], ["", "def", "_lcs", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the longest common subsequence between a and b\"\"\"", "\n", "dp", "=", "_lcs_dp", "(", "a", ",", "b", ")", "\n", "i", "=", "len", "(", "a", ")", "\n", "j", "=", "len", "(", "b", ")", "\n", "lcs", "=", "deque", "(", ")", "\n", "while", "(", "i", ">", "0", "and", "j", ">", "0", ")", ":", "\n", "        ", "if", "a", "[", "i", "-", "1", "]", "==", "b", "[", "j", "-", "1", "]", ":", "\n", "            ", "lcs", ".", "appendleft", "(", "a", "[", "i", "-", "1", "]", ")", "\n", "i", "-=", "1", "\n", "j", "-=", "1", "\n", "", "elif", "dp", "[", "i", "-", "1", "]", "[", "j", "]", ">=", "dp", "[", "i", "]", "[", "j", "-", "1", "]", ":", "\n", "            ", "i", "-=", "1", "\n", "", "else", ":", "\n", "            ", "j", "-=", "1", "\n", "", "", "assert", "len", "(", "lcs", ")", "==", "dp", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "return", "lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_extract_label": [[61, 83], ["make_group_ext_labels_new.get_extract_label_for_one_abstract_sent", "extracted.append", "extracted_major.append", "distances.append", "improvements.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_extract_label_for_one_abstract_sent"], ["", "def", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ":", "\n", "    ", "\"\"\" greedily match summary sentences to article sentences\"\"\"", "\n", "extracted", "=", "[", "]", "\n", "#scores = []", "\n", "#indices = list(range(len(art_sents)))", "\n", "extracted_major", "=", "[", "]", "\n", "distances", "=", "[", "]", "\n", "improvements", "=", "[", "]", "\n", "for", "abst", "in", "abs_sents", ":", "\n", "#rouges = list(map(compute_rouge_l(reference=abst, mode='r'), art_sents))", "\n", "#rouges = list(map(compute_rouge_l(reference=abst, mode=ROUGE_mode), art_sents))  # Rouge-L F1", "\n", "        ", "ext", ",", "distance", ",", "improvement", "=", "get_extract_label_for_one_abstract_sent", "(", "art_sents", ",", "abst", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ",", "extracted_major", ")", "\n", "#ext = max(indices, key=lambda i: rouges[i])", "\n", "#indices.remove(ext)", "\n", "extracted", ".", "append", "(", "ext", ")", "\n", "extracted_major", ".", "append", "(", "ext", "[", "0", "]", ")", "\n", "distances", ".", "append", "(", "distance", ")", "\n", "improvements", ".", "append", "(", "improvement", ")", "\n", "#scores.append(rouges[ext])", "\n", "if", "len", "(", "extracted_major", ")", "==", "len", "(", "abs_sents", ")", ":", "\n", "            ", "break", "\n", "", "", "return", "extracted", ",", "distances", ",", "improvements", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_extract_label_for_one_abstract_sent": [[85, 117], ["make_group_ext_labels_new.find_max_val_and_idx", "make_group_ext_labels_new._lcs_len", "make_group_ext_labels_new.find_max_val_and_idx", "metric.compute_rouge_l", "enumerate", "ext_labels.append", "abs", "make_group_ext_labels_new.find_max_val_and_idx", "make_group_ext_labels_new.get_union_lcs_len", "enumerate", "ext_labels.append", "abs", "metric.compute_rouge_l_summ", "enumerate"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_len", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_union_lcs_len", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l_summ"], ["", "def", "get_extract_label_for_one_abstract_sent", "(", "art_sents", ",", "abs_sent", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ",", "escape_art_sent_indices", "=", "[", "]", ")", ":", "\n", "# pick doc sentences with the highest Rouge-L recall (major sentence)", "\n", "#indices = list(range(len(art_sents)))", "\n", "#rouges = list(map(compute_rouge_l(reference=abs_sent, mode=ROUGE_mode), art_sents))  # Rouge-L F1", "\n", "    ", "rouges", "=", "[", "compute_rouge_l", "(", "output", "=", "art_sent", ",", "reference", "=", "abs_sent", ",", "mode", "=", "ROUGE_mode", ")", "if", "art_sent_idx", "not", "in", "escape_art_sent_indices", "else", "-", "1", "for", "art_sent_idx", ",", "art_sent", "in", "enumerate", "(", "art_sents", ")", "]", "\n", "major_art_sent_rouge", ",", "major_art_sent_idx", "=", "find_max_val_and_idx", "(", "rouges", ")", "\n", "major_art_sent", "=", "art_sents", "[", "major_art_sent_idx", "]", "\n", "ext_labels", "=", "[", "major_art_sent_idx", "]", "\n", "distance", "=", "None", "\n", "\n", "if", "ext_type", "==", "0", ":", "\n", "#threshold = 3", "\n", "        ", "major_lcs_len", "=", "_lcs_len", "(", "major_art_sent", ",", "abs_sent", ")", "\n", "#major_sent_idx = max(indices, key=lambda i: rouges[i])", "\n", "\n", "union_lcs_len_list", "=", "[", "get_union_lcs_len", "(", "[", "major_art_sent", ",", "art_sent", "]", ",", "abs_sent", ")", "if", "art_sent_idx", "!=", "major_art_sent_idx", "else", "-", "1", "for", "art_sent_idx", ",", "art_sent", "in", "enumerate", "(", "art_sents", ")", "]", "\n", "major_minor_union_lcs_len", ",", "minor_art_sent_idx", "=", "find_max_val_and_idx", "(", "union_lcs_len_list", ")", "\n", "if", "major_minor_union_lcs_len", "-", "major_lcs_len", ">=", "threshold", ":", "\n", "            ", "ext_labels", ".", "append", "(", "minor_art_sent_idx", ")", "\n", "distance", "=", "abs", "(", "major_art_sent_idx", "-", "minor_art_sent_idx", ")", "\n", "", "", "elif", "ext_type", "==", "1", ":", "\n", "#threshold = 0.01", "\n", "        ", "minor_sent_candidates_rouges", "=", "[", "\n", "compute_rouge_l_summ", "(", "[", "major_art_sent", ",", "art_sent", "]", ",", "[", "abs_sent", "]", ",", "mode", "=", "ROUGE_mode", ")", "if", "art_sent_idx", "!=", "major_art_sent_idx", "else", "-", "1", "for", "\n", "art_sent_idx", ",", "art_sent", "in", "enumerate", "(", "art_sents", ")", "]", "\n", "major_minor_rouge", ",", "minor_art_sent_idx", "=", "find_max_val_and_idx", "(", "minor_sent_candidates_rouges", ")", "\n", "improvement", "=", "major_minor_rouge", "-", "major_art_sent_rouge", "\n", "if", "improvement", ">=", "threshold", ":", "\n", "            ", "ext_labels", ".", "append", "(", "minor_art_sent_idx", ")", "\n", "distance", "=", "abs", "(", "major_art_sent_idx", "-", "minor_art_sent_idx", ")", "\n", "# compute_rouge_l_summ", "\n", "", "", "return", "ext_labels", ",", "distance", ",", "improvement", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.find_max_val_and_idx": [[119, 127], ["enumerate"], "function", ["None"], ["", "def", "find_max_val_and_idx", "(", "val_list", ")", ":", "\n", "    ", "max_val", "=", "-", "1000000", "\n", "max_idx", "=", "-", "1", "\n", "for", "idx", ",", "val", "in", "enumerate", "(", "val_list", ")", ":", "\n", "        ", "if", "val", ">", "max_val", ":", "\n", "            ", "max_val", "=", "val", "\n", "max_idx", "=", "idx", "\n", "", "", "return", "max_val", ",", "max_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.get_union_lcs_len": [[129, 141], ["collections.Counter", "collections.Counter", "cytoolz.concat", "make_group_ext_labels_new._lcs"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs"], ["", "def", "get_union_lcs_len", "(", "art_sents", ",", "ref_sent", ")", ":", "\n", "    ", "tot_hit", "=", "0", "\n", "art_cnt", "=", "Counter", "(", "concat", "(", "art_sents", ")", ")", "\n", "ref_cnt", "=", "Counter", "(", "ref_sent", ")", "\n", "for", "art_sent", "in", "art_sents", ":", "\n", "        ", "lcs", "=", "_lcs", "(", "art_sent", ",", "ref_sent", ")", "\n", "for", "gram", "in", "lcs", ":", "\n", "            ", "if", "ref_cnt", "[", "gram", "]", ">", "0", "and", "art_cnt", "[", "gram", "]", ">", "0", ":", "\n", "                ", "tot_hit", "+=", "1", "\n", "", "ref_cnt", "[", "gram", "]", "-=", "1", "\n", "art_cnt", "[", "gram", "]", "-=", "1", "\n", "", "", "return", "tot_hit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_len": [[143, 147], ["make_group_ext_labels_new._lcs_dp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new._lcs_dp"], ["", "def", "_lcs_len", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\" compute the length of longest common subsequence between a and b\"\"\"", "\n", "dp", "=", "_lcs_dp", "(", "a", ",", "b", ")", "\n", "return", "dp", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.process": [[149, 168], ["os.path.join", "cytoolz.compose", "cytoolz.compose.", "cytoolz.compose.", "open", "json.loads", "make_group_ext_labels_new.get_extract_label", "open", "json.dump", "os.path.join", "f.read", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.get_extract_label", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "@", "curry", "\n", "def", "process", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ",", "i", ")", ":", "\n", "    ", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "tokenize", "=", "compose", "(", "list", ",", "_split_words", ")", "\n", "art_sents", "=", "tokenize", "(", "data", "[", "'article'", "]", ")", "\n", "abs_sents", "=", "tokenize", "(", "data", "[", "'abstract'", "]", ")", "\n", "if", "art_sents", "and", "abs_sents", ":", "# some data contains empty article/abstract", "\n", "        ", "extracted", ",", "distances", ",", "improvements", "=", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", "\n", "", "else", ":", "\n", "        ", "extracted", ",", "distances", ",", "improvements", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "#data.pop('extracted_by_lcs', None)", "\n", "#data.pop('distances_lcs', None)", "\n", "", "data", "[", "'extracted_two_to_one_{}'", ".", "format", "(", "threshold", ")", "]", "=", "extracted", "\n", "data", "[", "'distances_two_to_one_{}'", ".", "format", "(", "threshold", ")", "]", "=", "distances", "\n", "data", "[", "'improvements'", "]", "=", "improvements", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.label_mp": [[170, 180], ["time.time", "print", "os.path.join", "utils.count_data", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "make_group_ext_labels_new.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "label_mp", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_group_ext_labels_new.main": [[182, 188], ["make_group_ext_labels_new.label_mp", "make_group_ext_labels_new.label_mp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp"], ["", "def", "main", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'train'", "]", ":", "# no need of extraction label when testing", "\n", "            ", "label_mp", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", "\n", "", "", "else", ":", "\n", "        ", "label_mp", "(", "split", ",", "ROUGE_mode", ",", "ext_type", ",", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_pubmed.main": [[17, 44], ["os.makedirs", "enumerate", "print", "print", "open", "f_in.readlines", "os.path.join", "json.loads", "line.strip", "abs_sent.replace().replace().strip.replace().replace().strip", "new_abstract.append", "open", "json.dump", "os.path.join", "abs_sent.replace().replace().strip.replace().replace", "abs_sent.replace().replace().strip.replace"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["def", "main", "(", "file_name", ",", "out_data_dir", ",", "split", ")", ":", "\n", "    ", "with", "open", "(", "file_name", ")", "as", "f_in", ":", "\n", "        ", "all_lines", "=", "f_in", ".", "readlines", "(", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "join", "(", "out_data_dir", ",", "split", ")", ")", "\n", "num_processed_samples", "=", "0", "\n", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "all_lines", ")", ":", "\n", "        ", "sample_obj", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "sample_obj", "[", "\"article\"", "]", "=", "sample_obj", "[", "\"article_text\"", "]", "\n", "del", "sample_obj", "[", "\"article_text\"", "]", "\n", "#sample_obj[\"abstract\"] = sample_obj[\"abstract_text\"]", "\n", "#del sample_obj[\"abstract_text\"]", "\n", "\n", "new_abstract", "=", "[", "]", "\n", "for", "abs_sent", "in", "sample_obj", "[", "\"abstract_text\"", "]", ":", "\n", "            ", "abs_sent", "=", "abs_sent", ".", "replace", "(", "\"<S>\"", ",", "\"\"", ")", ".", "replace", "(", "\"</S>\"", ",", "\"\"", ")", ".", "strip", "(", ")", "\n", "new_abstract", ".", "append", "(", "abs_sent", ")", "\n", "", "sample_obj", "[", "\"abstract\"", "]", "=", "new_abstract", "\n", "del", "sample_obj", "[", "\"abstract_text\"", "]", "\n", "\n", "with", "open", "(", "join", "(", "out_data_dir", ",", "split", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "sample_obj", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "num_processed_samples", "+=", "1", "\n", "\n", "", "print", "(", "\"Processed {} samples\"", ".", "format", "(", "num_processed_samples", ")", ")", "\n", "print", "(", "\"Finished!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels._split_words": [[22, 24], ["map", "t.split"], "function", ["None"], ["", "def", "_split_words", "(", "texts", ")", ":", "\n", "    ", "return", "map", "(", "lambda", "t", ":", "t", ".", "split", "(", ")", ",", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.get_extract_label": [[26, 41], ["list", "range", "list", "max", "list.remove", "extracted.append", "scores.append", "len", "map", "metric.compute_rouge_l"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.metric.compute_rouge_l"], ["", "def", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ")", ":", "\n", "    ", "\"\"\" greedily match summary sentences to article sentences\"\"\"", "\n", "extracted", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "indices", "=", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "for", "abst", "in", "abs_sents", ":", "\n", "#rouges = list(map(compute_rouge_l(reference=abst, mode='r'), art_sents))", "\n", "        ", "rouges", "=", "list", "(", "map", "(", "compute_rouge_l", "(", "reference", "=", "abst", ",", "mode", "=", "ROUGE_mode", ")", ",", "art_sents", ")", ")", "# Rouge-L F1", "\n", "ext", "=", "max", "(", "indices", ",", "key", "=", "lambda", "i", ":", "rouges", "[", "i", "]", ")", "\n", "indices", ".", "remove", "(", "ext", ")", "\n", "extracted", ".", "append", "(", "ext", ")", "\n", "scores", ".", "append", "(", "rouges", "[", "ext", "]", ")", "\n", "if", "not", "indices", ":", "\n", "            ", "break", "\n", "", "", "return", "extracted", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process": [[43, 69], ["os.path.join", "cytoolz.compose", "cytoolz.compose.", "cytoolz.compose.", "sent.lower", "sent.lower", "make_extraction_labels.get_extract_label", "open", "json.dump", "open", "json.loads", "print", "os.path.join", "os.path.join", "f.read"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.get_extract_label", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "@", "curry", "\n", "def", "process", "(", "split", ",", "ROUGE_mode", ",", "i", ")", ":", "\n", "    ", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "try", ":", "\n", "        ", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "", "except", ":", "\n", "        ", "print", "(", "i", ")", "\n", "return", "\n", "", "tokenize", "=", "compose", "(", "list", ",", "_split_words", ")", "\n", "article", "=", "data", "[", "'article'", "]", "\n", "abstract", "=", "data", "[", "'abstract'", "]", "\n", "article_lower", "=", "[", "sent", ".", "lower", "(", ")", "for", "sent", "in", "data", "[", "'article'", "]", "]", "\n", "abstract_lower", "=", "[", "sent", ".", "lower", "(", ")", "for", "sent", "in", "data", "[", "'abstract'", "]", "]", "\n", "#art_sents = tokenize(data['article'])", "\n", "#abs_sents = tokenize(data['abstract'])", "\n", "art_sents", "=", "tokenize", "(", "article_lower", ")", "\n", "abs_sents", "=", "tokenize", "(", "abstract_lower", ")", "\n", "if", "art_sents", "and", "abs_sents", ":", "# some data contains empty article/abstract", "\n", "        ", "extracted", ",", "scores", "=", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ")", "\n", "", "else", ":", "\n", "        ", "extracted", ",", "scores", "=", "[", "]", ",", "[", "]", "\n", "", "data", "[", "'extracted'", "]", "=", "extracted", "\n", "data", "[", "'score'", "]", "=", "scores", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp": [[71, 81], ["time.time", "print", "os.path.join", "utils.count_data", "print", "multiprocessing.Pool", "list", "pool.imap_unordered", "datetime.timedelta", "make_extraction_labels.process", "list", "range", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "", "def", "label_mp", "(", "split", ",", "ROUGE_mode", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "with", "mp", ".", "Pool", "(", ")", "as", "pool", ":", "\n", "        ", "list", "(", "pool", ".", "imap_unordered", "(", "process", "(", "split", ",", "ROUGE_mode", ")", ",", "\n", "list", "(", "range", "(", "n_data", ")", ")", ",", "chunksize", "=", "1024", ")", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label": [[83, 92], ["time.time", "print", "os.path.join", "utils.count_data", "range", "print", "make_extraction_labels.process", "datetime.timedelta", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.process"], ["", "def", "label", "(", "split", ",", "ROUGE_mode", ")", ":", "\n", "    ", "\"\"\" process the data split with multi-processing\"\"\"", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "process", "(", "split", ",", "ROUGE_mode", ",", "i", ")", "\n", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_backup": [[94, 113], ["time.time", "print", "os.path.join", "utils.count_data", "range", "print", "print", "cytoolz.compose", "cytoolz.compose.", "cytoolz.compose.", "make_extraction_labels.get_extract_label", "open", "json.loads", "open", "json.dump", "datetime.timedelta", "os.path.join", "f.read", "os.path.join", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.get_extract_label", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "label_backup", "(", "split", ",", "ROUGE_mode", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "print", "(", "'start processing {} split...'", ".", "format", "(", "split", ")", ")", "\n", "data_dir", "=", "join", "(", "DATA_DIR", ",", "split", ")", "\n", "n_data", "=", "count_data", "(", "data_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "print", "(", "'processing {}/{} ({:.2f}%%)\\r'", ".", "format", "(", "i", ",", "n_data", ",", "100", "*", "i", "/", "n_data", ")", ",", "\n", "end", "=", "''", ")", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "tokenize", "=", "compose", "(", "list", ",", "_split_words", ")", "\n", "art_sents", "=", "tokenize", "(", "data", "[", "'article'", "]", ")", "\n", "abs_sents", "=", "tokenize", "(", "data", "[", "'abstract'", "]", ")", "\n", "extracted", ",", "scores", "=", "get_extract_label", "(", "art_sents", ",", "abs_sents", ",", "ROUGE_mode", ")", "\n", "data", "[", "'extracted'", "]", "=", "extracted", "\n", "data", "[", "'score'", "]", "=", "scores", "\n", "with", "open", "(", "join", "(", "data_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "", "print", "(", "'finished in {}'", ".", "format", "(", "timedelta", "(", "seconds", "=", "time", "(", ")", "-", "start", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.main": [[115, 122], ["make_extraction_labels.label_mp", "make_extraction_labels.label_mp"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_extraction_labels.label_mp"], ["", "def", "main", "(", "split", ",", "ROUGE_mode", ")", ":", "\n", "    ", "if", "split", "==", "'all'", ":", "\n", "        ", "for", "split", "in", "[", "'val'", ",", "'train'", "]", ":", "# no need of extraction label when testing", "\n", "            ", "label_mp", "(", "split", ",", "ROUGE_mode", ")", "\n", "#label(split, ROUGE_mode)", "\n", "", "", "else", ":", "\n", "        ", "label_mp", "(", "split", ",", "ROUGE_mode", ")", "\n", "#label(split, ROUGE_mode)", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor_two_to_one.TwoToOneMatchDataset.__init__": [[45, 48], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "threshold", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor_two_to_one.TwoToOneMatchDataset.__getitem__": [[49, 64], ["data.data.CnnDmDataset.__getitem__", "zip", "len", "matched_arts.append", "matched_abss.append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted_two_to_one_{}'", ".", "format", "(", "self", ".", "threshold", ")", "]", ")", "\n", "# only keep the sentences with two ext labels", "\n", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "\n", "for", "ext", ",", "abst", "in", "zip", "(", "extracts", ",", "abs_sents", ")", ":", "\n", "            ", "if", "len", "(", "ext", ")", "==", "2", ":", "\n", "#ext = sorted(ext)", "\n", "                ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "[", "0", "]", "]", "+", "' '", "+", "art_sents", "[", "ext", "[", "1", "]", "]", ")", "\n", "matched_abss", ".", "append", "(", "abst", ")", "\n", "# return matched_arts, abs_sents[:len(extracts)]", "\n", "", "", "return", "matched_arts", ",", "matched_abss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor_two_to_one.configure_net": [[66, 77], ["model.copy_summ.CopySumm"], "function", ["None"], ["", "", "def", "configure_net", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ")", ":", "\n", "    ", "net_args", "=", "{", "}", "\n", "net_args", "[", "'vocab_size'", "]", "=", "vocab_size", "\n", "net_args", "[", "'emb_dim'", "]", "=", "emb_dim", "\n", "net_args", "[", "'n_hidden'", "]", "=", "n_hidden", "\n", "net_args", "[", "'bidirectional'", "]", "=", "bidirectional", "\n", "net_args", "[", "'n_layer'", "]", "=", "n_layer", "\n", "\n", "net", "=", "CopySumm", "(", "**", "net_args", ")", "\n", "return", "net", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor_two_to_one.configure_training": [[79, 96], ["torch.nn.functional.nll_loss", "model.util.sequence_loss"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss"], ["", "def", "configure_training", "(", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\" supports Adam optimizer only\"\"\"", "\n", "assert", "opt", "in", "[", "'adam'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "\n", "nll", "=", "lambda", "logit", ",", "target", ":", "F", ".", "nll_loss", "(", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "def", "criterion", "(", "logits", ",", "targets", ")", ":", "\n", "        ", "return", "sequence_loss", "(", "logits", ",", "targets", ",", "nll", ",", "pad_idx", "=", "PAD", ")", "\n", "\n", "", "return", "criterion", ",", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor_two_to_one.build_batchers": [[97, 124], ["data.batcher.prepro_fn", "cytoolz.compose", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "data.batcher.batchify_fn_copy", "data.batcher.convert_batch_copy", "train_abstractor_two_to_one.TwoToOneMatchDataset", "train_abstractor_two_to_one.TwoToOneMatchDataset", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy"], ["", "def", "build_batchers", "(", "word2id", ",", "cuda", ",", "threshold", ",", "debug", ")", ":", "\n", "    ", "prepro", "=", "prepro_fn", "(", "args", ".", "max_art", ",", "args", ".", "max_abs", ")", "\n", "def", "sort_key", "(", "sample", ")", ":", "\n", "        ", "src", ",", "target", "=", "sample", "\n", "return", "(", "len", "(", "target", ")", ",", "len", "(", "src", ")", ")", "\n", "", "batchify", "=", "compose", "(", "\n", "batchify_fn_copy", "(", "PAD", ",", "START", ",", "END", ",", "cuda", "=", "cuda", ")", ",", "# padding", "\n", "convert_batch_copy", "(", "UNK", ",", "word2id", ")", "# convert to idx", "\n", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "\n", "TwoToOneMatchDataset", "(", "'train'", ",", "threshold", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "not", "debug", ",", "\n", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn", "\n", ")", "\n", "train_batcher", "=", "BucketedGenerater", "(", "train_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "False", ",", "fork", "=", "not", "debug", ")", "\n", "\n", "val_loader", "=", "DataLoader", "(", "\n", "TwoToOneMatchDataset", "(", "'val'", ",", "threshold", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn", "\n", ")", "\n", "val_batcher", "=", "BucketedGenerater", "(", "val_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "fork", "=", "not", "debug", ")", "\n", "return", "train_batcher", ",", "val_batcher", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor_two_to_one.main": [[125, 180], ["utils.make_vocab", "train_abstractor_two_to_one.build_batchers", "train_abstractor_two_to_one.configure_net", "train_abstractor_two_to_one.configure_training", "training.basic_validate", "training.get_basic_grad_fn", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "training.BasicPipeline", "training.BasicTrainer", "print", "print", "training.BasicTrainer.train", "open", "pickle.load", "len", "utils.make_embedding", "net.cuda.set_embedding", "os.path.exists", "os.makedirs", "open", "pickle.dump", "open", "json.dump", "net.cuda.parameters", "net.cuda.cuda", "os.path.join", "os.path.join", "os.path.join", "utils.make_vocab.items"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "args", ")", ":", "\n", "# create data batcher, vocabulary", "\n", "# batcher", "\n", "    ", "with", "open", "(", "join", "(", "DATA_DIR", ",", "'vocab_cnt.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "wc", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "word2id", "=", "make_vocab", "(", "wc", ",", "args", ".", "vsize", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "word2id", ",", "\n", "args", ".", "cuda", ",", "args", ".", "threshold", ",", "args", ".", "debug", ")", "\n", "\n", "# make net", "\n", "net", ",", "net_args", "=", "configure_net", "(", "len", "(", "word2id", ")", ",", "args", ".", "emb_dim", ",", "\n", "args", ".", "n_hidden", ",", "args", ".", "bi", ",", "args", ".", "n_layer", ")", "\n", "if", "args", ".", "w2v", ":", "\n", "# NOTE: the pretrained embedding having the same dimension", "\n", "#       as args.emb_dim should already be trained", "\n", "        ", "embedding", ",", "_", "=", "make_embedding", "(", "\n", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", ",", "args", ".", "w2v", ")", "\n", "net", ".", "set_embedding", "(", "embedding", ")", "\n", "\n", "# configure training setting", "\n", "", "criterion", ",", "train_params", "=", "configure_training", "(", "\n", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", "\n", ")", "\n", "\n", "# save experiment setting", "\n", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "word2id", ",", "f", ",", "protocol", "=", "4", ")", "\n", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'base_abstractor'", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'traing_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "val_fn", "=", "basic_validate", "(", "net", ",", "criterion", ")", "\n", "grad_fn", "=", "get_basic_grad_fn", "(", "net", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "0", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "", "pipeline", "=", "BasicPipeline", "(", "meta", "[", "'net'", "]", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "args", ".", "batch", ",", "val_fn", ",", "\n", "criterion", ",", "optimizer", ",", "grad_fn", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines.decode": [[28, 138], ["time.time", "decoding.DecodeDataset", "len", "torch.utils.data.DataLoader", "range", "print", "decoding.Extractor", "list", "os.makedirs", "json.load", "json.load", "open", "json.dump", "torch.no_grad", "enumerate", "filter", "os.path.join", "open", "open", "os.path.join", "map", "len", "enumerate", "decoding.ConditionalAbstractor", "decoding.BeamConditionalAbstractor", "decoding.Abstractor", "decoding.BeamAbstractor", "list", "os.path.join", "os.path.join", "data.batcher.tokenize", "decoding.Extractor.", "len", "enumerate", "print", "range", "len", "enumerate", "list", "decoding.BeamAbstractor.", "decoding.BeamAbstractor.", "decode_baselines.rerank_mp", "decoding.BeamAbstractor.", "decoding.BeamAbstractor.", "len", "len", "len", "sequential_ext_sents[].append", "sequential_article_ids[].append", "map", "open", "f.write", "datetime.timedelta", "sequential_ext_sents.append", "sequential_article_ids.append", "os.path.join", "decoding.make_html_safe", "range", "range", "int", "len", "len", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["def", "decode", "(", "save_path", ",", "abs_dir", ",", "ext_dir", ",", "split", ",", "batch_size", ",", "max_len", ",", "beam_size", ",", "diverse", ",", "cuda", ",", "disable_final_rerank", "=", "False", ",", "is_conditional_abs", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "if", "abs_dir", "is", "None", ":", "\n", "# NOTE: if no abstractor is provided then", "\n", "#       the whole model would be extractive summarization", "\n", "        ", "abstractor", "=", "identity", "\n", "", "elif", "is_conditional_abs", ":", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "ConditionalAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamConditionalAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "", "else", ":", "\n", "#abstractor = Abstractor(abs_dir, max_len, cuda)", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "", "", "if", "ext_dir", "is", "None", ":", "\n", "# NOTE: if no abstractor is provided then", "\n", "#       it would be  the lead-N extractor", "\n", "        ", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "[", ":", "MAX_ABS_NUM", "]", "\n", "", "else", ":", "\n", "        ", "extractor", "=", "Extractor", "(", "ext_dir", ",", "max_ext", "=", "MAX_ABS_NUM", ",", "cuda", "=", "cuda", ")", "\n", "\n", "# setup loader", "\n", "", "def", "coll", "(", "batch", ")", ":", "\n", "        ", "articles", "=", "list", "(", "filter", "(", "bool", ",", "batch", ")", ")", "\n", "return", "articles", "\n", "", "dataset", "=", "DecodeDataset", "(", "split", ")", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "for", "i", "in", "range", "(", "MAX_ABS_NUM", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output_{}'", ".", "format", "(", "i", ")", ")", ")", "\n", "", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "None", "if", "abs_dir", "is", "None", "\n", "else", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "(", "None", "if", "ext_dir", "is", "None", "\n", "else", "json", ".", "load", "(", "open", "(", "join", "(", "ext_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "raw_article_batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "tokenized_article_batch", "=", "map", "(", "tokenize", "(", "None", ")", ",", "raw_article_batch", ")", "\n", "num_articles", "=", "len", "(", "raw_article_batch", ")", "\n", "num_ext_sents", "=", "0", "\n", "if", "is_conditional_abs", ":", "\n", "                ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "ext_arts", "=", "[", "]", "\n", "", "ext_inds", "=", "[", "]", "\n", "for", "article_i", ",", "raw_art_sents", "in", "enumerate", "(", "tokenized_article_batch", ")", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "num_ext_sents", ",", "len", "(", "ext", ")", ")", "]", "\n", "num_ext_sents", "+=", "len", "(", "ext", ")", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "                    ", "num_selected_sents_excluded_eos", "=", "len", "(", "ext", ")", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                        ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "idx_i", ",", "idx", "in", "enumerate", "(", "ext", ")", ":", "\n", "                        ", "sequential_ext_sents", "[", "idx_i", "]", ".", "append", "(", "raw_art_sents", "[", "idx", "]", ")", "\n", "sequential_article_ids", "[", "idx_i", "]", ".", "append", "(", "article_i", ")", "\n", "", "", "else", ":", "\n", "                    ", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "\n", "", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "if", "is_conditional_abs", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ",", "beam_size", ",", "\n", "diverse", ")", "\n", "", "else", ":", "\n", "                    ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "disable_final_rerank", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "is_conditional_abs", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "", "", "assert", "i", "==", "batch_size", "*", "i_debug", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "                ", "decoded_sents", "=", "[", "' '", ".", "join", "(", "dec", ")", "for", "dec", "in", "dec_outs", "[", "j", ":", "j", "+", "n", "]", "]", "\n", "for", "k", ",", "dec_str", "in", "enumerate", "(", "decoded_sents", ")", ":", "\n", "                    ", "with", "open", "(", "join", "(", "save_path", ",", "'output_{}/{}.dec'", ".", "format", "(", "k", ",", "i", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                        ", "f", ".", "write", "(", "make_html_safe", "(", "dec_str", ")", ")", "\n", "\n", "", "", "i", "+=", "1", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines.rerank": [[144, 151], ["list", "list", "cytoolz.concat", "cytoolz.concat", "map", "map"], "function", ["None"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "disable_final_rerank", "=", "False", ")", ":", "\n", "    ", "final_rerank", "=", "not", "disable_final_rerank", "\n", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "if", "final_rerank", ":", "\n", "        ", "return", "list", "(", "concat", "(", "map", "(", "rerank_one", ",", "beam_lists", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "list", "(", "concat", "(", "map", "(", "top_one", ",", "beam_lists", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines.rerank_mp": [[152, 162], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map"], "function", ["None"], ["", "", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "disable_final_rerank", "=", "False", ")", ":", "\n", "    ", "final_rerank", "=", "not", "disable_final_rerank", "\n", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "reranked", "=", "pool", ".", "map", "(", "rerank_one", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "reranked", "=", "pool", ".", "map", "(", "top_one", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "reranked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines.rerank_one": [[163, 173], ["map", "max", "decode_baselines.rerank_one.process_beam"], "function", ["None"], ["", "def", "rerank_one", "(", "beams", ")", ":", "\n", "    ", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "best_hyps", "=", "max", "(", "product", "(", "*", "beams", ")", ",", "key", "=", "_compute_score", ")", "# a tuple of", "\n", "dec_outs", "=", "[", "h", ".", "sequence", "for", "h", "in", "best_hyps", "]", "\n", "return", "dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines.top_one": [[174, 176], ["None"], "function", ["None"], ["", "def", "top_one", "(", "beams", ")", ":", "\n", "    ", "return", "[", "hyps", "[", "0", "]", ".", "sequence", "for", "hyps", "in", "beams", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines._make_n_gram": [[177, 179], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_baselines._compute_score": [[180, 185], ["functools.reduce", "sum", "collections.Counter", "sum", "sum", "functools.reduce.items", "len"], "function", ["None"], ["", "def", "_compute_score", "(", "hyps", ")", ":", "\n", "    ", "all_cnt", "=", "reduce", "(", "op", ".", "iadd", ",", "(", "h", ".", "gram_cnt", "for", "h", "in", "hyps", ")", ",", "Counter", "(", ")", ")", "\n", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "all_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "sum", "(", "h", ".", "logprob", "for", "h", "in", "hyps", ")", "/", "sum", "(", "len", "(", "h", ".", "sequence", ")", "for", "h", "in", "hyps", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model.decode": [[25, 139], ["time.time", "decoding.RLExtractor", "decoding.DecodeDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "print", "open", "json.loads", "list", "os.path.join", "open", "json.dump", "torch.no_grad", "enumerate", "os.path.join", "f.read", "filter", "os.path.join", "map", "len", "enumerate", "decoding.ConditionalAbstractor", "decoding.BeamConditionalAbstractor", "decoding.Abstractor", "decoding.BeamAbstractor", "data.batcher.tokenize", "len", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "decoding.RLExtractor.", "len", "enumerate", "decoding.BeamAbstractor.", "decoding.BeamAbstractor.", "decode_full_model.rerank_mp", "decoding.BeamAbstractor.", "decoding.BeamAbstractor.", "open", "f.write", "list", "i.item", "len", "len", "sequential_ext_sents[].append", "sequential_article_ids[].append", "os.path.join", "decoding.make_html_safe", "datetime.timedelta", "range", "len", "sequential_ext_sents.append", "sequential_article_ids.append", "range", "range", "int", "len", "len", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["def", "decode", "(", "save_path", ",", "model_dir", ",", "split", ",", "batch_size", ",", "\n", "beam_size", ",", "diverse", ",", "max_len", ",", "cuda", ",", "is_conditional_abs", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "# setup model", "\n", "with", "open", "(", "join", "(", "model_dir", ",", "'meta.json'", ")", ")", "as", "f", ":", "\n", "        ", "meta", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "if", "meta", "[", "'net_args'", "]", "[", "'abstractor'", "]", "is", "None", ":", "\n", "# NOTE: if no abstractor is provided then", "\n", "#       the whole model would be extractive summarization", "\n", "        ", "assert", "beam_size", "==", "1", "\n", "abstractor", "=", "identity", "\n", "", "elif", "is_conditional_abs", ":", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "ConditionalAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamConditionalAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "beam_size", "==", "1", ":", "\n", "            ", "abstractor", "=", "Abstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "            ", "abstractor", "=", "BeamAbstractor", "(", "join", "(", "model_dir", ",", "'abstractor'", ")", ",", "\n", "max_len", ",", "cuda", ")", "\n", "", "", "extractor", "=", "RLExtractor", "(", "model_dir", ",", "cuda", "=", "cuda", ")", "\n", "\n", "# setup loaders", "\n", "def", "coll", "(", "batch", ")", ":", "\n", "        ", "articles", "=", "list", "(", "filter", "(", "bool", ",", "batch", ")", ")", "\n", "return", "articles", "\n", "", "dataset", "=", "DecodeDataset", "(", "split", ")", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output'", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "meta", "[", "'net_args'", "]", "[", "'abstractor'", "]", "\n", "dec_log", "[", "'extractor'", "]", "=", "meta", "[", "'net_args'", "]", "[", "'extractor'", "]", "\n", "dec_log", "[", "'rl'", "]", "=", "True", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "# idx for decoded article", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "raw_article_batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "tokenized_article_batch", "=", "map", "(", "tokenize", "(", "100", ",", "\"w2v\"", ",", "1", ")", ",", "raw_article_batch", ")", "\n", "num_articles", "=", "len", "(", "raw_article_batch", ")", "\n", "num_ext_sents", "=", "0", "\n", "if", "is_conditional_abs", ":", "\n", "                ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "ext_arts", "=", "[", "]", "\n", "", "ext_inds", "=", "[", "]", "\n", "for", "article_i", ",", "raw_art_sents", "in", "enumerate", "(", "tokenized_article_batch", ")", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "[", ":", "-", "1", "]", "# exclude EOE", "\n", "if", "not", "ext", ":", "\n", "# use top-5 if nothing is extracted", "\n", "# in some rare cases rnn-ext does not extract at all", "\n", "                    ", "ext", "=", "list", "(", "range", "(", "5", ")", ")", "[", ":", "len", "(", "raw_art_sents", ")", "]", "\n", "", "else", ":", "\n", "                    ", "ext", "=", "[", "i", ".", "item", "(", ")", "for", "i", "in", "ext", "]", "\n", "", "ext_inds", "+=", "[", "(", "num_ext_sents", ",", "len", "(", "ext", ")", ")", "]", "\n", "num_ext_sents", "+=", "len", "(", "ext", ")", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "                    ", "num_selected_sents_excluded_eos", "=", "len", "(", "ext", ")", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                        ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "idx_i", ",", "idx", "in", "enumerate", "(", "ext", ")", ":", "\n", "                        ", "sequential_ext_sents", "[", "idx_i", "]", ".", "append", "(", "raw_art_sents", "[", "idx", "]", ")", "\n", "sequential_article_ids", "[", "idx_i", "]", ".", "append", "(", "article_i", ")", "\n", "", "", "else", ":", "\n", "                    ", "ext_arts", "+=", "[", "raw_art_sents", "[", "i", "]", "for", "i", "in", "ext", "]", "\n", "\n", "", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "if", "is_conditional_abs", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ",", "beam_size", ",", "diverse", ")", "\n", "", "else", ":", "\n", "                    ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ")", "\n", "#dec_outs = rerank(all_beams, ext_inds)", "\n", "", "", "else", ":", "\n", "                ", "if", "is_conditional_abs", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "                    ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "", "", "assert", "i", "==", "batch_size", "*", "i_debug", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "                ", "decoded_sents", "=", "[", "' '", ".", "join", "(", "dec", ")", "for", "dec", "in", "dec_outs", "[", "j", ":", "j", "+", "n", "]", "]", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'output/{}.dec'", ".", "format", "(", "i", ")", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "decoded_sents", ")", ")", ")", "\n", "", "i", "+=", "1", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model.rerank": [[145, 148], ["list", "cytoolz.concat", "map"], "function", ["None"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "return", "list", "(", "concat", "(", "map", "(", "rerank_one", ",", "beam_lists", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model.rerank_mp": [[149, 155], ["list", "torch.multiprocessing.Pool", "pool.map", "cytoolz.concat"], "function", ["None"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "reranked", "=", "pool", ".", "map", "(", "rerank_one", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "reranked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model.rerank_one": [[156, 166], ["map", "max", "decode_full_model.rerank_one.process_beam"], "function", ["None"], ["", "def", "rerank_one", "(", "beams", ")", ":", "\n", "    ", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "best_hyps", "=", "max", "(", "product", "(", "*", "beams", ")", ",", "key", "=", "_compute_score", ")", "# a tuple", "\n", "dec_outs", "=", "[", "h", ".", "sequence", "for", "h", "in", "best_hyps", "]", "\n", "return", "dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model._make_n_gram": [[167, 169], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_full_model._compute_score": [[170, 175], ["functools.reduce", "sum", "collections.Counter", "sum", "sum", "functools.reduce.items", "len"], "function", ["None"], ["", "def", "_compute_score", "(", "hyps", ")", ":", "\n", "    ", "all_cnt", "=", "reduce", "(", "op", ".", "iadd", ",", "(", "h", ".", "gram_cnt", "for", "h", "in", "hyps", ")", ",", "Counter", "(", ")", ")", "\n", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "all_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "sum", "(", "h", ".", "logprob", "for", "h", "in", "hyps", ")", "/", "sum", "(", "len", "(", "h", ".", "sequence", ")", "for", "h", "in", "hyps", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.opennmt_pred_to_dec.main": [[13, 33], ["os.makedirs", "os.path.join", "os.makedirs", "print", "open", "f_in.readlines", "pred_summary.strip().replace().replace().strip.strip().replace().replace().strip", "nltk.tokenize.sent_tokenize", "open", "json.dump", "open", "f.write", "os.path.join", "pred_summary.strip().replace().replace().strip.strip().replace().replace", "os.path.join", "decoding.make_html_safe", "pred_summary.strip().replace().replace().strip.strip().replace", "pred_summary.strip().replace().replace().strip.strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["def", "main", "(", "pred_path", ",", "out_dir", ")", ":", "\n", "    ", "with", "open", "(", "pred_path", ")", "as", "f_in", ":", "\n", "        ", "pred_lines", "=", "f_in", ".", "readlines", "(", ")", "\n", "", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "out_dec_dir", "=", "join", "(", "out_dir", ",", "'output'", ")", "\n", "os", ".", "makedirs", "(", "out_dec_dir", ")", "\n", "num_exported_samples", "=", "0", "\n", "for", "pred_summary", "in", "pred_lines", ":", "\n", "        ", "pred_summary", "=", "pred_summary", ".", "strip", "(", ")", ".", "replace", "(", "'<t>'", ",", "''", ")", ".", "replace", "(", "'</t>'", ",", "''", ")", ".", "strip", "(", ")", "\n", "pred_summary_sent_list", "=", "nltk", ".", "tokenize", ".", "sent_tokenize", "(", "pred_summary", ")", "\n", "with", "open", "(", "join", "(", "out_dec_dir", ",", "'{}.dec'", ".", "format", "(", "num_exported_samples", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "pred_summary_sent_list", ")", ")", ")", "\n", "", "num_exported_samples", "+=", "1", "\n", "\n", "", "log_dict", "=", "{", "\"split\"", ":", "\"test\"", "}", "\n", "\n", "with", "open", "(", "join", "(", "out_dir", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "log_dict", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "", "print", "(", "\"Decoded {} samples.\"", ".", "format", "(", "num_exported_samples", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.count_data": [[11, 18], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["def", "count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab": [[27, 36], ["enumerate", "wc.most_common"], "function", ["None"], ["def", "make_vocab", "(", "wc", ",", "vocab_size", ")", ":", "\n", "    ", "word2id", ",", "id2word", "=", "{", "}", ",", "{", "}", "\n", "word2id", "[", "'<pad>'", "]", "=", "PAD", "\n", "word2id", "[", "'<unk>'", "]", "=", "UNK", "\n", "word2id", "[", "'<start>'", "]", "=", "START", "\n", "word2id", "[", "'<end>'", "]", "=", "END", "\n", "for", "i", ",", "(", "w", ",", "_", ")", "in", "enumerate", "(", "wc", ".", "most_common", "(", "vocab_size", ")", ",", "4", ")", ":", "\n", "        ", "word2id", "[", "w", "]", "=", "i", "\n", "", "return", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab_cond": [[38, 48], ["enumerate", "wc.most_common"], "function", ["None"], ["", "def", "make_vocab_cond", "(", "wc", ",", "vocab_size", ")", ":", "\n", "    ", "word2id", ",", "id2word", "=", "{", "}", ",", "{", "}", "\n", "word2id", "[", "'<pad>'", "]", "=", "PAD", "\n", "word2id", "[", "'<unk>'", "]", "=", "UNK", "\n", "word2id", "[", "'<start>'", "]", "=", "START", "\n", "word2id", "[", "'<end>'", "]", "=", "END", "\n", "word2id", "[", "'<empty_mem>'", "]", "=", "EMPTY_MEM", "\n", "for", "i", ",", "(", "w", ",", "_", ")", "in", "enumerate", "(", "wc", ".", "most_common", "(", "vocab_size", ")", ",", "5", ")", ":", "\n", "        ", "word2id", "[", "w", "]", "=", "i", "\n", "", "return", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding": [[50, 72], ["os.path.basename().split", "len", "int", "gensim.models.Word2Vec.load", "torch.nn.Embedding", "initializer", "torch.no_grad", "range", "os.path.basename", "len", "torch.Tensor", "torch.Tensor", "torch.Tensor", "oovs.append"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "def", "make_embedding", "(", "id2word", ",", "w2v_file", ",", "initializer", "=", "None", ")", ":", "\n", "    ", "attrs", "=", "basename", "(", "w2v_file", ")", ".", "split", "(", "'.'", ")", "#word2vec.{dim}d.{vsize}k.bin", "\n", "w2v", "=", "gensim", ".", "models", ".", "Word2Vec", ".", "load", "(", "w2v_file", ")", ".", "wv", "\n", "vocab_size", "=", "len", "(", "id2word", ")", "\n", "emb_dim", "=", "int", "(", "attrs", "[", "-", "3", "]", "[", ":", "-", "1", "]", ")", "\n", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "emb_dim", ")", ".", "weight", "\n", "if", "initializer", "is", "not", "None", ":", "\n", "        ", "initializer", "(", "embedding", ")", "\n", "\n", "", "oovs", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "id2word", ")", ")", ":", "\n", "# NOTE: id2word can be list or dict", "\n", "            ", "if", "i", "==", "START", ":", "\n", "                ", "embedding", "[", "i", ",", ":", "]", "=", "torch", ".", "Tensor", "(", "w2v", "[", "'<s>'", "]", ")", "\n", "", "elif", "i", "==", "END", ":", "\n", "                ", "embedding", "[", "i", ",", ":", "]", "=", "torch", ".", "Tensor", "(", "w2v", "[", "r'<\\s>'", "]", ")", "\n", "", "elif", "id2word", "[", "i", "]", "in", "w2v", ":", "\n", "                ", "embedding", "[", "i", ",", ":", "]", "=", "torch", ".", "Tensor", "(", "w2v", "[", "id2word", "[", "i", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "oovs", ".", "append", "(", "i", ")", "\n", "", "", "", "return", "embedding", ",", "oovs", "\n", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.MatchDataset.__init__": [[59, 62], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "filter_out_low_recall", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "filter_out_low_recall", "=", "filter_out_low_recall", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.MatchDataset.__getitem__": [[63, 86], ["data.data.CnnDmDataset.__getitem__", "enumerate", "zip", "matched_arts.append", "matched_abss.append", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", ",", "scores", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted'", "]", ",", "js_data", "[", "'score'", "]", ")", "\n", "\n", "if", "self", ".", "filter_out_low_recall", ":", "\n", "            ", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "\n", "for", "ext_i", ",", "(", "ext", ",", "abst", ")", "in", "enumerate", "(", "zip", "(", "extracts", ",", "abs_sents", ")", ")", ":", "\n", "                ", "if", "scores", "[", "ext_i", "]", ">=", "0.2", ":", "\n", "                    ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "]", ")", "\n", "matched_abss", ".", "append", "(", "abst", ")", "\n", "", "", "", "else", ":", "\n", "            ", "matched_arts", "=", "[", "art_sents", "[", "i", "]", "for", "i", "in", "extracts", "]", "\n", "matched_abss", "=", "abs_sents", "[", ":", "len", "(", "extracts", ")", "]", "\n", "\n", "#print(\"matched_arts: \")", "\n", "#print(matched_arts)", "\n", "#print(\"matched_abss: \")", "\n", "#print(matched_abss)", "\n", "\n", "", "return", "matched_arts", ",", "matched_abss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net": [[88, 104], ["model.copy_summ.CopySumm", "sum", "print", "p.numel", "model.copy_summ.CopySumm.parameters"], "function", ["None"], ["", "", "def", "configure_net", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ")", ":", "\n", "    ", "net_args", "=", "{", "}", "\n", "net_args", "[", "'vocab_size'", "]", "=", "vocab_size", "\n", "net_args", "[", "'emb_dim'", "]", "=", "emb_dim", "\n", "net_args", "[", "'n_hidden'", "]", "=", "n_hidden", "\n", "net_args", "[", "'bidirectional'", "]", "=", "bidirectional", "\n", "net_args", "[", "'n_layer'", "]", "=", "n_layer", "\n", "\n", "net", "=", "CopySumm", "(", "**", "net_args", ")", "\n", "\n", "# print number of trainable parameters", "\n", "trainable_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "net", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "\"Number of trainable parameters: {}\"", ".", "format", "(", "trainable_params", ")", ")", "\n", "\n", "return", "net", ",", "net_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training": [[106, 123], ["torch.nn.functional.nll_loss", "model.util.sequence_loss"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss"], ["", "def", "configure_training", "(", "opt", ",", "lr", ",", "clip_grad", ",", "lr_decay", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\" supports Adam optimizer only\"\"\"", "\n", "assert", "opt", "in", "[", "'adam'", "]", "\n", "opt_kwargs", "=", "{", "}", "\n", "opt_kwargs", "[", "'lr'", "]", "=", "lr", "\n", "\n", "train_params", "=", "{", "}", "\n", "train_params", "[", "'optimizer'", "]", "=", "(", "opt", ",", "opt_kwargs", ")", "\n", "train_params", "[", "'clip_grad_norm'", "]", "=", "clip_grad", "\n", "train_params", "[", "'batch_size'", "]", "=", "batch_size", "\n", "train_params", "[", "'lr_decay'", "]", "=", "lr_decay", "\n", "\n", "nll", "=", "lambda", "logit", ",", "target", ":", "F", ".", "nll_loss", "(", "logit", ",", "target", ",", "reduce", "=", "False", ")", "\n", "def", "criterion", "(", "logits", ",", "targets", ")", ":", "\n", "        ", "return", "sequence_loss", "(", "logits", ",", "targets", ",", "nll", ",", "pad_idx", "=", "PAD", ")", "\n", "\n", "", "return", "criterion", ",", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers": [[124, 151], ["data.batcher.prepro_fn", "cytoolz.compose", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "torch.utils.data.DataLoader", "data.batcher.BucketedGenerater", "data.batcher.batchify_fn_copy", "data.batcher.convert_batch_copy", "train_abstractor.MatchDataset", "train_abstractor.MatchDataset", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy"], ["", "def", "build_batchers", "(", "word2id", ",", "cuda", ",", "filter_out_low_recall", ",", "debug", ")", ":", "\n", "    ", "prepro", "=", "prepro_fn", "(", "args", ".", "max_art", ",", "args", ".", "max_abs", ")", "\n", "def", "sort_key", "(", "sample", ")", ":", "\n", "        ", "src", ",", "target", "=", "sample", "\n", "return", "(", "len", "(", "target", ")", ",", "len", "(", "src", ")", ")", "\n", "", "batchify", "=", "compose", "(", "\n", "batchify_fn_copy", "(", "PAD", ",", "START", ",", "END", ",", "cuda", "=", "cuda", ")", ",", "# padding", "\n", "convert_batch_copy", "(", "UNK", ",", "word2id", ")", "# convert to idx", "\n", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "\n", "MatchDataset", "(", "'train'", ",", "filter_out_low_recall", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "not", "debug", ",", "\n", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn", "\n", ")", "\n", "train_batcher", "=", "BucketedGenerater", "(", "train_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "False", ",", "fork", "=", "not", "debug", ")", "\n", "\n", "val_loader", "=", "DataLoader", "(", "\n", "MatchDataset", "(", "'val'", ",", "filter_out_low_recall", ")", ",", "batch_size", "=", "BUCKET_SIZE", ",", "\n", "shuffle", "=", "False", ",", "num_workers", "=", "4", "if", "cuda", "and", "not", "debug", "else", "0", ",", "\n", "collate_fn", "=", "coll_fn", "\n", ")", "\n", "val_batcher", "=", "BucketedGenerater", "(", "val_loader", ",", "prepro", ",", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "fork", "=", "not", "debug", ")", "\n", "return", "train_batcher", ",", "val_batcher", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.main": [[152, 207], ["utils.make_vocab", "train_abstractor.build_batchers", "train_abstractor.configure_net", "train_abstractor.configure_training", "training.basic_validate", "training.get_basic_grad_fn", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "training.BasicPipeline", "training.BasicTrainer", "print", "print", "training.BasicTrainer.train", "open", "pickle.load", "len", "utils.make_embedding", "net.cuda.set_embedding", "os.path.exists", "os.makedirs", "open", "pickle.dump", "open", "json.dump", "net.cuda.parameters", "net.cuda.cuda", "os.path.join", "os.path.join", "os.path.join", "utils.make_vocab.items"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_vocab", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.build_batchers", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_net", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.train_abstractor.configure_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.basic_validate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.get_basic_grad_fn", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.utils.make_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "main", "(", "args", ")", ":", "\n", "# create data batcher, vocabulary", "\n", "# batcher", "\n", "    ", "with", "open", "(", "join", "(", "DATA_DIR", ",", "'vocab_cnt.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "wc", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "word2id", "=", "make_vocab", "(", "wc", ",", "args", ".", "vsize", ")", "\n", "train_batcher", ",", "val_batcher", "=", "build_batchers", "(", "word2id", ",", "\n", "args", ".", "cuda", ",", "args", ".", "filter_out_low_recall", ",", "args", ".", "debug", ")", "\n", "\n", "# make net", "\n", "net", ",", "net_args", "=", "configure_net", "(", "len", "(", "word2id", ")", ",", "args", ".", "emb_dim", ",", "\n", "args", ".", "n_hidden", ",", "args", ".", "bi", ",", "args", ".", "n_layer", ")", "\n", "if", "args", ".", "w2v", ":", "\n", "# NOTE: the pretrained embedding having the same dimension", "\n", "#       as args.emb_dim should already be trained", "\n", "        ", "embedding", ",", "_", "=", "make_embedding", "(", "\n", "{", "i", ":", "w", "for", "w", ",", "i", "in", "word2id", ".", "items", "(", ")", "}", ",", "args", ".", "w2v", ")", "\n", "net", ".", "set_embedding", "(", "embedding", ")", "\n", "\n", "# configure training setting", "\n", "", "criterion", ",", "train_params", "=", "configure_training", "(", "\n", "'adam'", ",", "args", ".", "lr", ",", "args", ".", "clip", ",", "args", ".", "decay", ",", "args", ".", "batch", "\n", ")", "\n", "\n", "# save experiment setting", "\n", "if", "not", "exists", "(", "args", ".", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "path", ")", "\n", "", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'vocab.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "word2id", ",", "f", ",", "pkl", ".", "HIGHEST_PROTOCOL", ")", "\n", "", "meta", "=", "{", "}", "\n", "meta", "[", "'net'", "]", "=", "'base_abstractor'", "\n", "meta", "[", "'net_args'", "]", "=", "net_args", "\n", "meta", "[", "'traing_params'", "]", "=", "train_params", "\n", "with", "open", "(", "join", "(", "args", ".", "path", ",", "'meta.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "meta", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# prepare trainer", "\n", "", "val_fn", "=", "basic_validate", "(", "net", ",", "criterion", ")", "\n", "grad_fn", "=", "get_basic_grad_fn", "(", "net", ",", "args", ".", "clip", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "**", "train_params", "[", "'optimizer'", "]", "[", "1", "]", ")", "\n", "scheduler", "=", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ",", "verbose", "=", "True", ",", "\n", "factor", "=", "args", ".", "decay", ",", "min_lr", "=", "0", ",", "\n", "patience", "=", "args", ".", "lr_p", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "", "pipeline", "=", "BasicPipeline", "(", "meta", "[", "'net'", "]", ",", "net", ",", "\n", "train_batcher", ",", "val_batcher", ",", "args", ".", "batch", ",", "val_fn", ",", "\n", "criterion", ",", "optimizer", ",", "grad_fn", ")", "\n", "trainer", "=", "BasicTrainer", "(", "pipeline", ",", "args", ".", "path", ",", "\n", "args", ".", "ckpt_freq", ",", "args", ".", "patience", ",", "scheduler", ")", "\n", "\n", "print", "(", "'start training with the following hyper-parameters:'", ")", "\n", "print", "(", "meta", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_amazon_data._count_data": [[9, 16], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.preprocess_amazon_data.extract_source": [[18, 33], ["preprocess_amazon_data._count_data", "os.makedirs", "range", "print", "os.path.join", "os.path.join", "json.load", "json.load.pop", "json.load.pop", "open", "open", "json.dump", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "extract_source", "(", "data_dir", ",", "split", ",", "out_dir", ")", ":", "\n", "    ", "n_data", "=", "_count_data", "(", "join", "(", "data_dir", ",", "split", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "out_dir", ",", "split", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "js", "=", "json", ".", "load", "(", "open", "(", "join", "(", "data_dir", ",", "split", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "js", "[", "'article'", "]", "=", "js", "[", "'reviewText'", "]", "\n", "js", "[", "'abstract'", "]", "=", "js", "[", "'summary'", "]", "\n", "js", ".", "pop", "(", "'reviewText'", ",", "None", ")", "\n", "js", ".", "pop", "(", "'summary'", ",", "None", ")", "\n", "\n", "with", "open", "(", "join", "(", "out_dir", ",", "split", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "js", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "", "", "print", "(", "\"Finished split: {}\"", ".", "format", "(", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.eval_full_model_pubmed.main": [[16, 39], ["os.path.join", "os.path.join", "os.path.exists", "print", "open", "evaluate.eval_rouge", "evaluate.eval_meteor", "open", "f.write", "os.path.join", "json.loads", "os.path.join", "f.read"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_rouge", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.evaluate.eval_meteor"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "dec_dir", "=", "join", "(", "args", ".", "decode_dir", ",", "'output'", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'log.json'", ")", ")", "as", "f", ":", "\n", "        ", "split", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "[", "'split'", "]", "\n", "", "ref_dir", "=", "join", "(", "_DATA_DIR", ",", "'refs'", ",", "split", ")", "\n", "assert", "exists", "(", "ref_dir", ")", "\n", "\n", "if", "args", ".", "rouge", ":", "\n", "        ", "dec_pattern", "=", "r'(\\d+).dec'", "\n", "ref_pattern", "=", "'#ID#.ref'", "\n", "output", "=", "eval_rouge", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ",", "\n", "cmd", "=", "'-a -c 95 -r 1000 -f A -n 2'", ")", "\n", "#-c 95 -r 1000 -n 2 -m'", "\n", "# ROUGE-1.5.5 -e data -a -n 2 -r 1000 -f A -z SPL config_file", "\n", "metric", "=", "'rouge'", "\n", "", "else", ":", "\n", "        ", "dec_pattern", "=", "'[0-9]+.dec'", "\n", "ref_pattern", "=", "'[0-9]+.ref'", "\n", "output", "=", "eval_meteor", "(", "dec_pattern", ",", "dec_dir", ",", "ref_pattern", ",", "ref_dir", ")", "\n", "metric", "=", "'meteor'", "\n", "", "print", "(", "output", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "decode_dir", ",", "'{}.txt'", ".", "format", "(", "metric", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.tokenize_duc._count_data": [[15, 22], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.tokenize_duc.main": [[24, 46], ["os.path.join", "os.makedirs", "os.path.join", "os.makedirs", "tokenize_duc._count_data", "range", "json.load", "open", "tokenized_summary_list.append", "open", "json.dump", "os.path.join", "corenlp_parser.tokenize", "os.path.join", "doc_sent.strip", "corenlp_parser.tokenize", "summary_sent.strip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "main", "(", "data_dir", ",", "out_dir", ")", ":", "\n", "    ", "split_dir", "=", "join", "(", "data_dir", ",", "'test'", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "out_test_dir", "=", "join", "(", "out_dir", ",", "'test'", ")", "\n", "os", ".", "makedirs", "(", "out_test_dir", ")", "\n", "n_data", "=", "_count_data", "(", "split_dir", ")", "\n", "for", "i", "in", "range", "(", "n_data", ")", ":", "\n", "        ", "js", "=", "json", ".", "load", "(", "open", "(", "join", "(", "split_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "\n", "document", "=", "js", "[", "'article'", "]", "\n", "tokenized_document", "=", "[", "' '", ".", "join", "(", "corenlp_parser", ".", "tokenize", "(", "doc_sent", ".", "strip", "(", ")", ")", ")", "for", "doc_sent", "in", "document", "]", "\n", "summary_list", "=", "js", "[", "'abstract'", "]", "\n", "\n", "tokenized_summary_list", "=", "[", "]", "\n", "for", "summary", "in", "summary_list", ":", "\n", "            ", "tokenized_summary", "=", "[", "' '", ".", "join", "(", "corenlp_parser", ".", "tokenize", "(", "summary_sent", ".", "strip", "(", ")", ")", ")", "for", "summary_sent", "in", "summary", "]", "\n", "tokenized_summary_list", ".", "append", "(", "tokenized_summary", ")", "\n", "", "js", "[", "'article'", "]", "=", "tokenized_document", "\n", "js", "[", "'abstract'", "]", "=", "tokenized_summary_list", "\n", "\n", "with", "open", "(", "join", "(", "out_test_dir", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "js", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.TwoToOneMatchDataset.__init__": [[38, 41], ["data.data.CnnDmDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "threshold", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "DATA_DIR", ")", "\n", "self", ".", "threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.TwoToOneMatchDataset.__getitem__": [[42, 64], ["data.data.CnnDmDataset.__getitem__", "zip", "matched_abss.append", "len", "matched_arts.append", "len", "matched_arts.append", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "js_data", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "art_sents", ",", "abs_sents", ",", "extracts", "=", "(", "\n", "js_data", "[", "'article'", "]", ",", "js_data", "[", "'abstract'", "]", ",", "js_data", "[", "'extracted_two_to_one_{}'", ".", "format", "(", "self", ".", "threshold", ")", "]", ")", "\n", "# only keep the sentences with two ext labels", "\n", "matched_arts", "=", "[", "]", "\n", "matched_abss", "=", "[", "]", "\n", "for", "ext", ",", "abst", "in", "zip", "(", "extracts", ",", "abs_sents", ")", ":", "\n", "            ", "if", "len", "(", "ext", ")", "==", "2", ":", "\n", "                ", "primary_idx", "=", "ext", "[", "0", "]", "\n", "if", "primary_idx", ">", "0", ":", "\n", "                    ", "secondary_idx", "=", "primary_idx", "-", "1", "\n", "", "else", ":", "\n", "                    ", "secondary_idx", "=", "-", "1", "\n", "", "matched_arts", ".", "append", "(", "art_sents", "[", "primary_idx", "]", "+", "' '", "+", "art_sents", "[", "secondary_idx", "]", ")", "\n", "", "elif", "len", "(", "ext", ")", "==", "1", ":", "\n", "                ", "matched_arts", ".", "append", "(", "art_sents", "[", "ext", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Bug!\"", ")", "\n", "", "matched_abss", ".", "append", "(", "abst", ")", "\n", "# return matched_arts, abs_sents[:len(extracts)]", "\n", "", "return", "matched_arts", ",", "matched_abss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.coll": [[66, 73], ["enumerate", "filtered_art_batch.append", "filtered_abs_batch.append"], "function", ["None"], ["", "", "def", "coll", "(", "batch", ")", ":", "\n", "    ", "filtered_art_batch", "=", "[", "]", "\n", "filtered_abs_batch", "=", "[", "]", "\n", "for", "batch_i", ",", "(", "art_sents", ",", "abs_sents", ")", "in", "enumerate", "(", "batch", ")", ":", "\n", "        ", "filtered_art_batch", ".", "append", "(", "art_sents", ")", "\n", "filtered_abs_batch", ".", "append", "(", "abs_sents", ")", "\n", "", "return", "filtered_art_batch", ",", "filtered_abs_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.decode": [[75, 240], ["time.time", "decode_two_to_one_previous_neighbor.TwoToOneMatchDataset", "len", "torch.utils.data.DataLoader", "os.makedirs", "os.makedirs", "json.load", "print", "decoding.Abstractor", "decoding.BeamAbstractor", "list", "os.path.join", "open", "open", "json.dump", "torch.no_grad", "enumerate", "range", "os.path.join", "os.path.join", "list", "len", "print", "print", "print", "print", "print", "map", "print", "print", "print", "extractor", "list", "print", "print", "decoding.BeamAbstractor.", "decode_two_to_one_previous_neighbor.rerank_mp", "decoding.BeamAbstractor.", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "data.batcher.tokenize", "map", "len", "print", "enumerate", "print", "range", "print", "print", "open", "f.write", "len", "len", "len", "print", "len", "os.path.join", "decoding.make_html_safe", "datetime.timedelta", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decoding.make_html_safe"], ["", "def", "decode", "(", "save_path", ",", "abs_dir", ",", "split", ",", "batch_size", ",", "beam_size", ",", "diverse", ",", "max_len", ",", "final_rerank", ",", "cuda", ",", "debug", "=", "False", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "topk", "=", "1", "\n", "\n", "# setup model", "\n", "assert", "abs_dir", "is", "not", "None", "\n", "if", "beam_size", "==", "1", ":", "\n", "        ", "abstractor", "=", "Abstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "", "else", ":", "\n", "        ", "abstractor", "=", "BeamAbstractor", "(", "abs_dir", ",", "max_len", ",", "cuda", ")", "\n", "\n", "# a dummy extractor that extract all the sentences", "\n", "", "extractor", "=", "lambda", "art_sents", ":", "list", "(", "range", "(", "len", "(", "art_sents", ")", ")", ")", "\n", "\n", "dataset", "=", "TwoToOneMatchDataset", "(", "split", ",", "threshold", "=", "0.15", ")", "# only need json['article'] and json['abstract']", "\n", "\n", "n_data", "=", "len", "(", "dataset", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ",", "\n", "collate_fn", "=", "coll", "\n", ")", "\n", "\n", "# prepare save paths and logs", "\n", "os", ".", "makedirs", "(", "save_path", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "save_path", ",", "'output'", ")", ")", "\n", "dec_log", "=", "{", "}", "\n", "dec_log", "[", "'abstractor'", "]", "=", "(", "json", ".", "load", "(", "open", "(", "join", "(", "abs_dir", ",", "'meta.json'", ")", ")", ")", ")", "\n", "dec_log", "[", "'extractor'", "]", "=", "None", "\n", "dec_log", "[", "'rl'", "]", "=", "False", "\n", "dec_log", "[", "'split'", "]", "=", "split", "\n", "dec_log", "[", "'beam'", "]", "=", "beam_size", "\n", "dec_log", "[", "'diverse'", "]", "=", "diverse", "\n", "with", "open", "(", "join", "(", "save_path", ",", "'log.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dec_log", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "# Decoding", "\n", "", "i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_debug", ",", "(", "raw_article_batch", ",", "raw_abs_batch", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "debug", ":", "\n", "                ", "print", "(", "\"raw article batch\"", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "raw_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "print", "(", "\"article lengths\"", ")", "\n", "print", "(", "[", "len", "(", "art", ")", "for", "art", "in", "raw_article_batch", "]", ")", "\n", "# pick out the original sentence", "\n", "# raw_article_batch a list of list of sentences, article, then sentence in article", "\n", "#raw_original_article_batch = []", "\n", "#for raw_article_sents in raw_article_batch:", "\n", "#    original_article_sents = [article_sent for cand_i, article_sent in enumerate(raw_article_sents) if cand_i % exist_candidates == 0]", "\n", "#    raw_original_article_batch.append(original_article_sents)", "\n", "\n", "", "tokenized_original_article_batch", "=", "list", "(", "map", "(", "tokenize", "(", "None", ")", ",", "raw_article_batch", ")", ")", "\n", "if", "debug", ":", "\n", "                ", "print", "(", "\"tokenized_original_article_batch\"", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "tokenized_original_article_batch", "[", "0", "]", "[", "1", "]", ")", "\n", "\n", "", "ext_arts", "=", "[", "]", "\n", "ext_inds", "=", "[", "]", "\n", "for", "raw_art_sents", "in", "tokenized_original_article_batch", ":", "\n", "                ", "ext", "=", "extractor", "(", "raw_art_sents", ")", "\n", "ext_inds", "+=", "[", "(", "len", "(", "ext_arts", ")", ",", "len", "(", "ext", ")", ")", "]", "\n", "ext_arts", "+=", "list", "(", "map", "(", "lambda", "i", ":", "raw_art_sents", "[", "i", "]", ",", "ext", ")", ")", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"ext_inds\"", ")", "\n", "print", "(", "ext_inds", ")", "\n", "", "if", "beam_size", ">", "1", ":", "\n", "                ", "all_beams", "=", "abstractor", "(", "ext_arts", ",", "beam_size", ",", "diverse", ")", "# a list of beam for the whole batch", "\n", "dec_outs", "=", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "topk", ",", "final_rerank", ")", "\n", "# dec_outs: a list of list of token list", "\n", "", "else", ":", "\n", "                ", "dec_outs", "=", "abstractor", "(", "ext_arts", ")", "\n", "\n", "#assert i == batch_size * i_debug", "\n", "", "if", "i", "!=", "batch_size", "*", "i_debug", ":", "\n", "                ", "print", "(", "\"i: {}\"", ".", "format", "(", "i", ")", ")", "\n", "print", "(", "\"batch_size: {}, i_debug: {}, batch_size * i_debug: {}\"", ".", "format", "(", "batch_size", ",", "i_debug", ",", "batch_size", "*", "i_debug", ")", ")", "\n", "raise", "ValueError", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "\"dec_outs[0]\"", ")", "\n", "print", "(", "dec_outs", "[", "0", "]", ")", "\n", "print", "(", "\"dec_outs[1]\"", ")", "\n", "print", "(", "dec_outs", "[", "1", "]", ")", "\n", "print", "(", "\"dec_outs[2]\"", ")", "\n", "print", "(", "dec_outs", "[", "2", "]", ")", "\n", "print", "(", "\"dec_outs[3]\"", ")", "\n", "print", "(", "dec_outs", "[", "3", "]", ")", "\n", "print", "(", "\"length of dec_out\"", ")", "\n", "print", "(", "len", "(", "dec_outs", ")", ")", "\n", "print", "(", "\"article output\"", ")", "\n", "\n", "", "\"\"\"\n            if i_debug == 18:\n                print(\"Length of ext_ids: {}\".format(len(ext_inds)))\n                print(\"Length of raw_rticle_batch: {}\".format(len(raw_article_batch)))\n                print(\"Length of tokenized_article_batch: {}\".format(len(list(tokenized_article_batch))))\n                print(\"i: {}\".format(i))\n            \"\"\"", "\n", "\n", "batch_i", "=", "0", "\n", "for", "j", ",", "n", "in", "ext_inds", ":", "\n", "\n", "                ", "if", "debug", ":", "\n", "                    ", "print", "(", "\"j: {}, n: {}\"", ".", "format", "(", "j", ",", "n", ")", ")", "\n", "\n", "# one article", "\n", "", "article_decoded_sents", "=", "[", "]", "# a list of all candidate sentences for one article", "\n", "\n", "if", "j", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# if the input article is not empty", "\n", "\n", "# construct a list of all candidate sentences in one article, a list of str.", "\n", "                    ", "for", "sent_i", ",", "sent", "in", "enumerate", "(", "dec_outs", "[", "j", ":", "j", "+", "n", "]", ")", ":", "\n", "                        ", "candidate_list", "=", "[", "]", "\n", "\n", "# one sent", "\n", "if", "beam_size", ">", "1", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "candidate", ")", "for", "candidate", "in", "sent", "]", "\n", "", "else", ":", "\n", "                            ", "candidate_list", "+=", "[", "' '", ".", "join", "(", "sent", ")", "]", "\n", "\n", "#if keep_original_sent:", "\n", "#    candidate_list.insert(0, raw_article_batch[batch_i][sent_i])", "\n", "\n", "", "article_decoded_sents", "+=", "candidate_list", "\n", "# fetch the abstract of the original sample", "\n", "", "raw_abstract", "=", "raw_abs_batch", "[", "batch_i", "]", "\n", "batch_i", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raw_abstract", "=", "[", "]", "\n", "\n", "", "if", "debug", ":", "\n", "                    ", "print", "(", "\"article_decoded_sents[0]\"", ")", "\n", "for", "z", "in", "range", "(", "9", ")", ":", "\n", "                        ", "print", "(", "article_decoded_sents", "[", "z", "]", ")", "\n", "", "print", "(", "\"article_decoded_sents len\"", ")", "\n", "print", "(", "len", "(", "article_decoded_sents", ")", ")", "\n", "\n", "", "with", "open", "(", "join", "(", "save_path", ",", "'output'", ",", "'{}.dec'", ".", "format", "(", "i", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "make_html_safe", "(", "'\\n'", ".", "join", "(", "article_decoded_sents", ")", ")", ")", "\n", "\n", "", "i", "+=", "1", "\n", "\n", "\"\"\"\n                if i_debug == 18:\n                    art_len = len(json_dict['abstract'])\n                    print(\"length of current article: {}\".format(art_len))\n                    if art_len > 0:\n                        print(json_dict['abstract'][0])\n                    print(\"i increases to: {}\\n\".format(i))\n                \"\"\"", "\n", "\n", "print", "(", "'{}/{} ({:.2f}%) decoded in {} seconds\\r'", ".", "format", "(", "\n", "i", ",", "n_data", ",", "i", "/", "n_data", "*", "100", ",", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", "\n", ")", ",", "end", "=", "''", ")", "\n", "\n", "if", "debug", ":", "\n", "                    ", "raise", "ValueError", "\n", "", "", "\"\"\"\n            if i_debug == 18:\n                raise ValueError\n            \"\"\"", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank": [[248, 256], ["list", "map", "map", "cytoolz.concat", "decode_two_to_one_previous_neighbor.rereank_topk_one", "decode_two_to_one_previous_neighbor.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["def", "rerank", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "(", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", ")", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "if", "final_rerank", ":", "\n", "        ", "topked", "=", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "        ", "topked", "=", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rerank_mp": [[258, 267], ["list", "torch.multiprocessing.Pool", "cytoolz.concat", "pool.map", "pool.map", "decode_two_to_one_previous_neighbor.rereank_topk_one", "decode_two_to_one_previous_neighbor.topk_one"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one"], ["", "def", "rerank_mp", "(", "all_beams", ",", "ext_inds", ",", "k", ",", "final_rerank", "=", "False", ")", ":", "\n", "    ", "beam_lists", "=", "[", "all_beams", "[", "i", ":", "i", "+", "n", "]", "for", "i", ",", "n", "in", "ext_inds", "if", "n", ">", "0", "]", "\n", "# a list of beam list, each beam list contains the beam for one article", "\n", "with", "mp", ".", "Pool", "(", "8", ")", "as", "pool", ":", "\n", "        ", "if", "final_rerank", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "rereank_topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "else", ":", "\n", "            ", "topked", "=", "pool", ".", "map", "(", "topk_one", "(", "k", "=", "k", ")", ",", "beam_lists", ")", "\n", "", "", "return", "list", "(", "concat", "(", "topked", ")", ")", "# a list contains the candidates sentences for all articles in the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.rereank_topk_one": [[269, 287], ["map", "decode_two_to_one_previous_neighbor.rereank_topk_one.process_beam"], "function", ["None"], ["", "@", "curry", "\n", "def", "rereank_topk_one", "(", "beams", ",", "k", ")", ":", "\n", "    ", "\"\"\"\n    :param beams: a list of beam in one article\n    :param k:\n    :return: art_dec_outs: a list of list of token list, len(art_dec_outs)=num_sents_in_article, len(art_dec_outs[0])=num_cands_in_sent_0\n    \"\"\"", "\n", "@", "curry", "\n", "def", "process_beam", "(", "beam", ",", "n", ")", ":", "\n", "        ", "for", "b", "in", "beam", "[", ":", "n", "]", ":", "\n", "            ", "b", ".", "gram_cnt", "=", "Counter", "(", "_make_n_gram", "(", "b", ".", "sequence", ")", ")", "\n", "", "return", "beam", "[", ":", "n", "]", "\n", "", "beams", "=", "map", "(", "process_beam", "(", "n", "=", "_PRUNE", "[", "len", "(", "beams", ")", "]", ")", ",", "beams", ")", "\n", "beams_with_topk_hyps", "=", "[", "heapq", ".", "nlargest", "(", "k", ",", "hyps", ",", "key", "=", "_compute_score", ")", "for", "hyps", "in", "beams", "]", "\n", "art_dec_outs", "=", "[", "]", "\n", "for", "topk_hyps", "in", "beams_with_topk_hyps", ":", "\n", "        ", "art_dec_outs", ".", "append", "(", "[", "h", ".", "sequence", "for", "h", "in", "topk_hyps", "]", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor.topk_one": [[289, 297], ["art_dec_outs.append"], "function", ["None"], ["", "@", "curry", "\n", "def", "topk_one", "(", "beams", ",", "k", ")", ":", "\n", "# beams: a list of beam in one article", "\n", "    ", "art_dec_outs", "=", "[", "]", "# a list of token list for an article, each token list is a candidate sentence", "\n", "for", "hyps", "in", "beams", ":", "# hypotheses for each input sentence", "\n", "        ", "sent_candidates", "=", "[", "h", ".", "sequence", "for", "h", "in", "hyps", "[", ":", "k", "]", "]", "\n", "art_dec_outs", ".", "append", "(", "sent_candidates", ")", "\n", "", "return", "art_dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor._make_n_gram": [[299, 301], ["tuple", "range", "len"], "function", ["None"], ["", "def", "_make_n_gram", "(", "sequence", ",", "n", "=", "2", ")", ":", "\n", "    ", "return", "(", "tuple", "(", "sequence", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sequence", ")", "-", "(", "n", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.decode_two_to_one_previous_neighbor._compute_score": [[303, 307], ["sum", "len", "hyp.gram_cnt.items"], "function", ["None"], ["", "def", "_compute_score", "(", "hyp", ")", ":", "\n", "    ", "repeat", "=", "sum", "(", "c", "-", "1", "for", "g", ",", "c", "in", "hyp", ".", "gram_cnt", ".", "items", "(", ")", "if", "c", ">", "1", ")", "\n", "lp", "=", "hyp", ".", "logprob", "/", "len", "(", "hyp", ".", "sequence", ")", "\n", "return", "(", "-", "repeat", ",", "lp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.StackedLSTMCells.__init__": [[62, 66], ["torch.nn.Module.__init__", "torch.nn.ModuleList"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "cells", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_cells", "=", "nn", ".", "ModuleList", "(", "cells", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.StackedLSTMCells.forward": [[67, 91], ["enumerate", "torch.stack", "torch.stack", "cell", "hs.append", "cs.append", "torch.nn.functional.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_", ",", "state", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n            input_: FloatTensor (batch, input_size)\n            states: tuple of the H, C LSTM states\n                FloatTensor (num_layers, batch, hidden_size)\n        Returns:\n            LSTM states\n            new_h: (num_layers, batch, hidden_size)\n            new_c: (num_layers, batch, hidden_size)\n        \"\"\"", "\n", "hs", "=", "[", "]", "\n", "cs", "=", "[", "]", "\n", "for", "i", ",", "cell", "in", "enumerate", "(", "self", ".", "_cells", ")", ":", "\n", "            ", "s", "=", "(", "state", "[", "0", "]", "[", "i", ",", ":", ",", ":", "]", ",", "state", "[", "1", "]", "[", "i", ",", ":", ",", ":", "]", ")", "\n", "h", ",", "c", "=", "cell", "(", "input_", ",", "s", ")", "\n", "hs", ".", "append", "(", "h", ")", "\n", "cs", ".", "append", "(", "c", ")", "\n", "input_", "=", "F", ".", "dropout", "(", "h", ",", "p", "=", "self", ".", "_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "", "new_h", "=", "torch", ".", "stack", "(", "hs", ",", "dim", "=", "0", ")", "\n", "new_c", "=", "torch", ".", "stack", "(", "cs", ",", "dim", "=", "0", ")", "\n", "\n", "return", "new_h", ",", "new_c", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.StackedLSTMCells.hidden_size": [[92, 95], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "hidden_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_cells", "[", "0", "]", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.StackedLSTMCells.input_size": [[96, 99], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "input_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_cells", "[", "0", "]", ".", "input_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.StackedLSTMCells.num_layers": [[100, 103], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_layers", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_cells", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.StackedLSTMCells.bidirectional": [[104, 107], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_cells", "[", "0", "]", ".", "bidirectional", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.__init__": [[114, 122], ["cells.append", "range", "rnn.StackedLSTMCells.__init__", "torch.nn.LSTMCell", "cells.append", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "num_layers", ",", "\n", "bias", "=", "True", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "\"\"\" same as nn.LSTM but without (bidirectional)\"\"\"", "\n", "cells", "=", "[", "]", "\n", "cells", ".", "append", "(", "nn", ".", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "bias", ")", ")", "\n", "for", "_", "in", "range", "(", "num_layers", "-", "1", ")", ":", "\n", "            ", "cells", ".", "append", "(", "nn", ".", "LSTMCell", "(", "hidden_size", ",", "hidden_size", ",", "bias", ")", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "cells", ",", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.bidirectional": [[123, 126], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.reset_parameters": [[127, 137], ["torch.chunk", "[].data.fill_", "torch.nn.init.xavier_normal_", "torch.chunk"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "for", "cell", "in", "self", ".", "_cells", ":", "\n", "# xavier initilization", "\n", "            ", "gate_size", "=", "self", ".", "hidden_size", "/", "4", "\n", "for", "weight", "in", "[", "cell", ".", "weight_ih", ",", "cell", ".", "weight_hh", "]", ":", "\n", "                ", "for", "w", "in", "torch", ".", "chunk", "(", "weight", ",", "4", ",", "dim", "=", "0", ")", ":", "\n", "                    ", "init", ".", "xavier_normal_", "(", "w", ")", "\n", "#forget bias = 1", "\n", "", "", "for", "bias", "in", "[", "cell", ".", "bias_ih", ",", "cell", ".", "bias_hh", "]", ":", "\n", "                ", "torch", ".", "chunk", "(", "bias", ",", "4", ",", "dim", "=", "0", ")", "[", "1", "]", ".", "data", ".", "fill_", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert": [[138, 150], ["rnn.MultiLayerLSTMCells", "enumerate", "cell.weight_ih.data.copy_", "cell.weight_hh.data.copy_", "cell.bias_ih.data.copy_", "cell.bias_hh.data.copy_", "getattr", "getattr", "getattr", "getattr"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "convert", "(", "lstm", ")", ":", "\n", "        ", "\"\"\" convert from a cudnn LSTM\"\"\"", "\n", "lstm_cell", "=", "MultiLayerLSTMCells", "(", "\n", "lstm", ".", "input_size", ",", "lstm", ".", "hidden_size", ",", "\n", "lstm", ".", "num_layers", ",", "dropout", "=", "lstm", ".", "dropout", ")", "\n", "for", "i", ",", "cell", "in", "enumerate", "(", "lstm_cell", ".", "_cells", ")", ":", "\n", "            ", "cell", ".", "weight_ih", ".", "data", ".", "copy_", "(", "getattr", "(", "lstm", ",", "'weight_ih_l{}'", ".", "format", "(", "i", ")", ")", ")", "\n", "cell", ".", "weight_hh", ".", "data", ".", "copy_", "(", "getattr", "(", "lstm", ",", "'weight_hh_l{}'", ".", "format", "(", "i", ")", ")", ")", "\n", "cell", ".", "bias_ih", ".", "data", ".", "copy_", "(", "getattr", "(", "lstm", ",", "'bias_ih_l{}'", ".", "format", "(", "i", ")", ")", ")", "\n", "cell", ".", "bias_hh", ".", "data", ".", "copy_", "(", "getattr", "(", "lstm", ",", "'bias_hh_l{}'", ".", "format", "(", "i", ")", ")", ")", "\n", "", "return", "lstm_cell", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.lstm_encoder": [[9, 49], ["sequence.transpose.size", "sequence.transpose.transpose", "sequence.transpose.detach().cpu().numpy", "util.reorder_sequence.detach().cpu().numpy", "sorted", "util.reorder_sequence", "rnn.init_lstm_states", "torch.nn.utils.rnn.pack_padded_sequence", "lstm", "torch.nn.utils.rnn.pad_packed_sequence", "util.reorder_sequence", "util.reorder_lstm_states", "lstm", "embedding", "len", "range", "init_states[].contiguous", "init_states[].contiguous", "sequence.transpose.detach().cpu", "util.reorder_sequence.detach().cpu", "len", "enumerate", "range", "len", "sequence.transpose.detach", "util.reorder_sequence.detach"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.reorder_sequence", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.init_lstm_states", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.reorder_sequence", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.reorder_lstm_states"], ["def", "lstm_encoder", "(", "sequence", ",", "lstm", ",", "\n", "seq_lens", "=", "None", ",", "init_states", "=", "None", ",", "embedding", "=", "None", ")", ":", "\n", "    ", "\"\"\" functional LSTM encoder (sequence is [b, t]/[b, t, d],\n    lstm should be rolled lstm)\"\"\"", "\n", "batch_size", "=", "sequence", ".", "size", "(", "0", ")", "\n", "if", "not", "lstm", ".", "batch_first", ":", "\n", "        ", "sequence", "=", "sequence", ".", "transpose", "(", "0", ",", "1", ")", "\n", "sequence_np", "=", "sequence", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "emb_sequence", "=", "(", "embedding", "(", "sequence", ")", "if", "embedding", "is", "not", "None", "\n", "else", "sequence", ")", "\n", "emb_sequence_np", "=", "emb_sequence", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "if", "seq_lens", ":", "\n", "        ", "assert", "batch_size", "==", "len", "(", "seq_lens", ")", "\n", "sort_ind", "=", "sorted", "(", "range", "(", "len", "(", "seq_lens", ")", ")", ",", "\n", "key", "=", "lambda", "i", ":", "seq_lens", "[", "i", "]", ",", "reverse", "=", "True", ")", "\n", "seq_lens", "=", "[", "seq_lens", "[", "i", "]", "for", "i", "in", "sort_ind", "]", "\n", "emb_sequence", "=", "reorder_sequence", "(", "emb_sequence", ",", "sort_ind", ",", "\n", "lstm", ".", "batch_first", ")", "\n", "\n", "", "if", "init_states", "is", "None", ":", "\n", "        ", "device", "=", "sequence", ".", "device", "\n", "init_states", "=", "init_lstm_states", "(", "lstm", ",", "batch_size", ",", "device", ")", "\n", "", "else", ":", "\n", "        ", "init_states", "=", "(", "init_states", "[", "0", "]", ".", "contiguous", "(", ")", ",", "\n", "init_states", "[", "1", "]", ".", "contiguous", "(", ")", ")", "\n", "\n", "", "if", "seq_lens", ":", "\n", "        ", "packed_seq", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "emb_sequence", ",", "\n", "seq_lens", ")", "\n", "packed_out", ",", "final_states", "=", "lstm", "(", "packed_seq", ",", "init_states", ")", "\n", "lstm_out", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_out", ")", "\n", "\n", "back_map", "=", "{", "ind", ":", "i", "for", "i", ",", "ind", "in", "enumerate", "(", "sort_ind", ")", "}", "\n", "reorder_ind", "=", "[", "back_map", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "seq_lens", ")", ")", "]", "\n", "lstm_out", "=", "reorder_sequence", "(", "lstm_out", ",", "reorder_ind", ",", "lstm", ".", "batch_first", ")", "\n", "final_states", "=", "reorder_lstm_states", "(", "final_states", ",", "reorder_ind", ")", "\n", "", "else", ":", "\n", "        ", "lstm_out", ",", "final_states", "=", "lstm", "(", "emb_sequence", ",", "init_states", ")", "\n", "\n", "", "return", "lstm_out", ",", "final_states", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.init_lstm_states": [[51, 58], ["torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros"], "function", ["None"], ["", "def", "init_lstm_states", "(", "lstm", ",", "batch_size", ",", "device", ")", ":", "\n", "    ", "n_layer", "=", "lstm", ".", "num_layers", "*", "(", "2", "if", "lstm", ".", "bidirectional", "else", "1", ")", "\n", "n_hidden", "=", "lstm", ".", "hidden_size", "\n", "\n", "states", "=", "(", "torch", ".", "zeros", "(", "n_layer", ",", "batch_size", ",", "n_hidden", ")", ".", "to", "(", "device", ")", ",", "\n", "torch", ".", "zeros", "(", "n_layer", ",", "batch_size", ",", "n_hidden", ")", ".", "to", "(", "device", ")", ")", "\n", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._Hypothesis.__init__": [[10, 21], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sequence", ",", "logprob", ",", "hists", ",", "attns", "=", "[", "]", ")", ":", "\n", "        ", "\"\"\"\n        seqence: list of int tokens\n        logprob: current log probability\n        hists: history of prevous convolution list(n_layers)/\n               prev_states and output of lstm ((H, C), out)\n        \"\"\"", "\n", "self", ".", "sequence", "=", "sequence", "\n", "self", ".", "logprob", "=", "logprob", "\n", "self", ".", "hists", "=", "hists", "\n", "self", ".", "attns", "=", "attns", "# for unk replacement", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._Hypothesis.extend_k": [[22, 30], ["beam_search._Hypothesis", "enumerate", "zip", "t.item", "lp.item"], "methods", ["None"], ["", "def", "extend_k", "(", "self", ",", "topk", ",", "logprobs", ",", "hists", ",", "attn", "=", "None", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "if", "attn", "is", "None", ":", "\n", "            ", "attns", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "attns", "=", "self", ".", "attns", "+", "[", "attn", "]", "\n", "", "return", "[", "_Hypothesis", "(", "self", ".", "sequence", "+", "[", "t", ".", "item", "(", ")", "]", ",", "\n", "self", ".", "logprob", "+", "lp", ".", "item", "(", ")", "-", "diverse", "*", "i", ",", "hists", ",", "attns", ")", "\n", "for", "i", ",", "(", "t", ",", "lp", ")", "in", "enumerate", "(", "zip", "(", "topk", ",", "logprobs", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._Hypothesis.__lt__": [[31, 34], ["len", "len"], "methods", ["None"], ["", "def", "__lt__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "(", "other", ".", "logprob", "/", "len", "(", "other", ".", "sequence", ")", "\n", "<", "self", ".", "logprob", "/", "len", "(", "self", ".", "sequence", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.init_beam": [[36, 39], ["beam_search._Hypothesis"], "function", ["None"], ["", "", "def", "init_beam", "(", "start", ",", "hists", ")", ":", "\n", "    ", "\"\"\" get a initial beam to start beam search\"\"\"", "\n", "return", "[", "_Hypothesis", "(", "[", "start", "]", ",", "0", ",", "hists", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.create_beam": [[41, 46], ["tok.size", "beam_search._Hypothesis", "lp[].item", "range", "tok[].item"], "function", ["None"], ["", "def", "create_beam", "(", "tok", ",", "lp", ",", "hists", ")", ":", "\n", "    ", "\"\"\" initailiza a beam with top k token\"\"\"", "\n", "k", "=", "tok", ".", "size", "(", "0", ")", "\n", "return", "[", "_Hypothesis", "(", "[", "tok", "[", "i", "]", ".", "item", "(", ")", "]", ",", "lp", "[", "i", "]", ".", "item", "(", ")", ",", "hists", ")", "\n", "for", "i", "in", "range", "(", "k", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.pack_beam": [[48, 57], ["torch.LongTensor", "tuple", "token.to.to", "torch.stack", "enumerate"], "function", ["None"], ["", "def", "pack_beam", "(", "hyps", ",", "device", ")", ":", "\n", "    ", "\"\"\"pack a list of hypothesis to decoder input batches\"\"\"", "\n", "token", "=", "torch", ".", "LongTensor", "(", "[", "h", ".", "sequence", "[", "-", "1", "]", "for", "h", "in", "hyps", "]", ")", "\n", "\n", "hists", "=", "tuple", "(", "torch", ".", "stack", "(", "[", "hyp", ".", "hists", "[", "i", "]", "for", "hyp", "in", "hyps", "]", ",", "dim", "=", "d", ")", "\n", "for", "i", ",", "d", "in", "enumerate", "(", "[", "1", ",", "1", ",", "0", "]", ")", ")", "\n", "token", "=", "token", ".", "to", "(", "device", ")", "\n", "states", "=", "(", "(", "hists", "[", "0", "]", ",", "hists", "[", "1", "]", ")", ",", "hists", "[", "2", "]", ")", "\n", "return", "token", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.next_search_beam": [[59, 70], ["beam_search._unpack_topk", "list", "beam_search._clean_beam", "h.extend_k", "cytoolz.concat", "enumerate"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._unpack_topk", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._clean_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._Hypothesis.extend_k"], ["", "def", "next_search_beam", "(", "beam", ",", "beam_size", ",", "finished", ",", "\n", "end", ",", "topk", ",", "lp", ",", "hists", ",", "attn", "=", "None", ",", "diverse", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"generate the next beam(K-best hyps)\"\"\"", "\n", "topks", ",", "lps", ",", "hists_list", ",", "attns", "=", "_unpack_topk", "(", "topk", ",", "lp", ",", "hists", ",", "attn", ")", "\n", "hyps_lists", "=", "[", "h", ".", "extend_k", "(", "topks", "[", "i", "]", ",", "lps", "[", "i", "]", ",", "\n", "hists_list", "[", "i", "]", ",", "attns", "[", "i", "]", ",", "diverse", ")", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "beam", ")", "]", "\n", "hyps", "=", "list", "(", "concat", "(", "hyps_lists", ")", ")", "\n", "finished", ",", "beam", "=", "_clean_beam", "(", "finished", ",", "hyps", ",", "end", ",", "beam_size", ")", "\n", "\n", "return", "finished", ",", "beam", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.best_sequence": [[72, 87], ["None"], "function", ["None"], ["", "def", "best_sequence", "(", "finished", ",", "beam", "=", "None", ")", ":", "\n", "    ", "\"\"\" return the sequence with the highest prob(normalized by length)\"\"\"", "\n", "if", "beam", "is", "None", ":", "# not empty", "\n", "        ", "best_beam", "=", "finished", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "if", "finished", "and", "beam", "[", "0", "]", "<", "finished", "[", "0", "]", ":", "\n", "            ", "best_beam", "=", "finished", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "best_beam", "=", "beam", "[", "0", "]", "\n", "\n", "", "", "best_seq", "=", "best_beam", ".", "sequence", "[", "1", ":", "]", "\n", "if", "best_beam", ".", "attns", ":", "\n", "        ", "return", "best_seq", ",", "best_beam", ".", "attns", "\n", "", "else", ":", "\n", "        ", "return", "best_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._unpack_topk": [[89, 102], ["topk.size", "range", "range"], "function", ["None"], ["", "", "def", "_unpack_topk", "(", "topk", ",", "lp", ",", "hists", ",", "attn", "=", "None", ")", ":", "\n", "    ", "\"\"\"unpack the decoder output\"\"\"", "\n", "beam", ",", "_", "=", "topk", ".", "size", "(", ")", "\n", "topks", "=", "[", "t", "for", "t", "in", "topk", "]", "\n", "lps", "=", "[", "l", "for", "l", "in", "lp", "]", "\n", "k_hists", "=", "[", "(", "hists", "[", "0", "]", "[", ":", ",", "i", ",", ":", "]", ",", "hists", "[", "1", "]", "[", ":", ",", "i", ",", ":", "]", ",", "hists", "[", "2", "]", "[", "i", ",", ":", "]", ")", "\n", "for", "i", "in", "range", "(", "beam", ")", "]", "\n", "\n", "if", "attn", "is", "None", ":", "\n", "        ", "return", "topks", ",", "lps", ",", "k_hists", "\n", "", "else", ":", "\n", "        ", "attns", "=", "[", "attn", "[", "i", "]", "for", "i", "in", "range", "(", "beam", ")", "]", "\n", "return", "topks", ",", "lps", ",", "k_hists", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._clean_beam": [[104, 127], ["sorted", "sorted", "beam_search._has_repeat_tri", "beam_search._Hypothesis", "sorted.append", "new_beam.append", "len", "len", "new_beam.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._has_repeat_tri"], ["", "", "def", "_clean_beam", "(", "finished", ",", "beam", ",", "end_tok", ",", "beam_size", ",", "remove_tri", "=", "True", ")", ":", "\n", "    ", "\"\"\" remove completed sequence from beam \"\"\"", "\n", "new_beam", "=", "[", "]", "\n", "for", "h", "in", "sorted", "(", "beam", ",", "reverse", "=", "True", ",", "\n", "key", "=", "lambda", "h", ":", "h", ".", "logprob", "/", "len", "(", "h", ".", "sequence", ")", ")", ":", "\n", "        ", "if", "remove_tri", "and", "_has_repeat_tri", "(", "h", ".", "sequence", ")", ":", "\n", "            ", "h", ".", "logprob", "=", "-", "1e9", "\n", "", "if", "h", ".", "sequence", "[", "-", "1", "]", "==", "end_tok", ":", "\n", "            ", "finished_hyp", "=", "_Hypothesis", "(", "h", ".", "sequence", "[", ":", "-", "1", "]", ",", "# remove EOS", "\n", "h", ".", "logprob", ",", "h", ".", "hists", ",", "h", ".", "attns", ")", "\n", "finished", ".", "append", "(", "finished_hyp", ")", "\n", "", "else", ":", "\n", "            ", "new_beam", ".", "append", "(", "h", ")", "\n", "", "if", "len", "(", "new_beam", ")", "==", "beam_size", ":", "\n", "            ", "break", "\n", "", "", "else", ":", "\n", "# ensure beam size", "\n", "        ", "while", "len", "(", "new_beam", ")", "<", "beam_size", ":", "\n", "            ", "new_beam", ".", "append", "(", "new_beam", "[", "0", "]", ")", "\n", "\n", "", "", "finished", "=", "sorted", "(", "finished", ",", "reverse", "=", "True", ",", "\n", "key", "=", "lambda", "h", ":", "h", ".", "logprob", "/", "len", "(", "h", ".", "sequence", ")", ")", "\n", "return", "finished", ",", "new_beam", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search._has_repeat_tri": [[129, 133], ["collections.Counter", "tuple", "all", "range", "len"], "function", ["None"], ["", "def", "_has_repeat_tri", "(", "grams", ")", ":", "\n", "    ", "tri_grams", "=", "[", "tuple", "(", "grams", "[", "i", ":", "i", "+", "3", "]", ")", "for", "i", "in", "range", "(", "len", "(", "grams", ")", "-", "2", ")", "]", "\n", "cnt", "=", "Counter", "(", "tri_grams", ")", "\n", "return", "not", "all", "(", "(", "cnt", "[", "g", "]", "<=", "1", "for", "g", "in", "cnt", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondSumm.__init__": [[14, 35], ["copy_summ.CopySumm.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.LSTM", "torch.nn.Sequential", "copy_cond_summ.CopyCondLSTMDecoder", "torch.Tensor", "torch.Tensor", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "dropout", ")", "\n", "# multiplicative attention", "\n", "enc_out_dim", "=", "n_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "self", ".", "_attn_wm_external", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "enc_out_dim", ",", "n_hidden", ")", ")", "\n", "self", ".", "_attn_wq_external", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ",", "n_hidden", ")", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wm_external", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wq_external", ")", "\n", "\n", "self", ".", "_mem_enc_lstm", "=", "nn", ".", "LSTM", "(", "emb_dim", ",", "n_hidden", ",", "n_layer", ",", "bidirectional", "=", "bidirectional", ",", "dropout", "=", "dropout", ")", "\n", "# project decoder output to emb_dim, then", "\n", "# apply weight matrix from embedding layer", "\n", "self", ".", "_projection", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "3", "*", "n_hidden", ",", "n_hidden", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "n_hidden", ",", "emb_dim", ",", "bias", "=", "False", ")", "\n", ")", "\n", "self", ".", "_decoder", "=", "CopyCondLSTMDecoder", "(", "self", ".", "_attn_wq_external", ",", "self", ".", "_copy", ",", "self", ".", "_embedding", ",", "self", ".", "_dec_lstm", ",", "\n", "self", ".", "_attn_wq", ",", "self", ".", "_projection", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondSumm.encode": [[36, 77], ["rnn.lstm_encoder", "torch.stack", "torch.stack", "torch.matmul().transpose", "util.sequence_mean", "rnn.lstm_encoder", "torch.matmul().transpose", "util.sequence_mean", "copy_cond_summ.CopyCondSumm._projection", "copy_cond_summ.CopyCondSumm._init_enc_h.size", "copy_cond_summ.CopyCondSumm._init_enc_h.size", "copy_cond_summ.CopyCondSumm._init_enc_h.unsqueeze().expand", "copy_cond_summ.CopyCondSumm._init_enc_c.unsqueeze().expand", "torch.cat", "len", "torch.cat", "torch.cat", "copy_cond_summ.CopyCondSumm._dec_h", "copy_cond_summ.CopyCondSumm._dec_c", "torch.matmul", "torch.matmul", "copy_cond_summ.CopyCondSumm._init_enc_h.unsqueeze", "copy_cond_summ.CopyCondSumm._init_enc_c.unsqueeze", "h.chunk", "c.chunk"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.lstm_encoder", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_mean", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.lstm_encoder", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_mean"], ["", "def", "encode", "(", "self", ",", "article", ",", "raw_memory", ",", "art_lens", "=", "None", ",", "memory_lens", "=", "None", ")", ":", "# article: [batch_size, seq_len]", "\n", "# encode document", "\n", "        ", "size", "=", "(", "\n", "self", ".", "_init_enc_h", ".", "size", "(", "0", ")", ",", "\n", "len", "(", "art_lens", ")", "if", "art_lens", "else", "1", ",", "\n", "self", ".", "_init_enc_h", ".", "size", "(", "1", ")", "\n", ")", "\n", "init_enc_states", "=", "(", "\n", "self", ".", "_init_enc_h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ",", "\n", "self", ".", "_init_enc_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", "\n", ")", "\n", "enc_art", ",", "final_states", "=", "lstm_encoder", "(", "\n", "article", ",", "self", ".", "_enc_lstm", ",", "art_lens", ",", "\n", "init_enc_states", ",", "self", ".", "_embedding", "\n", ")", "# enc_art: [seq_len, batch, 2*hidden_dim], final_states: ([1, batch, 2*hidden_dim], [1, batch, 2*hidden_dim])", "\n", "if", "self", ".", "_enc_lstm", ".", "bidirectional", ":", "\n", "            ", "h", ",", "c", "=", "final_states", "\n", "final_states", "=", "(", "\n", "torch", ".", "cat", "(", "h", ".", "chunk", "(", "2", ",", "dim", "=", "0", ")", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "cat", "(", "c", ".", "chunk", "(", "2", ",", "dim", "=", "0", ")", ",", "dim", "=", "2", ")", "\n", ")", "\n", "", "init_h", "=", "torch", ".", "stack", "(", "[", "self", ".", "_dec_h", "(", "s", ")", "\n", "for", "s", "in", "final_states", "[", "0", "]", "]", ",", "dim", "=", "0", ")", "# [1, batch_size, hidden_dim]", "\n", "init_c", "=", "torch", ".", "stack", "(", "[", "self", ".", "_dec_c", "(", "s", ")", "\n", "for", "s", "in", "final_states", "[", "1", "]", "]", ",", "dim", "=", "0", ")", "\n", "init_dec_states", "=", "(", "init_h", ",", "init_c", ")", "\n", "attention", "=", "torch", ".", "matmul", "(", "enc_art", ",", "self", ".", "_attn_wm", ")", ".", "transpose", "(", "0", ",", "1", ")", "# [batch, seq_len, hidden_dim]", "\n", "init_attn_out", "=", "sequence_mean", "(", "attention", ",", "art_lens", ",", "dim", "=", "1", ")", "# [batch, hidden_dim]", "\n", "\n", "# encode memory", "\n", "enc_mem", ",", "_", "=", "lstm_encoder", "(", "raw_memory", ",", "self", ".", "_mem_enc_lstm", ",", "memory_lens", ",", "init_states", "=", "None", ",", "\n", "embedding", "=", "self", ".", "_embedding", ")", "# enc_mem: [seq_len, batch, 2*hidden_dim]", "\n", "attention_external", "=", "torch", ".", "matmul", "(", "enc_mem", ",", "self", ".", "_attn_wm_external", ")", ".", "transpose", "(", "0", ",", "1", ")", "# [batch, seq_len, hidden_dim]", "\n", "\n", "init_ext_attn_out", "=", "sequence_mean", "(", "attention_external", ",", "memory_lens", ",", "dim", "=", "1", ")", "# [batch, hidden_dim]", "\n", "\n", "init_dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "\n", "[", "init_h", "[", "-", "1", "]", ",", "init_attn_out", ",", "init_ext_attn_out", "]", ",", "dim", "=", "1", "\n", ")", ")", "# [batch, 3 * hidden_dim] -> [batch, embed_size]", "\n", "\n", "return", "attention", ",", "attention_external", ",", "(", "init_dec_states", ",", "init_dec_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondSumm.forward": [[78, 89], ["copy_cond_summ.CopyCondSumm.encode", "util.len_mask().unsqueeze", "util.len_mask().unsqueeze", "copy_cond_summ.CopyCondSumm._decoder", "util.len_mask", "util.len_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "forward", "(", "self", ",", "article", ",", "art_lens", ",", "memory", ",", "mem_lens", ",", "abstract", ",", "extend_art", ",", "extend_vsize", ")", ":", "\n", "        ", "attention", ",", "attention_external", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "memory", ",", "art_lens", ",", "mem_lens", ")", "\n", "#encoded_memory, init_memory_out = self.encode_external_memory(memory, mem_lens)", "\n", "art_mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "mem_mask", "=", "len_mask", "(", "mem_lens", ",", "memory", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "\n", "logit", "=", "self", ".", "_decoder", "(", "\n", "(", "attention", ",", "art_mask", ",", "attention_external", ",", "mem_mask", ",", "extend_art", ",", "extend_vsize", ")", ",", "\n", "init_dec_states", ",", "abstract", "\n", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondSumm.batch_decode": [[90, 110], ["len", "copy_cond_summ.CopyCondSumm.encode", "util.len_mask().unsqueeze", "util.len_mask().unsqueeze", "torch.LongTensor().to", "range", "copy_cond_summ.CopyCondSumm._decoder.decode_step", "attns.append", "outputs.append", "torch.LongTensor().to.masked_fill_", "util.len_mask", "util.len_mask", "torch.LongTensor", "tok[].clone"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "batch_decode", "(", "self", ",", "article", ",", "art_lens", ",", "memory", ",", "mem_lens", ",", "extend_art", ",", "extend_vsize", ",", "\n", "go", ",", "eos", ",", "unk", ",", "max_len", ")", ":", "\n", "        ", "\"\"\" greedy decode support batching\"\"\"", "\n", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "attention_external", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "memory", ",", "art_lens", ",", "mem_lens", ")", "\n", "art_mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "mem_mask", "=", "len_mask", "(", "mem_lens", ",", "memory", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "attention", "=", "(", "attention", ",", "art_mask", ",", "attention_external", ",", "mem_mask", ",", "extend_art", ",", "extend_vsize", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", "*", "batch_size", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "attns", ".", "append", "(", "attn_score", ")", "\n", "outputs", ".", "append", "(", "tok", "[", ":", ",", "0", "]", ".", "clone", "(", ")", ")", "\n", "tok", ".", "masked_fill_", "(", "tok", ">=", "vsize", ",", "unk", ")", "\n", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondSumm.decode": [[111, 129], ["copy_cond_summ.CopyCondSumm.encode", "torch.LongTensor().to", "range", "copy_cond_summ.CopyCondSumm._decoder.decode_step", "outputs.append", "attns.append", "torch.LongTensor", "tok[].item", "tok[].item", "attn_score.squeeze", "tok[].item"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step"], ["", "def", "decode", "(", "self", ",", "article", ",", "memory", ",", "extend_art", ",", "extend_vsize", ",", "go", ",", "eos", ",", "unk", ",", "max_len", ")", ":", "\n", "        ", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "attention_external", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "memory", ")", "\n", "attention", "=", "(", "attention", ",", "None", ",", "attention_external", ",", "None", ",", "extend_art", ",", "extend_vsize", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", "==", "eos", ":", "\n", "                ", "break", "\n", "", "outputs", ".", "append", "(", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ")", "\n", "attns", ".", "append", "(", "attn_score", ".", "squeeze", "(", "0", ")", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ">=", "vsize", ":", "\n", "                ", "tok", "[", "0", ",", "0", "]", "=", "unk", "\n", "", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondSumm.batched_beamsearch": [[130, 211], ["len", "copy_cond_summ.CopyCondSumm.encode", "util.len_mask().unsqueeze", "util.len_mask().unsqueeze", "range", "beam_search.init_beam", "filter", "torch.stack", "torch.stack.masked_fill_", "copy_cond_summ.CopyCondSumm._decoder.topk_step", "enumerate", "all", "enumerate", "util.len_mask", "util.len_mask", "range", "range", "range", "beam_search.pack_beam", "toks.append", "all_states.append", "torch.stack", "zip", "beam_search.next_search_beam", "zip", "torch.stack", "torch.stack", "len", "torch.LongTensor().to", "map", "torch.stack", "torch.stack", "enumerate", "enumerate", "enumerate", "torch.LongTensor", "v.index_select"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.init_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder.topk_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.pack_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.next_search_beam"], ["", "def", "batched_beamsearch", "(", "self", ",", "article", ",", "art_lens", ",", "memory", ",", "mem_lens", ",", "\n", "extend_art", ",", "extend_vsize", ",", "\n", "go", ",", "eos", ",", "unk", ",", "max_len", ",", "beam_size", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "attention_external", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "memory", ",", "art_lens", ",", "mem_lens", ")", "\n", "art_mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "mem_mask", "=", "len_mask", "(", "mem_lens", ",", "memory", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "all_attention", "=", "(", "attention", ",", "art_mask", ",", "attention_external", ",", "mem_mask", ",", "extend_art", ",", "extend_vsize", ")", "\n", "attention", "=", "all_attention", "\n", "(", "h", ",", "c", ")", ",", "prev", "=", "init_dec_states", "\n", "all_beams", "=", "[", "bs", ".", "init_beam", "(", "go", ",", "(", "h", "[", ":", ",", "i", ",", ":", "]", ",", "c", "[", ":", ",", "i", ",", ":", "]", ",", "prev", "[", "i", "]", ")", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "finished_beams", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "outputs", "=", "[", "None", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "for", "t", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "toks", "=", "[", "]", "\n", "all_states", "=", "[", "]", "\n", "for", "beam", "in", "filter", "(", "bool", ",", "all_beams", ")", ":", "\n", "                ", "token", ",", "states", "=", "bs", ".", "pack_beam", "(", "beam", ",", "article", ".", "device", ")", "\n", "toks", ".", "append", "(", "token", ")", "\n", "all_states", ".", "append", "(", "states", ")", "\n", "", "token", "=", "torch", ".", "stack", "(", "toks", ",", "dim", "=", "1", ")", "\n", "states", "=", "(", "(", "torch", ".", "stack", "(", "[", "h", "for", "(", "h", ",", "_", ")", ",", "_", "in", "all_states", "]", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "stack", "(", "[", "c", "for", "(", "_", ",", "c", ")", ",", "_", "in", "all_states", "]", ",", "dim", "=", "2", ")", ")", ",", "\n", "torch", ".", "stack", "(", "[", "prev", "for", "_", ",", "prev", "in", "all_states", "]", ",", "dim", "=", "1", ")", ")", "\n", "token", ".", "masked_fill_", "(", "token", ">=", "vsize", ",", "unk", ")", "\n", "\n", "topk", ",", "lp", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "topk_step", "(", "\n", "token", ",", "states", ",", "attention", ",", "beam_size", ")", "\n", "\n", "batch_i", "=", "0", "\n", "for", "i", ",", "(", "beam", ",", "finished", ")", "in", "enumerate", "(", "zip", "(", "all_beams", ",", "\n", "finished_beams", ")", ")", ":", "\n", "                ", "if", "not", "beam", ":", "\n", "                    ", "continue", "\n", "", "finished", ",", "new_beam", "=", "bs", ".", "next_search_beam", "(", "\n", "beam", ",", "beam_size", ",", "finished", ",", "eos", ",", "\n", "topk", "[", ":", ",", "batch_i", ",", ":", "]", ",", "lp", "[", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "(", "states", "[", "0", "]", "[", "0", "]", "[", ":", ",", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "states", "[", "0", "]", "[", "1", "]", "[", ":", ",", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "states", "[", "1", "]", "[", ":", ",", "batch_i", ",", ":", "]", ")", ",", "\n", "attn_score", "[", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "diverse", "\n", ")", "\n", "batch_i", "+=", "1", "\n", "if", "len", "(", "finished", ")", ">=", "beam_size", ":", "\n", "                    ", "all_beams", "[", "i", "]", "=", "[", "]", "\n", "outputs", "[", "i", "]", "=", "finished", "[", ":", "beam_size", "]", "\n", "# exclude finished inputs", "\n", "(", "attention", ",", "art_mask", ",", "attention_external", ",", "mem_mask", ",", "extend_art", ",", "extend_vsize", "\n", ")", "=", "all_attention", "\n", "art_masks", "=", "[", "art_mask", "[", "j", "]", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "\n", "if", "o", "is", "None", "]", "\n", "mem_masks", "=", "[", "mem_mask", "[", "j", "]", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "\n", "if", "o", "is", "None", "]", "\n", "ind", "=", "[", "j", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "if", "o", "is", "None", "]", "\n", "ind", "=", "torch", ".", "LongTensor", "(", "ind", ")", ".", "to", "(", "attention", ".", "device", ")", "\n", "attention", ",", "attention_external", ",", "extend_art", "=", "map", "(", "\n", "lambda", "v", ":", "v", ".", "index_select", "(", "dim", "=", "0", ",", "index", "=", "ind", ")", ",", "\n", "[", "attention", ",", "attention_external", ",", "extend_art", "]", "\n", ")", "\n", "if", "art_masks", ":", "\n", "                        ", "art_mask", "=", "torch", ".", "stack", "(", "art_masks", ",", "dim", "=", "0", ")", "\n", "mem_mask", "=", "torch", ".", "stack", "(", "mem_masks", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "art_mask", "=", "None", "\n", "mem_mask", "=", "None", "\n", "", "attention", "=", "(", "\n", "attention", ",", "art_mask", ",", "attention_external", ",", "mem_mask", ",", "extend_art", ",", "extend_vsize", ")", "\n", "", "else", ":", "\n", "                    ", "all_beams", "[", "i", "]", "=", "new_beam", "\n", "finished_beams", "[", "i", "]", "=", "finished", "\n", "", "", "if", "all", "(", "outputs", ")", ":", "\n", "                ", "break", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "(", "o", ",", "f", ",", "b", ")", "in", "enumerate", "(", "zip", "(", "outputs", ",", "\n", "finished_beams", ",", "all_beams", ")", ")", ":", "\n", "                ", "if", "o", "is", "None", ":", "\n", "                    ", "outputs", "[", "i", "]", "=", "(", "f", "+", "b", ")", "[", ":", "beam_size", "]", "\n", "", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondLSTMDecoder.__init__": [[214, 217], ["copy_summ.CopyLSTMDecoder.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "attn_w_external", ",", "copy", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_attn_w_external", "=", "attn_w_external", "\n", "super", "(", ")", ".", "__init__", "(", "copy", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondLSTMDecoder._step": [[218, 248], ["torch.cat", "copy_cond_summ.CopyCondLSTMDecoder._lstm", "torch.mm", "torch.mm", "attention.step_attention", "attention.step_attention", "copy_cond_summ.CopyCondLSTMDecoder._projection", "copy_cond_summ.CopyCondLSTMDecoder._compute_gen_prob", "torch.sigmoid", "torch.log", "torch.cat", "copy_cond_summ.CopyCondLSTMDecoder._copy", "copy_cond_summ.CopyCondLSTMDecoder._embedding().squeeze", "copy_cond_summ.CopyCondLSTMDecoder._embedding", "extend_src.expand_as"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log"], ["", "def", "_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ")", ":", "\n", "        ", "prev_states", ",", "prev_out", "=", "states", "\n", "lstm_in", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ".", "squeeze", "(", "1", ")", ",", "prev_out", "]", ",", "\n", "dim", "=", "1", "\n", ")", "\n", "states", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "query", "=", "torch", ".", "mm", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "# [batch_size, hidden_dim]", "\n", "query_external", "=", "torch", ".", "mm", "(", "lstm_out", ",", "self", ".", "_attn_w_external", ")", "# [batch_size, hidden_dim]", "\n", "attention", ",", "attn_mask", ",", "attention_external", ",", "attn_mask_external", ",", "extend_src", ",", "extend_vsize", "=", "attention", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "context_external", ",", "score_external", "=", "step_attention", "(", "query_external", ",", "attention_external", ",", "attention_external", ",", "attn_mask_external", ")", "\n", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", ",", "context_external", "]", ",", "dim", "=", "1", ")", ")", "# [batch, 3*hidden_dim]", "\n", "\n", "# extend generation prob to extended vocabulary", "\n", "gen_prob", "=", "self", ".", "_compute_gen_prob", "(", "dec_out", ",", "extend_vsize", ")", "\n", "# compute the probabilty of each copying", "\n", "copy_prob", "=", "torch", ".", "sigmoid", "(", "self", ".", "_copy", "(", "context", ",", "states", "[", "0", "]", "[", "-", "1", "]", ",", "lstm_in", ")", ")", "\n", "# add the copy prob to existing vocab distribution", "\n", "lp", "=", "torch", ".", "log", "(", "\n", "(", "(", "-", "copy_prob", "+", "1", ")", "*", "gen_prob", "\n", ")", ".", "scatter_add", "(", "\n", "dim", "=", "1", ",", "\n", "index", "=", "extend_src", ".", "expand_as", "(", "score", ")", ",", "\n", "source", "=", "score", "*", "copy_prob", "\n", ")", "+", "1e-8", ")", "# numerical stability for log", "\n", "return", "lp", ",", "(", "states", ",", "dec_out", ")", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_cond_summ.CopyCondLSTMDecoder.topk_step": [[249, 293], ["h.size", "tok.size", "torch.cat", "torch.cat.contiguous().view", "copy_cond_summ.CopyCondLSTMDecoder._lstm", "torch.matmul", "torch.matmul", "attention.step_attention", "attention.step_attention", "copy_cond_summ.CopyCondLSTMDecoder._projection", "copy_cond_summ.CopyCondLSTMDecoder._compute_gen_prob", "torch.sigmoid().contiguous().view", "torch.log().contiguous().view", "torch.log().contiguous().view.topk", "h.contiguous().view", "c.contiguous().view", "h.contiguous().view", "c.contiguous().view", "torch.cat", "copy_cond_summ.CopyCondLSTMDecoder.contiguous().view", "copy_cond_summ.CopyCondLSTMDecoder._embedding", "torch.cat.contiguous", "torch.sigmoid().contiguous", "torch.log().contiguous", "h.contiguous", "c.contiguous", "h.contiguous", "c.contiguous", "copy_cond_summ.CopyCondLSTMDecoder.contiguous", "torch.sigmoid", "torch.log", "copy_cond_summ.CopyCondLSTMDecoder._copy", "extend_src.expand_as().contiguous().view", "score.contiguous().view", "extend_src.expand_as().contiguous", "score.contiguous", "extend_src.expand_as"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log"], ["", "def", "topk_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ",", "k", ")", ":", "\n", "        ", "\"\"\"tok:[BB, B], states ([L, BB, B, D]*2, [BB, B, D])\"\"\"", "\n", "(", "h", ",", "c", ")", ",", "prev_out", "=", "states", "\n", "\n", "# lstm is not bemable", "\n", "nl", ",", "_", ",", "_", ",", "d", "=", "h", ".", "size", "(", ")", "\n", "beam", ",", "batch", "=", "tok", ".", "size", "(", ")", "\n", "lstm_in_beamable", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ",", "prev_out", "]", ",", "dim", "=", "-", "1", ")", "\n", "lstm_in", "=", "lstm_in_beamable", ".", "contiguous", "(", ")", ".", "view", "(", "beam", "*", "batch", ",", "-", "1", ")", "\n", "prev_states", "=", "(", "h", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "-", "1", ",", "d", ")", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "-", "1", ",", "d", ")", ")", "\n", "h", ",", "c", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "states", "=", "(", "h", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "beam", ",", "batch", ",", "-", "1", ")", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "beam", ",", "batch", ",", "-", "1", ")", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "\n", "# attention is beamable", "\n", "query", "=", "torch", ".", "matmul", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "\n", "query_external", "=", "torch", ".", "matmul", "(", "lstm_out", ",", "self", ".", "_attn_w_external", ")", "\n", "attention", ",", "attn_mask", ",", "attention_external", ",", "attn_mask_external", ",", "extend_src", ",", "extend_vsize", "=", "attention", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "context_external", ",", "score_external", "=", "step_attention", "(", "query_external", ",", "attention_external", ",", "attention_external", ",", "\n", "attn_mask_external", ")", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", ",", "context_external", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "# copy mechanism is not beamable", "\n", "gen_prob", "=", "self", ".", "_compute_gen_prob", "(", "\n", "dec_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", "*", "beam", ",", "-", "1", ")", ",", "extend_vsize", ")", "\n", "copy_prob", "=", "torch", ".", "sigmoid", "(", "\n", "self", ".", "_copy", "(", "context", ",", "lstm_out", ",", "lstm_in_beamable", ")", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "lp", "=", "torch", ".", "log", "(", "\n", "(", "(", "-", "copy_prob", "+", "1", ")", "*", "gen_prob", "\n", ")", ".", "scatter_add", "(", "\n", "dim", "=", "1", ",", "\n", "index", "=", "extend_src", ".", "expand_as", "(", "score", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "beam", "*", "batch", ",", "-", "1", ")", ",", "\n", "source", "=", "score", ".", "contiguous", "(", ")", ".", "view", "(", "beam", "*", "batch", ",", "-", "1", ")", "*", "copy_prob", "\n", ")", "+", "1e-8", ")", ".", "contiguous", "(", ")", ".", "view", "(", "beam", ",", "batch", ",", "-", "1", ")", "\n", "\n", "k_lp", ",", "k_tok", "=", "lp", ".", "topk", "(", "k", "=", "k", ",", "dim", "=", "-", "1", ")", "\n", "return", "k_tok", ",", "k_lp", ",", "(", "states", ",", "dec_out", ")", ",", "score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.Seq2SeqSumm.__init__": [[15, 62], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.LSTM", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "rnn.MultiLayerLSTMCells", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.Sequential", "summ.AttentionalLSTMDecoder", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# embedding weight parameter is shared between encoder, decoder,", "\n", "# and used as final projection layer to vocab logit", "\n", "# can initialize with pretrained word vectors", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "emb_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "_enc_lstm", "=", "nn", ".", "LSTM", "(", "\n", "emb_dim", ",", "n_hidden", ",", "n_layer", ",", "\n", "bidirectional", "=", "bidirectional", ",", "dropout", "=", "dropout", "\n", ")", "\n", "# initial encoder LSTM states are learned parameters", "\n", "state_layer", "=", "n_layer", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "self", ".", "_init_enc_h", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "state_layer", ",", "n_hidden", ")", "\n", ")", "\n", "self", ".", "_init_enc_c", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "state_layer", ",", "n_hidden", ")", "\n", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_enc_h", ",", "-", "INIT", ",", "INIT", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_enc_c", ",", "-", "INIT", ",", "INIT", ")", "\n", "\n", "# vanillat lstm / LNlstm", "\n", "self", ".", "_dec_lstm", "=", "MultiLayerLSTMCells", "(", "\n", "2", "*", "emb_dim", ",", "n_hidden", ",", "n_layer", ",", "dropout", "=", "dropout", "\n", ")", "\n", "# project encoder final states to decoder initial states", "\n", "enc_out_dim", "=", "n_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "self", ".", "_dec_h", "=", "nn", ".", "Linear", "(", "enc_out_dim", ",", "n_hidden", ",", "bias", "=", "False", ")", "\n", "self", ".", "_dec_c", "=", "nn", ".", "Linear", "(", "enc_out_dim", ",", "n_hidden", ",", "bias", "=", "False", ")", "\n", "# multiplicative attention", "\n", "self", ".", "_attn_wm", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "enc_out_dim", ",", "n_hidden", ")", ")", "\n", "self", ".", "_attn_wq", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ",", "n_hidden", ")", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wm", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wq", ")", "\n", "# project decoder output to emb_dim, then", "\n", "# apply weight matrix from embedding layer", "\n", "self", ".", "_projection", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "2", "*", "n_hidden", ",", "n_hidden", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "n_hidden", ",", "emb_dim", ",", "bias", "=", "False", ")", "\n", ")", "\n", "# functional object for easier usage", "\n", "self", ".", "_decoder", "=", "AttentionalLSTMDecoder", "(", "\n", "self", ".", "_embedding", ",", "self", ".", "_dec_lstm", ",", "\n", "self", ".", "_attn_wq", ",", "self", ".", "_projection", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.Seq2SeqSumm.forward": [[64, 69], ["summ.Seq2SeqSumm.encode", "util.len_mask().unsqueeze", "summ.Seq2SeqSumm._decoder", "util.len_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "forward", "(", "self", ",", "article", ",", "art_lens", ",", "abstract", ")", ":", "\n", "        ", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "logit", "=", "self", ".", "_decoder", "(", "(", "attention", ",", "mask", ")", ",", "init_dec_states", ",", "abstract", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.Seq2SeqSumm.encode": [[70, 100], ["rnn.lstm_encoder", "torch.stack", "torch.stack", "torch.matmul().transpose", "summ.Seq2SeqSumm._projection", "summ.Seq2SeqSumm._init_enc_h.size", "summ.Seq2SeqSumm._init_enc_h.size", "summ.Seq2SeqSumm._init_enc_h.unsqueeze().expand", "summ.Seq2SeqSumm._init_enc_c.unsqueeze().expand", "torch.cat", "len", "torch.cat", "torch.cat", "summ.Seq2SeqSumm._dec_h", "summ.Seq2SeqSumm._dec_c", "torch.matmul", "summ.Seq2SeqSumm._init_enc_h.unsqueeze", "summ.Seq2SeqSumm._init_enc_c.unsqueeze", "h.chunk", "c.chunk", "util.sequence_mean"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.lstm_encoder", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_mean"], ["", "def", "encode", "(", "self", ",", "article", ",", "art_lens", "=", "None", ")", ":", "# article: [batch_size, seq_len]", "\n", "        ", "size", "=", "(", "\n", "self", ".", "_init_enc_h", ".", "size", "(", "0", ")", ",", "\n", "len", "(", "art_lens", ")", "if", "art_lens", "else", "1", ",", "\n", "self", ".", "_init_enc_h", ".", "size", "(", "1", ")", "\n", ")", "\n", "init_enc_states", "=", "(", "\n", "self", ".", "_init_enc_h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ",", "\n", "self", ".", "_init_enc_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", "\n", ")", "\n", "enc_art", ",", "final_states", "=", "lstm_encoder", "(", "\n", "article", ",", "self", ".", "_enc_lstm", ",", "art_lens", ",", "\n", "init_enc_states", ",", "self", ".", "_embedding", "\n", ")", "# enc_art: [seq_len, batch, 2*hidden_dim], final_states: ([1, batch, 2*hidden_dim], [1, batch, 2*hidden_dim])", "\n", "if", "self", ".", "_enc_lstm", ".", "bidirectional", ":", "\n", "            ", "h", ",", "c", "=", "final_states", "\n", "final_states", "=", "(", "\n", "torch", ".", "cat", "(", "h", ".", "chunk", "(", "2", ",", "dim", "=", "0", ")", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "cat", "(", "c", ".", "chunk", "(", "2", ",", "dim", "=", "0", ")", ",", "dim", "=", "2", ")", "\n", ")", "\n", "", "init_h", "=", "torch", ".", "stack", "(", "[", "self", ".", "_dec_h", "(", "s", ")", "\n", "for", "s", "in", "final_states", "[", "0", "]", "]", ",", "dim", "=", "0", ")", "# [1, batch_size, hidden_dim]", "\n", "init_c", "=", "torch", ".", "stack", "(", "[", "self", ".", "_dec_c", "(", "s", ")", "\n", "for", "s", "in", "final_states", "[", "1", "]", "]", ",", "dim", "=", "0", ")", "\n", "init_dec_states", "=", "(", "init_h", ",", "init_c", ")", "\n", "attention", "=", "torch", ".", "matmul", "(", "enc_art", ",", "self", ".", "_attn_wm", ")", ".", "transpose", "(", "0", ",", "1", ")", "# [batch, seq_len, hidden_dim]", "\n", "init_attn_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "\n", "[", "init_h", "[", "-", "1", "]", ",", "sequence_mean", "(", "attention", ",", "art_lens", ",", "dim", "=", "1", ")", "]", ",", "dim", "=", "1", "\n", ")", ")", "#  init_h[-1]: [batch_size, hidden_dim], seq_mean (batch, hidden_dim)", "\n", "return", "attention", ",", "(", "init_dec_states", ",", "init_attn_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.Seq2SeqSumm.batch_decode": [[101, 117], ["len", "summ.Seq2SeqSumm.encode", "util.len_mask().unsqueeze", "torch.LongTensor().to", "range", "summ.Seq2SeqSumm._decoder.decode_step", "outputs.append", "attns.append", "util.len_mask", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "batch_decode", "(", "self", ",", "article", ",", "art_lens", ",", "go", ",", "eos", ",", "max_len", ")", ":", "\n", "        ", "\"\"\" greedy decode support batching\"\"\"", "\n", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "attention", "=", "(", "attention", ",", "mask", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", "*", "batch_size", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "outputs", ".", "append", "(", "tok", "[", ":", ",", "0", "]", ")", "\n", "attns", ".", "append", "(", "attn_score", ")", "\n", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.Seq2SeqSumm.decode": [[118, 133], ["summ.Seq2SeqSumm.encode", "torch.LongTensor().to", "range", "summ.Seq2SeqSumm._decoder.decode_step", "outputs.append", "attns.append", "torch.LongTensor", "tok[].item", "tok[].item", "attn_score.squeeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step"], ["", "def", "decode", "(", "self", ",", "article", ",", "go", ",", "eos", ",", "max_len", ")", ":", "\n", "        ", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ")", "\n", "attention", "=", "(", "attention", ",", "None", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", "==", "eos", ":", "\n", "                ", "break", "\n", "", "outputs", ".", "append", "(", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ")", "\n", "attns", ".", "append", "(", "attn_score", ".", "squeeze", "(", "0", ")", ")", "\n", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.Seq2SeqSumm.set_embedding": [[134, 138], ["summ.Seq2SeqSumm._embedding.weight.data.copy_", "summ.Seq2SeqSumm._embedding.weight.size", "embedding.size"], "methods", ["None"], ["", "def", "set_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "\"\"\"embedding is the weight matrix\"\"\"", "\n", "assert", "self", ".", "_embedding", ".", "weight", ".", "size", "(", ")", "==", "embedding", ".", "size", "(", ")", "\n", "self", ".", "_embedding", ".", "weight", ".", "data", ".", "copy_", "(", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.__init__": [[141, 147], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embedding", ",", "lstm", ",", "attn_w", ",", "projection", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_embedding", "=", "embedding", "\n", "self", ".", "_lstm", "=", "lstm", "\n", "self", ".", "_attn_w", "=", "attn_w", "\n", "self", ".", "_projection", "=", "projection", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.__call__": [[148, 158], ["target.size", "range", "torch.stack", "summ.AttentionalLSTMDecoder._step", "logits.append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._step"], ["", "def", "__call__", "(", "self", ",", "attention", ",", "init_states", ",", "target", ")", ":", "\n", "        ", "max_len", "=", "target", ".", "size", "(", "1", ")", "\n", "states", "=", "init_states", "\n", "logits", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", "=", "target", "[", ":", ",", "i", ":", "i", "+", "1", "]", "\n", "logit", ",", "states", ",", "_", "=", "self", ".", "_step", "(", "tok", ",", "states", ",", "attention", ")", "\n", "logits", ".", "append", "(", "logit", ")", "\n", "", "logit", "=", "torch", ".", "stack", "(", "logits", ",", "dim", "=", "1", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder._step": [[159, 175], ["torch.cat", "summ.AttentionalLSTMDecoder._lstm", "torch.mm", "attention.step_attention", "summ.AttentionalLSTMDecoder._projection", "torch.mm", "torch.cat", "summ.AttentionalLSTMDecoder._embedding.weight.t", "summ.AttentionalLSTMDecoder._embedding().squeeze", "summ.AttentionalLSTMDecoder._embedding"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention"], ["", "def", "_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ")", ":", "\n", "        ", "prev_states", ",", "prev_out", "=", "states", "\n", "lstm_in", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ".", "squeeze", "(", "1", ")", ",", "prev_out", "]", ",", "\n", "dim", "=", "1", "\n", ")", "\n", "states", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "query", "=", "torch", ".", "mm", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "\n", "attention", ",", "attn_mask", "=", "attention", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", "]", ",", "dim", "=", "1", ")", ")", "\n", "states", "=", "(", "states", ",", "dec_out", ")", "\n", "logit", "=", "torch", ".", "mm", "(", "dec_out", ",", "self", ".", "_embedding", ".", "weight", ".", "t", "(", ")", ")", "\n", "return", "logit", ",", "states", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step": [[176, 180], ["summ.AttentionalLSTMDecoder._step", "torch.max"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._step"], ["", "def", "decode_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ")", ":", "\n", "        ", "logit", ",", "states", ",", "score", "=", "self", ".", "_step", "(", "tok", ",", "states", ",", "attention", ")", "\n", "out", "=", "torch", ".", "max", "(", "logit", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "return", "out", ",", "states", ",", "score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorRL.__init__": [[16, 34], ["torch.nn.Module.__init__", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "rnn.MultiLayerLSTMCells.convert", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "ptr_net._init_h.clone", "ptr_net._init_c.clone", "ptr_net._init_i.clone", "ptr_net._attn_wm.clone", "ptr_net._attn_wq.clone", "ptr_net._attn_v.clone", "ptr_net._hop_wm.clone", "ptr_net._hop_wq.clone", "ptr_net._hop_v.clone"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert"], ["from", "training", "import", "BasicPipeline", "\n", "import", "sys", "\n", "\n", "#from decoding import make_html_safe", "\n", "from", "data", ".", "batcher", "import", "bert_tokenizer", "\n", "\n", "\n", "def", "a2c_validate", "(", "agent", ",", "abstractor", ",", "loader", ",", "disable_selected_mask", "=", "False", ",", "is_conditional_abs", "=", "False", ")", ":", "\n", "    ", "agent", ".", "eval", "(", ")", "\n", "start", "=", "time", "(", ")", "\n", "print", "(", "'start running validation...'", ",", "end", "=", "''", ")", "\n", "avg_reward", "=", "0", "\n", "i", "=", "0", "\n", "\n", "# debug", "\n", "#extracted_local_idx_2dlist = []", "\n", "#num_batches = 0", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorRL.forward": [[35, 61], ["torch.mm", "torch.mm", "rl.PtrExtractorRL._init_i.unsqueeze", "range", "rl.PtrExtractorRL._init_h.unsqueeze", "rl.PtrExtractorRL._init_c.unsqueeze", "rl.PtrExtractorRL._lstm_cell", "range", "rl.PtrExtractorRL.attention_score", "outputs.append", "attn_mem[].unsqueeze", "rl.PtrExtractorRL.attention", "torch.nn.functional.softmax", "torch.distributions.Categorical", "rl.PtrExtractorRL.attention_score", "out[].item", "o[].item"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score"], ["        ", "for", "art_batch", ",", "raw_art_batch", ",", "raw_abs_batch", "in", "loader", ":", "\n", "\n", "# debug", "\n", "#num_batches += 1", "\n", "\n", "            ", "num_articles", "=", "len", "(", "art_batch", ")", "\n", "num_ext_sents", "=", "0", "\n", "if", "is_conditional_abs", ":", "\n", "                ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "ext_sents", "=", "[", "]", "\n", "", "ext_inds", "=", "[", "]", "\n", "for", "article_i", ",", "raw_arts_tokenized", "in", "enumerate", "(", "art_batch", ")", ":", "\n", "\n", "# debug", "\n", "                ", "\"\"\"\n                if num_batches == 2 and article_i == 27:\n                    print(\"disable_selected_mask\")\n                    print(disable_selected_mask)\n                    print(\"raw_arts_tokenized\")\n                    print(raw_arts_tokenized)\n                    exit()\n                \"\"\"", "\n", "\n", "indices", "=", "agent", "(", "raw_arts_tokenized", ",", "disable_selected_mask", ")", "\n", "ext_inds", "+=", "[", "(", "num_ext_sents", ",", "len", "(", "indices", ")", "-", "1", ")", "]", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorRL.attention_score": [[62, 68], ["torch.mm().t", "torch.mm", "torch.mm", "torch.nn.functional.tanh", "v.unsqueeze"], "methods", ["None"], ["num_ext_sents", "+=", "len", "(", "indices", ")", "-", "1", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "                    ", "num_selected_sents_excluded_eos", "=", "len", "(", "indices", ")", "-", "1", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                        ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorRL.attention": [[69, 76], ["torch.nn.functional.softmax", "torch.mm", "rl.PtrExtractorRL.attention_score"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score"], ["range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "\n", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "idx_i", ",", "idx", "in", "enumerate", "(", "indices", ")", ":", "\n", "                        ", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", ":", "\n", "# ext_sents.append(raw_arts_tokenized[idx.item()])", "\n", "                            ", "sequential_ext_sents", "[", "idx_i", "]", ".", "append", "(", "raw_arts_tokenized", "[", "idx", ".", "item", "(", ")", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorRLStop.__init__": [[79, 89], ["rl.PtrExtractorRL.__init__", "isinstance", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["                    ", "ext_sents", "+=", "[", "raw_art_batch", "[", "article_i", "]", "[", "idx", ".", "item", "(", ")", "]", "for", "idx", "in", "indices", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", "]", "\n", "\n", "# debug", "\n", "#extracted_local_idx_2dlist.append([idx.item() for idx in indices if idx.item() < len(raw_arts_tokenized)])", "\n", "\n", "# abstract", "\n", "", "", "if", "is_conditional_abs", ":", "\n", "                ", "all_summs", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "                ", "all_summs", "=", "abstractor", "(", "ext_sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorRLStop.forward": [[90, 133], ["torch.cat.size", "torch.cat", "torch.mm", "torch.mm", "rl.PtrExtractorRLStop._init_i.unsqueeze", "rl.PtrExtractorRL.forward", "rl.PtrExtractorRLStop._init_h.unsqueeze", "rl.PtrExtractorRLStop._init_c.unsqueeze", "rl.PtrExtractorRLStop._lstm_cell", "range", "rl.PtrExtractorRL.attention_score", "outputs.append", "attn_mem[].unsqueeze", "rl.PtrExtractorRLStop._stop.unsqueeze", "rl.PtrExtractorRL.attention", "torch.nn.functional.softmax", "torch.distributions.Categorical", "dists.append", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample.item", "rl.PtrExtractorRL.attention_score", "torch.distributions.Categorical.sample.item", "o.item"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.forward", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score"], ["", "for", "(", "j", ",", "n", ")", ",", "abs_sents", "in", "zip", "(", "ext_inds", ",", "raw_abs_batch", ")", ":", "\n", "                ", "summs", "=", "all_summs", "[", "j", ":", "j", "+", "n", "]", "\n", "# python ROUGE-1 (not official evaluation)", "\n", "avg_reward", "+=", "compute_rouge_n", "(", "list", "(", "concat", "(", "summs", ")", ")", ",", "\n", "list", "(", "concat", "(", "abs_sents", ")", ")", ",", "n", "=", "1", ")", "\n", "\n", "i", "+=", "1", "\n", "", "", "", "avg_reward", "/=", "(", "i", "/", "100", ")", "\n", "print", "(", "'finished in {}! avg reward: {:.2f}'", ".", "format", "(", "\n", "timedelta", "(", "seconds", "=", "int", "(", "time", "(", ")", "-", "start", ")", ")", ",", "avg_reward", ")", ")", "\n", "\n", "# debug", "\n", "#extracted_local_idx_2darray = np.array(extracted_local_idx_2dlist)", "\n", "#extracted_local_idx_2darray.dump('/home/ubuntu/ken/projects/abstract_then_extract/val_selected_indices_2d.dat')", "\n", "\n", "return", "{", "'reward'", ":", "avg_reward", "}", "\n", "\n", "\n", "", "def", "a2c_train_step", "(", "agent", ",", "abstractor", ",", "loader", ",", "opt", ",", "grad_fn", ",", "\n", "gamma", "=", "0.99", ",", "reward_fn", "=", "compute_rouge_l", ",", "\n", "stop_reward_fn", "=", "compute_rouge_n", "(", "n", "=", "1", ")", ",", "stop_coeff", "=", "1.0", ",", "reward_type", "=", "0", ",", "disable_selected_mask", "=", "False", ",", "is_conditional_abs", "=", "False", ",", "debug", "=", "False", ")", ":", "\n", "#print('a2c train step')", "\n", "#sys.stdout.flush()", "\n", "    ", "opt", ".", "zero_grad", "(", ")", "\n", "indices", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "baselines", "=", "[", "]", "\n", "if", "is_conditional_abs", ":", "\n", "        ", "sequential_ext_sents", "=", "[", "]", "\n", "sequential_article_ids", "=", "[", "]", "\n", "", "else", ":", "\n", "        ", "ext_sents", "=", "[", "]", "\n", "", "art_batch", ",", "raw_art_batch", ",", "raw_abs_batch", "=", "next", "(", "loader", ")", "\n", "#print('Loader next')", "\n", "#sys.stdout.flush()", "\n", "num_articles", "=", "len", "(", "art_batch", ")", "\n", "# extract", "\n", "for", "article_i", ",", "raw_arts_tokenized", "in", "enumerate", "(", "art_batch", ")", ":", "# extract sent indices for each article", "\n", "        "]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorWithCandidateRLStop.__init__": [[136, 138], ["rl.PtrExtractorRLStop.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["(", "inds", ",", "ms", ")", ",", "bs", "=", "agent", "(", "raw_arts_tokenized", ",", "disable_selected_mask", ",", "debug", "=", "debug", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrExtractorWithCandidateRLStop.forward": [[139, 187], ["torch.cat.size", "torch.cat", "torch.mm", "torch.mm", "rl.PtrExtractorWithCandidateRLStop._init_i.unsqueeze", "rl.PtrExtractorRLStop.forward", "rl.PtrExtractorWithCandidateRLStop._init_h.unsqueeze", "rl.PtrExtractorWithCandidateRLStop._init_c.unsqueeze", "rl.PtrExtractorWithCandidateRLStop._lstm_cell", "range", "rl.PtrExtractorRL.attention_score", "outputs.append", "attn_mem[].unsqueeze", "rl.PtrExtractorWithCandidateRLStop._stop.unsqueeze", "rl.PtrExtractorRL.attention", "torch.nn.functional.softmax", "torch.distributions.Categorical", "dists.append", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample.item", "torch.distributions.Categorical.sample.item", "rl.PtrExtractorRL.attention_score", "range", "torch.distributions.Categorical.sample.item"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.forward", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score"], ["\n", "\n", "baselines", ".", "append", "(", "bs", ")", "\n", "indices", ".", "append", "(", "inds", ")", "\n", "probs", ".", "append", "(", "ms", ")", "\n", "\n", "\n", "if", "is_conditional_abs", ":", "\n", "# insert place holder to sequential_ext_sents", "\n", "            ", "num_selected_sents_excluded_eos", "=", "len", "(", "inds", ")", "-", "1", "\n", "if", "num_selected_sents_excluded_eos", ">", "len", "(", "sequential_ext_sents", ")", ":", "\n", "                ", "[", "sequential_ext_sents", ".", "append", "(", "[", "]", ")", "for", "_", "in", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_ext_sents", ")", ")", "]", "\n", "[", "sequential_article_ids", ".", "append", "(", "[", "]", ")", "for", "_", "in", "range", "(", "num_selected_sents_excluded_eos", "-", "len", "(", "sequential_article_ids", ")", ")", "]", "\n", "\n", "", "for", "i", ",", "idx", "in", "enumerate", "(", "inds", ")", ":", "\n", "                ", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", ":", "\n", "#ext_sents.append(raw_arts_tokenized[idx.item()])", "\n", "                    ", "sequential_ext_sents", "[", "i", "]", ".", "append", "(", "raw_arts_tokenized", "[", "idx", ".", "item", "(", ")", "]", ")", "\n", "sequential_article_ids", "[", "i", "]", ".", "append", "(", "article_i", ")", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "ext_sents", "+=", "[", "raw_art_batch", "[", "article_i", "]", "[", "idx", ".", "item", "(", ")", "]", "\n", "for", "idx", "in", "inds", "if", "idx", ".", "item", "(", ")", "<", "len", "(", "raw_arts_tokenized", ")", "]", "\n", "\n", "# abstract", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "if", "is_conditional_abs", ":", "\n", "            ", "summaries", "=", "abstractor", "(", "sequential_ext_sents", ",", "sequential_article_ids", ",", "num_articles", ")", "\n", "", "else", ":", "\n", "            ", "summaries", "=", "abstractor", "(", "ext_sents", ")", "\n", "\n", "", "", "i", "=", "0", "\n", "rewards", "=", "[", "]", "\n", "avg_reward", "=", "0", "\n", "#reward_lens = []", "\n", "#print(\"len indices and batch\")", "\n", "#print(len(indices))", "\n", "#print(len(abs_batch))", "\n", "#print()", "\n", "#print()", "\n", "for", "inds", ",", "abss", "in", "zip", "(", "indices", ",", "raw_abs_batch", ")", ":", "\n", "# process each article", "\n", "        ", "if", "reward_type", "==", "0", ":", "\n", "            ", "rs", "=", "(", "[", "reward_fn", "(", "summaries", "[", "i", "+", "j", "]", ",", "abss", "[", "j", "]", ")", "for", "j", "in", "range", "(", "min", "(", "len", "(", "inds", ")", "-", "1", ",", "len", "(", "abss", ")", ")", ")", "]", "\n", "+", "[", "0", "for", "_", "in", "range", "(", "max", "(", "0", ",", "len", "(", "inds", ")", "-", "1", "-", "len", "(", "abss", ")", ")", ")", "]", "\n", "+", "[", "stop_coeff", "*", "stop_reward_fn", "(", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrScorer.__init__": [[190, 211], ["torch.nn.Module.__init__", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "rnn.MultiLayerLSTMCells.convert", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "ptr_net._init_h.clone", "ptr_net._init_c.clone", "ptr_net._init_i.clone", "ptr_net._attn_wm.clone", "ptr_net._attn_wq.clone", "ptr_net._attn_v.clone", "ptr_net._hop_wm.clone", "ptr_net._hop_wq.clone", "ptr_net._hop_v.clone"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert"], ["", "elif", "reward_type", "==", "1", ":", "\n", "            ", "sent_reward", "=", "[", "reward_fn", "(", "summaries", "[", "i", ":", "i", "+", "j", "]", ",", "abss", ")", "for", "j", "in", "range", "(", "len", "(", "inds", ")", "-", "1", ")", "]", "\n", "shaped_reward", "=", "[", "sent_reward", "[", "0", "]", "]", "+", "[", "sent_reward", "[", "i", "+", "1", "]", "-", "sent_reward", "[", "i", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sent_reward", ")", "-", "1", ")", "]", "if", "len", "(", "sent_reward", ")", ">", "0", "else", "[", "]", "\n", "\n", "rs", "=", "shaped_reward", "+", "[", "stop_coeff", "*", "stop_reward_fn", "(", "\n", "list", "(", "concat", "(", "summaries", "[", "i", ":", "i", "+", "len", "(", "inds", ")", "-", "1", "]", ")", ")", ",", "\n", "list", "(", "concat", "(", "abss", ")", ")", ")", "]", "\n", "#print(\"rs: {}\".format(rs))", "\n", "", "elif", "reward_type", "==", "2", "or", "reward_type", "==", "3", "or", "reward_type", "==", "4", ":", "\n", "# debug", "\n", "#print(\"abss\")", "\n", "#print(abss)", "\n", "#print()", "\n", "#print(\"prediction\")", "\n", "#print([ (i,i + j) for j in range(min(len(inds) - 1, len(abss)))])", "\n", "#print( [summaries[i:i + j] for j in range( min(len(inds)-1, len(abss)) )] )", "\n", "#print()", "\n", "\n", "            ", "sent_reward", "=", "[", "reward_fn", "(", "summaries", "[", "i", ":", "i", "+", "j", "+", "1", "]", ",", "abss", ")", "for", "j", "in", "range", "(", "min", "(", "len", "(", "inds", ")", "-", "1", ",", "len", "(", "abss", ")", ")", ")", "]", "\n", "shaped_reward", "=", "[", "sent_reward", "[", "0", "]", "]", "+", "[", "sent_reward", "[", "i", "+", "1", "]", "-", "sent_reward", "[", "i", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sent_reward", ")", "-", "1", ")", "]", "if", "len", "(", "sent_reward", ")", ">", "0", "else", "[", "]", "\n", "\n", "# debug", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrScorer.forward": [[212, 231], ["torch.mm", "torch.mm", "rl.PtrScorer._init_i.unsqueeze", "range", "rl.PtrScorer._init_h.unsqueeze", "rl.PtrScorer._init_c.unsqueeze", "rl.PtrScorer._lstm_cell", "range", "rl.PtrScorer.attention", "rl.PtrScorer._score_linear", "scores.append", "rl.PtrScorer.attention"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention"], ["#print(\"sent_reward\")", "\n", "#print(sent_reward)", "\n", "#print(\"shaped_reward\")", "\n", "#print(shaped_reward)", "\n", "#print()", "\n", "\n", "rs", "=", "shaped_reward", "+", "[", "0", "for", "_", "in", "range", "(", "max", "(", "0", ",", "len", "(", "inds", ")", "-", "1", "-", "len", "(", "abss", ")", ")", ")", "]", "+", "[", "stop_coeff", "*", "stop_reward_fn", "(", "\n", "list", "(", "concat", "(", "summaries", "[", "i", ":", "i", "+", "len", "(", "inds", ")", "-", "1", "]", ")", ")", ",", "\n", "list", "(", "concat", "(", "abss", ")", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "#print(\"inds: {}\".format(inds))", "\n", "#print(\"rs: {}\".format(rs))", "\n", "#if len(inds) - 1 == 0:", "\n", "#    print(\"rs: {}\".format(rs))", "\n", "#    print(\"stop reward: {}\".format([stop_coeff*stop_reward_fn(", "\n", "#              list(concat(summaries[i:i+len(inds)-1])),", "\n", "#              list(concat(abss)))]))", "\n", "\n", "", "assert", "len", "(", "rs", ")", "==", "len", "(", "inds", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.PtrScorer.attention": [[232, 239], ["torch.nn.functional.softmax", "torch.mm", "torch.mm", "torch.mm().t", "torch.mm", "torch.nn.functional.tanh", "v.unsqueeze"], "methods", ["None"], ["#raise ValueError", "\n", "\n", "avg_reward", "+=", "rs", "[", "-", "1", "]", "/", "stop_coeff", "\n", "i", "+=", "len", "(", "inds", ")", "-", "1", "\n", "# compute discounted rewards", "\n", "R", "=", "0", "\n", "disc_rs", "=", "[", "]", "# a list of discounted reward, with len=len(inds)", "\n", "for", "r", "in", "rs", "[", ":", ":", "-", "1", "]", ":", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCritic.__init__": [[243, 252], ["torch.nn.Module.__init__", "rl.PtrExtractorRLStop", "rl.PtrScorer"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["", "rewards", "+=", "disc_rs", "# a list of all discounted reward in the batch.", "\n", "\n", "# baselines", "\n", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCritic.forward": [[253, 273], ["rl.ActorCritic._batcher", "rl.ActorCritic._sent_enc().unsqueeze", "rl.ActorCritic._art_enc().squeeze", "min", "rl.ActorCritic._ext", "rl.ActorCritic._ext", "rl.ActorCritic._scr", "rl.ActorCritic._sent_enc", "rl.ActorCritic._art_enc", "len", "len"], "methods", ["None"], ["\n", "\n", "indices", "=", "list", "(", "concat", "(", "indices", ")", ")", "\n", "probs", "=", "list", "(", "concat", "(", "probs", ")", ")", "\n", "baselines", "=", "list", "(", "concat", "(", "baselines", ")", ")", "\n", "# standardize rewards", "\n", "reward", "=", "torch", ".", "Tensor", "(", "rewards", ")", ".", "to", "(", "baselines", "[", "0", "]", ".", "device", ")", "\n", "reward", "=", "(", "reward", "-", "reward", ".", "mean", "(", ")", ")", "/", "(", "\n", "reward", ".", "std", "(", ")", "+", "float", "(", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ")", ")", "\n", "baseline", "=", "torch", ".", "cat", "(", "baselines", ")", ".", "squeeze", "(", ")", "\n", "avg_advantage", "=", "0", "\n", "losses", "=", "[", "]", "\n", "for", "action", ",", "p", ",", "r", ",", "b", "in", "zip", "(", "indices", ",", "probs", ",", "reward", ",", "baseline", ")", ":", "\n", "        ", "advantage", "=", "r", "-", "b", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCriticCand.__init__": [[275, 284], ["torch.nn.Module.__init__", "rl.PtrExtractorWithCandidateRLStop", "rl.PtrScorer"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["losses", ".", "append", "(", "-", "p", ".", "log_prob", "(", "action", ")", "\n", "*", "(", "advantage", "/", "len", "(", "indices", ")", ")", ")", "# divide by T*B", "\n", "", "critic_loss", "=", "F", ".", "mse_loss", "(", "baseline", ",", "reward", ")", "\n", "# backprop and update", "\n", "autograd", ".", "backward", "(", "\n", "[", "critic_loss", "]", "+", "losses", ",", "\n", "[", "torch", ".", "tensor", "(", "1.0", ")", ".", "to", "(", "critic_loss", ".", "device", ")", "]", "+", "[", "torch", ".", "ones", "(", "1", ")", ".", "to", "(", "critic_loss", ".", "device", ")", "]", "*", "(", "len", "(", "losses", ")", ")", "\n", ")", "\n", "grad_log", "=", "grad_fn", "(", ")", "\n", "opt", ".", "step", "(", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCriticCand.forward": [[285, 326], ["rl.ActorCriticCand._batcher", "rl.ActorCriticCand._candidate_sent_enc().view", "rl.ActorCriticCand._candidate_agg().unsqueeze", "rl.ActorCriticCand._art_enc().squeeze", "rl.ActorCriticCand.size", "torch.cat", "enc_out.view.view.view", "min", "rl.ActorCriticCand._ext", "rl.ActorCriticCand._ext", "rl.ActorCriticCand._scr", "rl.ActorCriticCand._candidate_sent_enc", "rl.ActorCriticCand.size", "rl.ActorCriticCand._candidate_agg", "rl.ActorCriticCand._art_enc", "rl.ActorCriticCand.unsqueeze().expand", "len", "len", "rl.ActorCriticCand.unsqueeze"], "methods", ["None"], ["log_dict", "=", "{", "}", "\n", "log_dict", ".", "update", "(", "grad_log", ")", "\n", "log_dict", "[", "'reward'", "]", "=", "avg_reward", "/", "len", "(", "art_batch", ")", "\n", "log_dict", "[", "'advantage'", "]", "=", "avg_advantage", ".", "item", "(", ")", "/", "len", "(", "indices", ")", "\n", "log_dict", "[", "'mse'", "]", "=", "critic_loss", ".", "item", "(", ")", "\n", "assert", "not", "math", ".", "isnan", "(", "log_dict", "[", "'grad_norm'", "]", ")", "\n", "return", "log_dict", "\n", "\n", "\n", "", "def", "get_grad_fn", "(", "agent", ",", "clip_grad", ",", "max_grad", "=", "1e2", ")", ":", "\n", "    ", "\"\"\" monitor gradient for each sub-component\"\"\"", "\n", "params", "=", "[", "p", "for", "p", "in", "agent", ".", "parameters", "(", ")", "]", "\n", "def", "f", "(", ")", ":", "\n", "        ", "grad_log", "=", "{", "}", "\n", "for", "n", ",", "m", "in", "agent", ".", "named_children", "(", ")", ":", "\n", "            ", "tot_grad", "=", "0", "\n", "for", "p", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "tot_grad", "+=", "p", ".", "grad", ".", "norm", "(", "2", ")", "**", "2", "\n", "", "", "tot_grad", "=", "tot_grad", "**", "(", "1", "/", "2", ")", "\n", "#grad_log['grad_norm'+n] = tot_grad.item()", "\n", "grad_log", "[", "'grad_norm'", "+", "n", "]", "=", "tot_grad", "\n", "", "grad_norm", "=", "clip_grad_norm_", "(", "\n", "[", "p", "for", "p", "in", "params", "if", "p", ".", "requires_grad", "]", ",", "clip_grad", ")", "\n", "grad_norm", "=", "grad_norm", "\n", "#grad_norm = grad_norm.item()", "\n", "if", "max_grad", "is", "not", "None", "and", "grad_norm", ">=", "max_grad", ":", "\n", "            ", "print", "(", "'WARNING: Exploding Gradients {:.2f}'", ".", "format", "(", "grad_norm", ")", ")", "\n", "grad_norm", "=", "max_grad", "\n", "", "grad_log", "[", "'grad_norm'", "]", "=", "grad_norm", "\n", "return", "grad_log", "\n", "", "return", "f", "\n", "\n", "\n", "", "class", "A2CPipeline", "(", "BasicPipeline", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "name", ",", "\n", "net", ",", "abstractor", ",", "\n", "train_batcher", ",", "val_batcher", ",", "\n", "optim", ",", "grad_fn", ",", "\n", "reward_fn", ",", "gamma", ",", "\n", "stop_reward_fn", ",", "stop_coeff", ",", "reward_type", ",", "disable_selected_mask", "=", "False", ",", "is_conditional_abs", "=", "False", ",", "debug", "=", "False", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCriticSentBertCand.__init__": [[328, 341], ["torch.nn.Module.__init__", "rl.ActorCriticSentBertCand._sentence_encoder.eval", "rl.ActorCriticSentBertCand._sentence_encoder.parameters", "rl.PtrExtractorWithCandidateRLStop", "rl.PtrScorer"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["self", ".", "_train_batcher", "=", "train_batcher", "\n", "self", ".", "_val_batcher", "=", "val_batcher", "\n", "self", ".", "_opt", "=", "optim", "\n", "self", ".", "_grad_fn", "=", "grad_fn", "\n", "self", ".", "_abstractor", "=", "abstractor", "\n", "self", ".", "_gamma", "=", "gamma", "\n", "self", ".", "_reward_fn", "=", "reward_fn", "\n", "self", ".", "_stop_reward_fn", "=", "stop_reward_fn", "\n", "self", ".", "_stop_coeff", "=", "stop_coeff", "\n", "self", ".", "_disable_selected_mask", "=", "disable_selected_mask", "\n", "self", ".", "_n_epoch", "=", "0", "# epoch not very useful?", "\n", "self", ".", "_reward_type", "=", "reward_type", "\n", "self", ".", "debug", "=", "debug", "\n", "self", ".", "_is_conditional_abs", "=", "is_conditional_abs", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCriticSentBertCand.forward": [[342, 388], ["rl.ActorCriticSentBertCand._batcher", "torch.ne().to", "rl.ActorCriticSentBertCand._sentence_encoder.encode_tensor", "rl.ActorCriticSentBertCand._bert_w", "enc_candidates.view.view.view", "rl.ActorCriticSentBertCand._candidate_agg().unsqueeze", "rl.ActorCriticSentBertCand._art_enc().squeeze", "rl.ActorCriticSentBertCand.size", "torch.cat", "enc_out.view.view.view", "min", "rl.ActorCriticSentBertCand._ext", "rl.ActorCriticSentBertCand._ext", "rl.ActorCriticSentBertCand._scr", "torch.ne", "rl.ActorCriticSentBertCand.size", "rl.ActorCriticSentBertCand._candidate_agg", "rl.ActorCriticSentBertCand._art_enc", "rl.ActorCriticSentBertCand.unsqueeze().expand", "len", "len", "rl.ActorCriticSentBertCand.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.encode_tensor"], ["\n", "", "def", "batches", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'A2C does not use batcher'", ")", "\n", "\n", "", "def", "train_step", "(", "self", ")", ":", "\n", "# forward pass of model", "\n", "        ", "self", ".", "_net", ".", "train", "(", ")", "\n", "#print('train step')", "\n", "#sys.stdout.flush()", "\n", "log_dict", "=", "a2c_train_step", "(", "\n", "self", ".", "_net", ",", "self", ".", "_abstractor", ",", "\n", "self", ".", "_train_batcher", ",", "\n", "self", ".", "_opt", ",", "self", ".", "_grad_fn", ",", "\n", "self", ".", "_gamma", ",", "self", ".", "_reward_fn", ",", "\n", "self", ".", "_stop_reward_fn", ",", "self", ".", "_stop_coeff", ",", "self", ".", "_reward_type", ",", "self", ".", "_disable_selected_mask", ",", "self", ".", "_is_conditional_abs", ",", "self", ".", "debug", "\n", ")", "\n", "return", "log_dict", "\n", "\n", "", "def", "validate", "(", "self", ")", ":", "\n", "        ", "return", "a2c_validate", "(", "self", ".", "_net", ",", "self", ".", "_abstractor", ",", "self", ".", "_val_batcher", ",", "self", ".", "_disable_selected_mask", ",", "self", ".", "_is_conditional_abs", ")", "\n", "\n", "", "def", "checkpoint", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# explicitly use inherited function in case I forgot :)", "\n", "        ", "return", "super", "(", ")", ".", "checkpoint", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "def", "terminate", "(", "self", ")", ":", "\n", "        ", "pass", "# No extra processs so do nothing", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCriticSentWordBertCand.__init__": [[391, 406], ["torch.nn.Module.__init__", "rl.ActorCriticSentWordBertCand._sentence_encoder.eval", "rl.ActorCriticSentWordBertCand._sentence_encoder.parameters", "rl.PtrExtractorWithCandidateRLStop", "rl.PtrScorer"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rl.ActorCriticSentWordBertCand.forward": [[407, 455], ["rl.ActorCriticSentWordBertCand._batcher", "rl.ActorCriticSentWordBertCand.size", "torch.ne().to", "rl.ActorCriticSentWordBertCand._sentence_encoder.encode_word", "rl.ActorCriticSentWordBertCand._bert_w", "rl.ActorCriticSentWordBertCand._candidate_sent_enc().view", "rl.ActorCriticSentWordBertCand._candidate_agg().unsqueeze", "rl.ActorCriticSentWordBertCand._art_enc().squeeze", "rl.ActorCriticSentWordBertCand.size", "torch.cat", "enc_out.view.view.view", "min", "rl.ActorCriticSentWordBertCand._ext", "rl.ActorCriticSentWordBertCand._ext", "rl.ActorCriticSentWordBertCand._scr", "torch.ne", "rl.ActorCriticSentWordBertCand._candidate_sent_enc", "rl.ActorCriticSentWordBertCand._candidate_agg", "rl.ActorCriticSentWordBertCand._art_enc", "rl.ActorCriticSentWordBertCand.unsqueeze().expand", "len", "len", "rl.ActorCriticSentWordBertCand.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.encode_word"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.dot_attention_score": [[5, 8], ["query.matmul", "key.transpose"], "function", ["None"], ["def", "dot_attention_score", "(", "key", ",", "query", ")", ":", "\n", "    ", "\"\"\"[B, Tk, D], [(Bs), B, Tq, D] -> [(Bs), B, Tq, Tk]\"\"\"", "\n", "return", "query", ".", "matmul", "(", "key", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.prob_normalize": [[9, 15], ["score.masked_fill.masked_fill", "torch.nn.functional.softmax"], "function", ["None"], ["", "def", "prob_normalize", "(", "score", ",", "mask", ")", ":", "\n", "    ", "\"\"\" [(...), T]\n    user should handle mask shape\"\"\"", "\n", "score", "=", "score", ".", "masked_fill", "(", "mask", "==", "0", ",", "-", "1e18", ")", "\n", "norm_score", "=", "F", ".", "softmax", "(", "score", ",", "dim", "=", "-", "1", ")", "\n", "return", "norm_score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.attention_aggregate": [[16, 20], ["score.matmul"], "function", ["None"], ["", "def", "attention_aggregate", "(", "value", ",", "score", ")", ":", "\n", "    ", "\"\"\"[B, Tv, D], [(Bs), B, Tq, Tv] -> [(Bs), B, Tq, D]\"\"\"", "\n", "output", "=", "score", ".", "matmul", "(", "value", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention": [[22, 31], ["attention.dot_attention_score", "attention.attention_aggregate", "query.unsqueeze", "torch.nn.functional.softmax", "attention.prob_normalize", "attention_aggregate.squeeze", "prob_normalize.squeeze"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.dot_attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.attention_aggregate", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.prob_normalize"], ["", "def", "step_attention", "(", "query", ",", "key", ",", "value", ",", "mem_mask", "=", "None", ")", ":", "\n", "    ", "\"\"\" query[(Bs), B, D], key[B, T, D], value[B, T, D]\"\"\"", "\n", "score", "=", "dot_attention_score", "(", "key", ",", "query", ".", "unsqueeze", "(", "-", "2", ")", ")", "\n", "if", "mem_mask", "is", "None", ":", "\n", "        ", "norm_score", "=", "F", ".", "softmax", "(", "score", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "norm_score", "=", "prob_normalize", "(", "score", ",", "mem_mask", ")", "\n", "", "output", "=", "attention_aggregate", "(", "value", ",", "norm_score", ")", "\n", "return", "output", ".", "squeeze", "(", "-", "2", ")", ",", "norm_score", ".", "squeeze", "(", "-", "2", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask": [[8, 19], ["max", "len", "torch.ByteTensor().to", "torch.ByteTensor().to.fill_", "enumerate", "mask[].fill_", "torch.ByteTensor"], "function", ["None"], ["def", "len_mask", "(", "lens", ",", "device", ")", ":", "\n", "    ", "\"\"\" users are resposible for shaping\n    Return: tensor_type [B, T]\n    \"\"\"", "\n", "max_len", "=", "max", "(", "lens", ")", "\n", "batch_size", "=", "len", "(", "lens", ")", "\n", "mask", "=", "torch", ".", "ByteTensor", "(", "batch_size", ",", "max_len", ")", ".", "to", "(", "device", ")", "\n", "mask", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lens", ")", ":", "\n", "        ", "mask", "[", "i", ",", ":", "l", "]", ".", "fill_", "(", "1", ")", "\n", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_mean": [[20, 28], ["torch.sum", "torch.stack", "torch.mean", "sequence.size", "len", "zip"], "function", ["None"], ["", "def", "sequence_mean", "(", "sequence", ",", "seq_lens", ",", "dim", "=", "1", ")", ":", "\n", "    ", "if", "seq_lens", ":", "\n", "        ", "assert", "sequence", ".", "size", "(", "0", ")", "==", "len", "(", "seq_lens", ")", "# batch_size", "\n", "sum_", "=", "torch", ".", "sum", "(", "sequence", ",", "dim", "=", "dim", ",", "keepdim", "=", "False", ")", "\n", "mean", "=", "torch", ".", "stack", "(", "[", "s", "/", "l", "for", "s", ",", "l", "in", "zip", "(", "sum_", ",", "seq_lens", ")", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "mean", "=", "torch", ".", "mean", "(", "sequence", ",", "dim", "=", "dim", ",", "keepdim", "=", "False", ")", "\n", "", "return", "mean", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_loss": [[29, 45], ["targets.masked_select", "logits.masked_select().contiguous().view", "targets.size", "logits.size", "xent_fn", "torch.nn.functional.cross_entropy", "logits.size", "logits.masked_select().contiguous", "math.isnan", "math.isinf", "F.cross_entropy.mean().item", "F.cross_entropy.mean().item", "logits.masked_select", "mask.unsqueeze().expand_as", "F.cross_entropy.mean", "F.cross_entropy.mean", "mask.unsqueeze"], "function", ["None"], ["", "def", "sequence_loss", "(", "logits", ",", "targets", ",", "xent_fn", "=", "None", ",", "pad_idx", "=", "0", ")", ":", "\n", "    ", "\"\"\" functional interface of SequenceLoss\"\"\"", "\n", "assert", "logits", ".", "size", "(", ")", "[", ":", "-", "1", "]", "==", "targets", ".", "size", "(", ")", "\n", "\n", "mask", "=", "targets", "!=", "pad_idx", "\n", "target", "=", "targets", ".", "masked_select", "(", "mask", ")", "\n", "logit", "=", "logits", ".", "masked_select", "(", "\n", "mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "logits", ")", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "xent_fn", ":", "\n", "        ", "loss", "=", "xent_fn", "(", "logit", ",", "target", ")", "\n", "", "else", ":", "\n", "        ", "loss", "=", "F", ".", "cross_entropy", "(", "logit", ",", "target", ")", "\n", "", "assert", "(", "not", "math", ".", "isnan", "(", "loss", ".", "mean", "(", ")", ".", "item", "(", ")", ")", "\n", "and", "not", "math", ".", "isinf", "(", "loss", ".", "mean", "(", ")", ".", "item", "(", ")", ")", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.reorder_sequence": [[49, 62], ["torch.LongTensor", "order.to.to", "sequence_emb.index_select", "len", "sequence_emb.size"], "function", ["None"], ["", "def", "reorder_sequence", "(", "sequence_emb", ",", "order", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    sequence_emb: [T, B, D] if not batch_first\n    order: list of sequence length\n    \"\"\"", "\n", "batch_dim", "=", "0", "if", "batch_first", "else", "1", "\n", "assert", "len", "(", "order", ")", "==", "sequence_emb", ".", "size", "(", ")", "[", "batch_dim", "]", "\n", "device", "=", "sequence_emb", ".", "device", "\n", "order", "=", "torch", ".", "LongTensor", "(", "order", ")", "\n", "order", "=", "order", ".", "to", "(", "device", ")", "\n", "sorted_", "=", "sequence_emb", ".", "index_select", "(", "index", "=", "order", ",", "dim", "=", "batch_dim", ")", "\n", "\n", "return", "sorted_", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.reorder_lstm_states": [[63, 78], ["isinstance", "torch.LongTensor().to", "len", "lstm_states[].size", "lstm_states[].size", "len", "lstm_states[].index_select", "lstm_states[].index_select", "lstm_states[].size", "torch.LongTensor"], "function", ["None"], ["", "def", "reorder_lstm_states", "(", "lstm_states", ",", "order", ")", ":", "\n", "    ", "\"\"\"\n    lstm_states: (H, C) of tensor [layer, batch, hidden]\n    order: list of sequence length\n    \"\"\"", "\n", "assert", "isinstance", "(", "lstm_states", ",", "tuple", ")", "\n", "assert", "len", "(", "lstm_states", ")", "==", "2", "\n", "assert", "lstm_states", "[", "0", "]", ".", "size", "(", ")", "==", "lstm_states", "[", "1", "]", ".", "size", "(", ")", "\n", "assert", "len", "(", "order", ")", "==", "lstm_states", "[", "0", "]", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "order", "=", "torch", ".", "LongTensor", "(", "order", ")", ".", "to", "(", "lstm_states", "[", "0", "]", ".", "device", ")", "\n", "sorted_states", "=", "(", "lstm_states", "[", "0", "]", ".", "index_select", "(", "index", "=", "order", ",", "dim", "=", "1", ")", ",", "\n", "lstm_states", "[", "1", "]", ".", "index_select", "(", "index", "=", "order", ",", "dim", "=", "1", ")", ")", "\n", "\n", "return", "sorted_states", "\n", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoder.__init__": [[23, 31], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.ModuleList", "torch.nn.Conv1d", "range"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_dim", ",", "n_hidden", ",", "dropout", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "emb_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "_convs", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv1d", "(", "emb_dim", ",", "n_hidden", ",", "i", ")", "\n", "for", "i", "in", "range", "(", "3", ",", "6", ")", "]", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_grad_handle", "=", "None", "\n", "self", ".", "_output_size", "=", "3", "*", "n_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoder.forward": [[32, 59], ["torch.cat.size", "extract.ConvSentEncoder._embedding", "torch.nn.functional.dropout", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "extract.ConvSentEncoder.transpose", "torch.cat", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "print", "print", "print", "exit", "torch.cat.size", "extract.ConvSentEncoder.size", "torch.nn.functional.dropout.size", "torch.nn.functional.relu().max", "torch.nn.functional.relu", "conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "# input_:  [batch, seq_len]", "\n", "#print(\"conv_size\")", "\n", "#print(input_.size())", "\n", "        ", "batch_size", ",", "seq_len", "=", "input_", ".", "size", "(", ")", "\n", "if", "seq_len", "<=", "2", ":", "\n", "            ", "left_padding", "=", "input_", ".", "new_zeros", "(", "batch_size", ",", "2", ")", "\n", "right_padding", "=", "input_", ".", "new_zeros", "(", "batch_size", ",", "2", ")", "\n", "input_", "=", "torch", ".", "cat", "(", "[", "left_padding", ",", "input_", ",", "right_padding", "]", ",", "dim", "=", "1", ")", "\n", "", "elif", "seq_len", "<=", "4", ":", "\n", "            ", "left_padding", "=", "input_", ".", "new_zeros", "(", "batch_size", ",", "1", ")", "\n", "right_padding", "=", "input_", ".", "new_zeros", "(", "batch_size", ",", "1", ")", "\n", "input_", "=", "torch", ".", "cat", "(", "[", "left_padding", ",", "input_", ",", "right_padding", "]", ",", "dim", "=", "1", ")", "\n", "", "emb_input", "=", "self", ".", "_embedding", "(", "input_", ")", "# [batch, seq_len, embed_size]", "\n", "conv_in", "=", "F", ".", "dropout", "(", "emb_input", ".", "transpose", "(", "1", ",", "2", ")", ",", "\n", "self", ".", "_dropout", ",", "training", "=", "self", ".", "training", ")", "# [batch, embed_size, seq_len]", "\n", "try", ":", "\n", "            ", "output", "=", "torch", ".", "cat", "(", "[", "F", ".", "relu", "(", "conv", "(", "conv_in", ")", ")", ".", "max", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", "for", "conv", "in", "self", ".", "_convs", "]", ",", "dim", "=", "1", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "input_", ".", "size", "(", ")", ")", "\n", "print", "(", "emb_input", ".", "size", "(", ")", ")", "\n", "print", "(", "conv_in", ".", "size", "(", ")", ")", "\n", "exit", "(", ")", "\n", "#print(\"conv_out_size\")", "\n", "#print(output.size())", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoder.set_embedding": [[60, 64], ["extract.ConvSentEncoder._embedding.weight.data.copy_", "extract.ConvSentEncoder._embedding.weight.size", "embedding.size"], "methods", ["None"], ["", "def", "set_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "\"\"\"embedding is the weight matrix\"\"\"", "\n", "assert", "self", ".", "_embedding", ".", "weight", ".", "size", "(", ")", "==", "embedding", ".", "size", "(", ")", "\n", "self", ".", "_embedding", ".", "weight", ".", "data", ".", "copy_", "(", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoder.output_size": [[65, 68], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoderNoEmbedding.__init__": [[75, 82], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.Conv1d", "range"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "n_hidden", ",", "dropout", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_convs", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv1d", "(", "emb_dim", ",", "n_hidden", ",", "i", ")", "\n", "for", "i", "in", "range", "(", "3", ",", "6", ")", "]", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_grad_handle", "=", "None", "\n", "self", ".", "_output_size", "=", "3", "*", "n_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoderNoEmbedding.forward": [[83, 95], ["torch.nn.functional.dropout", "emb_input.transpose", "torch.cat", "print", "print", "exit", "emb_input.size", "torch.nn.functional.dropout.size", "torch.nn.functional.relu().max", "torch.nn.functional.relu", "conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "emb_input", ")", ":", "\n", "# emb_input:  [batch, seq_len, embed_size]", "\n", "        ", "conv_in", "=", "F", ".", "dropout", "(", "emb_input", ".", "transpose", "(", "1", ",", "2", ")", ",", "\n", "self", ".", "_dropout", ",", "training", "=", "self", ".", "training", ")", "# [batch, embed_size, seq_len]", "\n", "try", ":", "\n", "            ", "output", "=", "torch", ".", "cat", "(", "[", "F", ".", "relu", "(", "conv", "(", "conv_in", ")", ")", ".", "max", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", "for", "conv", "in", "self", ".", "_convs", "]", ",", "dim", "=", "1", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "emb_input", ".", "size", "(", ")", ")", "\n", "print", "(", "conv_in", ".", "size", "(", ")", ")", "\n", "exit", "(", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvSentEncoderNoEmbedding.output_size": [[96, 99], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvCandidateAggregator.__init__": [[106, 111], ["torch.nn.Module.__init__", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "n_hidden", ",", "dropout", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "input_dim", ",", "n_hidden", ",", "3", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_grad_handle", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ConvCandidateAggregator.forward": [[112, 121], ["torch.nn.functional.dropout", "input_.transpose", "torch.nn.functional.relu().max", "torch.nn.functional.relu", "extract.ConvCandidateAggregator.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "        ", "\"\"\"\n        :param input_: [batch, num_candidates, input_dim]\n        :return:\n        \"\"\"", "\n", "conv_in", "=", "F", ".", "dropout", "(", "input_", ".", "transpose", "(", "1", ",", "2", ")", ",", "\n", "self", ".", "_dropout", ",", "training", "=", "self", ".", "training", ")", "# [batch, num_candidates, input_dim]", "\n", "output", "=", "F", ".", "relu", "(", "self", ".", "conv", "(", "conv_in", ")", ")", ".", "max", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.FCAggregator.__init__": [[127, 131], ["torch.nn.Module.__init__", "torch.nn.Linear", "extract.FCAggregator._fc.weight.data.fill_"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "num_candidates", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_fc", "=", "nn", ".", "Linear", "(", "num_candidates", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "_fc", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", "/", "num_candidates", ")", "# init it to be mean pooling", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.FCAggregator.forward": [[132, 141], ["input_.transpose", "extract.FCAggregator._fc", "extract.FCAggregator.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "        ", "\"\"\"\n        :param input_: [batch, num_candidates, input_dim]\n        :return:\n        \"\"\"", "\n", "#fc_in = F.dropout(input_.transpose(1, 2), self._dropout, training=self.training)  # [batch, input_dim, num_candidates]", "\n", "fc_in", "=", "input_", ".", "transpose", "(", "1", ",", "2", ")", "# [batch, input_dim, num_candidates]", "\n", "output", "=", "self", ".", "_fc", "(", "fc_in", ")", "# [batch, input_dim, 1]", "\n", "return", "output", ".", "squeeze", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMEncoder.__init__": [[144, 154], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.LSTM", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "dropout", ",", "bidirectional", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init_h", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "n_layer", "*", "(", "2", "if", "bidirectional", "else", "1", ")", ",", "n_hidden", ")", ")", "\n", "self", ".", "_init_c", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "n_layer", "*", "(", "2", "if", "bidirectional", "else", "1", ")", ",", "n_hidden", ")", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_h", ",", "-", "INI", ",", "INI", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_c", ",", "-", "INI", ",", "INI", ")", "\n", "self", ".", "_lstm", "=", "nn", ".", "LSTM", "(", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMEncoder.forward": [[155, 163], ["rnn.lstm_encoder", "lstm_out.transpose", "extract.LSTMEncoder._init_h.size", "input_.size", "extract.LSTMEncoder._init_h.size", "extract.LSTMEncoder._init_h.unsqueeze().expand", "extract.LSTMEncoder._init_c.unsqueeze().expand", "extract.LSTMEncoder._init_h.unsqueeze", "extract.LSTMEncoder._init_c.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.lstm_encoder"], ["", "def", "forward", "(", "self", ",", "input_", ",", "in_lens", "=", "None", ")", ":", "\n", "        ", "\"\"\" [batch_size, max_num_sent, input_dim] Tensor\"\"\"", "\n", "size", "=", "(", "self", ".", "_init_h", ".", "size", "(", "0", ")", ",", "input_", ".", "size", "(", "0", ")", ",", "self", ".", "_init_h", ".", "size", "(", "1", ")", ")", "\n", "init_states", "=", "(", "self", ".", "_init_h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ",", "\n", "self", ".", "_init_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ")", "\n", "lstm_out", ",", "_", "=", "lstm_encoder", "(", "\n", "input_", ",", "self", ".", "_lstm", ",", "in_lens", ",", "init_states", ")", "\n", "return", "lstm_out", ".", "transpose", "(", "0", ",", "1", ")", "# [num_sents * num_]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMEncoder.input_size": [[164, 167], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "input_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_lstm", ".", "input_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMEncoder.hidden_size": [[168, 171], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "hidden_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_lstm", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMEncoder.num_layers": [[172, 175], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_lstm", ".", "num_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMEncoder.bidirectional": [[176, 179], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_lstm", ".", "bidirectional", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ExtractSumm.__init__": [[183, 197], ["torch.nn.Module.__init__", "extract.ConvSentEncoder", "extract.LSTMEncoder", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_dim", ",", "\n", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "bidirectional", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_sent_enc", "=", "ConvSentEncoder", "(", "\n", "vocab_size", ",", "emb_dim", ",", "conv_hidden", ",", "dropout", ")", "\n", "self", ".", "_art_enc", "=", "LSTMEncoder", "(", "\n", "3", "*", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", "\n", ")", "\n", "\n", "lstm_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "self", ".", "_sent_linear", "=", "nn", ".", "Linear", "(", "lstm_out_dim", ",", "1", ")", "\n", "self", ".", "_art_linear", "=", "nn", ".", "Linear", "(", "lstm_out_dim", ",", "lstm_out_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ExtractSumm.forward": [[198, 208], ["extract.ExtractSumm._encode", "torch.matmul", "torch.cat", "extract.ExtractSumm._sent_linear", "enc_art.unsqueeze", "torch.cat", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "forward", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "enc_sent", ",", "enc_art", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "saliency", "=", "torch", ".", "matmul", "(", "enc_sent", ",", "enc_art", ".", "unsqueeze", "(", "2", ")", ")", "\n", "saliency", "=", "torch", ".", "cat", "(", "\n", "[", "s", "[", ":", "n", "]", "for", "s", ",", "n", "in", "zip", "(", "saliency", ",", "sent_nums", ")", "]", ",", "dim", "=", "0", ")", "\n", "content", "=", "self", ".", "_sent_linear", "(", "\n", "torch", ".", "cat", "(", "[", "s", "[", ":", "n", "]", "for", "s", ",", "n", "in", "zip", "(", "enc_sent", ",", "sent_nums", ")", "]", ",", "dim", "=", "0", ")", "\n", ")", "\n", "logit", "=", "(", "content", "+", "saliency", ")", ".", "squeeze", "(", "1", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ExtractSumm.extract": [[209, 225], ["extract.ExtractSumm._encode", "torch.matmul", "extract.ExtractSumm._sent_linear", "enc_art.unsqueeze", "logit.size", "[].tolist", "len", "[].tolist", "zip", "logit[].topk", "l[].topk"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "extract", "(", "self", ",", "article_sents", ",", "sent_nums", "=", "None", ",", "k", "=", "4", ")", ":", "\n", "        ", "\"\"\" extract top-k scored sentences from article (eval only)\"\"\"", "\n", "enc_sent", ",", "enc_art", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "saliency", "=", "torch", ".", "matmul", "(", "enc_sent", ",", "enc_art", ".", "unsqueeze", "(", "2", ")", ")", "\n", "content", "=", "self", ".", "_sent_linear", "(", "enc_sent", ")", "\n", "logit", "=", "(", "content", "+", "saliency", ")", ".", "squeeze", "(", "2", ")", "\n", "if", "sent_nums", "is", "None", ":", "# test-time extract only", "\n", "            ", "assert", "len", "(", "article_sents", ")", "==", "1", "\n", "n_sent", "=", "logit", ".", "size", "(", "1", ")", "\n", "extracted", "=", "logit", "[", "0", "]", ".", "topk", "(", "\n", "k", "if", "k", "<", "n_sent", "else", "n_sent", ",", "sorted", "=", "False", "# original order", "\n", ")", "[", "1", "]", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "extracted", "=", "[", "l", "[", ":", "n", "]", ".", "topk", "(", "k", "if", "k", "<", "n", "else", "n", ")", "[", "1", "]", ".", "tolist", "(", ")", "\n", "for", "n", ",", "l", "in", "zip", "(", "sent_nums", ",", "logit", ")", "]", "\n", "", "return", "extracted", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ExtractSumm._encode": [[226, 247], ["extract.ExtractSumm._art_enc", "torch.nn.functional.tanh", "extract.ExtractSumm._sent_enc().unsqueeze", "max", "torch.stack", "extract.ExtractSumm._art_linear", "extract.ExtractSumm._sent_enc", "torch.zeros().to", "util.sequence_mean", "extract.ExtractSumm._sent_enc", "torch.zeros", "torch.cat", "zip", "extract.ExtractSumm._encode.zero"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.sequence_mean"], ["", "def", "_encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "if", "sent_nums", "is", "None", ":", "# test-time extract only", "\n", "            ", "enc_sent", "=", "self", ".", "_sent_enc", "(", "article_sents", "[", "0", "]", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "max_n", "=", "max", "(", "sent_nums", ")", "\n", "enc_sents", "=", "[", "self", ".", "_sent_enc", "(", "art_sent", ")", "\n", "for", "art_sent", "in", "article_sents", "]", "\n", "def", "zero", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "_art_enc", ".", "input_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "", "enc_sent", "=", "torch", ".", "stack", "(", "\n", "[", "torch", ".", "cat", "(", "[", "s", ",", "zero", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "\n", "dim", "=", "0", ")", "if", "n", "!=", "max_n", "\n", "else", "s", "\n", "for", "s", ",", "n", "in", "zip", "(", "enc_sents", ",", "sent_nums", ")", "]", ",", "\n", "dim", "=", "0", "\n", ")", "\n", "", "lstm_out", "=", "self", ".", "_art_enc", "(", "enc_sent", ",", "sent_nums", ")", "\n", "enc_art", "=", "F", ".", "tanh", "(", "\n", "self", ".", "_art_linear", "(", "sequence_mean", "(", "lstm_out", ",", "sent_nums", ",", "dim", "=", "1", ")", ")", ")", "\n", "return", "lstm_out", ",", "enc_art", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.ExtractSumm.set_embedding": [[248, 250], ["extract.ExtractSumm._sent_enc.set_embedding"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding"], ["", "def", "set_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "self", ".", "_sent_enc", ".", "set_embedding", "(", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNet.__init__": [[254, 289], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.LSTM", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.uniform_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.uniform_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init_h", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_layer", ",", "n_hidden", ")", ")", "\n", "self", ".", "_init_c", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_layer", ",", "n_hidden", ")", ")", "\n", "self", ".", "_init_i", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ")", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_h", ",", "-", "INI", ",", "INI", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_c", ",", "-", "INI", ",", "INI", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_i", ",", "-", "0.1", ",", "0.1", ")", "\n", "self", ".", "_lstm", "=", "nn", ".", "LSTM", "(", "\n", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "\n", "bidirectional", "=", "False", ",", "dropout", "=", "dropout", "\n", ")", "\n", "self", ".", "_lstm_cell", "=", "None", "\n", "\n", "# attention parameters", "\n", "self", ".", "_attn_wm", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ",", "n_hidden", ")", ")", "\n", "self", ".", "_attn_wq", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ",", "n_hidden", ")", ")", "\n", "self", ".", "_attn_v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ")", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wm", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wq", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_attn_v", ",", "-", "INI", ",", "INI", ")", "\n", "\n", "# hop parameters", "\n", "self", ".", "_hop_wm", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ",", "n_hidden", ")", ")", "\n", "self", ".", "_hop_wq", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ",", "n_hidden", ")", ")", "\n", "self", ".", "_hop_v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ")", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_hop_wm", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_hop_wq", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_hop_v", ",", "-", "INI", ",", "INI", ")", "\n", "self", ".", "_n_hop", "=", "n_hop", "\n", "self", ".", "_auto_stop", "=", "auto_stop", "\n", "if", "auto_stop", ":", "\n", "            ", "self", ".", "_stop", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ")", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_stop", ",", "-", "INI", ",", "INI", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNet.forward": [[290, 313], ["extract.LSTMPointerNet._prepare", "extract.LSTMPointerNet._lstm", "extract.LSTMPointerNet.attention", "range", "extract.LSTMPointerNet.attention_score", "torch.cat.size", "torch.cat", "torch.LongTensor().to", "extract.LSTMPointerNet._stop.unsqueeze().expand", "torch.cat().transpose", "init_i.transpose", "extract.LSTMPointerNet.attention", "torch.zeros().to", "torch.LongTensor", "extract.LSTMPointerNet._stop.unsqueeze", "torch.cat", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet._prepare", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention"], ["", "", "def", "forward", "(", "self", ",", "attn_mem", ",", "mem_sizes", ",", "lstm_in", ")", ":", "\n", "        ", "\"\"\"atten_mem: Tensor of size [batch_size, max_sent_num, input_dim]\"\"\"", "\n", "if", "self", ".", "_auto_stop", ":", "\n", "            ", "batch_size", ",", "max_sent_num", ",", "input_dim", "=", "attn_mem", ".", "size", "(", ")", "\n", "# insert stop representation", "\n", "attn_mem", "=", "torch", ".", "cat", "(", "[", "attn_mem", ",", "torch", ".", "zeros", "(", "batch_size", ",", "1", ",", "input_dim", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "# [batch, max_sent_num+1, input_dim]", "\n", "mem_sizes_tensor", "=", "torch", ".", "LongTensor", "(", "mem_sizes", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "\n", "attn_mem", "[", ":", ",", "mem_sizes_tensor", ",", ":", "]", "=", "self", ".", "_stop", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", "# [batch_size, input_dim]", "\n", "mem_sizes", "=", "[", "s", "+", "1", "for", "s", "in", "mem_sizes", "]", "\n", "\n", "", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "init_i", "=", "self", ".", "_prepare", "(", "attn_mem", ")", "\n", "if", "lstm_in", "is", "not", "None", ":", "\n", "            ", "lstm_in", "=", "torch", ".", "cat", "(", "[", "init_i", ",", "lstm_in", "]", ",", "dim", "=", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "lstm_in", "=", "init_i", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "query", ",", "final_states", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "lstm_states", ")", "\n", "query", "=", "query", ".", "transpose", "(", "0", ",", "1", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "_n_hop", ")", ":", "\n", "            ", "query", "=", "LSTMPointerNet", ".", "attention", "(", "\n", "hop_feat", ",", "query", ",", "self", ".", "_hop_v", ",", "self", ".", "_hop_wq", ",", "mem_sizes", ")", "\n", "", "output", "=", "LSTMPointerNet", ".", "attention_score", "(", "\n", "attn_feat", ",", "query", ",", "self", ".", "_attn_v", ",", "self", ".", "_attn_wq", ")", "\n", "return", "output", "# unormalized extraction logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNet.extract": [[314, 353], ["extract.LSTMPointerNet._prepare", "lstm_in.squeeze.squeeze.squeeze", "torch.cat.size", "torch.cat", "rnn.MultiLayerLSTMCells.convert().to", "extract.LSTMPointerNet._lstm_cell", "range", "extract.LSTMPointerNet.attention_score", "score.squeeze.squeeze.squeeze", "[].item", "extracts.append", "extract.LSTMPointerNet.attention", "extract.LSTMPointerNet._stop.view", "rnn.MultiLayerLSTMCells.convert", "score.squeeze.squeeze.max"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet._prepare", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert"], ["", "def", "extract", "(", "self", ",", "attn_mem", ",", "mem_sizes", ",", "k", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "\"\"\"extract k sentences, decode only, batch_size==1\"\"\"", "\n", "# atten_mem: Tensor of size [1, max_sent_num, input_dim]", "\n", "if", "self", ".", "_auto_stop", ":", "\n", "            ", "end_step", "=", "attn_mem", ".", "size", "(", "1", ")", "\n", "attn_mem", "=", "torch", ".", "cat", "(", "[", "attn_mem", ",", "self", ".", "_stop", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "]", ",", "dim", "=", "1", ")", "# [1, max_sent_num+1, input_dim]", "\n", "\n", "", "use_selected_mask", "=", "not", "disable_selected_mask", "\n", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "lstm_in", "=", "self", ".", "_prepare", "(", "attn_mem", ")", "\n", "lstm_in", "=", "lstm_in", ".", "squeeze", "(", "1", ")", "\n", "if", "self", ".", "_lstm_cell", "is", "None", ":", "\n", "            ", "self", ".", "_lstm_cell", "=", "MultiLayerLSTMCells", ".", "convert", "(", "\n", "self", ".", "_lstm", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "\n", "", "extracts", "=", "[", "]", "\n", "num_extracted_sent", "=", "0", "\n", "while", "True", ":", "\n", "            ", "h", ",", "c", "=", "self", ".", "_lstm_cell", "(", "lstm_in", ",", "lstm_states", ")", "\n", "query", "=", "h", "[", "-", "1", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_n_hop", ")", ":", "\n", "                ", "query", "=", "LSTMPointerNet", ".", "attention", "(", "\n", "hop_feat", ",", "query", ",", "self", ".", "_hop_v", ",", "self", ".", "_hop_wq", ",", "mem_sizes", ")", "\n", "", "score", "=", "LSTMPointerNet", ".", "attention_score", "(", "\n", "attn_feat", ",", "query", ",", "self", ".", "_attn_v", ",", "self", ".", "_attn_wq", ")", "# [1, 1, Ns]", "\n", "score", "=", "score", ".", "squeeze", "(", ")", "# [Ns]", "\n", "# set logit to -inf if the sentence is selected before", "\n", "if", "use_selected_mask", ":", "\n", "                ", "for", "e", "in", "extracts", ":", "\n", "                    ", "score", "[", "e", "]", "=", "-", "1e6", "\n", "", "", "ext", "=", "score", ".", "max", "(", "dim", "=", "0", ")", "[", "1", "]", ".", "item", "(", ")", "\n", "if", "self", ".", "_auto_stop", ":", "# break the loop if eos is selected, does not include eos to the extracts", "\n", "                ", "if", "ext", "==", "end_step", ":", "\n", "                    ", "break", "\n", "", "", "extracts", ".", "append", "(", "ext", ")", "\n", "num_extracted_sent", "+=", "1", "\n", "if", "(", "not", "self", ".", "_auto_stop", "and", "num_extracted_sent", "==", "k", ")", "or", "num_extracted_sent", "==", "MAX_EXT", ":", "\n", "                ", "break", "\n", "", "lstm_states", "=", "(", "h", ",", "c", ")", "\n", "lstm_in", "=", "attn_mem", "[", ":", ",", "ext", ",", ":", "]", "\n", "", "return", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNet._prepare": [[354, 365], ["torch.matmul", "torch.matmul", "attn_mem.size", "extract.LSTMPointerNet._init_h.size", "extract.LSTMPointerNet._init_i.size", "extract.LSTMPointerNet._init_i.unsqueeze().unsqueeze().expand", "extract.LSTMPointerNet._attn_wm.unsqueeze", "extract.LSTMPointerNet._hop_wm.unsqueeze", "extract.LSTMPointerNet._init_h.unsqueeze().expand().contiguous", "extract.LSTMPointerNet._init_c.unsqueeze().expand().contiguous", "extract.LSTMPointerNet._init_i.unsqueeze().unsqueeze", "extract.LSTMPointerNet._init_h.unsqueeze().expand", "extract.LSTMPointerNet._init_c.unsqueeze().expand", "extract.LSTMPointerNet._init_i.unsqueeze", "extract.LSTMPointerNet._init_h.unsqueeze", "extract.LSTMPointerNet._init_c.unsqueeze"], "methods", ["None"], ["", "def", "_prepare", "(", "self", ",", "attn_mem", ")", ":", "\n", "        ", "attn_feat", "=", "torch", ".", "matmul", "(", "attn_mem", ",", "self", ".", "_attn_wm", ".", "unsqueeze", "(", "0", ")", ")", "\n", "hop_feat", "=", "torch", ".", "matmul", "(", "attn_mem", ",", "self", ".", "_hop_wm", ".", "unsqueeze", "(", "0", ")", ")", "\n", "bs", "=", "attn_mem", ".", "size", "(", "0", ")", "\n", "n_l", ",", "d", "=", "self", ".", "_init_h", ".", "size", "(", ")", "\n", "size", "=", "(", "n_l", ",", "bs", ",", "d", ")", "\n", "lstm_states", "=", "(", "self", ".", "_init_h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ".", "contiguous", "(", ")", ",", "\n", "self", ".", "_init_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ".", "contiguous", "(", ")", ")", "\n", "d", "=", "self", ".", "_init_i", ".", "size", "(", "0", ")", "\n", "init_i", "=", "self", ".", "_init_i", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "1", ",", "d", ")", "\n", "return", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "init_i", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNet.attention_score": [[366, 376], ["torch.matmul().squeeze", "attention.unsqueeze", "torch.matmul().unsqueeze", "torch.matmul", "torch.matmul", "torch.nn.functional.tanh", "v.unsqueeze().unsqueeze().unsqueeze", "w.unsqueeze", "v.unsqueeze().unsqueeze", "v.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "attention_score", "(", "attention", ",", "query", ",", "v", ",", "w", ")", ":", "\n", "        ", "\"\"\" unnormalized attention score\"\"\"", "\n", "sum_", "=", "attention", ".", "unsqueeze", "(", "1", ")", "+", "torch", ".", "matmul", "(", "\n", "query", ",", "w", ".", "unsqueeze", "(", "0", ")", "\n", ")", ".", "unsqueeze", "(", "2", ")", "# [B, Nq, Ns, D]", "\n", "score", "=", "torch", ".", "matmul", "(", "\n", "F", ".", "tanh", "(", "sum_", ")", ",", "v", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "3", ")", "\n", ")", ".", "squeeze", "(", "3", ")", "# [B, Nq, Ns]", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNet.attention": [[377, 388], ["extract.LSTMPointerNet.attention_score", "torch.matmul", "torch.nn.functional.softmax", "util.len_mask().unsqueeze", "attention.prob_normalize", "util.len_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.prob_normalize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "@", "staticmethod", "\n", "def", "attention", "(", "attention", ",", "query", ",", "v", ",", "w", ",", "mem_sizes", ")", ":", "\n", "        ", "\"\"\" attention context vector\"\"\"", "\n", "score", "=", "LSTMPointerNet", ".", "attention_score", "(", "attention", ",", "query", ",", "v", ",", "w", ")", "\n", "if", "mem_sizes", "is", "None", ":", "\n", "            ", "norm_score", "=", "F", ".", "softmax", "(", "score", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "mask", "=", "len_mask", "(", "mem_sizes", ",", "score", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "norm_score", "=", "prob_normalize", "(", "score", ",", "mask", ")", "\n", "", "output", "=", "torch", ".", "matmul", "(", "norm_score", ",", "attention", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNetWithCandidate.__init__": [[392, 394], ["extract.LSTMPointerNet.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "dropout", ",", "n_hop", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "dropout", ",", "n_hop", ",", "auto_stop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.LSTMPointerNetWithCandidate.extract": [[395, 441], ["extract.LSTMPointerNetWithCandidate._prepare", "lstm_in.squeeze.squeeze.squeeze", "torch.cat.size", "torch.cat", "rnn.MultiLayerLSTMCells.convert().to", "extract.LSTMPointerNetWithCandidate._lstm_cell", "range", "extract.LSTMPointerNet.attention_score", "score.squeeze.squeeze.squeeze", "[].item", "extracts.append", "extract.LSTMPointerNet.attention", "extract.LSTMPointerNetWithCandidate._stop.view", "rnn.MultiLayerLSTMCells.convert", "score.squeeze.squeeze.max", "range"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet._prepare", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert"], ["", "def", "extract", "(", "self", ",", "attn_mem", ",", "mem_sizes", ",", "k", ",", "num_candidates", "=", "1", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "\"\"\"extract k sentences, decode only, batch_size==1\"\"\"", "\n", "# atten_mem: Tensor of size [1, max_sent_num, input_dim]", "\n", "if", "self", ".", "_auto_stop", ":", "\n", "            ", "end_step", "=", "attn_mem", ".", "size", "(", "1", ")", "\n", "attn_mem", "=", "torch", ".", "cat", "(", "[", "attn_mem", ",", "self", ".", "_stop", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "]", ",", "dim", "=", "1", ")", "# [1, max_sent_num+1, input_dim]", "\n", "\n", "", "use_selected_mask", "=", "not", "disable_selected_mask", "\n", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "lstm_in", "=", "self", ".", "_prepare", "(", "attn_mem", ")", "\n", "lstm_in", "=", "lstm_in", ".", "squeeze", "(", "1", ")", "\n", "if", "self", ".", "_lstm_cell", "is", "None", ":", "\n", "            ", "self", ".", "_lstm_cell", "=", "MultiLayerLSTMCells", ".", "convert", "(", "\n", "self", ".", "_lstm", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "\n", "", "extracts", "=", "[", "]", "\n", "num_extracted_sent", "=", "0", "\n", "masked_candidates", "=", "[", "]", "\n", "while", "True", ":", "\n", "            ", "h", ",", "c", "=", "self", ".", "_lstm_cell", "(", "lstm_in", ",", "lstm_states", ")", "\n", "query", "=", "h", "[", "-", "1", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_n_hop", ")", ":", "\n", "                ", "query", "=", "LSTMPointerNet", ".", "attention", "(", "\n", "hop_feat", ",", "query", ",", "self", ".", "_hop_v", ",", "self", ".", "_hop_wq", ",", "mem_sizes", ")", "\n", "", "score", "=", "LSTMPointerNet", ".", "attention_score", "(", "\n", "attn_feat", ",", "query", ",", "self", ".", "_attn_v", ",", "self", ".", "_attn_wq", ")", "# [1, 1, Ns]", "\n", "score", "=", "score", ".", "squeeze", "(", ")", "# [Ns]", "\n", "\n", "# set logit to -inf if the sentence is selected before", "\n", "for", "e", "in", "masked_candidates", ":", "\n", "                ", "score", "[", "e", "]", "=", "-", "1e6", "\n", "\n", "", "ext", "=", "score", ".", "max", "(", "dim", "=", "0", ")", "[", "1", "]", ".", "item", "(", ")", "\n", "if", "self", ".", "_auto_stop", ":", "# break the loop if eos is selected, does not include eos to the extracts", "\n", "                ", "if", "ext", "==", "end_step", ":", "\n", "                    ", "break", "\n", "", "", "extracts", ".", "append", "(", "ext", ")", "\n", "num_extracted_sent", "+=", "1", "\n", "if", "(", "not", "self", ".", "_auto_stop", "and", "num_extracted_sent", "==", "k", ")", "or", "num_extracted_sent", "==", "MAX_EXT", ":", "\n", "                ", "break", "\n", "# map the extracted candidate idx to all the candidate indices in that sentence and mask them out", "\n", "", "selected_sent_idx", "=", "ext", "//", "num_candidates", "\n", "if", "use_selected_mask", ":", "\n", "                ", "masked_candidates", "+=", "[", "selected_sent_idx", "*", "num_candidates", "+", "i", "for", "i", "in", "range", "(", "num_candidates", ")", "]", "\n", "\n", "", "lstm_states", "=", "(", "h", ",", "c", ")", "\n", "lstm_in", "=", "attn_mem", "[", ":", ",", "ext", ",", ":", "]", "\n", "", "return", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractSumm.__init__": [[445, 459], ["torch.nn.Module.__init__", "extract.ConvSentEncoder", "extract.LSTMEncoder", "extract.LSTMPointerNet"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "vocab_size", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "\n", "n_hop", "=", "1", ",", "dropout", "=", "0.0", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_sent_enc", "=", "ConvSentEncoder", "(", "\n", "vocab_size", ",", "emb_dim", ",", "conv_hidden", ",", "dropout", ")", "\n", "self", ".", "_art_enc", "=", "LSTMEncoder", "(", "\n", "3", "*", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", "\n", ")", "\n", "enc_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "self", ".", "_extractor", "=", "LSTMPointerNet", "(", "\n", "enc_out_dim", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractSumm.forward": [[461, 473], ["extract.PtrExtractSumm._encode", "extract.PtrExtractSumm._extractor", "target.size", "extract.PtrExtractSumm.size", "torch.gather", "target.unsqueeze().expand", "target.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "forward", "(", "self", ",", "article_sents", ",", "sent_nums", ",", "target", ")", ":", "\n", "        ", "enc_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "bs", ",", "nt", "=", "target", ".", "size", "(", ")", "\n", "d", "=", "enc_out", ".", "size", "(", "2", ")", "\n", "ptr_in", "=", "torch", ".", "gather", "(", "\n", "enc_out", ",", "dim", "=", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "bs", ",", "nt", ",", "d", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "ptr_in", "=", "None", "\n", "", "output", "=", "self", ".", "_extractor", "(", "enc_out", ",", "sent_nums", ",", "ptr_in", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractSumm.extract": [[474, 478], ["extract.PtrExtractSumm._encode", "extract.PtrExtractSumm._extractor.extract"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract"], ["", "def", "extract", "(", "self", ",", "article_sents", ",", "sent_nums", "=", "None", ",", "k", "=", "4", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "enc_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "output", "=", "self", ".", "_extractor", ".", "extract", "(", "enc_out", ",", "sent_nums", ",", "k", ",", "disable_selected_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractSumm._encode": [[479, 505], ["extract.PtrExtractSumm._art_enc", "extract.PtrExtractSumm._sent_enc().unsqueeze", "max", "torch.stack", "extract.PtrExtractSumm._sent_enc", "torch.zeros().to", "extract.PtrExtractSumm._sent_enc", "torch.zeros", "torch.cat", "zip", "extract.PtrExtractSumm._encode.zero"], "methods", ["None"], ["", "def", "_encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "\"\"\"\n        :param article_sents:\n        :param sent_nums: a list of sent_nums\n        :return:\n        \"\"\"", "\n", "if", "sent_nums", "is", "None", ":", "# test-time excode only", "\n", "            ", "enc_sent", "=", "self", ".", "_sent_enc", "(", "article_sents", "[", "0", "]", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "max_n", "=", "max", "(", "sent_nums", ")", "\n", "enc_sents", "=", "[", "self", ".", "_sent_enc", "(", "art_sent", ")", "\n", "for", "art_sent", "in", "article_sents", "]", "# each item has dimension [num_sents, 3*conv_size]", "\n", "def", "zero", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "_art_enc", ".", "input_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "# pad the article that with sent_num less than max_n", "\n", "", "enc_sent", "=", "torch", ".", "stack", "(", "\n", "[", "torch", ".", "cat", "(", "[", "s", ",", "zero", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "\n", "if", "n", "!=", "max_n", "\n", "else", "s", "\n", "for", "s", ",", "n", "in", "zip", "(", "enc_sents", ",", "sent_nums", ")", "]", ",", "\n", "dim", "=", "0", "\n", ")", "\n", "", "lstm_out", "=", "self", ".", "_art_enc", "(", "enc_sent", ",", "sent_nums", ")", "\n", "return", "lstm_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractSumm.set_embedding": [[506, 508], ["extract.PtrExtractSumm._sent_enc.set_embedding"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding"], ["", "def", "set_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "self", ".", "_sent_enc", ".", "set_embedding", "(", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.__init__": [[512, 536], ["torch.nn.Module.__init__", "extract.ConvSentEncoder", "extract.LSTMEncoder", "extract.LSTMPointerNetWithCandidate", "extract.FCAggregator", "torch.mean"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "vocab_size", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "\n", "num_candidates", ",", "n_hop", "=", "1", ",", "dropout", "=", "0.0", ",", "candidate_agg_type", "=", "'mean'", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_candidate_sent_enc", "=", "ConvSentEncoder", "(", "\n", "vocab_size", ",", "emb_dim", ",", "conv_hidden", ",", "dropout", ")", "\n", "#self._candidate_agg  = ConvCandidateAggregator(3*conv_hidden, 3*conv_hidden, dropout)", "\n", "if", "candidate_agg_type", "==", "'mean'", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "lambda", "x", ":", "torch", ".", "mean", "(", "x", ",", "dim", "=", "1", ")", "# mean pooling over the candidate representations", "\n", "", "else", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "FCAggregator", "(", "num_candidates", ")", "\n", "\n", "", "self", ".", "_art_enc", "=", "LSTMEncoder", "(", "\n", "3", "*", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", "\n", ")", "\n", "enc_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "+", "3", "*", "conv_hidden", "\n", "self", ".", "_extractor", "=", "LSTMPointerNetWithCandidate", "(", "\n", "enc_out_dim", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "\n", ")", "\n", "lstm_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "\n", "self", ".", "num_candidates", "=", "num_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.forward": [[537, 593], ["extract.PtrExtractRewrittenSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenSumm._extractor", "target.size", "enc_out.view.view.size", "torch.gather", "context_aware_encode_out.unsqueeze().expand", "target.unsqueeze().expand", "context_aware_encode_out.unsqueeze", "target.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "forward", "(", "self", ",", "article_sents", ",", "total_cand_nums", ",", "target", ")", ":", "\n", "        ", "sent_nums", "=", "[", "cand_num", "//", "self", ".", "num_candidates", "for", "cand_num", "in", "total_cand_nums", "]", "\n", "\n", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "# debug", "\n", "\"\"\"\n        print(\"global encode_out size: {}\".format(context_aware_encode_out.size()))\n        print(\"local encode_out size: {}\".format(local_encode_out.size()))\n        print(\"local_encode[0, 0, 0, 0]: {}\".format(local_encode_out[0, 0, 0, 0:5]))\n        print(\"local_encode[0, 0, 1, 0]: {}\".format(local_encode_out[0, 0, 1, 0:5]))\n        print(\"local_encode[0, 1, 0, 0]: {}\".format(local_encode_out[0, 1, 0, 0:5]))\n        print(\"local_encode[0, 1, 1, 0]: {}\".format(local_encode_out[0, 1, 1, 0:5]))\n        print(\"local_encode[1, 0, 0, 0]: {}\".format(local_encode_out[1, 0, 0, 0:5]))\n        print(\"local_encode[1, 0, 1, 0]: {}\".format(local_encode_out[1, 0, 1, 0:5]))\n        print(\"global_encode[0, 0, 0]: {}\".format(context_aware_encode_out[0, 0, 0:5]))\n        print(\"global_encode[0, 1, 0]: {}\".format(context_aware_encode_out[0, 1, 0:5]))\n        print(\"global_encode[1, 0, 0]: {}\".format(context_aware_encode_out[1, 0, 0:5]))\n        \"\"\"", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "# debug", "\n", "\"\"\"\n        print(\"enc_out[0, 0, 0]: {}\".format(enc_out[0, 0, 0:5]))\n        print(\"enc_out[0, 0, 300]: {}\".format(enc_out[0, 0, 300:305]))\n        print(\"enc_out[0, 1, 0]: {}\".format(enc_out[0, 1, 0:5]))\n        print(\"enc_out[0, 1, 300]: {}\".format(enc_out[0, 1, 300:305]))\n        print(\"enc_out[0, 2, 0]: {}\".format(enc_out[0, 2, 0:5]))\n        print(\"enc_out[0, 2, 300]: {}\".format(enc_out[0, 2, 300:305]))\n        print(\"enc_out[0, 3, 0]: {}\".format(enc_out[0, 3, 0:5]))\n        print(\"enc_out[0, 3, 300]: {}\".format(enc_out[0, 3, 300:305]))\n        print(\"enc_out[1, 0, 0]: {}\".format(enc_out[1, 0, 0:5]))\n        print(\"enc_out[1, 0, 300]: {}\".format(enc_out[1, 0, 300:305]))\n        print(\"enc_out[1, 1, 0]: {}\".format(enc_out[1, 1, 0:5]))\n        print(\"enc_out[1, 1, 300]: {}\".format(enc_out[1, 1, 300:305]))\n        print()\n        print(\"enc out size: {}\".format(enc_out.size()))\n        \"\"\"", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "batch_size", ",", "nt", "=", "target", ".", "size", "(", ")", "\n", "d", "=", "enc_out", ".", "size", "(", "2", ")", "\n", "ptr_in", "=", "torch", ".", "gather", "(", "\n", "enc_out", ",", "dim", "=", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "nt", ",", "d", ")", "\n", ")", "# [batch, nt, d]", "\n", "", "else", ":", "\n", "            ", "ptr_in", "=", "None", "\n", "\n", "#total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "", "output", "=", "self", ".", "_extractor", "(", "enc_out", ",", "total_cand_nums", ",", "ptr_in", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.extract": [[594, 613], ["extract.PtrExtractRewrittenSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenSumm._extractor.extract", "context_aware_encode_out.unsqueeze().expand", "context_aware_encode_out.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract"], ["", "def", "extract", "(", "self", ",", "article_sents", ",", "total_cand_nums", "=", "None", ",", "k", "=", "4", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "if", "total_cand_nums", "is", "not", "None", ":", "\n", "            ", "sent_nums", "=", "[", "n", "//", "self", ".", "num_candidates", "for", "n", "in", "total_cand_nums", "]", "\n", "", "else", ":", "\n", "            ", "sent_nums", "=", "None", "\n", "", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "\n", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "\n", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "#total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "output", "=", "self", ".", "_extractor", ".", "extract", "(", "enc_out", ",", "total_cand_nums", ",", "k", ",", "self", ".", "num_candidates", ",", "disable_selected_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm._encode": [[614, 667], ["len", "extract.PtrExtractRewrittenSumm._art_enc", "extract.PtrExtractRewrittenSumm._candidate_sent_enc().view", "extract.PtrExtractRewrittenSumm._candidate_agg", "extract.PtrExtractRewrittenSumm.unsqueeze", "extract.PtrExtractRewrittenSumm.unsqueeze", "max", "torch.cat", "extract.PtrExtractRewrittenSumm._candidate_agg", "torch.stack().contiguous", "torch.stack", "extract.PtrExtractRewrittenSumm._candidate_sent_enc().view", "torch.zeros().to", "torch.stack().contiguous.append", "torch.zeros().to", "extract.PtrExtractRewrittenSumm._candidate_sent_enc", "article_sents[].size", "torch.cat", "torch.stack", "extract.PtrExtractRewrittenSumm._candidate_sent_enc", "art_sent.size", "torch.zeros", "torch.zeros", "torch.cat", "zip", "extract.PtrExtractRewrittenSumm._encode.zero_enc_sent"], "methods", ["None"], ["", "def", "_encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "\"\"\"\n        :param article_sents: a list of tensor, each tensor has dim [num_sent * num_candidates, seq_len]\n        :param sent_nums: a list of sent_nums\n        :return: lstm_out: [batch, max_n, 2 * lstm_hidden], enc_candidates_out: [batch, max_n, num_cands, 3*conv_size]\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "article_sents", ")", "\n", "if", "sent_nums", "is", "None", ":", "# test-time excode only", "\n", "            ", "enc_candidates", "=", "self", ".", "_candidate_sent_enc", "(", "article_sents", "[", "0", "]", ")", ".", "view", "(", "article_sents", "[", "0", "]", ".", "size", "(", "0", ")", "//", "self", ".", "num_candidates", ",", "self", ".", "num_candidates", ",", "-", "1", ")", "\n", "# [num_sents, num_cands, 3*conv_size]", "\n", "enc_sents", "=", "self", ".", "_candidate_agg", "(", "enc_candidates", ")", "# [num_sents, 3*conv_size]", "\n", "enc_sents_padded", "=", "enc_sents", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, 3*conv_size]", "\n", "enc_candidates_out", "=", "enc_candidates", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, num_cands, 3*conv_size]", "\n", "\n", "", "else", ":", "\n", "            ", "max_n", "=", "max", "(", "sent_nums", ")", "\n", "enc_candidates", "=", "[", "self", ".", "_candidate_sent_enc", "(", "art_sent", ")", ".", "view", "(", "art_sent", ".", "size", "(", "0", ")", "//", "self", ".", "num_candidates", ",", "self", ".", "num_candidates", ",", "-", "1", ")", "\n", "for", "art_sent", "in", "article_sents", "]", "# each item has dimension [num_sents, num_cands, 3*conv_size]", "\n", "enc_candidates_flattened", "=", "torch", ".", "cat", "(", "enc_candidates", ",", "dim", "=", "0", ")", "# [total_num_sents_in_batch, num_cands, 3*conv_size]", "\n", "enc_sents_flattened", "=", "self", ".", "_candidate_agg", "(", "enc_candidates_flattened", ")", "# [total_num_sents_in_batch, 3*conv_size]", "\n", "\n", "\n", "# pad an article if its sent_num less than max_n", "\n", "def", "zero_enc_sent", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "_art_enc", ".", "input_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "", "start_idx", "=", "0", "\n", "enc_sents_padded", "=", "[", "]", "\n", "for", "n", "in", "sent_nums", ":", "\n", "                ", "end_idx", "=", "start_idx", "+", "n", "\n", "s", "=", "enc_sents_flattened", "[", "start_idx", ":", "end_idx", ",", ":", "]", "\n", "padded_s", "=", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_sent", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "if", "n", "!=", "max_n", "else", "s", "# [max_n, 3*conv_size]", "\n", "enc_sents_padded", ".", "append", "(", "padded_s", ")", "\n", "start_idx", "=", "end_idx", "\n", "\n", "# pad enc_sentences to max_n", "\n", "", "enc_sents_padded", "=", "torch", ".", "stack", "(", "enc_sents_padded", ",", "dim", "=", "0", ")", ".", "contiguous", "(", ")", "# [batch, max_n, 3*conv_size]", "\n", "\n", "# pad enc_candidates to max_n", "\n", "def", "zero_enc_candidate", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "num_candidates", ",", "self", ".", "_candidate_sent_enc", ".", "output_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "# a list of [num_sents, num_cands, 3*conv_size] -> [batch, max_n, num_cands, 3*conv_size]", "\n", "", "enc_candidates_out", "=", "torch", ".", "stack", "(", "[", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_candidate", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "\n", "if", "n", "!=", "max_n", "else", "s", "for", "s", ",", "n", "in", "zip", "(", "enc_candidates", ",", "sent_nums", ")", "]", ",", "\n", "dim", "=", "0", ")", "# [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "# compute context-aware embedding for each article sentence", "\n", "", "lstm_out", "=", "self", ".", "_art_enc", "(", "enc_sents_padded", ",", "sent_nums", ")", "# [batch, max_n, 2 * lstm_hidden]", "\n", "\n", "return", "lstm_out", ",", "enc_candidates_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding": [[668, 670], ["extract.PtrExtractRewrittenSumm._candidate_sent_enc.set_embedding"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSumm.set_embedding"], ["", "def", "set_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "self", ".", "_candidate_sent_enc", ".", "set_embedding", "(", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm.__init__": [[674, 702], ["torch.nn.Module.__init__", "sentence_transformer_wrapper.SentenceTransformerWrapper", "extract.PtrExtractRewrittenSentBertSumm._sentence_encoder.eval", "extract.PtrExtractRewrittenSentBertSumm._sentence_encoder.parameters", "torch.nn.Linear", "extract.LSTMEncoder", "extract.LSTMPointerNetWithCandidate", "extract.FCAggregator", "torch.mean"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "vocab_size", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "\n", "num_candidates", ",", "cache_dir", ",", "n_hop", "=", "1", ",", "dropout", "=", "0.0", ",", "candidate_agg_type", "=", "'mean'", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_bert_layers", "=", "3", "\n", "self", ".", "_sentence_encoder", "=", "SentenceTransformerWrapper", "(", "model_name_or_path", "=", "'bert-base-nli-mean-tokens'", ",", "num_bert_layers", "=", "self", ".", "num_bert_layers", ")", "\n", "self", ".", "_sentence_encoder", ".", "eval", "(", ")", "\n", "for", "p", "in", "self", ".", "_sentence_encoder", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "", "self", ".", "_bert_w", "=", "nn", ".", "Linear", "(", "self", ".", "num_bert_layers", "*", "768", ",", "3", "*", "conv_hidden", ")", "\n", "#self._candidate_agg  = ConvCandidateAggregator(3*conv_hidden, 3*conv_hidden, dropout)", "\n", "if", "candidate_agg_type", "==", "'mean'", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "lambda", "x", ":", "torch", ".", "mean", "(", "x", ",", "dim", "=", "1", ")", "# mean pooling over the candidate representations", "\n", "", "else", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "FCAggregator", "(", "num_candidates", ")", "\n", "\n", "", "self", ".", "_art_enc", "=", "LSTMEncoder", "(", "\n", "3", "*", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", "\n", ")", "\n", "enc_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "+", "3", "*", "conv_hidden", "\n", "self", ".", "_extractor", "=", "LSTMPointerNetWithCandidate", "(", "\n", "enc_out_dim", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "\n", ")", "\n", "lstm_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "\n", "self", ".", "num_candidates", "=", "num_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm.forward": [[703, 759], ["extract.PtrExtractRewrittenSentBertSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenSentBertSumm._extractor", "target.size", "enc_out.view.view.size", "torch.gather", "context_aware_encode_out.unsqueeze().expand", "target.unsqueeze().expand", "context_aware_encode_out.unsqueeze", "target.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "forward", "(", "self", ",", "article_sents", ",", "total_cand_nums", ",", "target", ")", ":", "\n", "        ", "sent_nums", "=", "[", "cand_num", "//", "self", ".", "num_candidates", "for", "cand_num", "in", "total_cand_nums", "]", "\n", "\n", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "# debug", "\n", "\"\"\"\n        print(\"global encode_out size: {}\".format(context_aware_encode_out.size()))\n        print(\"local encode_out size: {}\".format(local_encode_out.size()))\n        print(\"local_encode[0, 0, 0, 0]: {}\".format(local_encode_out[0, 0, 0, 0:5]))\n        print(\"local_encode[0, 0, 1, 0]: {}\".format(local_encode_out[0, 0, 1, 0:5]))\n        print(\"local_encode[0, 1, 0, 0]: {}\".format(local_encode_out[0, 1, 0, 0:5]))\n        print(\"local_encode[0, 1, 1, 0]: {}\".format(local_encode_out[0, 1, 1, 0:5]))\n        print(\"local_encode[1, 0, 0, 0]: {}\".format(local_encode_out[1, 0, 0, 0:5]))\n        print(\"local_encode[1, 0, 1, 0]: {}\".format(local_encode_out[1, 0, 1, 0:5]))\n        print(\"global_encode[0, 0, 0]: {}\".format(context_aware_encode_out[0, 0, 0:5]))\n        print(\"global_encode[0, 1, 0]: {}\".format(context_aware_encode_out[0, 1, 0:5]))\n        print(\"global_encode[1, 0, 0]: {}\".format(context_aware_encode_out[1, 0, 0:5]))\n        \"\"\"", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "# debug", "\n", "\"\"\"\n        print(\"enc_out[0, 0, 0]: {}\".format(enc_out[0, 0, 0:5]))\n        print(\"enc_out[0, 0, 300]: {}\".format(enc_out[0, 0, 300:305]))\n        print(\"enc_out[0, 1, 0]: {}\".format(enc_out[0, 1, 0:5]))\n        print(\"enc_out[0, 1, 300]: {}\".format(enc_out[0, 1, 300:305]))\n        print(\"enc_out[0, 2, 0]: {}\".format(enc_out[0, 2, 0:5]))\n        print(\"enc_out[0, 2, 300]: {}\".format(enc_out[0, 2, 300:305]))\n        print(\"enc_out[0, 3, 0]: {}\".format(enc_out[0, 3, 0:5]))\n        print(\"enc_out[0, 3, 300]: {}\".format(enc_out[0, 3, 300:305]))\n        print(\"enc_out[1, 0, 0]: {}\".format(enc_out[1, 0, 0:5]))\n        print(\"enc_out[1, 0, 300]: {}\".format(enc_out[1, 0, 300:305]))\n        print(\"enc_out[1, 1, 0]: {}\".format(enc_out[1, 1, 0:5]))\n        print(\"enc_out[1, 1, 300]: {}\".format(enc_out[1, 1, 300:305]))\n        print()\n        print(\"enc out size: {}\".format(enc_out.size()))\n        \"\"\"", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "batch_size", ",", "nt", "=", "target", ".", "size", "(", ")", "\n", "d", "=", "enc_out", ".", "size", "(", "2", ")", "\n", "ptr_in", "=", "torch", ".", "gather", "(", "\n", "enc_out", ",", "dim", "=", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "nt", ",", "d", ")", "\n", ")", "# [batch, nt, d]", "\n", "", "else", ":", "\n", "            ", "ptr_in", "=", "None", "\n", "\n", "#total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "", "output", "=", "self", ".", "_extractor", "(", "enc_out", ",", "total_cand_nums", ",", "ptr_in", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm.extract": [[760, 779], ["extract.PtrExtractRewrittenSentBertSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenSentBertSumm._extractor.extract", "context_aware_encode_out.unsqueeze().expand", "context_aware_encode_out.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract"], ["", "def", "extract", "(", "self", ",", "article_sents", ",", "total_cand_nums", "=", "None", ",", "k", "=", "4", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "if", "total_cand_nums", "is", "not", "None", ":", "\n", "            ", "sent_nums", "=", "[", "n", "//", "self", ".", "num_candidates", "for", "n", "in", "total_cand_nums", "]", "\n", "", "else", ":", "\n", "            ", "sent_nums", "=", "None", "\n", "", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "\n", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "\n", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "#total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "output", "=", "self", ".", "_extractor", ".", "extract", "(", "enc_out", ",", "total_cand_nums", ",", "k", ",", "self", ".", "num_candidates", ",", "disable_selected_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm._encode": [[780, 833], ["len", "extract.PtrExtractRewrittenSentBertSumm._art_enc", "extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode().view", "extract.PtrExtractRewrittenSentBertSumm._candidate_agg", "extract.PtrExtractRewrittenSentBertSumm.unsqueeze", "extract.PtrExtractRewrittenSentBertSumm.unsqueeze", "max", "torch.cat", "extract.PtrExtractRewrittenSentBertSumm._candidate_agg", "torch.stack().contiguous", "torch.stack", "extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode().view", "torch.zeros().to", "torch.stack().contiguous.append", "torch.zeros().to", "extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode", "article_sents[].size", "torch.cat", "torch.stack", "extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode", "art_sent.size", "torch.zeros", "torch.zeros", "torch.cat", "zip", "extract.PtrExtractRewrittenSentBertSumm._encode.zero_enc_sent"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode"], ["", "def", "_encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "\"\"\"\n        :param article_sents: a list of tensor, each tensor has dim [num_sent * num_candidates, seq_len]\n        :param sent_nums: a list of sent_nums\n        :return: lstm_out: [batch, max_n, 2 * lstm_hidden], enc_candidates_out: [batch, max_n, num_cands, 3*conv_size]\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "article_sents", ")", "\n", "if", "sent_nums", "is", "None", ":", "# test-time excode only", "\n", "            ", "enc_candidates", "=", "self", ".", "_candidate_sent_encode", "(", "article_sents", "[", "0", "]", ")", ".", "view", "(", "article_sents", "[", "0", "]", ".", "size", "(", "0", ")", "//", "self", ".", "num_candidates", ",", "self", ".", "num_candidates", ",", "-", "1", ")", "\n", "# [num_sents, num_cands, 3*conv_size]", "\n", "enc_sents", "=", "self", ".", "_candidate_agg", "(", "enc_candidates", ")", "# [num_sents, 3*conv_size]", "\n", "enc_sents_padded", "=", "enc_sents", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, 3*conv_size]", "\n", "enc_candidates_out", "=", "enc_candidates", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, num_cands, 3*conv_size]", "\n", "\n", "", "else", ":", "\n", "            ", "max_n", "=", "max", "(", "sent_nums", ")", "\n", "enc_candidates", "=", "[", "self", ".", "_candidate_sent_encode", "(", "art_sent", ")", ".", "view", "(", "art_sent", ".", "size", "(", "0", ")", "//", "self", ".", "num_candidates", ",", "self", ".", "num_candidates", ",", "-", "1", ")", "\n", "for", "art_sent", "in", "article_sents", "]", "# each item has dimension [num_sents, num_cands, 3*conv_size]", "\n", "enc_candidates_flattened", "=", "torch", ".", "cat", "(", "enc_candidates", ",", "dim", "=", "0", ")", "# [total_num_sents_in_batch, num_cands, 3*conv_size]", "\n", "enc_sents_flattened", "=", "self", ".", "_candidate_agg", "(", "enc_candidates_flattened", ")", "# [total_num_sents_in_batch, 3*conv_size]", "\n", "\n", "\n", "# pad an article if its sent_num less than max_n", "\n", "def", "zero_enc_sent", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "_art_enc", ".", "input_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "", "start_idx", "=", "0", "\n", "enc_sents_padded", "=", "[", "]", "\n", "for", "n", "in", "sent_nums", ":", "\n", "                ", "end_idx", "=", "start_idx", "+", "n", "\n", "s", "=", "enc_sents_flattened", "[", "start_idx", ":", "end_idx", ",", ":", "]", "\n", "padded_s", "=", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_sent", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "if", "n", "!=", "max_n", "else", "s", "# [max_n, 3*conv_size]", "\n", "enc_sents_padded", ".", "append", "(", "padded_s", ")", "\n", "start_idx", "=", "end_idx", "\n", "\n", "# pad enc_sentences to max_n", "\n", "", "enc_sents_padded", "=", "torch", ".", "stack", "(", "enc_sents_padded", ",", "dim", "=", "0", ")", ".", "contiguous", "(", ")", "# [batch, max_n, 3*conv_size]", "\n", "\n", "# pad enc_candidates to max_n", "\n", "def", "zero_enc_candidate", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "num_candidates", ",", "enc_candidates", "[", "0", "]", ".", "size", "(", "2", ")", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "# a list of [num_sents, num_cands, 3*conv_size] -> [batch, max_n, num_cands, 3*conv_size]", "\n", "", "enc_candidates_out", "=", "torch", ".", "stack", "(", "[", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_candidate", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "\n", "if", "n", "!=", "max_n", "else", "s", "for", "s", ",", "n", "in", "zip", "(", "enc_candidates", ",", "sent_nums", ")", "]", ",", "\n", "dim", "=", "0", ")", "# [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "# compute context-aware embedding for each article sentence", "\n", "", "lstm_out", "=", "self", ".", "_art_enc", "(", "enc_sents_padded", ",", "sent_nums", ")", "# [batch, max_n, 2 * lstm_hidden]", "\n", "\n", "return", "lstm_out", ",", "enc_candidates_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentBertSumm._candidate_sent_encode": [[834, 849], ["torch.ne().to", "extract.PtrExtractRewrittenSentBertSumm._sentence_encoder.encode_tensor", "extract.PtrExtractRewrittenSentBertSumm._bert_w", "torch.ne"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.encode_tensor"], ["", "def", "_candidate_sent_encode", "(", "self", ",", "sents_tensor", ")", ":", "\n", "# sents_tensor: [num_sent * num_candidates, seq_len]", "\n", "#num_sents_all, sent_len = sents_tensor.size()", "\n", "\n", "        ", "attention_mask_tensor", "=", "torch", ".", "ne", "(", "sents_tensor", ",", "0", ")", ".", "to", "(", "sents_tensor", ".", "device", ")", "\n", "sent_bert_embeddings", "=", "self", ".", "_sentence_encoder", ".", "encode_tensor", "(", "sents_tensor", ",", "attention_mask_tensor", ")", "\n", "# [num_sents * num_cands, 768 * num_bert_layers]", "\n", "\n", "sent_embeddings", "=", "self", ".", "_bert_w", "(", "sent_bert_embeddings", ")", "# [num_sent * num_candidates, 3 * conv_size]", "\n", "#enc_candidate = sent_embeddings.view(num_sents_all // self.num_candidates, self.num_candidates, -1)", "\n", "# [num_doc_sents, num_cands, 3 * conv_size]", "\n", "#print(\"sent_embeedings size\")", "\n", "#print(sent_embeddings.size())", "\n", "#exit()", "\n", "return", "sent_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenBertSumm.__init__": [[854, 887], ["torch.nn.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertModel.from_pretrained", "extract.PtrExtractRewrittenBertSumm.bert_model.eval", "extract.PtrExtractRewrittenBertSumm.bert_model.parameters", "torch.nn.Linear", "extract.ConvSentEncoderNoEmbedding", "extract.LSTMEncoder", "extract.LSTMPointerNetWithCandidate", "extract.FCAggregator", "torch.mean"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "vocab_size", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "\n", "num_candidates", ",", "cache_dir", ",", "n_hop", "=", "1", ",", "dropout", "=", "0.0", ",", "candidate_agg_type", "=", "'mean'", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "#model_name_or_path = \"bert-large-uncased\"", "\n", "model_name_or_path", "=", "\"bert-base-uncased\"", "\n", "config", "=", "BertConfig", ".", "from_pretrained", "(", "model_name_or_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "config", ".", "output_hidden_states", "=", "True", "\n", "self", ".", "bert_model", "=", "BertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "config", "=", "config", ",", "cache_dir", "=", "cache_dir", ")", "\n", "self", ".", "bert_model", ".", "eval", "(", ")", "\n", "for", "p", "in", "self", ".", "bert_model", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "#self._bert_w = nn.Linear(1024 * 4, emb_dim)", "\n", "", "self", ".", "_bert_w", "=", "nn", ".", "Linear", "(", "768", "*", "4", ",", "emb_dim", ")", "\n", "self", ".", "_candidate_sent_enc", "=", "ConvSentEncoderNoEmbedding", "(", "emb_dim", ",", "conv_hidden", ",", "dropout", ")", "\n", "# self._candidate_agg  = ConvCandidateAggregator(3*conv_hidden, 3*conv_hidden, dropout)", "\n", "if", "candidate_agg_type", "==", "'mean'", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "lambda", "x", ":", "torch", ".", "mean", "(", "x", ",", "dim", "=", "1", ")", "# mean pooling over the candidate representations", "\n", "", "else", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "FCAggregator", "(", "num_candidates", ")", "\n", "\n", "", "self", ".", "_art_enc", "=", "LSTMEncoder", "(", "\n", "3", "*", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", "\n", ")", "\n", "enc_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "+", "3", "*", "conv_hidden", "\n", "self", ".", "_extractor", "=", "LSTMPointerNetWithCandidate", "(", "\n", "enc_out_dim", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "\n", ")", "\n", "lstm_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "\n", "self", ".", "num_candidates", "=", "num_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenBertSumm.forward": [[888, 946], ["extract.PtrExtractRewrittenBertSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenBertSumm._extractor", "target.size", "enc_out.view.view.size", "torch.gather", "context_aware_encode_out.unsqueeze().expand", "target.unsqueeze().expand", "context_aware_encode_out.unsqueeze", "target.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "forward", "(", "self", ",", "article_sents", ",", "total_cand_nums", ",", "target", ")", ":", "\n", "        ", "sent_nums", "=", "[", "cand_num", "//", "self", ".", "num_candidates", "for", "cand_num", "in", "total_cand_nums", "]", "\n", "\n", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "# debug", "\n", "\"\"\"\n        print(\"global encode_out size: {}\".format(context_aware_encode_out.size()))\n        print(\"local encode_out size: {}\".format(local_encode_out.size()))\n        print(\"local_encode[0, 0, 0, 0]: {}\".format(local_encode_out[0, 0, 0, 0:5]))\n        print(\"local_encode[0, 0, 1, 0]: {}\".format(local_encode_out[0, 0, 1, 0:5]))\n        print(\"local_encode[0, 1, 0, 0]: {}\".format(local_encode_out[0, 1, 0, 0:5]))\n        print(\"local_encode[0, 1, 1, 0]: {}\".format(local_encode_out[0, 1, 1, 0:5]))\n        print(\"local_encode[1, 0, 0, 0]: {}\".format(local_encode_out[1, 0, 0, 0:5]))\n        print(\"local_encode[1, 0, 1, 0]: {}\".format(local_encode_out[1, 0, 1, 0:5]))\n        print(\"global_encode[0, 0, 0]: {}\".format(context_aware_encode_out[0, 0, 0:5]))\n        print(\"global_encode[0, 1, 0]: {}\".format(context_aware_encode_out[0, 1, 0:5]))\n        print(\"global_encode[1, 0, 0]: {}\".format(context_aware_encode_out[1, 0, 0:5]))\n        \"\"\"", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "\n", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "\n", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "\n", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "# debug", "\n", "\"\"\"\n        print(\"enc_out[0, 0, 0]: {}\".format(enc_out[0, 0, 0:5]))\n        print(\"enc_out[0, 0, 300]: {}\".format(enc_out[0, 0, 300:305]))\n        print(\"enc_out[0, 1, 0]: {}\".format(enc_out[0, 1, 0:5]))\n        print(\"enc_out[0, 1, 300]: {}\".format(enc_out[0, 1, 300:305]))\n        print(\"enc_out[0, 2, 0]: {}\".format(enc_out[0, 2, 0:5]))\n        print(\"enc_out[0, 2, 300]: {}\".format(enc_out[0, 2, 300:305]))\n        print(\"enc_out[0, 3, 0]: {}\".format(enc_out[0, 3, 0:5]))\n        print(\"enc_out[0, 3, 300]: {}\".format(enc_out[0, 3, 300:305]))\n        print(\"enc_out[1, 0, 0]: {}\".format(enc_out[1, 0, 0:5]))\n        print(\"enc_out[1, 0, 300]: {}\".format(enc_out[1, 0, 300:305]))\n        print(\"enc_out[1, 1, 0]: {}\".format(enc_out[1, 1, 0:5]))\n        print(\"enc_out[1, 1, 300]: {}\".format(enc_out[1, 1, 300:305]))\n        print()\n        print(\"enc out size: {}\".format(enc_out.size()))\n        \"\"\"", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "batch_size", ",", "nt", "=", "target", ".", "size", "(", ")", "\n", "d", "=", "enc_out", ".", "size", "(", "2", ")", "\n", "ptr_in", "=", "torch", ".", "gather", "(", "\n", "enc_out", ",", "dim", "=", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "nt", ",", "d", ")", "\n", ")", "# [batch, nt, d]", "\n", "", "else", ":", "\n", "            ", "ptr_in", "=", "None", "\n", "\n", "# total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "", "output", "=", "self", ".", "_extractor", "(", "enc_out", ",", "total_cand_nums", ",", "ptr_in", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenBertSumm.extract": [[947, 966], ["extract.PtrExtractRewrittenBertSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenBertSumm._extractor.extract", "context_aware_encode_out.unsqueeze().expand", "context_aware_encode_out.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract"], ["", "def", "extract", "(", "self", ",", "article_sents", ",", "total_cand_nums", "=", "None", ",", "k", "=", "4", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "if", "total_cand_nums", "is", "not", "None", ":", "\n", "            ", "sent_nums", "=", "[", "n", "//", "self", ".", "num_candidates", "for", "n", "in", "total_cand_nums", "]", "\n", "", "else", ":", "\n", "            ", "sent_nums", "=", "None", "\n", "", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "\n", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "\n", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "#total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "output", "=", "self", ".", "_extractor", ".", "extract", "(", "enc_out", ",", "total_cand_nums", ",", "k", ",", "self", ".", "num_candidates", ",", "disable_selected_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenBertSumm._encode": [[967, 1029], ["len", "extract.PtrExtractRewrittenBertSumm._art_enc", "extract.PtrExtractRewrittenBertSumm._article_encode", "extract.PtrExtractRewrittenBertSumm._candidate_agg", "extract.PtrExtractRewrittenBertSumm.unsqueeze", "extract.PtrExtractRewrittenBertSumm.unsqueeze", "max", "torch.cat", "extract.PtrExtractRewrittenBertSumm._candidate_agg", "torch.stack().contiguous", "torch.stack", "extract.PtrExtractRewrittenBertSumm._article_encode", "torch.zeros().to", "torch.stack().contiguous.append", "torch.zeros().to", "torch.cat", "torch.stack", "torch.zeros", "torch.zeros", "torch.cat", "zip", "extract.PtrExtractRewrittenBertSumm._encode.zero_enc_sent"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._article_encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._article_encode"], ["", "def", "_encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "\"\"\"\n        :param article_sents: a list of tensor, each tensor has dim [num_sent * num_candidates, seq_len]\n        :param sent_nums: a list of sent_nums\n        :return: lstm_out: [batch, max_n, 2 * lstm_hidden], enc_candidates_out: [batch, max_n, num_cands, 3*conv_size]\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "article_sents", ")", "\n", "device", "=", "article_sents", "[", "0", "]", ".", "device", "\n", "if", "sent_nums", "is", "None", ":", "# test-time excode only", "\n", "            ", "enc_candidates", "=", "self", ".", "_article_encode", "(", "article", "=", "article_sents", "[", "0", "]", ",", "device", "=", "device", ")", "# [num_sents, num_cands, 3*conv_size]", "\n", "enc_sents", "=", "self", ".", "_candidate_agg", "(", "enc_candidates", ")", "# [num_sents, 3*conv_size]", "\n", "enc_sents_padded", "=", "enc_sents", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, 3*conv_size]", "\n", "enc_candidates_out", "=", "enc_candidates", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, num_cands, 3*conv_size]", "\n", "", "else", ":", "\n", "            ", "max_n", "=", "max", "(", "sent_nums", ")", "\n", "enc_candidates", "=", "[", "self", ".", "_article_encode", "(", "article", "=", "article", ",", "device", "=", "device", ")", "for", "article", "in", "article_sents", "]", "\n", "# enc_candidates: each item has size of [num_sents, num_cands, 3*conv_size]", "\n", "enc_candidates_flattened", "=", "torch", ".", "cat", "(", "enc_candidates", ",", "dim", "=", "0", ")", "# [total_num_sents_in_batch, num_cands, 3*conv_size]", "\n", "#print()", "\n", "#print(\"enc_candidates_flattened.size\")", "\n", "#print(enc_candidates_flattened.size())", "\n", "enc_sents_flattened", "=", "self", ".", "_candidate_agg", "(", "enc_candidates_flattened", ")", "# [total_num_sents_in_batch, 3*conv_size]", "\n", "#print(\"enc_sents_flattened.size()\")", "\n", "#print(enc_sents_flattened.size())", "\n", "\n", "# pad an article if its sent_num less than max_n", "\n", "def", "zero_enc_sent", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "_art_enc", ".", "input_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "", "start_idx", "=", "0", "\n", "enc_sents_padded", "=", "[", "]", "\n", "for", "n", "in", "sent_nums", ":", "\n", "                ", "end_idx", "=", "start_idx", "+", "n", "\n", "s", "=", "enc_sents_flattened", "[", "start_idx", ":", "end_idx", ",", ":", "]", "\n", "padded_s", "=", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_sent", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "if", "n", "!=", "max_n", "else", "s", "# [max_n, 3*conv_size]", "\n", "enc_sents_padded", ".", "append", "(", "padded_s", ")", "\n", "start_idx", "=", "end_idx", "\n", "\n", "# pad enc_sentences to max_n", "\n", "", "enc_sents_padded", "=", "torch", ".", "stack", "(", "enc_sents_padded", ",", "dim", "=", "0", ")", ".", "contiguous", "(", ")", "# [batch, max_n, 3*conv_size]", "\n", "#print(\"enc_sents_padded_size\")", "\n", "#print(enc_sents_padded.size())", "\n", "\n", "# pad enc_candidates to max_n", "\n", "def", "zero_enc_candidate", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "num_candidates", ",", "self", ".", "_candidate_sent_enc", ".", "output_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "# a list of [num_sents, num_cands, 3*conv_size] -> [batch, max_n, num_cands, 3*conv_size]", "\n", "", "enc_candidates_out", "=", "torch", ".", "stack", "(", "[", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_candidate", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "\n", "if", "n", "!=", "max_n", "else", "s", "for", "s", ",", "n", "in", "zip", "(", "enc_candidates", ",", "sent_nums", ")", "]", ",", "\n", "dim", "=", "0", ")", "# [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "# compute context-aware embedding for each article sentence", "\n", "", "lstm_out", "=", "self", ".", "_art_enc", "(", "enc_sents_padded", ",", "sent_nums", ")", "# [batch, max_n, 2 * lstm_hidden]", "\n", "#print(\"lstm_out.size()\")", "\n", "#print(lstm_out.size())", "\n", "#print(\"enc_candidates_out_size\")", "\n", "#print(enc_candidates_out.size())", "\n", "\n", "return", "lstm_out", ",", "enc_candidates_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenBertSumm._article_encode": [[1030, 1112], ["article.size", "range", "range", "max", "range", "torch.LongTensor().to", "torch.LongTensor().to", "extract.PtrExtractRewrittenBertSumm.bert_model", "torch.cat", "extract.PtrExtractRewrittenBertSumm._bert_w", "extract.PtrExtractRewrittenBertSumm.size", "torch.zeros().to", "range", "range", "extract.PtrExtractRewrittenBertSumm._candidate_sent_enc", "range", "article_token_ids[].append", "article_input_mask.append", "torch.cat.size", "range", "enc_candidate.view.view.view", "range", "len", "article_token_ids[].append", "article_input_mask[].append", "len", "len", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "print", "print", "print", "print", "print", "print", "print", "print", "exit", "article_token_ids[].append", "torch.zeros().to.size", "extract.PtrExtractRewrittenBertSumm.size", "enc_candidate.view.view.size"], "methods", ["None"], ["", "def", "_article_encode", "(", "self", ",", "article", ",", "device", ",", "pad_idx", "=", "0", ")", ":", "\n", "# process one article", "\n", "# create input for BERT", "\n", "        ", "num_sents_all", ",", "sent_len", "=", "article", ".", "size", "(", ")", "\n", "#print(\"article_size\")", "\n", "#print(article.size())", "\n", "article_token_ids", "=", "[", "[", "CLS_ID", "]", "for", "_", "in", "range", "(", "self", ".", "num_candidates", ")", "]", "# 2d list [num_cands, art_len]", "\n", "article_input_mask", "=", "[", "]", "\n", "article_cand_lens", "=", "[", "1", "]", "*", "self", ".", "num_candidates", "# include cls and sep", "\n", "for", "i", "in", "range", "(", "num_sents_all", ")", ":", "\n", "            ", "cand_i", "=", "i", "%", "self", ".", "num_candidates", "\n", "for", "j", "in", "range", "(", "sent_len", ")", ":", "\n", "                ", "if", "article", "[", "i", "]", "[", "j", "]", "!=", "pad_idx", ":", "\n", "                    ", "article_token_ids", "[", "cand_i", "]", ".", "append", "(", "article", "[", "i", "]", "[", "j", "]", ")", "\n", "article_cand_lens", "[", "cand_i", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "", "for", "cand_i", "in", "range", "(", "self", ".", "num_candidates", ")", ":", "\n", "            ", "article_token_ids", "[", "cand_i", "]", ".", "append", "(", "SEP_ID", ")", "\n", "article_cand_lens", "[", "cand_i", "]", "+=", "1", "\n", "article_input_mask", ".", "append", "(", "[", "1", "]", "*", "article_cand_lens", "[", "cand_i", "]", ")", "\n", "\n", "# compute max seq_len among all cands", "\n", "", "max_article_len", "=", "max", "(", "article_cand_lens", ")", "\n", "\n", "# padding", "\n", "for", "cand_i", "in", "range", "(", "self", ".", "num_candidates", ")", ":", "\n", "            ", "while", "len", "(", "article_token_ids", "[", "cand_i", "]", ")", "<", "max_article_len", ":", "\n", "                ", "article_token_ids", "[", "cand_i", "]", ".", "append", "(", "0", ")", "\n", "article_input_mask", "[", "cand_i", "]", ".", "append", "(", "0", ")", "\n", "", "assert", "len", "(", "article_token_ids", "[", "cand_i", "]", ")", "==", "max_article_len", "\n", "assert", "len", "(", "article_input_mask", "[", "cand_i", "]", ")", "==", "max_article_len", "\n", "\n", "", "bert_attention_mask", "=", "torch", ".", "LongTensor", "(", "article_input_mask", ")", ".", "to", "(", "device", ")", "\n", "\n", "bert_input_ids", "=", "torch", ".", "LongTensor", "(", "article_token_ids", ")", ".", "to", "(", "device", ")", "\n", "\n", "last_hidden_state", ",", "_", ",", "hidden_states", "=", "self", ".", "bert_model", "(", "input_ids", "=", "bert_input_ids", ",", "\n", "attention_mask", "=", "bert_attention_mask", ",", "token_type_ids", "=", "None", ")", "\n", "\n", "bert_emb_out", "=", "torch", ".", "cat", "(", "[", "hidden_states", "[", "-", "1", "]", ",", "hidden_states", "[", "-", "2", "]", ",", "hidden_states", "[", "-", "3", "]", ",", "hidden_states", "[", "-", "4", "]", "]", ",", "dim", "=", "-", "1", ")", "\n", "assert", "bert_emb_out", ".", "size", "(", ")", "==", "(", "self", ".", "num_candidates", ",", "max_article_len", ",", "3072", ")", "\n", "#assert bert_emb_out.size() == (self.num_candidates, max_article_len, 4096)", "\n", "\n", "emb_out", "=", "self", ".", "_bert_w", "(", "bert_emb_out", ")", "# [num_cands, max_article_len, 128]", "\n", "#print(\"emb_out\")", "\n", "#print(emb_out.size())", "\n", "\n", "emb_dim", "=", "emb_out", ".", "size", "(", "-", "1", ")", "# 128", "\n", "\n", "emb_input", "=", "torch", ".", "zeros", "(", "num_sents_all", ",", "sent_len", ",", "emb_dim", ")", ".", "to", "(", "device", ")", "\n", "cur_ids", "=", "[", "1", "]", "*", "self", ".", "num_candidates", "# after [CLS]", "\n", "for", "i", "in", "range", "(", "num_sents_all", ")", ":", "\n", "            ", "cand_i", "=", "i", "%", "self", ".", "num_candidates", "\n", "for", "j", "in", "range", "(", "sent_len", ")", ":", "\n", "                ", "if", "article", "[", "i", "]", "[", "j", "]", "!=", "pad_idx", ":", "\n", "                    ", "emb_input", "[", "i", "]", "[", "j", "]", "=", "emb_out", "[", "cand_i", "]", "[", "cur_ids", "[", "cand_i", "]", "]", "\n", "cur_ids", "[", "cand_i", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "for", "cand_i", "in", "range", "(", "self", ".", "num_candidates", ")", ":", "\n", "            ", "assert", "cur_ids", "[", "cand_i", "]", "==", "article_cand_lens", "[", "cand_i", "]", "-", "1", "\n", "#print()", "\n", "\n", "", "enc_candidate", "=", "self", ".", "_candidate_sent_enc", "(", "emb_input", ")", "\n", "try", ":", "\n", "            ", "enc_candidate", "=", "enc_candidate", ".", "view", "(", "num_sents_all", "//", "self", ".", "num_candidates", ",", "self", ".", "num_candidates", ",", "-", "1", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "\"emb_input size\"", ")", "\n", "print", "(", "emb_input", ".", "size", "(", ")", ")", "\n", "print", "(", "\"num_sents_all\"", ")", "\n", "print", "(", "num_sents_all", ")", "\n", "print", "(", "\"emb_out\"", ")", "\n", "print", "(", "emb_out", ".", "size", "(", ")", ")", "\n", "print", "(", "\"enc_candidate.size()\"", ")", "\n", "print", "(", "enc_candidate", ".", "size", "(", ")", ")", "\n", "exit", "(", ")", "\n", "# [num_doc_sents, num_cands, 3 * conv_size]", "\n", "#print(\"enc_candidate_size:\")", "\n", "#print(enc_candidate.size())", "\n", "", "return", "enc_candidate", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm.__init__": [[1117, 1147], ["torch.nn.Module.__init__", "sentence_transformer_wrapper.SentenceTransformerWrapper", "extract.PtrExtractRewrittenSentWordBertSumm._sentence_encoder.eval", "extract.PtrExtractRewrittenSentWordBertSumm._sentence_encoder.parameters", "torch.nn.Linear", "extract.ConvSentEncoderNoEmbedding", "extract.LSTMEncoder", "extract.LSTMPointerNetWithCandidate", "extract.FCAggregator", "torch.mean"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "vocab_size", ",", "conv_hidden", ",", "\n", "lstm_hidden", ",", "lstm_layer", ",", "bidirectional", ",", "\n", "num_candidates", ",", "cache_dir", ",", "n_hop", "=", "1", ",", "dropout", "=", "0.0", ",", "candidate_agg_type", "=", "'mean'", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# self.num_bert_layers = num_bert_layers", "\n", "self", ".", "num_bert_layers", "=", "1", "\n", "self", ".", "_sentence_encoder", "=", "SentenceTransformerWrapper", "(", "model_name_or_path", "=", "'bert-base-nli-mean-tokens'", ",", "num_bert_layers", "=", "self", ".", "num_bert_layers", ")", "\n", "self", ".", "_sentence_encoder", ".", "eval", "(", ")", "\n", "for", "p", "in", "self", ".", "_sentence_encoder", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "", "self", ".", "_bert_w", "=", "nn", ".", "Linear", "(", "self", ".", "num_bert_layers", "*", "768", ",", "emb_dim", ")", "\n", "self", ".", "_candidate_sent_enc", "=", "ConvSentEncoderNoEmbedding", "(", "emb_dim", ",", "conv_hidden", ",", "dropout", ")", "\n", "# self._candidate_agg  = ConvCandidateAggregator(3*conv_hidden, 3*conv_hidden, dropout)", "\n", "if", "candidate_agg_type", "==", "'mean'", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "lambda", "x", ":", "torch", ".", "mean", "(", "x", ",", "dim", "=", "1", ")", "# mean pooling over the candidate representations", "\n", "", "else", ":", "\n", "            ", "self", ".", "_candidate_agg", "=", "FCAggregator", "(", "num_candidates", ")", "\n", "\n", "", "self", ".", "_art_enc", "=", "LSTMEncoder", "(", "\n", "3", "*", "conv_hidden", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", "\n", ")", "\n", "enc_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "+", "3", "*", "conv_hidden", "\n", "self", ".", "_extractor", "=", "LSTMPointerNetWithCandidate", "(", "\n", "enc_out_dim", ",", "lstm_hidden", ",", "lstm_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "\n", ")", "\n", "lstm_out_dim", "=", "lstm_hidden", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "\n", "self", ".", "num_candidates", "=", "num_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm.forward": [[1148, 1206], ["extract.PtrExtractRewrittenSentWordBertSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenSentWordBertSumm._extractor", "target.size", "enc_out.view.view.size", "torch.gather", "context_aware_encode_out.unsqueeze().expand", "target.unsqueeze().expand", "context_aware_encode_out.unsqueeze", "target.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode"], ["", "def", "forward", "(", "self", ",", "article_sents", ",", "total_cand_nums", ",", "target", ")", ":", "\n", "        ", "sent_nums", "=", "[", "cand_num", "//", "self", ".", "num_candidates", "for", "cand_num", "in", "total_cand_nums", "]", "\n", "\n", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "# debug", "\n", "\"\"\"\n        print(\"global encode_out size: {}\".format(context_aware_encode_out.size()))\n        print(\"local encode_out size: {}\".format(local_encode_out.size()))\n        print(\"local_encode[0, 0, 0, 0]: {}\".format(local_encode_out[0, 0, 0, 0:5]))\n        print(\"local_encode[0, 0, 1, 0]: {}\".format(local_encode_out[0, 0, 1, 0:5]))\n        print(\"local_encode[0, 1, 0, 0]: {}\".format(local_encode_out[0, 1, 0, 0:5]))\n        print(\"local_encode[0, 1, 1, 0]: {}\".format(local_encode_out[0, 1, 1, 0:5]))\n        print(\"local_encode[1, 0, 0, 0]: {}\".format(local_encode_out[1, 0, 0, 0:5]))\n        print(\"local_encode[1, 0, 1, 0]: {}\".format(local_encode_out[1, 0, 1, 0:5]))\n        print(\"global_encode[0, 0, 0]: {}\".format(context_aware_encode_out[0, 0, 0:5]))\n        print(\"global_encode[0, 1, 0]: {}\".format(context_aware_encode_out[0, 1, 0:5]))\n        print(\"global_encode[1, 0, 0]: {}\".format(context_aware_encode_out[1, 0, 0:5]))\n        \"\"\"", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "\n", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "\n", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "\n", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "# debug", "\n", "\"\"\"\n        print(\"enc_out[0, 0, 0]: {}\".format(enc_out[0, 0, 0:5]))\n        print(\"enc_out[0, 0, 300]: {}\".format(enc_out[0, 0, 300:305]))\n        print(\"enc_out[0, 1, 0]: {}\".format(enc_out[0, 1, 0:5]))\n        print(\"enc_out[0, 1, 300]: {}\".format(enc_out[0, 1, 300:305]))\n        print(\"enc_out[0, 2, 0]: {}\".format(enc_out[0, 2, 0:5]))\n        print(\"enc_out[0, 2, 300]: {}\".format(enc_out[0, 2, 300:305]))\n        print(\"enc_out[0, 3, 0]: {}\".format(enc_out[0, 3, 0:5]))\n        print(\"enc_out[0, 3, 300]: {}\".format(enc_out[0, 3, 300:305]))\n        print(\"enc_out[1, 0, 0]: {}\".format(enc_out[1, 0, 0:5]))\n        print(\"enc_out[1, 0, 300]: {}\".format(enc_out[1, 0, 300:305]))\n        print(\"enc_out[1, 1, 0]: {}\".format(enc_out[1, 1, 0:5]))\n        print(\"enc_out[1, 1, 300]: {}\".format(enc_out[1, 1, 300:305]))\n        print()\n        print(\"enc out size: {}\".format(enc_out.size()))\n        \"\"\"", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "batch_size", ",", "nt", "=", "target", ".", "size", "(", ")", "\n", "d", "=", "enc_out", ".", "size", "(", "2", ")", "\n", "ptr_in", "=", "torch", ".", "gather", "(", "\n", "enc_out", ",", "dim", "=", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "nt", ",", "d", ")", "\n", ")", "# [batch, nt, d]", "\n", "", "else", ":", "\n", "            ", "ptr_in", "=", "None", "\n", "\n", "# total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "", "output", "=", "self", ".", "_extractor", "(", "enc_out", ",", "total_cand_nums", ",", "ptr_in", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm.extract": [[1207, 1226], ["extract.PtrExtractRewrittenSentWordBertSumm._encode", "context_aware_encode_out.size", "torch.cat", "enc_out.view.view.view", "extract.PtrExtractRewrittenSentWordBertSumm._extractor.extract", "context_aware_encode_out.unsqueeze().expand", "context_aware_encode_out.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract"], ["", "def", "extract", "(", "self", ",", "article_sents", ",", "total_cand_nums", "=", "None", ",", "k", "=", "4", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "if", "total_cand_nums", "is", "not", "None", ":", "\n", "            ", "sent_nums", "=", "[", "n", "//", "self", ".", "num_candidates", "for", "n", "in", "total_cand_nums", "]", "\n", "", "else", ":", "\n", "            ", "sent_nums", "=", "None", "\n", "", "context_aware_encode_out", ",", "local_encode_out", "=", "self", ".", "_encode", "(", "article_sents", ",", "sent_nums", ")", "\n", "# [batch, max_n, 2 * lstm_hidden], [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "batch_size", ",", "max_n", ",", "lstm_out_dim", "=", "context_aware_encode_out", ".", "size", "(", ")", "\n", "# concat local representation with context-aware representation", "\n", "# new dim=[batch, max_n, num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "enc_out", "=", "torch", ".", "cat", "(", "[", "local_encode_out", ",", "\n", "context_aware_encode_out", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "max_n", ",", "self", ".", "num_candidates", ",", "\n", "lstm_out_dim", ")", "]", ",", "dim", "=", "3", ")", "\n", "enc_out", "=", "enc_out", ".", "view", "(", "batch_size", ",", "max_n", "*", "self", ".", "num_candidates", ",", "-", "1", ")", "# [batch, max_n * num_cand, 2*lstm_hidden + 3*conv_size]", "\n", "\n", "#total_cand_nums = [n * self.num_candidates for n in sent_nums]", "\n", "output", "=", "self", ".", "_extractor", ".", "extract", "(", "enc_out", ",", "total_cand_nums", ",", "k", ",", "self", ".", "num_candidates", ",", "disable_selected_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._encode": [[1227, 1289], ["len", "extract.PtrExtractRewrittenSentWordBertSumm._art_enc", "extract.PtrExtractRewrittenSentWordBertSumm._article_encode", "extract.PtrExtractRewrittenSentWordBertSumm._candidate_agg", "extract.PtrExtractRewrittenSentWordBertSumm.unsqueeze", "extract.PtrExtractRewrittenSentWordBertSumm.unsqueeze", "max", "torch.cat", "extract.PtrExtractRewrittenSentWordBertSumm._candidate_agg", "torch.stack().contiguous", "torch.stack", "extract.PtrExtractRewrittenSentWordBertSumm._article_encode", "torch.zeros().to", "torch.stack().contiguous.append", "torch.zeros().to", "torch.cat", "torch.stack", "torch.zeros", "torch.zeros", "torch.cat", "zip", "extract.PtrExtractRewrittenSentWordBertSumm._encode.zero_enc_sent"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._article_encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._article_encode"], ["", "def", "_encode", "(", "self", ",", "article_sents", ",", "sent_nums", ")", ":", "\n", "        ", "\"\"\"\n        :param article_sents: a list of tensor, each tensor has dim [num_sent * num_candidates, seq_len]\n        :param sent_nums: a list of sent_nums\n        :return: lstm_out: [batch, max_n, 2 * lstm_hidden], enc_candidates_out: [batch, max_n, num_cands, 3*conv_size]\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "article_sents", ")", "\n", "device", "=", "article_sents", "[", "0", "]", ".", "device", "\n", "if", "sent_nums", "is", "None", ":", "# test-time excode only", "\n", "            ", "enc_candidates", "=", "self", ".", "_article_encode", "(", "article", "=", "article_sents", "[", "0", "]", ",", "device", "=", "device", ")", "# [num_sents, num_cands, 3*conv_size]", "\n", "enc_sents", "=", "self", ".", "_candidate_agg", "(", "enc_candidates", ")", "# [num_sents, 3*conv_size]", "\n", "enc_sents_padded", "=", "enc_sents", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, 3*conv_size]", "\n", "enc_candidates_out", "=", "enc_candidates", ".", "unsqueeze", "(", "0", ")", "# [1, num_sents, num_cands, 3*conv_size]", "\n", "", "else", ":", "\n", "            ", "max_n", "=", "max", "(", "sent_nums", ")", "\n", "enc_candidates", "=", "[", "self", ".", "_article_encode", "(", "article", "=", "article", ",", "device", "=", "device", ")", "for", "article", "in", "article_sents", "]", "\n", "# enc_candidates: each item has size of [num_sents, num_cands, 3*conv_size]", "\n", "enc_candidates_flattened", "=", "torch", ".", "cat", "(", "enc_candidates", ",", "dim", "=", "0", ")", "# [total_num_sents_in_batch, num_cands, 3*conv_size]", "\n", "#print()", "\n", "#print(\"enc_candidates_flattened.size\")", "\n", "#print(enc_candidates_flattened.size())", "\n", "enc_sents_flattened", "=", "self", ".", "_candidate_agg", "(", "enc_candidates_flattened", ")", "# [total_num_sents_in_batch, 3*conv_size]", "\n", "#print(\"enc_sents_flattened.size()\")", "\n", "#print(enc_sents_flattened.size())", "\n", "\n", "# pad an article if its sent_num less than max_n", "\n", "def", "zero_enc_sent", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "_art_enc", ".", "input_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "", "start_idx", "=", "0", "\n", "enc_sents_padded", "=", "[", "]", "\n", "for", "n", "in", "sent_nums", ":", "\n", "                ", "end_idx", "=", "start_idx", "+", "n", "\n", "s", "=", "enc_sents_flattened", "[", "start_idx", ":", "end_idx", ",", ":", "]", "\n", "padded_s", "=", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_sent", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "if", "n", "!=", "max_n", "else", "s", "# [max_n, 3*conv_size]", "\n", "enc_sents_padded", ".", "append", "(", "padded_s", ")", "\n", "start_idx", "=", "end_idx", "\n", "\n", "# pad enc_sentences to max_n", "\n", "", "enc_sents_padded", "=", "torch", ".", "stack", "(", "enc_sents_padded", ",", "dim", "=", "0", ")", ".", "contiguous", "(", ")", "# [batch, max_n, 3*conv_size]", "\n", "#print(\"enc_sents_padded_size\")", "\n", "#print(enc_sents_padded.size())", "\n", "\n", "# pad enc_candidates to max_n", "\n", "def", "zero_enc_candidate", "(", "n", ",", "device", ")", ":", "\n", "                ", "z", "=", "torch", ".", "zeros", "(", "n", ",", "self", ".", "num_candidates", ",", "self", ".", "_candidate_sent_enc", ".", "output_size", ")", ".", "to", "(", "device", ")", "\n", "return", "z", "\n", "\n", "# a list of [num_sents, num_cands, 3*conv_size] -> [batch, max_n, num_cands, 3*conv_size]", "\n", "", "enc_candidates_out", "=", "torch", ".", "stack", "(", "[", "torch", ".", "cat", "(", "[", "s", ",", "zero_enc_candidate", "(", "max_n", "-", "n", ",", "s", ".", "device", ")", "]", ",", "dim", "=", "0", ")", "\n", "if", "n", "!=", "max_n", "else", "s", "for", "s", ",", "n", "in", "zip", "(", "enc_candidates", ",", "sent_nums", ")", "]", ",", "\n", "dim", "=", "0", ")", "# [batch, max_n, num_cands, 3*conv_size]", "\n", "\n", "# compute context-aware embedding for each article sentence", "\n", "", "lstm_out", "=", "self", ".", "_art_enc", "(", "enc_sents_padded", ",", "sent_nums", ")", "# [batch, max_n, 2 * lstm_hidden]", "\n", "#print(\"lstm_out.size()\")", "\n", "#print(lstm_out.size())", "\n", "#print(\"enc_candidates_out_size\")", "\n", "#print(enc_candidates_out.size())", "\n", "\n", "return", "lstm_out", ",", "enc_candidates_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.PtrExtractRewrittenSentWordBertSumm._article_encode": [[1290, 1322], ["article.size", "torch.ne().to", "extract.PtrExtractRewrittenSentWordBertSumm._sentence_encoder.encode_word", "extract.PtrExtractRewrittenSentWordBertSumm._bert_w", "extract.PtrExtractRewrittenSentWordBertSumm._candidate_sent_enc", "enc_candidate.view.view.view", "torch.ne"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.sentence_transformer_wrapper.SentenceTransformerWrapper.encode_word"], ["", "def", "_article_encode", "(", "self", ",", "article", ",", "device", ",", "pad_idx", "=", "0", ")", ":", "\n", "# process one article", "\n", "# article: [num_sents * num_cands, sent_len]", "\n", "# create input for BERT", "\n", "        ", "num_sents_all", ",", "sent_len", "=", "article", ".", "size", "(", ")", "\n", "#print(\"article_size\")", "\n", "#print(article.size())", "\n", "\n", "# padding if needed", "\n", "\"\"\"\n        if sent_len <= 2:\n            left_padding = article.new_zeros(num_sents_all, 2)\n            right_padding = article.new_zeros(num_sents_all, 2)\n            article = torch.cat([left_padding, article, right_padding], dim=1)\n        elif sent_len <= 4:\n            left_padding = article.new_zeros(num_sents_all, 1)\n            right_padding = article.new_zeros(num_sents_all, 1)\n            article = torch.cat([left_padding, article, right_padding], dim=1)\n        \"\"\"", "\n", "\n", "attention_mask", "=", "torch", ".", "ne", "(", "article", ",", "0", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "token_embeddings", "=", "self", ".", "_sentence_encoder", ".", "encode_word", "(", "article", ",", "attention_mask", ")", "\n", "# [num_sents * num_cands, sent_len, num_bert_layers * 768]", "\n", "\n", "emb_input", "=", "self", ".", "_bert_w", "(", "token_embeddings", ")", "# [num_sents * num_cands, sent_len, emb_dim]", "\n", "\n", "enc_candidate", "=", "self", ".", "_candidate_sent_enc", "(", "emb_input", ")", "\n", "enc_candidate", "=", "enc_candidate", ".", "view", "(", "num_sents_all", "//", "self", ".", "num_candidates", ",", "self", ".", "num_candidates", ",", "-", "1", ")", "\n", "# [num_doc_sents, num_cands, 3 * conv_size]", "\n", "#print(\"enc_candidate_size:\")", "\n", "#print(enc_candidate.size())", "\n", "return", "enc_candidate", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.__init__": [[1326, 1361], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.LSTM", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.uniform_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.uniform_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "\n", "dropout", ",", "n_hop", ",", "auto_stop", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init_h", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_layer", ",", "n_hidden", ")", ")", "\n", "self", ".", "_init_c", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_layer", ",", "n_hidden", ")", ")", "\n", "self", ".", "_init_i", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ")", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_h", ",", "-", "INI", ",", "INI", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_c", ",", "-", "INI", ",", "INI", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_init_i", ",", "-", "0.1", ",", "0.1", ")", "\n", "self", ".", "_lstm", "=", "nn", ".", "LSTM", "(", "\n", "input_dim", ",", "n_hidden", ",", "n_layer", ",", "\n", "bidirectional", "=", "False", ",", "dropout", "=", "dropout", "\n", ")", "\n", "self", ".", "_lstm_cell", "=", "None", "\n", "\n", "# attention parameters", "\n", "self", ".", "_attn_wm", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ",", "n_hidden", ")", ")", "\n", "self", ".", "_attn_wq", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ",", "n_hidden", ")", ")", "\n", "self", ".", "_attn_v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ")", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wm", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_attn_wq", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_attn_v", ",", "-", "INI", ",", "INI", ")", "\n", "\n", "# hop parameters", "\n", "self", ".", "_hop_wm", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ",", "n_hidden", ")", ")", "\n", "self", ".", "_hop_wq", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ",", "n_hidden", ")", ")", "\n", "self", ".", "_hop_v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "n_hidden", ")", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_hop_wm", ")", "\n", "init", ".", "xavier_normal_", "(", "self", ".", "_hop_wq", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_hop_v", ",", "-", "INI", ",", "INI", ")", "\n", "self", ".", "_n_hop", "=", "n_hop", "\n", "self", ".", "_auto_stop", "=", "auto_stop", "\n", "if", "auto_stop", ":", "\n", "            ", "self", ".", "_stop", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ")", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_stop", ",", "-", "INI", ",", "INI", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.forward": [[1362, 1382], ["extract.HierarchicalLSTMPointerNet._prepare", "torch.cat().transpose", "extract.HierarchicalLSTMPointerNet._lstm", "extract.LSTMPointerNet.attention", "range", "extract.LSTMPointerNet.attention_score", "torch.cat.size", "torch.cat", "torch.LongTensor().to", "extract.HierarchicalLSTMPointerNet._stop.unsqueeze().expand", "extract.LSTMPointerNet.attention", "torch.cat", "torch.zeros().to", "torch.LongTensor", "extract.HierarchicalLSTMPointerNet._stop.unsqueeze", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet._prepare", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention"], ["", "", "def", "forward", "(", "self", ",", "attn_mem", ",", "mem_sizes", ",", "lstm_in", ")", ":", "\n", "        ", "\"\"\"atten_mem: Tensor of size [batch_size, max_sent_num, input_dim]\"\"\"", "\n", "if", "self", ".", "_auto_stop", ":", "\n", "            ", "batch_size", ",", "max_sent_num", ",", "input_dim", "=", "attn_mem", ".", "size", "(", ")", "\n", "# insert stop representation", "\n", "attn_mem", "=", "torch", ".", "cat", "(", "[", "attn_mem", ",", "torch", ".", "zeros", "(", "batch_size", ",", "1", ",", "input_dim", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "# [batch, max_sent_num+1, input_dim]", "\n", "mem_sizes_tensor", "=", "torch", ".", "LongTensor", "(", "mem_sizes", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "\n", "attn_mem", "[", ":", ",", "mem_sizes_tensor", ",", ":", "]", "=", "self", ".", "_stop", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", "# [batch_size, input_dim]", "\n", "mem_sizes", "=", "[", "s", "+", "1", "for", "s", "in", "mem_sizes", "]", "\n", "\n", "", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "init_i", "=", "self", ".", "_prepare", "(", "attn_mem", ")", "\n", "lstm_in", "=", "torch", ".", "cat", "(", "[", "init_i", ",", "lstm_in", "]", ",", "dim", "=", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "query", ",", "final_states", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "lstm_states", ")", "\n", "query", "=", "query", ".", "transpose", "(", "0", ",", "1", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "_n_hop", ")", ":", "\n", "            ", "query", "=", "LSTMPointerNet", ".", "attention", "(", "\n", "hop_feat", ",", "query", ",", "self", ".", "_hop_v", ",", "self", ".", "_hop_wq", ",", "mem_sizes", ")", "\n", "", "output", "=", "LSTMPointerNet", ".", "attention_score", "(", "\n", "attn_feat", ",", "query", ",", "self", ".", "_attn_v", ",", "self", ".", "_attn_wq", ")", "\n", "return", "output", "# unormalized extraction logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.extract": [[1383, 1422], ["extract.HierarchicalLSTMPointerNet._prepare", "lstm_in.squeeze.squeeze.squeeze", "torch.cat.size", "torch.cat", "rnn.MultiLayerLSTMCells.convert().to", "extract.HierarchicalLSTMPointerNet._lstm_cell", "range", "extract.LSTMPointerNet.attention_score", "score.squeeze.squeeze.squeeze", "[].item", "extracts.append", "extract.LSTMPointerNet.attention", "extract.HierarchicalLSTMPointerNet._stop.view", "rnn.MultiLayerLSTMCells.convert", "score.squeeze.squeeze.max"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet._prepare", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.rnn.MultiLayerLSTMCells.convert"], ["", "def", "extract", "(", "self", ",", "attn_mem", ",", "mem_sizes", ",", "k", ",", "disable_selected_mask", "=", "False", ")", ":", "\n", "        ", "\"\"\"extract k sentences, decode only, batch_size==1\"\"\"", "\n", "# atten_mem: Tensor of size [1, max_sent_num, input_dim]", "\n", "if", "self", ".", "_auto_stop", ":", "\n", "            ", "end_step", "=", "attn_mem", ".", "size", "(", "1", ")", "\n", "attn_mem", "=", "torch", ".", "cat", "(", "[", "attn_mem", ",", "self", ".", "_stop", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "]", ",", "dim", "=", "1", ")", "# [1, max_sent_num+1, input_dim]", "\n", "\n", "", "use_selected_mask", "=", "not", "disable_selected_mask", "\n", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "lstm_in", "=", "self", ".", "_prepare", "(", "attn_mem", ")", "\n", "lstm_in", "=", "lstm_in", ".", "squeeze", "(", "1", ")", "\n", "if", "self", ".", "_lstm_cell", "is", "None", ":", "\n", "            ", "self", ".", "_lstm_cell", "=", "MultiLayerLSTMCells", ".", "convert", "(", "\n", "self", ".", "_lstm", ")", ".", "to", "(", "attn_mem", ".", "device", ")", "\n", "", "extracts", "=", "[", "]", "\n", "num_extracted_sent", "=", "0", "\n", "while", "True", ":", "\n", "            ", "h", ",", "c", "=", "self", ".", "_lstm_cell", "(", "lstm_in", ",", "lstm_states", ")", "\n", "query", "=", "h", "[", "-", "1", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_n_hop", ")", ":", "\n", "                ", "query", "=", "LSTMPointerNet", ".", "attention", "(", "\n", "hop_feat", ",", "query", ",", "self", ".", "_hop_v", ",", "self", ".", "_hop_wq", ",", "mem_sizes", ")", "\n", "", "score", "=", "LSTMPointerNet", ".", "attention_score", "(", "\n", "attn_feat", ",", "query", ",", "self", ".", "_attn_v", ",", "self", ".", "_attn_wq", ")", "# [1, 1, Ns]", "\n", "score", "=", "score", ".", "squeeze", "(", ")", "# [Ns]", "\n", "# set logit to -inf if the sentence is selected before", "\n", "if", "use_selected_mask", ":", "\n", "                ", "for", "e", "in", "extracts", ":", "\n", "                    ", "score", "[", "e", "]", "=", "-", "1e6", "\n", "", "", "ext", "=", "score", ".", "max", "(", "dim", "=", "0", ")", "[", "1", "]", ".", "item", "(", ")", "\n", "if", "self", ".", "_auto_stop", ":", "# break the loop if eos is selected, does not include eos to the extracts", "\n", "                ", "if", "ext", "==", "end_step", ":", "\n", "                    ", "break", "\n", "", "", "extracts", ".", "append", "(", "ext", ")", "\n", "num_extracted_sent", "+=", "1", "\n", "if", "(", "not", "self", ".", "_auto_stop", "and", "num_extracted_sent", "==", "k", ")", "or", "num_extracted_sent", "==", "MAX_EXT", ":", "\n", "                ", "break", "\n", "", "lstm_states", "=", "(", "h", ",", "c", ")", "\n", "lstm_in", "=", "attn_mem", "[", ":", ",", "ext", ",", ":", "]", "\n", "", "return", "extracts", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet._prepare": [[1423, 1434], ["torch.matmul", "torch.matmul", "attn_mem.size", "extract.HierarchicalLSTMPointerNet._init_h.size", "extract.HierarchicalLSTMPointerNet._init_i.size", "extract.HierarchicalLSTMPointerNet._init_i.unsqueeze().unsqueeze().expand", "extract.HierarchicalLSTMPointerNet._attn_wm.unsqueeze", "extract.HierarchicalLSTMPointerNet._hop_wm.unsqueeze", "extract.HierarchicalLSTMPointerNet._init_h.unsqueeze().expand().contiguous", "extract.HierarchicalLSTMPointerNet._init_c.unsqueeze().expand().contiguous", "extract.HierarchicalLSTMPointerNet._init_i.unsqueeze().unsqueeze", "extract.HierarchicalLSTMPointerNet._init_h.unsqueeze().expand", "extract.HierarchicalLSTMPointerNet._init_c.unsqueeze().expand", "extract.HierarchicalLSTMPointerNet._init_i.unsqueeze", "extract.HierarchicalLSTMPointerNet._init_h.unsqueeze", "extract.HierarchicalLSTMPointerNet._init_c.unsqueeze"], "methods", ["None"], ["", "def", "_prepare", "(", "self", ",", "attn_mem", ")", ":", "\n", "        ", "attn_feat", "=", "torch", ".", "matmul", "(", "attn_mem", ",", "self", ".", "_attn_wm", ".", "unsqueeze", "(", "0", ")", ")", "\n", "hop_feat", "=", "torch", ".", "matmul", "(", "attn_mem", ",", "self", ".", "_hop_wm", ".", "unsqueeze", "(", "0", ")", ")", "\n", "bs", "=", "attn_mem", ".", "size", "(", "0", ")", "\n", "n_l", ",", "d", "=", "self", ".", "_init_h", ".", "size", "(", ")", "\n", "size", "=", "(", "n_l", ",", "bs", ",", "d", ")", "\n", "lstm_states", "=", "(", "self", ".", "_init_h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ".", "contiguous", "(", ")", ",", "\n", "self", ".", "_init_c", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "*", "size", ")", ".", "contiguous", "(", ")", ")", "\n", "d", "=", "self", ".", "_init_i", ".", "size", "(", "0", ")", "\n", "init_i", "=", "self", ".", "_init_i", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "1", ",", "d", ")", "\n", "return", "attn_feat", ",", "hop_feat", ",", "lstm_states", ",", "init_i", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score": [[1435, 1445], ["torch.matmul().squeeze", "attention.unsqueeze", "torch.matmul().unsqueeze", "torch.matmul", "torch.matmul", "torch.nn.functional.tanh", "v.unsqueeze().unsqueeze().unsqueeze", "w.unsqueeze", "v.unsqueeze().unsqueeze", "v.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "attention_score", "(", "attention", ",", "query", ",", "v", ",", "w", ")", ":", "\n", "        ", "\"\"\" unnormalized attention score\"\"\"", "\n", "sum_", "=", "attention", ".", "unsqueeze", "(", "1", ")", "+", "torch", ".", "matmul", "(", "\n", "query", ",", "w", ".", "unsqueeze", "(", "0", ")", "\n", ")", ".", "unsqueeze", "(", "2", ")", "# [B, Nq, Ns, D]", "\n", "score", "=", "torch", ".", "matmul", "(", "\n", "F", ".", "tanh", "(", "sum_", ")", ",", "v", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "3", ")", "\n", ")", ".", "squeeze", "(", "3", ")", "# [B, Nq, Ns]", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention": [[1446, 1457], ["extract.LSTMPointerNet.attention_score", "torch.matmul", "torch.nn.functional.softmax", "util.len_mask().unsqueeze", "attention.prob_normalize", "util.len_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.extract.HierarchicalLSTMPointerNet.attention_score", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.prob_normalize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "@", "staticmethod", "\n", "def", "attention", "(", "attention", ",", "query", ",", "v", ",", "w", ",", "mem_sizes", ")", ":", "\n", "        ", "\"\"\" attention context vector\"\"\"", "\n", "score", "=", "LSTMPointerNet", ".", "attention_score", "(", "attention", ",", "query", ",", "v", ",", "w", ")", "\n", "if", "mem_sizes", "is", "None", ":", "\n", "            ", "norm_score", "=", "F", ".", "softmax", "(", "score", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "mask", "=", "len_mask", "(", "mem_sizes", ",", "score", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "norm_score", "=", "prob_normalize", "(", "score", ",", "mask", ")", "\n", "", "output", "=", "torch", ".", "matmul", "(", "norm_score", ",", "attention", ")", "\n", "return", "output", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlSumm.__init__": [[16, 28], ["copy_summ.CopySumm.__init__", "copy_summ._CopyLinear", "rnn.MultiLayerLSTMCells", "torch.nn.Embedding", "torch.nn.init.uniform_", "controllable_abstractor.CompressControlDecoder"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "n_compression_levels", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "dropout", ")", "\n", "self", ".", "_copy", "=", "_CopyLinear", "(", "n_hidden", ",", "n_hidden", ",", "3", "*", "emb_dim", ")", "\n", "self", ".", "_dec_lstm", "=", "MultiLayerLSTMCells", "(", "3", "*", "emb_dim", ",", "n_hidden", ",", "n_layer", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "_compress_level_embedding", "=", "nn", ".", "Embedding", "(", "n_compression_levels", ",", "emb_dim", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_compress_level_embedding", ".", "weight", ",", "-", "0.1", ",", "0.1", ")", "\n", "\n", "self", ".", "_decoder", "=", "CompressControlDecoder", "(", "\n", "self", ".", "_copy", ",", "self", ".", "_embedding", ",", "self", ".", "_dec_lstm", ",", "\n", "self", ".", "_attn_wq", ",", "self", ".", "_projection", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlSumm.forward": [[30, 45], ["controllable_abstractor.CompressControlSumm.encode", "util.len_mask().unsqueeze", "controllable_abstractor.CompressControlSumm._compress_level_embedding", "controllable_abstractor.CompressControlSumm._decoder", "util.len_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "forward", "(", "self", ",", "article", ",", "art_lens", ",", "compression_levels", ",", "abstract", ",", "extend_art", ",", "extend_vsize", ")", ":", "\n", "        ", "\"\"\"\n        :param article:\n        :param art_lens:\n        :param compression_levels: int Tensor, size=[batch_size]\n        :param abstract:\n        :param extend_art:\n        :param extend_vsize:\n        :return:\n        \"\"\"", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "compression_levels_representation", "=", "self", ".", "_compress_level_embedding", "(", "compression_levels", ")", "# [batch, embed_size]", "\n", "logit", "=", "self", ".", "_decoder", "(", "(", "attention", ",", "mask", ",", "compression_levels_representation", ",", "extend_art", ",", "extend_vsize", ")", ",", "init_dec_states", ",", "abstract", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlSumm.batch_decode": [[46, 66], ["len", "controllable_abstractor.CompressControlSumm.encode", "util.len_mask().unsqueeze", "controllable_abstractor.CompressControlSumm._compress_level_embedding", "torch.LongTensor().to", "range", "controllable_abstractor.CompressControlSumm._decoder.decode_step", "attns.append", "outputs.append", "torch.LongTensor().to.masked_fill_", "util.len_mask", "torch.LongTensor", "tok[].clone"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "batch_decode", "(", "self", ",", "article", ",", "art_lens", ",", "compression_levels", ",", "extend_art", ",", "extend_vsize", ",", "\n", "go", ",", "eos", ",", "unk", ",", "max_len", ")", ":", "\n", "        ", "\"\"\" greedy decode support batching\"\"\"", "\n", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "compression_levels_representation", "=", "self", ".", "_compress_level_embedding", "(", "compression_levels", ")", "\n", "attention", "=", "(", "attention", ",", "mask", ",", "compression_levels_representation", ",", "extend_art", ",", "extend_vsize", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", "*", "batch_size", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "attns", ".", "append", "(", "attn_score", ")", "\n", "outputs", ".", "append", "(", "tok", "[", ":", ",", "0", "]", ".", "clone", "(", ")", ")", "\n", "tok", ".", "masked_fill_", "(", "tok", ">=", "vsize", ",", "unk", ")", "\n", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlSumm.decode": [[67, 86], ["controllable_abstractor.CompressControlSumm.encode", "controllable_abstractor.CompressControlSumm._compress_level_embedding", "torch.LongTensor().to", "range", "controllable_abstractor.CompressControlSumm._decoder.decode_step", "outputs.append", "attns.append", "torch.LongTensor", "tok[].item", "tok[].item", "attn_score.squeeze", "tok[].item"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step"], ["", "def", "decode", "(", "self", ",", "article", ",", "compression_levels", ",", "extend_art", ",", "extend_vsize", ",", "go", ",", "eos", ",", "unk", ",", "max_len", ")", ":", "\n", "        ", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ")", "\n", "compression_levels_representation", "=", "self", ".", "_compress_level_embedding", "(", "compression_levels", ")", "\n", "attention", "=", "(", "attention", ",", "None", ",", "compression_levels_representation", ",", "extend_art", ",", "extend_vsize", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", "==", "eos", ":", "\n", "                ", "break", "\n", "", "outputs", ".", "append", "(", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ")", "\n", "attns", ".", "append", "(", "attn_score", ".", "squeeze", "(", "0", ")", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ">=", "vsize", ":", "\n", "                ", "tok", "[", "0", ",", "0", "]", "=", "unk", "\n", "", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlSumm.batched_beamsearch": [[87, 164], ["len", "controllable_abstractor.CompressControlSumm.encode", "util.len_mask().unsqueeze", "controllable_abstractor.CompressControlSumm._compress_level_embedding", "range", "beam_search.init_beam", "filter", "torch.stack", "torch.stack.masked_fill_", "controllable_abstractor.CompressControlSumm._decoder.topk_step", "enumerate", "all", "enumerate", "util.len_mask", "range", "range", "range", "beam_search.pack_beam", "toks.append", "all_states.append", "torch.stack", "zip", "beam_search.next_search_beam", "zip", "torch.stack", "torch.stack", "len", "torch.LongTensor().to", "map", "torch.stack", "enumerate", "enumerate", "torch.LongTensor", "v.index_select"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.init_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder.topk_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.pack_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.next_search_beam"], ["", "def", "batched_beamsearch", "(", "self", ",", "article", ",", "art_lens", ",", "compression_levels", ",", "\n", "extend_art", ",", "extend_vsize", ",", "\n", "go", ",", "eos", ",", "unk", ",", "max_len", ",", "beam_size", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "compression_levels_representation", "=", "self", ".", "_compress_level_embedding", "(", "compression_levels", ")", "\n", "all_attention", "=", "(", "attention", ",", "mask", ",", "compression_levels_representation", ",", "extend_art", ",", "extend_vsize", ")", "\n", "attention", "=", "all_attention", "\n", "(", "h", ",", "c", ")", ",", "prev", "=", "init_dec_states", "\n", "all_beams", "=", "[", "bs", ".", "init_beam", "(", "go", ",", "(", "h", "[", ":", ",", "i", ",", ":", "]", ",", "c", "[", ":", ",", "i", ",", ":", "]", ",", "prev", "[", "i", "]", ")", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "finished_beams", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "outputs", "=", "[", "None", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "for", "t", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "toks", "=", "[", "]", "\n", "all_states", "=", "[", "]", "\n", "for", "beam", "in", "filter", "(", "bool", ",", "all_beams", ")", ":", "\n", "                ", "token", ",", "states", "=", "bs", ".", "pack_beam", "(", "beam", ",", "article", ".", "device", ")", "\n", "toks", ".", "append", "(", "token", ")", "\n", "all_states", ".", "append", "(", "states", ")", "\n", "", "token", "=", "torch", ".", "stack", "(", "toks", ",", "dim", "=", "1", ")", "\n", "states", "=", "(", "(", "torch", ".", "stack", "(", "[", "h", "for", "(", "h", ",", "_", ")", ",", "_", "in", "all_states", "]", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "stack", "(", "[", "c", "for", "(", "_", ",", "c", ")", ",", "_", "in", "all_states", "]", ",", "dim", "=", "2", ")", ")", ",", "\n", "torch", ".", "stack", "(", "[", "prev", "for", "_", ",", "prev", "in", "all_states", "]", ",", "dim", "=", "1", ")", ")", "\n", "token", ".", "masked_fill_", "(", "token", ">=", "vsize", ",", "unk", ")", "\n", "\n", "topk", ",", "lp", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "topk_step", "(", "\n", "token", ",", "states", ",", "attention", ",", "beam_size", ")", "\n", "\n", "batch_i", "=", "0", "\n", "for", "i", ",", "(", "beam", ",", "finished", ")", "in", "enumerate", "(", "zip", "(", "all_beams", ",", "\n", "finished_beams", ")", ")", ":", "\n", "                ", "if", "not", "beam", ":", "\n", "                    ", "continue", "\n", "", "finished", ",", "new_beam", "=", "bs", ".", "next_search_beam", "(", "\n", "beam", ",", "beam_size", ",", "finished", ",", "eos", ",", "\n", "topk", "[", ":", ",", "batch_i", ",", ":", "]", ",", "lp", "[", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "(", "states", "[", "0", "]", "[", "0", "]", "[", ":", ",", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "states", "[", "0", "]", "[", "1", "]", "[", ":", ",", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "states", "[", "1", "]", "[", ":", ",", "batch_i", ",", ":", "]", ")", ",", "\n", "attn_score", "[", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "diverse", "\n", ")", "\n", "batch_i", "+=", "1", "\n", "if", "len", "(", "finished", ")", ">=", "beam_size", ":", "\n", "                    ", "all_beams", "[", "i", "]", "=", "[", "]", "\n", "outputs", "[", "i", "]", "=", "finished", "[", ":", "beam_size", "]", "\n", "# exclude finished inputs", "\n", "(", "attention", ",", "mask", ",", "compression_levels_representation", ",", "extend_art", ",", "extend_vsize", "\n", ")", "=", "all_attention", "\n", "masks", "=", "[", "mask", "[", "j", "]", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "\n", "if", "o", "is", "None", "]", "\n", "ind", "=", "[", "j", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "if", "o", "is", "None", "]", "\n", "ind", "=", "torch", ".", "LongTensor", "(", "ind", ")", ".", "to", "(", "attention", ".", "device", ")", "\n", "attention", ",", "compression_levels_representation", ",", "extend_art", "=", "map", "(", "\n", "lambda", "v", ":", "v", ".", "index_select", "(", "dim", "=", "0", ",", "index", "=", "ind", ")", ",", "\n", "[", "attention", ",", "compression_levels_representation", ",", "extend_art", "]", "\n", ")", "\n", "if", "masks", ":", "\n", "                        ", "mask", "=", "torch", ".", "stack", "(", "masks", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "mask", "=", "None", "\n", "", "attention", "=", "(", "\n", "attention", ",", "mask", ",", "compression_levels_representation", ",", "extend_art", ",", "extend_vsize", ")", "\n", "", "else", ":", "\n", "                    ", "all_beams", "[", "i", "]", "=", "new_beam", "\n", "finished_beams", "[", "i", "]", "=", "finished", "\n", "", "", "if", "all", "(", "outputs", ")", ":", "\n", "                ", "break", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "(", "o", ",", "f", ",", "b", ")", "in", "enumerate", "(", "zip", "(", "outputs", ",", "\n", "finished_beams", ",", "all_beams", ")", ")", ":", "\n", "                ", "if", "o", "is", "None", ":", "\n", "                    ", "outputs", "[", "i", "]", "=", "(", "f", "+", "b", ")", "[", ":", "beam_size", "]", "\n", "", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlDecoder.__init__": [[166, 168], ["copy_summ.CopyLSTMDecoder.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "copy", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "copy", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlDecoder._step": [[169, 196], ["torch.cat", "controllable_abstractor.CompressControlDecoder._lstm", "torch.mm", "attention.step_attention", "controllable_abstractor.CompressControlDecoder._projection", "controllable_abstractor.CompressControlDecoder._compute_gen_prob", "torch.sigmoid", "torch.log", "torch.cat", "controllable_abstractor.CompressControlDecoder._copy", "controllable_abstractor.CompressControlDecoder._embedding().squeeze", "controllable_abstractor.CompressControlDecoder._embedding", "extend_src.expand_as"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log"], ["", "def", "_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ")", ":", "\n", "        ", "prev_states", ",", "prev_out", "=", "states", "# prev_out: [batch_size, embed_size]", "\n", "attention", ",", "attn_mask", ",", "compression_levels_representation", ",", "extend_src", ",", "extend_vsize", "=", "attention", "# compression_levels_representation: [batch_size, embed_size]", "\n", "lstm_in", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ".", "squeeze", "(", "1", ")", ",", "prev_out", ",", "compression_levels_representation", "]", ",", "\n", "dim", "=", "1", "\n", ")", "\n", "states", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "query", "=", "torch", ".", "mm", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "# [batch_size, hidden_dim]", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# extend generation prob to extended vocabulary", "\n", "gen_prob", "=", "self", ".", "_compute_gen_prob", "(", "dec_out", ",", "extend_vsize", ")", "\n", "# compute the probabilty of each copying", "\n", "copy_prob", "=", "torch", ".", "sigmoid", "(", "self", ".", "_copy", "(", "context", ",", "states", "[", "0", "]", "[", "-", "1", "]", ",", "lstm_in", ")", ")", "\n", "# add the copy prob to existing vocab distribution", "\n", "lp", "=", "torch", ".", "log", "(", "\n", "(", "(", "-", "copy_prob", "+", "1", ")", "*", "gen_prob", "\n", ")", ".", "scatter_add", "(", "\n", "dim", "=", "1", ",", "\n", "index", "=", "extend_src", ".", "expand_as", "(", "score", ")", ",", "\n", "src", "=", "score", "*", "copy_prob", "\n", ")", "+", "1e-8", ")", "# numerical stability for log", "\n", "return", "lp", ",", "(", "states", ",", "dec_out", ")", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.controllable_abstractor.CompressControlDecoder.topk_step": [[197, 237], ["h.size", "tok.size", "torch.cat", "torch.cat.contiguous().view", "controllable_abstractor.CompressControlDecoder._lstm", "torch.matmul", "attention.step_attention", "controllable_abstractor.CompressControlDecoder._projection", "controllable_abstractor.CompressControlDecoder._compute_gen_prob", "torch.sigmoid().contiguous().view", "torch.log().contiguous().view", "torch.log().contiguous().view.topk", "h.contiguous().view", "c.contiguous().view", "h.contiguous().view", "c.contiguous().view", "torch.cat", "controllable_abstractor.CompressControlDecoder.contiguous().view", "controllable_abstractor.CompressControlDecoder._embedding", "compression_levels_representation.unsqueeze().expand", "torch.cat.contiguous", "torch.sigmoid().contiguous", "torch.log().contiguous", "prev_out.size", "h.contiguous", "c.contiguous", "h.contiguous", "c.contiguous", "controllable_abstractor.CompressControlDecoder.contiguous", "compression_levels_representation.unsqueeze", "torch.sigmoid", "torch.log", "controllable_abstractor.CompressControlDecoder._copy", "extend_src.expand_as().contiguous().view", "score.contiguous().view", "extend_src.expand_as().contiguous", "score.contiguous", "extend_src.expand_as"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log"], ["", "def", "topk_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ",", "k", ")", ":", "\n", "        ", "\"\"\"tok:[BB, B], states ([L, BB, B, D]*2, [BB, B, D])\"\"\"", "\n", "(", "h", ",", "c", ")", ",", "prev_out", "=", "states", "\n", "attention", ",", "attn_mask", ",", "compression_levels_representation", ",", "extend_src", ",", "extend_vsize", "=", "attention", "\n", "# lstm is not bemable", "\n", "nl", ",", "_", ",", "_", ",", "d", "=", "h", ".", "size", "(", ")", "\n", "beam", ",", "batch", "=", "tok", ".", "size", "(", ")", "\n", "lstm_in_beamable", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ",", "prev_out", ",", "compression_levels_representation", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "prev_out", ".", "size", "(", "0", ")", ",", "-", "1", ",", "-", "1", ")", ",", "]", ",", "dim", "=", "-", "1", ")", "\n", "lstm_in", "=", "lstm_in_beamable", ".", "contiguous", "(", ")", ".", "view", "(", "beam", "*", "batch", ",", "-", "1", ")", "\n", "prev_states", "=", "(", "h", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "-", "1", ",", "d", ")", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "-", "1", ",", "d", ")", ")", "\n", "h", ",", "c", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "states", "=", "(", "h", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "beam", ",", "batch", ",", "-", "1", ")", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "beam", ",", "batch", ",", "-", "1", ")", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "\n", "# attention is beamable", "\n", "query", "=", "torch", ".", "matmul", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "# copy mechanism is not beamable", "\n", "gen_prob", "=", "self", ".", "_compute_gen_prob", "(", "\n", "dec_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", "*", "beam", ",", "-", "1", ")", ",", "extend_vsize", ")", "\n", "copy_prob", "=", "torch", ".", "sigmoid", "(", "\n", "self", ".", "_copy", "(", "context", ",", "lstm_out", ",", "lstm_in_beamable", ")", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "lp", "=", "torch", ".", "log", "(", "\n", "(", "(", "-", "copy_prob", "+", "1", ")", "*", "gen_prob", "\n", ")", ".", "scatter_add", "(", "\n", "dim", "=", "1", ",", "\n", "index", "=", "extend_src", ".", "expand_as", "(", "score", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "beam", "*", "batch", ",", "-", "1", ")", ",", "\n", "src", "=", "score", ".", "contiguous", "(", ")", ".", "view", "(", "beam", "*", "batch", ",", "-", "1", ")", "*", "copy_prob", "\n", ")", "+", "1e-8", ")", ".", "contiguous", "(", ")", ".", "view", "(", "beam", ",", "batch", ",", "-", "1", ")", "\n", "\n", "k_lp", ",", "k_tok", "=", "lp", ".", "topk", "(", "k", "=", "k", ",", "dim", "=", "-", "1", ")", "\n", "return", "k_tok", ",", "k_lp", ",", "(", "states", ",", "dec_out", ")", ",", "score", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ._CopyLinear.__init__": [[16, 28], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.nn.init.uniform_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.Parameter", "copy_summ._CopyLinear.regiser_module", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "context_dim", ",", "state_dim", ",", "input_dim", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_v_c", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "context_dim", ")", ")", "\n", "self", ".", "_v_s", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "state_dim", ")", ")", "\n", "self", ".", "_v_i", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_dim", ")", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_v_c", ",", "-", "INIT", ",", "INIT", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_v_s", ",", "-", "INIT", ",", "INIT", ")", "\n", "init", ".", "uniform_", "(", "self", ".", "_v_i", ",", "-", "INIT", ",", "INIT", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "_b", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "regiser_module", "(", "None", ",", "'_b'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ._CopyLinear.forward": [[29, 36], ["torch.matmul", "torch.matmul", "torch.matmul", "copy_summ._CopyLinear._v_i.unsqueeze", "copy_summ._CopyLinear._b.unsqueeze", "copy_summ._CopyLinear._v_c.unsqueeze", "copy_summ._CopyLinear._v_s.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "context", ",", "state", ",", "input_", ")", ":", "\n", "        ", "output", "=", "(", "torch", ".", "matmul", "(", "context", ",", "self", ".", "_v_c", ".", "unsqueeze", "(", "1", ")", ")", "\n", "+", "torch", ".", "matmul", "(", "state", ",", "self", ".", "_v_s", ".", "unsqueeze", "(", "1", ")", ")", "\n", "+", "torch", ".", "matmul", "(", "input_", ",", "self", ".", "_v_i", ".", "unsqueeze", "(", "1", ")", ")", ")", "\n", "if", "self", ".", "_b", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "_b", ".", "unsqueeze", "(", "0", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.__init__": [[39, 47], ["summ.Seq2SeqSumm.__init__", "copy_summ._CopyLinear", "copy_summ.CopyLSTMDecoder"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab_size", ",", "emb_dim", ",", "\n", "n_hidden", ",", "bidirectional", ",", "n_layer", ",", "dropout", ")", "\n", "self", ".", "_copy", "=", "_CopyLinear", "(", "n_hidden", ",", "n_hidden", ",", "2", "*", "emb_dim", ")", "\n", "self", ".", "_decoder", "=", "CopyLSTMDecoder", "(", "\n", "self", ".", "_copy", ",", "self", ".", "_embedding", ",", "self", ".", "_dec_lstm", ",", "\n", "self", ".", "_attn_wq", ",", "self", ".", "_projection", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.forward": [[49, 57], ["copy_summ.CopySumm.encode", "util.len_mask().unsqueeze", "copy_summ.CopySumm._decoder", "util.len_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "forward", "(", "self", ",", "article", ",", "art_lens", ",", "abstract", ",", "extend_art", ",", "extend_vsize", ")", ":", "\n", "        ", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "logit", "=", "self", ".", "_decoder", "(", "\n", "(", "attention", ",", "mask", ",", "extend_art", ",", "extend_vsize", ")", ",", "\n", "init_dec_states", ",", "abstract", "\n", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batch_decode": [[58, 77], ["len", "copy_summ.CopySumm.encode", "util.len_mask().unsqueeze", "torch.LongTensor().to", "range", "copy_summ.CopySumm._decoder.decode_step", "attns.append", "outputs.append", "torch.LongTensor().to.masked_fill_", "util.len_mask", "torch.LongTensor", "tok[].clone"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask"], ["", "def", "batch_decode", "(", "self", ",", "article", ",", "art_lens", ",", "extend_art", ",", "extend_vsize", ",", "\n", "go", ",", "eos", ",", "unk", ",", "max_len", ")", ":", "\n", "        ", "\"\"\" greedy decode support batching\"\"\"", "\n", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "attention", "=", "(", "attention", ",", "mask", ",", "extend_art", ",", "extend_vsize", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", "*", "batch_size", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "attns", ".", "append", "(", "attn_score", ")", "\n", "outputs", ".", "append", "(", "tok", "[", ":", ",", "0", "]", ".", "clone", "(", ")", ")", "\n", "tok", ".", "masked_fill_", "(", "tok", ">=", "vsize", ",", "unk", ")", "\n", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.decode": [[78, 96], ["copy_summ.CopySumm.encode", "torch.LongTensor().to", "range", "copy_summ.CopySumm._decoder.decode_step", "outputs.append", "attns.append", "torch.LongTensor", "tok[].item", "tok[].item", "attn_score.squeeze", "tok[].item"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.summ.AttentionalLSTMDecoder.decode_step"], ["", "def", "decode", "(", "self", ",", "article", ",", "extend_art", ",", "extend_vsize", ",", "go", ",", "eos", ",", "unk", ",", "max_len", ")", ":", "\n", "        ", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ")", "\n", "attention", "=", "(", "attention", ",", "None", ",", "extend_art", ",", "extend_vsize", ")", "\n", "tok", "=", "torch", ".", "LongTensor", "(", "[", "go", "]", ")", ".", "to", "(", "article", ".", "device", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "states", "=", "init_dec_states", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "tok", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "decode_step", "(", "\n", "tok", ",", "states", ",", "attention", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", "==", "eos", ":", "\n", "                ", "break", "\n", "", "outputs", ".", "append", "(", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ")", "\n", "attns", ".", "append", "(", "attn_score", ".", "squeeze", "(", "0", ")", ")", "\n", "if", "tok", "[", "0", ",", "0", "]", ".", "item", "(", ")", ">=", "vsize", ":", "\n", "                ", "tok", "[", "0", ",", "0", "]", "=", "unk", "\n", "", "", "return", "outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopySumm.batched_beamsearch": [[97, 173], ["len", "copy_summ.CopySumm.encode", "util.len_mask().unsqueeze", "range", "beam_search.init_beam", "filter", "torch.stack", "torch.stack.masked_fill_", "copy_summ.CopySumm._decoder.topk_step", "enumerate", "all", "enumerate", "util.len_mask", "range", "range", "range", "beam_search.pack_beam", "toks.append", "all_states.append", "torch.stack", "zip", "beam_search.next_search_beam", "zip", "torch.stack", "torch.stack", "len", "torch.LongTensor().to", "map", "torch.stack", "enumerate", "enumerate", "torch.LongTensor", "v.index_select"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.init_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder.topk_step", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.util.len_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.pack_beam", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.beam_search.next_search_beam"], ["", "def", "batched_beamsearch", "(", "self", ",", "article", ",", "art_lens", ",", "\n", "extend_art", ",", "extend_vsize", ",", "\n", "go", ",", "eos", ",", "unk", ",", "max_len", ",", "beam_size", ",", "diverse", "=", "1.0", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "art_lens", ")", "\n", "vsize", "=", "self", ".", "_embedding", ".", "num_embeddings", "\n", "attention", ",", "init_dec_states", "=", "self", ".", "encode", "(", "article", ",", "art_lens", ")", "\n", "mask", "=", "len_mask", "(", "art_lens", ",", "attention", ".", "device", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "all_attention", "=", "(", "attention", ",", "mask", ",", "extend_art", ",", "extend_vsize", ")", "\n", "attention", "=", "all_attention", "\n", "(", "h", ",", "c", ")", ",", "prev", "=", "init_dec_states", "\n", "all_beams", "=", "[", "bs", ".", "init_beam", "(", "go", ",", "(", "h", "[", ":", ",", "i", ",", ":", "]", ",", "c", "[", ":", ",", "i", ",", ":", "]", ",", "prev", "[", "i", "]", ")", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "finished_beams", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "outputs", "=", "[", "None", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "for", "t", "in", "range", "(", "max_len", ")", ":", "\n", "            ", "toks", "=", "[", "]", "\n", "all_states", "=", "[", "]", "\n", "for", "beam", "in", "filter", "(", "bool", ",", "all_beams", ")", ":", "\n", "                ", "token", ",", "states", "=", "bs", ".", "pack_beam", "(", "beam", ",", "article", ".", "device", ")", "\n", "toks", ".", "append", "(", "token", ")", "\n", "all_states", ".", "append", "(", "states", ")", "\n", "", "token", "=", "torch", ".", "stack", "(", "toks", ",", "dim", "=", "1", ")", "\n", "states", "=", "(", "(", "torch", ".", "stack", "(", "[", "h", "for", "(", "h", ",", "_", ")", ",", "_", "in", "all_states", "]", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "stack", "(", "[", "c", "for", "(", "_", ",", "c", ")", ",", "_", "in", "all_states", "]", ",", "dim", "=", "2", ")", ")", ",", "\n", "torch", ".", "stack", "(", "[", "prev", "for", "_", ",", "prev", "in", "all_states", "]", ",", "dim", "=", "1", ")", ")", "\n", "token", ".", "masked_fill_", "(", "token", ">=", "vsize", ",", "unk", ")", "\n", "\n", "topk", ",", "lp", ",", "states", ",", "attn_score", "=", "self", ".", "_decoder", ".", "topk_step", "(", "\n", "token", ",", "states", ",", "attention", ",", "beam_size", ")", "\n", "\n", "batch_i", "=", "0", "\n", "for", "i", ",", "(", "beam", ",", "finished", ")", "in", "enumerate", "(", "zip", "(", "all_beams", ",", "\n", "finished_beams", ")", ")", ":", "\n", "                ", "if", "not", "beam", ":", "\n", "                    ", "continue", "\n", "", "finished", ",", "new_beam", "=", "bs", ".", "next_search_beam", "(", "\n", "beam", ",", "beam_size", ",", "finished", ",", "eos", ",", "\n", "topk", "[", ":", ",", "batch_i", ",", ":", "]", ",", "lp", "[", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "(", "states", "[", "0", "]", "[", "0", "]", "[", ":", ",", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "states", "[", "0", "]", "[", "1", "]", "[", ":", ",", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "states", "[", "1", "]", "[", ":", ",", "batch_i", ",", ":", "]", ")", ",", "\n", "attn_score", "[", ":", ",", "batch_i", ",", ":", "]", ",", "\n", "diverse", "\n", ")", "\n", "batch_i", "+=", "1", "\n", "if", "len", "(", "finished", ")", ">=", "beam_size", ":", "\n", "                    ", "all_beams", "[", "i", "]", "=", "[", "]", "\n", "outputs", "[", "i", "]", "=", "finished", "[", ":", "beam_size", "]", "\n", "# exclude finished inputs", "\n", "(", "attention", ",", "mask", ",", "extend_art", ",", "extend_vsize", "\n", ")", "=", "all_attention", "\n", "masks", "=", "[", "mask", "[", "j", "]", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "\n", "if", "o", "is", "None", "]", "\n", "ind", "=", "[", "j", "for", "j", ",", "o", "in", "enumerate", "(", "outputs", ")", "if", "o", "is", "None", "]", "\n", "ind", "=", "torch", ".", "LongTensor", "(", "ind", ")", ".", "to", "(", "attention", ".", "device", ")", "\n", "attention", ",", "extend_art", "=", "map", "(", "\n", "lambda", "v", ":", "v", ".", "index_select", "(", "dim", "=", "0", ",", "index", "=", "ind", ")", ",", "\n", "[", "attention", ",", "extend_art", "]", "\n", ")", "\n", "if", "masks", ":", "\n", "                        ", "mask", "=", "torch", ".", "stack", "(", "masks", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "mask", "=", "None", "\n", "", "attention", "=", "(", "\n", "attention", ",", "mask", ",", "extend_art", ",", "extend_vsize", ")", "\n", "", "else", ":", "\n", "                    ", "all_beams", "[", "i", "]", "=", "new_beam", "\n", "finished_beams", "[", "i", "]", "=", "finished", "\n", "", "", "if", "all", "(", "outputs", ")", ":", "\n", "                ", "break", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "(", "o", ",", "f", ",", "b", ")", "in", "enumerate", "(", "zip", "(", "outputs", ",", "\n", "finished_beams", ",", "all_beams", ")", ")", ":", "\n", "                ", "if", "o", "is", "None", ":", "\n", "                    ", "outputs", "[", "i", "]", "=", "(", "f", "+", "b", ")", "[", ":", "beam_size", "]", "\n", "", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder.__init__": [[176, 179], ["summ.AttentionalLSTMDecoder.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "copy", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "_copy", "=", "copy", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._step": [[180, 207], ["torch.cat", "copy_summ.CopyLSTMDecoder._lstm", "torch.mm", "attention.step_attention", "copy_summ.CopyLSTMDecoder._projection", "copy_summ.CopyLSTMDecoder._compute_gen_prob", "torch.sigmoid", "torch.log", "torch.cat", "copy_summ.CopyLSTMDecoder._copy", "copy_summ.CopyLSTMDecoder._embedding().squeeze", "copy_summ.CopyLSTMDecoder._embedding", "extend_src.expand_as"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log"], ["", "def", "_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ")", ":", "\n", "        ", "prev_states", ",", "prev_out", "=", "states", "\n", "lstm_in", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ".", "squeeze", "(", "1", ")", ",", "prev_out", "]", ",", "\n", "dim", "=", "1", "\n", ")", "\n", "states", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "query", "=", "torch", ".", "mm", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "# [batch_size, hidden_dim]", "\n", "attention", ",", "attn_mask", ",", "extend_src", ",", "extend_vsize", "=", "attention", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# extend generation prob to extended vocabulary", "\n", "gen_prob", "=", "self", ".", "_compute_gen_prob", "(", "dec_out", ",", "extend_vsize", ")", "\n", "# compute the probabilty of each copying", "\n", "copy_prob", "=", "torch", ".", "sigmoid", "(", "self", ".", "_copy", "(", "context", ",", "states", "[", "0", "]", "[", "-", "1", "]", ",", "lstm_in", ")", ")", "\n", "# add the copy prob to existing vocab distribution", "\n", "lp", "=", "torch", ".", "log", "(", "\n", "(", "(", "-", "copy_prob", "+", "1", ")", "*", "gen_prob", "\n", ")", ".", "scatter_add", "(", "\n", "dim", "=", "1", ",", "\n", "index", "=", "extend_src", ".", "expand_as", "(", "score", ")", ",", "\n", "src", "=", "score", "*", "copy_prob", "\n", ")", "+", "1e-8", ")", "# numerical stability for log", "\n", "return", "lp", ",", "(", "states", ",", "dec_out", ")", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder.topk_step": [[209, 250], ["h.size", "tok.size", "torch.cat", "torch.cat.contiguous().view", "copy_summ.CopyLSTMDecoder._lstm", "torch.matmul", "attention.step_attention", "copy_summ.CopyLSTMDecoder._projection", "copy_summ.CopyLSTMDecoder._compute_gen_prob", "torch.sigmoid().contiguous().view", "torch.log().contiguous().view", "torch.log().contiguous().view.topk", "h.contiguous().view", "c.contiguous().view", "h.contiguous().view", "c.contiguous().view", "torch.cat", "copy_summ.CopyLSTMDecoder.contiguous().view", "copy_summ.CopyLSTMDecoder._embedding", "torch.cat.contiguous", "torch.sigmoid().contiguous", "torch.log().contiguous", "h.contiguous", "c.contiguous", "h.contiguous", "c.contiguous", "copy_summ.CopyLSTMDecoder.contiguous", "torch.sigmoid", "torch.log", "copy_summ.CopyLSTMDecoder._copy", "extend_src.expand_as().contiguous().view", "score.contiguous().view", "extend_src.expand_as().contiguous", "score.contiguous", "extend_src.expand_as"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.attention.step_attention", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.log"], ["", "def", "topk_step", "(", "self", ",", "tok", ",", "states", ",", "attention", ",", "k", ")", ":", "\n", "        ", "\"\"\"tok:[BB, B], states ([L, BB, B, D]*2, [BB, B, D])\"\"\"", "\n", "(", "h", ",", "c", ")", ",", "prev_out", "=", "states", "\n", "\n", "# lstm is not bemable", "\n", "nl", ",", "_", ",", "_", ",", "d", "=", "h", ".", "size", "(", ")", "\n", "beam", ",", "batch", "=", "tok", ".", "size", "(", ")", "\n", "lstm_in_beamable", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_embedding", "(", "tok", ")", ",", "prev_out", "]", ",", "dim", "=", "-", "1", ")", "\n", "lstm_in", "=", "lstm_in_beamable", ".", "contiguous", "(", ")", ".", "view", "(", "beam", "*", "batch", ",", "-", "1", ")", "\n", "prev_states", "=", "(", "h", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "-", "1", ",", "d", ")", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "-", "1", ",", "d", ")", ")", "\n", "h", ",", "c", "=", "self", ".", "_lstm", "(", "lstm_in", ",", "prev_states", ")", "\n", "states", "=", "(", "h", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "beam", ",", "batch", ",", "-", "1", ")", ",", "\n", "c", ".", "contiguous", "(", ")", ".", "view", "(", "nl", ",", "beam", ",", "batch", ",", "-", "1", ")", ")", "\n", "lstm_out", "=", "states", "[", "0", "]", "[", "-", "1", "]", "\n", "\n", "# attention is beamable", "\n", "query", "=", "torch", ".", "matmul", "(", "lstm_out", ",", "self", ".", "_attn_w", ")", "\n", "attention", ",", "attn_mask", ",", "extend_src", ",", "extend_vsize", "=", "attention", "\n", "context", ",", "score", "=", "step_attention", "(", "\n", "query", ",", "attention", ",", "attention", ",", "attn_mask", ")", "\n", "dec_out", "=", "self", ".", "_projection", "(", "torch", ".", "cat", "(", "[", "lstm_out", ",", "context", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "# copy mechanism is not beamable", "\n", "gen_prob", "=", "self", ".", "_compute_gen_prob", "(", "\n", "dec_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", "*", "beam", ",", "-", "1", ")", ",", "extend_vsize", ")", "\n", "copy_prob", "=", "torch", ".", "sigmoid", "(", "\n", "self", ".", "_copy", "(", "context", ",", "lstm_out", ",", "lstm_in_beamable", ")", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "lp", "=", "torch", ".", "log", "(", "\n", "(", "(", "-", "copy_prob", "+", "1", ")", "*", "gen_prob", "\n", ")", ".", "scatter_add", "(", "\n", "dim", "=", "1", ",", "\n", "index", "=", "extend_src", ".", "expand_as", "(", "score", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "beam", "*", "batch", ",", "-", "1", ")", ",", "\n", "src", "=", "score", ".", "contiguous", "(", ")", ".", "view", "(", "beam", "*", "batch", ",", "-", "1", ")", "*", "copy_prob", "\n", ")", "+", "1e-8", ")", ".", "contiguous", "(", ")", ".", "view", "(", "beam", ",", "batch", ",", "-", "1", ")", "\n", "\n", "k_lp", ",", "k_tok", "=", "lp", ".", "topk", "(", "k", "=", "k", ",", "dim", "=", "-", "1", ")", "\n", "return", "k_tok", ",", "k_lp", ",", "(", "states", ",", "dec_out", ")", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_gen_prob": [[251, 263], ["torch.mm", "torch.mm.size", "torch.nn.functional.softmax", "copy_summ.CopyLSTMDecoder._embedding.weight.t", "torch.Tensor().to", "torch.Tensor().to.fill_", "torch.cat", "torch.Tensor"], "methods", ["None"], ["", "def", "_compute_gen_prob", "(", "self", ",", "dec_out", ",", "extend_vsize", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "logit", "=", "torch", ".", "mm", "(", "dec_out", ",", "self", ".", "_embedding", ".", "weight", ".", "t", "(", ")", ")", "\n", "bsize", ",", "vsize", "=", "logit", ".", "size", "(", ")", "\n", "if", "extend_vsize", ">", "vsize", ":", "\n", "            ", "ext_logit", "=", "torch", ".", "Tensor", "(", "bsize", ",", "extend_vsize", "-", "vsize", "\n", ")", ".", "to", "(", "logit", ".", "device", ")", "\n", "ext_logit", ".", "fill_", "(", "eps", ")", "\n", "gen_logit", "=", "torch", ".", "cat", "(", "[", "logit", ",", "ext_logit", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "gen_logit", "=", "logit", "\n", "", "gen_prob", "=", "F", ".", "softmax", "(", "gen_logit", ",", "dim", "=", "-", "1", ")", "\n", "return", "gen_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.model.copy_summ.CopyLSTMDecoder._compute_copy_activation": [[264, 267], ["copy_summ.CopyLSTMDecoder._copy"], "methods", ["None"], ["", "def", "_compute_copy_activation", "(", "self", ",", "context", ",", "state", ",", "input_", ",", "score", ")", ":", "\n", "        ", "copy", "=", "self", ".", "_copy", "(", "context", ",", "state", ",", "input_", ")", "*", "score", "\n", "return", "copy", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.LoggingHandler.LoggingHandler.__init__": [[5, 7], ["logging.Handler.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "level", "=", "logging", ".", "NOTSET", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "level", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.LoggingHandler.LoggingHandler.emit": [[8, 17], ["LoggingHandler.LoggingHandler.format", "tqdm.tqdm.write", "LoggingHandler.LoggingHandler.flush", "LoggingHandler.LoggingHandler.handleError"], "methods", ["None"], ["", "def", "emit", "(", "self", ",", "record", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "msg", "=", "self", ".", "format", "(", "record", ")", "\n", "tqdm", ".", "tqdm", ".", "write", "(", "msg", ")", "\n", "self", ".", "flush", "(", ")", "\n", "", "except", "(", "KeyboardInterrupt", ",", "SystemExit", ")", ":", "\n", "            ", "raise", "\n", "", "except", ":", "\n", "            ", "self", ".", "handleError", "(", "record", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.__init__": [[29, 96], ["torch.nn.Sequential.__init__", "torch.device", "torch.device", "torch.device", "torch.device", "collections.OrderedDict", "logging.info", "logging.info", "isinstance", "logging.info", "model_name_or_path.startswith", "model_name_or_path.startswith", "os.path.join", "os.path.join", "os.makedirs", "logging.info", "os.path.exists", "collections.OrderedDict", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "os.path.isdir", "model_url.replace().replace().replace", "_get_torch_home", "os.listdir", "logging.info", "os.path.join", "open", "json.load", "util.import_from_string", "util.import_from_string.load", "str", "enumerate", "os.path.expanduser", "os.path.join", "util.http_get", "open", "json.load", "os.path.join", "os.path.join", "model_url.replace().replace", "os.getenv", "zipfile.ZipFile", "zip.extractall", "shutil.rmtree", "os.path.join", "logging.warning", "os.path.join", "model_url.replace", "os.getenv"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.import_from_string", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.http_get", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["    ", "def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", "=", "None", ",", "modules", ":", "Iterable", "[", "nn", ".", "Module", "]", "=", "None", ",", "device", ":", "str", "=", "None", ")", ":", "\n", "        ", "if", "modules", "is", "not", "None", "and", "not", "isinstance", "(", "modules", ",", "OrderedDict", ")", ":", "\n", "            ", "modules", "=", "OrderedDict", "(", "[", "(", "str", "(", "idx", ")", ",", "module", ")", "for", "idx", ",", "module", "in", "enumerate", "(", "modules", ")", "]", ")", "\n", "\n", "", "if", "model_name_or_path", "is", "not", "None", "and", "model_name_or_path", "!=", "\"\"", ":", "\n", "            ", "logging", ".", "info", "(", "\"Load pretrained SentenceTransformer: {}\"", ".", "format", "(", "model_name_or_path", ")", ")", "\n", "\n", "if", "'/'", "not", "in", "model_name_or_path", "and", "'\\\\'", "not", "in", "model_name_or_path", "and", "not", "os", ".", "path", ".", "isdir", "(", "model_name_or_path", ")", ":", "\n", "                ", "logging", ".", "info", "(", "\"Did not find a '/' or '\\\\' in the name. Assume to download model from server.\"", ")", "\n", "model_name_or_path", "=", "__DOWNLOAD_SERVER__", "+", "model_name_or_path", "+", "'.zip'", "\n", "\n", "", "if", "model_name_or_path", ".", "startswith", "(", "'http://'", ")", "or", "model_name_or_path", ".", "startswith", "(", "'https://'", ")", ":", "\n", "                ", "model_url", "=", "model_name_or_path", "\n", "folder_name", "=", "model_url", ".", "replace", "(", "\"https://\"", ",", "\"\"", ")", ".", "replace", "(", "\"http://\"", ",", "\"\"", ")", ".", "replace", "(", "\"/\"", ",", "\"_\"", ")", "[", ":", "250", "]", "\n", "\n", "try", ":", "\n", "                    ", "from", "torch", ".", "hub", "import", "_get_torch_home", "\n", "torch_cache_home", "=", "_get_torch_home", "(", ")", "\n", "", "except", "ImportError", ":", "\n", "                    ", "torch_cache_home", "=", "os", ".", "path", ".", "expanduser", "(", "\n", "os", ".", "getenv", "(", "'TORCH_HOME'", ",", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "getenv", "(", "'XDG_CACHE_HOME'", ",", "'~/.cache'", ")", ",", "'torch'", ")", ")", ")", "\n", "", "default_cache_path", "=", "os", ".", "path", ".", "join", "(", "torch_cache_home", ",", "'sentence_transformers'", ")", "\n", "model_path", "=", "os", ".", "path", ".", "join", "(", "default_cache_path", ",", "folder_name", ")", "\n", "os", ".", "makedirs", "(", "model_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "not", "os", ".", "listdir", "(", "model_path", ")", ":", "\n", "                    ", "if", "model_url", "[", "-", "1", "]", "==", "\"/\"", ":", "\n", "                        ", "model_url", "=", "model_url", "[", ":", "-", "1", "]", "\n", "", "logging", ".", "info", "(", "\"Downloading sentence transformer model from {} and saving it at {}\"", ".", "format", "(", "model_url", ",", "model_path", ")", ")", "\n", "try", ":", "\n", "                        ", "zip_save_path", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "'model.zip'", ")", "\n", "http_get", "(", "model_url", ",", "zip_save_path", ")", "\n", "with", "ZipFile", "(", "zip_save_path", ",", "'r'", ")", "as", "zip", ":", "\n", "                            ", "zip", ".", "extractall", "(", "model_path", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                        ", "shutil", ".", "rmtree", "(", "model_path", ")", "\n", "raise", "e", "\n", "", "", "", "else", ":", "\n", "                ", "model_path", "=", "model_name_or_path", "\n", "\n", "#### Load from disk", "\n", "", "if", "model_path", "is", "not", "None", ":", "\n", "                ", "logging", ".", "info", "(", "\"Load SentenceTransformer from folder: {}\"", ".", "format", "(", "model_path", ")", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "'config.json'", ")", ")", ":", "\n", "                    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "                        ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "if", "config", "[", "'__version__'", "]", ">", "__version__", ":", "\n", "                            ", "logging", ".", "warning", "(", "\"You try to use a model that was created with version {}, however, your version is {}. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\\n\\n\\n\"", ".", "format", "(", "config", "[", "'__version__'", "]", ",", "__version__", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "'modules.json'", ")", ")", "as", "fIn", ":", "\n", "                    ", "contained_modules", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "modules", "=", "OrderedDict", "(", ")", "\n", "for", "module_config", "in", "contained_modules", ":", "\n", "                    ", "module_class", "=", "import_from_string", "(", "module_config", "[", "'type'", "]", ")", "\n", "module", "=", "module_class", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "module_config", "[", "'path'", "]", ")", ")", "\n", "modules", "[", "module_config", "[", "'name'", "]", "]", "=", "module", "\n", "\n", "\n", "", "", "", "super", "(", ")", ".", "__init__", "(", "modules", ")", "\n", "if", "device", "is", "None", ":", "\n", "            ", "device", "=", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", "\n", "logging", ".", "info", "(", "\"Use pytorch device: {}\"", ".", "format", "(", "device", ")", ")", "\n", "\n", "", "self", ".", "_target_device", "=", "torch", ".", "device", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode": [[98, 175], ["SentenceTransformer.SentenceTransformer.eval", "isinstance", "SentenceTransformer.SentenceTransformer.to", "numpy.argsort", "datasets.EncodeDataset.EncodeDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.autonotebook.tqdm.autonotebook.tqdm", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "len", "features[].to", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "SentenceTransformer.SentenceTransformer.forward", "numpy.asarray.extend", "numpy.argsort", "numpy.asarray", "logging.getLogger().getEffectiveLevel", "logging.getLogger().getEffectiveLevel", "input_mask.unsqueeze().expand().float", "emb.cpu().detach().numpy", "logging.getLogger", "logging.getLogger", "input_mask.unsqueeze().expand", "embeddings.size", "emb.cpu().detach", "input_mask.unsqueeze", "emb.cpu"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.forward"], ["", "def", "encode", "(", "self", ",", "sentences", ":", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "show_progress_bar", ":", "bool", "=", "None", ",", "\n", "output_value", ":", "str", "=", "'sentence_embedding'", ",", "\n", "convert_to_numpy", ":", "bool", "=", "True", ",", "\n", "convert_to_tensor", ":", "bool", "=", "False", ",", "\n", "is_pretokenized", ":", "bool", "=", "False", ",", "\n", "device", ":", "str", "=", "None", ",", "\n", "num_workers", ":", "int", "=", "0", ")", "->", "Union", "[", "List", "[", "Tensor", "]", ",", "ndarray", ",", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes sentence embeddings\n\n\n        :param sentences: the sentences to embed\n        :param batch_size: the batch size used for the computation\n        :param show_progress_bar: Output a progress bar when encode sentences\n        :param output_value:  Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings.\n        :param convert_to_numpy: If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.\n        :param convert_to_tensor: If true, you get one large tensor as return. Overwrites any setting from conver_to_numy\n        :param is_pretokenized: If is_pretokenized=True, sentences must be a list of integers, containing the tokenized sentences with each token convert to the respective int.\n        :param device: Which torch.device to use for the computation\n        :param num_workers: Number of background-workers to tokenize data. Set to positive number to increase tokenization speed\n        :return:\n           By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\n        \"\"\"", "\n", "self", ".", "eval", "(", ")", "\n", "if", "show_progress_bar", "is", "None", ":", "\n", "            ", "show_progress_bar", "=", "(", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "INFO", "or", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "DEBUG", ")", "\n", "\n", "", "input_was_string", "=", "False", "\n", "if", "isinstance", "(", "sentences", ",", "str", ")", ":", "#Cast an individual sentence to a list with length 1", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "input_was_string", "=", "True", "\n", "\n", "", "if", "device", "is", "None", ":", "\n", "            ", "device", "=", "self", ".", "_target_device", "\n", "\n", "", "self", ".", "to", "(", "device", ")", "\n", "\n", "all_embeddings", "=", "[", "]", "\n", "length_sorted_idx", "=", "np", ".", "argsort", "(", "[", "len", "(", "sen", ")", "for", "sen", "in", "sentences", "]", ")", "\n", "sentences_sorted", "=", "[", "sentences", "[", "idx", "]", "for", "idx", "in", "length_sorted_idx", "]", "\n", "inp_dataset", "=", "EncodeDataset", "(", "sentences_sorted", ",", "model", "=", "self", ",", "is_tokenized", "=", "is_pretokenized", ")", "\n", "inp_dataloader", "=", "DataLoader", "(", "inp_dataset", ",", "batch_size", "=", "batch_size", ",", "collate_fn", "=", "self", ".", "smart_batching_collate_text_only", ",", "num_workers", "=", "num_workers", ",", "shuffle", "=", "False", ")", "\n", "\n", "iterator", "=", "inp_dataloader", "\n", "if", "show_progress_bar", ":", "\n", "            ", "iterator", "=", "tqdm", "(", "inp_dataloader", ",", "desc", "=", "\"Batches\"", ")", "\n", "\n", "", "for", "features", "in", "iterator", ":", "\n", "            ", "for", "feature_name", "in", "features", ":", "\n", "                ", "features", "[", "feature_name", "]", "=", "features", "[", "feature_name", "]", ".", "to", "(", "device", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "out_features", "=", "self", ".", "forward", "(", "features", ")", "\n", "embeddings", "=", "out_features", "[", "output_value", "]", "\n", "\n", "if", "output_value", "==", "'token_embeddings'", ":", "\n", "#Set token embeddings to 0 for padding tokens", "\n", "                    ", "input_mask", "=", "out_features", "[", "'attention_mask'", "]", "\n", "input_mask_expanded", "=", "input_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "embeddings", ".", "size", "(", ")", ")", ".", "float", "(", ")", "\n", "embeddings", "=", "embeddings", "*", "input_mask_expanded", "\n", "\n", "", "all_embeddings", ".", "extend", "(", "embeddings", ")", "\n", "\n", "\n", "", "", "all_embeddings", "=", "[", "all_embeddings", "[", "idx", "]", "for", "idx", "in", "np", ".", "argsort", "(", "length_sorted_idx", ")", "]", "\n", "\n", "if", "convert_to_tensor", ":", "\n", "            ", "all_embeddings", "=", "torch", ".", "stack", "(", "all_embeddings", ")", "\n", "", "elif", "convert_to_numpy", ":", "\n", "            ", "all_embeddings", "=", "np", ".", "asarray", "(", "[", "emb", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "for", "emb", "in", "all_embeddings", "]", ")", "\n", "\n", "", "if", "input_was_string", ":", "\n", "            ", "all_embeddings", "=", "all_embeddings", "[", "0", "]", "\n", "\n", "", "return", "all_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.start_multi_process_pool": [[178, 208], ["logging.info", "torch.get_context", "torch.get_context", "torch.get_context.Queue", "torch.get_context.Queue", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.get_context.Process", "mp.get_context.Process.start", "processes.append", "logging.info", "map", "range", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count"], "methods", ["None"], ["", "def", "start_multi_process_pool", "(", "self", ",", "target_devices", ":", "List", "[", "str", "]", "=", "None", ",", "encode_batch_size", ":", "int", "=", "32", ")", ":", "\n", "        ", "\"\"\"\n        Starts multi process to process the encode with several, independent  process.\n        This methos is recommend if you want to encode on multiple GPUs. It is advised\n        to start only one process per GPU. This method works together with encode_multi_process\n\n        :param target_devices: PyTorch target devices, e.g. cuda:0, cuda:1... If None, all available CUDA devices will be used\n        :param encode_batch_size: Batch size for each process when calling encode\n        :return: Returns a dict with the target processes, an input queue and and output queue.\n        \"\"\"", "\n", "if", "target_devices", "is", "None", ":", "\n", "            ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "target_devices", "=", "[", "'cuda:{}'", ".", "format", "(", "i", ")", "for", "i", "in", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "logging", ".", "info", "(", "\"CUDA is not available. Start 4 CPU worker\"", ")", "\n", "target_devices", "=", "[", "'cpu'", "]", "*", "4", "\n", "\n", "", "", "logging", ".", "info", "(", "\"Start multi-process pool on devices: {}\"", ".", "format", "(", "', '", ".", "join", "(", "map", "(", "str", ",", "target_devices", ")", ")", ")", ")", "\n", "\n", "ctx", "=", "mp", ".", "get_context", "(", "'spawn'", ")", "\n", "input_queue", "=", "ctx", ".", "Queue", "(", ")", "\n", "output_queue", "=", "ctx", ".", "Queue", "(", ")", "\n", "processes", "=", "[", "]", "\n", "\n", "for", "cuda_id", "in", "target_devices", ":", "\n", "            ", "p", "=", "ctx", ".", "Process", "(", "target", "=", "SentenceTransformer", ".", "_encode_multi_process_worker", ",", "args", "=", "(", "cuda_id", ",", "self", ",", "input_queue", ",", "output_queue", ",", "encode_batch_size", ")", ",", "daemon", "=", "True", ")", "\n", "p", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "p", ")", "\n", "\n", "", "return", "{", "'input'", ":", "input_queue", ",", "'output'", ":", "output_queue", ",", "'processes'", ":", "processes", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.stop_multi_process_pool": [[210, 224], ["pool[].close", "pool[].close", "p.terminate", "p.join", "p.close"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.terminate"], ["", "@", "staticmethod", "\n", "def", "stop_multi_process_pool", "(", "pool", ")", ":", "\n", "        ", "\"\"\"\n        Stops all processes started with start_multi_process_pool\n        \"\"\"", "\n", "for", "p", "in", "pool", "[", "'processes'", "]", ":", "\n", "            ", "p", ".", "terminate", "(", ")", "\n", "\n", "", "for", "p", "in", "pool", "[", "'processes'", "]", ":", "\n", "            ", "p", ".", "join", "(", ")", "\n", "p", ".", "close", "(", ")", "\n", "\n", "", "pool", "[", "'input'", "]", ".", "close", "(", ")", "\n", "pool", "[", "'output'", "]", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode_multi_process": [[226, 265], ["min", "logging.info", "sorted", "numpy.concatenate", "math.ceil", "map", "chunk.append", "len", "input_queue.put", "len", "input_queue.put", "output_queue.get", "range", "len", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count"], "methods", ["None"], ["", "def", "encode_multi_process", "(", "self", ",", "sentences", ":", "List", "[", "str", "]", ",", "pool", ":", "Dict", "[", "str", ",", "object", "]", ",", "is_pretokenized", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        This method allows to run encode() on multiple GPUs. The sentences are chunked into smaller packages\n        and sent to individual processes, which encode these on the different GPUs. This method is only suitable\n        for encoding large sets of sentences\n\n        :param sentences: List of sentences\n        :param pool: A pool of workers started with SentenceTransformer.start_multi_process_pool\n        :param is_pretokenized: If true, no tokenization will be applied. It is expected that the input sentences are list of ints.\n        :return: Numpy matrix with all embeddings\n        \"\"\"", "\n", "\n", "chunk_size", "=", "min", "(", "math", ".", "ceil", "(", "len", "(", "sentences", ")", "/", "torch", ".", "cuda", ".", "device_count", "(", ")", "/", "10", ")", ",", "5000", ")", "\n", "logging", ".", "info", "(", "\"Chunk data into packages of size {}\"", ".", "format", "(", "chunk_size", ")", ")", "\n", "\n", "if", "is_pretokenized", ":", "\n", "            ", "sentences_tokenized", "=", "sentences", "\n", "", "else", ":", "\n", "            ", "sentences_tokenized", "=", "map", "(", "self", ".", "tokenize", ",", "sentences", ")", "\n", "\n", "", "input_queue", "=", "pool", "[", "'input'", "]", "\n", "num_chunks", "=", "0", "\n", "chunk", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "sentences_tokenized", ":", "\n", "            ", "chunk", ".", "append", "(", "sentence", ")", "\n", "if", "len", "(", "chunk", ")", ">=", "chunk_size", ":", "\n", "                ", "input_queue", ".", "put", "(", "[", "num_chunks", ",", "chunk", "]", ")", "\n", "num_chunks", "+=", "1", "\n", "chunk", "=", "[", "]", "\n", "\n", "", "", "if", "len", "(", "chunk", ")", ">=", "chunk_size", ":", "\n", "            ", "input_queue", ".", "put", "(", "[", "num_chunks", ",", "chunk", "]", ")", "\n", "num_chunks", "+=", "1", "\n", "\n", "", "output_queue", "=", "pool", "[", "'output'", "]", "\n", "results_list", "=", "sorted", "(", "[", "output_queue", ".", "get", "(", ")", "for", "_", "in", "range", "(", "num_chunks", ")", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "embeddings", "=", "np", ".", "concatenate", "(", "[", "result", "[", "1", "]", "for", "result", "in", "results_list", "]", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._encode_multi_process_worker": [[266, 278], ["input_queue.get", "model.encode", "results_queue.put"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "@", "staticmethod", "\n", "def", "_encode_multi_process_worker", "(", "target_device", ":", "str", ",", "model", ",", "input_queue", ",", "results_queue", ",", "encode_batch_size", ")", ":", "\n", "        ", "\"\"\"\n        Internal working process to encode sentences in multi-process setup\n        \"\"\"", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "id", ",", "sentences", "=", "input_queue", ".", "get", "(", ")", "\n", "embeddings", "=", "model", ".", "encode", "(", "sentences", ",", "device", "=", "target_device", ",", "is_pretokenized", "=", "True", ",", "show_progress_bar", "=", "False", ",", "convert_to_numpy", "=", "True", ",", "batch_size", "=", "encode_batch_size", ")", "\n", "results_queue", ".", "put", "(", "[", "id", ",", "embeddings", "]", ")", "\n", "", "except", "queue", ".", "Empty", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.get_max_seq_length": [[280, 288], ["hasattr", "SentenceTransformer.SentenceTransformer._first_module", "SentenceTransformer.SentenceTransformer._first_module"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module"], ["", "", "", "def", "get_max_seq_length", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the maximal sequence length for input the model accepts. Longer inputs will be truncated\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ".", "_first_module", "(", ")", ",", "'max_seq_length'", ")", ":", "\n", "            ", "return", "self", ".", "_first_module", "(", ")", ".", "max_seq_length", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.tokenize": [[289, 294], ["SentenceTransformer.SentenceTransformer._first_module().tokenize", "SentenceTransformer.SentenceTransformer._first_module"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Tokenizes the text\n        \"\"\"", "\n", "return", "self", ".", "_first_module", "(", ")", ".", "tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.get_sentence_features": [[295, 297], ["SentenceTransformer.SentenceTransformer._first_module().get_sentence_features", "SentenceTransformer.SentenceTransformer._first_module"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_features", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module"], ["", "def", "get_sentence_features", "(", "self", ",", "*", "features", ")", ":", "\n", "        ", "return", "self", ".", "_first_module", "(", ")", ".", "get_sentence_features", "(", "*", "features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.get_sentence_embedding_dimension": [[298, 300], ["SentenceTransformer.SentenceTransformer._last_module().get_sentence_embedding_dimension", "SentenceTransformer.SentenceTransformer._last_module"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_embedding_dimension", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._last_module"], ["", "def", "get_sentence_embedding_dimension", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_last_module", "(", ")", ".", "get_sentence_embedding_dimension", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module": [[301, 304], ["next", "iter"], "methods", ["None"], ["", "def", "_first_module", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the first module of this sequential embedder\"\"\"", "\n", "return", "self", ".", "_modules", "[", "next", "(", "iter", "(", "self", ".", "_modules", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._last_module": [[305, 308], ["next", "reversed"], "methods", ["None"], ["", "def", "_last_module", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the last module of this sequential embedder\"\"\"", "\n", "return", "self", ".", "_modules", "[", "next", "(", "reversed", "(", "self", ".", "_modules", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.save": [[309, 331], ["logging.info", "enumerate", "os.path.join", "os.makedirs", "module.save", "contained_modules.append", "open", "json.dump", "open", "json.dump", "os.path.join", "os.path.join", "os.path.basename", "str", "type", "type"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "save", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        Saves all elements for this seq. sentence embedder into different sub-folders\n        \"\"\"", "\n", "if", "path", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "logging", ".", "info", "(", "\"Save model to {}\"", ".", "format", "(", "path", ")", ")", "\n", "contained_modules", "=", "[", "]", "\n", "\n", "for", "idx", ",", "name", "in", "enumerate", "(", "self", ".", "_modules", ")", ":", "\n", "            ", "module", "=", "self", ".", "_modules", "[", "name", "]", "\n", "model_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "str", "(", "idx", ")", "+", "\"_\"", "+", "type", "(", "module", ")", ".", "__name__", ")", "\n", "os", ".", "makedirs", "(", "model_path", ",", "exist_ok", "=", "True", ")", "\n", "module", ".", "save", "(", "model_path", ")", "\n", "contained_modules", ".", "append", "(", "{", "'idx'", ":", "idx", ",", "'name'", ":", "name", ",", "'path'", ":", "os", ".", "path", ".", "basename", "(", "model_path", ")", ",", "'type'", ":", "type", "(", "module", ")", ".", "__module__", "}", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'modules.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "contained_modules", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'__version__'", ":", "__version__", "}", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.smart_batching_collate": [[332, 374], ["len", "range", "labels.append", "range", "features.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "range", "paired_texts[].append", "max", "SentenceTransformer.SentenceTransformer.get_sentence_features", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "feature_lists[].append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_features"], ["", "", "def", "smart_batching_collate", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"\n        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n\n        :param batch:\n            a batch from a SmartBatchingDataset\n        :return:\n            a batch of tensors for the model\n        \"\"\"", "\n", "num_texts", "=", "len", "(", "batch", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "labels", "=", "[", "]", "\n", "paired_texts", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_texts", ")", "]", "\n", "max_seq_len", "=", "[", "0", "]", "*", "num_texts", "\n", "for", "tokens", ",", "label", "in", "batch", ":", "\n", "            ", "labels", ".", "append", "(", "label", ")", "\n", "for", "i", "in", "range", "(", "num_texts", ")", ":", "\n", "                ", "paired_texts", "[", "i", "]", ".", "append", "(", "tokens", "[", "i", "]", ")", "\n", "max_seq_len", "[", "i", "]", "=", "max", "(", "max_seq_len", "[", "i", "]", ",", "len", "(", "tokens", "[", "i", "]", ")", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "num_texts", ")", ":", "\n", "            ", "max_len", "=", "max_seq_len", "[", "idx", "]", "\n", "feature_lists", "=", "{", "}", "\n", "\n", "for", "text", "in", "paired_texts", "[", "idx", "]", ":", "\n", "                ", "sentence_features", "=", "self", ".", "get_sentence_features", "(", "text", ",", "max_len", ")", "\n", "\n", "for", "feature_name", "in", "sentence_features", ":", "\n", "                    ", "if", "feature_name", "not", "in", "feature_lists", ":", "\n", "                        ", "feature_lists", "[", "feature_name", "]", "=", "[", "]", "\n", "\n", "", "feature_lists", "[", "feature_name", "]", ".", "append", "(", "sentence_features", "[", "feature_name", "]", ")", "\n", "\n", "\n", "", "", "for", "feature_name", "in", "feature_lists", ":", "\n", "#feature_lists[feature_name] = torch.tensor(np.asarray(feature_lists[feature_name]))", "\n", "                ", "feature_lists", "[", "feature_name", "]", "=", "torch", ".", "cat", "(", "feature_lists", "[", "feature_name", "]", ")", "\n", "\n", "", "features", ".", "append", "(", "feature_lists", ")", "\n", "\n", "", "return", "{", "'features'", ":", "features", ",", "'labels'", ":", "torch", ".", "stack", "(", "labels", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.smart_batching_collate_text_only": [[376, 401], ["max", "SentenceTransformer.SentenceTransformer.get_sentence_features", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "feature_lists[].append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_features"], ["", "def", "smart_batching_collate_text_only", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"\n        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n\n        :param batch:\n            a batch from a SmartBatchingDataset\n        :return:\n            a batch of tensors for the model\n        \"\"\"", "\n", "\n", "max_seq_len", "=", "max", "(", "[", "len", "(", "text", ")", "for", "text", "in", "batch", "]", ")", "\n", "feature_lists", "=", "{", "}", "\n", "\n", "for", "text", "in", "batch", ":", "\n", "            ", "sentence_features", "=", "self", ".", "get_sentence_features", "(", "text", ",", "max_seq_len", ")", "\n", "for", "feature_name", "in", "sentence_features", ":", "\n", "                ", "if", "feature_name", "not", "in", "feature_lists", ":", "\n", "                    ", "feature_lists", "[", "feature_name", "]", "=", "[", "]", "\n", "\n", "", "feature_lists", "[", "feature_name", "]", ".", "append", "(", "sentence_features", "[", "feature_name", "]", ")", "\n", "\n", "", "", "for", "feature_name", "in", "feature_lists", ":", "\n", "            ", "feature_lists", "[", "feature_name", "]", "=", "torch", ".", "cat", "(", "feature_lists", "[", "feature_name", "]", ")", "\n", "\n", "", "return", "feature_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.fit": [[404, 556], ["SentenceTransformer.SentenceTransformer.to", "int", "len", "tqdm.autonotebook.tqdm.autonotebook.trange", "os.makedirs", "loss_model.to", "min", "list", "optimizer_class", "SentenceTransformer.SentenceTransformer._get_scheduler", "optimizers.append", "schedulers.append", "range", "iter", "tqdm.autonotebook.tqdm.autonotebook.trange", "SentenceTransformer.SentenceTransformer._eval_during_training", "ValueError", "loss_model.named_parameters", "len", "amp.initialize", "loss_model.zero_grad", "loss_model.train", "range", "len", "len", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "ImportError", "util.batch_to_device", "loss_model", "optimizer_class.step", "scheduler.step", "optimizer_class.zero_grad", "SentenceTransformer.SentenceTransformer._eval_during_training", "os.listdir", "next", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "loss_model.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "loss_model.zero_grad", "loss_model.train", "any", "iter", "next", "amp.scale_loss", "scaled_loss.backward", "amp.master_params", "loss_model.parameters", "any"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._get_scheduler", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._eval_during_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.batch_to_device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._eval_during_training", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.training.BasicTrainer.train"], ["", "def", "fit", "(", "self", ",", "\n", "train_objectives", ":", "Iterable", "[", "Tuple", "[", "DataLoader", ",", "nn", ".", "Module", "]", "]", ",", "\n", "evaluator", ":", "SentenceEvaluator", ",", "\n", "epochs", ":", "int", "=", "1", ",", "\n", "steps_per_epoch", "=", "None", ",", "\n", "scheduler", ":", "str", "=", "'WarmupLinear'", ",", "\n", "warmup_steps", ":", "int", "=", "10000", ",", "\n", "optimizer_class", ":", "Type", "[", "Optimizer", "]", "=", "transformers", ".", "AdamW", ",", "\n", "optimizer_params", ":", "Dict", "[", "str", ",", "object", "]", "=", "{", "'lr'", ":", "2e-5", ",", "'eps'", ":", "1e-6", ",", "'correct_bias'", ":", "False", "}", ",", "\n", "weight_decay", ":", "float", "=", "0.01", ",", "\n", "evaluation_steps", ":", "int", "=", "0", ",", "\n", "output_path", ":", "str", "=", "None", ",", "\n", "output_path_ignore_not_empty", ":", "bool", "=", "False", ",", "\n", "save_best_model", ":", "bool", "=", "True", ",", "\n", "max_grad_norm", ":", "float", "=", "1", ",", "\n", "fp16", ":", "bool", "=", "False", ",", "\n", "fp16_opt_level", ":", "str", "=", "'O1'", ",", "\n", "local_rank", ":", "int", "=", "-", "1", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Train the model with the given training objective\n\n        Each training objective is sampled in turn for one batch.\n        We sample only as many batches from each objective as there are in the smallest one\n        to make sure of equal training with each dataset.\n\n        :param train_objectives: Tuples of (DataLoader, LossFunction). Pass more than one for multi-task learning\n        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n        :param epochs: Number of epochs for training\n        :param steps_per_epoch: Number of training steps per epoch. If set to None (default), one epoch is equal the DataLoader size from train_objectives.\n        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n        :param optimizer_class: Optimizer\n        :param optimizer_params: Optimizer parameters\n        :param weight_decay: Weight decay for model parameters\n        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n        :param output_path: Storage path for the model and evaluation files\n        :param output_path_ignore_not_empty: By default, training will stop if output_path is not empty. If set to true, this error will be ignored and training proceeds.\n        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n        :param max_grad_norm: Used for gradient normalization.\n        \"\"\"", "\n", "self", ".", "to", "(", "self", ".", "_target_device", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_path", ",", "exist_ok", "=", "True", ")", "\n", "if", "not", "output_path_ignore_not_empty", "and", "len", "(", "os", ".", "listdir", "(", "output_path", ")", ")", ">", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "\n", "output_path", ")", ")", "\n", "\n", "", "", "dataloaders", "=", "[", "dataloader", "for", "dataloader", ",", "_", "in", "train_objectives", "]", "\n", "\n", "# Use smart batching", "\n", "for", "dataloader", "in", "dataloaders", ":", "\n", "            ", "dataloader", ".", "collate_fn", "=", "self", ".", "smart_batching_collate", "\n", "\n", "", "loss_models", "=", "[", "loss", "for", "_", ",", "loss", "in", "train_objectives", "]", "\n", "device", "=", "self", ".", "_target_device", "\n", "\n", "for", "loss_model", "in", "loss_models", ":", "\n", "            ", "loss_model", ".", "to", "(", "device", ")", "\n", "\n", "", "self", ".", "best_score", "=", "-", "9999999", "\n", "\n", "if", "steps_per_epoch", "is", "None", "or", "steps_per_epoch", "==", "0", ":", "\n", "            ", "steps_per_epoch", "=", "min", "(", "[", "len", "(", "dataloader", ")", "for", "dataloader", "in", "dataloaders", "]", ")", "\n", "\n", "", "num_train_steps", "=", "int", "(", "steps_per_epoch", "*", "epochs", ")", "\n", "\n", "# Prepare optimizers", "\n", "optimizers", "=", "[", "]", "\n", "schedulers", "=", "[", "]", "\n", "for", "loss_model", "in", "loss_models", ":", "\n", "            ", "param_optimizer", "=", "list", "(", "loss_model", ".", "named_parameters", "(", ")", ")", "\n", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "weight_decay", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "t_total", "=", "num_train_steps", "\n", "if", "local_rank", "!=", "-", "1", ":", "\n", "                ", "t_total", "=", "t_total", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "optimizer", "=", "optimizer_class", "(", "optimizer_grouped_parameters", ",", "**", "optimizer_params", ")", "\n", "scheduler_obj", "=", "self", ".", "_get_scheduler", "(", "optimizer", ",", "scheduler", "=", "scheduler", ",", "warmup_steps", "=", "warmup_steps", ",", "t_total", "=", "t_total", ")", "\n", "\n", "optimizers", ".", "append", "(", "optimizer", ")", "\n", "schedulers", ".", "append", "(", "scheduler_obj", ")", "\n", "\n", "", "if", "fp16", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "                ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "\n", "", "for", "train_idx", "in", "range", "(", "len", "(", "loss_models", ")", ")", ":", "\n", "                ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "loss_models", "[", "train_idx", "]", ",", "optimizers", "[", "train_idx", "]", ",", "opt_level", "=", "fp16_opt_level", ")", "\n", "loss_models", "[", "train_idx", "]", "=", "model", "\n", "optimizers", "[", "train_idx", "]", "=", "optimizer", "\n", "\n", "", "", "global_step", "=", "0", "\n", "data_iterators", "=", "[", "iter", "(", "dataloader", ")", "for", "dataloader", "in", "dataloaders", "]", "\n", "\n", "num_train_objectives", "=", "len", "(", "train_objectives", ")", "\n", "\n", "for", "epoch", "in", "trange", "(", "epochs", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "training_steps", "=", "0", "\n", "\n", "for", "loss_model", "in", "loss_models", ":", "\n", "                ", "loss_model", ".", "zero_grad", "(", ")", "\n", "loss_model", ".", "train", "(", ")", "\n", "\n", "", "for", "_", "in", "trange", "(", "steps_per_epoch", ",", "desc", "=", "\"Iteration\"", ",", "smoothing", "=", "0.05", ")", ":", "\n", "                ", "for", "train_idx", "in", "range", "(", "num_train_objectives", ")", ":", "\n", "                    ", "loss_model", "=", "loss_models", "[", "train_idx", "]", "\n", "optimizer", "=", "optimizers", "[", "train_idx", "]", "\n", "scheduler", "=", "schedulers", "[", "train_idx", "]", "\n", "data_iterator", "=", "data_iterators", "[", "train_idx", "]", "\n", "\n", "try", ":", "\n", "                        ", "data", "=", "next", "(", "data_iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "#logging.info(\"Restart data_iterator\")", "\n", "                        ", "data_iterator", "=", "iter", "(", "dataloaders", "[", "train_idx", "]", ")", "\n", "data_iterators", "[", "train_idx", "]", "=", "data_iterator", "\n", "data", "=", "next", "(", "data_iterator", ")", "\n", "\n", "", "features", ",", "labels", "=", "batch_to_device", "(", "data", ",", "self", ".", "_target_device", ")", "\n", "loss_value", "=", "loss_model", "(", "features", ",", "labels", ")", "\n", "\n", "if", "fp16", ":", "\n", "                        ", "with", "amp", ".", "scale_loss", "(", "loss_value", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                            ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                        ", "loss_value", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "loss_model", ".", "parameters", "(", ")", ",", "max_grad_norm", ")", "\n", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "training_steps", "+=", "1", "\n", "global_step", "+=", "1", "\n", "\n", "if", "evaluation_steps", ">", "0", "and", "training_steps", "%", "evaluation_steps", "==", "0", ":", "\n", "                    ", "self", ".", "_eval_during_training", "(", "evaluator", ",", "output_path", ",", "save_best_model", ",", "epoch", ",", "training_steps", ")", "\n", "for", "loss_model", "in", "loss_models", ":", "\n", "                        ", "loss_model", ".", "zero_grad", "(", ")", "\n", "loss_model", ".", "train", "(", ")", "\n", "\n", "", "", "", "self", ".", "_eval_during_training", "(", "evaluator", ",", "output_path", ",", "save_best_model", ",", "epoch", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.evaluate": [[557, 569], ["evaluator", "os.makedirs"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "evaluator", ":", "SentenceEvaluator", ",", "output_path", ":", "str", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the model\n\n        :param evaluator:\n            the evaluator\n        :param output_path:\n            the evaluator can write the results to this path\n        \"\"\"", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_path", ",", "exist_ok", "=", "True", ")", "\n", "", "return", "evaluator", "(", "self", ",", "output_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._eval_during_training": [[570, 577], ["evaluator", "SentenceTransformer.SentenceTransformer.save"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save"], ["", "def", "_eval_during_training", "(", "self", ",", "evaluator", ",", "output_path", ",", "save_best_model", ",", "epoch", ",", "steps", ")", ":", "\n", "        ", "\"\"\"Runs evaluation during the training\"\"\"", "\n", "if", "evaluator", "is", "not", "None", ":", "\n", "            ", "score", "=", "evaluator", "(", "self", ",", "output_path", "=", "output_path", ",", "epoch", "=", "epoch", ",", "steps", "=", "steps", ")", "\n", "if", "score", ">", "self", ".", "best_score", "and", "save_best_model", ":", "\n", "                ", "self", ".", "save", "(", "output_path", ")", "\n", "self", ".", "best_score", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._get_scheduler": [[579, 596], ["scheduler.lower.lower.lower", "transformers.get_constant_schedule", "transformers.get_constant_schedule_with_warmup", "transformers.get_linear_schedule_with_warmup", "transformers.get_cosine_schedule_with_warmup", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "ValueError"], "methods", ["None"], ["", "", "", "def", "_get_scheduler", "(", "self", ",", "optimizer", ",", "scheduler", ":", "str", ",", "warmup_steps", ":", "int", ",", "t_total", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Returns the correct learning rate scheduler. Available scheduler: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n        \"\"\"", "\n", "scheduler", "=", "scheduler", ".", "lower", "(", ")", "\n", "if", "scheduler", "==", "'constantlr'", ":", "\n", "            ", "return", "transformers", ".", "get_constant_schedule", "(", "optimizer", ")", "\n", "", "elif", "scheduler", "==", "'warmupconstant'", ":", "\n", "            ", "return", "transformers", ".", "get_constant_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "warmup_steps", ")", "\n", "", "elif", "scheduler", "==", "'warmuplinear'", ":", "\n", "            ", "return", "transformers", ".", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "warmup_steps", ",", "num_training_steps", "=", "t_total", ")", "\n", "", "elif", "scheduler", "==", "'warmupcosine'", ":", "\n", "            ", "return", "transformers", ".", "get_cosine_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "warmup_steps", ",", "num_training_steps", "=", "t_total", ")", "\n", "", "elif", "scheduler", "==", "'warmupcosinewithhardrestarts'", ":", "\n", "            ", "return", "transformers", ".", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "warmup_steps", ",", "num_training_steps", "=", "t_total", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown scheduler {}\"", ".", "format", "(", "scheduler", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device": [[597, 614], ["next", "SentenceTransformer.SentenceTransformer._named_members", "next", "SentenceTransformer.SentenceTransformer.parameters", "module.__dict__.items", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "device", ":", "\n", "        ", "\"\"\"\n        Get torch.device from module, assuming that the whole module has one device.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "return", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", "", "except", "StopIteration", ":", "\n", "# For nn.DataParallel compatibility in PyTorch 1.5", "\n", "\n", "            ", "def", "find_tensor_attributes", "(", "module", ":", "nn", ".", "Module", ")", "->", "List", "[", "Tuple", "[", "str", ",", "Tensor", "]", "]", ":", "\n", "                ", "tuples", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "module", ".", "__dict__", ".", "items", "(", ")", "if", "torch", ".", "is_tensor", "(", "v", ")", "]", "\n", "return", "tuples", "\n", "\n", "", "gen", "=", "self", ".", "_named_members", "(", "get_members_fn", "=", "find_tensor_attributes", ")", "\n", "first_tuple", "=", "next", "(", "gen", ")", "\n", "return", "first_tuple", "[", "1", "]", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.tokenizer": [[622, 628], ["SentenceTransformer.SentenceTransformer._first_module"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module"], ["", "@", "tokenizer", ".", "setter", "\n", "def", "tokenizer", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"\n        Property to set the tokenizer that is should used by this model\n        \"\"\"", "\n", "self", ".", "_first_module", "(", ")", ".", "tokenizer", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.max_seq_length": [[636, 642], ["SentenceTransformer.SentenceTransformer._first_module"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer._first_module"], ["", "@", "max_seq_length", ".", "setter", "\n", "def", "max_seq_length", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"\n        Property to set the maximal input sequence length for the model. Longer inputs will be truncated.\n        \"\"\"", "\n", "self", ".", "_first_module", "(", ")", ".", "max_seq_length", "=", "value", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.pytorch_cos_sim": [[12, 33], ["torch.mm", "isinstance", "torch.tensor", "isinstance", "torch.tensor", "len", "a.unsqueeze.unsqueeze", "len", "b.unsqueeze.unsqueeze", "b_norm.transpose", "a.unsqueeze.norm", "b.unsqueeze.norm"], "function", ["None"], ["max_len", "=", "max", "(", "lens", ")", "\n", "batch_size", "=", "len", "(", "lens", ")", "\n", "mask", "=", "torch", ".", "ByteTensor", "(", "batch_size", ",", "max_len", ")", ".", "to", "(", "device", ")", "\n", "mask", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lens", ")", ":", "\n", "        ", "mask", "[", "i", ",", ":", "l", "]", ".", "fill_", "(", "1", ")", "\n", "", "return", "mask", "\n", "\n", "", "def", "sequence_mean", "(", "sequence", ",", "seq_lens", ",", "dim", "=", "1", ")", ":", "\n", "    ", "if", "seq_lens", ":", "\n", "        ", "assert", "sequence", ".", "size", "(", "0", ")", "==", "len", "(", "seq_lens", ")", "# batch_size", "\n", "sum_", "=", "torch", ".", "sum", "(", "sequence", ",", "dim", "=", "dim", ",", "keepdim", "=", "False", ")", "\n", "mean", "=", "torch", ".", "stack", "(", "[", "s", "/", "l", "for", "s", ",", "l", "in", "zip", "(", "sum_", ",", "seq_lens", ")", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "mean", "=", "torch", ".", "mean", "(", "sequence", ",", "dim", "=", "dim", ",", "keepdim", "=", "False", ")", "\n", "", "return", "mean", "\n", "\n", "", "def", "sequence_loss", "(", "logits", ",", "targets", ",", "xent_fn", "=", "None", ",", "pad_idx", "=", "0", ")", ":", "\n", "    ", "\"\"\" functional interface of SequenceLoss\"\"\"", "\n", "assert", "logits", ".", "size", "(", ")", "[", ":", "-", "1", "]", "==", "targets", ".", "size", "(", ")", "\n", "\n", "mask", "=", "targets", "!=", "pad_idx", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.paraphrase_mining": [[36, 111], ["model.encode", "queue.PriorityQueue", "range", "set", "sorted", "len", "min", "range", "queue.PriorityQueue.empty", "queue.PriorityQueue.get", "sorted", "len", "len", "min", "pytorch_cos_sim().cpu().numpy", "numpy.nan_to_num", "numpy.argpartition", "range", "set.add", "sorted.append", "len", "max", "len", "pytorch_cos_sim().cpu", "len", "queue.PriorityQueue.put", "util.pytorch_cos_sim", "queue.PriorityQueue.get"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.pytorch_cos_sim"], ["mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "logits", ")", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "xent_fn", ":", "\n", "        ", "loss", "=", "xent_fn", "(", "logit", ",", "target", ")", "\n", "", "else", ":", "\n", "        ", "loss", "=", "F", ".", "cross_entropy", "(", "logit", ",", "target", ")", "\n", "", "assert", "(", "not", "math", ".", "isnan", "(", "loss", ".", "mean", "(", ")", ".", "item", "(", ")", ")", "\n", "and", "not", "math", ".", "isinf", "(", "loss", ".", "mean", "(", ")", ".", "item", "(", ")", ")", ")", "\n", "return", "loss", "\n", "\n", "\n", "#################### LSTM helper #########################", "\n", "\n", "", "def", "reorder_sequence", "(", "sequence_emb", ",", "order", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    sequence_emb: [T, B, D] if not batch_first\n    order: list of sequence length\n    \"\"\"", "\n", "batch_dim", "=", "0", "if", "batch_first", "else", "1", "\n", "assert", "len", "(", "order", ")", "==", "sequence_emb", ".", "size", "(", ")", "[", "batch_dim", "]", "\n", "device", "=", "sequence_emb", ".", "device", "\n", "order", "=", "torch", ".", "LongTensor", "(", "order", ")", "\n", "order", "=", "order", ".", "to", "(", "device", ")", "\n", "sorted_", "=", "sequence_emb", ".", "index_select", "(", "index", "=", "order", ",", "dim", "=", "batch_dim", ")", "\n", "\n", "return", "sorted_", "\n", "\n", "", "def", "reorder_lstm_states", "(", "lstm_states", ",", "order", ")", ":", "\n", "    ", "\"\"\"\n    lstm_states: (H, C) of tensor [layer, batch, hidden]\n    order: list of sequence length\n    \"\"\"", "\n", "assert", "isinstance", "(", "lstm_states", ",", "tuple", ")", "\n", "assert", "len", "(", "lstm_states", ")", "==", "2", "\n", "assert", "lstm_states", "[", "0", "]", ".", "size", "(", ")", "==", "lstm_states", "[", "1", "]", ".", "size", "(", ")", "\n", "assert", "len", "(", "order", ")", "==", "lstm_states", "[", "0", "]", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "order", "=", "torch", ".", "LongTensor", "(", "order", ")", ".", "to", "(", "lstm_states", "[", "0", "]", ".", "device", ")", "\n", "sorted_states", "=", "(", "lstm_states", "[", "0", "]", ".", "index_select", "(", "index", "=", "order", ",", "dim", "=", "1", ")", ",", "\n", "lstm_states", "[", "1", "]", ".", "index_select", "(", "index", "=", "order", ",", "dim", "=", "1", ")", ")", "\n", "\n", "return", "sorted_states", "\n", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.information_retrieval": [[113, 116], ["util.semantic_search"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.semantic_search"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.semantic_search": [[118, 181], ["isinstance", "isinstance", "range", "range", "torch.from_numpy", "isinstance", "len", "torch.stack.unsqueeze", "torch.from_numpy", "isinstance", "len", "min", "range", "len", "sorted", "torch.stack", "torch.stack", "torch.stack.norm", "torch.stack.norm", "range", "len", "len", "min", "torch.mm().cpu().numpy", "numpy.nan_to_num", "range", "len", "len", "numpy.argpartition", "len", "torch.mm().cpu", "min", "queries_result_list[].append", "torch.mm", "len", "corpus_embeddings[].transpose"], "function", ["None"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.http_get": [[183, 206], ["os.makedirs", "requests.get", "os.rename", "tqdm.tqdm.close", "os.path.dirname", "print", "requests.get.raise_for_status", "open", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "int", "tqdm.tqdm.update", "file_binary.write", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.batch_to_device": [[208, 219], ["range", "batch[].to", "len", "[].to"], "function", ["None"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.fullname": [[221, 232], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.import_from_string": [[233, 251], ["importlib.import_module", "dotted_path.rsplit", "getattr", "ImportError", "ImportError"], "function", ["None"], []], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.__init__": [[26, 78], ["min", "SentenceLabelDataset.SentenceLabelDataset.convert_input_examples", "numpy.arange", "multiprocessing.cpu_count", "len", "multiprocessing.get_start_method", "logging.info"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.convert_input_examples"], ["def", "__init__", "(", "self", ",", "examples", ":", "List", "[", "InputExample", "]", ",", "model", ":", "SentenceTransformer", ",", "provide_positive", ":", "bool", "=", "True", ",", "\n", "provide_negative", ":", "bool", "=", "True", ",", "\n", "parallel_tokenization", ":", "bool", "=", "True", ",", "\n", "max_processes", ":", "int", "=", "4", ",", "\n", "chunk_size", ":", "int", "=", "5000", ")", ":", "\n", "        ", "\"\"\"\n        Converts input examples to a SentenceLabelDataset usable to train the model with\n        SentenceTransformer.smart_batching_collate as the collate_fn for the DataLoader\n\n        Assumes only one sentence per InputExample and labels as integers from 0 to max_num_labels\n        and should be used in combination with dataset_reader.LabelSentenceReader.\n\n        Labels with only one example are ignored.\n\n        smart_batching_collate as collate_fn is required because it transforms the tokenized texts to the tensors.\n\n        :param examples:\n            the input examples for the training\n        :param model\n            the Sentence BERT model for the conversion\n        :param provide_positive:\n            set this to False, if you don't need a positive example (e.g. for BATCH_HARD_TRIPLET_LOSS).\n        :param provide_negative:\n            set this to False, if you don't need a negative example (e.g. for BATCH_HARD_TRIPLET_LOSS\n            or MULTIPLE_NEGATIVES_RANKING_LOSS).\n        :param parallel_tokenization\n            If true, multiple processes will be started for the tokenization\n        :param max_processes\n            Maximum number of processes started for tokenization. Cannot be larger can cpu_count()\n        :param chunk_size\n            #chunk_size number of examples are send to each process. Larger values increase overall tokenization speed\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "groups_right_border", "=", "[", "]", "\n", "self", ".", "grouped_inputs", "=", "[", "]", "\n", "self", ".", "grouped_labels", "=", "[", "]", "\n", "self", ".", "num_labels", "=", "0", "\n", "self", ".", "max_processes", "=", "min", "(", "max_processes", ",", "cpu_count", "(", ")", ")", "\n", "self", ".", "chunk_size", "=", "chunk_size", "\n", "self", ".", "parallel_tokenization", "=", "parallel_tokenization", "\n", "\n", "if", "self", ".", "parallel_tokenization", ":", "\n", "            ", "if", "multiprocessing", ".", "get_start_method", "(", ")", "!=", "'fork'", ":", "\n", "                ", "logging", ".", "info", "(", "\"Parallel tokenization is only available on Unix systems which allow to fork processes. Fall back to sequential tokenization\"", ")", "\n", "self", ".", "parallel_tokenization", "=", "False", "\n", "\n", "", "", "self", ".", "convert_input_examples", "(", "examples", ",", "model", ")", "\n", "\n", "self", ".", "idxs", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "grouped_inputs", ")", ")", "\n", "\n", "self", ".", "provide_positive", "=", "provide_positive", "\n", "self", ".", "provide_negative", "=", "provide_negative", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.convert_input_examples": [[80, 149], ["logging.info", "enumerate", "list", "range", "torch.tensor", "logging.info", "logging.info", "logging.info", "logging.info", "SentenceLabelDataset.SentenceLabelDataset.model.to", "tqdm.tqdm.tqdm", "inputs.append", "labels.append", "label_sent_mapping.keys", "len", "len", "SentenceLabelDataset.SentenceLabelDataset.tokenize_example", "multiprocessing.Pool", "list", "isinstance", "hasattr", "label_sent_mapping[].append", "len", "SentenceLabelDataset.SentenceLabelDataset.grouped_inputs.extend", "SentenceLabelDataset.SentenceLabelDataset.grouped_labels.extend", "SentenceLabelDataset.SentenceLabelDataset.groups_right_border.append", "len", "len", "p.imap", "isinstance", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.tokenize_example"], ["", "def", "convert_input_examples", "(", "self", ",", "examples", ":", "List", "[", "InputExample", "]", ",", "model", ":", "SentenceTransformer", ")", ":", "\n", "        ", "\"\"\"\n        Converts input examples to a SentenceLabelDataset.\n\n        Assumes only one sentence per InputExample and labels as integers from 0 to max_num_labels\n        and should be used in combination with dataset_reader.LabelSentenceReader.\n\n        Labels with only one example are ignored.\n\n        :param examples:\n            the input examples for the training\n        :param model\n            the Sentence Transformer model for the conversion\n        :param is_pretokenized\n            If set to true, no tokenization will be applied. It is expected that the input is tokenized via model.tokenize\n        \"\"\"", "\n", "\n", "inputs", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "label_sent_mapping", "=", "{", "}", "\n", "too_long", "=", "0", "\n", "label_type", "=", "None", "\n", "\n", "logging", ".", "info", "(", "\"Start tokenization\"", ")", "\n", "if", "not", "self", ".", "parallel_tokenization", "or", "self", ".", "max_processes", "==", "1", "or", "len", "(", "examples", ")", "<=", "self", ".", "chunk_size", ":", "\n", "            ", "tokenized_texts", "=", "[", "self", ".", "tokenize_example", "(", "example", ")", "for", "example", "in", "examples", "]", "\n", "", "else", ":", "\n", "            ", "logging", ".", "info", "(", "\"Use multi-process tokenization with {} processes\"", ".", "format", "(", "self", ".", "max_processes", ")", ")", "\n", "self", ".", "model", ".", "to", "(", "'cpu'", ")", "\n", "with", "Pool", "(", "self", ".", "max_processes", ")", "as", "p", ":", "\n", "                ", "tokenized_texts", "=", "list", "(", "p", ".", "imap", "(", "self", ".", "tokenize_example", ",", "examples", ",", "chunksize", "=", "self", ".", "chunk_size", ")", ")", "\n", "\n", "# Group examples and labels", "\n", "# Add examples with the same label to the same dict", "\n", "", "", "for", "ex_index", ",", "example", "in", "enumerate", "(", "tqdm", "(", "examples", ",", "desc", "=", "\"Convert dataset\"", ")", ")", ":", "\n", "            ", "if", "label_type", "is", "None", ":", "\n", "                ", "if", "isinstance", "(", "example", ".", "label", ",", "int", ")", ":", "\n", "                    ", "label_type", "=", "torch", ".", "long", "\n", "", "elif", "isinstance", "(", "example", ".", "label", ",", "float", ")", ":", "\n", "                    ", "label_type", "=", "torch", ".", "float", "\n", "", "", "tokenized_text", "=", "tokenized_texts", "[", "ex_index", "]", "[", "0", "]", "\n", "\n", "if", "hasattr", "(", "model", ",", "'max_seq_length'", ")", "and", "model", ".", "max_seq_length", "is", "not", "None", "and", "model", ".", "max_seq_length", ">", "0", "and", "len", "(", "tokenized_text", ")", ">", "model", ".", "max_seq_length", ":", "\n", "                ", "too_long", "+=", "1", "\n", "\n", "", "if", "example", ".", "label", "in", "label_sent_mapping", ":", "\n", "                ", "label_sent_mapping", "[", "example", ".", "label", "]", ".", "append", "(", "ex_index", ")", "\n", "", "else", ":", "\n", "                ", "label_sent_mapping", "[", "example", ".", "label", "]", "=", "[", "ex_index", "]", "\n", "\n", "", "inputs", ".", "append", "(", "tokenized_text", ")", "\n", "labels", ".", "append", "(", "example", ".", "label", ")", "\n", "\n", "# Group sentences, such that sentences with the same label", "\n", "# are besides each other. Only take labels with at least 2 examples", "\n", "", "distinct_labels", "=", "list", "(", "label_sent_mapping", ".", "keys", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "distinct_labels", ")", ")", ":", "\n", "            ", "label", "=", "distinct_labels", "[", "i", "]", "\n", "if", "len", "(", "label_sent_mapping", "[", "label", "]", ")", ">=", "2", ":", "\n", "                ", "self", ".", "grouped_inputs", ".", "extend", "(", "[", "inputs", "[", "j", "]", "for", "j", "in", "label_sent_mapping", "[", "label", "]", "]", ")", "\n", "self", ".", "grouped_labels", ".", "extend", "(", "[", "labels", "[", "j", "]", "for", "j", "in", "label_sent_mapping", "[", "label", "]", "]", ")", "\n", "self", ".", "groups_right_border", ".", "append", "(", "len", "(", "self", ".", "grouped_inputs", ")", ")", "#At which position does this label group / bucket end?", "\n", "self", ".", "num_labels", "+=", "1", "\n", "\n", "", "", "self", ".", "grouped_labels", "=", "torch", ".", "tensor", "(", "self", ".", "grouped_labels", ",", "dtype", "=", "label_type", ")", "\n", "logging", ".", "info", "(", "\"Num sentences: %d\"", "%", "(", "len", "(", "self", ".", "grouped_inputs", ")", ")", ")", "\n", "logging", ".", "info", "(", "\"Sentences longer than max_seqence_length: {}\"", ".", "format", "(", "too_long", ")", ")", "\n", "logging", ".", "info", "(", "\"Number of labels with >1 examples: {}\"", ".", "format", "(", "len", "(", "distinct_labels", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.tokenize_example": [[151, 156], ["SentenceLabelDataset.SentenceLabelDataset.model.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize_example", "(", "self", ",", "example", ")", ":", "\n", "        ", "if", "example", ".", "texts_tokenized", "is", "not", "None", ":", "\n", "            ", "return", "example", ".", "texts_tokenized", "\n", "\n", "", "return", "[", "self", ".", "model", ".", "tokenize", "(", "text", ")", "for", "text", "in", "example", ".", "texts", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.__getitem__": [[157, 182], ["bisect.bisect_right", "numpy.random.choice", "numpy.random.choice", "numpy.concatenate", "numpy.concatenate"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "not", "self", ".", "provide_positive", "and", "not", "self", ".", "provide_negative", ":", "\n", "            ", "return", "[", "self", ".", "grouped_inputs", "[", "item", "]", "]", ",", "self", ".", "grouped_labels", "[", "item", "]", "\n", "\n", "# Anchor element", "\n", "", "anchor", "=", "self", ".", "grouped_inputs", "[", "item", "]", "\n", "\n", "# Check start and end position for this label in our list of grouped sentences", "\n", "group_idx", "=", "bisect", ".", "bisect_right", "(", "self", ".", "groups_right_border", ",", "item", ")", "\n", "left_border", "=", "0", "if", "group_idx", "==", "0", "else", "self", ".", "groups_right_border", "[", "group_idx", "-", "1", "]", "\n", "right_border", "=", "self", ".", "groups_right_border", "[", "group_idx", "]", "\n", "\n", "if", "self", ".", "provide_positive", ":", "\n", "            ", "positive_item_idx", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "concatenate", "(", "[", "self", ".", "idxs", "[", "left_border", ":", "item", "]", ",", "self", ".", "idxs", "[", "item", "+", "1", ":", "right_border", "]", "]", ")", ")", "\n", "positive", "=", "self", ".", "grouped_inputs", "[", "positive_item_idx", "]", "\n", "", "else", ":", "\n", "            ", "positive", "=", "[", "]", "\n", "\n", "", "if", "self", ".", "provide_negative", ":", "\n", "            ", "negative_item_idx", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "concatenate", "(", "[", "self", ".", "idxs", "[", "0", ":", "left_border", "]", ",", "self", ".", "idxs", "[", "right_border", ":", "]", "]", ")", ")", "\n", "negative", "=", "self", ".", "grouped_inputs", "[", "negative_item_idx", "]", "\n", "", "else", ":", "\n", "            ", "negative", "=", "[", "]", "\n", "\n", "", "return", "[", "anchor", ",", "positive", ",", "negative", "]", ",", "self", ".", "grouped_labels", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentenceLabelDataset.SentenceLabelDataset.__len__": [[184, 186], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "grouped_inputs", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.__init__": [[26, 44], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "student_model", ":", "SentenceTransformer", ",", "teacher_model", ":", "SentenceTransformer", ",", "batch_size", ":", "int", "=", "8", ",", "use_embedding_cache", ":", "bool", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Parallel sentences dataset reader to train student model given a teacher model\n        :param student_model: Student sentence embedding model that should be trained\n        :param teacher_model: Teacher model, that provides the sentence embeddings for the first column in the dataset file\n        \"\"\"", "\n", "self", ".", "student_model", "=", "student_model", "\n", "self", ".", "teacher_model", "=", "teacher_model", "\n", "self", ".", "datasets", "=", "[", "]", "\n", "self", ".", "datasets_iterator", "=", "[", "]", "\n", "self", ".", "datasets_tokenized", "=", "[", "]", "\n", "self", ".", "dataset_indices", "=", "[", "]", "\n", "self", ".", "copy_dataset_indices", "=", "[", "]", "\n", "self", ".", "cache", "=", "[", "]", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "use_embedding_cache", "=", "use_embedding_cache", "\n", "self", ".", "embedding_cache", "=", "{", "}", "\n", "self", ".", "num_sentences", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.load_data": [[45, 72], ["logging.info", "ParallelSentencesDataset.ParallelSentencesDataset.add_dataset", "filepath.endswith", "gzip.open", "open", "line.strip().split", "parallel_sentences.append", "line.strip", "max", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.add_dataset"], ["", "def", "load_data", "(", "self", ",", "filepath", ":", "str", ",", "weight", ":", "int", "=", "100", ",", "max_sentences", ":", "int", "=", "None", ",", "max_sentence_length", ":", "int", "=", "128", ")", ":", "\n", "        ", "\"\"\"\n        Reads in a tab-seperated .txt/.csv/.tsv or .gz file. The different columns contain the different translations of the sentence in the first column\n\n        :param filepath: Filepath to the file\n        :param weight: If more that one dataset is loaded with load_data: With which frequency should data be sampled from this dataset?\n        :param max_sentences: Max number of lines to be read from filepath\n        :param max_sentence_length: Skip the example if one of the sentences is has more characters than max_sentence_length\n        :param batch_size: Size for encoding parallel sentences\n        :return:\n        \"\"\"", "\n", "\n", "logging", ".", "info", "(", "\"Load \"", "+", "filepath", ")", "\n", "parallel_sentences", "=", "[", "]", "\n", "\n", "with", "gzip", ".", "open", "(", "filepath", ",", "'rt'", ",", "encoding", "=", "'utf8'", ")", "if", "filepath", ".", "endswith", "(", "'.gz'", ")", "else", "open", "(", "filepath", ",", "encoding", "=", "'utf8'", ")", "as", "fIn", ":", "\n", "            ", "count", "=", "0", "\n", "for", "line", "in", "fIn", ":", "\n", "                ", "sentences", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "max_sentence_length", "is", "not", "None", "and", "max_sentence_length", ">", "0", "and", "max", "(", "[", "len", "(", "sent", ")", "for", "sent", "in", "sentences", "]", ")", ">", "max_sentence_length", ":", "\n", "                    ", "continue", "\n", "\n", "", "parallel_sentences", ".", "append", "(", "sentences", ")", "\n", "count", "+=", "1", "\n", "if", "max_sentences", "is", "not", "None", "and", "max_sentences", ">", "0", "and", "count", ">=", "max_sentences", ":", "\n", "                    ", "break", "\n", "", "", "", "self", ".", "add_dataset", "(", "parallel_sentences", ",", "weight", "=", "weight", ",", "max_sentences", "=", "max_sentences", ",", "max_sentence_length", "=", "max_sentence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.add_dataset": [[74, 100], ["sum", "len", "ParallelSentencesDataset.ParallelSentencesDataset.datasets.append", "ParallelSentencesDataset.ParallelSentencesDataset.datasets_iterator.append", "ParallelSentencesDataset.ParallelSentencesDataset.datasets_tokenized.append", "ParallelSentencesDataset.ParallelSentencesDataset.dataset_indices.extend", "len", "list", "set", "sentences_map[].add", "len", "sentences_map.items", "max", "len", "len"], "methods", ["None"], ["", "def", "add_dataset", "(", "self", ",", "parallel_sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "weight", ":", "int", "=", "1000", ",", "max_sentences", ":", "int", "=", "None", ",", "max_sentence_length", ":", "int", "=", "128", ")", ":", "\n", "        ", "sentences_map", "=", "{", "}", "\n", "for", "sentences", "in", "parallel_sentences", ":", "\n", "            ", "if", "max_sentence_length", "is", "not", "None", "and", "max_sentence_length", ">", "0", "and", "max", "(", "[", "len", "(", "sent", ")", "for", "sent", "in", "sentences", "]", ")", ">", "max_sentence_length", ":", "\n", "                ", "continue", "\n", "\n", "", "source_sentence", "=", "sentences", "[", "0", "]", "\n", "if", "source_sentence", "not", "in", "sentences_map", ":", "\n", "                ", "sentences_map", "[", "source_sentence", "]", "=", "set", "(", ")", "\n", "\n", "", "for", "sent", "in", "sentences", ":", "\n", "                ", "sentences_map", "[", "source_sentence", "]", ".", "add", "(", "sent", ")", "\n", "\n", "", "if", "max_sentences", "is", "not", "None", "and", "max_sentences", ">", "0", "and", "len", "(", "sentences_map", ")", ">=", "max_sentences", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "len", "(", "sentences_map", ")", "==", "0", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "num_sentences", "+=", "sum", "(", "[", "len", "(", "sentences_map", "[", "sent", "]", ")", "for", "sent", "in", "sentences_map", "]", ")", "\n", "\n", "dataset_id", "=", "len", "(", "self", ".", "datasets", ")", "\n", "self", ".", "datasets", ".", "append", "(", "list", "(", "sentences_map", ".", "items", "(", ")", ")", ")", "\n", "self", ".", "datasets_iterator", ".", "append", "(", "0", ")", "\n", "self", ".", "datasets_tokenized", ".", "append", "(", "False", ")", "\n", "self", ".", "dataset_indices", ".", "extend", "(", "[", "dataset_id", "]", "*", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.generate_data": [[101, 118], ["ParallelSentencesDataset.ParallelSentencesDataset.get_embeddings", "zip", "random.shuffle", "ParallelSentencesDataset.ParallelSentencesDataset.next_entry", "source_sentences_list.append", "target_sentences_list.append", "ParallelSentencesDataset.ParallelSentencesDataset.cache.append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.get_embeddings", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.next_entry"], ["", "def", "generate_data", "(", "self", ")", ":", "\n", "        ", "source_sentences_list", "=", "[", "]", "\n", "target_sentences_list", "=", "[", "]", "\n", "for", "data_idx", "in", "self", ".", "dataset_indices", ":", "\n", "            ", "src_sentence", ",", "trg_sentences", "=", "self", ".", "next_entry", "(", "data_idx", ")", "\n", "source_sentences_list", ".", "append", "(", "src_sentence", ")", "\n", "target_sentences_list", ".", "append", "(", "trg_sentences", ")", "\n", "\n", "\n", "#Generate embeddings", "\n", "", "src_embeddings", "=", "self", ".", "get_embeddings", "(", "source_sentences_list", ")", "\n", "\n", "for", "src_embedding", ",", "trg_sentences", "in", "zip", "(", "src_embeddings", ",", "target_sentences_list", ")", ":", "\n", "            ", "for", "trg_sentence", "in", "trg_sentences", ":", "\n", "                ", "self", ".", "cache", ".", "append", "(", "[", "[", "trg_sentence", "]", ",", "src_embedding", "]", ")", "\n", "\n", "", "", "random", ".", "shuffle", "(", "self", ".", "cache", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.next_entry": [[119, 133], ["len", "random.shuffle", "ParallelSentencesDataset.ParallelSentencesDataset.student_model.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "next_entry", "(", "self", ",", "data_idx", ")", ":", "\n", "        ", "source", ",", "target_sentences", "=", "self", ".", "datasets", "[", "data_idx", "]", "[", "self", ".", "datasets_iterator", "[", "data_idx", "]", "]", "\n", "\n", "if", "not", "self", ".", "datasets_tokenized", "[", "data_idx", "]", ":", "\n", "            ", "target_sentences", "=", "[", "self", ".", "student_model", ".", "tokenize", "(", "sent", ")", "for", "sent", "in", "target_sentences", "]", "\n", "self", ".", "datasets", "[", "data_idx", "]", "[", "self", ".", "datasets_iterator", "[", "data_idx", "]", "]", "=", "[", "source", ",", "target_sentences", "]", "\n", "\n", "", "self", ".", "datasets_iterator", "[", "data_idx", "]", "+=", "1", "\n", "if", "self", ".", "datasets_iterator", "[", "data_idx", "]", ">=", "len", "(", "self", ".", "datasets", "[", "data_idx", "]", ")", ":", "#Restart iterator", "\n", "            ", "self", ".", "datasets_iterator", "[", "data_idx", "]", "=", "0", "\n", "self", ".", "datasets_tokenized", "[", "data_idx", "]", "=", "True", "\n", "random", ".", "shuffle", "(", "self", ".", "datasets", "[", "data_idx", "]", ")", "\n", "\n", "", "return", "source", ",", "target_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.get_embeddings": [[134, 150], ["ParallelSentencesDataset.ParallelSentencesDataset.teacher_model.encode", "len", "ParallelSentencesDataset.ParallelSentencesDataset.teacher_model.encode", "zip", "new_sentences.append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "get_embeddings", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "if", "not", "self", ".", "use_embedding_cache", ":", "\n", "            ", "return", "self", ".", "teacher_model", ".", "encode", "(", "sentences", ",", "batch_size", "=", "self", ".", "batch_size", ",", "show_progress_bar", "=", "False", ",", "convert_to_numpy", "=", "False", ")", "\n", "\n", "#Use caching", "\n", "", "new_sentences", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "            ", "if", "sent", "not", "in", "self", ".", "embedding_cache", ":", "\n", "                ", "new_sentences", ".", "append", "(", "sent", ")", "\n", "\n", "", "", "if", "len", "(", "new_sentences", ")", ">", "0", ":", "\n", "            ", "new_embeddings", "=", "self", ".", "teacher_model", ".", "encode", "(", "new_sentences", ",", "batch_size", "=", "self", ".", "batch_size", ",", "show_progress_bar", "=", "False", ",", "convert_to_numpy", "=", "False", ")", "\n", "for", "sent", ",", "embedding", "in", "zip", "(", "new_sentences", ",", "new_embeddings", ")", ":", "\n", "                ", "self", ".", "embedding_cache", "[", "sent", "]", "=", "embedding", "\n", "\n", "", "", "return", "[", "self", ".", "embedding_cache", "[", "sent", "]", "for", "sent", "in", "sentences", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.__len__": [[151, 153], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.__getitem__": [[154, 159], ["ParallelSentencesDataset.ParallelSentencesDataset.cache.pop", "len", "ParallelSentencesDataset.ParallelSentencesDataset.generate_data"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.ParallelSentencesDataset.ParallelSentencesDataset.generate_data"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "cache", ")", "==", "0", ":", "\n", "            ", "self", ".", "generate_data", "(", ")", "\n", "\n", "", "return", "self", ".", "cache", ".", "pop", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentencesDataset.SentencesDataset.__init__": [[14, 29], ["isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "examples", ":", "List", "[", "InputExample", "]", ",", "\n", "model", ":", "SentenceTransformer", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Create a new SentencesDataset with the tokenized texts and the labels as Tensor\n\n        :param examples\n            A list of sentence.transformers.readers.InputExample\n        :param model:\n            SentenceTransformerModel\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "examples", "=", "examples", "\n", "self", ".", "label_type", "=", "torch", ".", "long", "if", "isinstance", "(", "self", ".", "examples", "[", "0", "]", ".", "label", ",", "int", ")", "else", "torch", ".", "float", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentencesDataset.SentencesDataset.__getitem__": [[31, 37], ["torch.tensor", "SentencesDataset.SentencesDataset.model.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "label", "=", "torch", ".", "tensor", "(", "self", ".", "examples", "[", "item", "]", ".", "label", ",", "dtype", "=", "self", ".", "label_type", ")", "\n", "if", "self", ".", "examples", "[", "item", "]", ".", "texts_tokenized", "is", "None", ":", "\n", "            ", "self", ".", "examples", "[", "item", "]", ".", "texts_tokenized", "=", "[", "self", ".", "model", ".", "tokenize", "(", "text", ")", "for", "text", "in", "self", ".", "examples", "[", "item", "]", ".", "texts", "]", "\n", "\n", "", "return", "self", ".", "examples", "[", "item", "]", ".", "texts_tokenized", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.SentencesDataset.SentencesDataset.__len__": [[39, 41], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.EncodeDataset.EncodeDataset.__init__": [[7, 18], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "sentences", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "model", ":", "SentenceTransformer", ",", "\n", "is_tokenized", ":", "bool", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        EncodeDataset is used by SentenceTransformer.encode method. It just stores\n        the input texts and returns a tokenized version of it.\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "sentences", "=", "sentences", "\n", "self", ".", "is_tokenized", "=", "is_tokenized", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.EncodeDataset.EncodeDataset.__getitem__": [[20, 22], ["EncodeDataset.EncodeDataset.model.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "sentences", "[", "item", "]", "if", "self", ".", "is_tokenized", "else", "self", ".", "model", ".", "tokenize", "(", "self", ".", "sentences", "[", "item", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.datasets.EncodeDataset.EncodeDataset.__len__": [[24, 26], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "sentences", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sampler.LabelSampler.LabelSampler.__init__": [[24, 45], ["torch.utils.data.Sampler.__init__", "numpy.arange", "numpy.random.shuffle"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "data_source", ":", "SentenceLabelDataset", ",", "samples_per_label", ":", "int", "=", "5", ",", "\n", "with_replacement", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Creates a LabelSampler for a SentenceLabelDataset.\n\n        :param data_source:\n            the dataset from which samples are drawn\n        :param samples_per_label:\n            the number of consecutive, random and unique samples drawn per label\n        :param with_replacement:\n            if this is True, then each sample is drawn at most once (depending on the total number of samples per label).\n            if this is False, then one sample can be drawn in multiple draws, but still not multiple times in the same\n            drawing.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "data_source", ")", "\n", "self", ".", "data_source", "=", "data_source", "\n", "self", ".", "samples_per_label", "=", "samples_per_label", "\n", "self", ".", "label_range", "=", "np", ".", "arange", "(", "data_source", ".", "num_labels", ")", "\n", "self", ".", "borders", "=", "data_source", ".", "groups_right_border", "\n", "self", ".", "with_replacement", "=", "with_replacement", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "label_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sampler.LabelSampler.LabelSampler.__iter__": [[46, 74], ["len", "set", "numpy.arange", "len", "numpy.random.choice", "len", "numpy.random.shuffle", "already_seen[].add", "numpy.arange"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "label_idx", "=", "0", "\n", "count", "=", "0", "\n", "already_seen", "=", "{", "}", "\n", "while", "count", "<", "len", "(", "self", ".", "data_source", ")", ":", "\n", "            ", "label", "=", "self", ".", "label_range", "[", "label_idx", "]", "\n", "if", "label", "not", "in", "already_seen", ":", "\n", "                ", "already_seen", "[", "label", "]", "=", "set", "(", ")", "\n", "\n", "", "left_border", "=", "0", "if", "label", "==", "0", "else", "self", ".", "borders", "[", "label", "-", "1", "]", "\n", "right_border", "=", "self", ".", "borders", "[", "label", "]", "\n", "\n", "if", "self", ".", "with_replacement", ":", "\n", "                ", "selection", "=", "np", ".", "arange", "(", "left_border", ",", "right_border", ")", "\n", "", "else", ":", "\n", "                ", "selection", "=", "[", "i", "for", "i", "in", "np", ".", "arange", "(", "left_border", ",", "right_border", ")", "if", "i", "not", "in", "already_seen", "[", "label", "]", "]", "\n", "\n", "", "if", "len", "(", "selection", ")", ">=", "self", ".", "samples_per_label", ":", "\n", "                ", "for", "element_idx", "in", "np", ".", "random", ".", "choice", "(", "selection", ",", "self", ".", "samples_per_label", ",", "replace", "=", "False", ")", ":", "\n", "                    ", "count", "+=", "1", "\n", "already_seen", "[", "label", "]", ".", "add", "(", "element_idx", ")", "\n", "yield", "element_idx", "\n", "\n", "", "", "label_idx", "+=", "1", "\n", "if", "label_idx", ">=", "len", "(", "self", ".", "label_range", ")", ":", "\n", "                ", "label_idx", "=", "0", "\n", "already_seen", "=", "{", "}", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "label_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sampler.LabelSampler.LabelSampler.__len__": [[75, 77], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_source", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.__init__": [[16, 31], ["torch.nn.Module.__init__", "transformers.CamembertModel.from_pretrained", "transformers.CamembertTokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "CamemBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "max_seq_length", ">", "511", ":", "\n", "            ", "logging", ".", "warning", "(", "\"CamemBERT only allows a max_seq_length of 511 (514 with special tokens). Value will be set to 511\"", ")", "\n", "max_seq_length", "=", "511", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "camembert", "=", "CamembertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "CamembertTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.forward": [[32, 45], ["CamemBERT.CamemBERT.camembert", "features.update", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "#CamemBERT does not use token_type_ids", "\n", "output_states", "=", "self", ".", "camembert", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "self", ".", "camembert", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "hidden_states", "=", "output_states", "[", "2", "]", "\n", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "hidden_states", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.get_word_embedding_dimension": [[46, 48], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "camembert", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.tokenize": [[49, 54], ["CamemBERT.CamemBERT.tokenizer.convert_tokens_to_ids", "CamemBERT.CamemBERT.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.get_sentence_features": [[55, 67], ["CamemBERT.CamemBERT.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "3", "#Add space for special tokens", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.get_config_dict": [[68, 70], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.save": [[71, 77], ["CamemBERT.CamemBERT.camembert.save_pretrained", "CamemBERT.CamemBERT.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "CamemBERT.CamemBERT.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "camembert", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_camembert_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CamemBERT.CamemBERT.load": [[78, 83], ["CamemBERT.CamemBERT", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_camembert_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "CamemBERT", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.__init__": [[15, 30], ["torch.nn.Module.__init__", "transformers.DistilBertModel.from_pretrained", "transformers.DistilBertTokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "DistilBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "max_seq_length", ">", "510", ":", "\n", "            ", "logging", ".", "warning", "(", "\"BERT only allows a max_seq_length of 510 (512 with special tokens). Value will be set to 510\"", ")", "\n", "max_seq_length", "=", "510", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "bert", "=", "DistilBertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "DistilBertTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.forward": [[31, 44], ["DistilBERT.DistilBERT.bert", "features.update", "len", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "# DistilBERT does not use token_type_ids", "\n", "output_states", "=", "self", ".", "bert", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "len", "(", "output_states", ")", ">", "1", ":", "\n", "            ", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "output_states", "[", "1", "]", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.get_word_embedding_dimension": [[45, 47], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "bert", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.tokenize": [[48, 53], ["DistilBERT.DistilBERT.tokenizer.convert_tokens_to_ids", "DistilBERT.DistilBERT.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.get_sentence_features": [[54, 66], ["DistilBERT.DistilBERT.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "2", "#Add space for special tokens", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.get_config_dict": [[67, 69], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.save": [[70, 76], ["DistilBERT.DistilBERT.bert.save_pretrained", "DistilBERT.DistilBERT.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "DistilBERT.DistilBERT.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "bert", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_distilbert_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.DistilBERT.DistilBERT.load": [[77, 82], ["DistilBERT.DistilBERT", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_distilbert_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "DistilBERT", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.__init__": [[15, 30], ["torch.nn.Module.__init__", "transformers.RobertaModel.from_pretrained", "transformers.RobertaTokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "RoBERTa", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "max_seq_length", ">", "512", ":", "\n", "            ", "logging", ".", "warning", "(", "\"RoBERTa only allows a max_seq_length of 512 (514 with special tokens). Value will be set to 512\"", ")", "\n", "max_seq_length", "=", "512", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.forward": [[32, 43], ["RoBERTa.RoBERTa.roberta", "features.update", "len", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "output_states", "=", "self", ".", "roberta", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "len", "(", "output_states", ")", ">", "2", ":", "\n", "            ", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "output_states", "[", "2", "]", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.get_word_embedding_dimension": [[44, 46], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "roberta", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.tokenize": [[47, 52], ["RoBERTa.RoBERTa.tokenizer.convert_tokens_to_ids", "RoBERTa.RoBERTa.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.get_sentence_features": [[53, 65], ["RoBERTa.RoBERTa.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "2", "##Add Space for CLS + SEP token", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.get_config_dict": [[66, 68], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.save": [[69, 75], ["RoBERTa.RoBERTa.roberta.save_pretrained", "RoBERTa.RoBERTa.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "RoBERTa.RoBERTa.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "roberta", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_roberta_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.RoBERTa.RoBERTa.load": [[76, 81], ["RoBERTa.RoBERTa", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_roberta_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "RoBERTa", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.__init__": [[18, 25], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "word_embedding_dimension", ",", "layer_start", ":", "int", "=", "4", ",", "context_window_size", ":", "int", "=", "2", ")", ":", "\n", "        ", "super", "(", "WKPooling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'word_embedding_dimension'", ",", "'layer_start'", ",", "'context_window_size'", "]", "\n", "self", ".", "word_embedding_dimension", "=", "word_embedding_dimension", "\n", "self", ".", "pooling_output_dimension", "=", "word_embedding_dimension", "\n", "self", ".", "layer_start", "=", "layer_start", "\n", "self", ".", "context_window_size", "=", "context_window_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.forward": [[26, 61], ["torch.stack().transpose", "all_layer_embedding.cpu.cpu.cpu", "features[].cpu().numpy", "range", "torch.stack().to", "features.update", "numpy.array", "len", "range", "features.update", "torch.stack", "WKPooling.WKPooling.unify_sentence", "embedding.append", "torch.stack", "features[].cpu", "WKPooling.WKPooling.unify_token", "torch.stack.append", "torch.stack", "sum"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.unify_sentence", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.unify_token"], ["", "def", "forward", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", ":", "\n", "        ", "ft_all_layers", "=", "features", "[", "'all_layer_embeddings'", "]", "\n", "org_device", "=", "ft_all_layers", "[", "0", "]", ".", "device", "\n", "all_layer_embedding", "=", "torch", ".", "stack", "(", "ft_all_layers", ")", ".", "transpose", "(", "1", ",", "0", ")", "\n", "all_layer_embedding", "=", "all_layer_embedding", "[", ":", ",", "self", ".", "layer_start", ":", ",", ":", ",", ":", "]", "# Start from 4th layers output", "\n", "\n", "# torch.qr is slow on GPU (see https://github.com/pytorch/pytorch/issues/22573). So compute it on CPU until issue is fixed", "\n", "all_layer_embedding", "=", "all_layer_embedding", ".", "cpu", "(", ")", "\n", "\n", "attention_mask", "=", "features", "[", "'attention_mask'", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "unmask_num", "=", "np", ".", "array", "(", "[", "sum", "(", "mask", ")", "for", "mask", "in", "attention_mask", "]", ")", "-", "1", "# Not considering the last item", "\n", "embedding", "=", "[", "]", "\n", "\n", "# One sentence at a time", "\n", "for", "sent_index", "in", "range", "(", "len", "(", "unmask_num", ")", ")", ":", "\n", "            ", "sentence_feature", "=", "all_layer_embedding", "[", "sent_index", ",", ":", ",", ":", "unmask_num", "[", "sent_index", "]", ",", ":", "]", "\n", "one_sentence_embedding", "=", "[", "]", "\n", "# Process each token", "\n", "for", "token_index", "in", "range", "(", "sentence_feature", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "token_feature", "=", "sentence_feature", "[", ":", ",", "token_index", ",", ":", "]", "\n", "# 'Unified Word Representation'", "\n", "token_embedding", "=", "self", ".", "unify_token", "(", "token_feature", ")", "\n", "one_sentence_embedding", ".", "append", "(", "token_embedding", ")", "\n", "\n", "", "features", ".", "update", "(", "{", "'sentence_embedding'", ":", "features", "[", "'cls_token_embeddings'", "]", "}", ")", "\n", "\n", "one_sentence_embedding", "=", "torch", ".", "stack", "(", "one_sentence_embedding", ")", "\n", "sentence_embedding", "=", "self", ".", "unify_sentence", "(", "sentence_feature", ",", "one_sentence_embedding", ")", "\n", "embedding", ".", "append", "(", "sentence_embedding", ")", "\n", "\n", "", "output_vector", "=", "torch", ".", "stack", "(", "embedding", ")", ".", "to", "(", "org_device", ")", "\n", "\n", "features", ".", "update", "(", "{", "'sentence_embedding'", ":", "output_vector", "}", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.unify_token": [[62, 91], ["torch.zeros", "torch.zeros", "range", "torch.mv", "torch.cat", "torch.qr", "torch.sum", "torch.sum", "torch.sum", "token_feature.t", "token_feature.size", "token_feature.size", "token_feature.size", "torch.mean().matmul", "torch.norm", "torch.abs", "torch.norm", "torch.mean", "WKPooling.WKPooling.norm_vector", "torch.cat.size"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.norm_vector"], ["", "def", "unify_token", "(", "self", ",", "token_feature", ")", ":", "\n", "        ", "\"\"\"\n            Unify Token Representation\n        \"\"\"", "\n", "window_size", "=", "self", ".", "context_window_size", "\n", "\n", "alpha_alignment", "=", "torch", ".", "zeros", "(", "token_feature", ".", "size", "(", ")", "[", "0", "]", ",", "device", "=", "token_feature", ".", "device", ")", "\n", "alpha_novelty", "=", "torch", ".", "zeros", "(", "token_feature", ".", "size", "(", ")", "[", "0", "]", ",", "device", "=", "token_feature", ".", "device", ")", "\n", "\n", "for", "k", "in", "range", "(", "token_feature", ".", "size", "(", ")", "[", "0", "]", ")", ":", "\n", "            ", "left_window", "=", "token_feature", "[", "k", "-", "window_size", ":", "k", ",", ":", "]", "\n", "right_window", "=", "token_feature", "[", "k", "+", "1", ":", "k", "+", "window_size", "+", "1", ",", ":", "]", "\n", "window_matrix", "=", "torch", ".", "cat", "(", "[", "left_window", ",", "right_window", ",", "token_feature", "[", "k", ",", ":", "]", "[", "None", ",", ":", "]", "]", ")", "\n", "Q", ",", "R", "=", "torch", ".", "qr", "(", "window_matrix", ".", "T", ")", "\n", "\n", "r", "=", "R", "[", ":", ",", "-", "1", "]", "\n", "alpha_alignment", "[", "k", "]", "=", "torch", ".", "mean", "(", "self", ".", "norm_vector", "(", "R", "[", ":", "-", "1", ",", ":", "-", "1", "]", ",", "dim", "=", "0", ")", ",", "dim", "=", "1", ")", ".", "matmul", "(", "R", "[", ":", "-", "1", ",", "-", "1", "]", ")", "/", "torch", ".", "norm", "(", "r", "[", ":", "-", "1", "]", ")", "\n", "alpha_alignment", "[", "k", "]", "=", "1", "/", "(", "alpha_alignment", "[", "k", "]", "*", "window_matrix", ".", "size", "(", ")", "[", "0", "]", "*", "2", ")", "\n", "alpha_novelty", "[", "k", "]", "=", "torch", ".", "abs", "(", "r", "[", "-", "1", "]", ")", "/", "torch", ".", "norm", "(", "r", ")", "\n", "\n", "# Sum Norm", "\n", "", "alpha_alignment", "=", "alpha_alignment", "/", "torch", ".", "sum", "(", "alpha_alignment", ")", "# Normalization Choice", "\n", "alpha_novelty", "=", "alpha_novelty", "/", "torch", ".", "sum", "(", "alpha_novelty", ")", "\n", "\n", "alpha", "=", "alpha_novelty", "+", "alpha_alignment", "\n", "alpha", "=", "alpha", "/", "torch", ".", "sum", "(", "alpha", ")", "# Normalize", "\n", "\n", "out_embedding", "=", "torch", ".", "mv", "(", "token_feature", ".", "t", "(", ")", ",", "alpha", ")", "\n", "return", "out_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.norm_vector": [[92, 98], ["torch.norm", "vec.div", "torch.norm.expand_as"], "methods", ["None"], ["", "def", "norm_vector", "(", "self", ",", "vec", ",", "p", "=", "2", ",", "dim", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Implements the normalize() function from sklearn\n        \"\"\"", "\n", "vec_norm", "=", "torch", ".", "norm", "(", "vec", ",", "p", "=", "p", ",", "dim", "=", "dim", ")", "\n", "return", "vec", ".", "div", "(", "vec_norm", ".", "expand_as", "(", "vec", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.unify_sentence": [[99, 115], ["torch.zeros", "range", "torch.mv", "one_sentence_embedding.size", "WKPooling.WKPooling.cosine_similarity_torch", "torch.var", "torch.sum", "one_sentence_embedding.t", "WKPooling.WKPooling.diagonal"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.cosine_similarity_torch"], ["", "def", "unify_sentence", "(", "self", ",", "sentence_feature", ",", "one_sentence_embedding", ")", ":", "\n", "        ", "\"\"\"\n            Unify Sentence By Token Importance\n        \"\"\"", "\n", "sent_len", "=", "one_sentence_embedding", ".", "size", "(", ")", "[", "0", "]", "\n", "\n", "var_token", "=", "torch", ".", "zeros", "(", "sent_len", ",", "device", "=", "one_sentence_embedding", ".", "device", ")", "\n", "for", "token_index", "in", "range", "(", "sent_len", ")", ":", "\n", "            ", "token_feature", "=", "sentence_feature", "[", ":", ",", "token_index", ",", ":", "]", "\n", "sim_map", "=", "self", ".", "cosine_similarity_torch", "(", "token_feature", ")", "\n", "var_token", "[", "token_index", "]", "=", "torch", ".", "var", "(", "sim_map", ".", "diagonal", "(", "-", "1", ")", ")", "\n", "\n", "", "var_token", "=", "var_token", "/", "torch", ".", "sum", "(", "var_token", ")", "\n", "sentence_embedding", "=", "torch", ".", "mv", "(", "one_sentence_embedding", ".", "t", "(", ")", ",", "var_token", ")", "\n", "\n", "return", "sentence_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.cosine_similarity_torch": [[116, 121], ["x1.norm", "x2.norm", "torch.mm", "x2.t", "w2.t"], "methods", ["None"], ["", "def", "cosine_similarity_torch", "(", "self", ",", "x1", ",", "x2", "=", "None", ",", "eps", "=", "1e-8", ")", ":", "\n", "        ", "x2", "=", "x1", "if", "x2", "is", "None", "else", "x2", "\n", "w1", "=", "x1", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "w2", "=", "w1", "if", "x2", "is", "x1", "else", "x2", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "torch", ".", "mm", "(", "x1", ",", "x2", ".", "t", "(", ")", ")", "/", "(", "w1", "*", "w2", ".", "t", "(", ")", ")", ".", "clamp", "(", "min", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.get_sentence_embedding_dimension": [[122, 124], ["None"], "methods", ["None"], ["", "def", "get_sentence_embedding_dimension", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pooling_output_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.get_config_dict": [[125, 127], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.save": [[128, 131], ["open", "json.dump", "os.path.join", "WKPooling.WKPooling.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WKPooling.WKPooling.load": [[132, 138], ["WKPooling.WKPooling", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "return", "WKPooling", "(", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.__init__": [[17, 33], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "len", "int", "torch.nn.Conv1d", "CNN.CNN.convs.append"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "in_word_embedding_dimension", ":", "int", ",", "out_channels", ":", "int", "=", "256", ",", "kernel_sizes", ":", "List", "[", "int", "]", "=", "[", "1", ",", "3", ",", "5", "]", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "config_keys", "=", "[", "'in_word_embedding_dimension'", ",", "'out_channels'", ",", "'kernel_sizes'", "]", "\n", "self", ".", "in_word_embedding_dimension", "=", "in_word_embedding_dimension", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_sizes", "=", "kernel_sizes", "\n", "\n", "self", ".", "embeddings_dimension", "=", "out_channels", "*", "len", "(", "kernel_sizes", ")", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "in_channels", "=", "in_word_embedding_dimension", "\n", "for", "kernel_size", "in", "kernel_sizes", ":", "\n", "            ", "padding_size", "=", "int", "(", "(", "kernel_size", "-", "1", ")", "/", "2", ")", "\n", "conv", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "padding_size", ")", "\n", "self", ".", "convs", ".", "append", "(", "conv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.forward": [[34, 43], ["token_embeddings.transpose.transpose.transpose", "torch.cat().transpose", "features.update", "conv", "torch.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "token_embeddings", "=", "features", "[", "'token_embeddings'", "]", "\n", "\n", "token_embeddings", "=", "token_embeddings", ".", "transpose", "(", "1", ",", "-", "1", ")", "\n", "vectors", "=", "[", "conv", "(", "token_embeddings", ")", "for", "conv", "in", "self", ".", "convs", "]", "\n", "out", "=", "torch", ".", "cat", "(", "vectors", ",", "1", ")", ".", "transpose", "(", "1", ",", "-", "1", ")", "\n", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "out", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.get_word_embedding_dimension": [[44, 46], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "embeddings_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.tokenize": [[47, 49], ["NotImplementedError"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.save": [[50, 55], ["torch.save", "open", "json.dump", "CNN.CNN.state_dict", "os.path.join", "os.path.join", "CNN.CNN.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'cnn_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n", "", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.get_config_dict": [[56, 58], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.CNN.CNN.load": [[59, 68], ["torch.load", "CNN.CNN", "CNN.load_state_dict", "open", "json.load", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'cnn_config.json'", ")", ",", "'r'", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "weights", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "model", "=", "CNN", "(", "**", "config", ")", "\n", "model", ".", "load_state_dict", "(", "weights", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Dense.Dense.__init__": [[21, 28], ["torch.nn.Tanh", "torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ":", "int", ",", "out_features", ":", "int", ",", "bias", ":", "bool", "=", "True", ",", "activation_function", "=", "nn", ".", "Tanh", "(", ")", ")", ":", "\n", "        ", "super", "(", "Dense", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "activation_function", "=", "activation_function", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Dense.Dense.forward": [[29, 32], ["features.update", "Dense.Dense.activation_function", "Dense.Dense.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", ":", "\n", "        ", "features", ".", "update", "(", "{", "'sentence_embedding'", ":", "self", ".", "activation_function", "(", "self", ".", "linear", "(", "features", "[", "'sentence_embedding'", "]", ")", ")", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Dense.Dense.get_sentence_embedding_dimension": [[33, 35], ["None"], "methods", ["None"], ["", "def", "get_sentence_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "out_features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Dense.Dense.save": [[36, 41], ["torch.save", "open", "json.dump", "Dense.Dense.state_dict", "os.path.join", "os.path.join", "util.fullname"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.fullname"], ["", "def", "save", "(", "self", ",", "output_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'in_features'", ":", "self", ".", "in_features", ",", "'out_features'", ":", "self", ".", "out_features", ",", "'bias'", ":", "self", ".", "bias", ",", "'activation_function'", ":", "fullname", "(", "self", ".", "activation_function", ")", "}", ",", "fOut", ")", "\n", "\n", "", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Dense.Dense.load": [[42, 51], ["Dense.Dense", "Dense.load_state_dict", "open", "json.load", "util.import_from_string", "torch.load", "os.path.join", "os.path.join", "torch.device"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.import_from_string", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "config", "[", "'activation_function'", "]", "=", "import_from_string", "(", "config", "[", "'activation_function'", "]", ")", "(", ")", "\n", "model", "=", "Dense", "(", "**", "config", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'pytorch_model.bin'", ")", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.__init__": [[15, 31], ["torch.nn.Module.__init__", "isinstance", "isinstance", "torch.from_numpy.size", "torch.nn.Embedding", "WordEmbeddings.WordEmbeddings.emb_layer.load_state_dict", "numpy.asarray", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "WordTokenizer", ",", "embedding_weights", ",", "update_embeddings", ":", "bool", "=", "False", ",", "max_seq_length", ":", "int", "=", "1000000", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "if", "isinstance", "(", "embedding_weights", ",", "list", ")", ":", "\n", "            ", "embedding_weights", "=", "np", ".", "asarray", "(", "embedding_weights", ")", "\n", "\n", "", "if", "isinstance", "(", "embedding_weights", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "embedding_weights", "=", "torch", ".", "from_numpy", "(", "embedding_weights", ")", "\n", "\n", "", "num_embeddings", ",", "embeddings_dimension", "=", "embedding_weights", ".", "size", "(", ")", "\n", "self", ".", "embeddings_dimension", "=", "embeddings_dimension", "\n", "self", ".", "emb_layer", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embeddings_dimension", ")", "\n", "self", ".", "emb_layer", ".", "load_state_dict", "(", "{", "'weight'", ":", "embedding_weights", "}", ")", "\n", "self", ".", "emb_layer", ".", "weight", ".", "requires_grad", "=", "update_embeddings", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "update_embeddings", "=", "update_embeddings", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.forward": [[32, 37], ["WordEmbeddings.WordEmbeddings.emb_layer", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "token_embeddings", "=", "self", ".", "emb_layer", "(", "features", "[", "'input_ids'", "]", ")", "\n", "cls_tokens", "=", "None", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "token_embeddings", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.get_sentence_features": [[38, 56], ["min", "len", "len", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "\n", "\n", "tokens", "=", "tokens", "[", "0", ":", "pad_seq_length", "]", "#Truncate tokens if needed", "\n", "input_ids", "=", "tokens", "\n", "\n", "sentence_length", "=", "len", "(", "input_ids", ")", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "padding", "=", "[", "0", "]", "*", "(", "pad_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "attention_mask", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "pad_seq_length", "\n", "assert", "len", "(", "attention_mask", ")", "==", "pad_seq_length", "\n", "\n", "return", "{", "'input_ids'", ":", "torch", ".", "tensor", "(", "[", "input_ids", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "'attention_mask'", ":", "torch", ".", "tensor", "(", "[", "attention_mask", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "'sentence_lengths'", ":", "torch", ".", "tensor", "(", "[", "sentence_length", "]", ",", "dtype", "=", "torch", ".", "long", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.get_word_embedding_dimension": [[57, 59], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "embeddings_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.tokenize": [[60, 62], ["WordEmbeddings.WordEmbeddings.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.save": [[63, 69], ["torch.save", "WordEmbeddings.WordEmbeddings.tokenizer.save", "open", "json.dump", "WordEmbeddings.WordEmbeddings.state_dict", "os.path.join", "os.path.join", "WordEmbeddings.WordEmbeddings.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'wordembedding_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n", "", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "self", ".", "tokenizer", ".", "save", "(", "output_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.get_config_dict": [[70, 72], ["util.fullname"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.fullname"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "'tokenizer_class'", ":", "fullname", "(", "self", ".", "tokenizer", ")", ",", "'update_embeddings'", ":", "self", ".", "update_embeddings", ",", "'max_seq_length'", ":", "self", ".", "max_seq_length", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.load": [[73, 84], ["util.import_from_string", "util.import_from_string.load", "torch.load", "WordEmbeddings.WordEmbeddings", "open", "json.load", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.import_from_string", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'wordembedding_config.json'", ")", ",", "'r'", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "tokenizer_class", "=", "import_from_string", "(", "config", "[", "'tokenizer_class'", "]", ")", "\n", "tokenizer", "=", "tokenizer_class", ".", "load", "(", "input_path", ")", "\n", "weights", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "embedding_weights", "=", "weights", "[", "'emb_layer.weight'", "]", "\n", "model", "=", "WordEmbeddings", "(", "tokenizer", "=", "tokenizer", ",", "embedding_weights", "=", "embedding_weights", ",", "update_embeddings", "=", "config", "[", "'update_embeddings'", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordEmbeddings.WordEmbeddings.from_text_file": [[85, 128], ["tokenizer.WhitespaceTokenizer", "logging.info", "os.path.exists", "logging.info", "util.http_get", "tqdm.tqdm.tqdm", "numpy.asarray", "tokenizer.set_vocab", "WordEmbeddings.WordEmbeddings", "ValueError", "embeddings_file_path.endswith", "gzip.open", "open", "line.rstrip().split", "numpy.array", "numpy.asarray.append", "vocab.append", "vocab.append", "numpy.asarray.append", "logging.error", "line.rstrip", "len", "numpy.zeros", "len", "float", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.http_get", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.set_vocab"], ["", "@", "staticmethod", "\n", "def", "from_text_file", "(", "embeddings_file_path", ":", "str", ",", "update_embeddings", ":", "bool", "=", "False", ",", "item_separator", ":", "str", "=", "\" \"", ",", "tokenizer", "=", "WhitespaceTokenizer", "(", ")", ",", "max_vocab_size", ":", "int", "=", "None", ")", ":", "\n", "        ", "logging", ".", "info", "(", "\"Read in embeddings file {}\"", ".", "format", "(", "embeddings_file_path", ")", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "embeddings_file_path", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\"{} does not exist, try to download from server\"", ".", "format", "(", "embeddings_file_path", ")", ")", "\n", "\n", "if", "'/'", "in", "embeddings_file_path", "or", "'\\\\'", "in", "embeddings_file_path", ":", "\n", "                ", "raise", "ValueError", "(", "\"Embeddings file not found: \"", ".", "format", "(", "embeddings_file_path", ")", ")", "\n", "\n", "", "url", "=", "\"https://public.ukp.informatik.tu-darmstadt.de/reimers/embeddings/\"", "+", "embeddings_file_path", "\n", "http_get", "(", "url", ",", "embeddings_file_path", ")", "\n", "\n", "", "embeddings_dimension", "=", "None", "\n", "vocab", "=", "[", "]", "\n", "embeddings", "=", "[", "]", "\n", "\n", "with", "gzip", ".", "open", "(", "embeddings_file_path", ",", "\"rt\"", ",", "encoding", "=", "\"utf8\"", ")", "if", "embeddings_file_path", ".", "endswith", "(", "'.gz'", ")", "else", "open", "(", "embeddings_file_path", ",", "encoding", "=", "\"utf8\"", ")", "as", "fIn", ":", "\n", "            ", "iterator", "=", "tqdm", "(", "fIn", ",", "desc", "=", "\"Load Word Embeddings\"", ",", "unit", "=", "\"Embeddings\"", ")", "\n", "for", "line", "in", "iterator", ":", "\n", "                ", "split", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "item_separator", ")", "\n", "word", "=", "split", "[", "0", "]", "\n", "\n", "if", "embeddings_dimension", "==", "None", ":", "\n", "                    ", "embeddings_dimension", "=", "len", "(", "split", ")", "-", "1", "\n", "vocab", ".", "append", "(", "\"PADDING_TOKEN\"", ")", "\n", "embeddings", ".", "append", "(", "np", ".", "zeros", "(", "embeddings_dimension", ")", ")", "\n", "\n", "", "if", "(", "len", "(", "split", ")", "-", "1", ")", "!=", "embeddings_dimension", ":", "# Assure that all lines in the embeddings file are of the same length", "\n", "                    ", "logging", ".", "error", "(", "\"ERROR: A line in the embeddings file had more or less  dimensions than expected. Skip token.\"", ")", "\n", "continue", "\n", "\n", "", "vector", "=", "np", ".", "array", "(", "[", "float", "(", "num", ")", "for", "num", "in", "split", "[", "1", ":", "]", "]", ")", "\n", "embeddings", ".", "append", "(", "vector", ")", "\n", "vocab", ".", "append", "(", "word", ")", "\n", "\n", "if", "max_vocab_size", "is", "not", "None", "and", "max_vocab_size", ">", "0", "and", "len", "(", "vocab", ")", ">", "max_vocab_size", ":", "\n", "                    ", "break", "\n", "\n", "", "", "embeddings", "=", "np", ".", "asarray", "(", "embeddings", ")", "\n", "\n", "tokenizer", ".", "set_vocab", "(", "vocab", ")", "\n", "return", "WordEmbeddings", "(", "tokenizer", "=", "tokenizer", ",", "embedding_weights", "=", "embeddings", ",", "update_embeddings", "=", "update_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.__init__": [[19, 31], ["torch.nn.Module.__init__", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "\n", "model_args", ":", "Dict", "=", "{", "}", ",", "cache_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "Transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", "]", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ",", "cache_dir", "=", "cache_dir", ")", "\n", "self", ".", "auto_model", "=", "AutoModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "config", "=", "config", ",", "cache_dir", "=", "cache_dir", ")", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "**", "tokenizer_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.forward": [[33, 50], ["Transformer.Transformer.auto_model", "features.update", "features.update", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "output_states", "=", "self", ".", "auto_model", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "self", ".", "auto_model", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "all_layer_idx", "=", "2", "\n", "if", "len", "(", "output_states", ")", "<", "3", ":", "#Some models only output last_hidden_states and all_hidden_states", "\n", "                ", "all_layer_idx", "=", "1", "\n", "\n", "", "hidden_states", "=", "output_states", "[", "all_layer_idx", "]", "\n", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "hidden_states", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.get_word_embedding_dimension": [[51, 53], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "auto_model", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.tokenize": [[54, 59], ["Transformer.Transformer.tokenizer.convert_tokens_to_ids", "Transformer.Transformer.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.get_sentence_features": [[60, 72], ["Transformer.Transformer.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "3", "#Add space for special tokens", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.get_config_dict": [[73, 75], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.save": [[76, 82], ["Transformer.Transformer.auto_model.save_pretrained", "Transformer.Transformer.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "Transformer.Transformer.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "auto_model", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_bert_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Transformer.Transformer.load": [[83, 88], ["Transformer.Transformer", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_bert_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "Transformer", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.__init__": [[15, 30], ["torch.nn.Module.__init__", "transformers.XLMRobertaModel.from_pretrained", "transformers.XLMRobertaTokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "XLMRoBERTa", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "xlm_roberta", "=", "XLMRobertaModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "XLMRobertaTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "\n", "if", "max_seq_length", ">", "self", ".", "tokenizer", ".", "max_len_single_sentence", ":", "\n", "            ", "logging", ".", "warning", "(", "\"XLM-RoBERTa only allows a max_seq_length of \"", "+", "self", ".", "tokenizer", ".", "max_len_single_sentence", ")", "\n", "max_seq_length", "=", "self", ".", "tokenizer", ".", "max_len_single_sentence", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.forward": [[32, 45], ["XLMRoBERTa.XLMRoBERTa.xlm_roberta", "features.update", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "#RoBERTa does not use token_type_ids", "\n", "output_states", "=", "self", ".", "xlm_roberta", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "self", ".", "xlm_roberta", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "hidden_states", "=", "output_states", "[", "2", "]", "\n", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "hidden_states", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.get_word_embedding_dimension": [[46, 48], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "xlm_roberta", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.tokenize": [[49, 54], ["XLMRoBERTa.XLMRoBERTa.tokenizer.convert_tokens_to_ids", "XLMRoBERTa.XLMRoBERTa.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.get_sentence_features": [[55, 67], ["XLMRoBERTa.XLMRoBERTa.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "2", "#Add space for special tokens", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.get_config_dict": [[68, 70], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.save": [[71, 77], ["XLMRoBERTa.XLMRoBERTa.xlm_roberta.save_pretrained", "XLMRoBERTa.XLMRoBERTa.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "XLMRoBERTa.XLMRoBERTa.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "xlm_roberta", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_xlm-roberta_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLMRoBERTa.XLMRoBERTa.load": [[78, 83], ["XLMRoBERTa.XLMRoBERTa", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_xlm-roberta_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "XLMRoBERTa", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.__init__": [[15, 30], ["torch.nn.Module.__init__", "transformers.AlbertModel.from_pretrained", "transformers.AlbertTokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "ALBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "max_seq_length", ">", "510", ":", "\n", "            ", "logging", ".", "warning", "(", "\"BERT only allows a max_seq_length of 510 (512 with special tokens). Value will be set to 510\"", ")", "\n", "max_seq_length", "=", "510", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "albert", "=", "AlbertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "AlbertTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.forward": [[31, 43], ["ALBERT.ALBERT.albert", "features.update", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "output_states", "=", "self", ".", "albert", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "self", ".", "albert", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "hidden_states", "=", "output_states", "[", "2", "]", "\n", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "hidden_states", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.get_word_embedding_dimension": [[44, 46], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "albert", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.tokenize": [[47, 52], ["ALBERT.ALBERT.tokenizer.convert_tokens_to_ids", "ALBERT.ALBERT.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.get_sentence_features": [[53, 65], ["ALBERT.ALBERT.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "3", "#Add space for special tokens", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.get_config_dict": [[67, 69], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.save": [[70, 76], ["ALBERT.ALBERT.albert.save_pretrained", "ALBERT.ALBERT.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "ALBERT.ALBERT.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "albert", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_albert_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.ALBERT.ALBERT.load": [[77, 82], ["ALBERT.ALBERT", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_albert_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "ALBERT", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.__init__": [[13, 27], ["torch.nn.Module.__init__", "torch.nn.LSTM"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "word_embedding_dimension", ":", "int", ",", "hidden_dim", ":", "int", ",", "num_layers", ":", "int", "=", "1", ",", "dropout", ":", "float", "=", "0", ",", "bidirectional", ":", "bool", "=", "True", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "config_keys", "=", "[", "'word_embedding_dimension'", ",", "'hidden_dim'", ",", "'num_layers'", ",", "'dropout'", ",", "'bidirectional'", "]", "\n", "self", ".", "word_embedding_dimension", "=", "word_embedding_dimension", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "\n", "self", ".", "embeddings_dimension", "=", "hidden_dim", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "embeddings_dimension", "*=", "2", "\n", "\n", "", "self", ".", "encoder", "=", "nn", ".", "LSTM", "(", "word_embedding_dimension", ",", "hidden_dim", ",", "num_layers", "=", "num_layers", ",", "dropout", "=", "dropout", ",", "bidirectional", "=", "bidirectional", ",", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.forward": [[28, 37], ["torch.clamp", "torch.nn.utils.rnn.pack_padded_sequence", "LSTM.LSTM.encoder", "features.update", "torch.nn.utils.rnn.pad_packed_sequence"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "token_embeddings", "=", "features", "[", "'token_embeddings'", "]", "\n", "sentence_lengths", "=", "torch", ".", "clamp", "(", "features", "[", "'sentence_lengths'", "]", ",", "min", "=", "1", ")", "\n", "\n", "packed", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "token_embeddings", ",", "sentence_lengths", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", "\n", "packed", "=", "self", ".", "encoder", "(", "packed", ")", "\n", "unpack", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed", "[", "0", "]", ",", "batch_first", "=", "True", ")", "[", "0", "]", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "unpack", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.get_word_embedding_dimension": [[38, 40], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "embeddings_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.tokenize": [[41, 43], ["NotImplementedError"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.save": [[44, 49], ["torch.save", "open", "json.dump", "LSTM.LSTM.state_dict", "os.path.join", "os.path.join", "LSTM.LSTM.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'lstm_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n", "", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.get_config_dict": [[50, 52], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.LSTM.LSTM.load": [[53, 62], ["torch.load", "LSTM.LSTM", "LSTM.load_state_dict", "open", "json.load", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'lstm_config.json'", ")", ",", "'r'", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "weights", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "model", "=", "LSTM", "(", "**", "config", ")", "\n", "model", ".", "load_state_dict", "(", "weights", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WeightedLayerPooling.WeightedLayerPooling.__init__": [[17, 24], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "word_embedding_dimension", ",", "num_hidden_layers", ":", "int", "=", "12", ",", "layer_start", ":", "int", "=", "4", ",", "layer_weights", "=", "None", ")", ":", "\n", "        ", "super", "(", "WeightedLayerPooling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'word_embedding_dimension'", ",", "'layer_start'", ",", "'num_hidden_layers'", "]", "\n", "self", ".", "word_embedding_dimension", "=", "word_embedding_dimension", "\n", "self", ".", "layer_start", "=", "layer_start", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "layer_weights", "=", "layer_weights", "if", "layer_weights", "is", "not", "None", "else", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "[", "1", "]", "*", "(", "num_hidden_layers", "+", "1", "-", "layer_start", ")", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WeightedLayerPooling.WeightedLayerPooling.forward": [[25, 36], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "WeightedLayerPooling.WeightedLayerPooling.layer_weights.unsqueeze().unsqueeze().unsqueeze().expand", "features.update", "torch.stack.size", "torch.stack.size", "WeightedLayerPooling.WeightedLayerPooling.layer_weights.sum", "WeightedLayerPooling.WeightedLayerPooling.layer_weights.unsqueeze().unsqueeze().unsqueeze", "WeightedLayerPooling.WeightedLayerPooling.layer_weights.unsqueeze().unsqueeze", "WeightedLayerPooling.WeightedLayerPooling.layer_weights.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", ":", "\n", "        ", "ft_all_layers", "=", "features", "[", "'all_layer_embeddings'", "]", "\n", "\n", "all_layer_embedding", "=", "torch", ".", "stack", "(", "ft_all_layers", ")", "\n", "all_layer_embedding", "=", "all_layer_embedding", "[", "self", ".", "layer_start", ":", ",", ":", ",", ":", ",", ":", "]", "# Start from 4th layers output", "\n", "\n", "weight_factor", "=", "self", ".", "layer_weights", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "all_layer_embedding", ".", "size", "(", ")", ")", "\n", "weighted_average", "=", "(", "weight_factor", "*", "all_layer_embedding", ")", ".", "sum", "(", "dim", "=", "0", ")", "/", "self", ".", "layer_weights", ".", "sum", "(", ")", "\n", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "weighted_average", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WeightedLayerPooling.WeightedLayerPooling.get_word_embedding_dimension": [[37, 39], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "word_embedding_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WeightedLayerPooling.WeightedLayerPooling.get_config_dict": [[40, 42], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WeightedLayerPooling.WeightedLayerPooling.save": [[43, 48], ["torch.save", "torch.save", "torch.save", "torch.save", "open", "json.dump", "WeightedLayerPooling.WeightedLayerPooling.state_dict", "os.path.join", "os.path.join", "WeightedLayerPooling.WeightedLayerPooling.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n", "", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_path", ",", "'pytorch_model.bin'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WeightedLayerPooling.WeightedLayerPooling.load": [[50, 58], ["WeightedLayerPooling.WeightedLayerPooling", "WeightedLayerPooling.load_state_dict", "open", "json.load", "torch.load", "torch.load", "torch.load", "torch.load", "os.path.join", "os.path.join", "torch.device", "torch.device", "torch.device", "torch.device"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.device"], ["", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "model", "=", "WeightedLayerPooling", "(", "**", "config", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'pytorch_model.bin'", ")", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Pooling.Pooling.__init__": [[21, 40], ["torch.nn.Module.__init__", "sum"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "\n", "word_embedding_dimension", ":", "int", ",", "\n", "pooling_mode_cls_token", ":", "bool", "=", "False", ",", "\n", "pooling_mode_max_tokens", ":", "bool", "=", "False", ",", "\n", "pooling_mode_mean_tokens", ":", "bool", "=", "True", ",", "\n", "pooling_mode_mean_sqrt_len_tokens", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", "Pooling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "config_keys", "=", "[", "'word_embedding_dimension'", ",", "'pooling_mode_cls_token'", ",", "'pooling_mode_mean_tokens'", ",", "'pooling_mode_max_tokens'", ",", "'pooling_mode_mean_sqrt_len_tokens'", "]", "\n", "\n", "self", ".", "word_embedding_dimension", "=", "word_embedding_dimension", "\n", "self", ".", "pooling_mode_cls_token", "=", "pooling_mode_cls_token", "\n", "self", ".", "pooling_mode_mean_tokens", "=", "pooling_mode_mean_tokens", "\n", "self", ".", "pooling_mode_max_tokens", "=", "pooling_mode_max_tokens", "\n", "self", ".", "pooling_mode_mean_sqrt_len_tokens", "=", "pooling_mode_mean_sqrt_len_tokens", "\n", "\n", "pooling_mode_multiplier", "=", "sum", "(", "[", "pooling_mode_cls_token", ",", "pooling_mode_max_tokens", ",", "pooling_mode_mean_tokens", ",", "pooling_mode_mean_sqrt_len_tokens", "]", ")", "\n", "self", ".", "pooling_output_dimension", "=", "(", "pooling_mode_multiplier", "*", "word_embedding_dimension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Pooling.Pooling.forward": [[41, 75], ["torch.cat", "features.update", "output_vectors.append", "attention_mask.unsqueeze().expand().float", "output_vectors.append", "attention_mask.unsqueeze().expand().float", "torch.sum", "torch.clamp", "torch.max", "features[].unsqueeze().expand", "attention_mask.unsqueeze().expand().float.sum", "output_vectors.append", "output_vectors.append", "attention_mask.unsqueeze().expand", "attention_mask.unsqueeze().expand", "torch.sum.size", "token_embeddings.size", "token_embeddings.size", "features[].unsqueeze", "torch.sqrt", "attention_mask.unsqueeze", "attention_mask.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", ":", "\n", "        ", "token_embeddings", "=", "features", "[", "'token_embeddings'", "]", "\n", "cls_token", "=", "features", "[", "'cls_token_embeddings'", "]", "\n", "attention_mask", "=", "features", "[", "'attention_mask'", "]", "\n", "\n", "## Pooling strategy", "\n", "output_vectors", "=", "[", "]", "\n", "if", "self", ".", "pooling_mode_cls_token", ":", "\n", "            ", "output_vectors", ".", "append", "(", "cls_token", ")", "\n", "", "if", "self", ".", "pooling_mode_max_tokens", ":", "\n", "            ", "input_mask_expanded", "=", "attention_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "token_embeddings", ".", "size", "(", ")", ")", ".", "float", "(", ")", "\n", "token_embeddings", "[", "input_mask_expanded", "==", "0", "]", "=", "-", "1e9", "# Set padding tokens to large negative value", "\n", "max_over_time", "=", "torch", ".", "max", "(", "token_embeddings", ",", "1", ")", "[", "0", "]", "\n", "output_vectors", ".", "append", "(", "max_over_time", ")", "\n", "", "if", "self", ".", "pooling_mode_mean_tokens", "or", "self", ".", "pooling_mode_mean_sqrt_len_tokens", ":", "\n", "            ", "input_mask_expanded", "=", "attention_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "token_embeddings", ".", "size", "(", ")", ")", ".", "float", "(", ")", "\n", "sum_embeddings", "=", "torch", ".", "sum", "(", "token_embeddings", "*", "input_mask_expanded", ",", "1", ")", "\n", "\n", "#If tokens are weighted (by WordWeights layer), feature 'token_weights_sum' will be present", "\n", "if", "'token_weights_sum'", "in", "features", ":", "\n", "                ", "sum_mask", "=", "features", "[", "'token_weights_sum'", "]", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "sum_embeddings", ".", "size", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "sum_mask", "=", "input_mask_expanded", ".", "sum", "(", "1", ")", "\n", "\n", "", "sum_mask", "=", "torch", ".", "clamp", "(", "sum_mask", ",", "min", "=", "1e-9", ")", "\n", "\n", "if", "self", ".", "pooling_mode_mean_tokens", ":", "\n", "                ", "output_vectors", ".", "append", "(", "sum_embeddings", "/", "sum_mask", ")", "\n", "", "if", "self", ".", "pooling_mode_mean_sqrt_len_tokens", ":", "\n", "                ", "output_vectors", ".", "append", "(", "sum_embeddings", "/", "torch", ".", "sqrt", "(", "sum_mask", ")", ")", "\n", "\n", "", "", "output_vector", "=", "torch", ".", "cat", "(", "output_vectors", ",", "1", ")", "\n", "features", ".", "update", "(", "{", "'sentence_embedding'", ":", "output_vector", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Pooling.Pooling.get_sentence_embedding_dimension": [[76, 78], ["None"], "methods", ["None"], ["", "def", "get_sentence_embedding_dimension", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pooling_output_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Pooling.Pooling.get_config_dict": [[79, 81], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Pooling.Pooling.save": [[82, 85], ["open", "json.dump", "os.path.join", "Pooling.Pooling.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.Pooling.Pooling.load": [[86, 92], ["Pooling.Pooling", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "return", "Pooling", "(", "**", "config", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.__init__": [[14, 30], ["torch.nn.Module.__init__", "transformers.T5Model.from_pretrained", "transformers.T5Tokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "task_identifier", ":", "str", "=", "'stsb sentence1: '", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "T5", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", ",", "'task_identifier'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "max_seq_length", ">", "512", ":", "\n", "            ", "logging", ".", "warning", "(", "\"T5 only allows a max_seq_length of 512. Value will be set to 512\"", ")", "\n", "max_seq_length", "=", "512", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "t5model", "=", "T5Model", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "T5Tokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "self", ".", "task_identifier", "=", "task_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.forward": [[31, 42], ["T5.T5.t5model.encoder", "features.update", "len", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "output_states", "=", "self", ".", "t5model", ".", "encoder", "(", "input_ids", "=", "features", "[", "'input_ids'", "]", ",", "attention_mask", "=", "features", "[", "'attention_mask'", "]", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", "}", ")", "\n", "\n", "if", "len", "(", "output_states", ")", ">", "1", ":", "\n", "            ", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "output_states", "[", "1", "]", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.get_word_embedding_dimension": [[43, 45], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "t5model", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.tokenize": [[46, 51], ["T5.T5.tokenizer.encode"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "encode", "(", "self", ".", "task_identifier", "+", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.get_sentence_features": [[52, 65], ["min", "T5.T5.tokenizer.prepare_for_model"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.get_config_dict": [[66, 68], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.save": [[69, 75], ["T5.T5.t5model.save_pretrained", "T5.T5.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "T5.T5.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "t5model", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_T5_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.T5.T5.load": [[76, 81], ["T5.T5", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_T5_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "T5", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.__init__": [[14, 30], ["torch.nn.Module.__init__", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "BERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "max_seq_length", ">", "510", ":", "\n", "            ", "logging", ".", "warning", "(", "\"BERT only allows a max_seq_length of 510 (512 with special tokens). Value will be set to 510\"", ")", "\n", "max_seq_length", "=", "510", "\n", "", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "model_args", "[", "\"output_hidden_states\"", "]", "=", "True", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.forward": [[32, 43], ["BERT.BERT.bert", "features.update", "len", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "output_states", "=", "self", ".", "bert", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "0", ",", ":", "]", "# CLS token is first token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "len", "(", "output_states", ")", ">", "2", ":", "\n", "            ", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "output_states", "[", "2", "]", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.get_word_embedding_dimension": [[44, 46], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "bert", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.tokenize": [[47, 52], ["BERT.BERT.tokenizer.convert_tokens_to_ids", "BERT.BERT.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.get_sentence_features": [[53, 66], ["BERT.BERT.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "2", "##Add Space for CLS + SEP token", "\n", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.get_config_dict": [[68, 70], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.save": [[71, 77], ["BERT.BERT.bert.save_pretrained", "BERT.BERT.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "BERT.BERT.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "bert", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_bert_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BERT.BERT.load": [[78, 83], ["BERT.BERT", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_bert_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "BERT", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.__init__": [[14, 27], ["torch.nn.Module.__init__", "transformers.XLNetModel.from_pretrained", "transformers.XLNetTokenizer.from_pretrained", "XLNet.XLNet.tokenizer.convert_tokens_to_ids", "XLNet.XLNet.tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model_name_or_path", ":", "str", ",", "max_seq_length", ":", "int", "=", "128", ",", "do_lower_case", ":", "Optional", "[", "bool", "]", "=", "None", ",", "model_args", ":", "Dict", "=", "{", "}", ",", "tokenizer_args", ":", "Dict", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "XLNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'max_seq_length'", ",", "'do_lower_case'", "]", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "if", "self", ".", "do_lower_case", "is", "not", "None", ":", "\n", "            ", "tokenizer_args", "[", "'do_lower_case'", "]", "=", "do_lower_case", "\n", "\n", "", "self", ".", "xlnet", "=", "XLNetModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "model_args", ")", "\n", "self", ".", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ",", "**", "tokenizer_args", ")", "\n", "self", ".", "cls_token_id", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "self", ".", "tokenizer", ".", "cls_token", "]", ")", "[", "0", "]", "\n", "self", ".", "sep_token_id", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "self", ".", "tokenizer", ".", "sep_token", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.forward": [[28, 40], ["XLNet.XLNet.xlnet", "features.update", "features.update"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Returns token_embeddings, cls_token\"\"\"", "\n", "output_states", "=", "self", ".", "xlnet", "(", "**", "features", ")", "\n", "output_tokens", "=", "output_states", "[", "0", "]", "\n", "cls_tokens", "=", "output_tokens", "[", ":", ",", "-", "1", ",", ":", "]", "# CLS token is the last token", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "output_tokens", ",", "'cls_token_embeddings'", ":", "cls_tokens", ",", "'attention_mask'", ":", "features", "[", "'attention_mask'", "]", "}", ")", "\n", "\n", "if", "self", ".", "xlnet", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "hidden_states", "=", "output_states", "[", "2", "]", "\n", "features", ".", "update", "(", "{", "'all_layer_embeddings'", ":", "hidden_states", "}", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.get_word_embedding_dimension": [[41, 43], ["None"], "methods", ["None"], ["", "def", "get_word_embedding_dimension", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "xlnet", ".", "config", ".", "d_model", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.tokenize": [[44, 49], ["XLNet.XLNet.tokenizer.convert_tokens_to_ids", "XLNet.XLNet.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"", "\n", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.get_sentence_features": [[50, 62], ["XLNet.XLNet.tokenizer.prepare_for_model", "min"], "methods", ["None"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", "->", "Dict", "[", "str", ",", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Convert tokenized sentence in its embedding ids, segment ids and mask\n\n        :param tokens:\n            a tokenized sentence\n        :param pad_seq_length:\n            the maximal length of the sequence. Cannot be greater than self.sentence_transformer_config.max_seq_length\n        :return: embedding ids, segment ids and mask for the sentence\n        \"\"\"", "\n", "pad_seq_length", "=", "min", "(", "pad_seq_length", ",", "self", ".", "max_seq_length", ")", "+", "3", "#Add space for special tokens", "\n", "return", "self", ".", "tokenizer", ".", "prepare_for_model", "(", "tokens", ",", "max_length", "=", "pad_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "'pt'", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.get_config_dict": [[63, 65], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.save": [[66, 72], ["XLNet.XLNet.xlnet.save_pretrained", "XLNet.XLNet.tokenizer.save_pretrained", "open", "json.dump", "os.path.join", "XLNet.XLNet.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "xlnet", ".", "save_pretrained", "(", "output_path", ")", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "output_path", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'sentence_xlnet_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.XLNet.XLNet.load": [[73, 78], ["XLNet.XLNet", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'sentence_xlnet_config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "", "return", "XLNet", "(", "model_name_or_path", "=", "input_path", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.__init__": [[17, 43], ["torch.nn.Module.__init__", "list", "logging.info", "tokenizer.WhitespaceTokenizer", "len", "set", "BoW.BoW.weights.append", "len", "set", "word.lower", "word.lower"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "vocab", ":", "List", "[", "str", "]", ",", "word_weights", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", ",", "unknown_word_weight", ":", "float", "=", "1", ",", "cumulative_term_frequency", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "BoW", ",", "self", ")", ".", "__init__", "(", ")", "\n", "vocab", "=", "list", "(", "set", "(", "vocab", ")", ")", "#Ensure vocab is unique", "\n", "self", ".", "config_keys", "=", "[", "'vocab'", ",", "'word_weights'", ",", "'unknown_word_weight'", ",", "'cumulative_term_frequency'", "]", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word_weights", "=", "word_weights", "\n", "self", ".", "unknown_word_weight", "=", "unknown_word_weight", "\n", "self", ".", "cumulative_term_frequency", "=", "cumulative_term_frequency", "\n", "\n", "#Maps wordIdx -> word weight", "\n", "self", ".", "weights", "=", "[", "]", "\n", "num_unknown_words", "=", "0", "\n", "for", "word", "in", "vocab", ":", "\n", "            ", "weight", "=", "unknown_word_weight", "\n", "if", "word", "in", "word_weights", ":", "\n", "                ", "weight", "=", "word_weights", "[", "word", "]", "\n", "", "elif", "word", ".", "lower", "(", ")", "in", "word_weights", ":", "\n", "                ", "weight", "=", "word_weights", "[", "word", ".", "lower", "(", ")", "]", "\n", "", "else", ":", "\n", "                ", "num_unknown_words", "+=", "1", "\n", "", "self", ".", "weights", ".", "append", "(", "weight", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"{} out of {} words without a weighting value. Set weight to {}\"", ".", "format", "(", "num_unknown_words", ",", "len", "(", "vocab", ")", ",", "unknown_word_weight", ")", ")", "\n", "\n", "self", ".", "tokenizer", "=", "WhitespaceTokenizer", "(", "vocab", ",", "stop_words", "=", "set", "(", ")", ",", "do_lower_case", "=", "False", ")", "\n", "self", ".", "sentence_embedding_dimension", "=", "len", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.forward": [[45, 48], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", ":", "\n", "#Nothing to do, everything is done in get_sentence_features", "\n", "        ", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.tokenize": [[49, 51], ["BoW.BoW.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_embedding_dimension": [[52, 54], ["None"], "methods", ["None"], ["", "def", "get_sentence_embedding_dimension", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sentence_embedding_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_features": [[55, 64], ["numpy.zeros", "BoW.BoW.get_sentence_embedding_dimension", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_sentence_embedding_dimension"], ["", "def", "get_sentence_features", "(", "self", ",", "tokens", ":", "List", "[", "int", "]", ",", "pad_seq_length", ":", "int", ")", ":", "\n", "        ", "vector", "=", "np", ".", "zeros", "(", "self", ".", "get_sentence_embedding_dimension", "(", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "cumulative_term_frequency", ":", "\n", "                ", "vector", "[", "token", "]", "+=", "self", ".", "weights", "[", "token", "]", "\n", "", "else", ":", "\n", "                ", "vector", "[", "token", "]", "=", "self", ".", "weights", "[", "token", "]", "\n", "\n", "", "", "return", "{", "'sentence_embedding'", ":", "torch", ".", "tensor", "(", "[", "vector", "]", ",", "dtype", "=", "torch", ".", "float", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.get_config_dict": [[65, 67], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.save": [[68, 71], ["open", "json.dump", "os.path.join", "BoW.BoW.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.BoW.BoW.load": [[72, 78], ["BoW.BoW", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "return", "BoW", "(", "**", "config", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.__init__": [[12, 44], ["torch.nn.Module.__init__", "logging.info", "torch.nn.Embedding", "WordWeights.WordWeights.emb_layer.load_state_dict", "weights.append", "len", "len", "torch.FloatTensor().unsqueeze", "word.lower", "torch.FloatTensor", "word.lower"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "vocab", ":", "List", "[", "str", "]", ",", "word_weights", ":", "Dict", "[", "str", ",", "float", "]", ",", "unknown_word_weight", ":", "float", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n\n        :param vocab:\n            Vocabulary of the tokenizer\n        :param word_weights:\n            Mapping of tokens to a float weight value. Words embeddings are multiplied by  this float value. Tokens in word_weights must not be equal to the vocab (can contain more or less values)\n        :param unknown_word_weight:\n            Weight for words in vocab, that do not appear in the word_weights lookup. These can be for example rare words in the vocab, where no weight exists.\n        \"\"\"", "\n", "super", "(", "WordWeights", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_keys", "=", "[", "'vocab'", ",", "'word_weights'", ",", "'unknown_word_weight'", "]", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word_weights", "=", "word_weights", "\n", "self", ".", "unknown_word_weight", "=", "unknown_word_weight", "\n", "\n", "weights", "=", "[", "]", "\n", "num_unknown_words", "=", "0", "\n", "for", "word", "in", "vocab", ":", "\n", "            ", "weight", "=", "unknown_word_weight", "\n", "if", "word", "in", "word_weights", ":", "\n", "                ", "weight", "=", "word_weights", "[", "word", "]", "\n", "", "elif", "word", ".", "lower", "(", ")", "in", "word_weights", ":", "\n", "                ", "weight", "=", "word_weights", "[", "word", ".", "lower", "(", ")", "]", "\n", "", "else", ":", "\n", "                ", "num_unknown_words", "+=", "1", "\n", "", "weights", ".", "append", "(", "weight", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"{} of {} words without a weighting value. Set weight to {}\"", ".", "format", "(", "num_unknown_words", ",", "len", "(", "vocab", ")", ",", "unknown_word_weight", ")", ")", "\n", "\n", "self", ".", "emb_layer", "=", "nn", ".", "Embedding", "(", "len", "(", "vocab", ")", ",", "1", ")", "\n", "self", ".", "emb_layer", ".", "load_state_dict", "(", "{", "'weight'", ":", "torch", ".", "FloatTensor", "(", "weights", ")", ".", "unsqueeze", "(", "1", ")", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.forward": [[46, 61], ["WordWeights.WordWeights.emb_layer().squeeze", "torch.sum", "token_weights.unsqueeze().expand", "features.update", "attention_mask.float", "token_embeddings.size", "WordWeights.WordWeights.emb_layer", "token_weights.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", ":", "\n", "        ", "attention_mask", "=", "features", "[", "'attention_mask'", "]", "\n", "token_embeddings", "=", "features", "[", "'token_embeddings'", "]", "\n", "\n", "#Compute a weight value for each token", "\n", "token_weights_raw", "=", "self", ".", "emb_layer", "(", "features", "[", "'input_ids'", "]", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "token_weights", "=", "token_weights_raw", "*", "attention_mask", ".", "float", "(", ")", "\n", "token_weights_sum", "=", "torch", ".", "sum", "(", "token_weights", ",", "1", ")", "\n", "\n", "#Multiply embedding by token weight value", "\n", "token_weights_expanded", "=", "token_weights", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "token_embeddings", ".", "size", "(", ")", ")", "\n", "token_embeddings", "=", "token_embeddings", "*", "token_weights_expanded", "\n", "\n", "features", ".", "update", "(", "{", "'token_embeddings'", ":", "token_embeddings", ",", "'token_weights_sum'", ":", "token_weights_sum", "}", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict": [[62, 64], ["None"], "methods", ["None"], ["", "def", "get_config_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "key", ":", "self", ".", "__dict__", "[", "key", "]", "for", "key", "in", "self", ".", "config_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.save": [[65, 68], ["open", "json.dump", "os.path.join", "WordWeights.WordWeights.get_config_dict"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.get_config_dict"], ["", "def", "save", "(", "self", ",", "output_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "get_config_dict", "(", ")", ",", "fOut", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.models.WordWeights.WordWeights.load": [[69, 75], ["WordWeights.WordWeights", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'config.json'", ")", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "return", "WordWeights", "(", "**", "config", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WhitespaceTokenizer.WhitespaceTokenizer.__init__": [[13, 17], ["set", "WhitespaceTokenizer.WhitespaceTokenizer.set_vocab"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.set_vocab"], ["def", "__init__", "(", "self", ",", "vocab", ":", "Iterable", "[", "str", "]", "=", "[", "]", ",", "stop_words", ":", "Iterable", "[", "str", "]", "=", "ENGLISH_STOP_WORDS", ",", "do_lower_case", ":", "bool", "=", "False", ")", ":", "\n", "        ", "self", ".", "stop_words", "=", "set", "(", "stop_words", ")", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "set_vocab", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WhitespaceTokenizer.WhitespaceTokenizer.get_vocab": [[18, 20], ["None"], "methods", ["None"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WhitespaceTokenizer.WhitespaceTokenizer.set_vocab": [[21, 24], ["collections.OrderedDict", "enumerate"], "methods", ["None"], ["", "def", "set_vocab", "(", "self", ",", "vocab", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word2idx", "=", "collections", ".", "OrderedDict", "(", "[", "(", "word", ",", "idx", ")", "for", "idx", ",", "word", "in", "enumerate", "(", "vocab", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WhitespaceTokenizer.WhitespaceTokenizer.tokenize": [[25, 54], ["text.lower.lower.split", "text.lower.lower.lower", "token.lower.lower.strip", "token.lower.lower.lower", "tokens_filtered.append", "tokens_filtered.append", "tokens_filtered.append", "len"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "if", "self", ".", "do_lower_case", ":", "\n", "            ", "text", "=", "text", ".", "lower", "(", ")", "\n", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "\n", "tokens_filtered", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "in", "self", ".", "stop_words", ":", "\n", "                ", "continue", "\n", "", "elif", "token", "in", "self", ".", "word2idx", ":", "\n", "                ", "tokens_filtered", ".", "append", "(", "self", ".", "word2idx", "[", "token", "]", ")", "\n", "continue", "\n", "\n", "", "token", "=", "token", ".", "strip", "(", "string", ".", "punctuation", ")", "\n", "if", "token", "in", "self", ".", "stop_words", ":", "\n", "                ", "continue", "\n", "", "elif", "len", "(", "token", ")", ">", "0", "and", "token", "in", "self", ".", "word2idx", ":", "\n", "                ", "tokens_filtered", ".", "append", "(", "self", ".", "word2idx", "[", "token", "]", ")", "\n", "continue", "\n", "\n", "", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "token", "in", "self", ".", "stop_words", ":", "\n", "                ", "continue", "\n", "", "elif", "token", "in", "self", ".", "word2idx", ":", "\n", "                ", "tokens_filtered", ".", "append", "(", "self", ".", "word2idx", "[", "token", "]", ")", "\n", "continue", "\n", "\n", "", "", "return", "tokens_filtered", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WhitespaceTokenizer.WhitespaceTokenizer.save": [[55, 58], ["open", "json.dump", "os.path.join", "list", "list", "WhitespaceTokenizer.WhitespaceTokenizer.word2idx.keys"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'whitespacetokenizer_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'vocab'", ":", "list", "(", "self", ".", "word2idx", ".", "keys", "(", ")", ")", ",", "'stop_words'", ":", "list", "(", "self", ".", "stop_words", ")", ",", "'do_lower_case'", ":", "self", ".", "do_lower_case", "}", ",", "fOut", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WhitespaceTokenizer.WhitespaceTokenizer.load": [[59, 65], ["WhitespaceTokenizer.WhitespaceTokenizer", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'whitespacetokenizer_config.json'", ")", ",", "'r'", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "return", "WhitespaceTokenizer", "(", "**", "config", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.PhraseTokenizer.PhraseTokenizer.__init__": [[16, 22], ["set", "PhraseTokenizer.PhraseTokenizer.set_vocab"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.set_vocab"], ["def", "__init__", "(", "self", ",", "vocab", ":", "Iterable", "[", "str", "]", "=", "[", "]", ",", "stop_words", ":", "Iterable", "[", "str", "]", "=", "ENGLISH_STOP_WORDS", ",", "do_lower_case", ":", "bool", "=", "False", ",", "ngram_separator", ":", "str", "=", "\"_\"", ",", "max_ngram_length", ":", "int", "=", "5", ")", ":", "\n", "        ", "self", ".", "stop_words", "=", "set", "(", "stop_words", ")", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "ngram_separator", "=", "ngram_separator", "\n", "self", ".", "max_ngram_length", "=", "max_ngram_length", "\n", "self", ".", "set_vocab", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.PhraseTokenizer.PhraseTokenizer.get_vocab": [[23, 25], ["None"], "methods", ["None"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.PhraseTokenizer.PhraseTokenizer.set_vocab": [[26, 45], ["collections.OrderedDict", "set", "set", "len", "logging.info", "logging.info", "enumerate", "word.count", "PhraseTokenizer.PhraseTokenizer.ngram_lookup.add", "PhraseTokenizer.PhraseTokenizer.ngram_lengths.add", "len"], "methods", ["None"], ["", "def", "set_vocab", "(", "self", ",", "vocab", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word2idx", "=", "collections", ".", "OrderedDict", "(", "[", "(", "word", ",", "idx", ")", "for", "idx", ",", "word", "in", "enumerate", "(", "vocab", ")", "]", ")", "\n", "\n", "# Check for ngram in vocab", "\n", "self", ".", "ngram_lookup", "=", "set", "(", ")", "\n", "self", ".", "ngram_lengths", "=", "set", "(", ")", "\n", "for", "word", "in", "vocab", ":", "\n", "\n", "            ", "if", "self", ".", "ngram_separator", "is", "not", "None", "and", "self", ".", "ngram_separator", "in", "word", ":", "\n", "# Sum words might me malformed in e.g. google news word2vec, containing two or more _ after each other", "\n", "                ", "ngram_count", "=", "word", ".", "count", "(", "self", ".", "ngram_separator", ")", "+", "1", "\n", "if", "self", ".", "ngram_separator", "+", "self", ".", "ngram_separator", "not", "in", "word", "and", "ngram_count", "<=", "self", ".", "max_ngram_length", ":", "\n", "                    ", "self", ".", "ngram_lookup", ".", "add", "(", "word", ")", "\n", "self", ".", "ngram_lengths", ".", "add", "(", "ngram_count", ")", "\n", "\n", "", "", "", "if", "len", "(", "vocab", ")", ">", "0", ":", "\n", "            ", "logging", ".", "info", "(", "\"PhraseTokenizer - Phrase ngram lengths: {}\"", ".", "format", "(", "self", ".", "ngram_lengths", ")", ")", "\n", "logging", ".", "info", "(", "\"PhraseTokenizer - Num phrases: {}\"", ".", "format", "(", "len", "(", "self", ".", "ngram_lookup", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.PhraseTokenizer.PhraseTokenizer.tokenize": [[46, 84], ["nltk.word_tokenize", "sorted", "token.strip.strip.lower", "token.strip.strip.strip", "PhraseTokenizer.PhraseTokenizer.ngram_separator.join", "len", "tokens_filtered.append", "tokens_filtered.append", "tokens_filtered.append", "PhraseTokenizer.PhraseTokenizer.lower", "len", "PhraseTokenizer.PhraseTokenizer.lower"], "methods", ["None"], ["", "", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "tokens", "=", "nltk", ".", "word_tokenize", "(", "text", ",", "preserve_line", "=", "True", ")", "\n", "\n", "#phrase detection", "\n", "for", "ngram_len", "in", "sorted", "(", "self", ".", "ngram_lengths", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "idx", "=", "0", "\n", "while", "idx", "<=", "len", "(", "tokens", ")", "-", "ngram_len", ":", "\n", "                ", "ngram", "=", "self", ".", "ngram_separator", ".", "join", "(", "tokens", "[", "idx", ":", "idx", "+", "ngram_len", "]", ")", "\n", "if", "ngram", "in", "self", ".", "ngram_lookup", ":", "\n", "                    ", "tokens", "[", "idx", ":", "idx", "+", "ngram_len", "]", "=", "[", "ngram", "]", "\n", "", "elif", "ngram", ".", "lower", "(", ")", "in", "self", ".", "ngram_lookup", ":", "\n", "                    ", "tokens", "[", "idx", ":", "idx", "+", "ngram_len", "]", "=", "[", "ngram", ".", "lower", "(", ")", "]", "\n", "", "idx", "+=", "1", "\n", "\n", "#Map tokens to idx, filter stop words", "\n", "", "", "tokens_filtered", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "in", "self", ".", "stop_words", ":", "\n", "                ", "continue", "\n", "", "elif", "token", "in", "self", ".", "word2idx", ":", "\n", "                ", "tokens_filtered", ".", "append", "(", "self", ".", "word2idx", "[", "token", "]", ")", "\n", "continue", "\n", "\n", "", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "token", "in", "self", ".", "stop_words", ":", "\n", "                ", "continue", "\n", "", "elif", "token", "in", "self", ".", "word2idx", ":", "\n", "                ", "tokens_filtered", ".", "append", "(", "self", ".", "word2idx", "[", "token", "]", ")", "\n", "continue", "\n", "\n", "", "token", "=", "token", ".", "strip", "(", "string", ".", "punctuation", ")", "\n", "if", "token", "in", "self", ".", "stop_words", ":", "\n", "                ", "continue", "\n", "", "elif", "len", "(", "token", ")", ">", "0", "and", "token", "in", "self", ".", "word2idx", ":", "\n", "                ", "tokens_filtered", ".", "append", "(", "self", ".", "word2idx", "[", "token", "]", ")", "\n", "continue", "\n", "\n", "", "", "return", "tokens_filtered", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.PhraseTokenizer.PhraseTokenizer.save": [[85, 88], ["open", "json.dump", "os.path.join", "list", "list", "PhraseTokenizer.PhraseTokenizer.word2idx.keys"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.None.make_eval_references_duc.dump"], ["", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_path", ",", "'phrasetokenizer_config.json'", ")", ",", "'w'", ")", "as", "fOut", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'vocab'", ":", "list", "(", "self", ".", "word2idx", ".", "keys", "(", ")", ")", ",", "'stop_words'", ":", "list", "(", "self", ".", "stop_words", ")", ",", "'do_lower_case'", ":", "self", ".", "do_lower_case", ",", "'ngram_separator'", ":", "self", ".", "ngram_separator", ",", "'max_ngram_length'", ":", "self", ".", "max_ngram_length", "}", ",", "fOut", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.PhraseTokenizer.PhraseTokenizer.load": [[89, 95], ["PhraseTokenizer.PhraseTokenizer", "open", "json.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_path", ",", "'phrasetokenizer_config.json'", ")", ",", "'r'", ")", "as", "fIn", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "fIn", ")", "\n", "\n", "", "return", "PhraseTokenizer", "(", "**", "config", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.set_vocab": [[8, 11], ["None"], "methods", ["None"], ["    ", "@", "abstractmethod", "\n", "def", "set_vocab", "(", "self", ",", "vocab", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.get_vocab": [[12, 15], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_vocab", "(", "self", ",", "vocab", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.tokenize": [[16, 19], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.save": [[20, 23], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "save", "(", "self", ",", "output_path", ":", "str", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.tokenizer.WordTokenizer.WordTokenizer.load": [[24, 28], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "abstractmethod", "\n", "def", "load", "(", "input_path", ":", "str", ")", ":", "\n", "        ", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.STSDataReader.STSDataReader.__init__": [[12, 23], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset_folder", ",", "s1_col_idx", "=", "0", ",", "s2_col_idx", "=", "1", ",", "score_col_idx", "=", "2", ",", "delimiter", "=", "\"\\t\"", ",", "\n", "quoting", "=", "csv", ".", "QUOTE_NONE", ",", "normalize_scores", "=", "True", ",", "min_score", "=", "0", ",", "max_score", "=", "5", ")", ":", "\n", "        ", "self", ".", "dataset_folder", "=", "dataset_folder", "\n", "self", ".", "score_col_idx", "=", "score_col_idx", "\n", "self", ".", "s1_col_idx", "=", "s1_col_idx", "\n", "self", ".", "s2_col_idx", "=", "s2_col_idx", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "self", ".", "quoting", "=", "quoting", "\n", "self", ".", "normalize_scores", "=", "normalize_scores", "\n", "self", ".", "min_score", "=", "min_score", "\n", "self", ".", "max_score", "=", "max_score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.STSDataReader.STSDataReader.get_examples": [[24, 45], ["os.path.join", "csv.reader", "enumerate", "filename.endswith", "gzip.open", "open", "float", "examples.append", "InputExample", "len", "str"], "methods", ["None"], ["", "def", "get_examples", "(", "self", ",", "filename", ",", "max_examples", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        filename specified which data split to use (train.csv, dev.csv, test.csv).\n        \"\"\"", "\n", "filepath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_folder", ",", "filename", ")", "\n", "with", "gzip", ".", "open", "(", "filepath", ",", "'rt'", ",", "encoding", "=", "'utf8'", ")", "if", "filename", ".", "endswith", "(", "'.gz'", ")", "else", "open", "(", "filepath", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fIn", ":", "\n", "            ", "data", "=", "csv", ".", "reader", "(", "fIn", ",", "delimiter", "=", "self", ".", "delimiter", ",", "quoting", "=", "self", ".", "quoting", ")", "\n", "examples", "=", "[", "]", "\n", "for", "id", ",", "row", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "score", "=", "float", "(", "row", "[", "self", ".", "score_col_idx", "]", ")", "\n", "if", "self", ".", "normalize_scores", ":", "# Normalize to a 0...1 value", "\n", "                    ", "score", "=", "(", "score", "-", "self", ".", "min_score", ")", "/", "(", "self", ".", "max_score", "-", "self", ".", "min_score", ")", "\n", "\n", "", "s1", "=", "row", "[", "self", ".", "s1_col_idx", "]", "\n", "s2", "=", "row", "[", "self", ".", "s2_col_idx", "]", "\n", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "filename", "+", "str", "(", "id", ")", ",", "texts", "=", "[", "s1", ",", "s2", "]", ",", "label", "=", "score", ")", ")", "\n", "\n", "if", "max_examples", ">", "0", "and", "len", "(", "examples", ")", ">=", "max_examples", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.STSDataReader.STSBenchmarkDataReader.__init__": [[51, 55], ["STSDataReader.STSDataReader.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "dataset_folder", ",", "s1_col_idx", "=", "5", ",", "s2_col_idx", "=", "6", ",", "score_col_idx", "=", "4", ",", "delimiter", "=", "\"\\t\"", ",", "\n", "quoting", "=", "csv", ".", "QUOTE_NONE", ",", "normalize_scores", "=", "True", ",", "min_score", "=", "0", ",", "max_score", "=", "5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset_folder", "=", "dataset_folder", ",", "s1_col_idx", "=", "s1_col_idx", ",", "s2_col_idx", "=", "s2_col_idx", ",", "score_col_idx", "=", "score_col_idx", ",", "delimiter", "=", "delimiter", ",", "\n", "quoting", "=", "quoting", ",", "normalize_scores", "=", "normalize_scores", ",", "min_score", "=", "min_score", ",", "max_score", "=", "max_score", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.InputExample.InputExample.__init__": [[8, 27], ["text.strip"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ":", "str", "=", "''", ",", "texts", ":", "List", "[", "str", "]", "=", "None", ",", "texts_tokenized", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "label", ":", "Union", "[", "int", ",", "float", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Creates one InputExample with the given texts, guid and label\n\n        str.strip() is called on both texts.\n\n        :param guid\n            id for the example\n        :param texts\n            the texts for the example\n        :param texts_tokenized\n            Optional: Texts that are already tokenized. If texts_tokenized is passed, texts must not be passed.\n        :param label\n            the label for the example\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "texts", "=", "[", "text", ".", "strip", "(", ")", "for", "text", "in", "texts", "]", "if", "texts", "is", "not", "None", "else", "texts", "\n", "self", ".", "texts_tokenized", "=", "texts_tokenized", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.InputExample.InputExample.__str__": [[28, 30], ["str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"<InputExample> label: {}, texts: {}\"", ".", "format", "(", "str", "(", "self", ".", "label", ")", ",", "\"; \"", ".", "join", "(", "self", ".", "texts", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.PairedFilesReader.PairedFilesReader.__init__": [[11, 13], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "filepaths", ")", ":", "\n", "        ", "self", ".", "filepaths", "=", "filepaths", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.PairedFilesReader.PairedFilesReader.get_examples": [[15, 45], ["fIns.append", "examples.append", "filepath.endswith", "gzip.open", "gzip.open", "gzip.open", "gzip.open", "open", "fIn.readline", "texts.append", "InputExample", "len", "str", "len"], "methods", ["None"], ["", "def", "get_examples", "(", "self", ",", "max_examples", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "fIns", "=", "[", "]", "\n", "for", "filepath", "in", "self", ".", "filepaths", ":", "\n", "            ", "fIn", "=", "gzip", ".", "open", "(", "filepath", ",", "'rt'", ",", "encoding", "=", "'utf-8'", ")", "if", "filepath", ".", "endswith", "(", "'.gz'", ")", "else", "open", "(", "filepath", ",", "encoding", "=", "'utf-8'", ")", "\n", "fIns", ".", "append", "(", "fIn", ")", "\n", "\n", "", "examples", "=", "[", "]", "\n", "\n", "eof", "=", "False", "\n", "while", "not", "eof", ":", "\n", "            ", "texts", "=", "[", "]", "\n", "for", "fIn", "in", "fIns", ":", "\n", "                ", "text", "=", "fIn", ".", "readline", "(", ")", "\n", "\n", "if", "text", "==", "''", ":", "\n", "                    ", "eof", "=", "True", "\n", "break", "\n", "\n", "", "texts", ".", "append", "(", "text", ")", "\n", "\n", "", "if", "eof", ":", "\n", "                ", "break", ";", "\n", "\n", "", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "str", "(", "len", "(", "examples", ")", ")", ",", "texts", "=", "texts", ",", "label", "=", "1", ")", ")", "\n", "if", "max_examples", ">", "0", "and", "len", "(", "examples", ")", ">=", "max_examples", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "examples", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.LabelSentenceReader.LabelSentenceReader.__init__": [[10, 15], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "folder", ",", "label_col_idx", "=", "0", ",", "sentence_col_idx", "=", "1", ")", ":", "\n", "        ", "self", ".", "folder", "=", "folder", "\n", "self", ".", "label_map", "=", "{", "}", "\n", "self", ".", "label_col_idx", "=", "label_col_idx", "\n", "self", ".", "sentence_col_idx", "=", "sentence_col_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.LabelSentenceReader.LabelSentenceReader.get_examples": [[16, 37], ["open", "os.path.join", "line.strip().split", "examples.append", "len", "InputExample", "line.strip"], "methods", ["None"], ["", "def", "get_examples", "(", "self", ",", "filename", ",", "max_examples", "=", "0", ")", ":", "\n", "        ", "examples", "=", "[", "]", "\n", "\n", "id", "=", "0", "\n", "for", "line", "in", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "folder", ",", "filename", ")", ",", "encoding", "=", "\"utf-8\"", ")", ":", "\n", "            ", "splits", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "splits", "[", "self", ".", "label_col_idx", "]", "\n", "sentence", "=", "splits", "[", "self", ".", "sentence_col_idx", "]", "\n", "\n", "if", "label", "not", "in", "self", ".", "label_map", ":", "\n", "                ", "self", ".", "label_map", "[", "label", "]", "=", "len", "(", "self", ".", "label_map", ")", "\n", "\n", "", "label_id", "=", "self", ".", "label_map", "[", "label", "]", "\n", "guid", "=", "\"%s-%d\"", "%", "(", "filename", ",", "id", ")", "\n", "id", "+=", "1", "\n", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "guid", ",", "texts", "=", "[", "sentence", "]", ",", "label", "=", "label_id", ")", ")", "\n", "\n", "if", "0", "<", "max_examples", "<=", "id", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "examples", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.__init__": [[11, 13], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset_folder", ")", ":", "\n", "        ", "self", ".", "dataset_folder", "=", "dataset_folder", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.get_examples": [[14, 38], ["gzip.open().readlines", "gzip.open().readlines", "gzip.open().readlines", "zip", "examples.append", "gzip.open", "gzip.open", "gzip.open", "InputExample", "len", "os.path.join", "os.path.join", "os.path.join", "NLIDataReader.NLIDataReader.map_label"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.map_label"], ["", "def", "get_examples", "(", "self", ",", "filename", ",", "max_examples", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        data_splits specified which data split to use (train, dev, test).\n        Expects that self.dataset_folder contains the files s1.$data_split.gz,  s2.$data_split.gz,\n        labels.$data_split.gz, e.g., for the train split, s1.train.gz, s2.train.gz, labels.train.gz\n        \"\"\"", "\n", "s1", "=", "gzip", ".", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_folder", ",", "'s1.'", "+", "filename", ")", ",", "\n", "mode", "=", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", ".", "readlines", "(", ")", "\n", "s2", "=", "gzip", ".", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_folder", ",", "'s2.'", "+", "filename", ")", ",", "\n", "mode", "=", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", ".", "readlines", "(", ")", "\n", "labels", "=", "gzip", ".", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_folder", ",", "'labels.'", "+", "filename", ")", ",", "\n", "mode", "=", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", ".", "readlines", "(", ")", "\n", "\n", "examples", "=", "[", "]", "\n", "id", "=", "0", "\n", "for", "sentence_a", ",", "sentence_b", ",", "label", "in", "zip", "(", "s1", ",", "s2", ",", "labels", ")", ":", "\n", "            ", "guid", "=", "\"%s-%d\"", "%", "(", "filename", ",", "id", ")", "\n", "id", "+=", "1", "\n", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "guid", ",", "texts", "=", "[", "sentence_a", ",", "sentence_b", "]", ",", "label", "=", "self", ".", "map_label", "(", "label", ")", ")", ")", "\n", "\n", "if", "0", "<", "max_examples", "<=", "len", "(", "examples", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.get_labels": [[39, 42], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_labels", "(", ")", ":", "\n", "        ", "return", "{", "\"contradiction\"", ":", "0", ",", "\"entailment\"", ":", "1", ",", "\"neutral\"", ":", "2", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.get_num_labels": [[43, 45], ["len", "NLIDataReader.NLIDataReader.get_labels"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.get_labels"], ["", "def", "get_num_labels", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "get_labels", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.map_label": [[46, 48], ["NLIDataReader.NLIDataReader.get_labels", "label.strip().lower", "label.strip"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.NLIDataReader.NLIDataReader.get_labels"], ["", "def", "map_label", "(", "self", ",", "label", ")", ":", "\n", "        ", "return", "self", ".", "get_labels", "(", ")", "[", "label", ".", "strip", "(", ")", ".", "lower", "(", ")", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.TripletReader.TripletReader.__init__": [[11, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset_folder", ",", "s1_col_idx", "=", "0", ",", "s2_col_idx", "=", "1", ",", "s3_col_idx", "=", "2", ",", "has_header", "=", "False", ",", "delimiter", "=", "\"\\t\"", ",", "\n", "quoting", "=", "csv", ".", "QUOTE_NONE", ")", ":", "\n", "        ", "self", ".", "dataset_folder", "=", "dataset_folder", "\n", "self", ".", "s1_col_idx", "=", "s1_col_idx", "\n", "self", ".", "s2_col_idx", "=", "s2_col_idx", "\n", "self", ".", "s3_col_idx", "=", "s3_col_idx", "\n", "self", ".", "has_header", "=", "has_header", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "self", ".", "quoting", "=", "quoting", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.readers.TripletReader.TripletReader.get_examples": [[21, 41], ["csv.reader", "enumerate", "open", "next", "examples.append", "os.path.join", "InputExample", "len"], "methods", ["None"], ["", "def", "get_examples", "(", "self", ",", "filename", ",", "max_examples", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n\n        \"\"\"", "\n", "data", "=", "csv", ".", "reader", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_folder", ",", "filename", ")", ",", "encoding", "=", "\"utf-8\"", ")", ",", "delimiter", "=", "self", ".", "delimiter", ",", "\n", "quoting", "=", "self", ".", "quoting", ")", "\n", "examples", "=", "[", "]", "\n", "if", "self", ".", "has_header", ":", "\n", "            ", "next", "(", "data", ")", "\n", "\n", "", "for", "id", ",", "row", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "s1", "=", "row", "[", "self", ".", "s1_col_idx", "]", "\n", "s2", "=", "row", "[", "self", ".", "s2_col_idx", "]", "\n", "s3", "=", "row", "[", "self", ".", "s3_col_idx", "]", "\n", "\n", "examples", ".", "append", "(", "InputExample", "(", "texts", "=", "[", "s1", ",", "s2", ",", "s3", "]", ")", ")", "\n", "if", "max_examples", ">", "0", "and", "len", "(", "examples", ")", ">=", "max_examples", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "examples", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.TranslationEvaluator.TranslationEvaluator.__init__": [[17, 44], ["len", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "source_sentences", ":", "List", "[", "str", "]", ",", "target_sentences", ":", "List", "[", "str", "]", ",", "show_progress_bar", ":", "bool", "=", "False", ",", "batch_size", ":", "int", "=", "16", ",", "name", ":", "str", "=", "''", ",", "print_wrong_matches", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Constructs an evaluator based for the dataset\n\n        The labels need to indicate the similarity between the sentences.\n\n        :param source_sentences:\n            List of sentences in source language\n        :param target_sentences:\n            List of sentences in target language\n        :param print_wrong_matches:\n            Prints incorrect matches\n        \"\"\"", "\n", "self", ".", "source_sentences", "=", "source_sentences", "\n", "self", ".", "target_sentences", "=", "target_sentences", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "self", ".", "print_wrong_matches", "=", "print_wrong_matches", "\n", "\n", "assert", "len", "(", "self", ".", "source_sentences", ")", "==", "len", "(", "self", ".", "target_sentences", ")", "\n", "\n", "if", "name", ":", "\n", "            ", "name", "=", "\"_\"", "+", "name", "\n", "\n", "", "self", ".", "csv_file", "=", "\"translation_evaluation\"", "+", "name", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\"src2trg\"", ",", "\"trg2src\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.TranslationEvaluator.TranslationEvaluator.__call__": [[45, 106], ["logging.info", "torch.stack", "torch.stack", "util.pytorch_cos_sim().detach().cpu().numpy", "range", "range", "logging.info", "logging.info", "model.encode", "model.encode", "len", "numpy.argmax", "len", "numpy.argmax", "len", "len", "os.path.join", "os.path.isfile", "util.pytorch_cos_sim().detach().cpu", "open", "csv.writer", "csv.writer.writerow", "print", "print", "print", "print", "zip", "sorted", "csv.writer.writerow", "util.pytorch_cos_sim().detach", "range", "print", "len", "util.pytorch_cos_sim"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.pytorch_cos_sim"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "if", "steps", "==", "-", "1", ":", "\n", "                ", "out_txt", "=", "\" after epoch {}:\"", ".", "format", "(", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "out_txt", "=", "\" in epoch {} after {} steps:\"", ".", "format", "(", "epoch", ",", "steps", ")", "\n", "", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"Evaluating translation matching Accuracy on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "\n", "embeddings1", "=", "torch", ".", "stack", "(", "model", ".", "encode", "(", "self", ".", "source_sentences", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "batch_size", "=", "self", ".", "batch_size", ",", "convert_to_numpy", "=", "False", ")", ")", "\n", "embeddings2", "=", "torch", ".", "stack", "(", "model", ".", "encode", "(", "self", ".", "target_sentences", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "batch_size", "=", "self", ".", "batch_size", ",", "convert_to_numpy", "=", "False", ")", ")", "\n", "\n", "\n", "cos_sims", "=", "pytorch_cos_sim", "(", "embeddings1", ",", "embeddings2", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "correct_src2trg", "=", "0", "\n", "correct_trg2src", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "cos_sims", ")", ")", ":", "\n", "            ", "max_idx", "=", "np", ".", "argmax", "(", "cos_sims", "[", "i", "]", ")", "\n", "\n", "if", "i", "==", "max_idx", ":", "\n", "                ", "correct_src2trg", "+=", "1", "\n", "", "elif", "self", ".", "print_wrong_matches", ":", "\n", "                ", "print", "(", "\"i:\"", ",", "i", ",", "\"j:\"", ",", "max_idx", ",", "\"INCORRECT\"", "if", "i", "!=", "max_idx", "else", "\"CORRECT\"", ")", "\n", "print", "(", "\"Src:\"", ",", "self", ".", "source_sentences", "[", "i", "]", ")", "\n", "print", "(", "\"Trg:\"", ",", "self", ".", "target_sentences", "[", "max_idx", "]", ")", "\n", "print", "(", "\"Argmax score:\"", ",", "cos_sims", "[", "i", "]", "[", "max_idx", "]", ",", "\"vs. correct score:\"", ",", "cos_sims", "[", "i", "]", "[", "i", "]", ")", "\n", "\n", "results", "=", "zip", "(", "range", "(", "len", "(", "cos_sims", "[", "i", "]", ")", ")", ",", "cos_sims", "[", "i", "]", ")", "\n", "results", "=", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "for", "idx", ",", "score", "in", "results", "[", "0", ":", "5", "]", ":", "\n", "                    ", "print", "(", "\"\\t\"", ",", "idx", ",", "\"(Score: %.4f)\"", "%", "(", "score", ")", ",", "self", ".", "target_sentences", "[", "idx", "]", ")", "\n", "\n", "\n", "\n", "", "", "", "cos_sims", "=", "cos_sims", ".", "T", "\n", "for", "i", "in", "range", "(", "len", "(", "cos_sims", ")", ")", ":", "\n", "            ", "max_idx", "=", "np", ".", "argmax", "(", "cos_sims", "[", "i", "]", ")", "\n", "if", "i", "==", "max_idx", ":", "\n", "                ", "correct_trg2src", "+=", "1", "\n", "\n", "", "", "acc_src2trg", "=", "correct_src2trg", "/", "len", "(", "cos_sims", ")", "\n", "acc_trg2src", "=", "correct_trg2src", "/", "len", "(", "cos_sims", ")", "\n", "\n", "logging", ".", "info", "(", "\"Accuracy src2trg: {:.2f}\"", ".", "format", "(", "acc_src2trg", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Accuracy trg2src: {:.2f}\"", ".", "format", "(", "acc_trg2src", "*", "100", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "output_file_exists", "=", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", "\n", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", "if", "output_file_exists", "else", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "if", "not", "output_file_exists", ":", "\n", "                    ", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "\n", "", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "acc_src2trg", ",", "acc_trg2src", "]", ")", "\n", "\n", "", "", "return", "(", "acc_src2trg", "+", "acc_trg2src", ")", "/", "2", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator.__init__": [[22, 76], ["list", "corpus.keys", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.csv_headers.append", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.csv_headers.append", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.csv_headers.append", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.csv_headers.append", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.csv_headers.append", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.queries_ids.append", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "queries", ":", "Dict", "[", "str", ",", "str", "]", ",", "#qid => query", "\n", "corpus", ":", "Dict", "[", "str", ",", "str", "]", ",", "#cid => doc", "\n", "relevant_docs", ":", "Dict", "[", "str", ",", "Set", "[", "str", "]", "]", ",", "#qid => Set[cid]", "\n", "query_chunk_size", ":", "int", "=", "1000", ",", "\n", "corpus_chunk_size", ":", "int", "=", "500000", ",", "\n", "mrr_at_k", ":", "List", "[", "int", "]", "=", "[", "10", "]", ",", "\n", "ndcg_at_k", ":", "List", "[", "int", "]", "=", "[", "10", "]", ",", "\n", "accuracy_at_k", ":", "List", "[", "int", "]", "=", "[", "1", ",", "3", ",", "5", ",", "10", "]", ",", "\n", "precision_recall_at_k", ":", "List", "[", "int", "]", "=", "[", "1", ",", "3", ",", "5", ",", "10", "]", ",", "\n", "show_progress_bar", ":", "bool", "=", "False", ",", "\n", "batch_size", ":", "int", "=", "16", ",", "\n", "name", ":", "str", "=", "''", ")", ":", "\n", "\n", "        ", "self", ".", "queries_ids", "=", "[", "]", "\n", "for", "qid", "in", "queries", ":", "\n", "            ", "if", "qid", "in", "relevant_docs", "and", "len", "(", "relevant_docs", "[", "qid", "]", ")", ">", "0", ":", "\n", "                ", "self", ".", "queries_ids", ".", "append", "(", "qid", ")", "\n", "\n", "", "", "self", ".", "queries", "=", "[", "queries", "[", "qid", "]", "for", "qid", "in", "self", ".", "queries_ids", "]", "\n", "\n", "self", ".", "corpus_ids", "=", "list", "(", "corpus", ".", "keys", "(", ")", ")", "\n", "self", ".", "corpus", "=", "[", "corpus", "[", "cid", "]", "for", "cid", "in", "self", ".", "corpus_ids", "]", "\n", "\n", "self", ".", "relevant_docs", "=", "relevant_docs", "\n", "self", ".", "query_chunk_size", "=", "query_chunk_size", "\n", "self", ".", "corpus_chunk_size", "=", "corpus_chunk_size", "\n", "self", ".", "mrr_at_k", "=", "mrr_at_k", "\n", "self", ".", "ndcg_at_k", "=", "ndcg_at_k", "\n", "self", ".", "accuracy_at_k", "=", "accuracy_at_k", "\n", "self", ".", "precision_recall_at_k", "=", "precision_recall_at_k", "\n", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "name", "=", "name", "\n", "\n", "if", "name", ":", "\n", "            ", "name", "=", "\"_\"", "+", "name", "\n", "\n", "", "self", ".", "csv_file", ":", "str", "=", "\"Information-Retrieval_evaluation\"", "+", "name", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", "]", "\n", "\n", "\n", "for", "k", "in", "accuracy_at_k", ":", "\n", "            ", "self", ".", "csv_headers", ".", "append", "(", "\"Accuracy@{}\"", ".", "format", "(", "k", ")", ")", "\n", "\n", "", "for", "k", "in", "precision_recall_at_k", ":", "\n", "            ", "self", ".", "csv_headers", ".", "append", "(", "\"Precision@{}\"", ".", "format", "(", "k", ")", ")", "\n", "self", ".", "csv_headers", ".", "append", "(", "\"Recall@{}\"", ".", "format", "(", "k", ")", ")", "\n", "\n", "", "for", "k", "in", "mrr_at_k", ":", "\n", "            ", "self", ".", "csv_headers", ".", "append", "(", "\"MRR@{}\"", ".", "format", "(", "k", ")", ")", "\n", "\n", "", "for", "k", "in", "ndcg_at_k", ":", "\n", "            ", "self", ".", "csv_headers", ".", "append", "(", "\"NDCG@{}\"", ".", "format", "(", "k", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator.__call__": [[77, 231], ["logging.info", "max", "model.encode", "model.encode", "range", "logging.info", "logging.info", "max", "max", "max", "max", "len", "min", "range", "range", "len", "numpy.mean", "numpy.mean", "numpy.mean", "len", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "os.path.join", "open.write", "open.write", "open.close", "len", "len", "min", "sentence_transformers.util.pytorch_cos_sim().cpu().numpy", "numpy.nan_to_num", "range", "len", "sorted", "len", "len", "os.path.isfile", "open", "open.write", "open.write", "open", "output_data.append", "output_data.append", "output_data.append", "output_data.append", "output_data.append", "max", "range", "len", "numpy.argpartition", "len", "precisions_at_k[].append", "recall_at_k[].append", "enumerate", "ndcg[].append", "map", "sentence_transformers.util.pytorch_cos_sim().cpu", "queries_result_list[].append", "len", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.compute_dcg_at_k", "InformationRetrievalEvaluator.InformationRetrievalEvaluator.compute_dcg_at_k", "len", "sentence_transformers.util.pytorch_cos_sim"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator.compute_dcg_at_k", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator.compute_dcg_at_k", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.pytorch_cos_sim"], ["", "", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "out_txt", "=", "\" after epoch {}:\"", ".", "format", "(", "epoch", ")", "if", "steps", "==", "-", "1", "else", "\" in epoch {} after {} steps:\"", ".", "format", "(", "epoch", ",", "steps", ")", "\n", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"Information Retrieval Evaluation on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "\n", "max_k", "=", "max", "(", "max", "(", "self", ".", "mrr_at_k", ")", ",", "max", "(", "self", ".", "ndcg_at_k", ")", ",", "max", "(", "self", ".", "accuracy_at_k", ")", ",", "max", "(", "self", ".", "precision_recall_at_k", ")", ")", "\n", "\n", "# Compute embedding for the queries", "\n", "query_embeddings", "=", "model", ".", "encode", "(", "self", ".", "queries", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "batch_size", "=", "self", ".", "batch_size", ",", "convert_to_tensor", "=", "True", ")", "\n", "\n", "\n", "#Init score computation values", "\n", "num_hits_at_k", "=", "{", "k", ":", "0", "for", "k", "in", "self", ".", "accuracy_at_k", "}", "\n", "\n", "precisions_at_k", "=", "{", "k", ":", "[", "]", "for", "k", "in", "self", ".", "precision_recall_at_k", "}", "\n", "recall_at_k", "=", "{", "k", ":", "[", "]", "for", "k", "in", "self", ".", "precision_recall_at_k", "}", "\n", "MRR", "=", "{", "k", ":", "0", "for", "k", "in", "self", ".", "mrr_at_k", "}", "\n", "ndcg", "=", "{", "k", ":", "[", "]", "for", "k", "in", "self", ".", "ndcg_at_k", "}", "\n", "\n", "#Compute embedding for the corpus", "\n", "corpus_embeddings", "=", "model", ".", "encode", "(", "self", ".", "corpus", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "batch_size", "=", "self", ".", "batch_size", ",", "convert_to_tensor", "=", "True", ")", "\n", "\n", "for", "query_start_idx", "in", "range", "(", "0", ",", "len", "(", "query_embeddings", ")", ",", "self", ".", "query_chunk_size", ")", ":", "\n", "            ", "query_end_idx", "=", "min", "(", "query_start_idx", "+", "self", ".", "query_chunk_size", ",", "len", "(", "query_embeddings", ")", ")", "\n", "\n", "queries_result_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "query_start_idx", ",", "query_end_idx", ")", "]", "\n", "\n", "#Iterate over chunks of the corpus", "\n", "for", "corpus_start_idx", "in", "range", "(", "0", ",", "len", "(", "corpus_embeddings", ")", ",", "self", ".", "corpus_chunk_size", ")", ":", "\n", "                ", "corpus_end_idx", "=", "min", "(", "corpus_start_idx", "+", "self", ".", "corpus_chunk_size", ",", "len", "(", "corpus_embeddings", ")", ")", "\n", "\n", "#Compute cosine similarites", "\n", "cos_scores", "=", "pytorch_cos_sim", "(", "query_embeddings", "[", "query_start_idx", ":", "query_end_idx", "]", ",", "corpus_embeddings", "[", "corpus_start_idx", ":", "corpus_end_idx", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "cos_scores", "=", "np", ".", "nan_to_num", "(", "cos_scores", ")", "\n", "\n", "#Partial sort scores", "\n", "cos_score_argpartition", "=", "np", ".", "argpartition", "(", "-", "cos_scores", ",", "max_k", ")", "[", ":", ",", "0", ":", "max_k", "]", "\n", "\n", "\n", "for", "query_itr", "in", "range", "(", "len", "(", "cos_scores", ")", ")", ":", "\n", "                    ", "for", "sub_corpus_id", "in", "cos_score_argpartition", "[", "query_itr", "]", ":", "\n", "                        ", "corpus_id", "=", "self", ".", "corpus_ids", "[", "corpus_start_idx", "+", "sub_corpus_id", "]", "\n", "score", "=", "cos_scores", "[", "query_itr", "]", "[", "sub_corpus_id", "]", "\n", "queries_result_list", "[", "query_itr", "]", ".", "append", "(", "{", "'corpus_id'", ":", "corpus_id", ",", "'score'", ":", "score", "}", ")", "\n", "\n", "", "", "", "for", "query_itr", "in", "range", "(", "len", "(", "queries_result_list", ")", ")", ":", "\n", "                ", "query_id", "=", "self", ".", "queries_ids", "[", "query_start_idx", "+", "query_itr", "]", "\n", "\n", "#Sort scores", "\n", "top_hits", "=", "sorted", "(", "queries_result_list", "[", "query_itr", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "'score'", "]", ",", "reverse", "=", "True", ")", "\n", "query_relevant_docs", "=", "self", ".", "relevant_docs", "[", "query_id", "]", "\n", "\n", "\n", "#Accuracy@k - We count the result correct, if at least one relevant doc is accross the top-k documents", "\n", "for", "k_val", "in", "self", ".", "accuracy_at_k", ":", "\n", "                    ", "for", "hit", "in", "top_hits", "[", "0", ":", "k_val", "]", ":", "\n", "                        ", "if", "hit", "[", "'corpus_id'", "]", "in", "query_relevant_docs", ":", "\n", "                            ", "num_hits_at_k", "[", "k_val", "]", "+=", "1", "\n", "break", "\n", "\n", "#Precision and Recall@k", "\n", "", "", "", "for", "k_val", "in", "self", ".", "precision_recall_at_k", ":", "\n", "                    ", "num_correct", "=", "0", "\n", "\n", "for", "hit", "in", "top_hits", "[", "0", ":", "k_val", "]", ":", "\n", "                        ", "if", "hit", "[", "'corpus_id'", "]", "in", "query_relevant_docs", ":", "\n", "                            ", "num_correct", "+=", "1", "\n", "\n", "", "", "precisions_at_k", "[", "k_val", "]", ".", "append", "(", "num_correct", "/", "k_val", ")", "\n", "recall_at_k", "[", "k_val", "]", ".", "append", "(", "num_correct", "/", "len", "(", "query_relevant_docs", ")", ")", "\n", "\n", "#MRR@k", "\n", "", "for", "k_val", "in", "self", ".", "mrr_at_k", ":", "\n", "                    ", "for", "rank", ",", "hit", "in", "enumerate", "(", "top_hits", "[", "0", ":", "k_val", "]", ")", ":", "\n", "                        ", "if", "hit", "[", "'corpus_id'", "]", "in", "query_relevant_docs", ":", "\n", "                            ", "MRR", "[", "k_val", "]", "+=", "1.0", "/", "(", "rank", "+", "1", ")", "\n", "break", "\n", "\n", "#NDCG@k", "\n", "", "", "", "for", "k_val", "in", "self", ".", "ndcg_at_k", ":", "\n", "                    ", "predicted_relevance", "=", "[", "1", "if", "top_hit", "[", "'corpus_id'", "]", "in", "query_relevant_docs", "else", "0", "for", "top_hit", "in", "top_hits", "[", "0", ":", "k_val", "]", "]", "\n", "true_relevances", "=", "[", "1", "]", "*", "len", "(", "query_relevant_docs", ")", "\n", "\n", "ndcg_value", "=", "self", ".", "compute_dcg_at_k", "(", "predicted_relevance", ",", "k_val", ")", "/", "self", ".", "compute_dcg_at_k", "(", "true_relevances", ",", "k_val", ")", "\n", "ndcg", "[", "k_val", "]", ".", "append", "(", "ndcg_value", ")", "\n", "\n", "#Compute averages", "\n", "", "", "", "for", "k", "in", "num_hits_at_k", ":", "\n", "            ", "num_hits_at_k", "[", "k", "]", "/=", "len", "(", "self", ".", "queries", ")", "\n", "\n", "", "for", "k", "in", "precisions_at_k", ":", "\n", "            ", "precisions_at_k", "[", "k", "]", "=", "np", ".", "mean", "(", "precisions_at_k", "[", "k", "]", ")", "\n", "\n", "", "for", "k", "in", "recall_at_k", ":", "\n", "            ", "recall_at_k", "[", "k", "]", "=", "np", ".", "mean", "(", "recall_at_k", "[", "k", "]", ")", "\n", "\n", "", "for", "k", "in", "ndcg", ":", "\n", "            ", "ndcg", "[", "k", "]", "=", "np", ".", "mean", "(", "ndcg", "[", "k", "]", ")", "\n", "\n", "", "for", "k", "in", "MRR", ":", "\n", "            ", "MRR", "[", "k", "]", "/=", "len", "(", "self", ".", "queries", ")", "\n", "\n", "#Output", "\n", "\n", "\n", "", "for", "k", "in", "num_hits_at_k", ":", "\n", "            ", "logging", ".", "info", "(", "\"Accuracy@{}: {:.2f}%\"", ".", "format", "(", "k", ",", "num_hits_at_k", "[", "k", "]", "*", "100", ")", ")", "\n", "\n", "", "for", "k", "in", "precisions_at_k", ":", "\n", "            ", "logging", ".", "info", "(", "\"Precision@{}: {:.2f}%\"", ".", "format", "(", "k", ",", "precisions_at_k", "[", "k", "]", "*", "100", ")", ")", "\n", "\n", "", "for", "k", "in", "recall_at_k", ":", "\n", "            ", "logging", ".", "info", "(", "\"Recall@{}: {:.2f}%\"", ".", "format", "(", "k", ",", "recall_at_k", "[", "k", "]", "*", "100", ")", ")", "\n", "\n", "", "for", "k", "in", "MRR", ":", "\n", "            ", "logging", ".", "info", "(", "\"MRR@{}: {:.4f}\"", ".", "format", "(", "k", ",", "MRR", "[", "k", "]", ")", ")", "\n", "\n", "", "for", "k", "in", "ndcg", ":", "\n", "            ", "logging", ".", "info", "(", "\"NDCG@{}: {:.4f}\"", ".", "format", "(", "k", ",", "ndcg", "[", "k", "]", ")", ")", "\n", "", "logging", ".", "info", "(", "\"Queries: {}\"", ".", "format", "(", "len", "(", "self", ".", "queries", ")", ")", ")", "\n", "logging", ".", "info", "(", "\"Corpus: {}\\n\"", ".", "format", "(", "len", "(", "self", ".", "corpus", ")", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", ":", "\n", "                ", "fOut", "=", "open", "(", "csv_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "fOut", ".", "write", "(", "\",\"", ".", "join", "(", "self", ".", "csv_headers", ")", ")", "\n", "fOut", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "else", ":", "\n", "                ", "fOut", "=", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "\n", "", "output_data", "=", "[", "epoch", ",", "steps", "]", "\n", "for", "k", "in", "self", ".", "accuracy_at_k", ":", "\n", "                ", "output_data", ".", "append", "(", "num_hits_at_k", "[", "k", "]", ")", "\n", "\n", "", "for", "k", "in", "self", ".", "precision_recall_at_k", ":", "\n", "                ", "output_data", ".", "append", "(", "precisions_at_k", "[", "k", "]", ")", "\n", "output_data", ".", "append", "(", "recall_at_k", "[", "k", "]", ")", "\n", "\n", "", "for", "k", "in", "self", ".", "mrr_at_k", ":", "\n", "                ", "output_data", ".", "append", "(", "MRR", "[", "k", "]", ")", "\n", "\n", "", "for", "k", "in", "self", ".", "ndcg_at_k", ":", "\n", "                ", "output_data", ".", "append", "(", "ndcg", "[", "k", "]", ")", "\n", "\n", "", "fOut", ".", "write", "(", "\",\"", ".", "join", "(", "map", "(", "str", ",", "output_data", ")", ")", ")", "\n", "fOut", ".", "write", "(", "\"\\n\"", ")", "\n", "fOut", ".", "close", "(", ")", "\n", "\n", "", "return", "MRR", "[", "max", "(", "self", ".", "mrr_at_k", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator.compute_dcg_at_k": [[232, 238], ["range", "min", "len", "numpy.log2"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_dcg_at_k", "(", "relevances", ",", "k", ")", ":", "\n", "        ", "dcg", "=", "0", "\n", "for", "i", "in", "range", "(", "min", "(", "len", "(", "relevances", ")", ",", "k", ")", ")", ":", "\n", "            ", "dcg", "+=", "relevances", "[", "i", "]", "/", "np", ".", "log2", "(", "i", "+", "2", ")", "#+2 as we start our idx at 0", "\n", "", "return", "dcg", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.__init__": [[25, 59], ["len", "len", "len", "len", "logging.getLogger().getEffectiveLevel", "logging.getLogger().getEffectiveLevel", "logging.getLogger", "logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentences1", ":", "List", "[", "str", "]", ",", "sentences2", ":", "List", "[", "str", "]", ",", "labels", ":", "List", "[", "int", "]", ",", "\n", "name", ":", "str", "=", "''", ",", "\n", "batch_size", ":", "int", "=", "16", ",", "show_progress_bar", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Constructs an evaluator based for the dataset\n\n        The labels need to be 0 for dissimilar pairs and 1 for similar pairs.\n\n        :param dataloader:\n            the data for the evaluation\n        :param main_similarity:\n            the similarity metric that will be used for the returned score\n        \"\"\"", "\n", "self", ".", "sentences1", "=", "sentences1", "\n", "self", ".", "sentences2", "=", "sentences2", "\n", "self", ".", "labels", "=", "labels", "\n", "\n", "assert", "len", "(", "self", ".", "sentences1", ")", "==", "len", "(", "self", ".", "sentences2", ")", "\n", "assert", "len", "(", "self", ".", "sentences1", ")", "==", "len", "(", "self", ".", "labels", ")", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "assert", "(", "label", "==", "0", "or", "label", "==", "1", ")", "\n", "\n", "\n", "", "self", ".", "name", "=", "name", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "if", "show_progress_bar", "is", "None", ":", "\n", "            ", "show_progress_bar", "=", "(", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "INFO", "or", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "DEBUG", ")", "\n", "", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "\n", "self", ".", "csv_file", ":", "str", "=", "\"binary_classification_evaluation\"", "+", "(", "\"_\"", "+", "name", "if", "name", "else", "''", ")", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\n", "\"cosine_acc\"", ",", "\"cosine_acc_threshold\"", ",", "\"cosine_f1\"", ",", "\"cosine_precision\"", ",", "\"cosine_recall\"", ",", "\"cosine_f1_threshold\"", ",", "\"cosine_average_precision\"", ",", "\n", "\"manhatten_acc\"", ",", "\"manhatten_acc_threshold\"", ",", "\"manhatten_f1\"", ",", "\"manhatten_precision\"", ",", "\"manhatten_recall\"", ",", "\"manhatten_f1_threshold\"", ",", "\"manhatten_average_precision\"", ",", "\n", "\"eucledian_acc\"", ",", "\"eucledian_acc_threshold\"", ",", "\"eucledian_f1\"", ",", "\"eucledian_precision\"", ",", "\"eucledian_recall\"", ",", "\"eucledian_f1_threshold\"", ",", "\"eucledian_average_precision\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.from_input_examples": [[61, 72], ["cls", "sentences1.append", "sentences2.append", "scores.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_input_examples", "(", "cls", ",", "examples", ":", "List", "[", "InputExample", "]", ",", "**", "kwargs", ")", ":", "\n", "        ", "sentences1", "=", "[", "]", "\n", "sentences2", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "\n", "for", "example", "in", "examples", ":", "\n", "            ", "sentences1", ".", "append", "(", "example", ".", "texts", "[", "0", "]", ")", "\n", "sentences2", ".", "append", "(", "example", ".", "texts", "[", "1", "]", ")", "\n", "scores", ".", "append", "(", "example", ".", "label", ")", "\n", "", "return", "cls", "(", "sentences1", ",", "sentences2", ",", "scores", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.__call__": [[73, 128], ["logging.info", "model.encode", "model.encode", "sklearn.metrics.pairwise.paired_manhattan_distances", "sklearn.metrics.pairwise.paired_euclidean_distances", "numpy.asarray", "sklearn.metrics.pairwise.paired_cosine_distances", "BinaryClassificationEvaluator.BinaryClassificationEvaluator.find_best_acc_and_threshold", "BinaryClassificationEvaluator.BinaryClassificationEvaluator.find_best_f1_and_threshold", "sklearn.metrics.average_precision_score", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "file_output_data.extend", "os.path.join", "os.path.isfile", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow", "open", "csv.writer", "csv.writer.writerow"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.find_best_acc_and_threshold", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.find_best_f1_and_threshold"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "if", "steps", "==", "-", "1", ":", "\n", "                ", "out_txt", "=", "f\" after epoch {epoch}:\"", "\n", "", "else", ":", "\n", "                ", "out_txt", "=", "f\" in epoch {epoch} after {steps} steps:\"", "\n", "", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"Binary Accuracy Evaluation of the model on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "embeddings1", "=", "model", ".", "encode", "(", "self", ".", "sentences1", ",", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "embeddings2", "=", "model", ".", "encode", "(", "self", ".", "sentences2", ",", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "\n", "cosine_scores", "=", "1", "-", "paired_cosine_distances", "(", "embeddings1", ",", "embeddings2", ")", "\n", "manhattan_distances", "=", "paired_manhattan_distances", "(", "embeddings1", ",", "embeddings2", ")", "\n", "euclidean_distances", "=", "paired_euclidean_distances", "(", "embeddings1", ",", "embeddings2", ")", "\n", "\n", "\n", "labels", "=", "np", ".", "asarray", "(", "self", ".", "labels", ")", "\n", "\n", "file_output_data", "=", "[", "epoch", ",", "steps", "]", "\n", "\n", "main_score", "=", "None", "\n", "for", "name", ",", "scores", ",", "reverse", "in", "[", "[", "'Cosine-Similarity'", ",", "cosine_scores", ",", "True", "]", ",", "[", "'Manhatten-Distance'", ",", "manhattan_distances", ",", "False", "]", ",", "[", "'Euclidean-Distance'", ",", "euclidean_distances", ",", "False", "]", "]", ":", "\n", "            ", "acc", ",", "acc_threshold", "=", "self", ".", "find_best_acc_and_threshold", "(", "scores", ",", "labels", ",", "reverse", ")", "\n", "f1", ",", "precision", ",", "recall", ",", "f1_threshold", "=", "self", ".", "find_best_f1_and_threshold", "(", "scores", ",", "labels", ",", "reverse", ")", "\n", "ap", "=", "average_precision_score", "(", "labels", ",", "scores", "*", "(", "1", "if", "reverse", "else", "-", "1", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"Accuracy with {}:           {:.2f}\\t(Threshold: {:.4f})\"", ".", "format", "(", "name", ",", "acc", "*", "100", ",", "acc_threshold", ")", ")", "\n", "logging", ".", "info", "(", "\"F1 with {}:                 {:.2f}\\t(Threshold: {:.4f})\"", ".", "format", "(", "name", ",", "f1", "*", "100", ",", "f1_threshold", ")", ")", "\n", "logging", ".", "info", "(", "\"Precision with {}:          {:.2f}\"", ".", "format", "(", "name", ",", "precision", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Recall with {}:             {:.2f}\"", ".", "format", "(", "name", ",", "recall", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Average Precision with {}:  {:.2f}\\n\"", ".", "format", "(", "name", ",", "ap", "*", "100", ")", ")", "\n", "\n", "file_output_data", ".", "extend", "(", "[", "acc", ",", "acc_threshold", ",", "f1", ",", "precision", ",", "recall", ",", "f1_threshold", ",", "ap", "]", ")", "\n", "\n", "if", "main_score", "is", "None", ":", "#Use AveragePrecision with Cosine-Similarity as main score", "\n", "                ", "main_score", "=", "ap", "\n", "\n", "", "", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "writer", ".", "writerow", "(", "file_output_data", ")", "\n", "", "", "else", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "file_output_data", ")", "\n", "\n", "", "", "", "return", "main_score", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.find_best_acc_and_threshold": [[129, 155], ["list", "sorted", "sum", "range", "len", "len", "zip", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "find_best_acc_and_threshold", "(", "scores", ",", "labels", ",", "high_score_more_similar", ":", "bool", ")", ":", "\n", "        ", "assert", "len", "(", "scores", ")", "==", "len", "(", "labels", ")", "\n", "rows", "=", "list", "(", "zip", "(", "scores", ",", "labels", ")", ")", "\n", "\n", "rows", "=", "sorted", "(", "rows", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "high_score_more_similar", ")", "\n", "\n", "max_acc", "=", "0", "\n", "best_threshold", "=", "-", "1", "\n", "\n", "positive_so_far", "=", "0", "\n", "remaining_negatives", "=", "sum", "(", "labels", "==", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "rows", ")", "-", "1", ")", ":", "\n", "            ", "score", ",", "label", "=", "rows", "[", "i", "]", "\n", "if", "label", "==", "1", ":", "\n", "                ", "positive_so_far", "+=", "1", "\n", "", "else", ":", "\n", "                ", "remaining_negatives", "-=", "1", "\n", "\n", "", "acc", "=", "(", "positive_so_far", "+", "remaining_negatives", ")", "/", "len", "(", "labels", ")", "\n", "if", "acc", ">", "max_acc", ":", "\n", "                ", "max_acc", "=", "acc", "\n", "best_threshold", "=", "(", "rows", "[", "i", "]", "[", "0", "]", "+", "rows", "[", "i", "+", "1", "]", "[", "0", "]", ")", "/", "2", "\n", "\n", "", "", "return", "max_acc", ",", "best_threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator.find_best_f1_and_threshold": [[156, 191], ["numpy.asarray", "numpy.asarray", "list", "sorted", "sum", "range", "len", "len", "zip", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "find_best_f1_and_threshold", "(", "scores", ",", "labels", ",", "high_score_more_similar", ":", "bool", ")", ":", "\n", "        ", "assert", "len", "(", "scores", ")", "==", "len", "(", "labels", ")", "\n", "\n", "scores", "=", "np", ".", "asarray", "(", "scores", ")", "\n", "labels", "=", "np", ".", "asarray", "(", "labels", ")", "\n", "\n", "rows", "=", "list", "(", "zip", "(", "scores", ",", "labels", ")", ")", "\n", "\n", "rows", "=", "sorted", "(", "rows", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "high_score_more_similar", ")", "\n", "\n", "best_f1", "=", "best_precision", "=", "best_recall", "=", "0", "\n", "threshold", "=", "0", "\n", "nextract", "=", "0", "\n", "ncorrect", "=", "0", "\n", "total_num_duplicates", "=", "sum", "(", "labels", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "rows", ")", "-", "1", ")", ":", "\n", "            ", "score", ",", "label", "=", "rows", "[", "i", "]", "\n", "nextract", "+=", "1", "\n", "\n", "if", "label", "==", "1", ":", "\n", "                ", "ncorrect", "+=", "1", "\n", "\n", "", "if", "ncorrect", ">", "0", ":", "\n", "                ", "precision", "=", "ncorrect", "/", "nextract", "\n", "recall", "=", "ncorrect", "/", "total_num_duplicates", "\n", "f1", "=", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", "if", "f1", ">", "best_f1", ":", "\n", "                    ", "best_f1", "=", "f1", "\n", "best_precision", "=", "precision", "\n", "best_recall", "=", "recall", "\n", "threshold", "=", "(", "rows", "[", "i", "]", "[", "0", "]", "+", "rows", "[", "i", "+", "1", "]", "[", "0", "]", ")", "/", "2", "\n", "\n", "", "", "", "return", "best_f1", ",", "best_precision", ",", "best_recall", ",", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.ParaphraseMiningEvaluator.ParaphraseMiningEvaluator.__init__": [[18, 83], ["collections.defaultdict", "sentences_map.items", "set", "len", "ParaphraseMiningEvaluator.ParaphraseMiningEvaluator.sentences.append", "ParaphraseMiningEvaluator.ParaphraseMiningEvaluator.ids.append", "collections.defaultdict", "list", "set.add", "list", "tuple", "sorted"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentences_map", ":", "Dict", "[", "str", ",", "str", "]", ",", "duplicates_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", "=", "None", ",", "duplicates_dict", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "bool", "]", "]", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "bool", ")", ")", ",", "add_transitive_closure", ":", "bool", "=", "False", ",", "query_chunk_size", ":", "int", "=", "5000", ",", "corpus_chunk_size", ":", "int", "=", "100000", ",", "max_pairs", ":", "int", "=", "500000", ",", "top_k", ":", "int", "=", "100", ",", "show_progress_bar", ":", "bool", "=", "False", ",", "batch_size", ":", "int", "=", "16", ",", "name", ":", "str", "=", "''", ")", ":", "\n", "        ", "\"\"\"\n\n        :param sentences_map: A dictionary that maps sentence-ids to sentences, i.e. sentences_map[id] => sentence.\n        :param duplicates_list: Duplicates_list is a list with id pairs [(id1, id2), (id1, id5)] that identifies the duplicates / paraphrases in the sentences_map\n        :param duplicates_dict: A default dictionary mapping [id1][id2] to true if id1 and id2 are duplicates. Must be symmetric, i.e., if [id1][id2] => True, then [id2][id1] => True.\n        :param add_transitive_closure: If true, it adds a transitive closure, i.e. if dup[a][b] and dup[b][c], then dup[a][c]\n        :param query_chunk_size: To identify the paraphrases, the cosine-similarity between all sentence-pairs will be computed. As this might require a lot of memory, we perform a batched computation.  #query_batch_size sentences will be compared against up to #corpus_batch_size sentences. In the default setting, 5000 sentences will be grouped together and compared up-to against 100k other sentences.\n        :param corpus_chunk_size: The corpus will be batched, to reduce the memory requirement\n        :param max_pairs: We will only extract up to #max_pairs potential paraphrase candidates.\n        :param top_k: For each query, we extract the top_k most similar pairs and add it to a sorted list. I.e., for one sentence we cannot find more than top_k paraphrases\n        :param show_progress_bar: Output a progress bar\n        :param batch_size: Batch size for computing sentence embeddings\n        :param name: Name of the experiment\n        \"\"\"", "\n", "self", ".", "sentences", "=", "[", "]", "\n", "self", ".", "ids", "=", "[", "]", "\n", "\n", "for", "id", ",", "sentence", "in", "sentences_map", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "self", ".", "ids", ".", "append", "(", "id", ")", "\n", "\n", "", "self", ".", "name", "=", "name", "\n", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "query_chunk_size", "=", "query_chunk_size", "\n", "self", ".", "corpus_chunk_size", "=", "corpus_chunk_size", "\n", "self", ".", "max_pairs", "=", "max_pairs", "\n", "self", ".", "top_k", "=", "top_k", "\n", "\n", "self", ".", "duplicates", "=", "duplicates_dict", "\n", "if", "duplicates_list", "is", "not", "None", ":", "\n", "            ", "for", "id1", ",", "id2", "in", "duplicates_list", ":", "\n", "                ", "if", "id1", "in", "sentences_map", "and", "id2", "in", "sentences_map", ":", "\n", "                    ", "self", ".", "duplicates", "[", "id1", "]", "[", "id2", "]", "=", "True", "\n", "self", ".", "duplicates", "[", "id2", "]", "[", "id1", "]", "=", "True", "\n", "\n", "\n", "#Add transitive closure", "\n", "", "", "", "if", "add_transitive_closure", ":", "\n", "            ", "new_entries", "=", "True", "\n", "while", "new_entries", ":", "\n", "                ", "new_entries", "=", "False", "\n", "for", "a", "in", "self", ".", "duplicates", ":", "\n", "                    ", "for", "b", "in", "list", "(", "self", ".", "duplicates", "[", "a", "]", ")", ":", "\n", "                        ", "for", "c", "in", "list", "(", "self", ".", "duplicates", "[", "b", "]", ")", ":", "\n", "                            ", "if", "a", "!=", "c", "and", "not", "self", ".", "duplicates", "[", "a", "]", "[", "c", "]", ":", "\n", "                                ", "new_entries", "=", "True", "\n", "self", ".", "duplicates", "[", "a", "]", "[", "c", "]", "=", "True", "\n", "self", ".", "duplicates", "[", "c", "]", "[", "a", "]", "=", "True", "\n", "\n", "\n", "", "", "", "", "", "", "positive_key_pairs", "=", "set", "(", ")", "\n", "for", "key1", "in", "self", ".", "duplicates", ":", "\n", "            ", "for", "key2", "in", "self", ".", "duplicates", "[", "key1", "]", ":", "\n", "                ", "if", "self", ".", "duplicates", "[", "key1", "]", "[", "key2", "]", "or", "self", ".", "duplicates", "[", "key2", "]", "[", "key1", "]", ":", "\n", "                    ", "positive_key_pairs", ".", "add", "(", "tuple", "(", "sorted", "(", "[", "key1", ",", "key2", "]", ")", ")", ")", "\n", "\n", "", "", "", "self", ".", "total_num_duplicates", "=", "len", "(", "positive_key_pairs", ")", "\n", "\n", "if", "name", ":", "\n", "            ", "name", "=", "\"_\"", "+", "name", "\n", "\n", "", "self", ".", "csv_file", ":", "str", "=", "\"paraphrase_mining_evaluation\"", "+", "name", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\"precision\"", ",", "\"recall\"", ",", "\"f1\"", ",", "\"threshold\"", ",", "\"average_precision\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.ParaphraseMiningEvaluator.ParaphraseMiningEvaluator.__call__": [[84, 145], ["logging.info", "sentence_transformers.util.paraphrase_mining", "logging.info", "range", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "len", "os.path.join", "str", "os.path.isfile", "len", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow", "open", "csv.writer", "csv.writer.writerow", "min", "len"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.paraphrase_mining"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "out_txt", "=", "f\" after epoch {epoch}:\"", "if", "steps", "==", "-", "1", "else", "f\" in epoch {epoch} after {steps} steps:\"", "\n", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"Paraphrase Mining Evaluation on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "\n", "#Compute embedding for the sentences", "\n", "pairs_list", "=", "paraphrase_mining", "(", "model", ",", "self", ".", "sentences", ",", "self", ".", "show_progress_bar", ",", "self", ".", "batch_size", ",", "self", ".", "query_chunk_size", ",", "self", ".", "corpus_chunk_size", ",", "self", ".", "max_pairs", ",", "self", ".", "top_k", ")", "\n", "\n", "\n", "logging", ".", "info", "(", "\"Number of candidate pairs: \"", "+", "str", "(", "len", "(", "pairs_list", ")", ")", ")", "\n", "\n", "#Compute F1 score and Average Precision", "\n", "n_extract", "=", "n_correct", "=", "0", "\n", "threshold", "=", "0", "\n", "best_f1", "=", "best_recall", "=", "best_precision", "=", "0", "\n", "\n", "average_precision", "=", "0", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "pairs_list", ")", ")", ":", "\n", "            ", "score", ",", "i", ",", "j", "=", "pairs_list", "[", "idx", "]", "\n", "id1", "=", "self", ".", "ids", "[", "i", "]", "\n", "id2", "=", "self", ".", "ids", "[", "j", "]", "\n", "\n", "#Compute optimal threshold and F1-score", "\n", "n_extract", "+=", "1", "\n", "if", "self", ".", "duplicates", "[", "id1", "]", "[", "id2", "]", "or", "self", ".", "duplicates", "[", "id2", "]", "[", "id1", "]", ":", "\n", "                ", "n_correct", "+=", "1", "\n", "precision", "=", "n_correct", "/", "n_extract", "\n", "recall", "=", "n_correct", "/", "self", ".", "total_num_duplicates", "\n", "f1", "=", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", "average_precision", "+=", "precision", "\n", "if", "f1", ">", "best_f1", ":", "\n", "                    ", "best_f1", "=", "f1", "\n", "best_precision", "=", "precision", "\n", "best_recall", "=", "recall", "\n", "threshold", "=", "(", "pairs_list", "[", "idx", "]", "[", "0", "]", "+", "pairs_list", "[", "min", "(", "idx", "+", "1", ",", "len", "(", "pairs_list", ")", "-", "1", ")", "]", "[", "0", "]", ")", "/", "2", "\n", "\n", "", "", "", "average_precision", "=", "average_precision", "/", "self", ".", "total_num_duplicates", "\n", "\n", "logging", ".", "info", "(", "\"Average Precision: {:.2f}\"", ".", "format", "(", "average_precision", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Optimal threshold: {:.4f}\"", ".", "format", "(", "threshold", ")", ")", "\n", "logging", ".", "info", "(", "\"Precision: {:.2f}\"", ".", "format", "(", "best_precision", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Recall: {:.2f}\"", ".", "format", "(", "best_recall", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"F1: {:.2f}\\n\"", ".", "format", "(", "best_f1", "*", "100", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "best_precision", ",", "best_recall", ",", "best_f1", ",", "threshold", ",", "average_precision", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "best_precision", ",", "best_recall", ",", "best_f1", ",", "threshold", ",", "average_precision", "]", ")", "\n", "\n", "", "", "", "return", "average_precision", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.LabelAccuracyEvaluator.LabelAccuracyEvaluator.__init__": [[19, 36], ["LabelAccuracyEvaluator.LabelAccuracyEvaluator.softmax_model.to"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataloader", ":", "DataLoader", ",", "name", ":", "str", "=", "\"\"", ",", "softmax_model", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Constructs an evaluator for the given dataset\n\n        :param dataloader:\n            the data for the evaluation\n        \"\"\"", "\n", "self", ".", "dataloader", "=", "dataloader", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "softmax_model", "=", "softmax_model", "\n", "self", ".", "softmax_model", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "name", ":", "\n", "            ", "name", "=", "\"_\"", "+", "name", "\n", "\n", "", "self", ".", "csv_file", "=", "\"accuracy_evaluation\"", "+", "name", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\"accuracy\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.LabelAccuracyEvaluator.LabelAccuracyEvaluator.__call__": [[37, 76], ["model.eval", "logging.info", "enumerate", "logging.info", "tqdm.tqdm.tqdm", "util.batch_to_device", "prediction.size", "torch.argmax().eq().sum().item", "os.path.join", "torch.no_grad", "LabelAccuracyEvaluator.LabelAccuracyEvaluator.softmax_model", "os.path.isfile", "torch.argmax().eq().sum", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow", "open", "csv.writer", "csv.writer.writerow", "torch.argmax().eq", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.batch_to_device"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "total", "=", "0", "\n", "correct", "=", "0", "\n", "\n", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "if", "steps", "==", "-", "1", ":", "\n", "                ", "out_txt", "=", "\" after epoch {}:\"", ".", "format", "(", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "out_txt", "=", "\" in epoch {} after {} steps:\"", ".", "format", "(", "epoch", ",", "steps", ")", "\n", "", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"Evaluation on the \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "self", ".", "dataloader", ".", "collate_fn", "=", "model", ".", "smart_batching_collate", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "self", ".", "dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ")", ":", "\n", "            ", "features", ",", "label_ids", "=", "batch_to_device", "(", "batch", ",", "model", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "_", ",", "prediction", "=", "self", ".", "softmax_model", "(", "features", ",", "labels", "=", "None", ")", "\n", "\n", "", "total", "+=", "prediction", ".", "size", "(", "0", ")", "\n", "correct", "+=", "torch", ".", "argmax", "(", "prediction", ",", "dim", "=", "1", ")", ".", "eq", "(", "label_ids", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "", "accuracy", "=", "correct", "/", "total", "\n", "\n", "logging", ".", "info", "(", "\"Accuracy: {:.4f} ({}/{})\\n\"", ".", "format", "(", "accuracy", ",", "correct", ",", "total", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "accuracy", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "accuracy", "]", ")", "\n", "\n", "", "", "", "return", "accuracy", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.TripletEvaluator.TripletEvaluator.__init__": [[18, 45], ["len", "len", "len", "len", "logging.getLogger().getEffectiveLevel", "logging.getLogger().getEffectiveLevel", "logging.getLogger", "logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "anchors", ":", "List", "[", "str", "]", ",", "positives", ":", "List", "[", "str", "]", ",", "negatives", ":", "List", "[", "str", "]", ",", "main_distance_function", ":", "SimilarityFunction", "=", "None", ",", "name", ":", "str", "=", "''", ",", "batch_size", ":", "int", "=", "16", ",", "show_progress_bar", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Constructs an evaluator based for the dataset\n\n\n        :param dataloader:\n            the data for the evaluation\n        :param main_similarity:\n            the similarity metric that will be used for the returned score\n        \"\"\"", "\n", "self", ".", "anchors", "=", "anchors", "\n", "self", ".", "positives", "=", "positives", "\n", "self", ".", "negatives", "=", "negatives", "\n", "self", ".", "name", "=", "name", "\n", "\n", "assert", "len", "(", "self", ".", "anchors", ")", "==", "len", "(", "self", ".", "positives", ")", "\n", "assert", "len", "(", "self", ".", "anchors", ")", "==", "len", "(", "self", ".", "negatives", ")", "\n", "\n", "self", ".", "main_distance_function", "=", "main_distance_function", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "if", "show_progress_bar", "is", "None", ":", "\n", "            ", "show_progress_bar", "=", "(", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "INFO", "or", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "DEBUG", ")", "\n", "", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "\n", "self", ".", "csv_file", ":", "str", "=", "\"triplet_evaluation\"", "+", "(", "\"_\"", "+", "name", "if", "name", "else", "''", ")", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\"accuracy_cosinus\"", ",", "\"accuracy_manhatten\"", ",", "\"accuracy_euclidean\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.TripletEvaluator.TripletEvaluator.from_input_examples": [[46, 57], ["cls", "anchors.append", "positives.append", "negatives.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_input_examples", "(", "cls", ",", "examples", ":", "List", "[", "InputExample", "]", ",", "**", "kwargs", ")", ":", "\n", "        ", "anchors", "=", "[", "]", "\n", "positives", "=", "[", "]", "\n", "negatives", "=", "[", "]", "\n", "\n", "for", "example", "in", "examples", ":", "\n", "            ", "anchors", ".", "append", "(", "example", ".", "texts", "[", "0", "]", ")", "\n", "positives", ".", "append", "(", "example", ".", "texts", "[", "1", "]", ")", "\n", "negatives", ".", "append", "(", "example", ".", "texts", "[", "2", "]", ")", "\n", "", "return", "cls", "(", "anchors", ",", "positives", ",", "negatives", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.TripletEvaluator.TripletEvaluator.__call__": [[58, 133], ["logging.info", "model.encode", "model.encode", "model.encode", "sklearn.metrics.pairwise.paired_cosine_distances", "sklearn.metrics.pairwise.paired_cosine_distances", "sklearn.metrics.pairwise.paired_manhattan_distances", "sklearn.metrics.pairwise.paired_manhattan_distances", "sklearn.metrics.pairwise.paired_euclidean_distances", "sklearn.metrics.pairwise.paired_euclidean_distances", "range", "logging.info", "logging.info", "logging.info", "max", "len", "os.path.join", "os.path.isfile", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow", "open", "csv.writer", "csv.writer.writerow"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "if", "steps", "==", "-", "1", ":", "\n", "                ", "out_txt", "=", "\" after epoch {}:\"", ".", "format", "(", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "out_txt", "=", "\" in epoch {} after {} steps:\"", ".", "format", "(", "epoch", ",", "steps", ")", "\n", "", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"TripletEvaluator: Evaluating the model on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "\n", "num_triplets", "=", "0", "\n", "num_correct_cos_triplets", ",", "num_correct_manhatten_triplets", ",", "num_correct_euclidean_triplets", "=", "0", ",", "0", ",", "0", "\n", "\n", "embeddings_anchors", "=", "model", ".", "encode", "(", "self", ".", "anchors", ",", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "embeddings_positives", "=", "model", ".", "encode", "(", "self", ".", "positives", ",", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "embeddings_negatives", "=", "model", ".", "encode", "(", "self", ".", "negatives", ",", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "\n", "\n", "#Cosine distance", "\n", "pos_cos_distance", "=", "paired_cosine_distances", "(", "embeddings_anchors", ",", "embeddings_positives", ")", "\n", "neg_cos_distances", "=", "paired_cosine_distances", "(", "embeddings_anchors", ",", "embeddings_negatives", ")", "\n", "\n", "# Manhatten", "\n", "pos_manhatten_distance", "=", "paired_manhattan_distances", "(", "embeddings_anchors", ",", "embeddings_positives", ")", "\n", "neg_manhatten_distances", "=", "paired_manhattan_distances", "(", "embeddings_anchors", ",", "embeddings_negatives", ")", "\n", "\n", "# Euclidean", "\n", "pos_euclidean_distance", "=", "paired_euclidean_distances", "(", "embeddings_anchors", ",", "embeddings_positives", ")", "\n", "neg_euclidean_distances", "=", "paired_euclidean_distances", "(", "embeddings_anchors", ",", "embeddings_negatives", ")", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "pos_cos_distance", ")", ")", ":", "\n", "            ", "num_triplets", "+=", "1", "\n", "\n", "if", "pos_cos_distance", "[", "idx", "]", "<", "neg_cos_distances", "[", "idx", "]", ":", "\n", "                ", "num_correct_cos_triplets", "+=", "1", "\n", "\n", "", "if", "pos_manhatten_distance", "[", "idx", "]", "<", "neg_manhatten_distances", "[", "idx", "]", ":", "\n", "                ", "num_correct_manhatten_triplets", "+=", "1", "\n", "\n", "", "if", "pos_euclidean_distance", "[", "idx", "]", "<", "neg_euclidean_distances", "[", "idx", "]", ":", "\n", "                ", "num_correct_euclidean_triplets", "+=", "1", "\n", "\n", "", "", "accuracy_cos", "=", "num_correct_cos_triplets", "/", "num_triplets", "\n", "accuracy_manhatten", "=", "num_correct_manhatten_triplets", "/", "num_triplets", "\n", "accuracy_euclidean", "=", "num_correct_euclidean_triplets", "/", "num_triplets", "\n", "\n", "logging", ".", "info", "(", "\"Accuracy Cosine Distance:   \\t{:.2f}\"", ".", "format", "(", "accuracy_cos", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Accuracy Manhatten Distance:\\t{:.2f}\"", ".", "format", "(", "accuracy_manhatten", "*", "100", ")", ")", "\n", "logging", ".", "info", "(", "\"Accuracy Euclidean Distance:\\t{:.2f}\\n\"", ".", "format", "(", "accuracy_euclidean", "*", "100", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "accuracy_cos", ",", "accuracy_manhatten", ",", "accuracy_euclidean", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "                ", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                    ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "accuracy_cos", ",", "accuracy_manhatten", ",", "accuracy_euclidean", "]", ")", "\n", "\n", "", "", "", "if", "self", ".", "main_distance_function", "==", "SimilarityFunction", ".", "COSINE", ":", "\n", "            ", "return", "accuracy_cos", "\n", "", "if", "self", ".", "main_distance_function", "==", "SimilarityFunction", ".", "MANHATTAN", ":", "\n", "            ", "return", "accuracy_manhatten", "\n", "", "if", "self", ".", "main_distance_function", "==", "SimilarityFunction", ".", "EUCLIDEAN", ":", "\n", "            ", "return", "accuracy_euclidean", "\n", "\n", "", "return", "max", "(", "accuracy_cos", ",", "accuracy_manhatten", ",", "accuracy_euclidean", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.SequentialEvaluator.SequentialEvaluator.__init__": [[11, 14], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "evaluators", ":", "Iterable", "[", "SentenceEvaluator", "]", ",", "main_score_function", "=", "lambda", "scores", ":", "scores", "[", "-", "1", "]", ")", ":", "\n", "        ", "self", ".", "evaluators", "=", "evaluators", "\n", "self", ".", "main_score_function", "=", "main_score_function", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.SequentialEvaluator.SequentialEvaluator.__call__": [[15, 21], ["SequentialEvaluator.SequentialEvaluator.main_score_function", "scores.append", "evaluator"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "scores", "=", "[", "]", "\n", "for", "evaluator", "in", "self", ".", "evaluators", ":", "\n", "            ", "scores", ".", "append", "(", "evaluator", "(", "model", ",", "output_path", ",", "epoch", ",", "steps", ")", ")", "\n", "\n", "", "return", "self", ".", "main_score_function", "(", "scores", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.SentenceEvaluator.SentenceEvaluator.__call__": [[8, 28], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        This is called during training to evaluate the model.\n        It returns a score for the evaluation with a higher score indicating a better result.\n\n        :param model:\n            the model to evaluate\n        :param output_path:\n            path where predictions and metrics are written to\n        :param epoch\n            the epoch where the evaluation takes place.\n            This is used for the file prefixes.\n            If this is -1, then we assume evaluation on test data.\n        :param steps\n            the steps in the current epoch at time of the evaluation.\n            This is used for the file prefixes.\n            If this is -1, then we assume evaluation at the end of the epoch.\n        :return: a score for the evaluation with a higher score indicating a better result\n        \"\"\"", "\n", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.MSEEvaluator.MSEEvaluator.__init__": [[13, 24], ["teacher_model.encode"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["def", "__init__", "(", "self", ",", "source_sentences", ":", "List", "[", "str", "]", ",", "target_sentences", ":", "List", "[", "str", "]", ",", "teacher_model", "=", "None", ",", "show_progress_bar", ":", "bool", "=", "False", ",", "batch_size", ":", "int", "=", "16", ",", "name", ":", "str", "=", "''", ")", ":", "\n", "        ", "self", ".", "source_sentences", "=", "source_sentences", "\n", "self", ".", "source_embeddings", "=", "teacher_model", ".", "encode", "(", "source_sentences", ",", "show_progress_bar", "=", "show_progress_bar", ",", "batch_size", "=", "batch_size", ",", "convert_to_numpy", "=", "True", ")", "\n", "\n", "self", ".", "target_sentences", "=", "target_sentences", "\n", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "name", "=", "name", "\n", "\n", "self", ".", "csv_file", "=", "\"mse_evaluation_\"", "+", "name", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\"MSE\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.MSEEvaluator.MSEEvaluator.__call__": [[25, 53], ["numpy.asarray", "logging.info", "logging.info", "model.encode", "os.path.join", "os.path.isfile", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ",", "epoch", "=", "-", "1", ",", "steps", "=", "-", "1", ")", ":", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "if", "steps", "==", "-", "1", ":", "\n", "                ", "out_txt", "=", "\" after epoch {}:\"", ".", "format", "(", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "out_txt", "=", "\" in epoch {} after {} steps:\"", ".", "format", "(", "epoch", ",", "steps", ")", "\n", "", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "target_embeddings", "=", "np", ".", "asarray", "(", "model", ".", "encode", "(", "self", ".", "source_sentences", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "batch_size", "=", "self", ".", "batch_size", ")", ")", "\n", "\n", "mse", "=", "(", "(", "self", ".", "source_embeddings", "-", "target_embeddings", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "mse", "*=", "100", "\n", "\n", "logging", ".", "info", "(", "\"MSE evaluation (lower = better) on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "logging", ".", "info", "(", "\"MSE (*100):\\t{:4f}\"", ".", "format", "(", "mse", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "output_file_exists", "=", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", "\n", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", "if", "output_file_exists", "else", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "if", "not", "output_file_exists", ":", "\n", "                    ", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "\n", "", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "mse", "]", ")", "\n", "\n", "", "", "return", "-", "mse", "#Return negative score as SentenceTransformers maximizes the performance", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator.__init__": [[21, 52], ["len", "len", "len", "len", "logging.getLogger().getEffectiveLevel", "logging.getLogger().getEffectiveLevel", "logging.getLogger", "logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentences1", ":", "List", "[", "str", "]", ",", "sentences2", ":", "List", "[", "str", "]", ",", "scores", ":", "List", "[", "float", "]", ",", "batch_size", ":", "int", "=", "16", ",", "main_similarity", ":", "SimilarityFunction", "=", "None", ",", "name", ":", "str", "=", "''", ",", "show_progress_bar", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Constructs an evaluator based for the dataset\n\n        The labels need to indicate the similarity between the sentences.\n\n        :param sentences1:\n            List with the first sentence in a pair\n        :param sentences2:\n            List with the second sentence in a pair\n        :param scores:\n            Similarity score between sentences1[i] and sentences2[i]\n\n        \"\"\"", "\n", "self", ".", "sentences1", "=", "sentences1", "\n", "self", ".", "sentences2", "=", "sentences2", "\n", "self", ".", "scores", "=", "scores", "\n", "\n", "assert", "len", "(", "self", ".", "sentences1", ")", "==", "len", "(", "self", ".", "sentences2", ")", "\n", "assert", "len", "(", "self", ".", "sentences1", ")", "==", "len", "(", "self", ".", "scores", ")", "\n", "\n", "self", ".", "main_similarity", "=", "main_similarity", "\n", "self", ".", "name", "=", "name", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "if", "show_progress_bar", "is", "None", ":", "\n", "            ", "show_progress_bar", "=", "(", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "INFO", "or", "logging", ".", "getLogger", "(", ")", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "DEBUG", ")", "\n", "", "self", ".", "show_progress_bar", "=", "show_progress_bar", "\n", "\n", "self", ".", "csv_file", "=", "\"similarity_evaluation\"", "+", "(", "\"_\"", "+", "name", "if", "name", "else", "''", ")", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", ",", "\"cosine_pearson\"", ",", "\"cosine_spearman\"", ",", "\"euclidean_pearson\"", ",", "\"euclidean_spearman\"", ",", "\"manhattan_pearson\"", ",", "\"manhattan_spearman\"", ",", "\"dot_pearson\"", ",", "\"dot_spearman\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator.from_input_examples": [[53, 64], ["cls", "sentences1.append", "sentences2.append", "scores.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_input_examples", "(", "cls", ",", "examples", ":", "List", "[", "InputExample", "]", ",", "**", "kwargs", ")", ":", "\n", "        ", "sentences1", "=", "[", "]", "\n", "sentences2", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "\n", "for", "example", "in", "examples", ":", "\n", "            ", "sentences1", ".", "append", "(", "example", ".", "texts", "[", "0", "]", ")", "\n", "sentences2", ".", "append", "(", "example", ".", "texts", "[", "1", "]", ")", "\n", "scores", ".", "append", "(", "example", ".", "label", ")", "\n", "", "return", "cls", "(", "sentences1", ",", "sentences2", ",", "scores", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator.__call__": [[66, 132], ["logging.info", "model.encode", "model.encode", "scipy.stats.pearsonr", "scipy.stats.spearmanr", "scipy.stats.pearsonr", "scipy.stats.spearmanr", "scipy.stats.pearsonr", "scipy.stats.spearmanr", "scipy.stats.pearsonr", "scipy.stats.spearmanr", "logging.info", "logging.info", "logging.info", "logging.info", "sklearn.metrics.pairwise.paired_cosine_distances", "sklearn.metrics.pairwise.paired_manhattan_distances", "sklearn.metrics.pairwise.paired_euclidean_distances", "numpy.dot", "os.path.join", "os.path.isfile", "zip", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow", "max", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", "->", "float", ":", "\n", "        ", "if", "epoch", "!=", "-", "1", ":", "\n", "            ", "if", "steps", "==", "-", "1", ":", "\n", "                ", "out_txt", "=", "\" after epoch {}:\"", ".", "format", "(", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "out_txt", "=", "\" in epoch {} after {} steps:\"", ".", "format", "(", "epoch", ",", "steps", ")", "\n", "", "", "else", ":", "\n", "            ", "out_txt", "=", "\":\"", "\n", "\n", "", "logging", ".", "info", "(", "\"Evaluation the model on \"", "+", "self", ".", "name", "+", "\" dataset\"", "+", "out_txt", ")", "\n", "\n", "embeddings1", "=", "model", ".", "encode", "(", "self", ".", "sentences1", ",", "batch_size", "=", "self", ".", "batch_size", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "embeddings2", "=", "model", ".", "encode", "(", "self", ".", "sentences2", ",", "batch_size", "=", "self", ".", "batch_size", ",", "show_progress_bar", "=", "self", ".", "show_progress_bar", ",", "convert_to_numpy", "=", "True", ")", "\n", "labels", "=", "self", ".", "scores", "\n", "\n", "cosine_scores", "=", "1", "-", "(", "paired_cosine_distances", "(", "embeddings1", ",", "embeddings2", ")", ")", "\n", "manhattan_distances", "=", "-", "paired_manhattan_distances", "(", "embeddings1", ",", "embeddings2", ")", "\n", "euclidean_distances", "=", "-", "paired_euclidean_distances", "(", "embeddings1", ",", "embeddings2", ")", "\n", "dot_products", "=", "[", "np", ".", "dot", "(", "emb1", ",", "emb2", ")", "for", "emb1", ",", "emb2", "in", "zip", "(", "embeddings1", ",", "embeddings2", ")", "]", "\n", "\n", "\n", "eval_pearson_cosine", ",", "_", "=", "pearsonr", "(", "labels", ",", "cosine_scores", ")", "\n", "eval_spearman_cosine", ",", "_", "=", "spearmanr", "(", "labels", ",", "cosine_scores", ")", "\n", "\n", "eval_pearson_manhattan", ",", "_", "=", "pearsonr", "(", "labels", ",", "manhattan_distances", ")", "\n", "eval_spearman_manhattan", ",", "_", "=", "spearmanr", "(", "labels", ",", "manhattan_distances", ")", "\n", "\n", "eval_pearson_euclidean", ",", "_", "=", "pearsonr", "(", "labels", ",", "euclidean_distances", ")", "\n", "eval_spearman_euclidean", ",", "_", "=", "spearmanr", "(", "labels", ",", "euclidean_distances", ")", "\n", "\n", "eval_pearson_dot", ",", "_", "=", "pearsonr", "(", "labels", ",", "dot_products", ")", "\n", "eval_spearman_dot", ",", "_", "=", "spearmanr", "(", "labels", ",", "dot_products", ")", "\n", "\n", "logging", ".", "info", "(", "\"Cosine-Similarity :\\tPearson: {:.4f}\\tSpearman: {:.4f}\"", ".", "format", "(", "\n", "eval_pearson_cosine", ",", "eval_spearman_cosine", ")", ")", "\n", "logging", ".", "info", "(", "\"Manhattan-Distance:\\tPearson: {:.4f}\\tSpearman: {:.4f}\"", ".", "format", "(", "\n", "eval_pearson_manhattan", ",", "eval_spearman_manhattan", ")", ")", "\n", "logging", ".", "info", "(", "\"Euclidean-Distance:\\tPearson: {:.4f}\\tSpearman: {:.4f}\"", ".", "format", "(", "\n", "eval_pearson_euclidean", ",", "eval_spearman_euclidean", ")", ")", "\n", "logging", ".", "info", "(", "\"Dot-Product-Similarity:\\tPearson: {:.4f}\\tSpearman: {:.4f}\"", ".", "format", "(", "\n", "eval_pearson_dot", ",", "eval_spearman_dot", ")", ")", "\n", "\n", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "output_file_exists", "=", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", "\n", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", "if", "output_file_exists", "else", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "if", "not", "output_file_exists", ":", "\n", "                    ", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "\n", "", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", ",", "eval_pearson_cosine", ",", "eval_spearman_cosine", ",", "eval_pearson_euclidean", ",", "\n", "eval_spearman_euclidean", ",", "eval_pearson_manhattan", ",", "eval_spearman_manhattan", ",", "eval_pearson_dot", ",", "eval_spearman_dot", "]", ")", "\n", "\n", "\n", "", "", "if", "self", ".", "main_similarity", "==", "SimilarityFunction", ".", "COSINE", ":", "\n", "            ", "return", "eval_spearman_cosine", "\n", "", "elif", "self", ".", "main_similarity", "==", "SimilarityFunction", ".", "EUCLIDEAN", ":", "\n", "            ", "return", "eval_spearman_euclidean", "\n", "", "elif", "self", ".", "main_similarity", "==", "SimilarityFunction", ".", "MANHATTAN", ":", "\n", "            ", "return", "eval_spearman_manhattan", "\n", "", "elif", "self", ".", "main_similarity", "==", "SimilarityFunction", ".", "DOT_PRODUCT", ":", "\n", "            ", "return", "eval_spearman_dot", "\n", "", "elif", "self", ".", "main_similarity", "is", "None", ":", "\n", "            ", "return", "max", "(", "eval_spearman_cosine", ",", "eval_spearman_manhattan", ",", "eval_spearman_euclidean", ",", "eval_spearman_dot", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown main_similarity value\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.MSEEvaluatorFromDataFrame.MSEEvaluatorFromDataFrame.__init__": [[25, 57], ["logging.info", "set", "list", "teacher_model.encode", "MSEEvaluatorFromDataFrame.MSEEvaluatorFromDataFrame.csv_headers.append", "zip", "list.add", "src_sentences.append", "trg_sentences.append", "row[].strip", "row[].strip"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["def", "__init__", "(", "self", ",", "dataframe", ":", "List", "[", "Dict", "[", "str", ",", "str", "]", "]", ",", "teacher_model", ":", "SentenceTransformer", ",", "combinations", ":", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "batch_size", ":", "int", "=", "8", ",", "name", "=", "''", ")", ":", "\n", "\n", "        ", "self", ".", "combinations", "=", "combinations", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "\n", "if", "name", ":", "\n", "            ", "name", "=", "\"_\"", "+", "name", "\n", "\n", "", "self", ".", "csv_file", "=", "\"mse_evaluation\"", "+", "name", "+", "\"_results.csv\"", "\n", "self", ".", "csv_headers", "=", "[", "\"epoch\"", ",", "\"steps\"", "]", "\n", "self", ".", "data", "=", "{", "}", "\n", "\n", "logging", ".", "info", "(", "\"Compute teacher embeddings\"", ")", "\n", "all_source_sentences", "=", "set", "(", ")", "\n", "for", "src_lang", ",", "trg_lang", "in", "self", ".", "combinations", ":", "\n", "            ", "src_sentences", "=", "[", "]", "\n", "trg_sentences", "=", "[", "]", "\n", "\n", "for", "row", "in", "dataframe", ":", "\n", "                ", "if", "row", "[", "src_lang", "]", ".", "strip", "(", ")", "!=", "\"\"", "and", "row", "[", "trg_lang", "]", ".", "strip", "(", ")", "!=", "\"\"", ":", "\n", "                    ", "all_source_sentences", ".", "add", "(", "row", "[", "src_lang", "]", ")", "\n", "src_sentences", ".", "append", "(", "row", "[", "src_lang", "]", ")", "\n", "trg_sentences", ".", "append", "(", "row", "[", "trg_lang", "]", ")", "\n", "\n", "", "", "self", ".", "data", "[", "(", "src_lang", ",", "trg_lang", ")", "]", "=", "(", "src_sentences", ",", "trg_sentences", ")", "\n", "self", ".", "csv_headers", ".", "append", "(", "\"{}-{}\"", ".", "format", "(", "src_lang", ",", "trg_lang", ")", ")", "\n", "\n", "", "all_source_sentences", "=", "list", "(", "all_source_sentences", ")", "\n", "all_src_embeddings", "=", "teacher_model", ".", "encode", "(", "all_source_sentences", ",", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "self", ".", "teacher_embeddings", "=", "{", "sent", ":", "emb", "for", "sent", ",", "emb", "in", "zip", "(", "all_source_sentences", ",", "all_src_embeddings", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.evaluation.MSEEvaluatorFromDataFrame.MSEEvaluatorFromDataFrame.__call__": [[58, 86], ["model.eval", "numpy.asarray", "numpy.asarray", "mse_scores.append", "logging.info", "logging.info", "os.path.join", "os.path.isfile", "numpy.mean", "model.encode", "open", "csv.writer", "csv.writer.writerow", "csv.writer.writerow"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.SentenceTransformer.SentenceTransformer.encode"], ["", "def", "__call__", "(", "self", ",", "model", ",", "output_path", ":", "str", "=", "None", ",", "epoch", ":", "int", "=", "-", "1", ",", "steps", ":", "int", "=", "-", "1", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "\n", "mse_scores", "=", "[", "]", "\n", "for", "src_lang", ",", "trg_lang", "in", "self", ".", "combinations", ":", "\n", "            ", "src_sentences", ",", "trg_sentences", "=", "self", ".", "data", "[", "(", "src_lang", ",", "trg_lang", ")", "]", "\n", "\n", "src_embeddings", "=", "np", ".", "asarray", "(", "[", "self", ".", "teacher_embeddings", "[", "sent", "]", "for", "sent", "in", "src_sentences", "]", ")", "\n", "trg_embeddings", "=", "np", ".", "asarray", "(", "model", ".", "encode", "(", "trg_sentences", ",", "batch_size", "=", "self", ".", "batch_size", ")", ")", "\n", "\n", "mse", "=", "(", "(", "src_embeddings", "-", "trg_embeddings", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "mse", "*=", "100", "\n", "mse_scores", ".", "append", "(", "mse", ")", "\n", "\n", "logging", ".", "info", "(", "\"MSE evaluation on {} dataset - {}-{}:\"", ".", "format", "(", "self", ".", "name", ",", "src_lang", ",", "trg_lang", ")", ")", "\n", "logging", ".", "info", "(", "\"MSE (*100):\\t{:4f}\"", ".", "format", "(", "mse", ")", ")", "\n", "\n", "", "if", "output_path", "is", "not", "None", ":", "\n", "            ", "csv_path", "=", "os", ".", "path", ".", "join", "(", "output_path", ",", "self", ".", "csv_file", ")", "\n", "output_file_exists", "=", "os", ".", "path", ".", "isfile", "(", "csv_path", ")", "\n", "with", "open", "(", "csv_path", ",", "mode", "=", "\"a\"", "if", "output_file_exists", "else", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "writer", "=", "csv", ".", "writer", "(", "f", ")", "\n", "if", "not", "output_file_exists", ":", "\n", "                    ", "writer", ".", "writerow", "(", "self", ".", "csv_headers", ")", "\n", "\n", "", "writer", ".", "writerow", "(", "[", "epoch", ",", "steps", "]", "+", "mse_scores", ")", "\n", "\n", "", "", "return", "-", "np", ".", "mean", "(", "mse_scores", ")", "#Return negative score as SentenceTransformers maximizes the performance", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.CosineSimilarityLoss.CosineSimilarityLoss.__init__": [[31, 36], ["torch.nn.MSELoss", "torch.nn.Identity", "torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "loss_fct", "=", "nn", ".", "MSELoss", "(", ")", ",", "cos_score_transformation", "=", "nn", ".", "Identity", "(", ")", ")", ":", "\n", "        ", "super", "(", "CosineSimilarityLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "loss_fct", "=", "loss_fct", "\n", "self", ".", "cos_score_transformation", "=", "cos_score_transformation", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.CosineSimilarityLoss.CosineSimilarityLoss.forward": [[38, 42], ["CosineSimilarityLoss.CosineSimilarityLoss.cos_score_transformation", "CosineSimilarityLoss.CosineSimilarityLoss.loss_fct", "torch.cosine_similarity", "labels.view", "CosineSimilarityLoss.CosineSimilarityLoss.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "embeddings", "=", "[", "self", ".", "model", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "output", "=", "self", ".", "cos_score_transformation", "(", "torch", ".", "cosine_similarity", "(", "embeddings", "[", "0", "]", ",", "embeddings", "[", "1", "]", ")", ")", "\n", "return", "self", ".", "loss_fct", "(", "output", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MSELoss.MSELoss.__init__": [[14, 17], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ")", ":", "\n", "        ", "super", "(", "MSELoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MSELoss.MSELoss.forward": [[18, 23], ["torch.nn.MSELoss", "torch.nn.MSELoss.", "MSELoss.MSELoss.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "rep", "=", "self", ".", "model", "(", "sentence_features", "[", "0", "]", ")", "[", "'sentence_embedding'", "]", "\n", "loss_fct", "=", "nn", ".", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "rep", ",", "labels", ")", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.SoftmaxLoss.SoftmaxLoss.__init__": [[31, 54], ["torch.nn.Module.__init__", "logging.info", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "\n", "model", ":", "SentenceTransformer", ",", "\n", "sentence_embedding_dimension", ":", "int", ",", "\n", "num_labels", ":", "int", ",", "\n", "concatenation_sent_rep", ":", "bool", "=", "True", ",", "\n", "concatenation_sent_difference", ":", "bool", "=", "True", ",", "\n", "concatenation_sent_multiplication", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "SoftmaxLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "concatenation_sent_rep", "=", "concatenation_sent_rep", "\n", "self", ".", "concatenation_sent_difference", "=", "concatenation_sent_difference", "\n", "self", ".", "concatenation_sent_multiplication", "=", "concatenation_sent_multiplication", "\n", "\n", "num_vectors_concatenated", "=", "0", "\n", "if", "concatenation_sent_rep", ":", "\n", "            ", "num_vectors_concatenated", "+=", "2", "\n", "", "if", "concatenation_sent_difference", ":", "\n", "            ", "num_vectors_concatenated", "+=", "1", "\n", "", "if", "concatenation_sent_multiplication", ":", "\n", "            ", "num_vectors_concatenated", "+=", "1", "\n", "", "logging", ".", "info", "(", "\"Softmax loss: #Vectors concatenated: {}\"", ".", "format", "(", "num_vectors_concatenated", ")", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "num_vectors_concatenated", "*", "sentence_embedding_dimension", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.SoftmaxLoss.SoftmaxLoss.forward": [[55, 80], ["torch.cat", "SoftmaxLoss.SoftmaxLoss.classifier", "torch.nn.CrossEntropyLoss", "vectors_concat.append", "vectors_concat.append", "vectors_concat.append", "vectors_concat.append", "torch.nn.CrossEntropyLoss.", "SoftmaxLoss.SoftmaxLoss.model", "torch.abs", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "model", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "rep_a", ",", "rep_b", "=", "reps", "\n", "\n", "vectors_concat", "=", "[", "]", "\n", "if", "self", ".", "concatenation_sent_rep", ":", "\n", "            ", "vectors_concat", ".", "append", "(", "rep_a", ")", "\n", "vectors_concat", ".", "append", "(", "rep_b", ")", "\n", "\n", "", "if", "self", ".", "concatenation_sent_difference", ":", "\n", "            ", "vectors_concat", ".", "append", "(", "torch", ".", "abs", "(", "rep_a", "-", "rep_b", ")", ")", "\n", "\n", "", "if", "self", ".", "concatenation_sent_multiplication", ":", "\n", "            ", "vectors_concat", ".", "append", "(", "rep_a", "*", "rep_b", ")", "\n", "\n", "", "features", "=", "torch", ".", "cat", "(", "vectors_concat", ",", "1", ")", "\n", "\n", "output", "=", "self", ".", "classifier", "(", "features", ")", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss", "=", "loss_fct", "(", "output", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reps", ",", "output", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.__init__": [[36, 41], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "BatchHardTripletLossDistanceFunction", ".", "eucledian_distance", ",", "margin", ":", "float", "=", "5", ")", ":", "\n", "        ", "super", "(", "BatchSemiHardTripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sentence_embedder", "=", "model", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.forward": [[42, 45], ["BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.batch_semi_hard_triplet_loss", "BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.sentence_embedder"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.batch_semi_hard_triplet_loss"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "sentence_embedder", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "return", "self", ".", "batch_semi_hard_triplet_loss", "(", "labels", ",", "reps", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.batch_semi_hard_triplet_loss": [[50, 94], ["labels.unsqueeze.unsqueeze.unsqueeze", "BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.distance_metric", "torch.numel", "BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.repeat", "torch.reshape", "mask_final.t.t.t", "torch.reshape", "negatives_outside.t.t.t", "BatchSemiHardTripletLoss._masked_maximum", "negatives_inside.repeat.repeat.repeat", "torch.where", "mask_positives.to.to.to", "torch.sum", "labels.unsqueeze.unsqueeze.t", "adjacency_not.repeat", "BatchSemiHardTripletLoss._masked_minimum", "adjacency.float().to", "torch.eye", "torch.sum", "torch.reshape", "torch.sum", "torch.max", "BatchSemiHardTripletLoss.BatchSemiHardTripletLoss.t", "adjacency.float", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss._masked_maximum", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss._masked_minimum"], ["", "def", "batch_semi_hard_triplet_loss", "(", "self", ",", "labels", ":", "Tensor", ",", "embeddings", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"Build the triplet loss over a batch of embeddings.\n        We generate all the valid triplets and average the loss over the positive ones.\n        Args:\n            labels: labels of the batch, of size (batch_size,)\n            embeddings: tensor of shape (batch_size, embed_dim)\n            margin: margin for triplet loss\n            squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                     If false, output is the pairwise euclidean distance matrix.\n        Returns:\n            Label_Sentence_Triplet: scalar tensor containing the triplet loss\n        \"\"\"", "\n", "labels", "=", "labels", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "pdist_matrix", "=", "self", ".", "distance_metric", "(", "embeddings", ")", "\n", "\n", "adjacency", "=", "labels", "==", "labels", ".", "t", "(", ")", "\n", "adjacency_not", "=", "~", "adjacency", "\n", "\n", "batch_size", "=", "torch", ".", "numel", "(", "labels", ")", "\n", "pdist_matrix_tile", "=", "pdist_matrix", ".", "repeat", "(", "[", "batch_size", ",", "1", "]", ")", "\n", "\n", "mask", "=", "adjacency_not", ".", "repeat", "(", "[", "batch_size", ",", "1", "]", ")", "&", "(", "pdist_matrix_tile", ">", "torch", ".", "reshape", "(", "pdist_matrix", ".", "t", "(", ")", ",", "[", "-", "1", ",", "1", "]", ")", ")", "\n", "\n", "mask_final", "=", "torch", ".", "reshape", "(", "torch", ".", "sum", "(", "mask", ",", "1", ",", "keepdims", "=", "True", ")", ">", "0.0", ",", "[", "batch_size", ",", "batch_size", "]", ")", "\n", "mask_final", "=", "mask_final", ".", "t", "(", ")", "\n", "\n", "negatives_outside", "=", "torch", ".", "reshape", "(", "BatchSemiHardTripletLoss", ".", "_masked_minimum", "(", "pdist_matrix_tile", ",", "mask", ")", ",", "[", "batch_size", ",", "batch_size", "]", ")", "\n", "negatives_outside", "=", "negatives_outside", ".", "t", "(", ")", "\n", "\n", "negatives_inside", "=", "BatchSemiHardTripletLoss", ".", "_masked_maximum", "(", "pdist_matrix", ",", "adjacency_not", ")", "\n", "negatives_inside", "=", "negatives_inside", ".", "repeat", "(", "[", "1", ",", "batch_size", "]", ")", "\n", "\n", "semi_hard_negatives", "=", "torch", ".", "where", "(", "mask_final", ",", "negatives_outside", ",", "negatives_inside", ")", "\n", "\n", "loss_mat", "=", "(", "pdist_matrix", "-", "semi_hard_negatives", ")", "+", "self", ".", "margin", "\n", "\n", "mask_positives", "=", "adjacency", ".", "float", "(", ")", ".", "to", "(", "labels", ".", "device", ")", "-", "torch", ".", "eye", "(", "batch_size", ",", "device", "=", "labels", ".", "device", ")", "\n", "mask_positives", "=", "mask_positives", ".", "to", "(", "labels", ".", "device", ")", "\n", "num_positives", "=", "torch", ".", "sum", "(", "mask_positives", ")", "\n", "\n", "triplet_loss", "=", "torch", ".", "sum", "(", "torch", ".", "max", "(", "loss_mat", "*", "mask_positives", ",", "torch", ".", "tensor", "(", "[", "0.0", "]", ",", "device", "=", "labels", ".", "device", ")", ")", ")", "/", "num_positives", "\n", "\n", "return", "triplet_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss._masked_minimum": [[95, 103], ["data.max", "masked_minimums.min"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_masked_minimum", "(", "data", ",", "mask", ",", "dim", "=", "1", ")", ":", "\n", "        ", "axis_maximums", ",", "_", "=", "data", ".", "max", "(", "dim", ",", "keepdims", "=", "True", ")", "\n", "masked_minimums", "=", "(", "data", "-", "axis_maximums", ")", "*", "mask", "\n", "masked_minimums", ",", "_", "=", "masked_minimums", ".", "min", "(", "dim", ",", "keepdims", "=", "True", ")", "\n", "masked_minimums", "+=", "axis_maximums", "\n", "\n", "return", "masked_minimums", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchSemiHardTripletLoss.BatchSemiHardTripletLoss._masked_maximum": [[104, 112], ["data.min", "masked_maximums.max"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_masked_maximum", "(", "data", ",", "mask", ",", "dim", "=", "1", ")", ":", "\n", "        ", "axis_minimums", ",", "_", "=", "data", ".", "min", "(", "dim", ",", "keepdims", "=", "True", ")", "\n", "masked_maximums", "=", "(", "data", "-", "axis_minimums", ")", "*", "mask", "\n", "masked_maximums", ",", "_", "=", "masked_maximums", ".", "max", "(", "dim", ",", "keepdims", "=", "True", ")", "\n", "masked_maximums", "+=", "axis_minimums", "\n", "\n", "return", "masked_maximums", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.OnlineContrastiveLoss.OnlineContrastiveLoss.__init__": [[32, 37], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "SiameseDistanceMetric", ".", "COSINE_DISTANCE", ",", "margin", ":", "float", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "OnlineContrastiveLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.OnlineContrastiveLoss.OnlineContrastiveLoss.forward": [[38, 53], ["OnlineContrastiveLoss.OnlineContrastiveLoss.distance_metric", "positive_pairs.pow().sum", "torch.relu().pow().sum", "OnlineContrastiveLoss.OnlineContrastiveLoss.model", "positive_pairs.pow", "torch.relu().pow", "poss.max", "negs.mean", "negs.min", "poss.mean", "torch.relu", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ",", "size_average", "=", "False", ")", ":", "\n", "        ", "embeddings", "=", "[", "self", ".", "model", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "\n", "distance_matrix", "=", "self", ".", "distance_metric", "(", "embeddings", "[", "0", "]", ",", "embeddings", "[", "1", "]", ")", "\n", "negs", "=", "distance_matrix", "[", "labels", "==", "0", "]", "\n", "poss", "=", "distance_matrix", "[", "labels", "==", "1", "]", "\n", "\n", "# select hard positive and hard negative pairs", "\n", "negative_pairs", "=", "negs", "[", "negs", "<", "(", "poss", ".", "max", "(", ")", "if", "len", "(", "poss", ")", ">", "1", "else", "negs", ".", "mean", "(", ")", ")", "]", "\n", "positive_pairs", "=", "poss", "[", "poss", ">", "(", "negs", ".", "min", "(", ")", "if", "len", "(", "negs", ")", ">", "1", "else", "poss", ".", "mean", "(", ")", ")", "]", "\n", "\n", "positive_loss", "=", "positive_pairs", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "negative_loss", "=", "F", ".", "relu", "(", "self", ".", "margin", "-", "negative_pairs", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "loss", "=", "positive_loss", "+", "negative_loss", "\n", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.__init__": [[34, 38], ["BatchHardTripletLoss.BatchHardTripletLoss.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "BatchHardTripletLossDistanceFunction", ".", "eucledian_distance", ")", ":", "\n", "        ", "super", "(", "BatchHardSoftMarginTripletLoss", ",", "self", ")", ".", "__init__", "(", "model", ")", "\n", "self", ".", "sentence_embedder", "=", "model", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.forward": [[39, 43], ["BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.batch_hard_triplet_soft_margin_loss", "BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.sentence_embedder"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.batch_hard_triplet_soft_margin_loss"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "sentence_embedder", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "\n", "return", "self", ".", "batch_hard_triplet_soft_margin_loss", "(", "labels", ",", "reps", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.batch_hard_triplet_soft_margin_loss": [[47, 90], ["BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.distance_metric", "BatchHardTripletLoss.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_positive_triplet_mask().float", "anchor_positive_dist.max", "BatchHardTripletLoss.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_negative_triplet_mask().float", "BatchHardSoftMarginTripletLoss.BatchHardSoftMarginTripletLoss.max", "anchor_negative_dist.min", "torch.log1p", "torch.log1p.mean", "torch.exp", "BatchHardTripletLoss.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_positive_triplet_mask", "BatchHardTripletLoss.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_negative_triplet_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_positive_triplet_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_negative_triplet_mask"], ["", "def", "batch_hard_triplet_soft_margin_loss", "(", "self", ",", "labels", ":", "Tensor", ",", "embeddings", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"Build the triplet loss over a batch of embeddings.\n        For each anchor, we get the hardest positive and hardest negative to form a triplet.\n        Args:\n            labels: labels of the batch, of size (batch_size,)\n            embeddings: tensor of shape (batch_size, embed_dim)\n            squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                     If false, output is the pairwise euclidean distance matrix.\n        Returns:\n            Label_Sentence_Triplet: scalar tensor containing the triplet loss\n        \"\"\"", "\n", "# Get the pairwise distance matrix", "\n", "pairwise_dist", "=", "self", ".", "distance_metric", "(", "embeddings", ")", "\n", "\n", "\n", "# For each anchor, get the hardest positive", "\n", "# First, we need to get a mask for every valid positive (they should have same label)", "\n", "mask_anchor_positive", "=", "BatchHardTripletLoss", ".", "get_anchor_positive_triplet_mask", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "# We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))", "\n", "anchor_positive_dist", "=", "mask_anchor_positive", "*", "pairwise_dist", "\n", "\n", "# shape (batch_size, 1)", "\n", "hardest_positive_dist", ",", "_", "=", "anchor_positive_dist", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# For each anchor, get the hardest negative", "\n", "# First, we need to get a mask for every valid negative (they should have different labels)", "\n", "mask_anchor_negative", "=", "BatchHardTripletLoss", ".", "get_anchor_negative_triplet_mask", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "# We add the maximum value in each row to the invalid negatives (label(a) == label(n))", "\n", "max_anchor_negative_dist", ",", "_", "=", "pairwise_dist", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "anchor_negative_dist", "=", "pairwise_dist", "+", "max_anchor_negative_dist", "*", "(", "1.0", "-", "mask_anchor_negative", ")", "\n", "\n", "# shape (batch_size,)", "\n", "hardest_negative_dist", ",", "_", "=", "anchor_negative_dist", ".", "min", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# Combine biggest d(a, p) and smallest d(a, n) into final triplet loss with soft margin", "\n", "#tl = hardest_positive_dist - hardest_negative_dist + margin", "\n", "#tl[tl < 0] = 0", "\n", "tl", "=", "torch", ".", "log1p", "(", "torch", ".", "exp", "(", "hardest_positive_dist", "-", "hardest_negative_dist", ")", ")", "\n", "triplet_loss", "=", "tl", ".", "mean", "(", ")", "\n", "\n", "return", "triplet_loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLossDistanceFunction.cosine_distance": [[12, 18], ["sentence_transformers.util.pytorch_cos_sim"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.sentence_transformers.util.pytorch_cos_sim"], ["@", "staticmethod", "\n", "def", "cosine_distance", "(", "embeddings", ")", ":", "\n", "        ", "\"\"\"\n        Compute the 2D matrix of cosine distances (1-cosine_similarity) between all embeddings.\n        \"\"\"", "\n", "return", "1", "-", "util", ".", "pytorch_cos_sim", "(", "embeddings", ",", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLossDistanceFunction.eucledian_distance": [[19, 55], ["torch.matmul", "torch.diag", "embeddings.t", "torch.diag.unsqueeze", "distances.eq().float", "torch.diag.unsqueeze", "torch.sqrt", "distances.eq"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "eucledian_distance", "(", "embeddings", ",", "squared", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Compute the 2D matrix of eucledian distances between all the embeddings.\n        Args:\n            embeddings: tensor of shape (batch_size, embed_dim)\n            squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                     If false, output is the pairwise euclidean distance matrix.\n        Returns:\n            pairwise_distances: tensor of shape (batch_size, batch_size)\n        \"\"\"", "\n", "\n", "dot_product", "=", "torch", ".", "matmul", "(", "embeddings", ",", "embeddings", ".", "t", "(", ")", ")", "\n", "\n", "# Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.", "\n", "# This also provides more numerical stability (the diagonal of the result will be exactly 0).", "\n", "# shape (batch_size,)", "\n", "square_norm", "=", "torch", ".", "diag", "(", "dot_product", ")", "\n", "\n", "# Compute the pairwise distance matrix as we have:", "\n", "# ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2", "\n", "# shape (batch_size, batch_size)", "\n", "distances", "=", "square_norm", ".", "unsqueeze", "(", "0", ")", "-", "2.0", "*", "dot_product", "+", "square_norm", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Because of computation errors, some distances might be negative so we put everything >= 0.0", "\n", "distances", "[", "distances", "<", "0", "]", "=", "0", "\n", "\n", "if", "not", "squared", ":", "\n", "# Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)", "\n", "# we need to add a small epsilon where distances == 0.0", "\n", "            ", "mask", "=", "distances", ".", "eq", "(", "0", ")", ".", "float", "(", ")", "\n", "distances", "=", "distances", "+", "mask", "*", "1e-16", "\n", "\n", "distances", "=", "(", "1.0", "-", "mask", ")", "*", "torch", ".", "sqrt", "(", "distances", ")", "\n", "\n", "", "return", "distances", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.__init__": [[85, 90], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "BatchHardTripletLossDistanceFunction", ".", "eucledian_distance", ",", "margin", ":", "float", "=", "5", ")", ":", "\n", "        ", "super", "(", "BatchHardTripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sentence_embedder", "=", "model", "\n", "self", ".", "triplet_margin", "=", "margin", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.forward": [[91, 94], ["BatchHardTripletLoss.BatchHardTripletLoss.batch_hard_triplet_loss", "BatchHardTripletLoss.BatchHardTripletLoss.sentence_embedder"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.batch_hard_triplet_loss"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "sentence_embedder", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "return", "self", ".", "batch_hard_triplet_loss", "(", "labels", ",", "reps", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.batch_hard_triplet_loss": [[100, 142], ["BatchHardTripletLoss.BatchHardTripletLoss.distance_metric", "BatchHardTripletLoss.get_anchor_positive_triplet_mask().float", "anchor_positive_dist.max", "BatchHardTripletLoss.get_anchor_negative_triplet_mask().float", "BatchHardTripletLoss.BatchHardTripletLoss.max", "anchor_negative_dist.min", "tl.mean", "BatchHardTripletLoss.get_anchor_positive_triplet_mask", "BatchHardTripletLoss.get_anchor_negative_triplet_mask"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_positive_triplet_mask", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_negative_triplet_mask"], ["", "def", "batch_hard_triplet_loss", "(", "self", ",", "labels", ":", "Tensor", ",", "embeddings", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"Build the triplet loss over a batch of embeddings.\n        For each anchor, we get the hardest positive and hardest negative to form a triplet.\n        Args:\n            labels: labels of the batch, of size (batch_size,)\n            embeddings: tensor of shape (batch_size, embed_dim)\n            margin: margin for triplet loss\n            squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                     If false, output is the pairwise euclidean distance matrix.\n        Returns:\n            Label_Sentence_Triplet: scalar tensor containing the triplet loss\n        \"\"\"", "\n", "# Get the pairwise distance matrix", "\n", "pairwise_dist", "=", "self", ".", "distance_metric", "(", "embeddings", ")", "\n", "\n", "# For each anchor, get the hardest positive", "\n", "# First, we need to get a mask for every valid positive (they should have same label)", "\n", "mask_anchor_positive", "=", "BatchHardTripletLoss", ".", "get_anchor_positive_triplet_mask", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "# We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))", "\n", "anchor_positive_dist", "=", "mask_anchor_positive", "*", "pairwise_dist", "\n", "\n", "# shape (batch_size, 1)", "\n", "hardest_positive_dist", ",", "_", "=", "anchor_positive_dist", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# For each anchor, get the hardest negative", "\n", "# First, we need to get a mask for every valid negative (they should have different labels)", "\n", "mask_anchor_negative", "=", "BatchHardTripletLoss", ".", "get_anchor_negative_triplet_mask", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "# We add the maximum value in each row to the invalid negatives (label(a) == label(n))", "\n", "max_anchor_negative_dist", ",", "_", "=", "pairwise_dist", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "anchor_negative_dist", "=", "pairwise_dist", "+", "max_anchor_negative_dist", "*", "(", "1.0", "-", "mask_anchor_negative", ")", "\n", "\n", "# shape (batch_size,)", "\n", "hardest_negative_dist", ",", "_", "=", "anchor_negative_dist", ".", "min", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# Combine biggest d(a, p) and smallest d(a, n) into final triplet loss", "\n", "tl", "=", "hardest_positive_dist", "-", "hardest_negative_dist", "+", "self", ".", "triplet_margin", "\n", "tl", "[", "tl", "<", "0", "]", "=", "0", "\n", "triplet_loss", "=", "tl", ".", "mean", "(", ")", "\n", "\n", "return", "triplet_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_triplet_mask": [[145, 170], ["torch.eye().bool", "indices_not_equal.unsqueeze", "indices_not_equal.unsqueeze", "indices_not_equal.unsqueeze", "label_equal.unsqueeze", "label_equal.unsqueeze", "labels.unsqueeze", "labels.unsqueeze", "torch.eye", "labels.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_triplet_mask", "(", "labels", ")", ":", "\n", "        ", "\"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n        A triplet (i, j, k) is valid if:\n            - i, j, k are distinct\n            - labels[i] == labels[j] and labels[i] != labels[k]\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n        \"\"\"", "\n", "# Check that i, j and k are distinct", "\n", "indices_equal", "=", "torch", ".", "eye", "(", "labels", ".", "size", "(", "0", ")", ",", "device", "=", "labels", ".", "device", ")", ".", "bool", "(", ")", "\n", "indices_not_equal", "=", "~", "indices_equal", "\n", "i_not_equal_j", "=", "indices_not_equal", ".", "unsqueeze", "(", "2", ")", "\n", "i_not_equal_k", "=", "indices_not_equal", ".", "unsqueeze", "(", "1", ")", "\n", "j_not_equal_k", "=", "indices_not_equal", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "distinct_indices", "=", "(", "i_not_equal_j", "&", "i_not_equal_k", ")", "&", "j_not_equal_k", "\n", "\n", "label_equal", "=", "labels", ".", "unsqueeze", "(", "0", ")", "==", "labels", ".", "unsqueeze", "(", "1", ")", "\n", "i_equal_j", "=", "label_equal", ".", "unsqueeze", "(", "2", ")", "\n", "i_equal_k", "=", "label_equal", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "valid_labels", "=", "~", "i_equal_k", "&", "i_equal_j", "\n", "\n", "return", "valid_labels", "&", "distinct_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_positive_triplet_mask": [[171, 190], ["torch.eye().bool", "labels.unsqueeze", "labels.unsqueeze", "torch.eye", "labels.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_anchor_positive_triplet_mask", "(", "labels", ")", ":", "\n", "        ", "\"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n        Returns:\n            mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n        \"\"\"", "\n", "# Check that i and j are distinct", "\n", "\n", "\n", "indices_equal", "=", "torch", ".", "eye", "(", "labels", ".", "size", "(", "0", ")", ",", "device", "=", "labels", ".", "device", ")", ".", "bool", "(", ")", "\n", "indices_not_equal", "=", "~", "indices_equal", "\n", "\n", "# Check if labels[i] == labels[j]", "\n", "# Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)", "\n", "labels_equal", "=", "labels", ".", "unsqueeze", "(", "0", ")", "==", "labels", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "return", "labels_equal", "&", "indices_not_equal", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_anchor_negative_triplet_mask": [[191, 203], ["labels.unsqueeze", "labels.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_anchor_negative_triplet_mask", "(", "labels", ")", ":", "\n", "        ", "\"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n        Returns:\n            mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n        \"\"\"", "\n", "# Check if labels[i] != labels[k]", "\n", "# Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)", "\n", "\n", "return", "~", "(", "labels", ".", "unsqueeze", "(", "0", ")", "==", "labels", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.ContrastiveLoss.ContrastiveLoss.__init__": [[45, 51], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "SiameseDistanceMetric", ".", "COSINE_DISTANCE", ",", "margin", ":", "float", "=", "0.5", ",", "size_average", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "ContrastiveLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "size_average", "=", "size_average", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.ContrastiveLoss.ContrastiveLoss.forward": [[52, 59], ["ContrastiveLoss.ContrastiveLoss.distance_metric", "len", "losses.mean", "losses.sum", "ContrastiveLoss.ContrastiveLoss.model", "labels.float", "ContrastiveLoss.ContrastiveLoss.pow", "torch.relu().pow", "torch.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "model", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "assert", "len", "(", "reps", ")", "==", "2", "\n", "rep_anchor", ",", "rep_other", "=", "reps", "\n", "distances", "=", "self", ".", "distance_metric", "(", "rep_anchor", ",", "rep_other", ")", "\n", "losses", "=", "0.5", "*", "(", "labels", ".", "float", "(", ")", "*", "distances", ".", "pow", "(", "2", ")", "+", "(", "1", "-", "labels", ")", ".", "float", "(", ")", "*", "F", ".", "relu", "(", "self", ".", "margin", "-", "distances", ")", ".", "pow", "(", "2", ")", ")", "\n", "return", "losses", ".", "mean", "(", ")", "if", "self", ".", "size_average", "else", "losses", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchAllTripletLoss.BatchAllTripletLoss.__init__": [[35, 40], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "BatchHardTripletLossDistanceFunction", ".", "eucledian_distance", ",", "margin", ":", "float", "=", "5", ")", ":", "\n", "        ", "super", "(", "BatchAllTripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sentence_embedder", "=", "model", "\n", "self", ".", "triplet_margin", "=", "margin", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchAllTripletLoss.BatchAllTripletLoss.forward": [[41, 44], ["BatchAllTripletLoss.BatchAllTripletLoss.batch_all_triplet_loss", "BatchAllTripletLoss.BatchAllTripletLoss.sentence_embedder"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchAllTripletLoss.BatchAllTripletLoss.batch_all_triplet_loss"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "sentence_embedder", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "return", "self", ".", "batch_all_triplet_loss", "(", "labels", ",", "reps", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchAllTripletLoss.BatchAllTripletLoss.batch_all_triplet_loss": [[47, 90], ["BatchAllTripletLoss.BatchAllTripletLoss.distance_metric", "BatchAllTripletLoss.BatchAllTripletLoss.unsqueeze", "BatchAllTripletLoss.BatchAllTripletLoss.unsqueeze", "BatchHardTripletLoss.BatchHardTripletLoss.BatchHardTripletLoss.get_triplet_mask", "valid_triplets.size", "BatchHardTripletLoss.BatchHardTripletLoss.get_triplet_mask.sum", "BatchHardTripletLoss.BatchHardTripletLoss.get_triplet_mask.float", "triplet_loss.sum", "BatchHardTripletLoss.get_triplet_mask.sum.float"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.BatchHardTripletLoss.BatchHardTripletLoss.get_triplet_mask"], ["", "def", "batch_all_triplet_loss", "(", "self", ",", "labels", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"Build the triplet loss over a batch of embeddings.\n        We generate all the valid triplets and average the loss over the positive ones.\n        Args:\n            labels: labels of the batch, of size (batch_size,)\n            embeddings: tensor of shape (batch_size, embed_dim)\n            margin: margin for triplet loss\n            squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                     If false, output is the pairwise euclidean distance matrix.\n        Returns:\n            Label_Sentence_Triplet: scalar tensor containing the triplet loss\n        \"\"\"", "\n", "# Get the pairwise distance matrix", "\n", "pairwise_dist", "=", "self", ".", "distance_metric", "(", "embeddings", ")", "\n", "\n", "anchor_positive_dist", "=", "pairwise_dist", ".", "unsqueeze", "(", "2", ")", "\n", "anchor_negative_dist", "=", "pairwise_dist", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Compute a 3D tensor of size (batch_size, batch_size, batch_size)", "\n", "# triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k", "\n", "# Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)", "\n", "# and the 2nd (batch_size, 1, batch_size)", "\n", "triplet_loss", "=", "anchor_positive_dist", "-", "anchor_negative_dist", "+", "self", ".", "triplet_margin", "\n", "\n", "# Put to zero the invalid triplets", "\n", "# (where label(a) != label(p) or label(n) == label(a) or a == p)", "\n", "mask", "=", "BatchHardTripletLoss", ".", "get_triplet_mask", "(", "labels", ")", "\n", "triplet_loss", "=", "mask", ".", "float", "(", ")", "*", "triplet_loss", "\n", "\n", "# Remove negative losses (i.e. the easy triplets)", "\n", "triplet_loss", "[", "triplet_loss", "<", "0", "]", "=", "0", "\n", "\n", "# Count number of positive triplets (where triplet_loss > 0)", "\n", "valid_triplets", "=", "triplet_loss", "[", "triplet_loss", ">", "1e-16", "]", "\n", "num_positive_triplets", "=", "valid_triplets", ".", "size", "(", "0", ")", "\n", "num_valid_triplets", "=", "mask", ".", "sum", "(", ")", "\n", "\n", "fraction_positive_triplets", "=", "num_positive_triplets", "/", "(", "num_valid_triplets", ".", "float", "(", ")", "+", "1e-16", ")", "\n", "\n", "# Get final mean triplet loss over the positive valid triplets", "\n", "triplet_loss", "=", "triplet_loss", ".", "sum", "(", ")", "/", "(", "num_positive_triplets", "+", "1e-16", ")", "\n", "\n", "return", "triplet_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.TripletLoss.TripletLoss.__init__": [[44, 49], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ",", "distance_metric", "=", "TripletDistanceMetric", ".", "EUCLIDEAN", ",", "triplet_margin", ":", "float", "=", "5", ")", ":", "\n", "        ", "super", "(", "TripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "distance_metric", "=", "distance_metric", "\n", "self", ".", "triplet_margin", "=", "triplet_margin", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.TripletLoss.TripletLoss.forward": [[51, 60], ["TripletLoss.TripletLoss.distance_metric", "TripletLoss.TripletLoss.distance_metric", "torch.relu", "torch.relu", "torch.relu.mean", "TripletLoss.TripletLoss.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "model", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "\n", "rep_anchor", ",", "rep_pos", ",", "rep_neg", "=", "reps", "\n", "distance_pos", "=", "self", ".", "distance_metric", "(", "rep_anchor", ",", "rep_pos", ")", "\n", "distance_neg", "=", "self", ".", "distance_metric", "(", "rep_anchor", ",", "rep_neg", ")", "\n", "\n", "losses", "=", "F", ".", "relu", "(", "distance_pos", "-", "distance_neg", "+", "self", ".", "triplet_margin", ")", "\n", "return", "losses", ".", "mean", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.__init__": [[41, 44], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "SentenceTransformer", ")", ":", "\n", "        ", "super", "(", "MultipleNegativesRankingLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.forward": [[46, 50], ["MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.multiple_negatives_ranking_loss", "MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.model"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.multiple_negatives_ranking_loss"], ["", "def", "forward", "(", "self", ",", "sentence_features", ":", "Iterable", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ",", "labels", ":", "Tensor", ")", ":", "\n", "        ", "reps", "=", "[", "self", ".", "model", "(", "sentence_feature", ")", "[", "'sentence_embedding'", "]", "for", "sentence_feature", "in", "sentence_features", "]", "\n", "reps_a", ",", "reps_b", "=", "reps", "\n", "return", "self", ".", "multiple_negatives_ranking_loss", "(", "reps_a", ",", "reps_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss.multiple_negatives_ranking_loss": [[52, 65], ["torch.matmul", "torch.mean", "torch.mean", "embeddings_b.t", "torch.diag", "torch.logsumexp"], "methods", ["None"], ["", "def", "multiple_negatives_ranking_loss", "(", "self", ",", "embeddings_a", ":", "Tensor", ",", "embeddings_b", ":", "Tensor", ")", ":", "\n", "        ", "\"\"\"\n        :param embeddings_a:\n            Tensor of shape (batch_size, embedding_dim)\n        :param embeddings_b:\n            Tensor of shape (batch_size, embedding_dim)\n        :return:\n            The scalar loss\n        \"\"\"", "\n", "scores", "=", "torch", ".", "matmul", "(", "embeddings_a", ",", "embeddings_b", ".", "t", "(", ")", ")", "\n", "diagonal_mean", "=", "torch", ".", "mean", "(", "torch", ".", "diag", "(", "scores", ")", ")", "\n", "mean_log_row_sum_exp", "=", "torch", ".", "mean", "(", "torch", ".", "logsumexp", "(", "scores", ",", "dim", "=", "1", ")", ")", "\n", "return", "-", "diagonal_mean", "+", "mean_log_row_sum_exp", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.__init__": [[472, 487], ["torch.get_context", "torch.get_context", "torch.get_context.Queue"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "loader", ",", "prepro", ",", "\n", "sort_key", ",", "batchify", ",", "\n", "single_run", "=", "True", ",", "queue_size", "=", "8", ",", "fork", "=", "True", ")", ":", "\n", "        ", "self", ".", "_loader", "=", "loader", "\n", "self", ".", "_prepro", "=", "prepro", "\n", "self", ".", "_sort_key", "=", "sort_key", "\n", "self", ".", "_batchify", "=", "batchify", "\n", "self", ".", "_single_run", "=", "single_run", "\n", "if", "fork", ":", "\n", "            ", "ctx", "=", "mp", ".", "get_context", "(", "'forkserver'", ")", "\n", "self", ".", "_queue", "=", "ctx", ".", "Queue", "(", "queue_size", ")", "\n", "", "else", ":", "\n", "# for easier debugging", "\n", "            ", "self", ".", "_queue", "=", "None", "\n", "", "self", ".", "_process", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.__call__": [[488, 538], ["list", "hyper_batch.sort", "torch.get_context", "torch.get_context", "torch.get_context.Process", "batcher.BucketedGenerater._process.start", "batcher.BucketedGenerater._process.join", "range", "random.shuffle", "random.shuffle", "batcher.BucketedGenerater._batchify", "batcher.BucketedGenerater._queue.get", "isinstance", "print", "len", "print", "batcher.BucketedGenerater.__call__.get_batches"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "batch_size", ":", "int", ")", ":", "\n", "        ", "def", "get_batches", "(", "hyper_batch", ")", ":", "\n", "            ", "indexes", "=", "list", "(", "range", "(", "0", ",", "len", "(", "hyper_batch", ")", ",", "batch_size", ")", ")", "\n", "if", "not", "self", ".", "_single_run", ":", "\n", "# random shuffle for training batches", "\n", "                ", "random", ".", "shuffle", "(", "hyper_batch", ")", "\n", "random", ".", "shuffle", "(", "indexes", ")", "\n", "", "hyper_batch", ".", "sort", "(", "key", "=", "self", ".", "_sort_key", ")", "\n", "for", "i", "in", "indexes", ":", "\n", "                ", "batch", "=", "self", ".", "_batchify", "(", "hyper_batch", "[", "i", ":", "i", "+", "batch_size", "]", ")", "\n", "\"\"\"\n                except:\n                    print(\"batch_size\")\n                    print(batch_size)\n                    print(\"hyper_batch[i:i+batch_size]\")\n                    print(hyper_batch[i:i+batch_size])\n                    print(\"convert!\")\n                    batch = convert_batch_extract_ptr_backup(1, {}, \"bert\", hyper_batch[i:i+batch_size])\n                    print(\"batch after convert\")\n                    print(batch)\n                    exit()\n                \"\"\"", "\n", "yield", "batch", "\n", "\n", "", "", "if", "self", ".", "_queue", "is", "not", "None", ":", "\n", "            ", "ctx", "=", "mp", ".", "get_context", "(", "'forkserver'", ")", "\n", "self", ".", "_process", "=", "ctx", ".", "Process", "(", "\n", "target", "=", "_batch2q", ",", "\n", "args", "=", "(", "self", ".", "_loader", ",", "self", ".", "_prepro", ",", "\n", "self", ".", "_queue", ",", "self", ".", "_single_run", ")", "\n", ")", "\n", "self", ".", "_process", ".", "start", "(", ")", "\n", "while", "True", ":", "\n", "                ", "d", "=", "self", ".", "_queue", ".", "get", "(", ")", "\n", "if", "d", "is", "None", ":", "\n", "                    ", "break", "\n", "", "if", "isinstance", "(", "d", ",", "int", ")", ":", "\n", "                    ", "print", "(", "'\\nepoch {} done'", ".", "format", "(", "d", ")", ")", "\n", "continue", "\n", "", "yield", "from", "get_batches", "(", "d", ")", "\n", "", "self", ".", "_process", ".", "join", "(", ")", "\n", "", "else", ":", "\n", "            ", "i", "=", "0", "\n", "while", "True", ":", "\n", "                ", "for", "batch", "in", "self", ".", "_loader", ":", "\n", "                    ", "yield", "from", "get_batches", "(", "self", ".", "_prepro", "(", "batch", ")", ")", "\n", "", "if", "self", ".", "_single_run", ":", "\n", "                    ", "break", "\n", "", "i", "+=", "1", "\n", "print", "(", "'\\nepoch {} done'", ".", "format", "(", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.terminate": [[539, 543], ["batcher.BucketedGenerater._process.terminate", "batcher.BucketedGenerater._process.join"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.BucketedGenerater.terminate"], ["", "", "", "def", "terminate", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_process", "is", "not", "None", ":", "\n", "            ", "self", ".", "_process", ".", "terminate", "(", ")", "\n", "self", ".", "_process", ".", "join", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.coll_fn_control_gen": [[22, 36], ["toolz.sandbox.unzip", "cytoolz.concat", "cytoolz.concat", "cytoolz.concat", "zip", "sources.append", "targets.append", "compression_levels.append"], "function", ["None"], ["", "def", "coll_fn_control_gen", "(", "data", ")", ":", "\n", "    ", "source_lists", ",", "target_lists", ",", "compression_level_lists", "=", "unzip", "(", "data", ")", "\n", "source_list", "=", "concat", "(", "source_lists", ")", "\n", "target_list", "=", "concat", "(", "target_lists", ")", "\n", "compression_level_list", "=", "concat", "(", "compression_level_lists", ")", "\n", "sources", "=", "[", "]", "\n", "targets", "=", "[", "]", "\n", "compression_levels", "=", "[", "]", "\n", "for", "source_sent", ",", "target_sent", ",", "compression_level", "in", "zip", "(", "source_list", ",", "target_list", ",", "compression_level_list", ")", ":", "\n", "        ", "if", "source_sent", "and", "target_sent", ":", "\n", "            ", "sources", ".", "append", "(", "source_sent", ")", "\n", "targets", ".", "append", "(", "target_sent", ")", "\n", "compression_levels", ".", "append", "(", "compression_level", ")", "\n", "", "", "return", "sources", ",", "targets", ",", "compression_levels", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.coll_fn_cond_gen": [[38, 53], ["toolz.sandbox.unzip", "cytoolz.concat", "cytoolz.concat", "cytoolz.concat", "zip", "all", "all", "sources.append", "targets.append", "memories.append"], "function", ["None"], ["", "def", "coll_fn_cond_gen", "(", "data", ")", ":", "\n", "    ", "source_lists", ",", "target_lists", ",", "memory_lists", "=", "unzip", "(", "data", ")", "\n", "source_list", "=", "concat", "(", "source_lists", ")", "\n", "target_list", "=", "concat", "(", "target_lists", ")", "\n", "memory_list", "=", "concat", "(", "memory_lists", ")", "\n", "sources", "=", "[", "]", "\n", "targets", "=", "[", "]", "\n", "memories", "=", "[", "]", "\n", "for", "source_sent", ",", "target_sent", ",", "memory", "in", "zip", "(", "source_list", ",", "target_list", ",", "memory_list", ")", ":", "\n", "        ", "if", "source_sent", "and", "target_sent", ":", "\n", "            ", "sources", ".", "append", "(", "source_sent", ")", "\n", "targets", ".", "append", "(", "target_sent", ")", "\n", "memories", ".", "append", "(", "memory", ")", "\n", "", "", "assert", "all", "(", "sources", ")", "and", "all", "(", "targets", ")", "\n", "return", "sources", ",", "targets", ",", "memories", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.coll_fn": [[54, 62], ["toolz.sandbox.unzip", "list", "list", "filter", "filter", "all", "all", "cytoolz.concat", "cytoolz.concat"], "function", ["None"], ["", "def", "coll_fn", "(", "data", ")", ":", "\n", "    ", "source_lists", ",", "target_lists", "=", "unzip", "(", "data", ")", "\n", "# NOTE: independent filtering works because", "\n", "#       source and targets are matched properly by the Dataset", "\n", "sources", "=", "list", "(", "filter", "(", "bool", ",", "concat", "(", "source_lists", ")", ")", ")", "\n", "targets", "=", "list", "(", "filter", "(", "bool", ",", "concat", "(", "target_lists", ")", ")", ")", "\n", "assert", "all", "(", "sources", ")", "and", "all", "(", "targets", ")", "\n", "return", "sources", ",", "targets", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.coll_fn_extract": [[63, 71], ["list", "all", "filter", "map"], "function", ["None"], ["", "def", "coll_fn_extract", "(", "data", ")", ":", "\n", "    ", "def", "is_good_data", "(", "d", ")", ":", "\n", "        ", "\"\"\" make sure data is not empty\"\"\"", "\n", "source_sents", ",", "extracts", "=", "d", "\n", "return", "source_sents", "and", "extracts", "\n", "", "batch", "=", "list", "(", "filter", "(", "is_good_data", ",", "data", ")", ")", "\n", "assert", "all", "(", "map", "(", "is_good_data", ",", "batch", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize": [[72, 137], ["t.lower().split", "t.lower", "bert_tokenizer.tokenize"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "@", "curry", "\n", "def", "tokenize", "(", "max_len", ",", "emb_type", ",", "num_candidates", ",", "texts", ")", ":", "\n", "    ", "if", "emb_type", "==", "\"w2v\"", ":", "\n", "        ", "return", "[", "t", ".", "lower", "(", ")", ".", "split", "(", ")", "[", ":", "max_len", "]", "for", "t", "in", "texts", "]", "\n", "", "else", ":", "\n", "#print(\"bert tokenize\")", "\n", "        ", "return", "[", "[", "CLS_WORD", "]", "+", "bert_tokenizer", ".", "tokenize", "(", "t", ")", "[", ":", "max_len", "]", "+", "[", "SEP_WORD", "]", "for", "t", "in", "texts", "]", "\n", "\"\"\"\n        truncated_article = []\n        #left = BERT_MAX_ARTICLE_LEN - 2\n        left = [BERT_MAX_ARTICLE_LEN - 2] * num_candidates\n        full_flag = False\n        for sent_i in range(len(texts)//num_candidates):\n            sent_cand_tokens = []\n            #print(\"sent_i\")\n            #print(sent_i)\n            for cand_i in range(num_candidates):\n                #print(\"cand_i\")\n                #print(cand_i)\n                cand_str = texts[sent_i * num_candidates + cand_i]\n                cand_tokens = bert_tokenizer.tokenize(cand_str)\n                if left[cand_i] >= len(cand_tokens):\n                    sent_cand_tokens.append(cand_tokens)\n                    left[cand_i] -= len(cand_tokens)\n                else:\n                    full_flag = True\n                    break\n                #print(\"sent_cand_tokens\")\n                #print(sent_cand_tokens)\n                #print(left)\n            if full_flag:\n                break\n            else:\n                truncated_article += sent_cand_tokens\n                #print(\"truncated_article\")\n                #print(truncated_article)\n        \"\"\"", "\n", "\"\"\"\n        for sent_i, sentence in enumerate(texts):\n            tokens = bert_tokenizer.tokenize(sentence)\n            tokens = tokens[:max_len]\n            cand_i = sent_i % num_candidates\n            if left[cand_i] >= len(tokens):\n                truncated_article.append(tokens)\n                left[cand_i] -= len(tokens)\n            else:\n                break\n        #print(\"truncated article\")\n        #print(truncated_article)\n        if len(truncated_article) % num_candidates != 0:\n            print(\"left:\")\n            print(left)\n            print(\"texts:\")\n            for sent in texts:\n                print(sent)\n            raise ValueError\n        \"\"\"", "\n", "\"\"\"\n        if len(truncated_article) == 0:\n            print(\"texts\")\n            print(texts)\n            print(\"len(texts)\")\n            print(len(texts))\n            exit()\n        \"\"\"", "\n", "#return truncated_article", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id": [[139, 142], ["collections.defaultdict"], "function", ["None"], ["", "", "def", "conver2id", "(", "unk", ",", "word2id", ",", "words_list", ")", ":", "\n", "    ", "word2id", "=", "defaultdict", "(", "lambda", ":", "unk", ",", "word2id", ")", "\n", "return", "[", "[", "word2id", "[", "w", "]", "for", "w", "in", "words", "]", "for", "words", "in", "words_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn": [[143, 150], ["batcher.tokenize", "batcher.tokenize", "list", "zip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "@", "curry", "\n", "def", "prepro_fn", "(", "max_src_len", ",", "max_tgt_len", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", "=", "batch", "\n", "sources", "=", "tokenize", "(", "max_src_len", ",", "\"w2v\"", ",", "1", ",", "sources", ")", "\n", "targets", "=", "tokenize", "(", "max_tgt_len", ",", "\"w2v\"", ",", "1", ",", "targets", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "targets", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_cond": [[151, 159], ["batcher.tokenize", "batcher.tokenize", "batcher.tokenize", "list", "zip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "@", "curry", "\n", "def", "prepro_fn_cond", "(", "max_src_len", ",", "max_tgt_len", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", ",", "memories", "=", "batch", "\n", "sources", "=", "tokenize", "(", "max_src_len", ",", "\"w2v\"", ",", "1", ",", "sources", ")", "\n", "targets", "=", "tokenize", "(", "max_tgt_len", ",", "\"w2v\"", ",", "1", ",", "targets", ")", "\n", "memories", "=", "tokenize", "(", "max_src_len", "*", "4", ",", "memories", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "targets", ",", "memories", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_control": [[160, 167], ["batcher.tokenize", "batcher.tokenize", "list", "zip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.tokenize"], ["", "@", "curry", "\n", "def", "prepro_fn_control", "(", "max_src_len", ",", "max_tgt_len", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", ",", "levels", "=", "batch", "\n", "sources", "=", "tokenize", "(", "max_src_len", ",", "\"w2v\"", ",", "1", ",", "sources", ")", "\n", "targets", "=", "tokenize", "(", "max_tgt_len", ",", "\"w2v\"", ",", "1", ",", "targets", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "targets", ",", "levels", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.prepro_fn_extract": [[168, 188], ["list", "batcher.prepro_fn_extract.prepro_one"], "function", ["None"], ["", "@", "curry", "\n", "def", "prepro_fn_extract", "(", "max_src_len", ",", "max_src_num", ",", "emb_type", ",", "num_candidates", ",", "batch", ")", ":", "\n", "    ", "def", "prepro_one", "(", "sample", ")", ":", "\n", "        ", "source_sents", ",", "extracts", "=", "sample", "\n", "#if len(source_sents) % 2 != 0:", "\n", "#    print(\"source not 2\")", "\n", "tokenized_sents", "=", "tokenize", "(", "max_src_len", ",", "emb_type", ",", "num_candidates", ",", "source_sents", ")", "[", ":", "max_src_num", "]", "\n", "#if len(tokenized_sents) % 2 != 0:", "\n", "#    print(\"tokenize not 2\")", "\n", "#    print(len(source_sents))", "\n", "#    print(len(tokenized_sents))", "\n", "cleaned_extracts", "=", "list", "(", "filter", "(", "lambda", "e", ":", "e", "<", "len", "(", "tokenized_sents", ")", ",", "\n", "extracts", ")", ")", "\n", "return", "tokenized_sents", ",", "cleaned_extracts", "\n", "", "batch_prepro", "=", "[", "]", "\n", "for", "sample", "in", "batch", ":", "\n", "        ", "tokenized_sents", ",", "cleaned_extracts", "=", "prepro_one", "(", "sample", ")", "\n", "if", "len", "(", "tokenized_sents", ")", ">", "0", ":", "\n", "            ", "batch_prepro", ".", "append", "(", "(", "tokenized_sents", ",", "cleaned_extracts", ")", ")", "\n", "", "", "return", "batch_prepro", "\n", "#batch = list(map(prepro_one, batch))", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch": [[191, 198], ["toolz.sandbox.unzip", "batcher.conver2id", "batcher.conver2id", "list", "zip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch", "(", "unk", ",", "word2id", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", "=", "unzip", "(", "batch", ")", "\n", "sources", "=", "conver2id", "(", "unk", ",", "word2id", ",", "sources", ")", "\n", "targets", "=", "conver2id", "(", "unk", ",", "word2id", ",", "targets", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "targets", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy": [[199, 213], ["map", "dict", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "list", "toolz.sandbox.unzip", "zip", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch_copy", "(", "unk", ",", "word2id", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", "=", "map", "(", "list", ",", "unzip", "(", "batch", ")", ")", "\n", "ext_word2id", "=", "dict", "(", "word2id", ")", "\n", "for", "source", "in", "sources", ":", "\n", "        ", "for", "word", "in", "source", ":", "\n", "            ", "if", "word", "not", "in", "ext_word2id", ":", "\n", "                ", "ext_word2id", "[", "word", "]", "=", "len", "(", "ext_word2id", ")", "\n", "", "", "", "src_exts", "=", "conver2id", "(", "unk", ",", "ext_word2id", ",", "sources", ")", "\n", "sources", "=", "conver2id", "(", "unk", ",", "word2id", ",", "sources", ")", "\n", "tar_ins", "=", "conver2id", "(", "unk", ",", "word2id", ",", "targets", ")", "\n", "targets", "=", "conver2id", "(", "unk", ",", "ext_word2id", ",", "targets", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "src_exts", ",", "tar_ins", ",", "targets", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy_cond": [[214, 229], ["map", "dict", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "list", "toolz.sandbox.unzip", "zip", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch_copy_cond", "(", "unk", ",", "word2id", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", ",", "memories", "=", "map", "(", "list", ",", "unzip", "(", "batch", ")", ")", "\n", "ext_word2id", "=", "dict", "(", "word2id", ")", "\n", "for", "source", "in", "sources", ":", "\n", "        ", "for", "word", "in", "source", ":", "\n", "            ", "if", "word", "not", "in", "ext_word2id", ":", "\n", "                ", "ext_word2id", "[", "word", "]", "=", "len", "(", "ext_word2id", ")", "\n", "", "", "", "src_exts", "=", "conver2id", "(", "unk", ",", "ext_word2id", ",", "sources", ")", "\n", "sources", "=", "conver2id", "(", "unk", ",", "word2id", ",", "sources", ")", "\n", "tar_ins", "=", "conver2id", "(", "unk", ",", "word2id", ",", "targets", ")", "\n", "targets", "=", "conver2id", "(", "unk", ",", "ext_word2id", ",", "targets", ")", "\n", "memories", "=", "conver2id", "(", "unk", ",", "word2id", ",", "memories", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "src_exts", ",", "memories", ",", "tar_ins", ",", "targets", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_copy_control": [[230, 244], ["map", "dict", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "batcher.conver2id", "list", "toolz.sandbox.unzip", "zip", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch_copy_control", "(", "unk", ",", "word2id", ",", "batch", ")", ":", "\n", "    ", "sources", ",", "targets", ",", "levels", "=", "map", "(", "list", ",", "unzip", "(", "batch", ")", ")", "\n", "ext_word2id", "=", "dict", "(", "word2id", ")", "\n", "for", "source", "in", "sources", ":", "\n", "        ", "for", "word", "in", "source", ":", "\n", "            ", "if", "word", "not", "in", "ext_word2id", ":", "\n", "                ", "ext_word2id", "[", "word", "]", "=", "len", "(", "ext_word2id", ")", "\n", "", "", "", "src_exts", "=", "conver2id", "(", "unk", ",", "ext_word2id", ",", "sources", ")", "\n", "sources", "=", "conver2id", "(", "unk", ",", "word2id", ",", "sources", ")", "\n", "tar_ins", "=", "conver2id", "(", "unk", ",", "word2id", ",", "targets", ")", "\n", "targets", "=", "conver2id", "(", "unk", ",", "ext_word2id", ",", "targets", ")", "\n", "batch", "=", "list", "(", "zip", "(", "sources", ",", "src_exts", ",", "levels", ",", "tar_ins", ",", "targets", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_extract_ptr": [[245, 262], ["list", "map", "batcher.conver2id", "bert_tokenizer.convert_tokens_to_ids"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch_extract_ptr", "(", "unk", ",", "word2id", ",", "emb_type", ",", "batch", ")", ":", "\n", "    ", "def", "convert_one", "(", "sample", ")", ":", "\n", "        ", "source_sents", ",", "extracts", "=", "sample", "\n", "if", "emb_type", "==", "\"w2v\"", ":", "\n", "            ", "id_sents", "=", "conver2id", "(", "unk", ",", "word2id", ",", "source_sents", ")", "\n", "", "else", ":", "\n", "            ", "id_sents", "=", "[", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "sentence", ")", "for", "sentence", "in", "source_sents", "]", "\n", "#print(\"id_sents\")", "\n", "#print(id_sents)", "\n", "#if len(id_sents) % 2 != 0:", "\n", "#    print(\"convert batch not 2!\")", "\n", "#    print(len(source_sents))", "\n", "#    print(len(id_sents))", "\n", "", "return", "id_sents", ",", "extracts", "\n", "", "batch", "=", "list", "(", "map", "(", "convert_one", ",", "batch", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_extract_ptr_backup": [[263, 282], ["list", "map", "batcher.conver2id", "print", "print", "print", "print", "bert_tokenizer.convert_tokens_to_ids"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch_extract_ptr_backup", "(", "unk", ",", "word2id", ",", "emb_type", ",", "batch", ")", ":", "\n", "    ", "def", "convert_one", "(", "sample", ")", ":", "\n", "        ", "source_sents", ",", "extracts", "=", "sample", "\n", "if", "emb_type", "==", "\"w2v\"", ":", "\n", "            ", "id_sents", "=", "conver2id", "(", "unk", ",", "word2id", ",", "source_sents", ")", "\n", "", "else", ":", "\n", "            ", "id_sents", "=", "[", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "sentence", ")", "for", "sentence", "in", "source_sents", "]", "\n", "print", "(", "\"source_sents\"", ")", "\n", "print", "(", "source_sents", ")", "\n", "print", "(", "\"id_sents\"", ")", "\n", "print", "(", "id_sents", ")", "\n", "#if len(id_sents) % 2 != 0:", "\n", "#    print(\"convert batch not 2!\")", "\n", "#    print(len(source_sents))", "\n", "#    print(len(id_sents))", "\n", "", "return", "id_sents", ",", "extracts", "\n", "", "batch", "=", "list", "(", "map", "(", "convert_one", ",", "batch", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.convert_batch_extract_ff": [[283, 294], ["list", "batcher.conver2id", "map", "len"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.conver2id"], ["", "@", "curry", "\n", "def", "convert_batch_extract_ff", "(", "unk", ",", "word2id", ",", "batch", ")", ":", "\n", "    ", "def", "convert_one", "(", "sample", ")", ":", "\n", "        ", "source_sents", ",", "extracts", "=", "sample", "\n", "id_sents", "=", "conver2id", "(", "unk", ",", "word2id", ",", "source_sents", ")", "\n", "binary_extracts", "=", "[", "0", "]", "*", "len", "(", "source_sents", ")", "\n", "for", "ext", "in", "extracts", ":", "\n", "            ", "binary_extracts", "[", "ext", "]", "=", "1", "\n", "", "return", "id_sents", ",", "binary_extracts", "\n", "", "batch", "=", "list", "(", "map", "(", "convert_one", ",", "batch", ")", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize": [[296, 333], ["len", "max", "tensor_type.fill_", "enumerate", "tensor_type", "tensor_type", "len", "all", "print", "print", "print", "print", "exit", "len", "len"], "function", ["None"], ["", "@", "curry", "\n", "def", "pad_batch_tensorize", "(", "inputs", ",", "pad", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "\"\"\"pad_batch_tensorize\n\n    :param inputs: List of size B containing torch tensors of shape [T, ...]\n    :type inputs: List[np.ndarray]\n    :rtype: TorchTensor of size (B, T, ...)\n    \"\"\"", "\n", "tensor_type", "=", "torch", ".", "cuda", ".", "LongTensor", "if", "cuda", "else", "torch", ".", "LongTensor", "\n", "batch_size", "=", "len", "(", "inputs", ")", "\n", "max_len", "=", "max", "(", "len", "(", "ids", ")", "for", "ids", "in", "inputs", ")", "\n", "\"\"\"\n    except:\n        print(\"inputs\")\n        print(inputs)\n        print(\"pad\")\n        print(pad)\n        print(\"cuda\")\n        print(cuda)\n        exit()\n    \"\"\"", "\n", "tensor_shape", "=", "(", "batch_size", ",", "max_len", ")", "\n", "try", ":", "\n", "        ", "tensor", "=", "tensor_type", "(", "*", "tensor_shape", ")", "\n", "", "except", ":", "\n", "        ", "if", "all", "(", "len", "(", "inpt", ")", "==", "0", "for", "inpt", "in", "inputs", ")", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"batch_size: {}\"", ".", "format", "(", "batch_size", ")", ")", "\n", "print", "(", "\"max_len:{}\"", ".", "format", "(", "max_len", ")", ")", "\n", "print", "(", "\"inputs: \"", ")", "\n", "print", "(", "inputs", ")", "\n", "exit", "(", ")", "\n", "", "", "tensor", ".", "fill_", "(", "pad", ")", "\n", "for", "i", ",", "ids", "in", "enumerate", "(", "inputs", ")", ":", "\n", "        ", "tensor", "[", "i", ",", ":", "len", "(", "ids", ")", "]", "=", "tensor_type", "(", "ids", ")", "\n", "", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn": [[334, 349], ["tuple", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "map", "len", "toolz.sandbox.unzip"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "@", "curry", "\n", "def", "batchify_fn", "(", "pad", ",", "start", ",", "end", ",", "data", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "sources", ",", "targets", "=", "tuple", "(", "map", "(", "list", ",", "unzip", "(", "data", ")", ")", ")", "\n", "\n", "src_lens", "=", "[", "len", "(", "src", ")", "for", "src", "in", "sources", "]", "\n", "tar_ins", "=", "[", "[", "start", "]", "+", "tgt", "for", "tgt", "in", "targets", "]", "\n", "targets", "=", "[", "tgt", "+", "[", "end", "]", "for", "tgt", "in", "targets", "]", "\n", "\n", "source", "=", "pad_batch_tensorize", "(", "sources", ",", "pad", ",", "cuda", ")", "\n", "tar_in", "=", "pad_batch_tensorize", "(", "tar_ins", ",", "pad", ",", "cuda", ")", "\n", "target", "=", "pad_batch_tensorize", "(", "targets", ",", "pad", ",", "cuda", ")", "\n", "\n", "fw_args", "=", "(", "source", ",", "src_lens", ",", "tar_in", ")", "\n", "loss_args", "=", "(", "target", ",", ")", "\n", "return", "fw_args", ",", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy": [[351, 371], ["tuple", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "map", "len", "pad_batch_tensorize.max().item", "toolz.sandbox.unzip", "pad_batch_tensorize.max"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "@", "curry", "\n", "def", "batchify_fn_copy", "(", "pad", ",", "start", ",", "end", ",", "data", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "sources", ",", "ext_srcs", ",", "tar_ins", ",", "targets", "=", "tuple", "(", "map", "(", "list", ",", "unzip", "(", "data", ")", ")", ")", "\n", "\n", "src_lens", "=", "[", "len", "(", "src", ")", "for", "src", "in", "sources", "]", "\n", "sources", "=", "[", "src", "for", "src", "in", "sources", "]", "\n", "ext_srcs", "=", "[", "ext", "for", "ext", "in", "ext_srcs", "]", "\n", "\n", "tar_ins", "=", "[", "[", "start", "]", "+", "tgt", "for", "tgt", "in", "tar_ins", "]", "\n", "targets", "=", "[", "tgt", "+", "[", "end", "]", "for", "tgt", "in", "targets", "]", "\n", "\n", "source", "=", "pad_batch_tensorize", "(", "sources", ",", "pad", ",", "cuda", ")", "\n", "tar_in", "=", "pad_batch_tensorize", "(", "tar_ins", ",", "pad", ",", "cuda", ")", "\n", "target", "=", "pad_batch_tensorize", "(", "targets", ",", "pad", ",", "cuda", ")", "\n", "ext_src", "=", "pad_batch_tensorize", "(", "ext_srcs", ",", "pad", ",", "cuda", ")", "\n", "\n", "ext_vsize", "=", "ext_src", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "fw_args", "=", "(", "source", ",", "src_lens", ",", "tar_in", ",", "ext_src", ",", "ext_vsize", ")", "\n", "loss_args", "=", "(", "target", ",", ")", "\n", "return", "fw_args", ",", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy_cond": [[373, 398], ["tuple", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "map", "len", "len", "pad_batch_tensorize.max().item", "toolz.sandbox.unzip", "pad_batch_tensorize.max"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "@", "curry", "\n", "def", "batchify_fn_copy_cond", "(", "pad", ",", "start", ",", "end", ",", "data", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "sources", ",", "ext_srcs", ",", "memories", ",", "tar_ins", ",", "targets", "=", "tuple", "(", "map", "(", "list", ",", "unzip", "(", "data", ")", ")", ")", "\n", "\n", "src_lens", "=", "[", "len", "(", "src", ")", "for", "src", "in", "sources", "]", "\n", "sources", "=", "[", "src", "for", "src", "in", "sources", "]", "\n", "ext_srcs", "=", "[", "ext", "for", "ext", "in", "ext_srcs", "]", "\n", "\n", "tar_ins", "=", "[", "[", "start", "]", "+", "tgt", "for", "tgt", "in", "tar_ins", "]", "\n", "targets", "=", "[", "tgt", "+", "[", "end", "]", "for", "tgt", "in", "targets", "]", "\n", "\n", "memories", "=", "[", "mem", "for", "mem", "in", "memories", "]", "\n", "\n", "source", "=", "pad_batch_tensorize", "(", "sources", ",", "pad", ",", "cuda", ")", "\n", "tar_in", "=", "pad_batch_tensorize", "(", "tar_ins", ",", "pad", ",", "cuda", ")", "\n", "target", "=", "pad_batch_tensorize", "(", "targets", ",", "pad", ",", "cuda", ")", "\n", "ext_src", "=", "pad_batch_tensorize", "(", "ext_srcs", ",", "pad", ",", "cuda", ")", "\n", "\n", "memory_lens", "=", "[", "len", "(", "mem", ")", "for", "mem", "in", "memories", "]", "\n", "memory", "=", "pad_batch_tensorize", "(", "memories", ",", "pad", ",", "cuda", ")", "\n", "\n", "ext_vsize", "=", "ext_src", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "fw_args", "=", "(", "source", ",", "src_lens", ",", "memory", ",", "memory_lens", ",", "tar_in", ",", "ext_src", ",", "ext_vsize", ")", "\n", "loss_args", "=", "(", "target", ",", ")", "\n", "return", "fw_args", ",", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_copy_control": [[400, 424], ["tuple", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "torch.LongTensor().to", "torch.LongTensor().to", "map", "len", "pad_batch_tensorize.max().item", "toolz.sandbox.unzip", "torch.LongTensor", "torch.LongTensor", "pad_batch_tensorize.max"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "@", "curry", "\n", "def", "batchify_fn_copy_control", "(", "pad", ",", "start", ",", "end", ",", "data", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "sources", ",", "ext_srcs", ",", "levels", ",", "tar_ins", ",", "targets", "=", "tuple", "(", "map", "(", "list", ",", "unzip", "(", "data", ")", ")", ")", "\n", "\n", "src_lens", "=", "[", "len", "(", "src", ")", "for", "src", "in", "sources", "]", "\n", "sources", "=", "[", "src", "for", "src", "in", "sources", "]", "\n", "ext_srcs", "=", "[", "ext", "for", "ext", "in", "ext_srcs", "]", "\n", "\n", "tar_ins", "=", "[", "[", "start", "]", "+", "tgt", "for", "tgt", "in", "tar_ins", "]", "\n", "targets", "=", "[", "tgt", "+", "[", "end", "]", "for", "tgt", "in", "targets", "]", "\n", "\n", "levels", "=", "[", "lev", "for", "lev", "in", "levels", "]", "\n", "\n", "source", "=", "pad_batch_tensorize", "(", "sources", ",", "pad", ",", "cuda", ")", "\n", "tar_in", "=", "pad_batch_tensorize", "(", "tar_ins", ",", "pad", ",", "cuda", ")", "\n", "target", "=", "pad_batch_tensorize", "(", "targets", ",", "pad", ",", "cuda", ")", "\n", "ext_src", "=", "pad_batch_tensorize", "(", "ext_srcs", ",", "pad", ",", "cuda", ")", "\n", "\n", "level", "=", "torch", ".", "LongTensor", "(", "levels", ")", ".", "to", "(", "source", ")", "\n", "\n", "ext_vsize", "=", "ext_src", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "fw_args", "=", "(", "source", ",", "src_lens", ",", "level", ",", "tar_in", ",", "ext_src", ",", "ext_vsize", ")", "\n", "loss_args", "=", "(", "target", ",", ")", "\n", "return", "fw_args", ",", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_extract_ptr": [[426, 444], ["tuple", "list", "list", "batcher.pad_batch_tensorize", "batcher.pad_batch_tensorize", "map", "map", "map", "list", "toolz.sandbox.unzip", "batcher.pad_batch_tensorize", "map"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize", "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "@", "curry", "\n", "def", "batchify_fn_extract_ptr", "(", "pad", ",", "data", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "source_lists", ",", "targets", "=", "tuple", "(", "map", "(", "list", ",", "unzip", "(", "data", ")", ")", ")", "\n", "\n", "src_nums", "=", "list", "(", "map", "(", "len", ",", "source_lists", ")", ")", "\n", "sources", "=", "list", "(", "map", "(", "pad_batch_tensorize", "(", "pad", "=", "pad", ",", "cuda", "=", "cuda", ")", ",", "source_lists", ")", ")", "\n", "\n", "# PAD is -1 (dummy extraction index) for using sequence loss", "\n", "target", "=", "pad_batch_tensorize", "(", "targets", ",", "pad", "=", "-", "1", ",", "cuda", "=", "cuda", ")", "\n", "remove_last", "=", "lambda", "tgt", ":", "tgt", "[", ":", "-", "1", "]", "\n", "tar_in", "=", "pad_batch_tensorize", "(", "\n", "list", "(", "map", "(", "remove_last", ",", "targets", ")", ")", ",", "\n", "pad", "=", "-", "0", ",", "cuda", "=", "cuda", "# use 0 here for feeding first conv sentence repr.", "\n", ")", "\n", "\n", "fw_args", "=", "(", "sources", ",", "src_nums", ",", "tar_in", ")", "\n", "loss_args", "=", "(", "target", ",", ")", "\n", "return", "fw_args", ",", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.batchify_fn_extract_ff": [[445, 458], ["tuple", "list", "list", "tensor_type", "map", "map", "map", "list", "toolz.sandbox.unzip", "batcher.pad_batch_tensorize", "cytoolz.concat"], "function", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher.pad_batch_tensorize"], ["", "@", "curry", "\n", "def", "batchify_fn_extract_ff", "(", "pad", ",", "data", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "source_lists", ",", "targets", "=", "tuple", "(", "map", "(", "list", ",", "unzip", "(", "data", ")", ")", ")", "\n", "\n", "src_nums", "=", "list", "(", "map", "(", "len", ",", "source_lists", ")", ")", "\n", "sources", "=", "list", "(", "map", "(", "pad_batch_tensorize", "(", "pad", "=", "pad", ",", "cuda", "=", "cuda", ")", ",", "source_lists", ")", ")", "\n", "\n", "tensor_type", "=", "torch", ".", "cuda", ".", "FloatTensor", "if", "cuda", "else", "torch", ".", "FloatTensor", "\n", "target", "=", "tensor_type", "(", "list", "(", "concat", "(", "targets", ")", ")", ")", "\n", "\n", "fw_args", "=", "(", "sources", ",", "src_nums", ")", "\n", "loss_args", "=", "(", "target", ",", ")", "\n", "return", "fw_args", ",", "loss_args", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.batcher._batch2q": [[460, 470], ["q.put", "q.put", "q.put", "prepro"], "function", ["None"], ["", "def", "_batch2q", "(", "loader", ",", "prepro", ",", "q", ",", "single_run", "=", "True", ")", ":", "\n", "    ", "epoch", "=", "0", "\n", "while", "True", ":", "\n", "        ", "for", "batch", "in", "loader", ":", "\n", "            ", "q", ".", "put", "(", "prepro", "(", "batch", ")", ")", "\n", "", "if", "single_run", ":", "\n", "            ", "break", "\n", "", "epoch", "+=", "1", "\n", "q", ".", "put", "(", "epoch", ")", "\n", "", "q", ".", "put", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDataset.__init__": [[11, 15], ["os.path.join", "data._count_data"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data"], ["    ", "def", "__init__", "(", "self", ",", "split", ":", "str", ",", "path", ":", "str", ")", "->", "None", ":", "\n", "#assert split in ['train', 'val', 'test']", "\n", "        ", "self", ".", "_data_path", "=", "join", "(", "path", ",", "split", ")", "\n", "self", ".", "_n_data", "=", "_count_data", "(", "self", ".", "_data_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDataset.__len__": [[16, 18], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDataset.__getitem__": [[19, 23], ["open", "json.loads", "os.path.join", "f.read"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ":", "int", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "_data_path", ",", "'{}.json'", ".", "format", "(", "i", ")", ")", ")", "as", "f", ":", "\n", "            ", "js", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "return", "js", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__init__": [[26, 31], ["os.path.join", "data._count_data"], "methods", ["home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data"], ["    ", "def", "__init__", "(", "self", ",", "split", ":", "str", ",", "path", ":", "str", ",", "start_idx", ":", "int", ")", "->", "None", ":", "\n", "#assert split in ['train', 'val', 'test']", "\n", "        ", "self", ".", "_data_path", "=", "join", "(", "path", ",", "split", ")", "\n", "self", ".", "_n_data", "=", "_count_data", "(", "self", ".", "_data_path", ")", "-", "start_idx", "\n", "self", ".", "start_idx", "=", "start_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__len__": [[32, 34], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_n_data", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data.CnnDmDatasetFromIdx.__getitem__": [[35, 39], ["open", "json.loads", "os.path.join", "f.read"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ":", "int", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "_data_path", ",", "'{}.json'", ".", "format", "(", "i", "+", "self", ".", "start_idx", ")", ")", ")", "as", "f", ":", "\n", "            ", "js", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "", "return", "js", "\n", "\n"]], "home.repos.pwc.inspect_result.kenchan0226_abs-then-ext-public.data.data._count_data": [[41, 48], ["re.compile", "os.listdir", "len", "bool", "list", "re.compile.match", "filter"], "function", ["None"], ["", "", "def", "_count_data", "(", "path", ")", ":", "\n", "    ", "\"\"\" count number of data in the given path\"\"\"", "\n", "matcher", "=", "re", ".", "compile", "(", "r'[0-9]+\\.json'", ")", "\n", "match", "=", "lambda", "name", ":", "bool", "(", "matcher", ".", "match", "(", "name", ")", ")", "\n", "names", "=", "os", ".", "listdir", "(", "path", ")", "\n", "n_data", "=", "len", "(", "list", "(", "filter", "(", "match", ",", "names", ")", ")", ")", "\n", "return", "n_data", "\n", "", ""]]}