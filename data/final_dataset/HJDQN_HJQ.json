{"home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.NoiseProcess.__init__": [[5, 8], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "self", ".", "dim", "=", "dim", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.NoiseProcess.sample": [[9, 11], ["None"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.NoiseProcess.reset": [[12, 14], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.IndependentGaussian.__init__": [[17, 21], ["noise.NoiseProcess.__init__"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "sigma", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "IndependentGaussian", ",", "self", ")", ".", "__init__", "(", "dim", "=", "dim", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.IndependentGaussian.sample": [[22, 24], ["numpy.random.randn"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sigma", "*", "np", ".", "random", ".", "randn", "(", "self", ".", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.IndependentGaussian.reset": [[25, 27], ["numpy.random.randn"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sigma", "*", "np", ".", "random", ".", "randn", "(", "self", ".", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.Zero.__init__": [[30, 33], ["noise.NoiseProcess.__init__"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "Zero", ",", "self", ")", ".", "__init__", "(", "dim", "=", "dim", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.Zero.sample": [[34, 36], ["numpy.zeros"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "zeros", "(", "self", ".", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.Zero.reset": [[37, 39], ["numpy.zeros"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "zeros", "(", "self", ".", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.__init__": [[47, 54], ["noise.NoiseProcess.__init__"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "sigma", ",", "dt", ",", "mu", "=", "-", "0.15", ")", ":", "\n", "        ", "super", "(", "SDE", ",", "self", ")", ".", "__init__", "(", "dim", "=", "dim", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "dt", "=", "dt", "\n", "self", ".", "mu", "=", "mu", "\n", "\n", "self", ".", "x", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample": [[55, 61], ["numpy.random.randn", "numpy.copy"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "dt", "=", "self", ".", "dt", "\n", "dBt", "=", "np", ".", "random", ".", "randn", "(", "self", ".", "dim", ")", "\n", "dx", "=", "self", ".", "mu", "*", "self", ".", "x", "*", "dt", "+", "(", "self", ".", "sigma", "*", "(", "dt", "**", ".5", ")", ")", "*", "dBt", "\n", "self", ".", "x", "+=", "dx", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.reset": [[62, 65], ["numpy.zeros", "numpy.copy"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "x", "=", "np", ".", "zeros", "(", "self", ".", "dim", ")", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.ReplayBuffer.__init__": [[9, 18], ["buffer.CircularBuffer", "buffer.CircularBuffer", "buffer.CircularBuffer", "buffer.CircularBuffer", "buffer.CircularBuffer"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "limit", ")", ":", "\n", "        ", "self", ".", "states", "=", "CircularBuffer", "(", "shape", "=", "(", "state_dim", ",", ")", ",", "limit", "=", "limit", ")", "\n", "self", ".", "actions", "=", "CircularBuffer", "(", "shape", "=", "(", "action_dim", ",", ")", ",", "limit", "=", "limit", ")", "\n", "self", ".", "rewards", "=", "CircularBuffer", "(", "shape", "=", "(", "1", ",", ")", ",", "limit", "=", "limit", ")", "\n", "self", ".", "next_states", "=", "CircularBuffer", "(", "shape", "=", "(", "state_dim", ",", ")", ",", "limit", "=", "limit", ")", "\n", "self", ".", "terminals", "=", "CircularBuffer", "(", "shape", "=", "(", "1", ",", ")", ",", "limit", "=", "limit", ")", "\n", "\n", "self", ".", "limit", "=", "limit", "# maximum capacity of the buffer", "\n", "self", ".", "size", "=", "0", "# current size of the buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.ReplayBuffer.append": [[19, 28], ["buffer.ReplayBuffer.states.append", "buffer.ReplayBuffer.actions.append", "buffer.ReplayBuffer.rewards.append", "buffer.ReplayBuffer.next_states.append", "buffer.ReplayBuffer.terminals.append"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append"], ["", "def", "append", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ":", "\n", "# store a given transition sample to the buffer", "\n", "        ", "self", ".", "states", ".", "append", "(", "state", ")", "\n", "self", ".", "actions", ".", "append", "(", "action", ")", "\n", "self", ".", "rewards", ".", "append", "(", "reward", ")", "\n", "self", ".", "next_states", ".", "append", "(", "next_state", ")", "\n", "self", ".", "terminals", ".", "append", "(", "done", ")", "\n", "\n", "self", ".", "size", "=", "self", ".", "states", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.ReplayBuffer.sample_batch": [[29, 48], ["numpy.random.default_rng", "numpy.random.default_rng.choice", "buffer.ReplayBuffer.states.get_batch", "buffer.ReplayBuffer.actions.get_batch", "buffer.ReplayBuffer.rewards.get_batch", "buffer.ReplayBuffer.next_states.get_batch", "buffer.ReplayBuffer.terminals.get_batch", "Batch"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.get_batch", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.get_batch", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.get_batch", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.get_batch", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.get_batch"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "# uniformly sample transition data from the buffer", "\n", "        ", "rng", "=", "np", ".", "random", ".", "default_rng", "(", ")", "\n", "idxs", "=", "rng", ".", "choice", "(", "self", ".", "size", ",", "batch_size", ")", "# sample indices", "\n", "\n", "# get batch from each buffer", "\n", "state_batch", "=", "self", ".", "states", ".", "get_batch", "(", "idxs", ")", "\n", "action_batch", "=", "self", ".", "actions", ".", "get_batch", "(", "idxs", ")", "\n", "reward_batch", "=", "self", ".", "rewards", ".", "get_batch", "(", "idxs", ")", "\n", "next_state_batch", "=", "self", ".", "next_states", ".", "get_batch", "(", "idxs", ")", "\n", "terminal_batch", "=", "self", ".", "terminals", ".", "get_batch", "(", "idxs", ")", "\n", "\n", "batch", "=", "Batch", "(", "state", "=", "state_batch", ",", "\n", "act", "=", "action_batch", ",", "\n", "next_state", "=", "next_state_batch", ",", "\n", "rew", "=", "reward_batch", ",", "\n", "done", "=", "terminal_batch", ")", "\n", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.__init__": [[59, 65], ["numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "shape", ",", "limit", "=", "1000000", ")", ":", "\n", "        ", "self", ".", "start", "=", "0", "\n", "self", ".", "data_shape", "=", "shape", "\n", "self", ".", "size", "=", "0", "\n", "self", ".", "limit", "=", "limit", "\n", "self", ".", "data", "=", "np", ".", "zeros", "(", "(", "self", ".", "limit", ",", ")", "+", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append": [[66, 73], ["None"], "methods", ["None"], ["", "def", "append", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "self", ".", "size", "<", "self", ".", "limit", ":", "\n", "            ", "self", ".", "size", "+=", "1", "\n", "", "else", ":", "\n", "            ", "self", ".", "start", "=", "(", "self", ".", "start", "+", "1", ")", "%", "self", ".", "limit", "\n", "\n", "", "self", ".", "data", "[", "(", "self", ".", "start", "+", "self", ".", "size", "-", "1", ")", "%", "self", ".", "limit", "]", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.get_batch": [[74, 77], ["None"], "methods", ["None"], ["", "def", "get_batch", "(", "self", ",", "idxs", ")", ":", "\n", "\n", "        ", "return", "self", ".", "data", "[", "(", "self", ".", "start", "+", "idxs", ")", "%", "self", ".", "limit", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.model.Actor.__init__": [[12, 19], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.__init__"], ["def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "hidden_size1", ",", "hidden_size2", ",", "ctrl_range", ")", ":", "\n", "        ", "super", "(", "Actor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "state_dim", ",", "hidden_size1", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_size1", ",", "hidden_size2", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "hidden_size2", ",", "action_dim", ")", "\n", "\n", "self", ".", "ctrl_range", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "ctrl_range", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.model.Actor.forward": [[20, 28], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "model.Actor.fc1", "model.Actor.fc2", "model.Actor.fc3"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "state", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc2", "(", "x", ")", ")", "\n", "x", "=", "torch", ".", "tanh", "(", "self", ".", "fc3", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "ctrl_range", "*", "x", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.model.Critic.__init__": [[35, 40], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.__init__"], ["def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "hidden_size1", ",", "hidden_size2", ")", ":", "\n", "        ", "super", "(", "Critic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "state_dim", "+", "action_dim", ",", "hidden_size1", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_size1", ",", "hidden_size2", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "hidden_size2", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.model.Critic.forward": [[41, 48], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "model.Critic.fc3", "model.Critic.fc1", "model.Critic.fc2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "fc3", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.freeze": [[6, 9], ["net.parameters", "p.requires_grad_"], "function", ["None"], ["def", "freeze", "(", "net", ")", ":", "\n", "    ", "for", "p", "in", "net", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad_", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.unfreeze": [[11, 14], ["net.parameters", "p.requires_grad_"], "function", ["None"], ["", "", "def", "unfreeze", "(", "net", ")", ":", "\n", "    ", "for", "p", "in", "net", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad_", "(", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.get_env_spec": [[16, 30], ["print", "print", "print", "print", "print", "print"], "function", ["None"], ["", "", "def", "get_env_spec", "(", "env", ")", ":", "\n", "# load environment information from created environment", "\n", "    ", "print", "(", "'environment : '", "+", "env", ".", "unwrapped", ".", "spec", ".", "id", ")", "\n", "dimS", "=", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "dimA", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "ctrl_range", "=", "env", ".", "action_space", ".", "high", "\n", "print", "(", "'-'", "*", "80", ")", "\n", "print", "(", "'observation dim : {} / action dim : {}'", ".", "format", "(", "dimS", ",", "dimA", ")", ")", "\n", "print", "(", "'dt : {}'", ".", "format", "(", "env", ".", "dt", ")", ")", "\n", "print", "(", "'control range : {}'", ".", "format", "(", "ctrl_range", ")", ")", "\n", "\n", "print", "(", "'-'", "*", "80", ")", "\n", "\n", "return", "dimS", ",", "dimA", ",", "env", ".", "dt", ",", "ctrl_range", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.set_log_dir": [[32, 51], ["os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir"], "function", ["None"], ["", "def", "set_log_dir", "(", "env_id", ")", ":", "\n", "# set up directories to save logs", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "'./train_log/'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'./train_log/'", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "'./eval_log/'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'./eval_log/'", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "'./train_log/'", "+", "env_id", "+", "'/'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'./train_log/'", "+", "env_id", "+", "'/'", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "'./eval_log/'", "+", "env_id", "+", "'/'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'./eval_log/'", "+", "env_id", "+", "'/'", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "'./checkpoints/'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'./checkpoints/'", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "'./checkpoints/'", "+", "env_id", "+", "'/'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'./checkpoints/'", "+", "env_id", "+", "'/'", ")", "\n", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.scaled_env": [[53, 80], ["gym.make", "isinstance", "int", "int", "int", "ValueError"], "function", ["None"], ["", "def", "scaled_env", "(", "env_id", ",", "scale_factor", ")", ":", "\n", "    ", "\"\"\"\n    adjust environment parameters related to time discretization\n    \"\"\"", "\n", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "\n", "if", "isinstance", "(", "env", ".", "unwrapped", ",", "MujocoEnv", ")", ":", "\n", "########################################################################################", "\n", "# time discretization in MuJoCo                                                        #", "\n", "# dt = env.model.opt.timestep * env.frame_skip                                         #", "\n", "# if scale factor = k, then dt <- k * dt, which is achieved by  frame_skip by k        #", "\n", "# physical time horizon T = episode steps * dt                                         #", "\n", "# episode length is also rescaled so that T remains the same                           #", "\n", "########################################################################################", "\n", "        ", "if", "(", "env", ".", "frame_skip", "*", "scale_factor", ")", "!=", "int", "(", "env", ".", "frame_skip", "*", "scale_factor", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'invalid scale factor -> frame skip = {}, scale_factor =  {}'", ".", "format", "(", "env", ".", "frame_skip", ",", "\n", "scale_factor", ")", ")", "\n", "\n", "", "env", ".", "unwrapped", ".", "frame_skip", "=", "int", "(", "env", ".", "frame_skip", "*", "scale_factor", ")", "\n", "\n", "", "else", ":", "\n", "        ", "env", ".", "dt", "*=", "scale_factor", "\n", "\n", "", "env", ".", "_max_episode_steps", "=", "int", "(", "env", ".", "_max_episode_steps", "/", "scale_factor", ")", "\n", "\n", "return", "env", "\n", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.__init__": [[12, 65], ["int", "algorithms.model.Actor().to", "algorithms.model.Critic().to", "copy.deepcopy().to", "copy.deepcopy().to", "algorithms.utils.freeze", "algorithms.utils.freeze", "algorithms.buffer.ReplayBuffer", "torch.optim.Adam", "torch.optim.Adam", "ddpg_agent.DDPGAgent.Q.parameters", "ddpg_agent.DDPGAgent.pi.parameters", "algorithms.model.Actor", "algorithms.model.Critic", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.freeze", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.freeze"], ["    ", "def", "__init__", "(", "self", ",", "\n", "dimS", ",", "\n", "dimA", ",", "\n", "ctrl_range", ",", "\n", "gamma", "=", "0.99", ",", "\n", "actor_lr", "=", "1e-4", ",", "\n", "critic_lr", "=", "1e-3", ",", "\n", "polyak", "=", "1e-3", ",", "\n", "sigma", "=", "0.1", ",", "\n", "hidden_size1", "=", "400", ",", "\n", "hidden_size2", "=", "300", ",", "\n", "buffer_size", "=", "int", "(", "1e6", ")", ",", "\n", "batch_size", "=", "128", ",", "\n", "h_scale", "=", "1.0", ",", "\n", "device", "=", "'cpu'", ",", "\n", "render", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param dimS: dimension of observation space\n        :param dimA: dimension of action space\n        :param ctrl_range: range of valid action range\n        description of the rest of the params are given in ddpg.py\n        \"\"\"", "\n", "self", ".", "dimS", "=", "dimS", "\n", "self", ".", "dimA", "=", "dimA", "\n", "self", ".", "ctrl_range", "=", "ctrl_range", "\n", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "pi_lr", "=", "actor_lr", "\n", "self", ".", "q_lr", "=", "critic_lr", "\n", "self", ".", "polyak", "=", "polyak", "\n", "self", ".", "sigma", "=", "sigma", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "# networks definition", "\n", "# pi : actor network, Q : critic network", "\n", "self", ".", "pi", "=", "Actor", "(", "dimS", ",", "dimA", ",", "hidden_size1", ",", "hidden_size2", ",", "ctrl_range", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "Q", "=", "Critic", "(", "dimS", ",", "dimA", ",", "hidden_size1", ",", "hidden_size2", ")", ".", "to", "(", "device", ")", "\n", "\n", "# target networks", "\n", "self", ".", "target_pi", "=", "copy", ".", "deepcopy", "(", "self", ".", "pi", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "target_Q", "=", "copy", ".", "deepcopy", "(", "self", ".", "Q", ")", ".", "to", "(", "device", ")", "\n", "\n", "freeze", "(", "self", ".", "target_pi", ")", "\n", "freeze", "(", "self", ".", "target_Q", ")", "\n", "\n", "self", ".", "buffer", "=", "ReplayBuffer", "(", "dimS", ",", "dimA", ",", "limit", "=", "buffer_size", ")", "\n", "\n", "self", ".", "Q_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "Q", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "q_lr", ")", "\n", "self", ".", "pi_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "pi", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "pi_lr", ")", "\n", "\n", "self", ".", "h_scale", "=", "h_scale", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "render", "=", "render", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.target_update": [[66, 74], ["zip", "zip", "ddpg_agent.DDPGAgent.pi.parameters", "ddpg_agent.DDPGAgent.target_pi.parameters", "target_p.data.copy_", "ddpg_agent.DDPGAgent.Q.parameters", "ddpg_agent.DDPGAgent.target_Q.parameters", "target_params.data.copy_"], "methods", ["None"], ["", "def", "target_update", "(", "self", ")", ":", "\n", "# soft-update for both actors and critics", "\n", "# \\theta^\\prime = \\tau * \\theta + (1 - \\tau) * \\theta^\\prime", "\n", "        ", "for", "p", ",", "target_p", "in", "zip", "(", "self", ".", "pi", ".", "parameters", "(", ")", ",", "self", ".", "target_pi", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_p", ".", "data", ".", "copy_", "(", "self", ".", "polyak", "*", "p", ".", "data", "+", "(", "1.0", "-", "self", ".", "polyak", ")", "*", "target_p", ".", "data", ")", "\n", "\n", "", "for", "params", ",", "target_params", "in", "zip", "(", "self", ".", "Q", ".", "parameters", "(", ")", ",", "self", ".", "target_Q", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_params", ".", "data", ".", "copy_", "(", "self", ".", "polyak", "*", "params", ".", "data", "+", "(", "1.0", "-", "self", ".", "polyak", ")", "*", "target_params", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.get_action": [[75, 91], ["torch.tensor().to", "torch.no_grad", "ddpg_agent.DDPGAgent.pi", "numpy.squeeze.cpu().detach().numpy", "numpy.squeeze", "numpy.clip", "torch.tensor", "numpy.random.randn", "numpy.squeeze.cpu().detach", "numpy.squeeze.cpu"], "methods", ["None"], ["", "", "def", "get_action", "(", "self", ",", "state_tensor", ",", "eval", "=", "False", ")", ":", "\n", "\n", "        ", "state_arr", "=", "state_tensor", "[", "np", ".", "newaxis", "]", "\n", "state_tensor", "=", "torch", ".", "tensor", "(", "state_arr", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "action", "=", "self", ".", "pi", "(", "state_tensor", ")", "\n", "action", "=", "action", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "action", "=", "np", ".", "squeeze", "(", "action", ",", "axis", "=", "0", ")", "\n", "\n", "", "if", "not", "eval", ":", "\n", "            ", "noise", "=", "self", ".", "sigma", "*", "np", ".", "random", ".", "randn", "(", "self", ".", "dimA", ")", "\n", "return", "np", ".", "clip", "(", "action", "+", "noise", ",", "-", "self", ".", "ctrl_range", ",", "self", ".", "ctrl_range", ")", "\n", "", "else", ":", "\n", "            ", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.train": [[92, 129], ["ddpg_agent.DDPGAgent.buffer.sample_batch", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "ddpg_agent.DDPGAgent.Q", "torch.nn.MSELoss", "torch.nn.MSELoss.", "ddpg_agent.DDPGAgent.Q_optimizer.zero_grad", "torch.nn.MSELoss.backward", "ddpg_agent.DDPGAgent.Q_optimizer.step", "algorithms.utils.freeze", "ddpg_agent.DDPGAgent.pi_optimizer.zero_grad", "pi_loss.backward", "ddpg_agent.DDPGAgent.pi_optimizer.step", "algorithms.utils.unfreeze", "ddpg_agent.DDPGAgent.target_update", "torch.no_grad", "torch.mean", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "ddpg_agent.DDPGAgent.Q", "ddpg_agent.DDPGAgent.target_Q", "ddpg_agent.DDPGAgent.pi", "ddpg_agent.DDPGAgent.target_pi"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.ReplayBuffer.sample_batch", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.freeze", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.unfreeze", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.target_update"], ["", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        train actor-critic network using DDPG\n        \"\"\"", "\n", "device", "=", "self", ".", "device", "\n", "\n", "batch", "=", "self", ".", "buffer", ".", "sample_batch", "(", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "\n", "# unroll batch", "\n", "observations", "=", "torch", ".", "tensor", "(", "batch", ".", "state", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "actions", "=", "torch", ".", "tensor", "(", "batch", ".", "act", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "rewards", "=", "torch", ".", "tensor", "(", "batch", ".", "rew", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "next_observations", "=", "torch", ".", "tensor", "(", "batch", ".", "next_state", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "terminals", "=", "torch", ".", "tensor", "(", "batch", ".", "done", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "\n", "mask", "=", "1", "-", "terminals", "\n", "\n", "# compute TD targets based on target networks", "\n", "# if done, set target value to reward", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "target", "=", "rewards", "+", "self", ".", "gamma", "*", "mask", "*", "self", ".", "target_Q", "(", "next_observations", ",", "self", ".", "target_pi", "(", "next_observations", ")", ")", "\n", "\n", "", "out", "=", "self", ".", "Q", "(", "observations", ",", "actions", ")", "\n", "loss_ftn", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_ftn", "(", "out", ",", "target", ")", "\n", "self", ".", "Q_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "Q_optimizer", ".", "step", "(", ")", "\n", "\n", "freeze", "(", "self", ".", "Q", ")", "\n", "pi_loss", "=", "-", "torch", ".", "mean", "(", "self", ".", "Q", "(", "observations", ",", "self", ".", "pi", "(", "observations", ")", ")", ")", "\n", "self", ".", "pi_optimizer", ".", "zero_grad", "(", ")", "\n", "pi_loss", ".", "backward", "(", ")", "\n", "self", ".", "pi_optimizer", ".", "step", "(", ")", "\n", "unfreeze", "(", "self", ".", "Q", ")", "\n", "\n", "self", ".", "target_update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.eval": [[130, 163], ["gym.make", "range", "print", "gym.make.reset", "log.append", "sum", "ddpg_agent.DDPGAgent.get_action", "gym.make.step", "gym.make.close", "gym.make.render"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.get_action", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.render"], ["", "def", "eval", "(", "self", ",", "env_id", ",", "t", ",", "eval_num", "=", "5", ")", ":", "\n", "        ", "\"\"\"\n        evaluation of agent\n        during evaluation, agent execute noiseless actions\n        \"\"\"", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "\n", "log", "=", "[", "]", "\n", "for", "ep", "in", "range", "(", "eval_num", ")", ":", "\n", "            ", "state", "=", "env", ".", "reset", "(", ")", "\n", "step_count", "=", "0", "\n", "ep_reward", "=", "0", "\n", "done", "=", "False", "\n", "\n", "while", "not", "done", ":", "\n", "                ", "if", "self", ".", "render", "and", "ep", "==", "0", ":", "\n", "                    ", "env", ".", "render", "(", ")", "\n", "\n", "", "action", "=", "self", ".", "get_action", "(", "state", ",", "eval", "=", "True", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "step_count", "+=", "1", "\n", "state", "=", "next_state", "\n", "ep_reward", "+=", "reward", "\n", "", "if", "self", ".", "render", "and", "ep", "==", "0", ":", "\n", "                ", "env", ".", "close", "(", ")", "\n", "\n", "", "log", ".", "append", "(", "ep_reward", ")", "\n", "# normalize score w.r.t. h for consistent return", "\n", "", "avg", "=", "sum", "(", "log", ")", "/", "eval_num", "\n", "\n", "print", "(", "'step {} : {:.4f}'", ".", "format", "(", "t", ",", "avg", ")", ")", "\n", "\n", "return", "[", "t", ",", "avg", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.save_model": [[165, 179], ["print", "torch.save", "ddpg_agent.DDPGAgent.pi.state_dict", "ddpg_agent.DDPGAgent.Q.state_dict", "ddpg_agent.DDPGAgent.target_pi.state_dict", "ddpg_agent.DDPGAgent.target_Q.state_dict", "ddpg_agent.DDPGAgent.pi_optimizer.state_dict", "ddpg_agent.DDPGAgent.Q_optimizer.state_dict"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "'adding checkpoints...'", ")", "\n", "checkpoint_path", "=", "path", "+", "'model.pth.tar'", "\n", "torch", ".", "save", "(", "\n", "{", "'actor'", ":", "self", ".", "pi", ".", "state_dict", "(", ")", ",", "\n", "'critic'", ":", "self", ".", "Q", ".", "state_dict", "(", ")", ",", "\n", "'target_actor'", ":", "self", ".", "target_pi", ".", "state_dict", "(", ")", ",", "\n", "'target_critic'", ":", "self", ".", "target_Q", ".", "state_dict", "(", ")", ",", "\n", "'actor_optimizer'", ":", "self", ".", "pi_optimizer", ".", "state_dict", "(", ")", ",", "\n", "'critic_optimizer'", ":", "self", ".", "Q_optimizer", ".", "state_dict", "(", ")", "\n", "}", ",", "\n", "checkpoint_path", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg_agent.DDPGAgent.load_model": [[180, 192], ["print", "torch.load", "ddpg_agent.DDPGAgent.pi.load_state_dict", "ddpg_agent.DDPGAgent.Q.load_state_dict", "ddpg_agent.DDPGAgent.target_pi.load_state_dict", "ddpg_agent.DDPGAgent.target_Q.load_state_dict", "ddpg_agent.DDPGAgent.pi_optimizer.load_state_dict", "ddpg_agent.DDPGAgent.Q_optimizer.load_state_dict"], "methods", ["None"], ["", "def", "load_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "'networks loading...'", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "path", ")", "\n", "\n", "self", ".", "pi", ".", "load_state_dict", "(", "checkpoint", "[", "'actor'", "]", ")", "\n", "self", ".", "Q", ".", "load_state_dict", "(", "checkpoint", "[", "'critic'", "]", ")", "\n", "self", ".", "target_pi", ".", "load_state_dict", "(", "checkpoint", "[", "'target_actor'", "]", ")", "\n", "self", ".", "target_Q", ".", "load_state_dict", "(", "checkpoint", "[", "'target_critic'", "]", ")", "\n", "self", ".", "pi_optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'actor_optimizer'", "]", ")", "\n", "self", ".", "Q_optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'critic_optimizer'", "]", ")", "\n", "\n", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.ddpg.ddpg.run_ddpg": [[10, 145], ["locals", "int", "int", "gym.make", "algorithms.utils.get_env_spec", "algorithms.ddpg.ddpg_agent.DDPGAgent", "algorithms.utils.set_log_dir", "time.strftime", "open", "open", "csv.writer", "csv.writer", "gym.make.reset", "range", "open.close", "open.close", "open", "locals.items", "gym.make.step", "algorithms.ddpg.ddpg_agent.DDPGAgent.buffer.append", "print", "gym.make.action_space.sample", "algorithms.ddpg.ddpg_agent.DDPGAgent.get_action", "csv.writer.writerow", "gym.make.reset", "range", "algorithms.ddpg.ddpg_agent.DDPGAgent.eval", "csv.writer.writerow", "algorithms.ddpg.ddpg_agent.DDPGAgent.save_model", "algorithms.ddpg.ddpg_agent.DDPGAgent.train"], "function", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.get_env_spec", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.set_log_dir", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.get_action", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.eval", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.save_model", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.train"], ["def", "run_ddpg", "(", "env_id", ",", "\n", "gamma", "=", "0.99", ",", "\n", "actor_lr", "=", "1e-4", ",", "\n", "critic_lr", "=", "1e-3", ",", "\n", "polyak", "=", "1e-3", ",", "\n", "sigma", "=", "0.1", ",", "\n", "hidden_size1", "=", "256", ",", "\n", "hidden_size2", "=", "256", ",", "\n", "max_iter", "=", "1e6", ",", "\n", "eval_interval", "=", "2000", ",", "\n", "start_train", "=", "10000", ",", "\n", "train_interval", "=", "50", ",", "\n", "buffer_size", "=", "1e6", ",", "\n", "fill_buffer", "=", "20000", ",", "\n", "batch_size", "=", "128", ",", "\n", "h_scale", "=", "1.0", ",", "\n", "ep_len", "=", "None", ",", "\n", "device", "=", "'cpu'", ",", "\n", "render", "=", "'False'", "\n", ")", ":", "\n", "    ", "\"\"\"\n    :param env_id: registered id of the environment\n    :param gamma: discount factor\n    :param actor_lr: learning rate of actor optimizer\n    :param critic_lr: learning rate of critic optimizer\n    :param sigma: noise scale of Gaussian noise\n    :param polyak: target smoothing coefficient\n    :param hidden1: number of nodes of hidden layer1 of critic\n    :param hidden2: number of nodes of hidden layer2 of critic\n    :param max_iter: total number of environment interactions\n    :param buffer_size: size of replay buffer\n    :param fill_buffer: number of execution of random policy\n    :param batch_size: size of minibatch to be sampled during training\n    :param train_interval: length of interval between consecutive training\n    :param start_train: the beginning step of training\n    :param eval_interval: length of interval between evaluation\n    :param h_scale: scale of timestep of environment\n    :param device: device used for training\n    :param render: bool type variable for rendering\n    \"\"\"", "\n", "args", "=", "locals", "(", ")", "\n", "\n", "max_iter", "=", "int", "(", "max_iter", ")", "\n", "buffer_size", "=", "int", "(", "buffer_size", ")", "\n", "\n", "num_checkpoints", "=", "5", "\n", "checkpoint_interval", "=", "max_iter", "//", "(", "num_checkpoints", "-", "1", ")", "\n", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "\n", "dimS", ",", "dimA", ",", "_", ",", "ctrl_range", "=", "get_env_spec", "(", "env", ")", "\n", "\n", "if", "ep_len", "is", "None", ":", "\n", "        ", "max_ep_len", "=", "env", ".", "_max_episode_steps", "\n", "", "else", ":", "\n", "        ", "max_ep_len", "=", "ep_len", "\n", "\n", "", "agent", "=", "DDPGAgent", "(", "dimS", ",", "\n", "dimA", ",", "\n", "ctrl_range", ",", "\n", "gamma", "=", "gamma", ",", "\n", "actor_lr", "=", "actor_lr", ",", "\n", "critic_lr", "=", "critic_lr", ",", "\n", "polyak", "=", "polyak", ",", "\n", "sigma", "=", "sigma", ",", "\n", "hidden_size1", "=", "hidden_size1", ",", "\n", "hidden_size2", "=", "hidden_size2", ",", "\n", "buffer_size", "=", "buffer_size", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "h_scale", "=", "h_scale", ",", "\n", "device", "=", "device", ",", "\n", "render", "=", "render", ")", "\n", "\n", "set_log_dir", "(", "env_id", ")", "\n", "current_time", "=", "time", ".", "strftime", "(", "\"%m%d-%H%M%S\"", ")", "\n", "train_log", "=", "open", "(", "'./train_log/'", "+", "env_id", "+", "'/DDPG_'", "+", "current_time", "+", "'.csv'", ",", "\n", "'w'", ",", "\n", "encoding", "=", "'utf-8'", ",", "\n", "newline", "=", "''", ")", "\n", "eval_log", "=", "open", "(", "'./eval_log/'", "+", "env_id", "+", "'/DDPG_'", "+", "current_time", "+", "'.csv'", ",", "\n", "'w'", ",", "\n", "encoding", "=", "'utf-8'", ",", "\n", "newline", "=", "''", ")", "\n", "\n", "train_logger", "=", "csv", ".", "writer", "(", "train_log", ")", "\n", "eval_logger", "=", "csv", ".", "writer", "(", "eval_log", ")", "\n", "with", "open", "(", "'./eval_log/'", "+", "env_id", "+", "'/DDPG_'", "+", "current_time", "+", "'.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "key", ",", "val", "in", "args", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "key", ",", "'='", ",", "val", ",", "file", "=", "f", ")", "\n", "\n", "", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "step_count", "=", "0", "\n", "ep_reward", "=", "0", "\n", "\n", "# main loop", "\n", "for", "t", "in", "range", "(", "max_iter", "+", "1", ")", ":", "\n", "        ", "if", "t", "<", "fill_buffer", ":", "\n", "# first collect sufficient number of samples during the initial stage", "\n", "            ", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "agent", ".", "get_action", "(", "state", ")", "\n", "\n", "", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "# env-agent interaction", "\n", "step_count", "+=", "1", "\n", "\n", "if", "step_count", "==", "max_ep_len", ":", "\n", "            ", "done", "=", "False", "\n", "\n", "", "agent", ".", "buffer", ".", "append", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "# save the transition sample", "\n", "\n", "state", "=", "next_state", "\n", "ep_reward", "+=", "reward", "\n", "\n", "if", "done", "or", "(", "step_count", "==", "max_ep_len", ")", ":", "\n", "            ", "train_logger", ".", "writerow", "(", "[", "t", ",", "ep_reward", "]", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "step_count", "=", "0", "\n", "ep_reward", "=", "0", "\n", "\n", "", "if", "(", "t", ">=", "start_train", ")", "and", "(", "t", "%", "train_interval", "==", "0", ")", ":", "\n", "# Start training after sufficient number of transition samples are gathered", "\n", "            ", "for", "_", "in", "range", "(", "train_interval", ")", ":", "\n", "                ", "agent", ".", "train", "(", ")", "\n", "\n", "", "", "if", "t", "%", "eval_interval", "==", "0", ":", "\n", "            ", "log", "=", "agent", ".", "eval", "(", "env_id", ",", "t", ")", "\n", "eval_logger", ".", "writerow", "(", "log", ")", "\n", "\n", "", "if", "t", "%", "checkpoint_interval", "==", "0", ":", "\n", "            ", "agent", ".", "save_model", "(", "'./checkpoints/'", "+", "env_id", "+", "'/ddpg_{}th_iter_'", ".", "format", "(", "t", ")", ")", "\n", "\n", "", "", "train_log", ".", "close", "(", ")", "\n", "eval_log", ".", "close", "(", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn.run_hjdqn": [[12, 188], ["locals", "int", "int", "algorithms.utils.scaled_env", "algorithms.utils.scaled_env", "algorithms.utils.get_env_spec", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent", "algorithms.utils.set_log_dir", "time.strftime", "open", "open", "csv.writer", "csv.writer", "algorithms.utils.scaled_env.reset", "algorithms.noise.Zero.reset", "algorithms.utils.scaled_env.action_space.sample", "range", "open.close", "open.close", "open", "locals.items", "algorithms.noise.IndependentGaussian", "algorithms.utils.scaled_env.step", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent.buffer.append", "print", "print", "algorithms.noise.SDE", "print", "algorithms.noise.Zero", "algorithms.utils.scaled_env.action_space.sample", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent.get_action", "algorithms.noise.Zero.sample", "csv.writer.writerow", "algorithms.utils.scaled_env.reset", "algorithms.noise.Zero.reset", "algorithms.utils.scaled_env.action_space.sample", "range", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent.eval", "csv.writer.writerow", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent.train", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent.save_model", "algorithms.hjdqn.hjdqn_agent.HJDQNAgent.save_model"], "function", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.scaled_env", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.scaled_env", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.get_env_spec", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.set_log_dir", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.get_action", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.eval", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.train", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.save_model", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.save_model"], ["def", "run_hjdqn", "(", "env_id", ",", "\n", "L", "=", "10.0", ",", "\n", "gamma", "=", "0.99", ",", "\n", "lr", "=", "1e-3", ",", "\n", "sigma", "=", "0.15", ",", "\n", "polyak", "=", "1e-3", ",", "\n", "hidden1", "=", "256", ",", "\n", "hidden2", "=", "256", ",", "\n", "max_iter", "=", "1e6", ",", "\n", "buffer_size", "=", "1e6", ",", "\n", "fill_buffer", "=", "20000", ",", "\n", "batch_size", "=", "128", ",", "\n", "train_interval", "=", "50", ",", "\n", "start_train", "=", "10000", ",", "\n", "eval_interval", "=", "2000", ",", "\n", "smooth", "=", "False", ",", "\n", "double", "=", "True", ",", "\n", "noise", "=", "'gaussian'", ",", "\n", "ep_len", "=", "None", ",", "\n", "h_scale", "=", "1.0", ",", "\n", "device", "=", "'cpu'", ",", "\n", "render", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    param env_id: registered id of the environment\n    param L: size of control constraint\n    param gamma: discount factor, corresponds to 1 - gamma * h\n    param lr: learning rate of optimizer\n    param sigma: noise scale of Gaussian noise\n    param polyak: target smoothing coefficient\n    param hidden1: number of nodes of hidden layer1 of critic\n    param hidden2: number of nodes of hidden layer2 of critic\n    param max_iter: total number of environment interactions\n    param buffer_size: size of replay buffer\n    param fill_buffer: number of execution of random policy\n    param batch_size: size of minibatch to be sampled during training\n    param train_interval: length of interval between consecutive training\n    param start_train: the beginning step of training\n    param eval_interval: length of interval between evaluation\n    param h_scale: scale of timestep of environment\n    param device: device used for training\n    param render: bool type variable for rendering\n    \"\"\"", "\n", "args", "=", "locals", "(", ")", "\n", "\n", "max_iter", "=", "int", "(", "max_iter", ")", "\n", "buffer_size", "=", "int", "(", "buffer_size", ")", "\n", "num_checkpoints", "=", "5", "\n", "checkpoint_interval", "=", "max_iter", "//", "(", "num_checkpoints", "-", "1", ")", "\n", "\n", "# create environment", "\n", "# adjust time step length, episode length if needed", "\n", "env", "=", "scaled_env", "(", "env_id", "=", "env_id", ",", "scale_factor", "=", "h_scale", ")", "\n", "test_env", "=", "scaled_env", "(", "env_id", "=", "env_id", ",", "scale_factor", "=", "h_scale", ")", "\n", "\n", "if", "ep_len", "is", "None", ":", "\n", "\n", "        ", "max_ep_len", "=", "env", ".", "_max_episode_steps", "\n", "", "else", ":", "\n", "# in case episode limit is specified as a parameter", "\n", "        ", "max_ep_len", "=", "ep_len", "\n", "\n", "", "dimS", ",", "dimA", ",", "h", ",", "ctrl_range", "=", "get_env_spec", "(", "env", ")", "\n", "\n", "# scale gamma & learning rate", "\n", "gamma", "=", "1.", "-", "h_scale", "*", "(", "1.", "-", "gamma", ")", "\n", "lr", "=", "h_scale", "*", "lr", "\n", "\n", "# create agent", "\n", "agent", "=", "HJDQNAgent", "(", "dimS", ",", "dimA", ",", "ctrl_range", ",", "\n", "gamma", ",", "\n", "h", ",", "L", ",", "sigma", ",", "\n", "hidden1", ",", "\n", "hidden2", ",", "\n", "lr", ",", "\n", "polyak", ",", "\n", "buffer_size", ",", "\n", "batch_size", ",", "\n", "smooth", "=", "smooth", ",", "\n", "device", "=", "device", ",", "\n", "double", "=", "double", ",", "\n", "render", "=", "render", ",", "\n", "scale_factor", "=", "h_scale", ")", "\n", "\n", "# logger set-up", "\n", "\n", "set_log_dir", "(", "env_id", ")", "\n", "current_time", "=", "time", ".", "strftime", "(", "\"%m%d-%H%M%S\"", ")", "\n", "train_log", "=", "open", "(", "'./train_log/'", "+", "env_id", "+", "'/HJDQN_'", "+", "current_time", "+", "'.csv'", ",", "\n", "'w'", ",", "\n", "encoding", "=", "'utf-8'", ",", "\n", "newline", "=", "''", ")", "\n", "\n", "eval_log", "=", "open", "(", "'./eval_log/'", "+", "env_id", "+", "'/HJDQN_'", "+", "current_time", "+", "'.csv'", ",", "\n", "'w'", ",", "\n", "encoding", "=", "'utf-8'", ",", "\n", "newline", "=", "''", ")", "\n", "\n", "train_logger", "=", "csv", ".", "writer", "(", "train_log", ")", "\n", "eval_logger", "=", "csv", ".", "writer", "(", "eval_log", ")", "\n", "\n", "with", "open", "(", "'./eval_log/'", "+", "env_id", "+", "'/HJDQN_'", "+", "current_time", "+", "'.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "key", ",", "val", "in", "args", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "key", ",", "'='", ",", "val", ",", "file", "=", "f", ")", "\n", "\n", "# set noise process for exploration", "\n", "", "", "if", "noise", "==", "'gaussian'", ":", "\n", "        ", "noise_process", "=", "IndependentGaussian", "(", "dim", "=", "dimA", ",", "sigma", "=", "sigma", ")", "\n", "", "elif", "noise", "==", "'sde'", ":", "\n", "        ", "print", "(", "'noise set to Ornstein-Uhlenbeck process'", ")", "\n", "noise_process", "=", "SDE", "(", "dim", "=", "dimA", ",", "sigma", "=", "sigma", ",", "dt", "=", "h", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'unidentified noise type : noise process is set to zero'", ")", "\n", "noise_process", "=", "Zero", "(", "dim", "=", "dimA", ")", "\n", "\n", "# start environment roll-out", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "noise", "=", "noise_process", ".", "reset", "(", ")", "\n", "step_count", "=", "0", "\n", "ep_reward", "=", "0.", "\n", "\n", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "\n", "# main loop", "\n", "for", "t", "in", "range", "(", "max_iter", "+", "1", ")", ":", "\n", "\n", "# t : number of env-agent interactions (=number of transition samples observed)", "\n", "        ", "if", "t", "<", "fill_buffer", ":", "\n", "# first collect sufficient number of samples during the initial stage", "\n", "            ", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "agent", ".", "get_action", "(", "state", ",", "action", ",", "noise", ")", "\n", "noise", "=", "noise_process", ".", "sample", "(", ")", "\n", "\n", "", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "# env-agent interaction", "\n", "step_count", "+=", "1", "\n", "\n", "if", "step_count", "==", "max_ep_len", ":", "\n", "# when the episode roll-out is truncated artificially(so that done=True), set done=False", "\n", "# thus, done=True only if the state is a terminal state", "\n", "            ", "done", "=", "False", "\n", "\n", "", "agent", ".", "buffer", ".", "append", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "# save the transition sample", "\n", "\n", "ep_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "\n", "if", "done", "or", "(", "step_count", "==", "max_ep_len", ")", ":", "\n", "            ", "train_logger", ".", "writerow", "(", "[", "t", ",", "ep_reward", "]", ")", "\n", "\n", "# restart an episode", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "noise", "=", "noise_process", ".", "reset", "(", ")", "\n", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "step_count", "=", "0", "\n", "ep_reward", "=", "0.", "\n", "\n", "# Start training after sufficient number of transition samples are gathered", "\n", "", "if", "(", "t", ">=", "start_train", ")", "and", "(", "t", "%", "train_interval", "==", "0", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "train_interval", ")", ":", "\n", "                ", "agent", ".", "train", "(", ")", "\n", "\n", "", "", "if", "t", "%", "eval_interval", "==", "0", ":", "\n", "            ", "eval_data", "=", "agent", ".", "eval", "(", "test_env", ",", "t", ")", "\n", "eval_logger", ".", "writerow", "(", "eval_data", ")", "\n", "\n", "", "if", "t", "%", "checkpoint_interval", "==", "0", ":", "\n", "            ", "if", "smooth", ":", "\n", "                ", "agent", ".", "save_model", "(", "'./checkpoints/'", "+", "env_id", "+", "'/hjdqn_{}th_iter_smooth_'", ".", "format", "(", "t", ")", ")", "\n", "", "else", ":", "\n", "                ", "agent", ".", "save_model", "(", "'./checkpoints/'", "+", "env_id", "+", "'/hjdqn_{}th_iter_'", ".", "format", "(", "t", ")", ")", "\n", "\n", "", "", "", "train_log", ".", "close", "(", ")", "\n", "eval_log", ".", "close", "(", ")", "\n", "\n", "return", "\n", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.__init__": [[13, 79], ["locals", "print", "print", "print", "print", "algorithms.model.Critic().to", "copy.deepcopy().to", "torch.optim.Adam", "algorithms.buffer.ReplayBuffer", "algorithms.utils.freeze", "hjdqn_agent.HJDQNAgent.Q.parameters", "algorithms.model.Critic", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.utils.freeze"], ["    ", "def", "__init__", "(", "self", ",", "\n", "dimS", ",", "\n", "dimA", ",", "\n", "ctrl_range", ",", "\n", "gamma", ",", "\n", "h", ",", "\n", "L", ",", "\n", "sigma", ",", "\n", "hidden1", ",", "\n", "hidden2", ",", "\n", "lr", ",", "\n", "polyak", ",", "\n", "buffer_size", ",", "\n", "batch_size", ",", "\n", "smooth", "=", "False", ",", "\n", "device", "=", "'cpu'", ",", "\n", "double", "=", "True", ",", "\n", "scale_factor", "=", "1.0", ",", "\n", "render", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param dimS: dimension of observation space\n        :param dimA: dimension of action space\n        :param ctrl_range: range of valid action range\n        description of the rest of the params are given in hjdqn.py\n        \"\"\"", "\n", "\n", "args", "=", "locals", "(", ")", "\n", "print", "(", "'agent spec'", ")", "\n", "print", "(", "'-'", "*", "80", ")", "\n", "print", "(", "args", ")", "\n", "print", "(", "'-'", "*", "80", ")", "\n", "\n", "self", ".", "dimS", "=", "dimS", "\n", "self", ".", "dimA", "=", "dimA", "\n", "self", ".", "ctrl_range", "=", "ctrl_range", "\n", "\n", "self", ".", "h", "=", "h", "\n", "\n", "# set networks", "\n", "self", ".", "Q", "=", "Critic", "(", "dimS", ",", "dimA", ",", "hidden_size1", "=", "hidden1", ",", "hidden_size2", "=", "hidden2", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "target_Q", "=", "copy", ".", "deepcopy", "(", "self", ".", "Q", ")", ".", "to", "(", "device", ")", "# set target network", "\n", "\n", "if", "double", ":", "\n", "# In double Q-learning setting, freeze the target network", "\n", "            ", "freeze", "(", "self", ".", "target_Q", ")", "# freeze target network in double Q learning setting", "\n", "\n", "", "self", ".", "optimizer", "=", "Adam", "(", "self", ".", "Q", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "polyak", "=", "polyak", "\n", "self", ".", "L", "=", "L", "\n", "\n", "self", ".", "sigma", "=", "sigma", "\n", "\n", "self", ".", "buffer", "=", "ReplayBuffer", "(", "dimS", ",", "dimA", ",", "buffer_size", ")", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "self", ".", "smooth", "=", "smooth", "\n", "\n", "self", ".", "double", "=", "double", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "render", "=", "render", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.target_update": [[80, 86], ["zip", "hjdqn_agent.HJDQNAgent.Q.parameters", "hjdqn_agent.HJDQNAgent.target_Q.parameters", "target_p.data.copy_"], "methods", ["None"], ["", "def", "target_update", "(", "self", ")", ":", "\n", "# soft target update", "\n", "        ", "for", "p", ",", "target_p", "in", "zip", "(", "self", ".", "Q", ".", "parameters", "(", ")", ",", "self", ".", "target_Q", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_p", ".", "data", ".", "copy_", "(", "self", ".", "polyak", "*", "p", ".", "data", "+", "(", "1.0", "-", "self", ".", "polyak", ")", "*", "target_p", ".", "data", ")", "\n", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.get_action": [[87, 120], ["torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to.requires_grad_", "hjdqn_agent.HJDQNAgent.Q", "hjdqn_agent.HJDQNAgent.backward", "a_dot.reshape.reshape.cpu().detach().numpy", "a_dot.reshape.reshape.reshape", "numpy.clip", "torch.no_grad", "torch.norm", "torch.tensor().view", "torch.tensor().view", "a_dot.reshape.reshape.cpu().detach", "torch.tensor", "torch.tensor", "a_dot.reshape.reshape.cpu", "torch.tanh"], "methods", ["None"], ["", "def", "get_action", "(", "self", ",", "state", ",", "action", ",", "noise", ")", ":", "\n", "        ", "\"\"\"\n\n        :param state: current state\n        :param action: current action\n        :param explore: bool type variable to indicate exploration\n        :return: next action\n        \"\"\"", "\n", "device", "=", "self", ".", "device", "\n", "\n", "dimS", "=", "self", ".", "dimS", "\n", "dimA", "=", "self", ".", "dimA", "\n", "\n", "s", "=", "torch", ".", "tensor", "(", "state", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "dimS", ")", ".", "to", "(", "device", ")", "\n", "a", "=", "torch", ".", "tensor", "(", "action", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "dimA", ")", ".", "to", "(", "device", ")", "\n", "a", ".", "requires_grad_", "(", "True", ")", "\n", "\n", "q", "=", "self", ".", "Q", "(", "s", ",", "a", ")", "\n", "q", ".", "backward", "(", ")", "\n", "dq", "=", "a", ".", "grad", "# compute gradient of Q w.r.t. a", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "n", "=", "torch", ".", "norm", "(", "dq", ")", "\n", "\n", "# compute increment of action", "\n", "if", "self", ".", "smooth", ":", "\n", "                ", "a_dot", "=", "(", "self", ".", "h", "*", "self", ".", "L", "*", "torch", ".", "tanh", "(", "n", "/", "self", ".", "L", ")", "/", "(", "n", "+", "1e-8", ")", ")", "*", "dq", "\n", "", "else", ":", "\n", "                ", "a_dot", "=", "(", "self", ".", "h", "*", "self", ".", "L", "/", "(", "n", "+", "1e-8", ")", ")", "*", "dq", "\n", "", "", "a_dot", "=", "a_dot", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "a_dot", "=", "a_dot", ".", "reshape", "(", "action", ".", "shape", ")", "\n", "\n", "return", "np", ".", "clip", "(", "action", "+", "a_dot", "+", "noise", ",", "-", "self", ".", "ctrl_range", ",", "self", ".", "ctrl_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.train": [[121, 178], ["torch.Tensor().to", "hjdqn_agent.HJDQNAgent.buffer.sample_batch", "torch.tensor().to.requires_grad_", "hjdqn_agent.HJDQNAgent.sum().backward", "hjdqn_agent.HJDQNAgent.Q", "torch.nn.MSELoss", "torch.nn.MSELoss.", "hjdqn_agent.HJDQNAgent.optimizer.zero_grad", "torch.nn.MSELoss.backward", "hjdqn_agent.HJDQNAgent.optimizer.step", "hjdqn_agent.HJDQNAgent.target_update", "torch.no_grad", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "hjdqn_agent.HJDQNAgent.Q", "hjdqn_agent.HJDQNAgent.target_Q", "torch.no_grad", "torch.min", "torch.Tensor", "hjdqn_agent.HJDQNAgent.sum", "torch.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.sqrt", "hjdqn_agent.HJDQNAgent.target_Q", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.ReplayBuffer.sample_batch", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.target_update"], ["", "def", "train", "(", "self", ")", ":", "\n", "\n", "        ", "device", "=", "self", ".", "device", "\n", "gamma", "=", "self", ".", "gamma", "\n", "h", "=", "self", ".", "h", "\n", "L", "=", "self", ".", "L", "\n", "# sample mini-batch", "\n", "lim", "=", "torch", ".", "Tensor", "(", "self", ".", "ctrl_range", ")", ".", "to", "(", "device", ")", "\n", "\n", "batch", "=", "self", ".", "buffer", ".", "sample_batch", "(", "self", ".", "batch_size", ")", "\n", "\n", "# unroll batch", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "observations", "=", "torch", ".", "tensor", "(", "batch", ".", "state", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "actions", "=", "torch", ".", "tensor", "(", "batch", ".", "act", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "rewards", "=", "torch", ".", "tensor", "(", "batch", ".", "rew", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "next_observations", "=", "torch", ".", "tensor", "(", "batch", ".", "next_state", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "terminals", "=", "torch", ".", "tensor", "(", "batch", ".", "done", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "device", ")", "\n", "\n", "mask", "=", "1.0", "-", "terminals", "\n", "\n", "# Since we need \\nabla_a Q(x, a ; \\theta), we need to backpropagate the derivatives through action batches.", "\n", "", "actions", ".", "requires_grad_", "(", "True", ")", "\n", "\n", "if", "self", ".", "double", ":", "\n", "# In double Q-learning setting, the control b is chosen using the Q-network instead of the target Q-network", "\n", "            ", "q", "=", "self", ".", "Q", "(", "observations", ",", "actions", ")", "\n", "", "else", ":", "\n", "            ", "q", "=", "self", ".", "target_Q", "(", "observations", ",", "actions", ")", "\n", "\n", "", "q", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "# compute sample-wise gradient of Q w.r.t. a", "\n", "# here, we use double Q-learning", "\n", "g", "=", "actions", ".", "grad", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# divide norm of grad by h in advance, just for computational efficiency", "\n", "            ", "norm", "=", "(", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "g", "**", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ")", "+", "1e-9", ")", "/", "(", "h", "*", "L", ")", "\n", "# compute increment of action in sample-wise manner", "\n", "da", "=", "g", "/", "norm", "\n", "# next_a = torch.clamp(actions + da, -self.ctrl_range, self.ctrl_range)", "\n", "next_a", "=", "torch", ".", "min", "(", "torch", ".", "max", "(", "actions", "+", "da", ",", "-", "lim", ")", ",", "lim", ")", "\n", "# target construction", "\n", "target", "=", "h", "*", "rewards", "+", "gamma", "*", "mask", "*", "self", ".", "target_Q", "(", "next_observations", ",", "next_a", ")", "\n", "\n", "", "out", "=", "self", ".", "Q", "(", "observations", ",", "actions", ")", "\n", "\n", "loss_ftn", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_ftn", "(", "out", ",", "target", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "self", ".", "target_update", "(", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.eval": [[179, 218], ["range", "print", "test_env.reset", "numpy.zeros", "log.append", "test_env.action_space.sample", "hjdqn_agent.HJDQNAgent.get_action", "test_env.step", "test_env.close", "sum", "test_env.render"], "methods", ["home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.buffer.CircularBuffer.append", "home.repos.pwc.inspect_result.HJDQN_HJQ.algorithms.noise.SDE.sample", "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.get_action", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close", "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.render"], ["", "def", "eval", "(", "self", ",", "test_env", ",", "t", ",", "eval_num", "=", "5", ")", ":", "\n", "        ", "\"\"\"\n        evaluation of agent\n        during evaluation, agent execute noiseless actions\n        \"\"\"", "\n", "\n", "log", "=", "[", "]", "\n", "for", "ep", "in", "range", "(", "eval_num", ")", ":", "\n", "            ", "state", "=", "test_env", ".", "reset", "(", ")", "\n", "\n", "noise", "=", "np", ".", "zeros", "(", "self", ".", "dimA", ")", "\n", "\n", "action", "=", "0.1", "*", "test_env", ".", "action_space", ".", "sample", "(", ")", "\n", "# action = test_env.action_space.sample()", "\n", "step_count", "=", "0", "\n", "ep_reward", "=", "0", "\n", "done", "=", "False", "\n", "\n", "while", "not", "done", ":", "\n", "\n", "                ", "if", "self", ".", "render", "and", "ep", "==", "0", ":", "\n", "                    ", "test_env", ".", "render", "(", ")", "\n", "\n", "", "action", "=", "self", ".", "get_action", "(", "state", ",", "action", ",", "noise", ")", "# deterministic action", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "test_env", ".", "step", "(", "action", ")", "\n", "step_count", "+=", "1", "\n", "state", "=", "next_state", "\n", "\n", "ep_reward", "+=", "reward", "\n", "", "if", "self", ".", "render", "and", "ep", "==", "0", ":", "\n", "                ", "test_env", ".", "close", "(", ")", "\n", "\n", "", "log", ".", "append", "(", "ep_reward", ")", "\n", "# normalize score w.r.t. h for consistent return", "\n", "", "avg", "=", "self", ".", "scale_factor", "*", "sum", "(", "log", ")", "/", "eval_num", "\n", "\n", "print", "(", "'step {} : {}'", ".", "format", "(", "t", ",", "avg", ")", ")", "\n", "\n", "return", "[", "t", ",", "avg", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.save_model": [[219, 231], ["print", "torch.save", "hjdqn_agent.HJDQNAgent.Q.state_dict", "hjdqn_agent.HJDQNAgent.target_Q.state_dict", "hjdqn_agent.HJDQNAgent.optimizer.state_dict"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "'adding checkpoint...'", ")", "\n", "checkpoint_path", "=", "path", "+", "'model.pth.tar'", "\n", "torch", ".", "save", "(", "\n", "{", "\n", "'critic'", ":", "self", ".", "Q", ".", "state_dict", "(", ")", ",", "\n", "'target_critic'", ":", "self", ".", "target_Q", ".", "state_dict", "(", ")", ",", "\n", "'critic_optimizer'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "}", ",", "\n", "checkpoint_path", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.hjdqn.hjdqn_agent.HJDQNAgent.load_model": [[232, 241], ["print", "torch.load", "hjdqn_agent.HJDQNAgent.Q.load_state_dict", "hjdqn_agent.HJDQNAgent.target_Q.load_state_dict", "hjdqn_agent.HJDQNAgent.optimizer.load_state_dict"], "methods", ["None"], ["", "def", "load_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "'networks loading...'", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "self", ".", "device", ")", "\n", "\n", "self", ".", "Q", ".", "load_state_dict", "(", "checkpoint", "[", "'critic'", "]", ")", "\n", "self", ".", "target_Q", ".", "load_state_dict", "(", "checkpoint", "[", "'target_critic'", "]", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'critic_optimizer'", "]", ")", "\n", "\n", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.__init__": [[9, 28], ["numpy.load", "numpy.load", "gym.spaces.Box", "gym.spaces.Box", "os.path.join", "os.path.join", "os.path.dirname", "os.path.dirname"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# simulate LQR until $T = 10$", "\n", "        ", "self", ".", "dt", "=", ".05", "\n", "self", ".", "n", "=", "20", "\n", "self", ".", "A", "=", "np", ".", "load", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "\"data/A20.npy\"", ")", ")", "\n", "self", ".", "B", "=", "np", ".", "load", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "\"data/B20.npy\"", ")", ")", "\n", "\n", "# random sampling from the action space : $U[-1, 1)$", "\n", "self", ".", "action_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "1.", ",", "\n", "high", "=", "1.", ",", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "np", ".", "inf", ",", "\n", "high", "=", "np", ".", "inf", ",", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "x", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.step": [[29, 48], ["lqr20d.LinearQuadraticRegulator20DEnv.step.ftn"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "u", ")", ":", "\n", "        ", "h", "=", "self", ".", "dt", "\n", "# quadratic cost function x^T Q x + u^T R u", "\n", "costs", "=", "5.", "*", "np", ".", "sum", "(", "self", ".", "x", "**", "2", ")", "+", "np", ".", "sum", "(", "u", "**", "2", ")", "# Q = 5I, R = I", "\n", "\n", "def", "ftn", "(", "t", ",", "y", ")", ":", "\n", "# nested function which is determined by free variables A, B, and u", "\n", "# note that our dynamical system is time-homogeneous", "\n", "            ", "return", "self", ".", "A", "@", "y", "+", "self", ".", "B", "@", "u", "\n", "\n", "# Runge-Kutta 4-th order method", "\n", "", "k1", "=", "ftn", "(", ".0", ",", "self", ".", "x", ")", "\n", "k2", "=", "ftn", "(", ".0", "+", ".5", "*", "h", ",", "self", ".", "x", "+", ".5", "*", "h", "*", "k1", ")", "\n", "k3", "=", "ftn", "(", ".0", "+", ".5", "*", "h", ",", "self", ".", "x", "+", ".5", "*", "h", "*", "k2", ")", "\n", "k4", "=", "ftn", "(", ".0", "+", "h", ",", "self", ".", "x", "+", "h", "*", "k3", ")", "\n", "\n", "dx", "=", "h", "*", "(", "k1", "+", "2.", "*", "k2", "+", "2.", "*", "k3", "+", "k4", ")", "/", "6.", "\n", "self", ".", "x", "=", "self", ".", "x", "+", "dx", "# x(t + h) = x(t) + \\int_t^{t+h} \\dot x dt", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", ",", "-", "costs", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.reset": [[49, 53], ["numpy.copy", "numpy.random.rand"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# sample the initial state vector uniformly from $U[-1, 1)$", "\n", "        ", "self", ".", "x", "=", "2.", "*", "np", ".", "random", ".", "rand", "(", "self", ".", "n", ")", "-", "1.", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.render": [[54, 56], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.close": [[57, 59], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.xnorm": [[60, 63], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "xnorm", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "x", "**", "2", ")", ".", "sum", "(", ")", "**", ".5", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr20d.LinearQuadraticRegulator20DEnv.set_state": [[64, 66], ["None"], "methods", ["None"], ["", "def", "set_state", "(", "self", ",", "x1", ")", ":", "\n", "        ", "self", ".", "x", "=", "x1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr50d.LinearQuadraticRegulator50DEnv.__init__": [[9, 28], ["numpy.load", "numpy.load", "gym.spaces.Box", "gym.spaces.Box", "os.path.join", "os.path.join", "os.path.dirname", "os.path.dirname"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# simulate LQR until $T = 10$", "\n", "        ", "self", ".", "dt", "=", ".05", "\n", "self", ".", "n", "=", "50", "\n", "self", ".", "A", "=", "np", ".", "load", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "\"data/A50.npy\"", ")", ")", "\n", "self", ".", "B", "=", "np", ".", "load", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "\"data/B50.npy\"", ")", ")", "\n", "\n", "# random sampling from the action space : $U[-1, 1)$", "\n", "self", ".", "action_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "1.", ",", "\n", "high", "=", "1.", ",", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "np", ".", "inf", ",", "\n", "high", "=", "np", ".", "inf", ",", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "x", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr50d.LinearQuadraticRegulator50DEnv.step": [[29, 49], ["lqr50d.LinearQuadraticRegulator50DEnv.step.ftn"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "u", ")", ":", "\n", "        ", "h", "=", "self", ".", "dt", "\n", "# quadratic cost function x^T Q x + u^T R u", "\n", "costs", "=", "5.", "*", "np", ".", "sum", "(", "self", ".", "x", "**", "2", ")", "+", "np", ".", "sum", "(", "u", "**", "2", ")", "# Q = 5I, R = I", "\n", "\n", "def", "ftn", "(", "t", ",", "y", ")", ":", "\n", "# nested function which is determined by free variables A, B, and u", "\n", "# note that our dynamical system is time-homogeneous", "\n", "            ", "return", "self", ".", "A", "@", "y", "+", "self", ".", "B", "@", "u", "\n", "\n", "# Runge-Kutta 4-th order method", "\n", "", "k1", "=", "ftn", "(", ".0", ",", "self", ".", "x", ")", "\n", "k2", "=", "ftn", "(", ".0", "+", ".5", "*", "h", ",", "self", ".", "x", "+", ".5", "*", "h", "*", "k1", ")", "\n", "k3", "=", "ftn", "(", ".0", "+", ".5", "*", "h", ",", "self", ".", "x", "+", ".5", "*", "h", "*", "k2", ")", "\n", "k4", "=", "ftn", "(", ".0", "+", "h", ",", "self", ".", "x", "+", "h", "*", "k3", ")", "\n", "\n", "dx", "=", "h", "*", "(", "k1", "+", "2.", "*", "k2", "+", "2.", "*", "k3", "+", "k4", ")", "/", "6.", "\n", "self", ".", "x", "=", "self", ".", "x", "+", "dx", "# x(t + h) = x(t) + h dx", "\n", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", ",", "-", "costs", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr50d.LinearQuadraticRegulator50DEnv.reset": [[50, 54], ["numpy.copy", "numpy.random.rand"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# sample the initial state vector uniformly from $U[-1, 1)$", "\n", "        ", "self", ".", "x", "=", "2.", "*", "np", ".", "random", ".", "rand", "(", "self", ".", "n", ")", "-", "1.", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr50d.LinearQuadraticRegulator50DEnv.render": [[55, 57], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr50d.LinearQuadraticRegulator50DEnv.close": [[58, 60], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr50d.LinearQuadraticRegulator50DEnv.xnorm": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "xnorm", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "x", "**", "2", ")", ".", "sum", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr30d.LinearQuadraticRegulator30DEnv.__init__": [[9, 28], ["numpy.load", "numpy.load", "gym.spaces.Box", "gym.spaces.Box", "os.path.join", "os.path.join", "os.path.dirname", "os.path.dirname"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# simulate LQR until $T = 10$", "\n", "        ", "self", ".", "dt", "=", ".05", "\n", "self", ".", "n", "=", "30", "\n", "self", ".", "A", "=", "np", ".", "load", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "\"data/A30.npy\"", ")", ")", "\n", "self", ".", "B", "=", "np", ".", "load", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "\"data/B30.npy\"", ")", ")", "\n", "\n", "# random sampling from the action space : $U[-1, 1)$", "\n", "self", ".", "action_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "1.", ",", "\n", "high", "=", "1.", ",", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "np", ".", "inf", ",", "\n", "high", "=", "np", ".", "inf", ",", "shape", "=", "(", "self", ".", "n", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "x", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr30d.LinearQuadraticRegulator30DEnv.step": [[29, 49], ["lqr30d.LinearQuadraticRegulator30DEnv.step.ftn"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "u", ")", ":", "\n", "        ", "h", "=", "self", ".", "dt", "\n", "# quadratic cost function x^T Q x + u^T R u", "\n", "costs", "=", "5.", "*", "np", ".", "sum", "(", "self", ".", "x", "**", "2", ")", "+", "np", ".", "sum", "(", "u", "**", "2", ")", "# Q = 10I, R = I", "\n", "\n", "def", "ftn", "(", "t", ",", "y", ")", ":", "\n", "# nested function which is determined by free variables A, B, and u", "\n", "# note that our dynamical system is time-homogeneous", "\n", "            ", "return", "self", ".", "A", "@", "y", "+", "self", ".", "B", "@", "u", "\n", "\n", "# Runge-Kutta 4-th order method", "\n", "", "k1", "=", "ftn", "(", ".0", ",", "self", ".", "x", ")", "\n", "k2", "=", "ftn", "(", ".0", "+", ".5", "*", "h", ",", "self", ".", "x", "+", ".5", "*", "h", "*", "k1", ")", "\n", "k3", "=", "ftn", "(", ".0", "+", ".5", "*", "h", ",", "self", ".", "x", "+", ".5", "*", "h", "*", "k2", ")", "\n", "k4", "=", "ftn", "(", ".0", "+", "h", ",", "self", ".", "x", "+", "h", "*", "k3", ")", "\n", "\n", "dx", "=", "h", "*", "(", "k1", "+", "2.", "*", "k2", "+", "2.", "*", "k3", "+", "k4", ")", "/", "6.", "\n", "self", ".", "x", "=", "self", ".", "x", "+", "dx", "# x(t + h) = x(t) + h dx", "\n", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", ",", "-", "costs", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr30d.LinearQuadraticRegulator30DEnv.reset": [[50, 54], ["numpy.copy", "numpy.random.rand"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# sample the initial state vector uniformly from $U[-1, 1)$", "\n", "        ", "self", ".", "x", "=", "2.", "*", "np", ".", "random", ".", "rand", "(", "self", ".", "n", ")", "-", "1.", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr30d.LinearQuadraticRegulator30DEnv.render": [[55, 57], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr30d.LinearQuadraticRegulator30DEnv.close": [[58, 60], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr30d.LinearQuadraticRegulator30DEnv.xnorm": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "xnorm", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "x", "**", "2", ")", ".", "sum", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.__init__": [[9, 25], ["gym.spaces.Box", "gym.spaces.Box"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# simulate LQR until $T = 10$", "\n", "        ", "self", ".", "dt", "=", ".05", "\n", "\n", "# random sampling from the action space : $U[-1, 1)$", "\n", "self", ".", "action_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "1.", ",", "\n", "high", "=", "1.", ",", "shape", "=", "(", "1", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "np", ".", "inf", ",", "\n", "high", "=", "np", ".", "inf", ",", "shape", "=", "(", "1", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "self", ".", "x", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.step": [[26, 35], ["numpy.sum", "numpy.copy", "numpy.sum"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "u", ")", ":", "\n", "        ", "h", "=", "self", ".", "dt", "\n", "# quadratic cost function x^T Q x + u^T R u", "\n", "costs", "=", "5.", "*", "np", ".", "sum", "(", "self", ".", "x", "**", "2", ")", "+", "np", ".", "sum", "(", "u", "**", "2", ")", "# Q = 10I, R = I", "\n", "\n", "dx", "=", "h", "*", "u", "\n", "self", ".", "x", "=", "self", ".", "x", "+", "dx", "# x(t + h) = x(t) + h dx", "\n", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", ",", "-", "costs", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset": [[36, 40], ["numpy.copy", "numpy.random.rand"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# sample the initial state vector uniformly from $U[-1, 1)$", "\n", "        ", "self", ".", "x", "=", "2.", "*", "np", ".", "random", ".", "rand", "(", ")", "-", "1.", "\n", "return", "np", ".", "copy", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.render": [[41, 43], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.close": [[44, 46], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.reset2one": [[47, 50], ["None"], "methods", ["None"], ["", "def", "reset2one", "(", "self", ")", ":", "\n", "        ", "self", ".", "x", "=", "1.", "\n", "return", "self", ".", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.HJDQN_HJQ.envs.lqr1d.LinearQuadraticRegulator1DEnv.xnorm": [[51, 54], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "xnorm", "(", "self", ")", ":", "\n", "        ", "return", "(", "(", "self", ".", "x", "**", "2", ")", ".", "sum", "(", ")", ")", "**", ".5", "\n", "", "", ""]]}