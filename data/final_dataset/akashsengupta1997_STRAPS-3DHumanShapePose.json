{"home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.None.run_predict.main": [[8, 20], ["models.regressor.SingleInputRegressor", "print", "models.regressor.SingleInputRegressor.to", "torch.load", "models.regressor.SingleInputRegressor.load_state_dict", "predict.predict_3D.predict_3D"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_3D.predict_3D"], ["def", "main", "(", "input_path", ",", "checkpoint_path", ",", "device", ",", "silhouettes_from", ")", ":", "\n", "    ", "regressor", "=", "SingleInputRegressor", "(", "resnet_in_channels", "=", "18", ",", "\n", "resnet_layers", "=", "18", ",", "\n", "ief_iters", "=", "3", ")", "\n", "\n", "print", "(", "\"Regressor loaded. Weights from:\"", ",", "checkpoint_path", ")", "\n", "regressor", ".", "to", "(", "device", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "device", ")", "\n", "regressor", ".", "load_state_dict", "(", "checkpoint", "[", "'best_model_state_dict'", "]", ")", "\n", "\n", "predict_3D", "(", "input_path", ",", "regressor", ",", "device", ",", "silhouettes_from", "=", "silhouettes_from", ",", "\n", "save_proxy_vis", "=", "True", ",", "render_vis", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_densepose_to_6part_lsp_labels": [[8, 37], ["numpy.zeros_like"], "function", ["None"], ["def", "convert_densepose_to_6part_lsp_labels", "(", "densepose_seg", ")", ":", "\n", "    ", "lsp_6part_seg", "=", "np", ".", "zeros_like", "(", "densepose_seg", ")", "\n", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "1", "]", "=", "6", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "2", "]", "=", "6", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "3", "]", "=", "2", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "4", "]", "=", "1", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "5", "]", "=", "4", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "6", "]", "=", "5", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "7", "]", "=", "5", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "8", "]", "=", "4", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "9", "]", "=", "5", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "10", "]", "=", "4", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "11", "]", "=", "5", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "12", "]", "=", "4", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "13", "]", "=", "5", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "14", "]", "=", "4", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "15", "]", "=", "1", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "16", "]", "=", "2", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "17", "]", "=", "1", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "18", "]", "=", "2", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "19", "]", "=", "1", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "20", "]", "=", "2", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "21", "]", "=", "1", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "22", "]", "=", "2", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "23", "]", "=", "3", "\n", "lsp_6part_seg", "[", "densepose_seg", "==", "24", "]", "=", "3", "\n", "\n", "return", "lsp_6part_seg", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_multiclass_to_binary_labels": [[39, 47], ["numpy.zeros_like"], "function", ["None"], ["", "def", "convert_multiclass_to_binary_labels", "(", "multiclass_labels", ")", ":", "\n", "    ", "\"\"\"\n    Converts multiclass segmentation labels into a binary mask.\n    \"\"\"", "\n", "binary_labels", "=", "np", ".", "zeros_like", "(", "multiclass_labels", ")", "\n", "binary_labels", "[", "multiclass_labels", "!=", "0", "]", "=", "1", "\n", "\n", "return", "binary_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_multiclass_to_binary_labels_torch": [[48, 56], ["torch.zeros_like"], "function", ["None"], ["", "def", "convert_multiclass_to_binary_labels_torch", "(", "multiclass_labels", ")", ":", "\n", "    ", "\"\"\"\n    Converts multiclass segmentation labels into a binary mask.\n    \"\"\"", "\n", "binary_labels", "=", "torch", ".", "zeros_like", "(", "multiclass_labels", ")", "\n", "binary_labels", "[", "multiclass_labels", "!=", "0", "]", "=", "1", "\n", "\n", "return", "binary_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps": [[58, 88], ["numpy.zeros", "range", "numpy.all", "numpy.all", "numpy.meshgrid", "numpy.sqrt", "numpy.exp", "max", "min", "max", "min", "max", "min", "max", "min", "numpy.linspace", "numpy.linspace"], "function", ["None"], ["", "def", "convert_2Djoints_to_gaussian_heatmaps", "(", "joints2D", ",", "img_wh", ",", "std", "=", "4", ")", ":", "\n", "    ", "\"\"\"\n    Converts 2D joints locations to img_wh x img_wh x num_joints gaussian heatmaps with given\n    standard deviation var.\n    \"\"\"", "\n", "num_joints", "=", "joints2D", ".", "shape", "[", "0", "]", "\n", "size", "=", "2", "*", "std", "# Truncate gaussian at 2 std from joint location.", "\n", "heatmaps", "=", "np", ".", "zeros", "(", "(", "img_wh", ",", "img_wh", ",", "num_joints", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "joints2D", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "if", "np", ".", "all", "(", "joints2D", "[", "i", "]", ">", "-", "size", ")", "and", "np", ".", "all", "(", "joints2D", "[", "i", "]", "<", "img_wh", "-", "1", "+", "size", ")", ":", "\n", "            ", "x", ",", "y", "=", "np", ".", "meshgrid", "(", "np", ".", "linspace", "(", "-", "size", ",", "size", ",", "2", "*", "size", ")", ",", "\n", "np", ".", "linspace", "(", "-", "size", ",", "size", ",", "2", "*", "size", ")", ")", "\n", "d", "=", "np", ".", "sqrt", "(", "x", "*", "x", "+", "y", "*", "y", ")", "\n", "gaussian", "=", "np", ".", "exp", "(", "-", "(", "d", "**", "2", "/", "(", "2.0", "*", "std", "**", "2", ")", ")", ")", "\n", "\n", "joint_centre", "=", "joints2D", "[", "i", "]", "\n", "hmap_start_x", "=", "max", "(", "0", ",", "joint_centre", "[", "0", "]", "-", "size", ")", "\n", "hmap_end_x", "=", "min", "(", "img_wh", "-", "1", ",", "joint_centre", "[", "0", "]", "+", "size", ")", "\n", "hmap_start_y", "=", "max", "(", "0", ",", "joint_centre", "[", "1", "]", "-", "size", ")", "\n", "hmap_end_y", "=", "min", "(", "img_wh", "-", "1", ",", "joint_centre", "[", "1", "]", "+", "size", ")", "\n", "\n", "g_start_x", "=", "max", "(", "0", ",", "size", "-", "joint_centre", "[", "0", "]", ")", "\n", "g_end_x", "=", "min", "(", "2", "*", "size", ",", "2", "*", "size", "-", "(", "size", "+", "joint_centre", "[", "0", "]", "-", "(", "img_wh", "-", "1", ")", ")", ")", "\n", "g_start_y", "=", "max", "(", "0", ",", "size", "-", "joint_centre", "[", "1", "]", ")", "\n", "g_end_y", "=", "min", "(", "2", "*", "size", ",", "2", "*", "size", "-", "(", "size", "+", "joint_centre", "[", "1", "]", "-", "(", "img_wh", "-", "1", ")", ")", ")", "\n", "\n", "heatmaps", "[", "hmap_start_y", ":", "hmap_end_y", ",", "\n", "hmap_start_x", ":", "hmap_end_x", ",", "i", "]", "=", "gaussian", "[", "g_start_y", ":", "g_end_y", ",", "g_start_x", ":", "g_end_x", "]", "\n", "\n", "", "", "return", "heatmaps", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps_torch": [[90, 128], ["joints2D.int", "torch.zeros().float", "torch.meshgrid", "x.to.to", "y.to.to", "torch.sqrt", "torch.exp", "range", "torch.linspace", "torch.linspace", "range", "torch.zeros", "torch.all", "torch.all", "max", "min", "max", "min", "max", "min", "max", "min", "joint_centre[].item", "joint_centre[].item", "joint_centre[].item", "joint_centre[].item", "joint_centre[].item", "joint_centre[].item", "joint_centre[].item", "joint_centre[].item"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["", "def", "convert_2Djoints_to_gaussian_heatmaps_torch", "(", "joints2D", ",", "img_wh", ",", "std", "=", "4", ")", ":", "\n", "    ", "\"\"\"\n    Converts 2D joints locations to img_wh x img_wh x num_joints gaussian heatmaps with given\n    standard deviation var.\n    :param joints2D: (B, N, 2) tensor - batch of 2D joints.\n    :return heatmaps: (B, N, img_wh, img_wh) - batch of 2D joint heatmaps.\n    \"\"\"", "\n", "joints2D_rounded", "=", "joints2D", ".", "int", "(", ")", "\n", "batch_size", "=", "joints2D_rounded", ".", "shape", "[", "0", "]", "\n", "num_joints", "=", "joints2D_rounded", ".", "shape", "[", "1", "]", "\n", "device", "=", "joints2D_rounded", ".", "device", "\n", "heatmaps", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "num_joints", ",", "img_wh", ",", "img_wh", ")", ",", "device", "=", "device", ")", ".", "float", "(", ")", "\n", "\n", "size", "=", "2", "*", "std", "# Truncate gaussian at 2 std from joint location.", "\n", "x", ",", "y", "=", "torch", ".", "meshgrid", "(", "torch", ".", "linspace", "(", "-", "size", ",", "size", ",", "2", "*", "size", ")", ",", "\n", "torch", ".", "linspace", "(", "-", "size", ",", "size", ",", "2", "*", "size", ")", ")", "\n", "x", "=", "x", ".", "to", "(", "device", ")", "\n", "y", "=", "y", ".", "to", "(", "device", ")", "\n", "d", "=", "torch", ".", "sqrt", "(", "x", "*", "x", "+", "y", "*", "y", ")", "\n", "gaussian", "=", "torch", ".", "exp", "(", "-", "(", "d", "**", "2", "/", "(", "2.0", "*", "std", "**", "2", ")", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_joints", ")", ":", "\n", "            ", "if", "torch", ".", "all", "(", "joints2D_rounded", "[", "i", ",", "j", "]", ">", "-", "size", ")", "and", "torch", ".", "all", "(", "joints2D_rounded", "[", "i", ",", "j", "]", "<", "img_wh", "-", "1", "+", "size", ")", ":", "\n", "                ", "joint_centre", "=", "joints2D_rounded", "[", "i", ",", "j", "]", "\n", "hmap_start_x", "=", "max", "(", "0", ",", "joint_centre", "[", "0", "]", ".", "item", "(", ")", "-", "size", ")", "\n", "hmap_end_x", "=", "min", "(", "img_wh", "-", "1", ",", "joint_centre", "[", "0", "]", ".", "item", "(", ")", "+", "size", ")", "\n", "hmap_start_y", "=", "max", "(", "0", ",", "joint_centre", "[", "1", "]", ".", "item", "(", ")", "-", "size", ")", "\n", "hmap_end_y", "=", "min", "(", "img_wh", "-", "1", ",", "joint_centre", "[", "1", "]", ".", "item", "(", ")", "+", "size", ")", "\n", "\n", "g_start_x", "=", "max", "(", "0", ",", "size", "-", "joint_centre", "[", "0", "]", ".", "item", "(", ")", ")", "\n", "g_end_x", "=", "min", "(", "2", "*", "size", ",", "2", "*", "size", "-", "(", "size", "+", "joint_centre", "[", "0", "]", ".", "item", "(", ")", "-", "(", "img_wh", "-", "1", ")", ")", ")", "\n", "g_start_y", "=", "max", "(", "0", ",", "size", "-", "joint_centre", "[", "1", "]", ".", "item", "(", ")", ")", "\n", "g_end_y", "=", "min", "(", "2", "*", "size", ",", "2", "*", "size", "-", "(", "size", "+", "joint_centre", "[", "1", "]", ".", "item", "(", ")", "-", "(", "img_wh", "-", "1", ")", ")", ")", "\n", "\n", "heatmaps", "[", "i", ",", "j", ",", "hmap_start_y", ":", "hmap_end_y", ",", "hmap_start_x", ":", "hmap_end_x", "]", "=", "gaussian", "[", "g_start_y", ":", "g_end_y", ",", "g_start_x", ":", "g_end_x", "]", "\n", "\n", "", "", "", "return", "heatmaps", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.model_utils.count_parameters": [[3, 5], ["sum", "p.numel", "model.parameters"], "function", ["None"], ["def", "count_parameters", "(", "model", ")", ":", "\n", "    ", "return", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.pad_to_square": [[6, 22], ["cv2.copyMakeBorder", "cv2.copyMakeBorder"], "function", ["None"], ["def", "pad_to_square", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Pad image to square shape.\n    \"\"\"", "\n", "height", ",", "width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "if", "width", "<", "height", ":", "\n", "        ", "border_width", "=", "(", "height", "-", "width", ")", "//", "2", "\n", "image", "=", "cv2", ".", "copyMakeBorder", "(", "image", ",", "0", ",", "0", ",", "border_width", ",", "border_width", ",", "\n", "cv2", ".", "BORDER_CONSTANT", ",", "value", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "border_width", "=", "(", "width", "-", "height", ")", "//", "2", "\n", "image", "=", "cv2", ".", "copyMakeBorder", "(", "image", ",", "border_width", ",", "border_width", ",", "0", ",", "0", ",", "\n", "cv2", ".", "BORDER_CONSTANT", ",", "value", "=", "0", ")", "\n", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.convert_bbox_corners_to_centre_hw": [[23, 33], ["numpy.array"], "function", ["None"], ["", "def", "convert_bbox_corners_to_centre_hw", "(", "bbox_corners", ")", ":", "\n", "    ", "\"\"\"\n    Converst bbox coordinates from x1, y1, x2, y2 to centre, height, width.\n    \"\"\"", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "bbox_corners", "\n", "centre", "=", "np", ".", "array", "(", "[", "(", "x1", "+", "x2", ")", "/", "2.0", ",", "(", "y1", "+", "y2", ")", "/", "2.0", "]", ")", "\n", "height", "=", "x2", "-", "x1", "\n", "width", "=", "y2", "-", "y1", "\n", "\n", "return", "centre", ",", "height", ",", "width", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.convert_bbox_centre_hw_to_corners": [[35, 42], ["numpy.array"], "function", ["None"], ["", "def", "convert_bbox_centre_hw_to_corners", "(", "centre", ",", "height", ",", "width", ")", ":", "\n", "    ", "x1", "=", "centre", "[", "0", "]", "-", "height", "/", "2.0", "\n", "x2", "=", "centre", "[", "0", "]", "+", "height", "/", "2.0", "\n", "y1", "=", "centre", "[", "1", "]", "-", "width", "/", "2.0", "\n", "y2", "=", "centre", "[", "1", "]", "+", "width", "/", "2.0", "\n", "\n", "return", "np", ".", "array", "(", "[", "x1", ",", "y1", ",", "x2", ",", "y2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.batch_crop_seg_to_bounding_box": [[44, 83], ["range", "numpy.argwhere", "numpy.concatenate", "image_utils.convert_bbox_corners_to_centre_hw", "image_utils.convert_bbox_centre_hw_to_corners", "bbox_corners[].astype", "bbox_corners[].astype", "all_cropped_joints2D.append", "all_cropped_segs.append", "numpy.amin", "numpy.amax", "max", "numpy.random.rand", "numpy.random.rand"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.convert_bbox_corners_to_centre_hw", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.convert_bbox_centre_hw_to_corners"], ["", "def", "batch_crop_seg_to_bounding_box", "(", "seg", ",", "joints2D", ",", "orig_scale_factor", "=", "1.2", ",", "delta_scale_range", "=", "None", ",", "delta_centre_range", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    seg: (bs, wh, wh)\n    joints2D: (bs, num joints, 2)\n    scale: bbox expansion scale\n    \"\"\"", "\n", "all_cropped_segs", "=", "[", "]", "\n", "all_cropped_joints2D", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "seg", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "body_pixels", "=", "np", ".", "argwhere", "(", "seg", "[", "i", "]", "!=", "0", ")", "\n", "bbox_corners", "=", "np", ".", "amin", "(", "body_pixels", ",", "axis", "=", "0", ")", ",", "np", ".", "amax", "(", "body_pixels", ",", "axis", "=", "0", ")", "\n", "bbox_corners", "=", "np", ".", "concatenate", "(", "bbox_corners", ")", "\n", "centre", ",", "height", ",", "width", "=", "convert_bbox_corners_to_centre_hw", "(", "bbox_corners", ")", "\n", "if", "delta_scale_range", "is", "not", "None", ":", "\n", "            ", "l", ",", "h", "=", "delta_scale_range", "\n", "delta_scale", "=", "(", "h", "-", "l", ")", "*", "np", ".", "random", ".", "rand", "(", ")", "+", "l", "\n", "scale_factor", "=", "orig_scale_factor", "+", "delta_scale", "\n", "", "else", ":", "\n", "            ", "scale_factor", "=", "orig_scale_factor", "\n", "\n", "", "if", "delta_centre_range", "is", "not", "None", ":", "\n", "            ", "l", ",", "h", "=", "delta_centre_range", "\n", "delta_centre", "=", "(", "h", "-", "l", ")", "*", "np", ".", "random", ".", "rand", "(", "2", ")", "+", "l", "\n", "centre", "=", "centre", "+", "delta_centre", "\n", "\n", "", "wh", "=", "max", "(", "height", ",", "width", ")", "*", "scale_factor", "\n", "\n", "bbox_corners", "=", "convert_bbox_centre_hw_to_corners", "(", "centre", ",", "wh", ",", "wh", ")", "\n", "\n", "top_left", "=", "bbox_corners", "[", ":", "2", "]", ".", "astype", "(", "np", ".", "int16", ")", "\n", "bottom_right", "=", "bbox_corners", "[", "2", ":", "]", ".", "astype", "(", "np", ".", "int16", ")", "\n", "top_left", "[", "top_left", "<", "0", "]", "=", "0", "\n", "bottom_right", "[", "bottom_right", "<", "0", "]", "=", "0", "\n", "\n", "cropped_joints2d", "=", "joints2D", "[", "i", "]", "-", "top_left", "[", ":", ":", "-", "1", "]", "\n", "cropped_seg", "=", "seg", "[", "i", ",", "top_left", "[", "0", "]", ":", "bottom_right", "[", "0", "]", ",", "top_left", "[", "1", "]", ":", "bottom_right", "[", "1", "]", "]", "\n", "all_cropped_joints2D", ".", "append", "(", "cropped_joints2d", ")", "\n", "all_cropped_segs", ".", "append", "(", "cropped_seg", ")", "\n", "", "return", "all_cropped_segs", ",", "all_cropped_joints2D", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.batch_resize": [[85, 106], ["range", "numpy.stack", "numpy.stack", "len", "cv2.resize", "np.stack.append", "np.stack.append", "numpy.array", "float", "float"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.resize"], ["", "def", "batch_resize", "(", "all_cropped_segs", ",", "all_cropped_joints2D", ",", "img_wh", ")", ":", "\n", "    ", "\"\"\"\n    all_cropped_seg: list of cropped segs with len = batch size\n    \"\"\"", "\n", "all_resized_segs", "=", "[", "]", "\n", "all_resized_joints2D", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "all_cropped_segs", ")", ")", ":", "\n", "        ", "seg", "=", "all_cropped_segs", "[", "i", "]", "\n", "orig_height", ",", "orig_width", "=", "seg", ".", "shape", "[", ":", "2", "]", "\n", "resized_seg", "=", "cv2", ".", "resize", "(", "seg", ",", "(", "img_wh", ",", "img_wh", ")", ",", "interpolation", "=", "cv2", ".", "INTER_NEAREST", ")", "\n", "all_resized_segs", ".", "append", "(", "resized_seg", ")", "\n", "\n", "joints2D", "=", "all_cropped_joints2D", "[", "i", "]", "\n", "resized_joints2D", "=", "joints2D", "*", "np", ".", "array", "(", "[", "img_wh", "/", "float", "(", "orig_width", ")", ",", "\n", "img_wh", "/", "float", "(", "orig_height", ")", "]", ")", "\n", "all_resized_joints2D", ".", "append", "(", "resized_joints2D", ")", "\n", "\n", "", "all_resized_segs", "=", "np", ".", "stack", "(", "all_resized_segs", ",", "axis", "=", "0", ")", "\n", "all_resized_joints2D", "=", "np", ".", "stack", "(", "all_resized_joints2D", ",", "axis", "=", "0", ")", "\n", "\n", "return", "all_resized_segs", ",", "all_resized_joints2D", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.crop_and_resize_silhouette_joints": [[108, 164], ["numpy.argwhere", "image_utils.convert_bbox_corners_to_centre_hw", "image_utils.convert_bbox_centre_hw_to_corners", "bbox_corners[].astype", "bbox_corners[].astype", "bbox_corners[].astype.copy", "bbox_corners[].astype.copy", "cv2.copyMakeBorder", "cv2.resize", "numpy.concatenate", "max", "numpy.array", "cv2.copyMakeBorder", "cv2.resize", "max", "max", "max", "max", "numpy.amin", "numpy.amax", "max", "max", "max", "max", "float", "float"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.convert_bbox_corners_to_centre_hw", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.convert_bbox_centre_hw_to_corners", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.resize", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.resize"], ["", "def", "crop_and_resize_silhouette_joints", "(", "silhouette", ",", "\n", "joints2D", ",", "\n", "out_wh", ",", "\n", "image", "=", "None", ",", "\n", "image_out_wh", "=", "None", ",", "\n", "bbox_scale_factor", "=", "1.2", ")", ":", "\n", "# Find bounding box around silhouette", "\n", "    ", "body_pixels", "=", "np", ".", "argwhere", "(", "silhouette", "!=", "0", ")", "\n", "bbox_centre", ",", "height", ",", "width", "=", "convert_bbox_corners_to_centre_hw", "(", "np", ".", "concatenate", "(", "[", "np", ".", "amin", "(", "body_pixels", ",", "axis", "=", "0", ")", ",", "\n", "np", ".", "amax", "(", "body_pixels", ",", "axis", "=", "0", ")", "]", ")", ")", "\n", "wh", "=", "max", "(", "height", ",", "width", ")", "*", "bbox_scale_factor", "# Make bounding box square with sides = wh", "\n", "bbox_corners", "=", "convert_bbox_centre_hw_to_corners", "(", "bbox_centre", ",", "wh", ",", "wh", ")", "\n", "top_left", "=", "bbox_corners", "[", ":", "2", "]", ".", "astype", "(", "np", ".", "int16", ")", "\n", "bottom_right", "=", "bbox_corners", "[", "2", ":", "]", ".", "astype", "(", "np", ".", "int16", ")", "\n", "top_left_orig", "=", "top_left", ".", "copy", "(", ")", "\n", "bottom_right_orig", "=", "bottom_right", ".", "copy", "(", ")", "\n", "top_left", "[", "top_left", "<", "0", "]", "=", "0", "\n", "bottom_right", "[", "bottom_right", "<", "0", "]", "=", "0", "\n", "# Crop silhouette", "\n", "orig_height", ",", "orig_width", "=", "silhouette", ".", "shape", "[", ":", "2", "]", "\n", "silhouette", "=", "silhouette", "[", "top_left", "[", "0", "]", ":", "bottom_right", "[", "0", "]", ",", "top_left", "[", "1", "]", ":", "bottom_right", "[", "1", "]", "]", "\n", "# Pad silhouette if crop not square", "\n", "silhouette", "=", "cv2", ".", "copyMakeBorder", "(", "src", "=", "silhouette", ",", "\n", "top", "=", "max", "(", "0", ",", "-", "top_left_orig", "[", "0", "]", ")", ",", "\n", "bottom", "=", "max", "(", "0", ",", "bottom_right_orig", "[", "0", "]", "-", "orig_height", ")", ",", "\n", "left", "=", "max", "(", "0", ",", "-", "top_left_orig", "[", "1", "]", ")", ",", "\n", "right", "=", "max", "(", "0", ",", "bottom_right_orig", "[", "1", "]", "-", "orig_width", ")", ",", "\n", "borderType", "=", "cv2", ".", "BORDER_CONSTANT", ",", "\n", "value", "=", "0", ")", "\n", "crop_height", ",", "crop_width", "=", "silhouette", ".", "shape", "[", ":", "2", "]", "\n", "# Resize silhouette", "\n", "silhouette", "=", "cv2", ".", "resize", "(", "silhouette", ",", "(", "out_wh", ",", "out_wh", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_NEAREST", ")", "\n", "\n", "# Translate and resize joints2D", "\n", "joints2D", "=", "joints2D", "[", ":", ",", ":", "2", "]", "-", "top_left_orig", "[", ":", ":", "-", "1", "]", "\n", "joints2D", "=", "joints2D", "*", "np", ".", "array", "(", "[", "out_wh", "/", "float", "(", "crop_width", ")", ",", "\n", "out_wh", "/", "float", "(", "crop_height", ")", "]", ")", "\n", "\n", "if", "image", "is", "not", "None", ":", "\n", "# Crop image", "\n", "        ", "orig_height", ",", "orig_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "image", "=", "image", "[", "top_left", "[", "0", "]", ":", "bottom_right", "[", "0", "]", ",", "top_left", "[", "1", "]", ":", "bottom_right", "[", "1", "]", "]", "\n", "# Pad image if crop not square", "\n", "image", "=", "cv2", ".", "copyMakeBorder", "(", "src", "=", "image", ",", "\n", "top", "=", "max", "(", "0", ",", "-", "top_left_orig", "[", "0", "]", ")", ",", "\n", "bottom", "=", "max", "(", "0", ",", "bottom_right_orig", "[", "0", "]", "-", "orig_height", ")", ",", "\n", "left", "=", "max", "(", "0", ",", "-", "top_left_orig", "[", "1", "]", ")", ",", "\n", "right", "=", "max", "(", "0", ",", "bottom_right_orig", "[", "1", "]", "-", "orig_width", ")", ",", "\n", "borderType", "=", "cv2", ".", "BORDER_CONSTANT", ",", "\n", "value", "=", "0", ")", "\n", "# Resize silhouette", "\n", "image", "=", "cv2", ".", "resize", "(", "image", ",", "(", "image_out_wh", ",", "image_out_wh", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_LINEAR", ")", "\n", "\n", "", "return", "silhouette", ",", "joints2D", ",", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.orthographic_project_torch": [[5, 27], ["torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.stack"], "function", ["None"], ["def", "orthographic_project_torch", "(", "points3D", ",", "cam_params", ")", ":", "\n", "    ", "\"\"\"\n    Scaled orthographic projection (i.e. weak perspective projection).\n    Should be going from SMPL 3D coords to [-1, 1] scaled image coords.\n    cam_params are [s, tx, ty]  - i.e. scaling and 2D translation.\n    \"\"\"", "\n", "x", "=", "points3D", "[", ":", ",", ":", ",", "0", "]", "\n", "y", "=", "points3D", "[", ":", ",", ":", ",", "1", "]", "\n", "\n", "# Scaling", "\n", "s", "=", "torch", ".", "unsqueeze", "(", "cam_params", "[", ":", ",", "0", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# Translation", "\n", "t_x", "=", "torch", ".", "unsqueeze", "(", "cam_params", "[", ":", ",", "1", "]", ",", "dim", "=", "1", ")", "\n", "t_y", "=", "torch", ".", "unsqueeze", "(", "cam_params", "[", ":", ",", "2", "]", ",", "dim", "=", "1", ")", "\n", "\n", "u", "=", "s", "*", "(", "x", "+", "t_x", ")", "\n", "v", "=", "s", "*", "(", "y", "+", "t_y", ")", "\n", "\n", "proj_points", "=", "torch", ".", "stack", "(", "[", "u", ",", "v", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "proj_points", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.get_intrinsics_matrix": [[29, 38], ["numpy.array"], "function", ["None"], ["", "def", "get_intrinsics_matrix", "(", "img_width", ",", "img_height", ",", "focal_length", ")", ":", "\n", "    ", "\"\"\"\n    Camera intrinsic matrix (calibration matrix) given focal length and img_width and\n    img_height. Assumes that principal point is at (width/2, height/2).\n    \"\"\"", "\n", "K", "=", "np", ".", "array", "(", "[", "[", "focal_length", ",", "0.", ",", "img_width", "/", "2.0", "]", ",", "\n", "[", "0.", ",", "focal_length", ",", "img_height", "/", "2.0", "]", ",", "\n", "[", "0.", ",", "0.", ",", "1.", "]", "]", ")", "\n", "return", "K", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.perspective_project_torch": [[40, 72], ["torch.einsum", "torch.einsum", "torch.from_numpy", "torch.cat", "cam_K.to.to", "translation.unsqueeze", "points[].unsqueeze", "get_intrinsics_matrix().astype", "cam_utils.get_intrinsics_matrix"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.get_intrinsics_matrix"], ["", "def", "perspective_project_torch", "(", "points", ",", "rotation", ",", "translation", ",", "cam_K", "=", "None", ",", "\n", "focal_length", "=", "None", ",", "img_wh", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    This function computes the perspective projection of a set of points in torch.\n    Input:\n        points (bs, N, 3): 3D points\n        rotation (bs, 3, 3): Camera rotation\n        translation (bs, 3): Camera translation\n        Either\n        cam_K (bs, 3, 3): Camera intrinsics matrix\n        Or\n        focal_length (bs,) or scalar: Focal length\n        camera_center (bs, 2): Camera center\n    \"\"\"", "\n", "batch_size", "=", "points", ".", "shape", "[", "0", "]", "\n", "if", "cam_K", "is", "None", ":", "\n", "        ", "cam_K", "=", "torch", ".", "from_numpy", "(", "get_intrinsics_matrix", "(", "img_wh", ",", "img_wh", ",", "focal_length", ")", ".", "astype", "(", "\n", "np", ".", "float32", ")", ")", "\n", "cam_K", "=", "torch", ".", "cat", "(", "batch_size", "*", "[", "cam_K", "[", "None", ",", ":", ",", ":", "]", "]", ",", "dim", "=", "0", ")", "\n", "cam_K", "=", "cam_K", ".", "to", "(", "points", ".", "device", ")", "\n", "\n", "# Transform points", "\n", "", "points", "=", "torch", ".", "einsum", "(", "'bij,bkj->bki'", ",", "rotation", ",", "points", ")", "\n", "points", "=", "points", "+", "translation", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Apply perspective distortion", "\n", "projected_points", "=", "points", "/", "points", "[", ":", ",", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# Apply camera intrinsics", "\n", "projected_points", "=", "torch", ".", "einsum", "(", "'bij,bkj->bki'", ",", "cam_K", ",", "projected_points", ")", "\n", "\n", "return", "projected_points", "[", ":", ",", ":", ",", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.convert_weak_perspective_to_camera_translation": [[74, 77], ["numpy.array", "numpy.np.float32"], "function", ["None"], ["", "def", "convert_weak_perspective_to_camera_translation", "(", "cam_wp", ",", "focal_length", ",", "resolution", ")", ":", "\n", "    ", "cam_t", "=", "np", ".", "array", "(", "[", "cam_wp", "[", "1", "]", ",", "cam_wp", "[", "2", "]", ",", "2", "*", "focal_length", "/", "(", "resolution", "*", "cam_wp", "[", "0", "]", "+", "1e-9", ")", "]", ")", "\n", "return", "cam_t", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.batch_convert_weak_perspective_to_camera_translation": [[79, 88], ["numpy.zeros", "range", "cam_utils.convert_weak_perspective_to_camera_translation", "convert_weak_perspective_to_camera_translation.astype"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.convert_weak_perspective_to_camera_translation"], ["", "def", "batch_convert_weak_perspective_to_camera_translation", "(", "wp_cams", ",", "focal_length", ",", "resolution", ")", ":", "\n", "    ", "num", "=", "wp_cams", ".", "shape", "[", "0", "]", "\n", "cam_ts", "=", "np", ".", "zeros", "(", "(", "num", ",", "3", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "num", ")", ":", "\n", "        ", "cam_t", "=", "convert_weak_perspective_to_camera_translation", "(", "wp_cams", "[", "i", "]", ",", "\n", "focal_length", ",", "\n", "resolution", ")", "\n", "cam_ts", "[", "i", "]", "=", "cam_t", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "cam_ts", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.undo_keypoint_normalisation": [[5, 11], ["None"], "function", ["None"], ["def", "undo_keypoint_normalisation", "(", "normalised_keypoints", ",", "img_wh", ")", ":", "\n", "    ", "\"\"\"\n    Converts normalised keypoints from [-1, 1] space to pixel space i.e. [0, img_wh]\n    \"\"\"", "\n", "keypoints", "=", "(", "normalised_keypoints", "+", "1", ")", "*", "(", "img_wh", "/", "2.0", ")", "\n", "return", "keypoints", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.check_joints2d_visibility": [[13, 21], ["numpy.ones"], "function", ["None"], ["", "def", "check_joints2d_visibility", "(", "joints2d", ",", "img_wh", ")", ":", "\n", "    ", "vis", "=", "np", ".", "ones", "(", "joints2d", ".", "shape", "[", "1", "]", ")", "\n", "vis", "[", "joints2d", "[", "0", "]", ">", "img_wh", "]", "=", "0", "\n", "vis", "[", "joints2d", "[", "1", "]", ">", "img_wh", "]", "=", "0", "\n", "vis", "[", "joints2d", "[", "0", "]", "<", "0", "]", "=", "0", "\n", "vis", "[", "joints2d", "[", "1", "]", "<", "0", "]", "=", "0", "\n", "\n", "return", "vis", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.check_joints2d_visibility_torch": [[23, 34], ["torch.ones"], "function", ["None"], ["", "def", "check_joints2d_visibility_torch", "(", "joints2d", ",", "img_wh", ")", ":", "\n", "    ", "\"\"\"\n    Checks if 2D joints are within the image dimensions.\n    \"\"\"", "\n", "vis", "=", "torch", ".", "ones", "(", "joints2d", ".", "shape", "[", ":", "2", "]", ",", "device", "=", "joints2d", ".", "device", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "vis", "[", "joints2d", "[", ":", ",", ":", ",", "0", "]", ">", "img_wh", "]", "=", "0", "\n", "vis", "[", "joints2d", "[", ":", ",", ":", ",", "1", "]", ">", "img_wh", "]", "=", "0", "\n", "vis", "[", "joints2d", "[", ":", ",", ":", ",", "0", "]", "<", "0", "]", "=", "0", "\n", "vis", "[", "joints2d", "[", ":", ",", ":", ",", "1", "]", "<", "0", "]", "=", "0", "\n", "\n", "return", "vis", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.rigid_transform_utils.rotate_translate_verts_torch": [[7, 25], ["torch.from_numpy().to", "torch.from_numpy().to", "torch.einsum", "cv2.Rodrigues", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().to.astype", "torch.from_numpy().to.astype"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["def", "rotate_translate_verts_torch", "(", "vertices", ",", "axis", ",", "angle", ",", "trans", ")", ":", "\n", "    ", "\"\"\"\n    Rotates and translates batch of vertices.\n    :param vertices: B, N, 3\n    :param axis: 3,\n    :param angle: angle in radians\n    :param trans: 3,\n    :return:\n    \"\"\"", "\n", "r", "=", "angle", "*", "axis", "\n", "R", "=", "cv2", ".", "Rodrigues", "(", "r", ")", "[", "0", "]", "\n", "R", "=", "torch", ".", "from_numpy", "(", "R", ".", "astype", "(", "np", ".", "float32", ")", ")", ".", "to", "(", "vertices", ".", "device", ")", "\n", "trans", "=", "torch", ".", "from_numpy", "(", "trans", ".", "astype", "(", "np", ".", "float32", ")", ")", ".", "to", "(", "vertices", ".", "device", ")", "\n", "\n", "vertices", "=", "torch", ".", "einsum", "(", "'ij,bkj->bki'", ",", "R", ",", "vertices", ")", "\n", "vertices", "=", "vertices", "+", "trans", "\n", "\n", "return", "vertices", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.rigid_transform_utils.rot6d_to_rotmat": [[27, 42], ["x.view.view", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.cross", "torch.stack", "torch.einsum().unsqueeze", "torch.einsum"], "function", ["None"], ["", "def", "rot6d_to_rotmat", "(", "x", ")", ":", "\n", "    ", "\"\"\"Convert 6D rotation representation to 3x3 rotation matrix.\n    Based on Zhou et al., \"On the Continuity of Rotation Representations in Neural Networks\", CVPR 2019\n    Input:\n        (B,6) Batch of 6-D rotation representations\n    Output:\n        (B,3,3) Batch of corresponding rotation matrices\n    \"\"\"", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "3", ",", "2", ")", "\n", "a1", "=", "x", "[", ":", ",", ":", ",", "0", "]", "\n", "a2", "=", "x", "[", ":", ",", ":", ",", "1", "]", "\n", "b1", "=", "F", ".", "normalize", "(", "a1", ")", "# Ensuring columns are unit vectors", "\n", "b2", "=", "F", ".", "normalize", "(", "a2", "-", "torch", ".", "einsum", "(", "'bi,bi->b'", ",", "b1", ",", "a2", ")", ".", "unsqueeze", "(", "-", "1", ")", "*", "b1", ")", "# Ensuring column 1 and column 2 are orthogonal", "\n", "b3", "=", "torch", ".", "cross", "(", "b1", ",", "b2", ")", "\n", "return", "torch", ".", "stack", "(", "(", "b1", ",", "b2", ",", "b3", ")", ",", "dim", "=", "-", "1", ")", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.compute_similarity_transform": [[7, 56], ["S1.mean", "S2.mean", "numpy.sum", "X1.dot", "numpy.linalg.svd", "numpy.eye", "numpy.sign", "V.dot", "numpy.linalg.det", "np.eye.dot", "numpy.trace", "U.dot", "V.dot.dot", "V.dot.dot", "V.dot.dot"], "function", ["None"], ["def", "compute_similarity_transform", "(", "S1", ",", "S2", ")", ":", "\n", "    ", "\"\"\"\n    Computes a similarity transform (sR, t) that takes\n    a set of 3D points S1 (3 x N) closest to a set of 3D points S2,\n    where R is an 3x3 rotation matrix, t 3x1 translation, s scale.\n    i.e. solves the orthogonal Procrutes problem.\n    \"\"\"", "\n", "transposed", "=", "False", "\n", "if", "S1", ".", "shape", "[", "0", "]", "!=", "3", "and", "S1", ".", "shape", "[", "0", "]", "!=", "2", ":", "\n", "        ", "S1", "=", "S1", ".", "T", "\n", "S2", "=", "S2", ".", "T", "\n", "transposed", "=", "True", "\n", "", "assert", "(", "S2", ".", "shape", "[", "1", "]", "==", "S1", ".", "shape", "[", "1", "]", ")", "\n", "\n", "# 1. Remove mean.", "\n", "mu1", "=", "S1", ".", "mean", "(", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "mu2", "=", "S2", ".", "mean", "(", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "X1", "=", "S1", "-", "mu1", "\n", "X2", "=", "S2", "-", "mu2", "\n", "\n", "# 2. Compute variance of X1 used for scale.", "\n", "var1", "=", "np", ".", "sum", "(", "X1", "**", "2", ")", "\n", "\n", "# 3. The outer product of X1 and X2.", "\n", "K", "=", "X1", ".", "dot", "(", "X2", ".", "T", ")", "\n", "\n", "# 4. Solution that Maximizes trace(R'K) is R=U*V', where U, V are", "\n", "# singular vectors of K.", "\n", "U", ",", "s", ",", "Vh", "=", "np", ".", "linalg", ".", "svd", "(", "K", ")", "\n", "V", "=", "Vh", ".", "T", "\n", "# Construct Z that fixes the orientation of R to get det(R)=1.", "\n", "Z", "=", "np", ".", "eye", "(", "U", ".", "shape", "[", "0", "]", ")", "\n", "Z", "[", "-", "1", ",", "-", "1", "]", "*=", "np", ".", "sign", "(", "np", ".", "linalg", ".", "det", "(", "U", ".", "dot", "(", "V", ".", "T", ")", ")", ")", "\n", "# Construct R.", "\n", "R", "=", "V", ".", "dot", "(", "Z", ".", "dot", "(", "U", ".", "T", ")", ")", "\n", "\n", "# 5. Recover scale.", "\n", "scale", "=", "np", ".", "trace", "(", "R", ".", "dot", "(", "K", ")", ")", "/", "var1", "\n", "\n", "# 6. Recover translation.", "\n", "t", "=", "mu2", "-", "scale", "*", "(", "R", ".", "dot", "(", "mu1", ")", ")", "\n", "\n", "# 7. Error:", "\n", "S1_hat", "=", "scale", "*", "R", ".", "dot", "(", "S1", ")", "+", "t", "\n", "\n", "if", "transposed", ":", "\n", "        ", "S1_hat", "=", "S1_hat", ".", "T", "\n", "\n", "", "return", "S1_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch": [[58, 64], ["numpy.zeros_like", "range", "eval_utils.compute_similarity_transform"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.compute_similarity_transform"], ["", "def", "procrustes_analysis_batch", "(", "S1", ",", "S2", ")", ":", "\n", "    ", "\"\"\"Batched version of compute_similarity_transform.\"\"\"", "\n", "S1_hat", "=", "np", ".", "zeros_like", "(", "S1", ")", "\n", "for", "i", "in", "range", "(", "S1", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "S1_hat", "[", "i", "]", "=", "compute_similarity_transform", "(", "S1", "[", "i", "]", ",", "S2", "[", "i", "]", ")", "\n", "", "return", "S1_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch": [[66, 86], ["numpy.mean", "numpy.sqrt", "numpy.mean", "numpy.sqrt", "numpy.sum", "numpy.sum"], "function", ["None"], ["", "def", "scale_and_translation_transform_batch", "(", "P", ",", "T", ")", ":", "\n", "    ", "\"\"\"\n    First Normalises batch of input 3D meshes P such that each mesh has mean (0, 0, 0) and\n    RMS distance from mean = 1.\n    Then transforms P such that it has the same mean and RMSD as T.\n    :param P: (batch_size, N, 3) batch of N 3D meshes to transform.\n    :param T: (batch_size, N, 3) batch of N reference 3D meshes.\n    :return: P transformed\n    \"\"\"", "\n", "P_mean", "=", "np", ".", "mean", "(", "P", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "P_trans", "=", "P", "-", "P_mean", "\n", "P_scale", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "P_trans", "**", "2", ",", "axis", "=", "(", "1", ",", "2", ")", ",", "keepdims", "=", "True", ")", "/", "P", ".", "shape", "[", "1", "]", ")", "\n", "P_normalised", "=", "P_trans", "/", "P_scale", "\n", "\n", "T_mean", "=", "np", ".", "mean", "(", "T", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "T_scale", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "(", "T", "-", "T_mean", ")", "**", "2", ",", "axis", "=", "(", "1", ",", "2", ")", ",", "keepdims", "=", "True", ")", "/", "T", ".", "shape", "[", "1", "]", ")", "\n", "\n", "P_transformed", "=", "P_normalised", "*", "T_scale", "+", "T_mean", "\n", "\n", "return", "P_transformed", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.checkpoint_utils.load_training_info_from_checkpoint": [[4, 27], ["print", "print", "print", "best_epoch_val_metrics.keys", "best_epoch_val_metrics.keys"], "function", ["None"], ["def", "load_training_info_from_checkpoint", "(", "checkpoint", ",", "save_val_metrics", ")", ":", "\n", "    ", "current_epoch", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "best_epoch", "=", "checkpoint", "[", "'best_epoch'", "]", "\n", "best_model_wts", "=", "checkpoint", "[", "'best_model_state_dict'", "]", "\n", "best_epoch_val_metrics", "=", "checkpoint", "[", "'best_epoch_val_metrics'", "]", "\n", "# ^ best val metrics, happened at best_epoch", "\n", "\n", "# If different save_val_metrics used upon re-starting training, set best values for those", "\n", "# metrics to infinity.", "\n", "for", "metric", "in", "save_val_metrics", ":", "\n", "        ", "if", "metric", "not", "in", "best_epoch_val_metrics", ".", "keys", "(", ")", ":", "\n", "            ", "best_epoch_val_metrics", "[", "metric", "]", "=", "np", ".", "inf", "\n", "", "", "metrics_to_del", "=", "[", "metric", "for", "metric", "in", "best_epoch_val_metrics", ".", "keys", "(", ")", "if", "\n", "metric", "not", "in", "save_val_metrics", "]", "\n", "for", "metric", "in", "metrics_to_del", ":", "\n", "        ", "del", "best_epoch_val_metrics", "[", "metric", "]", "\n", "\n", "", "print", "(", "'\\nTraining information loaded from checkpoint.'", ")", "\n", "print", "(", "'Current epoch:'", ",", "current_epoch", ")", "\n", "print", "(", "'Best epoch val metrics from last training run:'", ",", "best_epoch_val_metrics", ",", "\n", "' - achieved in epoch:'", ",", "best_epoch", ")", "\n", "\n", "return", "current_epoch", ",", "best_epoch", ",", "best_model_wts", ",", "best_epoch_val_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.WeakPerspectiveCamera.__init__": [[14, 27], ["pyrender.Camera.__init__"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "scale", ",", "\n", "translation", ",", "\n", "znear", "=", "pyrender", ".", "camera", ".", "DEFAULT_Z_NEAR", ",", "\n", "zfar", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "        ", "super", "(", "WeakPerspectiveCamera", ",", "self", ")", ".", "__init__", "(", "\n", "znear", "=", "znear", ",", "\n", "zfar", "=", "zfar", ",", "\n", "name", "=", "name", ",", "\n", ")", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "translation", "=", "translation", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.WeakPerspectiveCamera.get_projection_matrix": [[28, 36], ["numpy.eye"], "methods", ["None"], ["", "def", "get_projection_matrix", "(", "self", ",", "width", "=", "None", ",", "height", "=", "None", ")", ":", "\n", "        ", "P", "=", "np", ".", "eye", "(", "4", ")", "\n", "P", "[", "0", ",", "0", "]", "=", "self", ".", "scale", "[", "0", "]", "\n", "P", "[", "1", ",", "1", "]", "=", "self", ".", "scale", "[", "1", "]", "\n", "P", "[", "0", ",", "3", "]", "=", "self", ".", "translation", "[", "0", "]", "*", "self", ".", "scale", "[", "0", "]", "\n", "P", "[", "1", ",", "3", "]", "=", "-", "self", ".", "translation", "[", "1", "]", "*", "self", ".", "scale", "[", "1", "]", "\n", "P", "[", "2", ",", "2", "]", "=", "-", "1", "\n", "return", "P", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.Renderer.__init__": [[39, 60], ["numpy.load", "pyrender.OffscreenRenderer", "pyrender.Scene", "pyrender.PointLight", "numpy.eye", "weak_perspective_pyrender_renderer.Renderer.scene.add", "weak_perspective_pyrender_renderer.Renderer.scene.add"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load"], ["    ", "def", "__init__", "(", "self", ",", "resolution", "=", "(", "256", ",", "256", ")", ")", ":", "\n", "        ", "self", ".", "resolution", "=", "resolution", "\n", "\n", "self", ".", "faces", "=", "np", ".", "load", "(", "config", ".", "SMPL_FACES_PATH", ")", "\n", "self", ".", "renderer", "=", "pyrender", ".", "OffscreenRenderer", "(", "\n", "viewport_width", "=", "self", ".", "resolution", "[", "0", "]", ",", "\n", "viewport_height", "=", "self", ".", "resolution", "[", "1", "]", ",", "\n", "point_size", "=", "1.0", "\n", ")", "\n", "\n", "# set the scene", "\n", "self", ".", "scene", "=", "pyrender", ".", "Scene", "(", "bg_color", "=", "[", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", ",", "ambient_light", "=", "(", "0.3", ",", "0.3", ",", "0.3", ")", ")", "\n", "\n", "light", "=", "pyrender", ".", "PointLight", "(", "color", "=", "[", "1.0", ",", "1.0", ",", "1.0", "]", ",", "intensity", "=", "1.", ")", "\n", "\n", "light_pose", "=", "np", ".", "eye", "(", "4", ")", "\n", "light_pose", "[", ":", "3", ",", "3", "]", "=", "[", "0", ",", "-", "1", ",", "1", "]", "\n", "self", ".", "scene", ".", "add", "(", "light", ",", "pose", "=", "light_pose", ")", "\n", "\n", "light_pose", "[", ":", "3", ",", "3", "]", "=", "[", "0", ",", "1", ",", "1", "]", "\n", "self", ".", "scene", ".", "add", "(", "light", ",", "pose", "=", "light_pose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.Renderer.render": [[64, 119], ["trimesh.Trimesh", "trimesh.transformations.rotation_matrix", "pyrender.Mesh.from_trimesh.apply_transform", "weak_perspective_pyrender_renderer.WeakPerspectiveCamera", "pyrender.MetallicRoughnessMaterial", "pyrender.Mesh.from_trimesh", "weak_perspective_pyrender_renderer.Renderer.scene.add", "numpy.eye", "weak_perspective_pyrender_renderer.Renderer.scene.add", "weak_perspective_pyrender_renderer.Renderer.renderer.render", "math.radians", "pyrender.Mesh.from_trimesh.export", "trimesh.transformations.rotation_matrix", "pyrender.Mesh.from_trimesh.apply_transform", "output_img.astype", "weak_perspective_pyrender_renderer.Renderer.scene.remove_node", "weak_perspective_pyrender_renderer.Renderer.scene.remove_node", "math.radians", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.apply_transform", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.Renderer.render", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.apply_transform"], ["", "def", "render", "(", "self", ",", "verts", ",", "cam", ",", "img", "=", "None", ",", "angle", "=", "None", ",", "axis", "=", "None", ",", "mesh_filename", "=", "None", ",", "color", "=", "[", "0.8", ",", "0.3", ",", "0.3", "]", ",", "\n", "return_mask", "=", "False", ")", ":", "\n", "\n", "        ", "mesh", "=", "trimesh", ".", "Trimesh", "(", "vertices", "=", "verts", ",", "faces", "=", "self", ".", "faces", ")", "\n", "\n", "Rx", "=", "trimesh", ".", "transformations", ".", "rotation_matrix", "(", "math", ".", "radians", "(", "180", ")", ",", "[", "1", ",", "0", ",", "0", "]", ")", "\n", "mesh", ".", "apply_transform", "(", "Rx", ")", "\n", "\n", "if", "mesh_filename", "is", "not", "None", ":", "\n", "            ", "mesh", ".", "export", "(", "mesh_filename", ")", "\n", "\n", "", "if", "angle", "and", "axis", ":", "\n", "            ", "R", "=", "trimesh", ".", "transformations", ".", "rotation_matrix", "(", "math", ".", "radians", "(", "angle", ")", ",", "axis", ")", "\n", "mesh", ".", "apply_transform", "(", "R", ")", "\n", "\n", "", "if", "cam", ".", "shape", "[", "-", "1", "]", "==", "4", ":", "\n", "            ", "sx", ",", "sy", ",", "tx", ",", "ty", "=", "cam", "\n", "", "elif", "cam", ".", "shape", "[", "-", "1", "]", "==", "3", ":", "\n", "            ", "s", ",", "tx", ",", "ty", "=", "cam", "\n", "sx", "=", "sy", "=", "s", "\n", "\n", "", "camera", "=", "WeakPerspectiveCamera", "(", "\n", "scale", "=", "[", "sx", ",", "sy", "]", ",", "\n", "translation", "=", "[", "tx", ",", "ty", "]", ",", "\n", "zfar", "=", "1000.", "\n", ")", "\n", "\n", "material", "=", "pyrender", ".", "MetallicRoughnessMaterial", "(", "\n", "metallicFactor", "=", "0.2", ",", "\n", "alphaMode", "=", "'OPAQUE'", ",", "\n", "baseColorFactor", "=", "(", "color", "[", "0", "]", ",", "color", "[", "1", "]", ",", "color", "[", "2", "]", ",", "1.0", ")", "\n", ")", "\n", "\n", "mesh", "=", "pyrender", ".", "Mesh", ".", "from_trimesh", "(", "mesh", ",", "material", "=", "material", ")", "\n", "\n", "mesh_node", "=", "self", ".", "scene", ".", "add", "(", "mesh", ",", "'mesh'", ")", "\n", "\n", "camera_pose", "=", "np", ".", "eye", "(", "4", ")", "\n", "cam_node", "=", "self", ".", "scene", ".", "add", "(", "camera", ",", "pose", "=", "camera_pose", ")", "\n", "\n", "rgb", ",", "rend_depth", "=", "self", ".", "renderer", ".", "render", "(", "self", ".", "scene", ",", "flags", "=", "RenderFlags", ".", "RGBA", ")", "\n", "valid_mask", "=", "(", "rend_depth", ">", "0", ")", "\n", "if", "return_mask", ":", "\n", "            ", "return", "valid_mask", "\n", "", "else", ":", "\n", "            ", "if", "img", "is", "None", ":", "\n", "                ", "img", "=", "np", ".", "zeros", "(", "(", "self", ".", "resolution", "[", "0", "]", ",", "self", ".", "resolution", "[", "1", "]", ",", "3", ")", ")", "\n", "", "valid_mask", "=", "valid_mask", "[", ":", ",", ":", ",", "None", "]", "\n", "output_img", "=", "rgb", "[", ":", ",", ":", ",", ":", "-", "1", "]", "*", "valid_mask", "+", "(", "1", "-", "valid_mask", ")", "*", "img", "\n", "image", "=", "output_img", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "self", ".", "scene", ".", "remove_node", "(", "mesh_node", ")", "\n", "self", ".", "scene", ".", "remove_node", "(", "cam_node", ")", "\n", "\n", "return", "image", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.nmr_renderer.NMRRenderer.__init__": [[22, 75], ["torch.Module.__init__", "numpy.load", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "faces[].expand", "nmr_renderer.NMRRenderer.register_buffer", "neural_renderer.Renderer", "faces[].expand.astype", "numpy.load", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.ones.expand", "torch.ones.expand", "nmr_renderer.NMRRenderer.register_buffer", "numpy.load", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "nmr_renderer.NMRRenderer.register_buffer", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "nmr_renderer.NMRRenderer.register_buffer", "print", "cam_K[].expand", "cam_R[].expand", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load"], ["def", "__init__", "(", "self", ",", "\n", "batch_size", ",", "\n", "cam_K", ",", "\n", "cam_R", ",", "\n", "img_wh", "=", "256", ",", "\n", "rend_parts_seg", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param batch_size\n        :param cam_K: (bs, 3, 3) camera intrinsics matrix\n        :param cam_R: (bs, 3, 3) camera rotation matrix (usually identity).\n        :param img_wh: output render width/height\n        :param rend_parts_seg: if True, render 6 part segmentation, else render RGB.\n        \"\"\"", "\n", "super", "(", "NMRRenderer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "faces", "=", "np", ".", "load", "(", "config", ".", "SMPL_FACES_PATH", ")", "\n", "faces", "=", "torch", ".", "from_numpy", "(", "faces", ".", "astype", "(", "np", ".", "int32", ")", ")", "\n", "faces", "=", "faces", "[", "None", ",", ":", "]", ".", "expand", "(", "batch_size", ",", "-", "1", ",", "-", "1", ")", "\n", "self", ".", "register_buffer", "(", "'faces'", ",", "faces", ")", "\n", "\n", "if", "rend_parts_seg", ":", "\n", "            ", "textures", "=", "np", ".", "load", "(", "config", ".", "VERTEX_TEXTURE_PATH", ")", "\n", "textures", "=", "torch", ".", "from_numpy", "(", "textures", ")", ".", "float", "(", ")", "\n", "textures", "=", "textures", ".", "expand", "(", "batch_size", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "self", ".", "register_buffer", "(", "'textures'", ",", "textures", ")", "\n", "\n", "cube_parts", "=", "np", ".", "load", "(", "config", ".", "CUBE_PARTS_PATH", ")", "\n", "cube_parts", "=", "torch", ".", "from_numpy", "(", "cube_parts", ")", ".", "float", "(", ")", "\n", "self", ".", "register_buffer", "(", "'cube_parts'", ",", "cube_parts", ")", "\n", "", "else", ":", "\n", "            ", "texture_size", "=", "2", "\n", "textures", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "faces", ".", "shape", "[", "1", "]", ",", "texture_size", ",", "texture_size", ",", "\n", "texture_size", ",", "3", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "self", ".", "register_buffer", "(", "'textures'", ",", "textures", ")", "\n", "\n", "# Setup renderer", "\n", "", "if", "cam_K", ".", "ndim", "!=", "3", ":", "\n", "            ", "print", "(", "\"Expanding cam_K and cam_R by batch size.\"", ")", "\n", "cam_K", "=", "cam_K", "[", "None", ",", ":", ",", ":", "]", ".", "expand", "(", "batch_size", ",", "-", "1", ",", "-", "1", ")", "\n", "cam_R", "=", "cam_R", "[", "None", ",", ":", ",", ":", "]", ".", "expand", "(", "batch_size", ",", "-", "1", ",", "-", "1", ")", "\n", "", "renderer", "=", "nr", ".", "Renderer", "(", "camera_mode", "=", "'projection'", ",", "\n", "K", "=", "cam_K", ",", "\n", "R", "=", "cam_R", ",", "\n", "image_size", "=", "img_wh", ",", "\n", "orig_size", "=", "img_wh", ",", "\n", "light_direction", "=", "[", "0", ",", "0", ",", "1", "]", ")", "\n", "if", "rend_parts_seg", ":", "\n", "            ", "renderer", ".", "light_intensity_ambient", "=", "1", "\n", "renderer", ".", "anti_aliasing", "=", "False", "\n", "renderer", ".", "light_intensity_directional", "=", "0", "\n", "", "self", ".", "renderer", "=", "renderer", "\n", "\n", "self", ".", "rend_parts_seg", "=", "rend_parts_seg", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.nmr_renderer.NMRRenderer.forward": [[76, 92], ["cam_ts.unsqueeze.unsqueeze.unsqueeze", "nmr_renderer.NMRRenderer.renderer", "nmr_renderer.NMRRenderer.get_parts", "nmr_renderer.NMRRenderer.renderer"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.nmr_renderer.NMRRenderer.get_parts"], ["", "def", "forward", "(", "self", ",", "vertices", ",", "cam_ts", ")", ":", "\n", "        ", "\"\"\"\n        :param vertices: (B, N, 3)\n        :param cam_ts: (B, 1, 3)\n        \"\"\"", "\n", "if", "cam_ts", ".", "ndim", "==", "2", ":", "\n", "            ", "cam_ts", "=", "cam_ts", ".", "unsqueeze", "(", "1", ")", "\n", "", "if", "self", ".", "rend_parts_seg", ":", "\n", "            ", "parts", ",", "_", ",", "mask", "=", "self", ".", "renderer", "(", "vertices", ",", "self", ".", "faces", ",", "self", ".", "textures", ",", "\n", "t", "=", "cam_ts", ")", "\n", "parts", "=", "self", ".", "get_parts", "(", "parts", ",", "mask", ")", "\n", "return", "parts", "\n", "", "else", ":", "\n", "            ", "rend_image", ",", "depth", ",", "_", "=", "self", ".", "renderer", "(", "vertices", ",", "self", ".", "faces", ",", "self", ".", "textures", ",", "\n", "t", "=", "cam_ts", ")", "\n", "return", "rend_image", ",", "depth", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.nmr_renderer.NMRRenderer.get_parts": [[93, 102], ["mask.view.view.view", "torch.floor().long", "torch.floor().long", "torch.floor().long", "torch.floor().long", "parts.view().long.view().long.view().long", "torch.floor", "torch.floor", "torch.floor", "torch.floor", "parts.view().long.view().long.view", "parts.view().long.view().long.permute().contiguous().view", "parts.view().long.view().long.permute().contiguous", "parts.view().long.view().long.permute"], "methods", ["None"], ["", "", "def", "get_parts", "(", "self", ",", "parts", ",", "mask", ")", ":", "\n", "        ", "\"\"\"Process renderer part image to get body part indices.\"\"\"", "\n", "bn", ",", "c", ",", "h", ",", "w", "=", "parts", ".", "shape", "\n", "mask", "=", "mask", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "parts_index", "=", "torch", ".", "floor", "(", "100", "*", "parts", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", ".", "long", "(", ")", "\n", "parts", "=", "self", ".", "cube_parts", "[", "parts_index", "[", ":", ",", "0", "]", ",", "parts_index", "[", ":", ",", "1", "]", ",", "parts_index", "[", ":", ",", "2", "]", ",", "None", "]", "\n", "parts", "*=", "mask", "\n", "parts", "=", "parts", ".", "view", "(", "bn", ",", "h", ",", "w", ")", ".", "long", "(", ")", "\n", "return", "parts", "", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.cam_augmentation.augment_cam_t": [[4, 16], ["mean_cam_t.clone", "torch.randn", "torch.rand"], "function", ["None"], ["def", "augment_cam_t", "(", "mean_cam_t", ",", "xy_std", "=", "0.05", ",", "delta_z_range", "=", "[", "-", "5", ",", "5", "]", ")", ":", "\n", "    ", "batch_size", "=", "mean_cam_t", ".", "shape", "[", "0", "]", "\n", "device", "=", "mean_cam_t", ".", "device", "\n", "new_cam_t", "=", "mean_cam_t", ".", "clone", "(", ")", "\n", "delta_tx_ty", "=", "torch", ".", "randn", "(", "batch_size", ",", "2", ",", "device", "=", "device", ")", "*", "xy_std", "\n", "new_cam_t", "[", ":", ",", ":", "2", "]", "=", "mean_cam_t", "[", ":", ",", ":", "2", "]", "+", "delta_tx_ty", "\n", "\n", "l", ",", "h", "=", "delta_z_range", "\n", "delta_tz", "=", "(", "h", "-", "l", ")", "*", "torch", ".", "rand", "(", "batch_size", ",", "device", "=", "device", ")", "+", "l", "\n", "new_cam_t", "[", ":", ",", "2", "]", "=", "mean_cam_t", "[", ":", ",", "2", "]", "+", "delta_tz", "\n", "\n", "return", "new_cam_t", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.smpl_augmentation.uniform_sample_shape": [[6, 15], ["torch.rand"], "function", ["None"], ["def", "uniform_sample_shape", "(", "batch_size", ",", "mean_shape", ",", "delta_betas_range", ")", ":", "\n", "    ", "\"\"\"\n    Uniform sampling of shape parameter deviations from the mean.\n    \"\"\"", "\n", "device", "=", "mean_shape", ".", "device", "\n", "l", ",", "h", "=", "delta_betas_range", "\n", "delta_betas", "=", "(", "h", "-", "l", ")", "*", "torch", ".", "rand", "(", "batch_size", ",", "10", ",", "device", "=", "device", ")", "+", "l", "\n", "shape", "=", "delta_betas", "+", "mean_shape", "\n", "return", "shape", "# (bs, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.smpl_augmentation.normal_sample_shape": [[17, 25], ["torch.randn"], "function", ["None"], ["", "def", "normal_sample_shape", "(", "batch_size", ",", "mean_shape", ",", "std_vector", ")", ":", "\n", "    ", "\"\"\"\n    Gaussian sampling of shape parameter deviations from the mean.\n    \"\"\"", "\n", "device", "=", "mean_shape", ".", "device", "\n", "delta_betas", "=", "torch", ".", "randn", "(", "batch_size", ",", "10", ",", "device", "=", "device", ")", "*", "std_vector", "\n", "shape", "=", "delta_betas", "+", "mean_shape", "\n", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.smpl_augmentation.augment_smpl": [[27, 62], ["smplx.lbs.batch_rodrigues", "pose_rotmats.view.view", "smplx.lbs.batch_rodrigues", "glob_rotmats.unsqueeze.unsqueeze", "pose.contiguous().view", "global_orients.contiguous().view", "smpl_augmentation.uniform_sample_shape", "smpl_augmentation.normal_sample_shape", "pose.contiguous", "global_orients.contiguous"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.smpl_augmentation.uniform_sample_shape", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.smpl_augmentation.normal_sample_shape"], ["", "def", "augment_smpl", "(", "orig_shape", ",", "pose", ",", "global_orients", ",", "\n", "mean_shape", ",", "\n", "smpl_augment_params", ")", ":", "\n", "    ", "\"\"\"\n    Augments SMPL shape parameters. Also converts SMPL pose parameters (in axis angle form)\n    to rotation matrices.\n    :param orig_shape: original shape.\n    :param pose: pose parameters for the body (excluding root orientation).\n    :param global_orients: root orientation.\n    :param mean_shape: mean SMPL shape.\n    :param smpl_augment_params: dict containing parameters for SMPL augmentation.\n    \"\"\"", "\n", "augment_shape", "=", "smpl_augment_params", "[", "'augment_shape'", "]", "\n", "delta_betas_distribution", "=", "smpl_augment_params", "[", "'delta_betas_distribution'", "]", "# 'normal' or 'uniform' shape sampling distribution", "\n", "delta_betas_range", "=", "smpl_augment_params", "[", "'delta_betas_range'", "]", "# Range of uniformly-distributed shape parameters.", "\n", "delta_betas_std_vector", "=", "smpl_augment_params", "[", "'delta_betas_std_vector'", "]", "# std of normally-distributed the shape parameters.", "\n", "\n", "batch_size", "=", "orig_shape", ".", "shape", "[", "0", "]", "\n", "if", "augment_shape", ":", "\n", "        ", "assert", "delta_betas_distribution", "in", "[", "'uniform'", ",", "'normal'", "]", "\n", "if", "delta_betas_distribution", "==", "'uniform'", ":", "\n", "            ", "new_shape", "=", "uniform_sample_shape", "(", "batch_size", ",", "mean_shape", ",", "delta_betas_range", ")", "\n", "", "elif", "delta_betas_distribution", "==", "'normal'", ":", "\n", "            ", "assert", "delta_betas_std_vector", "is", "not", "None", "\n", "new_shape", "=", "normal_sample_shape", "(", "batch_size", ",", "mean_shape", ",", "delta_betas_std_vector", ")", "\n", "", "", "else", ":", "\n", "        ", "new_shape", "=", "orig_shape", "\n", "\n", "", "pose_rotmats", "=", "batch_rodrigues", "(", "pose", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", "\n", "pose_rotmats", "=", "pose_rotmats", ".", "view", "(", "-", "1", ",", "23", ",", "3", ",", "3", ")", "\n", "\n", "glob_rotmats", "=", "batch_rodrigues", "(", "global_orients", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", "\n", "glob_rotmats", "=", "glob_rotmats", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "return", "new_shape", ",", "pose_rotmats", ",", "glob_rotmats", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_verts2D_deviation": [[5, 23], ["vertices.clone", "torch.rand"], "function", ["None"], ["def", "random_verts2D_deviation", "(", "vertices", ",", "delta_verts2d_dev_range", "=", "[", "-", "0.01", ",", "0.01", "]", ")", ":", "\n", "    ", "\"\"\"\n    Randomly add 2D uniform noise to vertices to create silhouettes/part segmentations with\n    corrupted edges.\n    :param vertices: (bs, 6890, 3)\n    :param delta_verts2d_dev_range: range of uniform noise.\n    \"\"\"", "\n", "batch_size", "=", "vertices", ".", "shape", "[", "0", "]", "\n", "num_verts", "=", "vertices", ".", "shape", "[", "1", "]", "\n", "device", "=", "vertices", ".", "device", "\n", "\n", "noisy_vertices", "=", "vertices", ".", "clone", "(", ")", "\n", "\n", "l", ",", "h", "=", "delta_verts2d_dev_range", "\n", "delta_verts2d_dev", "=", "(", "h", "-", "l", ")", "*", "torch", ".", "rand", "(", "batch_size", ",", "num_verts", ",", "2", ",", "device", "=", "device", ")", "+", "l", "\n", "noisy_vertices", "[", ":", ",", ":", ",", ":", "2", "]", "=", "noisy_vertices", "[", ":", ",", ":", ",", ":", "2", "]", "+", "delta_verts2d_dev", "\n", "\n", "return", "noisy_vertices", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_joints2D_deviation": [[25, 50], ["torch.rand", "torch.rand", "len", "len"], "function", ["None"], ["", "def", "random_joints2D_deviation", "(", "joints2D", ",", "\n", "delta_j2d_dev_range", "=", "[", "-", "5", ",", "5", "]", ",", "\n", "delta_j2d_hip_dev_range", "=", "[", "-", "15", ",", "15", "]", ")", ":", "\n", "    ", "\"\"\"\n    Deviate 2D joint locations with uniform random noise.\n    :param joints2D: (bs, num joints, num joints)\n    :param delta_j2d_dev_range: uniform noise range.\n    :param delta_j2d_hip_dev_range: uniform noise range for hip joints. You may wish to make\n    this bigger than for other joints since hip joints are semantically hard to localise and\n    can be predicted inaccurately by joint detectors.\n    \"\"\"", "\n", "hip_joints", "=", "[", "11", ",", "12", "]", "\n", "other_joints", "=", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "9", ",", "10", ",", "13", ",", "14", ",", "15", ",", "16", "]", "\n", "batch_size", "=", "joints2D", ".", "shape", "[", "0", "]", "\n", "device", "=", "joints2D", ".", "device", "\n", "\n", "l", ",", "h", "=", "delta_j2d_dev_range", "\n", "delta_j2d_dev", "=", "(", "h", "-", "l", ")", "*", "torch", ".", "rand", "(", "batch_size", ",", "len", "(", "other_joints", ")", ",", "2", ",", "device", "=", "device", ")", "+", "l", "\n", "joints2D", "[", ":", ",", "other_joints", ",", ":", "]", "=", "joints2D", "[", ":", ",", "other_joints", ",", ":", "]", "+", "delta_j2d_dev", "\n", "\n", "l", ",", "h", "=", "delta_j2d_hip_dev_range", "\n", "delta_j2d_hip_dev_range", "=", "(", "h", "-", "l", ")", "*", "torch", ".", "rand", "(", "batch_size", ",", "len", "(", "hip_joints", ")", ",", "2", ",", "device", "=", "device", ")", "+", "l", "\n", "joints2D", "[", ":", ",", "hip_joints", ",", ":", "]", "=", "joints2D", "[", ":", ",", "hip_joints", ",", ":", "]", "+", "delta_j2d_hip_dev_range", "\n", "\n", "return", "joints2D", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_remove_bodyparts": [[52, 76], ["range", "len", "len", "len", "seg[].clone", "numpy.random.rand"], "function", ["None"], ["", "def", "random_remove_bodyparts", "(", "seg", ",", "classes_to_remove", ",", "probabilities_to_remove", ")", ":", "\n", "    ", "\"\"\"\n    Randomly remove bodyparts from silhouette/segmentation (i.e. set pixels to background\n    class).\n    :param seg: (bs, wh, wh)\n    :param classes_to_remove: list of classes to remove. Classes are integers (as defined in\n    nmr_renderer.py).\n    :param probabilities_to_remove: probability of removal for each class.\n    \"\"\"", "\n", "assert", "len", "(", "classes_to_remove", ")", "==", "len", "(", "probabilities_to_remove", ")", "\n", "\n", "batch_size", "=", "seg", ".", "shape", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "classes_to_remove", ")", ")", ":", "\n", "        ", "class_to_remove", "=", "classes_to_remove", "[", "i", "]", "\n", "prob_to_remove", "=", "probabilities_to_remove", "[", "i", "]", "\n", "\n", "# Determine which samples to augment in the batch", "\n", "rand_vec", "=", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "<", "prob_to_remove", "\n", "samples_to_augment", "=", "seg", "[", "rand_vec", "]", ".", "clone", "(", ")", "\n", "\n", "samples_to_augment", "[", "samples_to_augment", "==", "class_to_remove", "]", "=", "0", "\n", "seg", "[", "rand_vec", "]", "=", "samples_to_augment", "\n", "\n", "", "return", "seg", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_occlude": [[78, 102], ["numpy.random.rand", "range", "numpy.random.rand", "numpy.random.rand"], "function", ["None"], ["", "def", "random_occlude", "(", "seg", ",", "occlude_probability", "=", "0.5", ",", "occlude_box_dim", "=", "48", ")", ":", "\n", "    ", "\"\"\"\n    Randomly occlude silhouette/part segmentation with boxes.\n    :param seg: (bs, wh, wh)\n    \"\"\"", "\n", "batch_size", "=", "seg", ".", "shape", "[", "0", "]", "\n", "seg_wh", "=", "seg", ".", "shape", "[", "-", "1", "]", "\n", "seg_centre", "=", "seg_wh", "/", "2", "\n", "x_h", ",", "x_l", "=", "seg_centre", "-", "0.3", "*", "seg_wh", "/", "2", ",", "seg_centre", "+", "0.3", "*", "seg_wh", "/", "2", "\n", "y_h", ",", "y_l", "=", "seg_centre", "-", "0.3", "*", "seg_wh", "/", "2", ",", "seg_centre", "+", "0.3", "*", "seg_wh", "/", "2", "\n", "\n", "x", "=", "(", "x_h", "-", "x_l", ")", "*", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "+", "x_l", "\n", "y", "=", "(", "y_h", "-", "y_l", ")", "*", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "+", "y_l", "\n", "box_x1", "=", "(", "x", "-", "occlude_box_dim", "/", "2", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "box_x2", "=", "(", "x", "+", "occlude_box_dim", "/", "2", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "box_y1", "=", "(", "y", "-", "occlude_box_dim", "/", "2", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "box_y2", "=", "(", "y", "+", "occlude_box_dim", "/", "2", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "\n", "rand_vec", "=", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "if", "rand_vec", "[", "i", "]", "<", "occlude_probability", ":", "\n", "            ", "seg", "[", "i", ",", "box_x1", "[", "i", "]", ":", "box_x2", "[", "i", "]", ",", "box_y1", "[", "i", "]", ":", "box_y2", "[", "i", "]", "]", "=", "0", "\n", "\n", "", "", "return", "seg", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.augment_proxy_representation": [[104, 124], ["orig_segs.clone", "orig_joints2D.clone", "proxy_rep_augmentation.random_remove_bodyparts", "proxy_rep_augmentation.random_occlude", "proxy_rep_augmentation.random_joints2D_deviation"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_remove_bodyparts", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_occlude", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_joints2D_deviation"], ["", "def", "augment_proxy_representation", "(", "orig_segs", ",", "orig_joints2D", ",", "\n", "proxy_rep_augment_params", ")", ":", "\n", "    ", "new_segs", "=", "orig_segs", ".", "clone", "(", ")", "\n", "new_joints2D", "=", "orig_joints2D", ".", "clone", "(", ")", "\n", "\n", "if", "proxy_rep_augment_params", "[", "'remove_appendages'", "]", ":", "\n", "        ", "new_segs", "=", "random_remove_bodyparts", "(", "new_segs", ",", "\n", "classes_to_remove", "=", "proxy_rep_augment_params", "[", "'remove_appendages_classes'", "]", ",", "\n", "probabilities_to_remove", "=", "proxy_rep_augment_params", "[", "'remove_appendages_probabilities'", "]", ")", "\n", "", "if", "proxy_rep_augment_params", "[", "'occlude_seg'", "]", ":", "\n", "        ", "new_segs", "=", "random_occlude", "(", "new_segs", ",", "\n", "occlude_probability", "=", "proxy_rep_augment_params", "[", "'occlude_probability'", "]", ",", "\n", "occlude_box_dim", "=", "proxy_rep_augment_params", "[", "'occlude_box_dim'", "]", ")", "\n", "\n", "", "if", "proxy_rep_augment_params", "[", "'deviate_joints2D'", "]", ":", "\n", "        ", "new_joints2D", "=", "random_joints2D_deviation", "(", "new_joints2D", ",", "\n", "delta_j2d_dev_range", "=", "proxy_rep_augment_params", "[", "'delta_j2d_dev_range'", "]", ",", "\n", "delta_j2d_hip_dev_range", "=", "proxy_rep_augment_params", "[", "'delta_j2d_hip_dev_range'", "]", ")", "\n", "\n", "", "return", "new_segs", ",", "new_joints2D", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.regressor.SingleInputRegressor.__init__": [[13, 42], ["torch.Module.__init__", "models.resnet.resnet18", "models.ief_module.IEFModule", "models.resnet.resnet50", "models.ief_module.IEFModule"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet18", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet50"], ["def", "__init__", "(", "self", ",", "\n", "resnet_in_channels", "=", "1", ",", "\n", "resnet_layers", "=", "18", ",", "\n", "ief_iters", "=", "3", ")", ":", "\n", "        ", "\"\"\"\n        :param resnet_in_channels: 1 if input silhouette/segmentation, 1 + num_joints if\n        input silhouette/segmentation + joints.\n        :param resnet_layers: number of layers in ResNet backbone (18 or 50)\n        :param ief_iters: number of IEF iterations.\n        \"\"\"", "\n", "super", "(", "SingleInputRegressor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "num_pose_params", "=", "24", "*", "6", "\n", "num_output_params", "=", "3", "+", "num_pose_params", "+", "10", "\n", "\n", "if", "resnet_layers", "==", "18", ":", "\n", "            ", "self", ".", "image_encoder", "=", "resnet18", "(", "in_channels", "=", "resnet_in_channels", ",", "\n", "pretrained", "=", "False", ")", "\n", "self", ".", "ief_module", "=", "IEFModule", "(", "[", "512", ",", "512", "]", ",", "\n", "512", ",", "\n", "num_output_params", ",", "\n", "iterations", "=", "ief_iters", ")", "\n", "", "elif", "resnet_layers", "==", "50", ":", "\n", "            ", "self", ".", "image_encoder", "=", "resnet50", "(", "in_channels", "=", "resnet_in_channels", ",", "\n", "pretrained", "=", "False", ")", "\n", "self", ".", "ief_module", "=", "IEFModule", "(", "[", "1024", ",", "1024", "]", ",", "\n", "2048", ",", "\n", "num_output_params", ",", "\n", "iterations", "=", "ief_iters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.regressor.SingleInputRegressor.forward": [[43, 48], ["regressor.SingleInputRegressor.image_encoder", "regressor.SingleInputRegressor.ief_module"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "input_feats", "=", "self", ".", "image_encoder", "(", "input", ")", "\n", "cam_params", ",", "pose_params", ",", "shape_params", "=", "self", ".", "ief_module", "(", "input_feats", ")", "\n", "\n", "return", "cam_params", ",", "pose_params", ",", "shape_params", "\n", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.BasicBlock.__init__": [[43, 60], ["torch.Module.__init__", "resnet.conv3x3", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "resnet.conv3x3", "norm_layer", "ValueError", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv3x3", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv3x3"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "if", "groups", "!=", "1", "or", "base_width", "!=", "64", ":", "\n", "            ", "raise", "ValueError", "(", "'BasicBlock only supports groups=1 and base_width=64'", ")", "\n", "", "if", "dilation", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dilation > 1 not supported in BasicBlock\"", ")", "\n", "# Both self.conv1 and self.downsample layers downsample the input when stride != 1", "\n", "", "self", ".", "conv1", "=", "conv3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "conv3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "planes", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.BasicBlock.forward": [[61, 78], ["resnet.BasicBlock.conv1", "resnet.BasicBlock.bn1", "resnet.BasicBlock.relu", "resnet.BasicBlock.conv2", "resnet.BasicBlock.bn2", "resnet.BasicBlock.relu", "resnet.BasicBlock.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.Bottleneck.__init__": [[84, 100], ["torch.Module.__init__", "resnet.conv1x1", "norm_layer", "resnet.conv3x3", "norm_layer", "resnet.conv1x1", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "int"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv1x1", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv3x3", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv1x1"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "width", "=", "int", "(", "planes", "*", "(", "base_width", "/", "64.", ")", ")", "*", "groups", "\n", "# Both self.conv2 and self.downsample layers downsample the input when stride != 1", "\n", "self", ".", "conv1", "=", "conv1x1", "(", "inplanes", ",", "width", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "conv2", "=", "conv3x3", "(", "width", ",", "width", ",", "stride", ",", "groups", ",", "dilation", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "conv3", "=", "conv1x1", "(", "width", ",", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.Bottleneck.forward": [[101, 122], ["resnet.Bottleneck.conv1", "resnet.Bottleneck.bn1", "resnet.Bottleneck.relu", "resnet.Bottleneck.conv2", "resnet.Bottleneck.bn2", "resnet.Bottleneck.relu", "resnet.Bottleneck.conv3", "resnet.Bottleneck.bn3", "resnet.Bottleneck.relu", "resnet.Bottleneck.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet.__init__": [[126, 176], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "resnet.ResNet.modules", "len", "ValueError", "isinstance", "resnet.ResNet.modules", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "block", ",", "layers", ",", "in_channels", ",", "num_classes", "=", "1000", ",", "zero_init_residual", "=", "False", ",", "\n", "groups", "=", "1", ",", "width_per_group", "=", "64", ",", "replace_stride_with_dilation", "=", "None", ",", "\n", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "self", ".", "_norm_layer", "=", "norm_layer", "\n", "\n", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "dilation", "=", "1", "\n", "if", "replace_stride_with_dilation", "is", "None", ":", "\n", "# each element in the tuple indicates if we should replace", "\n", "# the 2x2 stride with a dilated convolution instead", "\n", "            ", "replace_stride_with_dilation", "=", "[", "False", ",", "False", ",", "False", "]", "\n", "", "if", "len", "(", "replace_stride_with_dilation", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\"replace_stride_with_dilation should be None \"", "\n", "\"or a 3-element tuple, got {}\"", ".", "format", "(", "replace_stride_with_dilation", ")", ")", "\n", "", "self", ".", "groups", "=", "groups", "\n", "self", ".", "base_width", "=", "width_per_group", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "self", ".", "inplanes", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "self", ".", "inplanes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "0", "]", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "1", "]", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "2", "]", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "# self.fc = nn.Linear(512 * block.expansion, num_classes) - don't need final FC layer", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "nn", ".", "BatchNorm2d", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n", "# Zero-initialize the last BN in each residual branch,", "\n", "# so that the residual branch starts with zeros, and each residual block behaves like an identity.", "\n", "# This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677", "\n", "", "", "if", "zero_init_residual", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "Bottleneck", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn3", ".", "weight", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "BasicBlock", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn2", ".", "weight", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet._make_layer": [[177, 200], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "resnet.conv1x1", "norm_layer", "block"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv1x1"], ["", "", "", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ",", "dilate", "=", "False", ")", ":", "\n", "        ", "norm_layer", "=", "self", ".", "_norm_layer", "\n", "downsample", "=", "None", "\n", "previous_dilation", "=", "self", ".", "dilation", "\n", "if", "dilate", ":", "\n", "            ", "self", ".", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "conv1x1", "(", "self", ".", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "stride", ")", ",", "\n", "norm_layer", "(", "planes", "*", "block", ".", "expansion", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "self", ".", "groups", ",", "\n", "self", ".", "base_width", ",", "previous_dilation", ",", "norm_layer", ")", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "_", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "groups", "=", "self", ".", "groups", ",", "\n", "base_width", "=", "self", ".", "base_width", ",", "dilation", "=", "self", ".", "dilation", ",", "\n", "norm_layer", "=", "norm_layer", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.ResNet.forward": [[201, 217], ["resnet.ResNet.conv1", "resnet.ResNet.bn1", "resnet.ResNet.relu", "resnet.ResNet.maxpool", "resnet.ResNet.layer1", "resnet.ResNet.layer2", "resnet.ResNet.layer3", "resnet.ResNet.layer4", "resnet.ResNet.avgpool", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "x", "=", "torch", ".", "flatten", "(", "x", ",", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv3x3": [[28, 32], ["torch.Conv2d"], "function", ["None"], ["def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "\"\"\"3x3 convolution with padding\"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "False", ",", "dilation", "=", "dilation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.conv1x1": [[34, 37], ["torch.Conv2d"], "function", ["None"], ["", "def", "conv1x1", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"1x1 convolution\"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet": [[219, 226], ["resnet.ResNet", "torch.load_url", "ResNet.load_state_dict"], "function", ["None"], ["", "", "def", "_resnet", "(", "arch", ",", "block", ",", "layers", ",", "in_channels", ",", "pretrained", ",", "progress", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "ResNet", "(", "block", ",", "layers", ",", "in_channels", ",", "**", "kwargs", ")", "\n", "if", "pretrained", ":", "\n", "        ", "state_dict", "=", "model_zoo", ".", "load_url", "(", "model_urls", "[", "arch", "]", ",", "\n", "progress", "=", "progress", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "# not using final FC layer", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet18": [[228, 237], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnet18", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-18 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_resnet", "(", "'resnet18'", ",", "BasicBlock", ",", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "in_channels", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet34": [[239, 248], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnet34", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-34 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_resnet", "(", "'resnet34'", ",", "BasicBlock", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "in_channels", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet50": [[250, 259], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnet50", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-50 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_resnet", "(", "'resnet50'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "in_channels", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet101": [[261, 270], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnet101", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-101 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_resnet", "(", "'resnet101'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "in_channels", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnet152": [[272, 281], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnet152", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-152 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_resnet", "(", "'resnet152'", ",", "Bottleneck", ",", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "in_channels", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnext50_32x4d": [[283, 294], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnext50_32x4d", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNeXt-50 32x4d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "kwargs", "[", "'groups'", "]", "=", "32", "\n", "kwargs", "[", "'width_per_group'", "]", "=", "4", "\n", "return", "_resnet", "(", "'resnext50_32x4d'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "in_channels", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.resnext101_32x8d": [[296, 307], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "resnext101_32x8d", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNeXt-101 32x8d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "kwargs", "[", "'groups'", "]", "=", "32", "\n", "kwargs", "[", "'width_per_group'", "]", "=", "8", "\n", "return", "_resnet", "(", "'resnext101_32x8d'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "in_channels", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.wide_resnet50_2": [[309, 323], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "wide_resnet50_2", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Wide ResNet-50-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "kwargs", "[", "'width_per_group'", "]", "=", "64", "*", "2", "\n", "return", "_resnet", "(", "'wide_resnet50_2'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet.wide_resnet101_2": [[325, 339], ["resnet._resnet"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.resnet._resnet"], ["", "def", "wide_resnet101_2", "(", "in_channels", ",", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Wide ResNet-101-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "kwargs", "[", "'width_per_group'", "]", "=", "64", "*", "2", "\n", "return", "_resnet", "(", "'wide_resnet101_2'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.smpl_official.SMPL.__init__": [[15, 26], ["smplx.SMPL.__init__", "numpy.load", "numpy.load", "numpy.load", "smpl_official.SMPL.register_buffer", "smpl_official.SMPL.register_buffer", "smpl_official.SMPL.register_buffer", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "SMPL", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "J_regressor_extra", "=", "np", ".", "load", "(", "config", ".", "J_REGRESSOR_EXTRA_PATH", ")", "\n", "J_regressor_cocoplus", "=", "np", ".", "load", "(", "config", ".", "COCOPLUS_REGRESSOR_PATH", ")", "\n", "J_regressor_h36m", "=", "np", ".", "load", "(", "config", ".", "H36M_REGRESSOR_PATH", ")", "\n", "self", ".", "register_buffer", "(", "'J_regressor_extra'", ",", "torch", ".", "tensor", "(", "J_regressor_extra", ",", "\n", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "self", ".", "register_buffer", "(", "'J_regressor_cocoplus'", ",", "torch", ".", "tensor", "(", "J_regressor_cocoplus", ",", "\n", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "self", ".", "register_buffer", "(", "'J_regressor_h36m'", ",", "torch", ".", "tensor", "(", "J_regressor_h36m", ",", "\n", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.smpl_official.SMPL.forward": [[27, 42], ["super().forward", "smplx.lbs.vertices2joints", "smplx.lbs.vertices2joints", "smplx.lbs.vertices2joints", "torch.cat", "smplx.body_models.ModelOutput"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.forward"], ["", "def", "forward", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", "[", "'get_skin'", "]", "=", "True", "\n", "smpl_output", "=", "super", "(", "SMPL", ",", "self", ")", ".", "forward", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "extra_joints", "=", "vertices2joints", "(", "self", ".", "J_regressor_extra", ",", "smpl_output", ".", "vertices", ")", "\n", "cocoplus_joints", "=", "vertices2joints", "(", "self", ".", "J_regressor_cocoplus", ",", "smpl_output", ".", "vertices", ")", "\n", "h36m_joints", "=", "vertices2joints", "(", "self", ".", "J_regressor_h36m", ",", "smpl_output", ".", "vertices", ")", "\n", "all_joints", "=", "torch", ".", "cat", "(", "[", "smpl_output", ".", "joints", ",", "extra_joints", ",", "cocoplus_joints", ",", "\n", "h36m_joints", "]", ",", "dim", "=", "1", ")", "\n", "output", "=", "ModelOutput", "(", "vertices", "=", "smpl_output", ".", "vertices", ",", "\n", "global_orient", "=", "smpl_output", ".", "global_orient", ",", "\n", "body_pose", "=", "smpl_output", ".", "body_pose", ",", "\n", "joints", "=", "all_joints", ",", "\n", "betas", "=", "smpl_output", ".", "betas", ",", "\n", "full_pose", "=", "smpl_output", ".", "full_pose", ")", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.ief_module.IEFModule.__init__": [[13, 32], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.nn.init.zeros_", "torch.Sequential", "torch.Sequential", "ief_module.IEFModule.load_mean_params_6d_pose"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.ief_module.IEFModule.load_mean_params_6d_pose"], ["def", "__init__", "(", "self", ",", "fc_layers_neurons", ",", "in_features", ",", "num_output_params", ",", "iterations", "=", "3", ")", ":", "\n", "        ", "super", "(", "IEFModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", "+", "num_output_params", ",", "fc_layers_neurons", "[", "0", "]", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "fc_layers_neurons", "[", "0", "]", ",", "fc_layers_neurons", "[", "1", "]", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "fc_layers_neurons", "[", "1", "]", ",", "num_output_params", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "torch", ".", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "fc1", ".", "bias", ")", "\n", "torch", ".", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "fc2", ".", "bias", ")", "\n", "torch", ".", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "fc3", ".", "bias", ")", "\n", "\n", "self", ".", "ief_layers", "=", "nn", ".", "Sequential", "(", "self", ".", "fc1", ",", "\n", "self", ".", "relu", ",", "\n", "self", ".", "fc2", ",", "\n", "self", ".", "relu", ",", "\n", "self", ".", "fc3", ")", "\n", "\n", "self", ".", "iterations", "=", "iterations", "\n", "self", ".", "initial_params_estimate", "=", "self", ".", "load_mean_params_6d_pose", "(", "config", ".", "SMPL_MEAN_PARAMS_PATH", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.ief_module.IEFModule.load_mean_params_6d_pose": [[33, 47], ["numpy.load", "numpy.zeros", "numpy.concatenate", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.zeros.astype"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load"], ["", "def", "load_mean_params_6d_pose", "(", "self", ",", "mean_params_path", ")", ":", "\n", "        ", "mean_smpl", "=", "np", ".", "load", "(", "mean_params_path", ")", "\n", "mean_pose", "=", "mean_smpl", "[", "'pose'", "]", "\n", "mean_shape", "=", "mean_smpl", "[", "'shape'", "]", "\n", "\n", "mean_params", "=", "np", ".", "zeros", "(", "3", "+", "24", "*", "6", "+", "10", ")", "\n", "mean_params", "[", "3", ":", "]", "=", "np", ".", "concatenate", "(", "(", "mean_pose", ",", "mean_shape", ")", ")", "\n", "\n", "# Set initial weak-perspective camera parameters - [s, tx, ty]", "\n", "mean_params", "[", "0", "]", "=", "0.9", "# Initialise scale to 0.9", "\n", "mean_params", "[", "1", "]", "=", "0.0", "\n", "mean_params", "[", "2", "]", "=", "0.0", "\n", "\n", "return", "torch", ".", "from_numpy", "(", "mean_params", ".", "astype", "(", "np", ".", "float32", ")", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.models.ief_module.IEFModule.forward": [[48, 65], ["img_features.size", "ief_module.IEFModule.initial_params_estimate.repeat", "params_estimate.to.to.to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "ief_module.IEFModule.ief_layers", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["", "def", "forward", "(", "self", ",", "img_features", ")", ":", "\n", "        ", "batch_size", "=", "img_features", ".", "size", "(", "0", ")", "\n", "\n", "params_estimate", "=", "self", ".", "initial_params_estimate", ".", "repeat", "(", "[", "batch_size", ",", "1", "]", ")", "\n", "params_estimate", "=", "params_estimate", ".", "to", "(", "img_features", ".", "device", ")", "\n", "\n", "state", "=", "torch", ".", "cat", "(", "[", "img_features", ",", "params_estimate", "]", ",", "dim", "=", "1", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "iterations", ")", ":", "\n", "            ", "delta", "=", "self", ".", "ief_layers", "(", "state", ")", "\n", "params_estimate", "+=", "delta", "\n", "state", "=", "torch", ".", "cat", "(", "[", "img_features", ",", "params_estimate", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "cam_params", "=", "params_estimate", "[", ":", ",", ":", "3", "]", "\n", "pose_params", "=", "params_estimate", "[", ":", ",", "3", ":", "3", "+", "24", "*", "6", "]", "\n", "shape_params", "=", "params_estimate", "[", ":", ",", "3", "+", "24", "*", "6", ":", "]", "\n", "\n", "return", "cam_params", ",", "pose_params", ",", "shape_params", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.train.train_synthetic_otf_rendering.train_synthetic_otf_rendering": [[27, 387], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "save_val_metrics.copy", "set().issubset", "metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker", "numpy.load", "torch.from_numpy().float().to", "range", "print", "regressor.load_state_dict", "save_val_metrics.copy.remove", "set", "utils.checkpoint_utils.load_training_info_from_checkpoint", "copy.deepcopy", "print", "print", "metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.initialise_loss_metric_sums", "print", "regressor.train", "enumerate", "print", "regressor.eval", "metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.update_per_epoch", "metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.determine_save_model_weights_this_epoch", "set", "regressor.state_dict", "torch.from_numpy().float", "tqdm.tqdm", "regressor", "smpl_model", "utils.cam_utils.orthographic_project_torch", "smpl_model", "torch.cat", "utils.joints2d_utils.check_joints2d_visibility_torch", "optimiser.zero_grad", "criterion", "loss.backward", "optimiser.step", "metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.update_per_batch", "torch.no_grad", "enumerate", "print", "copy.deepcopy", "print", "torch.save", "print", "torch.no_grad", "target_pose.to.to", "target_shape.to.to", "augmentation.smpl_augmentation.augment_smpl", "augmentation.cam_augmentation.augment_cam_t", "smpl_model", "utils.cam_utils.perspective_project_torch", "smpl_model", "nmr_parts_renderer", "augmentation.proxy_rep_augmentation.augment_proxy_representation", "utils.label_conversions.convert_multiclass_to_binary_labels_torch", "torch.from_numpy().float().to.unsqueeze", "utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps_torch", "torch.cat", "smplx.lbs.batch_rodrigues", "rot6d_to_rotmat().view.view", "tqdm.tqdm", "target_pose.to.to", "target_shape.to.to", "smpl_model", "utils.cam_utils.perspective_project_torch", "smpl_model", "nmr_parts_renderer", "utils.label_conversions.convert_multiclass_to_binary_labels_torch", "torch.from_numpy().float().to.unsqueeze", "utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps_torch", "torch.cat", "regressor", "smpl_model", "utils.cam_utils.orthographic_project_torch", "smpl_model", "smplx.lbs.batch_rodrigues", "target_pose_rotmats.view.view", "utils.joints2d_utils.check_joints2d_visibility_torch", "criterion", "metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.update_per_batch", "regressor.state_dict", "regressor.state_dict", "optimiser.state_dict", "criterion.state_dict", "torch.from_numpy", "augmentation.proxy_rep_augmentation.random_verts2D_deviation", "torch.from_numpy().float().to.cpu().detach().numpy", "torch.from_numpy().float().to.cpu().detach().numpy", "utils.image_utils.batch_crop_seg_to_bounding_box", "utils.image_utils.batch_resize", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "pred_pose.contiguous().view", "utils.rigid_transform_utils.rot6d_to_rotmat().view", "pred_pose_rotmats[].unsqueeze", "torch.from_numpy().float().to.cpu().detach().numpy", "torch.from_numpy().float().to.cpu().detach().numpy", "utils.image_utils.batch_crop_seg_to_bounding_box", "utils.image_utils.batch_resize", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "smplx.lbs.batch_rodrigues", "rot6d_to_rotmat().view.view", "target_pose.to.contiguous().view", "pred_pose.contiguous().view", "utils.rigid_transform_utils.rot6d_to_rotmat().view", "pred_pose_rotmats[].unsqueeze", "torch.from_numpy().float().to.cpu().detach", "torch.from_numpy().float().to.cpu().detach", "torch.from_numpy().float", "torch.from_numpy().float", "pred_pose.contiguous", "utils.rigid_transform_utils.rot6d_to_rotmat", "torch.from_numpy().float().to.cpu().detach", "torch.from_numpy().float().to.cpu().detach", "torch.from_numpy().float", "torch.from_numpy().float", "target_pose.to.contiguous", "pred_pose.contiguous", "pred_pose.contiguous", "utils.rigid_transform_utils.rot6d_to_rotmat", "torch.from_numpy().float().to.cpu", "torch.from_numpy().float().to.cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().float().to.cpu", "torch.from_numpy().float().to.cpu", "torch.from_numpy", "torch.from_numpy", "pred_pose.contiguous"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.checkpoint_utils.load_training_info_from_checkpoint", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.initialise_loss_metric_sums", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.update_per_epoch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.determine_save_model_weights_this_epoch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.orthographic_project_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.check_joints2d_visibility_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.update_per_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.smpl_augmentation.augment_smpl", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.cam_augmentation.augment_cam_t", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.perspective_project_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.augment_proxy_representation", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_multiclass_to_binary_labels_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.perspective_project_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_multiclass_to_binary_labels_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.orthographic_project_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.check_joints2d_visibility_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.update_per_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.augmentation.proxy_rep_augmentation.random_verts2D_deviation", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.batch_crop_seg_to_bounding_box", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.batch_resize", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.batch_crop_seg_to_bounding_box", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.batch_resize", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.rigid_transform_utils.rot6d_to_rotmat", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.rigid_transform_utils.rot6d_to_rotmat"], ["def", "train_synthetic_otf_rendering", "(", "device", ",", "\n", "regressor", ",", "\n", "smpl_model", ",", "\n", "nmr_parts_renderer", ",", "\n", "train_dataset", ",", "\n", "val_dataset", ",", "\n", "criterion", ",", "\n", "optimiser", ",", "\n", "batch_size", ",", "\n", "num_epochs", ",", "\n", "smpl_augment_params", ",", "\n", "cam_augment_params", ",", "\n", "bbox_augment_params", ",", "\n", "proxy_rep_augment_params", ",", "\n", "mean_cam_t", ",", "\n", "cam_K", ",", "\n", "cam_R", ",", "\n", "model_save_path", ",", "\n", "log_path", ",", "\n", "losses_to_track", ",", "\n", "metrics_to_track", ",", "\n", "save_val_metrics", ",", "\n", "epochs_per_save", "=", "10", ",", "\n", "checkpoint", "=", "None", ",", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "False", ")", ":", "\n", "\n", "    ", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ",", "num_workers", "=", "num_workers", ",", "\n", "pin_memory", "=", "pin_memory", ")", "\n", "\n", "val_dataloader", "=", "DataLoader", "(", "val_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ",", "num_workers", "=", "num_workers", ",", "\n", "pin_memory", "=", "pin_memory", ")", "\n", "\n", "# Ensure that all metrics used as model save conditions are being tracked (i.e. that", "\n", "# save_val_metrics is a subset of metrics_to_track).", "\n", "temp", "=", "save_val_metrics", ".", "copy", "(", ")", "\n", "if", "'loss'", "in", "save_val_metrics", ":", "\n", "        ", "temp", ".", "remove", "(", "'loss'", ")", "\n", "", "assert", "set", "(", "temp", ")", ".", "issubset", "(", "set", "(", "metrics_to_track", ")", ")", ",", "\"Not all save-condition metrics are being tracked!\"", "\n", "\n", "if", "checkpoint", "is", "not", "None", ":", "\n", "# Resuming training - note that current model and optimiser state dicts are loaded out", "\n", "# of train function (should be in run file).", "\n", "        ", "current_epoch", ",", "best_epoch", ",", "best_model_wts", ",", "best_epoch_val_metrics", "=", "load_training_info_from_checkpoint", "(", "checkpoint", ",", "save_val_metrics", ")", "\n", "load_logs", "=", "True", "\n", "\n", "", "else", ":", "\n", "        ", "current_epoch", "=", "0", "\n", "best_epoch_val_metrics", "=", "{", "}", "\n", "# metrics that decide whether to save model after each epoch or not", "\n", "for", "metric", "in", "save_val_metrics", ":", "\n", "            ", "best_epoch_val_metrics", "[", "metric", "]", "=", "np", ".", "inf", "\n", "", "best_epoch", "=", "current_epoch", "\n", "best_model_wts", "=", "copy", ".", "deepcopy", "(", "regressor", ".", "state_dict", "(", ")", ")", "\n", "load_logs", "=", "False", "\n", "\n", "# Instantiate metrics tracker.", "\n", "", "metrics_tracker", "=", "TrainingLossesAndMetricsTracker", "(", "losses_to_track", "=", "losses_to_track", ",", "\n", "metrics_to_track", "=", "metrics_to_track", ",", "\n", "img_wh", "=", "config", ".", "REGRESSOR_IMG_WH", ",", "\n", "log_path", "=", "log_path", ",", "\n", "load_logs", "=", "load_logs", ",", "\n", "current_epoch", "=", "current_epoch", ")", "\n", "\n", "# Loading mean shape (for augmentation function)", "\n", "mean_smpl", "=", "np", ".", "load", "(", "config", ".", "SMPL_MEAN_PARAMS_PATH", ")", "\n", "mean_shape", "=", "torch", ".", "from_numpy", "(", "mean_smpl", "[", "'shape'", "]", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Starting training loop", "\n", "for", "epoch", "in", "range", "(", "current_epoch", ",", "num_epochs", ")", ":", "\n", "        ", "print", "(", "'\\nEpoch {}/{}'", ".", "format", "(", "epoch", ",", "num_epochs", "-", "1", ")", ")", "\n", "print", "(", "'-'", "*", "10", ")", "\n", "metrics_tracker", ".", "initialise_loss_metric_sums", "(", ")", "\n", "\n", "# ################################################################################", "\n", "# ----------------------------------- TRAINING -----------------------------------", "\n", "# ################################################################################", "\n", "print", "(", "'Training.'", ")", "\n", "regressor", ".", "train", "(", ")", "\n", "for", "batch_num", ",", "samples_batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ")", ")", ":", "\n", "# ---------------- SYNTHETIC DATA GENERATION ----------------", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# TARGET SMPL PARAMETERS", "\n", "                ", "target_pose", "=", "samples_batch", "[", "'pose'", "]", "\n", "target_shape", "=", "samples_batch", "[", "'shape'", "]", "\n", "target_pose", "=", "target_pose", ".", "to", "(", "device", ")", "\n", "target_shape", "=", "target_shape", ".", "to", "(", "device", ")", "\n", "num_train_inputs_in_batch", "=", "target_pose", ".", "shape", "[", "0", "]", "# Same as bs since drop_last=True", "\n", "\n", "# SMPL AND CAM AUGMENTATION", "\n", "target_shape", ",", "target_pose_rotmats", ",", "target_glob_rotmats", "=", "augment_smpl", "(", "\n", "target_shape", ",", "\n", "target_pose", "[", ":", ",", "3", ":", "]", ",", "\n", "target_pose", "[", ":", ",", ":", "3", "]", ",", "\n", "mean_shape", ",", "\n", "smpl_augment_params", ")", "\n", "target_cam_t", "=", "augment_cam_t", "(", "mean_cam_t", ",", "\n", "xy_std", "=", "cam_augment_params", "[", "'xy_std'", "]", ",", "\n", "delta_z_range", "=", "cam_augment_params", "[", "'delta_z_range'", "]", ")", "\n", "\n", "# TARGET VERTICES AND JOINTS", "\n", "target_smpl_output", "=", "smpl_model", "(", "body_pose", "=", "target_pose_rotmats", ",", "\n", "global_orient", "=", "target_glob_rotmats", ",", "\n", "betas", "=", "target_shape", ",", "\n", "pose2rot", "=", "False", ")", "\n", "target_vertices", "=", "target_smpl_output", ".", "vertices", "\n", "target_joints_all", "=", "target_smpl_output", ".", "joints", "\n", "target_joints_h36m", "=", "target_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_H36M_MAP", ",", ":", "]", "\n", "target_joints_h36mlsp", "=", "target_joints_h36m", "[", ":", ",", "config", ".", "H36M_TO_J14", ",", ":", "]", "\n", "target_joints_coco", "=", "target_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_COCO_MAP", ",", ":", "]", "\n", "target_joints2d_coco", "=", "perspective_project_torch", "(", "target_joints_coco", ",", "cam_R", ",", "\n", "target_cam_t", ",", "\n", "cam_K", "=", "cam_K", ")", "\n", "target_reposed_smpl_output", "=", "smpl_model", "(", "betas", "=", "target_shape", ")", "\n", "target_reposed_vertices", "=", "target_reposed_smpl_output", ".", "vertices", "\n", "\n", "if", "proxy_rep_augment_params", "[", "'deviate_verts2D'", "]", ":", "\n", "# Vertex noise augmentation to give noisy proxy representation edges", "\n", "                    ", "target_vertices_for_rendering", "=", "random_verts2D_deviation", "(", "target_vertices", ",", "\n", "delta_verts2d_dev_range", "=", "proxy_rep_augment_params", "[", "'delta_verts2d_dev_range'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "target_vertices_for_rendering", "=", "target_vertices", "\n", "\n", "# INPUT PROXY REPRESENTATION GENERATION", "\n", "", "input", "=", "nmr_parts_renderer", "(", "target_vertices_for_rendering", ",", "target_cam_t", ")", "\n", "\n", "# BBOX AUGMENTATION AND CROPPING", "\n", "if", "bbox_augment_params", "[", "'crop_input'", "]", ":", "\n", "# Crop inputs according to bounding box", "\n", "# + add random scale and centre augmentation", "\n", "                    ", "input", "=", "input", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "target_joints2d_coco", "=", "target_joints2d_coco", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "all_cropped_segs", ",", "all_cropped_joints2D", "=", "batch_crop_seg_to_bounding_box", "(", "\n", "input", ",", "target_joints2d_coco", ",", "\n", "orig_scale_factor", "=", "bbox_augment_params", "[", "'mean_scale_factor'", "]", ",", "\n", "delta_scale_range", "=", "bbox_augment_params", "[", "'delta_scale_range'", "]", ",", "\n", "delta_centre_range", "=", "bbox_augment_params", "[", "'delta_centre_range'", "]", ")", "\n", "resized_input", ",", "resized_joints2D", "=", "batch_resize", "(", "all_cropped_segs", ",", "all_cropped_joints2D", ",", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "input", "=", "torch", ".", "from_numpy", "(", "resized_input", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "target_joints2d_coco", "=", "torch", ".", "from_numpy", "(", "resized_joints2D", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# PROXY REPRESENTATION AUGMENTATION", "\n", "", "input", ",", "target_joints2d_coco_input", "=", "augment_proxy_representation", "(", "input", ",", "\n", "target_joints2d_coco", ",", "\n", "proxy_rep_augment_params", ")", "\n", "\n", "# FINAL INPUT PROXY REPRESENTATION GENERATION WITH JOINT HEATMAPS", "\n", "input", "=", "convert_multiclass_to_binary_labels_torch", "(", "input", ")", "\n", "input", "=", "input", ".", "unsqueeze", "(", "1", ")", "\n", "j2d_heatmaps", "=", "convert_2Djoints_to_gaussian_heatmaps_torch", "(", "target_joints2d_coco_input", ",", "\n", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "j2d_heatmaps", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# ---------------- FORWARD PASS ----------------", "\n", "# (gradients being computed from here on)", "\n", "", "pred_cam_wp", ",", "pred_pose", ",", "pred_shape", "=", "regressor", "(", "input", ")", "\n", "\n", "# Convert pred pose to rotation matrices", "\n", "if", "pred_pose", ".", "shape", "[", "-", "1", "]", "==", "24", "*", "3", ":", "\n", "                ", "pred_pose_rotmats", "=", "batch_rodrigues", "(", "pred_pose", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", "\n", "pred_pose_rotmats", "=", "pred_pose_rotmats", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "", "elif", "pred_pose", ".", "shape", "[", "-", "1", "]", "==", "24", "*", "6", ":", "\n", "                ", "pred_pose_rotmats", "=", "rot6d_to_rotmat", "(", "pred_pose", ".", "contiguous", "(", ")", ")", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "\n", "# PREDICTED VERTICES AND JOINTS", "\n", "", "pred_smpl_output", "=", "smpl_model", "(", "body_pose", "=", "pred_pose_rotmats", "[", ":", ",", "1", ":", "]", ",", "\n", "global_orient", "=", "pred_pose_rotmats", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", ",", "\n", "betas", "=", "pred_shape", ",", "\n", "pose2rot", "=", "False", ")", "\n", "pred_vertices", "=", "pred_smpl_output", ".", "vertices", "\n", "pred_joints_all", "=", "pred_smpl_output", ".", "joints", "\n", "pred_joints_h36m", "=", "pred_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_H36M_MAP", ",", ":", "]", "\n", "pred_joints_h36mlsp", "=", "pred_joints_h36m", "[", ":", ",", "config", ".", "H36M_TO_J14", ",", ":", "]", "\n", "pred_joints_coco", "=", "pred_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_COCO_MAP", ",", ":", "]", "\n", "pred_joints2d_coco", "=", "orthographic_project_torch", "(", "pred_joints_coco", ",", "pred_cam_wp", ")", "\n", "pred_reposed_smpl_output", "=", "smpl_model", "(", "betas", "=", "pred_shape", ")", "\n", "pred_reposed_vertices", "=", "pred_reposed_smpl_output", ".", "vertices", "\n", "\n", "# ---------------- LOSS ----------------", "\n", "# Concatenate target pose and global rotmats for loss function", "\n", "target_pose_rotmats", "=", "torch", ".", "cat", "(", "[", "target_glob_rotmats", ",", "target_pose_rotmats", "]", ",", "\n", "dim", "=", "1", ")", "\n", "# Check joints visibility", "\n", "target_joints2d_vis_coco", "=", "check_joints2d_visibility_torch", "(", "target_joints2d_coco", ",", "\n", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "\n", "pred_dict_for_loss", "=", "{", "'joints2D'", ":", "pred_joints2d_coco", ",", "\n", "'verts'", ":", "pred_vertices", ",", "\n", "'shape_params'", ":", "pred_shape", ",", "\n", "'pose_params_rot_matrices'", ":", "pred_pose_rotmats", ",", "\n", "'joints3D'", ":", "pred_joints_h36mlsp", "}", "\n", "target_dict_for_loss", "=", "{", "'joints2D'", ":", "target_joints2d_coco", ",", "\n", "'verts'", ":", "target_vertices", ",", "\n", "'shape_params'", ":", "target_shape", ",", "\n", "'pose_params_rot_matrices'", ":", "target_pose_rotmats", ",", "\n", "'joints3D'", ":", "target_joints_h36mlsp", ",", "\n", "'vis'", ":", "target_joints2d_vis_coco", "}", "\n", "\n", "# ---------------- BACKWARD PASS ----------------", "\n", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ",", "task_losses_dict", "=", "criterion", "(", "target_dict_for_loss", ",", "pred_dict_for_loss", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimiser", ".", "step", "(", ")", "\n", "\n", "# ---------------- TRACK LOSS AND METRICS ----------------", "\n", "metrics_tracker", ".", "update_per_batch", "(", "'train'", ",", "loss", ",", "task_losses_dict", ",", "\n", "pred_dict_for_loss", ",", "target_dict_for_loss", ",", "\n", "num_train_inputs_in_batch", ",", "\n", "pred_reposed_vertices", "=", "pred_reposed_vertices", ",", "\n", "target_reposed_vertices", "=", "target_reposed_vertices", ")", "\n", "\n", "# ##################################################################################", "\n", "# ----------------------------------- VALIDATION -----------------------------------", "\n", "# ##################################################################################", "\n", "", "print", "(", "'Validation.'", ")", "\n", "regressor", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch_num", ",", "samples_batch", "in", "enumerate", "(", "tqdm", "(", "val_dataloader", ")", ")", ":", "\n", "# ---------------- SYNTHETIC DATA GENERATION ----------------", "\n", "# TARGET SMPL PARAMETERS", "\n", "                ", "target_pose", "=", "samples_batch", "[", "'pose'", "]", "\n", "target_shape", "=", "samples_batch", "[", "'shape'", "]", "\n", "target_pose", "=", "target_pose", ".", "to", "(", "device", ")", "\n", "target_shape", "=", "target_shape", ".", "to", "(", "device", ")", "\n", "num_val_inputs_in_batch", "=", "target_pose", ".", "shape", "[", "0", "]", "# Same as bs since drop_last=True", "\n", "\n", "# TARGET VERTICES AND JOINTS", "\n", "target_smpl_output", "=", "smpl_model", "(", "body_pose", "=", "target_pose", "[", ":", ",", "3", ":", "]", ",", "\n", "global_orient", "=", "target_pose", "[", ":", ",", ":", "3", "]", ",", "\n", "betas", "=", "target_shape", ")", "\n", "target_vertices", "=", "target_smpl_output", ".", "vertices", "\n", "target_joints_all", "=", "target_smpl_output", ".", "joints", "\n", "target_joints_h36m", "=", "target_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_H36M_MAP", ",", ":", "]", "\n", "target_joints_h36mlsp", "=", "target_joints_h36m", "[", ":", ",", "config", ".", "H36M_TO_J14", ",", ":", "]", "\n", "target_joints_coco", "=", "target_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_COCO_MAP", ",", ":", "]", "\n", "target_joints2d_coco", "=", "perspective_project_torch", "(", "target_joints_coco", ",", "cam_R", ",", "\n", "mean_cam_t", ",", "\n", "cam_K", "=", "cam_K", ")", "\n", "target_reposed_smpl_output", "=", "smpl_model", "(", "betas", "=", "target_shape", ")", "\n", "target_reposed_vertices", "=", "target_reposed_smpl_output", ".", "vertices", "\n", "\n", "# INPUT PROXY REPRESENTATION GENERATION", "\n", "input", "=", "nmr_parts_renderer", "(", "target_vertices", ",", "mean_cam_t", ")", "\n", "\n", "# BBOX AUGMENTATION AND CROPPING", "\n", "if", "bbox_augment_params", "[", "'crop_input'", "]", ":", "# Crop inputs according to bounding box", "\n", "                    ", "input", "=", "input", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "target_joints2d_coco", "=", "target_joints2d_coco", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "all_cropped_segs", ",", "all_cropped_joints2D", "=", "batch_crop_seg_to_bounding_box", "(", "\n", "input", ",", "target_joints2d_coco", ",", "\n", "orig_scale_factor", "=", "bbox_augment_params", "[", "'mean_scale_factor'", "]", ",", "\n", "delta_scale_range", "=", "None", ",", "\n", "delta_centre_range", "=", "None", ")", "\n", "resized_input", ",", "resized_joints2D", "=", "batch_resize", "(", "all_cropped_segs", ",", "\n", "all_cropped_joints2D", ",", "\n", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "input", "=", "torch", ".", "from_numpy", "(", "resized_input", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "target_joints2d_coco", "=", "torch", ".", "from_numpy", "(", "resized_joints2D", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# FINAL INPUT PROXY REPRESENTATION GENERATION WITH JOINT HEATMAPS", "\n", "", "input", "=", "convert_multiclass_to_binary_labels_torch", "(", "input", ")", "\n", "input", "=", "input", ".", "unsqueeze", "(", "1", ")", "\n", "j2d_heatmaps", "=", "convert_2Djoints_to_gaussian_heatmaps_torch", "(", "target_joints2d_coco", ",", "\n", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "j2d_heatmaps", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# ---------------- FORWARD PASS ----------------", "\n", "pred_cam_wp", ",", "pred_pose", ",", "pred_shape", "=", "regressor", "(", "input", ")", "\n", "# Convert pred pose to rotation matrices", "\n", "if", "pred_pose", ".", "shape", "[", "-", "1", "]", "==", "24", "*", "3", ":", "\n", "                    ", "pred_pose_rotmats", "=", "batch_rodrigues", "(", "pred_pose", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", "\n", "pred_pose_rotmats", "=", "pred_pose_rotmats", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "", "elif", "pred_pose", ".", "shape", "[", "-", "1", "]", "==", "24", "*", "6", ":", "\n", "                    ", "pred_pose_rotmats", "=", "rot6d_to_rotmat", "(", "pred_pose", ".", "contiguous", "(", ")", ")", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "\n", "# PREDICTED VERTICES AND JOINTS", "\n", "", "pred_smpl_output", "=", "smpl_model", "(", "body_pose", "=", "pred_pose_rotmats", "[", ":", ",", "1", ":", "]", ",", "\n", "global_orient", "=", "pred_pose_rotmats", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", ",", "\n", "betas", "=", "pred_shape", ",", "\n", "pose2rot", "=", "False", ")", "\n", "pred_vertices", "=", "pred_smpl_output", ".", "vertices", "\n", "pred_joints_all", "=", "pred_smpl_output", ".", "joints", "\n", "pred_joints_h36m", "=", "pred_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_H36M_MAP", ",", ":", "]", "\n", "pred_joints_h36mlsp", "=", "pred_joints_h36m", "[", ":", ",", "config", ".", "H36M_TO_J14", ",", ":", "]", "\n", "pred_joints_coco", "=", "pred_joints_all", "[", ":", ",", "config", ".", "ALL_JOINTS_TO_COCO_MAP", ",", ":", "]", "\n", "pred_joints2d_coco", "=", "orthographic_project_torch", "(", "pred_joints_coco", ",", "pred_cam_wp", ")", "\n", "pred_reposed_smpl_output", "=", "smpl_model", "(", "betas", "=", "pred_shape", ")", "\n", "pred_reposed_vertices", "=", "pred_reposed_smpl_output", ".", "vertices", "\n", "\n", "# ---------------- LOSS ----------------", "\n", "# Convert pose parameters to rotation matrices for loss function", "\n", "target_pose_rotmats", "=", "batch_rodrigues", "(", "target_pose", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", "\n", "target_pose_rotmats", "=", "target_pose_rotmats", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "\n", "# Check joints visibility", "\n", "target_joints2d_vis_coco", "=", "check_joints2d_visibility_torch", "(", "target_joints2d_coco", ",", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "\n", "pred_dict_for_loss", "=", "{", "'joints2D'", ":", "pred_joints2d_coco", ",", "\n", "'verts'", ":", "pred_vertices", ",", "\n", "'shape_params'", ":", "pred_shape", ",", "\n", "'pose_params_rot_matrices'", ":", "pred_pose_rotmats", ",", "\n", "'joints3D'", ":", "pred_joints_h36mlsp", "}", "\n", "target_dict_for_loss", "=", "{", "'joints2D'", ":", "target_joints2d_coco", ",", "\n", "'verts'", ":", "target_vertices", ",", "\n", "'shape_params'", ":", "target_shape", ",", "\n", "'pose_params_rot_matrices'", ":", "target_pose_rotmats", ",", "\n", "'joints3D'", ":", "target_joints_h36mlsp", ",", "\n", "'vis'", ":", "target_joints2d_vis_coco", "}", "\n", "\n", "val_loss", ",", "val_task_losses_dict", "=", "criterion", "(", "target_dict_for_loss", ",", "\n", "pred_dict_for_loss", ")", "\n", "\n", "# ---------------- TRACK LOSS AND METRICS ----------------", "\n", "metrics_tracker", ".", "update_per_batch", "(", "'val'", ",", "val_loss", ",", "val_task_losses_dict", ",", "\n", "pred_dict_for_loss", ",", "target_dict_for_loss", ",", "\n", "num_val_inputs_in_batch", ",", "\n", "pred_reposed_vertices", "=", "pred_reposed_vertices", ",", "\n", "target_reposed_vertices", "=", "target_reposed_vertices", ")", "\n", "\n", "# ----------------------- UPDATING LOSS AND METRICS HISTORY -----------------------", "\n", "", "", "metrics_tracker", ".", "update_per_epoch", "(", ")", "\n", "\n", "# ----------------------------------- SAVING -----------------------------------", "\n", "save_model_weights_this_epoch", "=", "metrics_tracker", ".", "determine_save_model_weights_this_epoch", "(", "save_val_metrics", ",", "\n", "best_epoch_val_metrics", ")", "\n", "\n", "if", "save_model_weights_this_epoch", ":", "\n", "            ", "for", "metric", "in", "save_val_metrics", ":", "\n", "                ", "best_epoch_val_metrics", "[", "metric", "]", "=", "metrics_tracker", ".", "history", "[", "'val_'", "+", "metric", "]", "[", "-", "1", "]", "\n", "", "print", "(", "\"Best epoch val metrics updated to \"", ",", "best_epoch_val_metrics", ")", "\n", "best_model_wts", "=", "copy", ".", "deepcopy", "(", "regressor", ".", "state_dict", "(", ")", ")", "\n", "best_epoch", "=", "epoch", "\n", "print", "(", "\"Best model weights updated!\"", ")", "\n", "\n", "", "if", "epoch", "%", "epochs_per_save", "==", "0", ":", "\n", "# Saving current epoch num, best epoch num, best validation metrics (occurred in best", "\n", "# epoch num), current regressor state_dict, best regressor state_dict, current", "\n", "# optimiser state dict and current criterion state_dict (i.e. multi-task loss weights).", "\n", "            ", "save_dict", "=", "{", "'epoch'", ":", "epoch", ",", "\n", "'best_epoch'", ":", "best_epoch", ",", "\n", "'best_epoch_val_metrics'", ":", "best_epoch_val_metrics", ",", "\n", "'model_state_dict'", ":", "regressor", ".", "state_dict", "(", ")", ",", "\n", "'best_model_state_dict'", ":", "best_model_wts", ",", "\n", "'optimiser_state_dict'", ":", "optimiser", ".", "state_dict", "(", ")", ",", "\n", "'criterion_state_dict'", ":", "criterion", ".", "state_dict", "(", ")", "}", "\n", "torch", ".", "save", "(", "save_dict", ",", "\n", "model_save_path", "+", "'_epoch{}'", ".", "format", "(", "epoch", ")", "+", "'.tar'", ")", "\n", "print", "(", "'Model saved! Best Val Metrics:\\n'", ",", "\n", "best_epoch_val_metrics", ",", "\n", "'\\nin epoch {}'", ".", "format", "(", "best_epoch", ")", ")", "\n", "\n", "", "", "print", "(", "'Training Completed. Best Val Metrics:\\n'", ",", "\n", "best_epoch_val_metrics", ")", "\n", "\n", "regressor", ".", "load_state_dict", "(", "best_model_wts", ")", "\n", "return", "regressor", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_joints2D.get_largest_centred_bounding_box": [[5, 31], ["numpy.argsort", "abs", "abs"], "function", ["None"], ["def", "get_largest_centred_bounding_box", "(", "bboxes", ",", "orig_w", ",", "orig_h", ")", ":", "\n", "    ", "\"\"\"\n    Given an array of bounding boxes, return the index of the largest + roughly-centred\n    bounding box.\n    :param bboxes: (N, 4) array of [x1 y1 x2 y2] bounding boxes\n    :param orig_w: original image width\n    :param orig_h: original image height\n    \"\"\"", "\n", "bboxes_area", "=", "(", "bboxes", "[", ":", ",", "2", "]", "-", "bboxes", "[", ":", ",", "0", "]", ")", "*", "(", "bboxes", "[", ":", ",", "3", "]", "-", "bboxes", "[", ":", ",", "1", "]", ")", "\n", "sorted_bbox_indices", "=", "np", ".", "argsort", "(", "bboxes_area", ")", "[", ":", ":", "-", "1", "]", "# Indices of bboxes sorted by area.", "\n", "bbox_found", "=", "False", "\n", "i", "=", "0", "\n", "while", "not", "bbox_found", "and", "i", "<", "sorted_bbox_indices", ".", "shape", "[", "0", "]", ":", "\n", "        ", "bbox_index", "=", "sorted_bbox_indices", "[", "i", "]", "\n", "bbox", "=", "bboxes", "[", "bbox_index", "]", "\n", "bbox_centre", "=", "(", "(", "bbox", "[", "0", "]", "+", "bbox", "[", "2", "]", ")", "/", "2.0", ",", "(", "bbox", "[", "1", "]", "+", "bbox", "[", "3", "]", ")", "/", "2.0", ")", "# Centre (width, height)", "\n", "if", "abs", "(", "bbox_centre", "[", "0", "]", "-", "orig_w", "/", "2.0", ")", "<", "orig_w", "/", "6.0", "and", "abs", "(", "bbox_centre", "[", "1", "]", "-", "orig_h", "/", "2.0", ")", "<", "orig_w", "/", "6.0", ":", "\n", "            ", "largest_centred_bbox_index", "=", "bbox_index", "\n", "bbox_found", "=", "True", "\n", "", "i", "+=", "1", "\n", "\n", "# If can't find bbox sufficiently close to centre, just use biggest bbox as prediction", "\n", "", "if", "not", "bbox_found", ":", "\n", "        ", "largest_centred_bbox_index", "=", "sorted_bbox_indices", "[", "0", "]", "\n", "\n", "", "return", "largest_centred_bbox_index", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_joints2D.predict_joints2D": [[33, 61], ["numpy.copy", "predictor", "outputs[].pred_boxes.tensor.cpu().numpy", "numpy.zeros", "predict_joints2D.get_largest_centred_bounding_box", "outputs[].pred_keypoints.cpu().numpy", "range", "outputs[].pred_boxes.tensor.cpu", "cv2.circle", "cv2.putText", "outputs[].pred_keypoints.cpu", "str"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.get_largest_centred_bounding_box"], ["", "def", "predict_joints2D", "(", "input_image", ",", "predictor", ")", ":", "\n", "    ", "\"\"\"\n    Predicts 2D joints (17 2D joints in COCO convention along with prediction confidence)\n    given a cropped and centred input image.\n    :param input_images: (wh, wh)\n    :param predictor: instance of detectron2 DefaultPredictor class, created with the\n    appropriate config file.\n    \"\"\"", "\n", "image", "=", "np", ".", "copy", "(", "input_image", ")", "\n", "orig_h", ",", "orig_w", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "outputs", "=", "predictor", "(", "image", ")", "# Multiple bboxes + keypoints predictions if there are multiple people in the image", "\n", "bboxes", "=", "outputs", "[", "'instances'", "]", ".", "pred_boxes", ".", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "bboxes", ".", "shape", "[", "0", "]", "==", "0", ":", "# Can't find any people in image", "\n", "        ", "keypoints", "=", "np", ".", "zeros", "(", "(", "17", ",", "3", ")", ")", "\n", "", "else", ":", "\n", "        ", "largest_centred_bbox_index", "=", "get_largest_centred_bounding_box", "(", "bboxes", ",", "orig_w", ",", "orig_h", ")", "# Picks out centred person that is largest in the image.", "\n", "keypoints", "=", "outputs", "[", "'instances'", "]", ".", "pred_keypoints", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "keypoints", "=", "keypoints", "[", "largest_centred_bbox_index", "]", "\n", "\n", "for", "j", "in", "range", "(", "keypoints", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "cv2", ".", "circle", "(", "image", ",", "(", "keypoints", "[", "j", ",", "0", "]", ",", "keypoints", "[", "j", ",", "1", "]", ")", ",", "5", ",", "(", "0", ",", "255", ",", "0", ")", ",", "-", "1", ")", "\n", "font", "=", "cv2", ".", "FONT_HERSHEY_SIMPLEX", "\n", "fontScale", "=", "0.5", "\n", "fontColor", "=", "(", "0", ",", "0", ",", "255", ")", "\n", "cv2", ".", "putText", "(", "image", ",", "str", "(", "j", ")", ",", "(", "keypoints", "[", "j", ",", "0", "]", ",", "keypoints", "[", "j", ",", "1", "]", ")", ",", "\n", "font", ",", "fontScale", ",", "fontColor", ",", "lineType", "=", "2", ")", "\n", "\n", "", "", "return", "keypoints", ",", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_silhouette_pointrend.get_largest_centred_mask": [[7, 37], ["numpy.sum", "numpy.argsort", "numpy.argwhere", "numpy.amin", "numpy.amax", "abs", "abs"], "function", ["None"], ["def", "get_largest_centred_mask", "(", "human_masks", ",", "orig_w", ",", "orig_h", ")", ":", "\n", "    ", "\"\"\"\n    Given an array of human segmentation masks, return the index of the largest +\n    roughly-centred mask.\n    :param human_masks: (N, img_wh, img_wh) human segmentation masks.\n    :param orig_w: original image width\n    :param orig_h: original image height\n    \"\"\"", "\n", "mask_areas", "=", "np", ".", "sum", "(", "human_masks", ",", "axis", "=", "(", "1", ",", "2", ")", ")", "\n", "sorted_mask_indices", "=", "np", ".", "argsort", "(", "mask_areas", ")", "[", ":", ":", "-", "1", "]", "# Indices of masks sorted by area.", "\n", "mask_found", "=", "False", "\n", "i", "=", "0", "\n", "while", "not", "mask_found", "and", "i", "<", "sorted_mask_indices", ".", "shape", "[", "0", "]", ":", "\n", "        ", "mask_index", "=", "sorted_mask_indices", "[", "i", "]", "\n", "mask", "=", "human_masks", "[", "mask_index", ",", ":", ",", ":", "]", "\n", "mask_pixels", "=", "np", ".", "argwhere", "(", "mask", "!=", "0", ")", "\n", "bbox_corners", "=", "np", ".", "amin", "(", "mask_pixels", ",", "axis", "=", "0", ")", ",", "np", ".", "amax", "(", "mask_pixels", ",", "axis", "=", "0", ")", "# (row_min, col_min), (row_max, col_max)", "\n", "bbox_centre", "=", "(", "(", "bbox_corners", "[", "0", "]", "[", "0", "]", "+", "bbox_corners", "[", "1", "]", "[", "0", "]", ")", "/", "2.0", ",", "\n", "(", "bbox_corners", "[", "0", "]", "[", "1", "]", "+", "bbox_corners", "[", "1", "]", "[", "1", "]", ")", "/", "2.0", ")", "# Centre in rows, columns (i.e. height, width)", "\n", "\n", "if", "abs", "(", "bbox_centre", "[", "0", "]", "-", "orig_h", "/", "2.0", ")", "<", "orig_w", "/", "4.0", "and", "abs", "(", "bbox_centre", "[", "1", "]", "-", "orig_w", "/", "2.0", ")", "<", "orig_w", "/", "6.0", ":", "\n", "            ", "largest_centred_mask_index", "=", "mask_index", "\n", "mask_found", "=", "True", "\n", "", "i", "+=", "1", "\n", "\n", "# If can't find mask sufficiently close to centre, just use biggest mask as prediction", "\n", "", "if", "not", "mask_found", ":", "\n", "        ", "largest_centred_mask_index", "=", "sorted_mask_indices", "[", "0", "]", "\n", "\n", "", "return", "largest_centred_mask_index", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_silhouette_pointrend.predict_silhouette_pointrend": [[39, 59], ["human_masks.cpu().detach().numpy.cpu().detach().numpy", "predict_silhouette_pointrend.get_largest_centred_mask", "human_masks[].astype", "cv2.addWeighted", "predictor", "human_masks.cpu().detach().numpy.cpu().detach", "numpy.tile", "human_masks.cpu().detach().numpy.cpu"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_silhouette_pointrend.get_largest_centred_mask"], ["", "def", "predict_silhouette_pointrend", "(", "input_image", ",", "predictor", ")", ":", "\n", "    ", "\"\"\"\n    Predicts human silhouette (binary segmetnation) given a cropped and centred input image.\n    :param input_images: (wh, wh)\n    :param predictor: instance of detectron2 DefaultPredictor class, created with the\n    appropriate config file.\n    \"\"\"", "\n", "orig_h", ",", "orig_w", "=", "input_image", ".", "shape", "[", ":", "2", "]", "\n", "outputs", "=", "predictor", "(", "input_image", ")", "[", "'instances'", "]", "# Multiple silhouette predictions if there are multiple people in the image", "\n", "classes", "=", "outputs", ".", "pred_classes", "\n", "masks", "=", "outputs", ".", "pred_masks", "\n", "human_masks", "=", "masks", "[", "classes", "==", "0", "]", "\n", "human_masks", "=", "human_masks", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "largest_centred_mask_index", "=", "get_largest_centred_mask", "(", "human_masks", ",", "orig_w", ",", "orig_h", ")", "# Picks out centred person that is largest in the image.", "\n", "human_mask", "=", "human_masks", "[", "largest_centred_mask_index", ",", ":", ",", ":", "]", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "overlay_vis", "=", "cv2", ".", "addWeighted", "(", "input_image", ",", "1.0", ",", "\n", "255", "*", "np", ".", "tile", "(", "human_mask", "[", ":", ",", ":", ",", "None", "]", ",", "[", "1", ",", "1", ",", "3", "]", ")", ",", "\n", "0.5", ",", "gamma", "=", "0", ")", "\n", "\n", "return", "human_mask", ",", "overlay_vis", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_3D.setup_detectron2_predictors": [[35, 65], ["detectron2.config.get_cfg", "detectron2.config.get_cfg.merge_from_file", "detectron2.model_zoo.get_checkpoint_url", "detectron2.config.get_cfg.freeze", "detectron2.engine.DefaultPredictor", "detectron2.model_zoo.get_config_file", "detectron2.config.get_cfg", "PointRend.point_rend.add_pointrend_config", "detectron2.config.get_cfg.merge_from_file", "detectron2.config.get_cfg.freeze", "detectron2.engine.DefaultPredictor", "detectron2.config.get_cfg", "DensePose.densepose.add_densepose_config", "detectron2.config.get_cfg.merge_from_file", "detectron2.config.get_cfg.freeze", "detectron2.engine.DefaultPredictor"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.config.add_pointrend_config", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.config.add_densepose_config"], ["def", "setup_detectron2_predictors", "(", "silhouettes_from", "=", "'densepose'", ")", ":", "\n", "# Keypoint-RCNN", "\n", "    ", "kprcnn_config_file", "=", "\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"", "\n", "kprcnn_cfg", "=", "get_cfg", "(", ")", "\n", "kprcnn_cfg", ".", "merge_from_file", "(", "model_zoo", ".", "get_config_file", "(", "kprcnn_config_file", ")", ")", "\n", "kprcnn_cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SCORE_THRESH_TEST", "=", "0.7", "# set threshold for this model", "\n", "kprcnn_cfg", ".", "MODEL", ".", "WEIGHTS", "=", "model_zoo", ".", "get_checkpoint_url", "(", "kprcnn_config_file", ")", "\n", "kprcnn_cfg", ".", "freeze", "(", ")", "\n", "joints2D_predictor", "=", "DefaultPredictor", "(", "kprcnn_cfg", ")", "\n", "\n", "if", "silhouettes_from", "==", "'pointrend'", ":", "\n", "# PointRend-RCNN-R50-FPN", "\n", "        ", "pointrend_config_file", "=", "\"PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\"", "\n", "pointrend_cfg", "=", "get_cfg", "(", ")", "\n", "add_pointrend_config", "(", "pointrend_cfg", ")", "\n", "pointrend_cfg", ".", "merge_from_file", "(", "pointrend_config_file", ")", "\n", "pointrend_cfg", ".", "MODEL", ".", "WEIGHTS", "=", "\"checkpoints/pointrend_rcnn_R_50_fpn.pkl\"", "\n", "pointrend_cfg", ".", "freeze", "(", ")", "\n", "silhouette_predictor", "=", "DefaultPredictor", "(", "pointrend_cfg", ")", "\n", "", "elif", "silhouettes_from", "==", "'densepose'", ":", "\n", "# DensePose-RCNN-R101-FPN", "\n", "        ", "densepose_config_file", "=", "\"DensePose/configs/densepose_rcnn_R_101_FPN_s1x.yaml\"", "\n", "densepose_cfg", "=", "get_cfg", "(", ")", "\n", "add_densepose_config", "(", "densepose_cfg", ")", "\n", "densepose_cfg", ".", "merge_from_file", "(", "densepose_config_file", ")", "\n", "densepose_cfg", ".", "MODEL", ".", "WEIGHTS", "=", "\"checkpoints/densepose_rcnn_R_101_fpn_s1x.pkl\"", "\n", "densepose_cfg", ".", "freeze", "(", ")", "\n", "silhouette_predictor", "=", "DefaultPredictor", "(", "densepose_cfg", ")", "\n", "\n", "", "return", "joints2D_predictor", ",", "silhouette_predictor", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_3D.create_proxy_representation": [[67, 77], ["utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps", "numpy.concatenate", "numpy.transpose", "joints2D.astype"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_2Djoints_to_gaussian_heatmaps"], ["", "def", "create_proxy_representation", "(", "silhouette", ",", "\n", "joints2D", ",", "\n", "out_wh", ")", ":", "\n", "\n", "    ", "heatmaps", "=", "convert_2Djoints_to_gaussian_heatmaps", "(", "joints2D", ".", "astype", "(", "np", ".", "int16", ")", ",", "\n", "out_wh", ")", "\n", "proxy_rep", "=", "np", ".", "concatenate", "(", "[", "silhouette", "[", ":", ",", ":", ",", "None", "]", ",", "heatmaps", "]", ",", "axis", "=", "-", "1", ")", "\n", "proxy_rep", "=", "np", ".", "transpose", "(", "proxy_rep", ",", "[", "2", ",", "0", ",", "1", "]", ")", "# (C, out_wh, out_WH)", "\n", "\n", "return", "proxy_rep", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_3D.predict_3D": [[79, 184], ["predict_3D.setup_detectron2_predictors", "models.smpl_official.SMPL().to", "os.path.isdir", "renderers.weak_perspective_pyrender_renderer.Renderer", "models.smpl_official.SMPL", "print", "cv2.imread", "utils.image_utils.pad_to_square", "cv2.resize", "predict.predict_joints2D.predict_joints2D", "utils.image_utils.crop_and_resize_silhouette_joints", "predict_3D.create_proxy_representation", "torch.from_numpy().float().to", "regressor.eval", "matplotlib.figure", "matplotlib.imshow", "matplotlib.scatter", "matplotlib.gca().set_axis_off", "matplotlib.subplots_adjust", "matplotlib.margins", "matplotlib.gca().xaxis.set_major_locator", "matplotlib.gca().yaxis.set_major_locator", "matplotlib.savefig", "sorted", "os.path.join", "predict.predict_silhouette_pointrend.predict_silhouette_pointrend", "torch.no_grad", "regressor", "SMPL().to.", "utils.cam_utils.orthographic_project_torch", "utils.joints2d_utils.undo_keypoint_normalisation", "SMPL().to.", "pred_vertices.cpu().detach().numpy", "utils.joints2d_utils.undo_keypoint_normalisation.cpu().detach().numpy", "pred_reposed_vertices.cpu().detach().numpy", "pred_cam_wp.cpu().detach().numpy", "os.path.isdir", "os.makedirs", "matplotlib.NullLocator", "matplotlib.NullLocator", "os.path.join", "renderers.weak_perspective_pyrender_renderer.Renderer.render", "renderers.weak_perspective_pyrender_renderer.Renderer.render", "cv2.imwrite", "cv2.imwrite", "cv2.imwrite", "cv2.imwrite", "os.listdir", "f.endswith", "f.endswith", "predict.predict_densepose.predict_densepose", "utils.label_conversions.convert_multiclass_to_binary_labels", "torch.from_numpy().float", "smplx.lbs.batch_rodrigues", "rot6d_to_rotmat().view.view", "os.path.join", "os.path.join", "matplotlib.gca", "os.path.isdir", "os.makedirs", "os.path.join", "os.path.join", "os.path.isdir", "os.makedirs", "os.path.join", "os.path.join", "pred_pose.contiguous().view", "utils.rigid_transform_utils.rot6d_to_rotmat().view", "pred_pose_rotmats[].unsqueeze", "pred_vertices.cpu().detach", "utils.joints2d_utils.undo_keypoint_normalisation.cpu().detach", "pred_reposed_vertices.cpu().detach", "pred_cam_wp.cpu().detach", "matplotlib.gca", "matplotlib.gca", "numpy.array", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "torch.from_numpy", "pred_pose.contiguous", "utils.rigid_transform_utils.rot6d_to_rotmat", "pred_vertices.cpu", "utils.joints2d_utils.undo_keypoint_normalisation.cpu", "pred_reposed_vertices.cpu", "pred_cam_wp.cpu", "pred_pose.contiguous"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_3D.setup_detectron2_predictors", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.pad_to_square", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.resize", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_joints2D.predict_joints2D", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.image_utils.crop_and_resize_silhouette_joints", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_3D.create_proxy_representation", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_silhouette_pointrend.predict_silhouette_pointrend", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.cam_utils.orthographic_project_torch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.undo_keypoint_normalisation", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.Renderer.render", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.renderers.weak_perspective_pyrender_renderer.Renderer.render", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.predict_densepose", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.label_conversions.convert_multiclass_to_binary_labels", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.rigid_transform_utils.rot6d_to_rotmat"], ["", "def", "predict_3D", "(", "input", ",", "\n", "regressor", ",", "\n", "device", ",", "\n", "silhouettes_from", "=", "'densepose'", ",", "\n", "proxy_rep_input_wh", "=", "512", ",", "\n", "save_proxy_vis", "=", "True", ",", "\n", "render_vis", "=", "True", ")", ":", "\n", "\n", "# Set-up proxy representation predictors.", "\n", "    ", "joints2D_predictor", ",", "silhouette_predictor", "=", "setup_detectron2_predictors", "(", "silhouettes_from", "=", "silhouettes_from", ")", "\n", "\n", "# Set-up SMPL model.", "\n", "smpl", "=", "SMPL", "(", "config", ".", "SMPL_MODEL_DIR", ",", "batch_size", "=", "1", ")", ".", "to", "(", "device", ")", "\n", "\n", "if", "render_vis", ":", "\n", "# Set-up renderer for visualisation.", "\n", "        ", "wp_renderer", "=", "Renderer", "(", "resolution", "=", "(", "proxy_rep_input_wh", ",", "proxy_rep_input_wh", ")", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "input", ")", ":", "\n", "        ", "image_fnames", "=", "[", "f", "for", "f", "in", "sorted", "(", "os", ".", "listdir", "(", "input", ")", ")", "if", "f", ".", "endswith", "(", "'.png'", ")", "or", "\n", "f", ".", "endswith", "(", "'.jpg'", ")", "]", "\n", "for", "fname", "in", "image_fnames", ":", "\n", "            ", "print", "(", "\"Predicting on:\"", ",", "fname", ")", "\n", "image", "=", "cv2", ".", "imread", "(", "os", ".", "path", ".", "join", "(", "input", ",", "fname", ")", ")", "\n", "# Pre-process for 2D detectors", "\n", "image", "=", "pad_to_square", "(", "image", ")", "\n", "image", "=", "cv2", ".", "resize", "(", "image", ",", "(", "proxy_rep_input_wh", ",", "proxy_rep_input_wh", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_LINEAR", ")", "\n", "# Predict 2D", "\n", "joints2D", ",", "joints2D_vis", "=", "predict_joints2D", "(", "image", ",", "joints2D_predictor", ")", "\n", "if", "silhouettes_from", "==", "'pointrend'", ":", "\n", "                ", "silhouette", ",", "silhouette_vis", "=", "predict_silhouette_pointrend", "(", "image", ",", "\n", "silhouette_predictor", ")", "\n", "", "elif", "silhouettes_from", "==", "'densepose'", ":", "\n", "                ", "silhouette", ",", "silhouette_vis", "=", "predict_densepose", "(", "image", ",", "silhouette_predictor", ")", "\n", "silhouette", "=", "convert_multiclass_to_binary_labels", "(", "silhouette", ")", "\n", "# Crop around silhouette", "\n", "", "silhouette", ",", "joints2D", ",", "image", "=", "crop_and_resize_silhouette_joints", "(", "silhouette", ",", "\n", "joints2D", ",", "\n", "out_wh", "=", "config", ".", "REGRESSOR_IMG_WH", ",", "\n", "image", "=", "image", ",", "\n", "image_out_wh", "=", "proxy_rep_input_wh", ",", "\n", "bbox_scale_factor", "=", "1.2", ")", "\n", "# Create proxy representation", "\n", "proxy_rep", "=", "create_proxy_representation", "(", "silhouette", ",", "joints2D", ",", "\n", "out_wh", "=", "config", ".", "REGRESSOR_IMG_WH", ")", "\n", "proxy_rep", "=", "proxy_rep", "[", "None", ",", ":", ",", ":", ",", ":", "]", "# add batch dimension", "\n", "proxy_rep", "=", "torch", ".", "from_numpy", "(", "proxy_rep", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Predict 3D", "\n", "regressor", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "pred_cam_wp", ",", "pred_pose", ",", "pred_shape", "=", "regressor", "(", "proxy_rep", ")", "\n", "# Convert pred pose to rotation matrices", "\n", "if", "pred_pose", ".", "shape", "[", "-", "1", "]", "==", "24", "*", "3", ":", "\n", "                    ", "pred_pose_rotmats", "=", "batch_rodrigues", "(", "pred_pose", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", ")", "\n", "pred_pose_rotmats", "=", "pred_pose_rotmats", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "", "elif", "pred_pose", ".", "shape", "[", "-", "1", "]", "==", "24", "*", "6", ":", "\n", "                    ", "pred_pose_rotmats", "=", "rot6d_to_rotmat", "(", "pred_pose", ".", "contiguous", "(", ")", ")", ".", "view", "(", "-", "1", ",", "24", ",", "3", ",", "3", ")", "\n", "\n", "", "pred_smpl_output", "=", "smpl", "(", "body_pose", "=", "pred_pose_rotmats", "[", ":", ",", "1", ":", "]", ",", "\n", "global_orient", "=", "pred_pose_rotmats", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", ",", "\n", "betas", "=", "pred_shape", ",", "\n", "pose2rot", "=", "False", ")", "\n", "pred_vertices", "=", "pred_smpl_output", ".", "vertices", "\n", "pred_vertices2d", "=", "orthographic_project_torch", "(", "pred_vertices", ",", "pred_cam_wp", ")", "\n", "pred_vertices2d", "=", "undo_keypoint_normalisation", "(", "pred_vertices2d", ",", "\n", "proxy_rep_input_wh", ")", "\n", "\n", "pred_reposed_smpl_output", "=", "smpl", "(", "betas", "=", "pred_shape", ")", "\n", "pred_reposed_vertices", "=", "pred_reposed_smpl_output", ".", "vertices", "\n", "\n", "# Numpy-fying", "\n", "", "pred_vertices", "=", "pred_vertices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "pred_vertices2d", "=", "pred_vertices2d", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "pred_reposed_vertices", "=", "pred_reposed_vertices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "pred_cam_wp", "=", "pred_cam_wp", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'verts_vis'", ")", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'verts_vis'", ")", ")", "\n", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "imshow", "(", "image", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ")", "\n", "plt", ".", "scatter", "(", "pred_vertices2d", "[", ":", ",", "0", "]", ",", "pred_vertices2d", "[", ":", ",", "1", "]", ",", "s", "=", "0.3", ")", "\n", "plt", ".", "gca", "(", ")", ".", "set_axis_off", "(", ")", "\n", "plt", ".", "subplots_adjust", "(", "top", "=", "1", ",", "bottom", "=", "0", ",", "right", "=", "1", ",", "left", "=", "0", ",", "hspace", "=", "0", ",", "wspace", "=", "0", ")", "\n", "plt", ".", "margins", "(", "0", ",", "0", ")", "\n", "plt", ".", "gca", "(", ")", ".", "xaxis", ".", "set_major_locator", "(", "plt", ".", "NullLocator", "(", ")", ")", "\n", "plt", ".", "gca", "(", ")", ".", "yaxis", ".", "set_major_locator", "(", "plt", ".", "NullLocator", "(", ")", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'verts_vis'", ",", "'verts_'", "+", "fname", ")", ")", "\n", "\n", "if", "render_vis", ":", "\n", "                ", "rend_img", "=", "wp_renderer", ".", "render", "(", "verts", "=", "pred_vertices", ",", "cam", "=", "pred_cam_wp", ",", "img", "=", "image", ")", "\n", "rend_reposed_img", "=", "wp_renderer", ".", "render", "(", "verts", "=", "pred_reposed_vertices", ",", "\n", "cam", "=", "np", ".", "array", "(", "[", "0.8", ",", "0.", ",", "-", "0.2", "]", ")", ",", "\n", "angle", "=", "180", ",", "\n", "axis", "=", "[", "1", ",", "0", ",", "0", "]", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'rend_vis'", ")", ")", ":", "\n", "                    ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'rend_vis'", ")", ")", "\n", "", "cv2", ".", "imwrite", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'rend_vis'", ",", "'rend_'", "+", "fname", ")", ",", "rend_img", ")", "\n", "cv2", ".", "imwrite", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'rend_vis'", ",", "'reposed_'", "+", "fname", ")", ",", "rend_reposed_img", ")", "\n", "", "if", "save_proxy_vis", ":", "\n", "                ", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'proxy_vis'", ")", ")", ":", "\n", "                    ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'proxy_vis'", ")", ")", "\n", "", "cv2", ".", "imwrite", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'proxy_vis'", ",", "'silhouette_'", "+", "fname", ")", ",", "silhouette_vis", ")", "\n", "cv2", ".", "imwrite", "(", "os", ".", "path", ".", "join", "(", "input", ",", "'proxy_vis'", ",", "'joints2D_'", "+", "fname", ")", ",", "joints2D_vis", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.apply_colormap": [[18, 46], ["np.clip.astype", "matplotlib.get_cmap", "plt.get_cmap.", "float", "numpy.clip", "numpy.min", "float", "numpy.clip", "numpy.max"], "function", ["None"], ["def", "apply_colormap", "(", "image", ",", "vmin", "=", "None", ",", "vmax", "=", "None", ",", "cmap", "=", "'viridis'", ",", "cmap_seed", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Apply a matplotlib colormap to an image.\n\n    This method will preserve the exact image size. `cmap` can be either a\n    matplotlib colormap name, a discrete number, or a colormap instance. If it\n    is a number, a discrete colormap will be generated based on the HSV\n    colorspace. The permutation of colors is random and can be controlled with\n    the `cmap_seed`. The state of the RNG is preserved.\n    \"\"\"", "\n", "image", "=", "image", ".", "astype", "(", "\"float64\"", ")", "# Returns a copy.", "\n", "# Normalization.", "\n", "if", "vmin", "is", "not", "None", ":", "\n", "        ", "imin", "=", "float", "(", "vmin", ")", "\n", "image", "=", "np", ".", "clip", "(", "image", ",", "vmin", ",", "sys", ".", "float_info", ".", "max", ")", "\n", "", "else", ":", "\n", "        ", "imin", "=", "np", ".", "min", "(", "image", ")", "\n", "", "if", "vmax", "is", "not", "None", ":", "\n", "        ", "imax", "=", "float", "(", "vmax", ")", "\n", "image", "=", "np", ".", "clip", "(", "image", ",", "-", "sys", ".", "float_info", ".", "max", ",", "vmax", ")", "\n", "", "else", ":", "\n", "        ", "imax", "=", "np", ".", "max", "(", "image", ")", "\n", "", "image", "-=", "imin", "\n", "image", "/=", "(", "imax", "-", "imin", ")", "\n", "# Visualization.", "\n", "cmap_", "=", "plt", ".", "get_cmap", "(", "cmap", ")", "\n", "vis", "=", "cmap_", "(", "image", ",", "bytes", "=", "True", ")", "\n", "return", "vis", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.get_largest_centred_bounding_box": [[48, 74], ["numpy.argsort", "abs", "abs"], "function", ["None"], ["", "def", "get_largest_centred_bounding_box", "(", "bboxes", ",", "orig_w", ",", "orig_h", ")", ":", "\n", "    ", "\"\"\"\n    Given an array of bounding boxes, return the index of the largest + roughly-centred\n    bounding box.\n    :param bboxes: (N, 4) array of [x1 y1 x2 y2] bounding boxes\n    :param orig_w: original image width\n    :param orig_h: original image height\n    \"\"\"", "\n", "bboxes_area", "=", "(", "bboxes", "[", ":", ",", "2", "]", "-", "bboxes", "[", ":", ",", "0", "]", ")", "*", "(", "bboxes", "[", ":", ",", "3", "]", "-", "bboxes", "[", ":", ",", "1", "]", ")", "\n", "sorted_bbox_indices", "=", "np", ".", "argsort", "(", "bboxes_area", ")", "[", ":", ":", "-", "1", "]", "# Indices of bboxes sorted by area.", "\n", "bbox_found", "=", "False", "\n", "i", "=", "0", "\n", "while", "not", "bbox_found", "and", "i", "<", "sorted_bbox_indices", ".", "shape", "[", "0", "]", ":", "\n", "        ", "bbox_index", "=", "sorted_bbox_indices", "[", "i", "]", "\n", "bbox", "=", "bboxes", "[", "bbox_index", "]", "\n", "bbox_centre", "=", "(", "(", "bbox", "[", "0", "]", "+", "bbox", "[", "2", "]", ")", "/", "2.0", ",", "(", "bbox", "[", "1", "]", "+", "bbox", "[", "3", "]", ")", "/", "2.0", ")", "# Centre (width, height)", "\n", "if", "abs", "(", "bbox_centre", "[", "0", "]", "-", "orig_w", "/", "2.0", ")", "<", "orig_w", "/", "5.0", "and", "abs", "(", "bbox_centre", "[", "1", "]", "-", "orig_h", "/", "2.0", ")", "<", "orig_w", "/", "5.0", ":", "\n", "            ", "largest_centred_bbox_index", "=", "bbox_index", "\n", "bbox_found", "=", "True", "\n", "", "i", "+=", "1", "\n", "\n", "# If can't find bbox sufficiently close to centre, just use biggest bbox as prediction", "\n", "", "if", "not", "bbox_found", ":", "\n", "        ", "largest_centred_bbox_index", "=", "sorted_bbox_indices", "[", "0", "]", "\n", "\n", "", "return", "largest_centred_bbox_index", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.predict_densepose": [[76, 117], ["outputs.pred_boxes.tensor.cpu", "detectron2.structures.boxes.BoxMode.convert", "bboxes.cpu().detach().numpy.cpu().detach().numpy", "predict_densepose.get_largest_centred_bounding_box", "outputs.pred_densepose.to_result", "DensePose.densepose.structures.DensePoseResult.decode_png_data", "numpy.zeros", "predict_densepose.apply_colormap", "numpy.zeros", "cv2.addWeighted", "predictor", "bboxes.cpu().detach().numpy.cpu().detach", "int", "int", "int", "int", "bboxes.cpu().detach().numpy.cpu"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.get_largest_centred_bounding_box", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.to_result", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.decode_png_data", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.predict.predict_densepose.apply_colormap"], ["", "def", "predict_densepose", "(", "input_image", ",", "predictor", ")", ":", "\n", "    ", "\"\"\"\n    Predicts densepose output given a cropped and centred input image.\n    :param input_images: (wh, wh)\n    :param predictor: instance of detectron2 DefaultPredictor class, created with the\n    appropriate config file.\n    \"\"\"", "\n", "orig_h", ",", "orig_w", "=", "input_image", ".", "shape", "[", ":", "2", "]", "\n", "outputs", "=", "predictor", "(", "input_image", ")", "[", "\"instances\"", "]", "\n", "bboxes", "=", "outputs", ".", "pred_boxes", ".", "tensor", ".", "cpu", "(", ")", "# Multiple densepose predictions if there are multiple people in the image", "\n", "bboxes_XYWH", "=", "BoxMode", ".", "convert", "(", "bboxes", ",", "BoxMode", ".", "XYXY_ABS", ",", "BoxMode", ".", "XYWH_ABS", ")", "\n", "bboxes", "=", "bboxes", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "largest_centred_bbox_index", "=", "get_largest_centred_bounding_box", "(", "bboxes", ",", "orig_w", ",", "orig_h", ")", "# Picks out centred person that is largest in the image.", "\n", "\n", "pred_densepose", "=", "outputs", ".", "pred_densepose", ".", "to_result", "(", "bboxes_XYWH", ")", "\n", "iuv_arr", "=", "DensePoseResult", ".", "decode_png_data", "(", "*", "pred_densepose", ".", "results", "[", "largest_centred_bbox_index", "]", ")", "\n", "\n", "# Round bbox to int", "\n", "largest_bbox", "=", "bboxes", "[", "largest_centred_bbox_index", "]", "\n", "w1", "=", "largest_bbox", "[", "0", "]", "\n", "w2", "=", "largest_bbox", "[", "0", "]", "+", "iuv_arr", ".", "shape", "[", "2", "]", "\n", "h1", "=", "largest_bbox", "[", "1", "]", "\n", "h2", "=", "largest_bbox", "[", "1", "]", "+", "iuv_arr", ".", "shape", "[", "1", "]", "\n", "\n", "I_image", "=", "np", ".", "zeros", "(", "(", "orig_h", ",", "orig_w", ")", ")", "\n", "I_image", "[", "int", "(", "h1", ")", ":", "int", "(", "h2", ")", ",", "int", "(", "w1", ")", ":", "int", "(", "w2", ")", "]", "=", "iuv_arr", "[", "0", ",", ":", ",", ":", "]", "\n", "# U_image = np.zeros((orig_h, orig_w))", "\n", "# U_image[int(h1):int(h2), int(w1):int(w2)] = iuv_arr[1, :, :]", "\n", "# V_image = np.zeros((orig_h, orig_w))", "\n", "# V_image[int(h1):int(h2), int(w1):int(w2)] = iuv_arr[2, :, :]", "\n", "\n", "vis_I_image", "=", "apply_colormap", "(", "I_image", ",", "vmin", "=", "0", ",", "vmax", "=", "24", ")", "\n", "vis_I_image", "=", "vis_I_image", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "vis_I_image", "[", "I_image", "==", "0", ",", ":", "]", "=", "np", ".", "zeros", "(", "3", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "overlay_vis", "=", "cv2", ".", "addWeighted", "(", "input_image", ",", "\n", "0.6", ",", "\n", "vis_I_image", ",", "\n", "0.4", ",", "\n", "gamma", "=", "0", ")", "\n", "\n", "return", "I_image", ",", "overlay_vis", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.config.add_densepose_config": [[7, 41], ["detectron2.config.CfgNode", "detectron2.config.CfgNode"], "function", ["None"], ["COCOPLUS_REGRESSOR_PATH", "=", "'additional/cocoplus_regressor.npy'", "\n", "H36M_REGRESSOR_PATH", "=", "'additional/J_regressor_h36m.npy'", "\n", "VERTEX_TEXTURE_PATH", "=", "'additional/vertex_texture.npy'", "\n", "CUBE_PARTS_PATH", "=", "'additional/cube_parts.npy'", "\n", "\n", "# ------------------------ Constants ------------------------", "\n", "FOCAL_LENGTH", "=", "5000.", "\n", "REGRESSOR_IMG_WH", "=", "256", "\n", "\n", "# ------------------------ Joint label conventions ------------------------", "\n", "# The SMPL model (im smpl_official.py) returns a large superset of joints.", "\n", "# Different subsets are used during training - e.g. H36M 3D joints convention and COCO 2D joints convention.", "\n", "# You may wish to use different subsets in accordance with your training data/inference needs.", "\n", "\n", "# The joints superset is broken down into: 45 SMPL joints (24 standard + additional fingers/toes/face),", "\n", "# 9 extra joints, 19 cocoplus joints and 17 H36M joints.", "\n", "# The 45 SMPL joints are converted to COCO joints with the map below.", "\n", "# (Not really sure how coco and cocoplus are related.)", "\n", "\n", "# Indices to get 17 COCO joints and 17 H36M joints from joints superset.", "\n", "ALL_JOINTS_TO_COCO_MAP", "=", "[", "24", ",", "26", ",", "25", ",", "28", ",", "27", ",", "16", ",", "17", ",", "18", ",", "19", ",", "20", ",", "21", ",", "1", ",", "2", ",", "4", ",", "5", ",", "7", ",", "8", "]", "\n", "ALL_JOINTS_TO_H36M_MAP", "=", "list", "(", "range", "(", "73", ",", "90", ")", ")", "\n", "\n", "# Indices to get the 14 LSP joints from the 17 H36M joints", "\n", "H36M_TO_J17", "=", "[", "6", ",", "5", ",", "4", ",", "1", ",", "2", ",", "3", ",", "16", ",", "15", ",", "14", ",", "11", ",", "12", ",", "13", ",", "8", ",", "10", ",", "0", ",", "7", ",", "9", "]", "\n", "H36M_TO_J14", "=", "H36M_TO_J17", "[", ":", "14", "]", "\n", "\n", "\n", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.__init__": [[18, 22], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "uv_symmetries", ")", ":", "\n", "        ", "self", ".", "mask_label_symmetries", "=", "DensePoseTransformData", ".", "MASK_LABEL_SYMMETRIES", "\n", "self", ".", "point_label_symmetries", "=", "DensePoseTransformData", ".", "POINT_LABEL_SYMMETRIES", "\n", "self", ".", "uv_symmetries", "=", "uv_symmetries", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load": [[23, 38], ["scipy.io.loadmat", "structures.DensePoseTransformData", "range", "uv_symmetry_map_torch[].append", "torch.from_numpy().to", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["", "@", "staticmethod", "\n", "def", "load", "(", "fpath", ")", ":", "\n", "        ", "import", "scipy", ".", "io", "\n", "\n", "uv_symmetry_map", "=", "scipy", ".", "io", ".", "loadmat", "(", "fpath", ")", "\n", "uv_symmetry_map_torch", "=", "{", "}", "\n", "for", "key", "in", "[", "\"U_transforms\"", ",", "\"V_transforms\"", "]", ":", "\n", "            ", "map_src", "=", "uv_symmetry_map", "[", "key", "]", "\n", "uv_symmetry_map_torch", "[", "key", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "uv_symmetry_map", "[", "key", "]", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "uv_symmetry_map_torch", "[", "key", "]", ".", "append", "(", "\n", "torch", ".", "from_numpy", "(", "map_src", "[", "0", ",", "i", "]", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float", ")", "\n", ")", "\n", "", "", "transform_data", "=", "DensePoseTransformData", "(", "uv_symmetry_map_torch", ")", "\n", "return", "transform_data", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.__init__": [[75, 87], ["structures.DensePoseDataRelative.validate_annotation", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "structures.DensePoseDataRelative.extract_segmentation_mask", "torch.device", "structures.DensePoseDataRelative.cleanup_annotation"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.validate_annotation", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.extract_segmentation_mask", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.cleanup_annotation"], ["def", "__init__", "(", "self", ",", "annotation", ",", "cleanup", "=", "False", ")", ":", "\n", "        ", "is_valid", ",", "reason_not_valid", "=", "DensePoseDataRelative", ".", "validate_annotation", "(", "annotation", ")", "\n", "assert", "is_valid", ",", "\"Invalid DensePose annotations: {}\"", ".", "format", "(", "reason_not_valid", ")", "\n", "self", ".", "x", "=", "torch", ".", "as_tensor", "(", "annotation", "[", "DensePoseDataRelative", ".", "X_KEY", "]", ")", "\n", "self", ".", "y", "=", "torch", ".", "as_tensor", "(", "annotation", "[", "DensePoseDataRelative", ".", "Y_KEY", "]", ")", "\n", "self", ".", "i", "=", "torch", ".", "as_tensor", "(", "annotation", "[", "DensePoseDataRelative", ".", "I_KEY", "]", ")", "\n", "self", ".", "u", "=", "torch", ".", "as_tensor", "(", "annotation", "[", "DensePoseDataRelative", ".", "U_KEY", "]", ")", "\n", "self", ".", "v", "=", "torch", ".", "as_tensor", "(", "annotation", "[", "DensePoseDataRelative", ".", "V_KEY", "]", ")", "\n", "self", ".", "segm", "=", "DensePoseDataRelative", ".", "extract_segmentation_mask", "(", "annotation", ")", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "if", "cleanup", ":", "\n", "            ", "DensePoseDataRelative", ".", "cleanup_annotation", "(", "annotation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.to": [[88, 101], ["object.__new__", "structures.DensePoseDataRelative.x.to", "structures.DensePoseDataRelative.y.to", "structures.DensePoseDataRelative.i.to", "structures.DensePoseDataRelative.u.to", "structures.DensePoseDataRelative.v.to", "structures.DensePoseDataRelative.segm.to"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["", "", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "        ", "if", "self", ".", "device", "==", "device", ":", "\n", "            ", "return", "self", "\n", "", "new_data", "=", "DensePoseDataRelative", ".", "__new__", "(", "DensePoseDataRelative", ")", "\n", "new_data", ".", "x", "=", "self", ".", "x", "\n", "new_data", ".", "x", "=", "self", ".", "x", ".", "to", "(", "device", ")", "\n", "new_data", ".", "y", "=", "self", ".", "y", ".", "to", "(", "device", ")", "\n", "new_data", ".", "i", "=", "self", ".", "i", ".", "to", "(", "device", ")", "\n", "new_data", ".", "u", "=", "self", ".", "u", ".", "to", "(", "device", ")", "\n", "new_data", ".", "v", "=", "self", ".", "v", ".", "to", "(", "device", ")", "\n", "new_data", ".", "segm", "=", "self", ".", "segm", ".", "to", "(", "device", ")", "\n", "new_data", ".", "device", "=", "device", "\n", "return", "new_data", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.extract_segmentation_mask": [[102, 114], ["torch.zeros", "range", "mask_utils.decode"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "extract_segmentation_mask", "(", "annotation", ")", ":", "\n", "        ", "import", "pycocotools", ".", "mask", "as", "mask_utils", "\n", "\n", "poly_specs", "=", "annotation", "[", "DensePoseDataRelative", ".", "S_KEY", "]", "\n", "segm", "=", "torch", ".", "zeros", "(", "(", "DensePoseDataRelative", ".", "MASK_SIZE", ",", ")", "*", "2", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "DensePoseDataRelative", ".", "N_BODY_PARTS", ")", ":", "\n", "            ", "poly_i", "=", "poly_specs", "[", "i", "]", "\n", "if", "poly_i", ":", "\n", "                ", "mask_i", "=", "mask_utils", ".", "decode", "(", "poly_i", ")", "\n", "segm", "[", "mask_i", ">", "0", "]", "=", "i", "+", "1", "\n", "", "", "return", "segm", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.validate_annotation": [[115, 128], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "validate_annotation", "(", "annotation", ")", ":", "\n", "        ", "for", "key", "in", "[", "\n", "DensePoseDataRelative", ".", "X_KEY", ",", "\n", "DensePoseDataRelative", ".", "Y_KEY", ",", "\n", "DensePoseDataRelative", ".", "I_KEY", ",", "\n", "DensePoseDataRelative", ".", "U_KEY", ",", "\n", "DensePoseDataRelative", ".", "V_KEY", ",", "\n", "DensePoseDataRelative", ".", "S_KEY", ",", "\n", "]", ":", "\n", "            ", "if", "key", "not", "in", "annotation", ":", "\n", "                ", "return", "False", ",", "\"no {key} data in the annotation\"", ".", "format", "(", "key", "=", "key", ")", "\n", "", "", "return", "True", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.cleanup_annotation": [[129, 141], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "cleanup_annotation", "(", "annotation", ")", ":", "\n", "        ", "for", "key", "in", "[", "\n", "DensePoseDataRelative", ".", "X_KEY", ",", "\n", "DensePoseDataRelative", ".", "Y_KEY", ",", "\n", "DensePoseDataRelative", ".", "I_KEY", ",", "\n", "DensePoseDataRelative", ".", "U_KEY", ",", "\n", "DensePoseDataRelative", ".", "V_KEY", ",", "\n", "DensePoseDataRelative", ".", "S_KEY", ",", "\n", "]", ":", "\n", "            ", "if", "key", "in", "annotation", ":", "\n", "                ", "del", "annotation", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative.apply_transform": [[142, 145], ["structures.DensePoseDataRelative._transform_pts", "structures.DensePoseDataRelative._transform_segm"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._transform_pts", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._transform_segm"], ["", "", "", "def", "apply_transform", "(", "self", ",", "transforms", ",", "densepose_transform_data", ")", ":", "\n", "        ", "self", ".", "_transform_pts", "(", "transforms", ",", "densepose_transform_data", ")", "\n", "self", ".", "_transform_segm", "(", "transforms", ",", "densepose_transform_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._transform_pts": [[146, 154], ["structures.DensePoseDataRelative._flip_iuv_semantics", "sum", "structures.DensePoseDataRelative.segm.size", "isinstance"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._flip_iuv_semantics"], ["", "def", "_transform_pts", "(", "self", ",", "transforms", ",", "dp_transform_data", ")", ":", "\n", "        ", "import", "detectron2", ".", "data", ".", "transforms", "as", "T", "\n", "\n", "# NOTE: This assumes that HorizFlipTransform is the only one that does flip", "\n", "do_hflip", "=", "sum", "(", "isinstance", "(", "t", ",", "T", ".", "HFlipTransform", ")", "for", "t", "in", "transforms", ".", "transforms", ")", "%", "2", "==", "1", "\n", "if", "do_hflip", ":", "\n", "            ", "self", ".", "x", "=", "self", ".", "segm", ".", "size", "(", "1", ")", "-", "self", ".", "x", "\n", "self", ".", "_flip_iuv_semantics", "(", "dp_transform_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._flip_iuv_semantics": [[155, 168], ["structures.DensePoseDataRelative.i.clone", "range"], "methods", ["None"], ["", "", "def", "_flip_iuv_semantics", "(", "self", ",", "dp_transform_data", ":", "DensePoseTransformData", ")", "->", "None", ":", "\n", "        ", "i_old", "=", "self", ".", "i", ".", "clone", "(", ")", "\n", "uv_symmetries", "=", "dp_transform_data", ".", "uv_symmetries", "\n", "pt_label_symmetries", "=", "dp_transform_data", ".", "point_label_symmetries", "\n", "for", "i", "in", "range", "(", "self", ".", "N_PART_LABELS", ")", ":", "\n", "            ", "if", "i", "+", "1", "in", "i_old", ":", "\n", "                ", "annot_indices_i", "=", "i_old", "==", "i", "+", "1", "\n", "if", "pt_label_symmetries", "[", "i", "+", "1", "]", "!=", "i", "+", "1", ":", "\n", "                    ", "self", ".", "i", "[", "annot_indices_i", "]", "=", "pt_label_symmetries", "[", "i", "+", "1", "]", "\n", "", "u_loc", "=", "(", "self", ".", "u", "[", "annot_indices_i", "]", "*", "255", ")", ".", "long", "(", ")", "\n", "v_loc", "=", "(", "self", ".", "v", "[", "annot_indices_i", "]", "*", "255", ")", ".", "long", "(", ")", "\n", "self", ".", "u", "[", "annot_indices_i", "]", "=", "uv_symmetries", "[", "\"U_transforms\"", "]", "[", "i", "]", "[", "v_loc", ",", "u_loc", "]", "\n", "self", ".", "v", "[", "annot_indices_i", "]", "=", "uv_symmetries", "[", "\"V_transforms\"", "]", "[", "i", "]", "[", "v_loc", ",", "u_loc", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._transform_segm": [[169, 177], ["torch.flip", "structures.DensePoseDataRelative._flip_segm_semantics", "sum", "isinstance"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._flip_segm_semantics"], ["", "", "", "def", "_transform_segm", "(", "self", ",", "transforms", ",", "dp_transform_data", ")", ":", "\n", "        ", "import", "detectron2", ".", "data", ".", "transforms", "as", "T", "\n", "\n", "# NOTE: This assumes that HorizFlipTransform is the only one that does flip", "\n", "do_hflip", "=", "sum", "(", "isinstance", "(", "t", ",", "T", ".", "HFlipTransform", ")", "for", "t", "in", "transforms", ".", "transforms", ")", "%", "2", "==", "1", "\n", "if", "do_hflip", ":", "\n", "            ", "self", ".", "segm", "=", "torch", ".", "flip", "(", "self", ".", "segm", ",", "[", "1", "]", ")", "\n", "self", ".", "_flip_segm_semantics", "(", "dp_transform_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseDataRelative._flip_segm_semantics": [[178, 184], ["structures.DensePoseDataRelative.segm.clone", "range"], "methods", ["None"], ["", "", "def", "_flip_segm_semantics", "(", "self", ",", "dp_transform_data", ")", ":", "\n", "        ", "old_segm", "=", "self", ".", "segm", ".", "clone", "(", ")", "\n", "mask_label_symmetries", "=", "dp_transform_data", ".", "mask_label_symmetries", "\n", "for", "i", "in", "range", "(", "self", ".", "N_BODY_PARTS", ")", ":", "\n", "            ", "if", "mask_label_symmetries", "[", "i", "+", "1", "]", "!=", "i", "+", "1", ":", "\n", "                ", "self", ".", "segm", "[", "old_segm", "==", "i", "+", "1", "]", "=", "mask_label_symmetries", "[", "i", "+", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.__init__": [[200, 206], ["structures.DensePoseOutput._check_output_dims"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput._check_output_dims"], ["    ", "def", "__init__", "(", "self", ",", "S", ",", "I", ",", "U", ",", "V", ")", ":", "\n", "        ", "self", ".", "S", "=", "S", "\n", "self", ".", "I", "=", "I", "# noqa: E741", "\n", "self", ".", "U", "=", "U", "\n", "self", ".", "V", "=", "V", "\n", "self", ".", "_check_output_dims", "(", "S", ",", "I", ",", "U", ",", "V", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput._check_output_dims": [[207, 245], ["len", "S.size", "len", "S.size", "len", "S.size", "len", "S.size", "len", "len", "len", "len", "I.size", "U.size", "I.size", "U.size", "I.size", "V.size", "I.size", "V.size", "S.size", "I.size", "U.size", "V.size", "S.size", "I.size", "S.size", "I.size"], "methods", ["None"], ["", "def", "_check_output_dims", "(", "self", ",", "S", ",", "I", ",", "U", ",", "V", ")", ":", "\n", "        ", "assert", "(", "\n", "len", "(", "S", ".", "size", "(", ")", ")", "==", "4", "\n", ")", ",", "\"Segmentation output should have 4 \"", "\"dimensions (NCHW), but has size {}\"", ".", "format", "(", "\n", "S", ".", "size", "(", ")", "\n", ")", "\n", "assert", "(", "\n", "len", "(", "I", ".", "size", "(", ")", ")", "==", "4", "\n", ")", ",", "\"Segmentation output should have 4 \"", "\"dimensions (NCHW), but has size {}\"", ".", "format", "(", "\n", "S", ".", "size", "(", ")", "\n", ")", "\n", "assert", "(", "\n", "len", "(", "U", ".", "size", "(", ")", ")", "==", "4", "\n", ")", ",", "\"Segmentation output should have 4 \"", "\"dimensions (NCHW), but has size {}\"", ".", "format", "(", "\n", "S", ".", "size", "(", ")", "\n", ")", "\n", "assert", "(", "\n", "len", "(", "V", ".", "size", "(", ")", ")", "==", "4", "\n", ")", ",", "\"Segmentation output should have 4 \"", "\"dimensions (NCHW), but has size {}\"", ".", "format", "(", "\n", "S", ".", "size", "(", ")", "\n", ")", "\n", "assert", "len", "(", "S", ")", "==", "len", "(", "I", ")", ",", "(", "\n", "\"Number of output segmentation planes {} \"", "\n", "\"should be equal to the number of output part index \"", "\n", "\"planes {}\"", ".", "format", "(", "len", "(", "S", ")", ",", "len", "(", "I", ")", ")", "\n", ")", "\n", "assert", "S", ".", "size", "(", ")", "[", "2", ":", "]", "==", "I", ".", "size", "(", ")", "[", "2", ":", "]", ",", "(", "\n", "\"Output segmentation plane size {} \"", "\n", "\"should be equal to the output part index \"", "\n", "\"plane size {}\"", ".", "format", "(", "S", ".", "size", "(", ")", "[", "2", ":", "]", ",", "I", ".", "size", "(", ")", "[", "2", ":", "]", ")", "\n", ")", "\n", "assert", "I", ".", "size", "(", ")", "==", "U", ".", "size", "(", ")", ",", "(", "\n", "\"Part index output shape {} \"", "\n", "\"should be the same as U coordinates output shape {}\"", ".", "format", "(", "I", ".", "size", "(", ")", ",", "U", ".", "size", "(", ")", ")", "\n", ")", "\n", "assert", "I", ".", "size", "(", ")", "==", "V", ".", "size", "(", ")", ",", "(", "\n", "\"Part index output shape {} \"", "\n", "\"should be the same as V coordinates output shape {}\"", ".", "format", "(", "I", ".", "size", "(", ")", ",", "V", ".", "size", "(", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.resize": [[247, 250], ["None"], "methods", ["None"], ["", "def", "resize", "(", "self", ",", "image_size_hw", ")", ":", "\n", "# do nothing - outputs are invariant to resize", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput._crop": [[251, 312], ["structures.normalized_coords_transform", "normalized_coords_transform.", "normalized_coords_transform.", "S.size", "S.size", "torch.meshgrid", "torch.stack().to", "torch.nn.functional.grid_sample().squeeze", "torch.nn.functional.grid_sample().squeeze", "torch.nn.functional.grid_sample().squeeze", "torch.nn.functional.grid_sample().squeeze", "torch.stack().to.size", "torch.stack().to.size", "torch.stack().to.size", "torch.stack().to.size", "torch.arange", "torch.arange", "torch.stack", "torch.nn.functional.grid_sample", "torch.nn.functional.grid_sample", "torch.nn.functional.grid_sample", "torch.nn.functional.grid_sample", "S.unsqueeze", "torch.unsqueeze", "I.unsqueeze", "torch.unsqueeze", "U.unsqueeze", "torch.unsqueeze", "V.unsqueeze", "torch.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.normalized_coords_transform", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["", "def", "_crop", "(", "self", ",", "S", ",", "I", ",", "U", ",", "V", ",", "bbox_old_xywh", ",", "bbox_new_xywh", ")", ":", "\n", "        ", "\"\"\"\n        Resample S, I, U, V from bbox_old to the cropped bbox_new\n        \"\"\"", "\n", "x0old", ",", "y0old", ",", "wold", ",", "hold", "=", "bbox_old_xywh", "\n", "x0new", ",", "y0new", ",", "wnew", ",", "hnew", "=", "bbox_new_xywh", "\n", "tr_coords", "=", "normalized_coords_transform", "(", "x0old", ",", "y0old", ",", "wold", ",", "hold", ")", "\n", "topleft", "=", "(", "x0new", ",", "y0new", ")", "\n", "bottomright", "=", "(", "x0new", "+", "wnew", ",", "y0new", "+", "hnew", ")", "\n", "topleft_norm", "=", "tr_coords", "(", "topleft", ")", "\n", "bottomright_norm", "=", "tr_coords", "(", "bottomright", ")", "\n", "hsize", "=", "S", ".", "size", "(", "1", ")", "\n", "wsize", "=", "S", ".", "size", "(", "2", ")", "\n", "grid", "=", "torch", ".", "meshgrid", "(", "\n", "torch", ".", "arange", "(", "\n", "topleft_norm", "[", "1", "]", ",", "\n", "bottomright_norm", "[", "1", "]", ",", "\n", "(", "bottomright_norm", "[", "1", "]", "-", "topleft_norm", "[", "1", "]", ")", "/", "hsize", ",", "\n", ")", "[", ":", "hsize", "]", ",", "\n", "torch", ".", "arange", "(", "\n", "topleft_norm", "[", "0", "]", ",", "\n", "bottomright_norm", "[", "0", "]", ",", "\n", "(", "bottomright_norm", "[", "0", "]", "-", "topleft_norm", "[", "0", "]", ")", "/", "wsize", ",", "\n", ")", "[", ":", "wsize", "]", ",", "\n", ")", "\n", "grid", "=", "torch", ".", "stack", "(", "grid", ",", "dim", "=", "2", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "assert", "(", "\n", "grid", ".", "size", "(", "0", ")", "==", "hsize", "\n", ")", ",", "\"Resampled grid expected \"", "\"height={}, actual height={}\"", ".", "format", "(", "hsize", ",", "grid", ".", "size", "(", "0", ")", ")", "\n", "assert", "grid", ".", "size", "(", "1", ")", "==", "wsize", ",", "\"Resampled grid expected \"", "\"width={}, actual width={}\"", ".", "format", "(", "\n", "wsize", ",", "grid", ".", "size", "(", "1", ")", "\n", ")", "\n", "S_new", "=", "F", ".", "grid_sample", "(", "\n", "S", ".", "unsqueeze", "(", "0", ")", ",", "\n", "torch", ".", "unsqueeze", "(", "grid", ",", "0", ")", ",", "\n", "mode", "=", "\"bilinear\"", ",", "\n", "padding_mode", "=", "\"border\"", ",", "\n", "align_corners", "=", "True", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "I_new", "=", "F", ".", "grid_sample", "(", "\n", "I", ".", "unsqueeze", "(", "0", ")", ",", "\n", "torch", ".", "unsqueeze", "(", "grid", ",", "0", ")", ",", "\n", "mode", "=", "\"bilinear\"", ",", "\n", "padding_mode", "=", "\"border\"", ",", "\n", "align_corners", "=", "True", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "U_new", "=", "F", ".", "grid_sample", "(", "\n", "U", ".", "unsqueeze", "(", "0", ")", ",", "\n", "torch", ".", "unsqueeze", "(", "grid", ",", "0", ")", ",", "\n", "mode", "=", "\"bilinear\"", ",", "\n", "padding_mode", "=", "\"border\"", ",", "\n", "align_corners", "=", "True", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "V_new", "=", "F", ".", "grid_sample", "(", "\n", "V", ".", "unsqueeze", "(", "0", ")", ",", "\n", "torch", ".", "unsqueeze", "(", "grid", ",", "0", ")", ",", "\n", "mode", "=", "\"bilinear\"", ",", "\n", "padding_mode", "=", "\"border\"", ",", "\n", "align_corners", "=", "True", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "return", "S_new", ",", "I_new", ",", "U_new", ",", "V_new", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.crop": [[313, 323], ["None"], "methods", ["None"], ["", "def", "crop", "(", "self", ",", "indices_cropped", ",", "bboxes_old", ",", "bboxes_new", ")", ":", "\n", "        ", "\"\"\"\n        Crop outputs for selected bounding boxes to the new bounding boxes.\n        \"\"\"", "\n", "# VK: cropping is ignored for now", "\n", "# for i, ic in enumerate(indices_cropped):", "\n", "#    self.S[ic], self.I[ic], self.U[ic], self.V[ic] = \\", "\n", "#        self._crop(self.S[ic], self.I[ic], self.U[ic], self.V[ic],", "\n", "#        bboxes_old[i], bboxes_new[i])", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.to_result": [[324, 331], ["structures.DensePoseResult"], "methods", ["None"], ["", "def", "to_result", "(", "self", ",", "boxes_xywh", ")", ":", "\n", "        ", "\"\"\"\n        Convert DensePose outputs to results format. Results are more compact,\n        but cannot be resampled any more\n        \"\"\"", "\n", "result", "=", "DensePoseResult", "(", "boxes_xywh", ",", "self", ".", "S", ",", "self", ".", "I", ",", "self", ".", "U", ",", "self", ".", "V", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.__getitem__": [[332, 344], ["isinstance", "structures.DensePoseOutput", "structures.DensePoseOutput.S[].unsqueeze", "structures.DensePoseOutput.I[].unsqueeze", "structures.DensePoseOutput.U[].unsqueeze", "structures.DensePoseOutput.V[].unsqueeze"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "S_selected", "=", "self", ".", "S", "[", "item", "]", ".", "unsqueeze", "(", "0", ")", "\n", "I_selected", "=", "self", ".", "I", "[", "item", "]", ".", "unsqueeze", "(", "0", ")", "\n", "U_selected", "=", "self", ".", "U", "[", "item", "]", ".", "unsqueeze", "(", "0", ")", "\n", "V_selected", "=", "self", ".", "V", "[", "item", "]", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "S_selected", "=", "self", ".", "S", "[", "item", "]", "\n", "I_selected", "=", "self", ".", "I", "[", "item", "]", "\n", "U_selected", "=", "self", ".", "U", "[", "item", "]", "\n", "V_selected", "=", "self", ".", "V", "[", "item", "]", "\n", "", "return", "DensePoseOutput", "(", "S_selected", ",", "I_selected", ",", "U_selected", ",", "V_selected", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.__str__": [[345, 350], ["list", "list", "list", "list", "structures.DensePoseOutput.S.size", "structures.DensePoseOutput.I.size", "structures.DensePoseOutput.U.size", "structures.DensePoseOutput.V.size"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"DensePoseOutput S {}, I {}, U {}, V {}\"", ".", "format", "(", "\n", "list", "(", "self", ".", "S", ".", "size", "(", ")", ")", ",", "list", "(", "self", ".", "I", ".", "size", "(", ")", ")", ",", "list", "(", "self", ".", "U", ".", "size", "(", ")", ")", ",", "list", "(", "self", ".", "V", ".", "size", "(", ")", ")", "\n", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseOutput.__len__": [[351, 353], ["structures.DensePoseOutput.S.size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "S", ".", "size", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.__init__": [[356, 367], ["boxes_xywh.cpu().tolist", "enumerate", "len", "boxes_xywh.size", "structures.DensePoseResult._output_to_result", "structures.DensePoseResult.cpu().numpy", "structures.DensePoseResult.encode_png_data", "structures.DensePoseResult.results.append", "boxes_xywh.cpu", "boxes_xywh.size", "structures.DensePoseResult.cpu"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult._output_to_result", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.encode_png_data"], ["    ", "def", "__init__", "(", "self", ",", "boxes_xywh", ",", "S", ",", "I", ",", "U", ",", "V", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "self", ".", "boxes_xywh", "=", "boxes_xywh", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "assert", "len", "(", "boxes_xywh", ".", "size", "(", ")", ")", "==", "2", "\n", "assert", "boxes_xywh", ".", "size", "(", "1", ")", "==", "4", "\n", "for", "i", ",", "box_xywh", "in", "enumerate", "(", "boxes_xywh", ")", ":", "\n", "            ", "result_i", "=", "self", ".", "_output_to_result", "(", "box_xywh", ",", "S", "[", "[", "i", "]", "]", ",", "I", "[", "[", "i", "]", "]", ",", "U", "[", "[", "i", "]", "]", ",", "V", "[", "[", "i", "]", "]", ")", "\n", "result_numpy_i", "=", "result_i", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "result_encoded_i", "=", "DensePoseResult", ".", "encode_png_data", "(", "result_numpy_i", ")", "\n", "result_encoded_with_shape_i", "=", "(", "result_numpy_i", ".", "shape", ",", "result_encoded_i", ")", "\n", "self", ".", "results", ".", "append", "(", "result_encoded_with_shape_i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.__str__": [[368, 373], ["len", "str", "list"], "methods", ["None"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"DensePoseResult: N={} [{}]\"", ".", "format", "(", "\n", "len", "(", "self", ".", "results", ")", ",", "\", \"", ".", "join", "(", "[", "str", "(", "list", "(", "r", "[", "0", "]", ")", ")", "for", "r", "in", "self", ".", "results", "]", ")", "\n", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult._output_to_result": [[374, 413], ["max", "max", "torch.zeros", "torch.nn.functional.interpolate().argmax", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "range", "int", "int", "len", "len", "len", "len", "len", "len", "len", "len", "torch.nn.functional.interpolate.size", "torch.zeros.size", "torch.zeros.size", "torch.zeros.size", "torch.zeros.size", "S.size", "S.size", "torch.nn.functional.interpolate", "I.size", "S.size", "U.size", "U.size", "V.size", "V.size", "torch.nn.functional.interpolate().argmax", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "_output_to_result", "(", "self", ",", "box_xywh", ",", "S", ",", "I", ",", "U", ",", "V", ")", ":", "\n", "        ", "x", ",", "y", ",", "w", ",", "h", "=", "box_xywh", "\n", "w", "=", "max", "(", "int", "(", "w", ")", ",", "1", ")", "\n", "h", "=", "max", "(", "int", "(", "h", ")", ",", "1", ")", "\n", "result", "=", "torch", ".", "zeros", "(", "[", "3", ",", "h", ",", "w", "]", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "U", ".", "device", ")", "\n", "assert", "(", "\n", "len", "(", "S", ".", "size", "(", ")", ")", "==", "4", "\n", ")", ",", "\"AnnIndex tensor size should have {} \"", "\"dimensions but has {}\"", ".", "format", "(", "4", ",", "len", "(", "S", ".", "size", "(", ")", ")", ")", "\n", "s_bbox", "=", "F", ".", "interpolate", "(", "S", ",", "(", "h", ",", "w", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "assert", "(", "\n", "len", "(", "I", ".", "size", "(", ")", ")", "==", "4", "\n", ")", ",", "\"IndexUV tensor size should have {} \"", "\"dimensions but has {}\"", ".", "format", "(", "4", ",", "len", "(", "S", ".", "size", "(", ")", ")", ")", "\n", "i_bbox", "=", "(", "\n", "F", ".", "interpolate", "(", "I", ",", "(", "h", ",", "w", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "*", "(", "s_bbox", ">", "0", ")", ".", "long", "(", ")", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "assert", "len", "(", "U", ".", "size", "(", ")", ")", "==", "4", ",", "\"U tensor size should have {} \"", "\"dimensions but has {}\"", ".", "format", "(", "\n", "4", ",", "len", "(", "U", ".", "size", "(", ")", ")", "\n", ")", "\n", "u_bbox", "=", "F", ".", "interpolate", "(", "U", ",", "(", "h", ",", "w", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "assert", "len", "(", "V", ".", "size", "(", ")", ")", "==", "4", ",", "\"V tensor size should have {} \"", "\"dimensions but has {}\"", ".", "format", "(", "\n", "4", ",", "len", "(", "V", ".", "size", "(", ")", ")", "\n", ")", "\n", "v_bbox", "=", "F", ".", "interpolate", "(", "V", ",", "(", "h", ",", "w", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "result", "[", "0", "]", "=", "i_bbox", "\n", "for", "part_id", "in", "range", "(", "1", ",", "u_bbox", ".", "size", "(", "1", ")", ")", ":", "\n", "            ", "result", "[", "1", "]", "[", "i_bbox", "==", "part_id", "]", "=", "(", "\n", "(", "u_bbox", "[", "0", ",", "part_id", "]", "[", "i_bbox", "==", "part_id", "]", "*", "255", ")", ".", "clamp", "(", "0", ",", "255", ")", ".", "to", "(", "torch", ".", "uint8", ")", "\n", ")", "\n", "result", "[", "2", "]", "[", "i_bbox", "==", "part_id", "]", "=", "(", "\n", "(", "v_bbox", "[", "0", ",", "part_id", "]", "[", "i_bbox", "==", "part_id", "]", "*", "255", ")", ".", "clamp", "(", "0", ",", "255", ")", ".", "to", "(", "torch", ".", "uint8", ")", "\n", ")", "\n", "", "assert", "(", "\n", "result", ".", "size", "(", "1", ")", "==", "h", "\n", ")", ",", "\"Results height {} should be equal\"", "\"to bounding box height {}\"", ".", "format", "(", "result", ".", "size", "(", "1", ")", ",", "h", ")", "\n", "assert", "(", "\n", "result", ".", "size", "(", "2", ")", "==", "w", "\n", ")", ",", "\"Results width {} should be equal\"", "\"to bounding box width {}\"", ".", "format", "(", "result", ".", "size", "(", "2", ")", ",", "w", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.encode_png_data": [[414, 436], ["numpy.moveaxis", "PIL.Image.fromarray", "io.BytesIO", "PIL.Image.fromarray.save", "base64.encodebytes().decode", "len", "len", "base64.encodebytes", "io.BytesIO.getvalue"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "encode_png_data", "(", "arr", ")", ":", "\n", "        ", "\"\"\"\n        Encode array data as a PNG image using the highest compression rate\n        @param arr [in] Data stored in an array of size (3, M, N) of type uint8\n        @return Base64-encoded string containing PNG-compressed data\n        \"\"\"", "\n", "assert", "len", "(", "arr", ".", "shape", ")", "==", "3", ",", "\"Expected a 3D array as an input,\"", "\" got a {0}D array\"", ".", "format", "(", "\n", "len", "(", "arr", ".", "shape", ")", "\n", ")", "\n", "assert", "arr", ".", "shape", "[", "0", "]", "==", "3", ",", "\"Expected first array dimension of size 3,\"", "\" got {0}\"", ".", "format", "(", "\n", "arr", ".", "shape", "[", "0", "]", "\n", ")", "\n", "assert", "arr", ".", "dtype", "==", "np", ".", "uint8", ",", "\"Expected an array of type np.uint8, \"", "\" got {0}\"", ".", "format", "(", "\n", "arr", ".", "dtype", "\n", ")", "\n", "data", "=", "np", ".", "moveaxis", "(", "arr", ",", "0", ",", "-", "1", ")", "\n", "im", "=", "Image", ".", "fromarray", "(", "data", ")", "\n", "fstream", "=", "BytesIO", "(", ")", "\n", "im", ".", "save", "(", "fstream", ",", "format", "=", "\"png\"", ",", "optimize", "=", "True", ")", "\n", "s", "=", "base64", ".", "encodebytes", "(", "fstream", ".", "getvalue", "(", ")", ")", ".", "decode", "(", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.decode_png_data": [[437, 448], ["io.BytesIO", "PIL.Image.open", "numpy.moveaxis", "numpy.moveaxis.reshape", "base64.decodebytes", "numpy.array", "s.encode", "PIL.Image.open.getdata"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "decode_png_data", "(", "shape", ",", "s", ")", ":", "\n", "        ", "\"\"\"\n        Decode array data from a string that contains PNG-compressed data\n        @param Base64-encoded string containing PNG-compressed data\n        @return Data stored in an array of size (3, M, N) of type uint8\n        \"\"\"", "\n", "fstream", "=", "BytesIO", "(", "base64", ".", "decodebytes", "(", "s", ".", "encode", "(", ")", ")", ")", "\n", "im", "=", "Image", ".", "open", "(", "fstream", ")", "\n", "data", "=", "np", ".", "moveaxis", "(", "np", ".", "array", "(", "im", ".", "getdata", "(", ")", ",", "dtype", "=", "np", ".", "uint8", ")", ",", "-", "1", ",", "0", ")", "\n", "return", "data", ".", "reshape", "(", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.__len__": [[449, 451], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseResult.__getitem__": [[452, 456], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "result_encoded", "=", "self", ".", "results", "[", "item", "]", "\n", "bbox_xywh", "=", "self", ".", "boxes_xywh", "[", "item", "]", "\n", "return", "result_encoded", ",", "bbox_xywh", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.__init__": [[462, 480], ["boxes_xyxy_abs.to", "len", "len", "len", "len", "structures.DensePoseList.densepose_datas.append", "isinstance", "type", "densepose_data.to"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["def", "__init__", "(", "self", ",", "densepose_datas", ",", "boxes_xyxy_abs", ",", "image_size_hw", ",", "device", "=", "_TORCH_DEVICE_CPU", ")", ":", "\n", "        ", "assert", "len", "(", "densepose_datas", ")", "==", "len", "(", "boxes_xyxy_abs", ")", ",", "(", "\n", "\"Attempt to initialize DensePoseList with {} DensePose datas \"", "\n", "\"and {} boxes\"", ".", "format", "(", "len", "(", "densepose_datas", ")", ",", "len", "(", "boxes_xyxy_abs", ")", ")", "\n", ")", "\n", "self", ".", "densepose_datas", "=", "[", "]", "\n", "for", "densepose_data", "in", "densepose_datas", ":", "\n", "            ", "assert", "isinstance", "(", "densepose_data", ",", "DensePoseDataRelative", ")", "or", "densepose_data", "is", "None", ",", "(", "\n", "\"Attempt to initialize DensePoseList with DensePose datas \"", "\n", "\"of type {}, expected DensePoseDataRelative\"", ".", "format", "(", "type", "(", "densepose_data", ")", ")", "\n", ")", "\n", "densepose_data_ondevice", "=", "(", "\n", "densepose_data", ".", "to", "(", "device", ")", "if", "densepose_data", "is", "not", "None", "else", "None", "\n", ")", "\n", "self", ".", "densepose_datas", ".", "append", "(", "densepose_data_ondevice", ")", "\n", "", "self", ".", "boxes_xyxy_abs", "=", "boxes_xyxy_abs", ".", "to", "(", "device", ")", "\n", "self", ".", "image_size_hw", "=", "image_size_hw", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to": [[481, 485], ["structures.DensePoseList"], "methods", ["None"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "        ", "if", "self", ".", "device", "==", "device", ":", "\n", "            ", "return", "self", "\n", "", "return", "DensePoseList", "(", "self", ".", "densepose_datas", ",", "self", ".", "boxes_xyxy_abs", ",", "self", ".", "image_size_hw", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.__iter__": [[486, 488], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "densepose_datas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.__len__": [[489, 491], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "densepose_datas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.__repr__": [[492, 498], ["len"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "s", "+=", "\"num_instances={}, \"", ".", "format", "(", "len", "(", "self", ".", "densepose_datas", ")", ")", "\n", "s", "+=", "\"image_width={}, \"", ".", "format", "(", "self", ".", "image_size_hw", "[", "1", "]", ")", "\n", "s", "+=", "\"image_height={})\"", ".", "format", "(", "self", ".", "image_size_hw", "[", "0", "]", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.__getitem__": [[499, 520], ["isinstance", "isinstance", "structures.DensePoseList", "isinstance", "structures.DensePoseList", "structures.DensePoseList", "enumerate"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "densepose_data_rel", "=", "self", ".", "densepose_datas", "[", "item", "]", "\n", "return", "densepose_data_rel", "\n", "", "elif", "isinstance", "(", "item", ",", "slice", ")", ":", "\n", "            ", "densepose_datas_rel", "=", "self", ".", "densepose_datas", "[", "item", "]", "\n", "boxes_xyxy_abs", "=", "self", ".", "boxes_xyxy_abs", "[", "item", "]", "\n", "return", "DensePoseList", "(", "\n", "densepose_datas_rel", ",", "boxes_xyxy_abs", ",", "self", ".", "image_size_hw", ",", "self", ".", "device", "\n", ")", "\n", "", "elif", "isinstance", "(", "item", ",", "torch", ".", "Tensor", ")", "and", "(", "item", ".", "dtype", "==", "torch", ".", "bool", ")", ":", "\n", "            ", "densepose_datas_rel", "=", "[", "self", ".", "densepose_datas", "[", "i", "]", "for", "i", ",", "x", "in", "enumerate", "(", "item", ")", "if", "x", ">", "0", "]", "\n", "boxes_xyxy_abs", "=", "self", ".", "boxes_xyxy_abs", "[", "item", "]", "\n", "return", "DensePoseList", "(", "\n", "densepose_datas_rel", ",", "boxes_xyxy_abs", ",", "self", ".", "image_size_hw", ",", "self", ".", "device", "\n", ")", "\n", "", "else", ":", "\n", "            ", "densepose_datas_rel", "=", "[", "self", ".", "densepose_datas", "[", "i", "]", "for", "i", "in", "item", "]", "\n", "boxes_xyxy_abs", "=", "self", ".", "boxes_xyxy_abs", "[", "item", "]", "\n", "return", "DensePoseList", "(", "\n", "densepose_datas_rel", ",", "boxes_xyxy_abs", ",", "self", ".", "image_size_hw", ",", "self", ".", "device", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.normalized_coords_transform": [[186, 197], ["None"], "function", ["None"], ["", "", "", "", "def", "normalized_coords_transform", "(", "x0", ",", "y0", ",", "w", ",", "h", ")", ":", "\n", "    ", "\"\"\"\n    Coordinates transform that maps top left corner to (-1, -1) and bottom\n    right corner to (1, 1). Used for torch.grid_sample to initialize the\n    grid\n    \"\"\"", "\n", "\n", "def", "f", "(", "p", ")", ":", "\n", "        ", "return", "(", "2", "*", "(", "p", "[", "0", "]", "-", "x0", ")", "/", "w", "-", "1", ",", "2", "*", "(", "p", "[", "1", "]", "-", "y0", ")", "/", "h", "-", "1", ")", "\n", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads.__init__": [[25, 28], ["detectron2.modeling.StandardROIHeads.__init__", "roi_head.DensePoseROIHeads._init_densepose_head"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads._init_densepose_head"], ["def", "__init__", "(", "self", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "cfg", ",", "input_shape", ")", "\n", "self", ".", "_init_densepose_head", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads._init_densepose_head": [[29, 52], ["densepose_head.build_densepose_data_filter", "tuple", "detectron2.modeling.poolers.ROIPooler", "densepose_head.build_densepose_head", "densepose_head.build_densepose_predictor", "densepose_head.build_densepose_losses"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_data_filter", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_head", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_predictor", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_losses"], ["", "def", "_init_densepose_head", "(", "self", ",", "cfg", ")", ":", "\n", "# fmt: off", "\n", "        ", "self", ".", "densepose_on", "=", "cfg", ".", "MODEL", ".", "DENSEPOSE_ON", "\n", "if", "not", "self", ".", "densepose_on", ":", "\n", "            ", "return", "\n", "", "self", ".", "densepose_data_filter", "=", "build_densepose_data_filter", "(", "cfg", ")", "\n", "dp_pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "POOLER_RESOLUTION", "\n", "dp_pooler_scales", "=", "tuple", "(", "1.0", "/", "self", ".", "feature_strides", "[", "k", "]", "for", "k", "in", "self", ".", "in_features", ")", "\n", "dp_pooler_sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "dp_pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "in_channels", "=", "[", "self", ".", "feature_channels", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", "[", "0", "]", "\n", "self", ".", "densepose_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "dp_pooler_resolution", ",", "\n", "scales", "=", "dp_pooler_scales", ",", "\n", "sampling_ratio", "=", "dp_pooler_sampling_ratio", ",", "\n", "pooler_type", "=", "dp_pooler_type", ",", "\n", ")", "\n", "self", ".", "densepose_head", "=", "build_densepose_head", "(", "cfg", ",", "in_channels", ")", "\n", "self", ".", "densepose_predictor", "=", "build_densepose_predictor", "(", "\n", "cfg", ",", "self", ".", "densepose_head", ".", "n_out_channels", "\n", ")", "\n", "self", ".", "densepose_losses", "=", "build_densepose_losses", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads._forward_densepose": [[53, 94], ["detectron2.modeling.roi_heads.select_foreground_proposals", "roi_head.DensePoseROIHeads.densepose_data_filter", "roi_head.DensePoseROIHeads.densepose_pooler", "densepose_head.densepose_inference", "len", "roi_head.DensePoseROIHeads.densepose_pooler", "roi_head.DensePoseROIHeads.densepose_head", "roi_head.DensePoseROIHeads.densepose_predictor", "roi_head.DensePoseROIHeads.densepose_losses", "len", "roi_head.DensePoseROIHeads.densepose_head", "roi_head.DensePoseROIHeads.densepose_predictor", "torch.zeros", "tuple"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.densepose_inference"], ["", "def", "_forward_densepose", "(", "self", ",", "features", ",", "instances", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the densepose prediction branch.\n\n        Args:\n            features (list[Tensor]): #level input features for densepose prediction\n            instances (list[Instances]): the per-image instances to train/predict densepose.\n                In training, they can be the proposals.\n                In inference, they can be the predicted boxes.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields \"densepose\" and return it.\n        \"\"\"", "\n", "if", "not", "self", ".", "densepose_on", ":", "\n", "            ", "return", "{", "}", "if", "self", ".", "training", "else", "instances", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "proposals", ",", "_", "=", "select_foreground_proposals", "(", "instances", ",", "self", ".", "num_classes", ")", "\n", "proposals_dp", "=", "self", ".", "densepose_data_filter", "(", "proposals", ")", "\n", "if", "len", "(", "proposals_dp", ")", ">", "0", ":", "\n", "                ", "proposal_boxes", "=", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals_dp", "]", "\n", "features_dp", "=", "self", ".", "densepose_pooler", "(", "features", ",", "proposal_boxes", ")", "\n", "densepose_head_outputs", "=", "self", ".", "densepose_head", "(", "features_dp", ")", "\n", "densepose_outputs", ",", "_", "=", "self", ".", "densepose_predictor", "(", "densepose_head_outputs", ")", "\n", "densepose_loss_dict", "=", "self", ".", "densepose_losses", "(", "proposals_dp", ",", "densepose_outputs", ")", "\n", "return", "densepose_loss_dict", "\n", "", "", "else", ":", "\n", "            ", "pred_boxes", "=", "[", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", "\n", "features_dp", "=", "self", ".", "densepose_pooler", "(", "features", ",", "pred_boxes", ")", "\n", "if", "len", "(", "features_dp", ")", ">", "0", ":", "\n", "                ", "densepose_head_outputs", "=", "self", ".", "densepose_head", "(", "features_dp", ")", "\n", "densepose_outputs", ",", "_", "=", "self", ".", "densepose_predictor", "(", "densepose_head_outputs", ")", "\n", "", "else", ":", "\n", "# If no detection occurred instances", "\n", "# set densepose_outputs to empty tensors", "\n", "                ", "empty_tensor", "=", "torch", ".", "zeros", "(", "size", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "device", "=", "features_dp", ".", "device", ")", "\n", "densepose_outputs", "=", "tuple", "(", "[", "empty_tensor", "]", "*", "4", ")", "\n", "\n", "", "densepose_inference", "(", "densepose_outputs", ",", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads.forward": [[95, 106], ["super().forward", "losses.update", "roi_head.DensePoseROIHeads._forward_densepose", "roi_head.DensePoseROIHeads._forward_densepose"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.forward", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads._forward_densepose", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.roi_head.DensePoseROIHeads._forward_densepose"], ["", "", "def", "forward", "(", "self", ",", "images", ",", "features", ",", "proposals", ",", "targets", "=", "None", ")", ":", "\n", "        ", "features_list", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", "\n", "\n", "instances", ",", "losses", "=", "super", "(", ")", ".", "forward", "(", "images", ",", "features", ",", "proposals", ",", "targets", ")", "\n", "del", "targets", ",", "images", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "losses", ".", "update", "(", "self", ".", "_forward_densepose", "(", "features_list", ",", "instances", ")", ")", "\n", "", "else", ":", "\n", "            ", "instances", "=", "self", ".", "_forward_densepose", "(", "features_list", ",", "instances", ")", "\n", "", "return", "instances", ",", "losses", "\n", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseDeepLabHead.__init__": [[26, 62], ["torch.nn.Module.__init__", "densepose_head.ASPP", "densepose_head.DensePoseDeepLabHead.add_module", "range", "densepose_head.NONLocalBlock2D", "densepose_head.DensePoseDeepLabHead.add_module", "detectron2.layers.Conv2d", "fvcore.c2_msra_fill", "densepose_head.DensePoseDeepLabHead._get_layer_name", "densepose_head.DensePoseDeepLabHead.add_module", "torch.nn.GroupNorm"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead._get_layer_name"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "input_channels", ")", ":", "\n", "        ", "super", "(", "DensePoseDeepLabHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# fmt: off", "\n", "hidden_dim", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "CONV_HEAD_DIM", "\n", "kernel_size", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "CONV_HEAD_KERNEL", "\n", "norm", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "DEEPLAB", ".", "NORM", "\n", "self", ".", "n_stacked_convs", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "NUM_STACKED_CONVS", "\n", "self", ".", "use_nonlocal", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "DEEPLAB", ".", "NONLOCAL_ON", "\n", "# fmt: on", "\n", "pad_size", "=", "kernel_size", "//", "2", "\n", "n_channels", "=", "input_channels", "\n", "\n", "self", ".", "ASPP", "=", "ASPP", "(", "input_channels", ",", "[", "6", ",", "12", ",", "56", "]", ",", "n_channels", ")", "# 6, 12, 56", "\n", "self", ".", "add_module", "(", "\"ASPP\"", ",", "self", ".", "ASPP", ")", "\n", "\n", "if", "self", ".", "use_nonlocal", ":", "\n", "            ", "self", ".", "NLBlock", "=", "NONLocalBlock2D", "(", "input_channels", ",", "bn_layer", "=", "True", ")", "\n", "self", ".", "add_module", "(", "\"NLBlock\"", ",", "self", ".", "NLBlock", ")", "\n", "# weight_init.c2_msra_fill(self.ASPP)", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "n_stacked_convs", ")", ":", "\n", "            ", "norm_module", "=", "nn", ".", "GroupNorm", "(", "32", ",", "hidden_dim", ")", "if", "norm", "==", "\"GN\"", "else", "None", "\n", "layer", "=", "Conv2d", "(", "\n", "n_channels", ",", "\n", "hidden_dim", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "pad_size", ",", "\n", "bias", "=", "not", "norm", ",", "\n", "norm", "=", "norm_module", ",", "\n", ")", "\n", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "n_channels", "=", "hidden_dim", "\n", "layer_name", "=", "self", ".", "_get_layer_name", "(", "i", ")", "\n", "self", ".", "add_module", "(", "layer_name", ",", "layer", ")", "\n", "", "self", ".", "n_out_channels", "=", "hidden_dim", "\n", "# initialize_module_params(self)", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseDeepLabHead.forward": [[64, 76], ["densepose_head.DensePoseDeepLabHead.ASPP", "range", "densepose_head.DensePoseDeepLabHead.NLBlock", "densepose_head.DensePoseDeepLabHead._get_layer_name", "torch.nn.functional.relu", "getattr"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead._get_layer_name"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "x0", "=", "features", "\n", "x", "=", "self", ".", "ASPP", "(", "x0", ")", "\n", "if", "self", ".", "use_nonlocal", ":", "\n", "            ", "x", "=", "self", ".", "NLBlock", "(", "x", ")", "\n", "", "output", "=", "x", "\n", "for", "i", "in", "range", "(", "self", ".", "n_stacked_convs", ")", ":", "\n", "            ", "layer_name", "=", "self", ".", "_get_layer_name", "(", "i", ")", "\n", "x", "=", "getattr", "(", "self", ",", "layer_name", ")", "(", "x", ")", "\n", "x", "=", "F", ".", "relu", "(", "x", ")", "\n", "output", "=", "x", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseDeepLabHead._get_layer_name": [[77, 80], ["None"], "methods", ["None"], ["", "def", "_get_layer_name", "(", "self", ",", "i", ")", ":", "\n", "        ", "layer_name", "=", "\"body_conv_fcn{}\"", ".", "format", "(", "i", "+", "1", ")", "\n", "return", "layer_name", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.ASPPConv.__init__": [[86, 95], ["torch.nn.Sequential.__init__", "torch.nn.Conv2d", "torch.nn.GroupNorm", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "dilation", ")", ":", "\n", "        ", "modules", "=", "[", "\n", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "3", ",", "padding", "=", "dilation", ",", "dilation", "=", "dilation", ",", "bias", "=", "False", "\n", ")", ",", "\n", "nn", ".", "GroupNorm", "(", "32", ",", "out_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "]", "\n", "super", "(", "ASPPConv", ",", "self", ")", ".", "__init__", "(", "*", "modules", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.ASPPPooling.__init__": [[98, 104], ["torch.nn.Sequential.__init__", "torch.nn.AdaptiveAvgPool2d", "torch.nn.Conv2d", "torch.nn.GroupNorm", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ")", ":", "\n", "        ", "super", "(", "ASPPPooling", ",", "self", ")", ".", "__init__", "(", "\n", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", ",", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "GroupNorm", "(", "32", ",", "out_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.ASPPPooling.forward": [[106, 110], ["super().forward", "torch.nn.functional.interpolate"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size", "=", "x", ".", "shape", "[", "-", "2", ":", "]", "\n", "x", "=", "super", "(", "ASPPPooling", ",", "self", ")", ".", "forward", "(", "x", ")", "\n", "return", "F", ".", "interpolate", "(", "x", ",", "size", "=", "size", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.ASPP.__init__": [[113, 136], ["torch.nn.Module.__init__", "modules.append", "tuple", "modules.append", "modules.append", "modules.append", "modules.append", "torch.nn.ModuleList", "torch.nn.Sequential", "torch.nn.Sequential", "densepose_head.ASPPConv", "densepose_head.ASPPConv", "densepose_head.ASPPConv", "densepose_head.ASPPPooling", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.Conv2d", "torch.nn.GroupNorm", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "atrous_rates", ",", "out_channels", ")", ":", "\n", "        ", "super", "(", "ASPP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "modules", "=", "[", "]", "\n", "modules", ".", "append", "(", "\n", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "GroupNorm", "(", "32", ",", "out_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "rate1", ",", "rate2", ",", "rate3", "=", "tuple", "(", "atrous_rates", ")", "\n", "modules", ".", "append", "(", "ASPPConv", "(", "in_channels", ",", "out_channels", ",", "rate1", ")", ")", "\n", "modules", ".", "append", "(", "ASPPConv", "(", "in_channels", ",", "out_channels", ",", "rate2", ")", ")", "\n", "modules", ".", "append", "(", "ASPPConv", "(", "in_channels", ",", "out_channels", ",", "rate3", ")", ")", "\n", "modules", ".", "append", "(", "ASPPPooling", "(", "in_channels", ",", "out_channels", ")", ")", "\n", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "modules", ")", "\n", "\n", "self", ".", "project", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "5", "*", "out_channels", ",", "out_channels", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "# nn.BatchNorm2d(out_channels),", "\n", "nn", ".", "ReLU", "(", ")", "\n", "# nn.Dropout(0.5)", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.ASPP.forward": [[139, 145], ["torch.cat", "densepose_head.ASPP.project", "torch.cat.append", "conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "res", "=", "[", "]", "\n", "for", "conv", "in", "self", ".", "convs", ":", "\n", "            ", "res", ".", "append", "(", "conv", "(", "x", ")", ")", "\n", "", "res", "=", "torch", ".", "cat", "(", "res", ",", "dim", "=", "1", ")", "\n", "return", "self", ".", "project", "(", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._NonLocalBlockND.__init__": [[151, 232], ["torch.nn.Module.__init__", "conv_nd", "conv_nd", "conv_nd", "torch.nn.MaxPool3d", "torch.nn.Sequential", "torch.nn.init.constant_", "torch.nn.init.constant_", "conv_nd", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.MaxPool2d", "torch.nn.MaxPool1d", "conv_nd", "bn"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "inter_channels", "=", "None", ",", "dimension", "=", "3", ",", "sub_sample", "=", "True", ",", "bn_layer", "=", "True", "\n", ")", ":", "\n", "        ", "super", "(", "_NonLocalBlockND", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "dimension", "in", "[", "1", ",", "2", ",", "3", "]", "\n", "\n", "self", ".", "dimension", "=", "dimension", "\n", "self", ".", "sub_sample", "=", "sub_sample", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "inter_channels", "=", "inter_channels", "\n", "\n", "if", "self", ".", "inter_channels", "is", "None", ":", "\n", "            ", "self", ".", "inter_channels", "=", "in_channels", "//", "2", "\n", "if", "self", ".", "inter_channels", "==", "0", ":", "\n", "                ", "self", ".", "inter_channels", "=", "1", "\n", "\n", "", "", "if", "dimension", "==", "3", ":", "\n", "            ", "conv_nd", "=", "nn", ".", "Conv3d", "\n", "max_pool_layer", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "1", ",", "2", ",", "2", ")", ")", "\n", "bn", "=", "nn", ".", "GroupNorm", "# (32, hidden_dim) #nn.BatchNorm3d", "\n", "", "elif", "dimension", "==", "2", ":", "\n", "            ", "conv_nd", "=", "nn", ".", "Conv2d", "\n", "max_pool_layer", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "(", "2", ",", "2", ")", ")", "\n", "bn", "=", "nn", ".", "GroupNorm", "# (32, hidden_dim)nn.BatchNorm2d", "\n", "", "else", ":", "\n", "            ", "conv_nd", "=", "nn", ".", "Conv1d", "\n", "max_pool_layer", "=", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "(", "2", ")", ")", "\n", "bn", "=", "nn", ".", "GroupNorm", "# (32, hidden_dim)nn.BatchNorm1d", "\n", "\n", "", "self", ".", "g", "=", "conv_nd", "(", "\n", "in_channels", "=", "self", ".", "in_channels", ",", "\n", "out_channels", "=", "self", ".", "inter_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", ")", "\n", "\n", "if", "bn_layer", ":", "\n", "            ", "self", ".", "W", "=", "nn", ".", "Sequential", "(", "\n", "conv_nd", "(", "\n", "in_channels", "=", "self", ".", "inter_channels", ",", "\n", "out_channels", "=", "self", ".", "in_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", ")", ",", "\n", "bn", "(", "32", ",", "self", ".", "in_channels", ")", ",", "\n", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "W", "[", "1", "]", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "W", "[", "1", "]", ".", "bias", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "W", "=", "conv_nd", "(", "\n", "in_channels", "=", "self", ".", "inter_channels", ",", "\n", "out_channels", "=", "self", ".", "in_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "W", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "W", ".", "bias", ",", "0", ")", "\n", "\n", "", "self", ".", "theta", "=", "conv_nd", "(", "\n", "in_channels", "=", "self", ".", "in_channels", ",", "\n", "out_channels", "=", "self", ".", "inter_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", ")", "\n", "self", ".", "phi", "=", "conv_nd", "(", "\n", "in_channels", "=", "self", ".", "in_channels", ",", "\n", "out_channels", "=", "self", ".", "inter_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", ")", "\n", "\n", "if", "sub_sample", ":", "\n", "            ", "self", ".", "g", "=", "nn", ".", "Sequential", "(", "self", ".", "g", ",", "max_pool_layer", ")", "\n", "self", ".", "phi", "=", "nn", ".", "Sequential", "(", "self", ".", "phi", ",", "max_pool_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._NonLocalBlockND.forward": [[233, 257], ["x.size", "densepose_head._NonLocalBlockND.g().view", "g_x.permute.permute.permute", "densepose_head._NonLocalBlockND.theta().view", "theta_x.permute.permute.permute", "densepose_head._NonLocalBlockND.phi().view", "torch.matmul", "torch.nn.functional.softmax", "torch.matmul", "y.view.view.permute().contiguous", "y.view.view.view", "densepose_head._NonLocalBlockND.W", "densepose_head._NonLocalBlockND.g", "densepose_head._NonLocalBlockND.theta", "densepose_head._NonLocalBlockND.phi", "y.view.view.permute", "x.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        :param x: (b, c, t, h, w)\n        :return:\n        \"\"\"", "\n", "\n", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "\n", "g_x", "=", "self", ".", "g", "(", "x", ")", ".", "view", "(", "batch_size", ",", "self", ".", "inter_channels", ",", "-", "1", ")", "\n", "g_x", "=", "g_x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "theta_x", "=", "self", ".", "theta", "(", "x", ")", ".", "view", "(", "batch_size", ",", "self", ".", "inter_channels", ",", "-", "1", ")", "\n", "theta_x", "=", "theta_x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "phi_x", "=", "self", ".", "phi", "(", "x", ")", ".", "view", "(", "batch_size", ",", "self", ".", "inter_channels", ",", "-", "1", ")", "\n", "f", "=", "torch", ".", "matmul", "(", "theta_x", ",", "phi_x", ")", "\n", "f_div_C", "=", "F", ".", "softmax", "(", "f", ",", "dim", "=", "-", "1", ")", "\n", "\n", "y", "=", "torch", ".", "matmul", "(", "f_div_C", ",", "g_x", ")", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "y", "=", "y", ".", "view", "(", "batch_size", ",", "self", ".", "inter_channels", ",", "*", "x", ".", "size", "(", ")", "[", "2", ":", "]", ")", "\n", "W_y", "=", "self", ".", "W", "(", "y", ")", "\n", "z", "=", "W_y", "+", "x", "\n", "\n", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.NONLocalBlock2D.__init__": [[260, 267], ["densepose_head._NonLocalBlockND.__init__"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "inter_channels", "=", "None", ",", "sub_sample", "=", "True", ",", "bn_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", "NONLocalBlock2D", ",", "self", ")", ".", "__init__", "(", "\n", "in_channels", ",", "\n", "inter_channels", "=", "inter_channels", ",", "\n", "dimension", "=", "2", ",", "\n", "sub_sample", "=", "sub_sample", ",", "\n", "bn_layer", "=", "bn_layer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead.__init__": [[272, 288], ["torch.nn.Module.__init__", "range", "densepose_head.initialize_module_params", "detectron2.layers.Conv2d", "densepose_head.DensePoseV1ConvXHead._get_layer_name", "densepose_head.DensePoseV1ConvXHead.add_module"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.initialize_module_params", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead._get_layer_name"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "input_channels", ")", ":", "\n", "        ", "super", "(", "DensePoseV1ConvXHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# fmt: off", "\n", "hidden_dim", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "CONV_HEAD_DIM", "\n", "kernel_size", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "CONV_HEAD_KERNEL", "\n", "self", ".", "n_stacked_convs", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "NUM_STACKED_CONVS", "\n", "# fmt: on", "\n", "pad_size", "=", "kernel_size", "//", "2", "\n", "n_channels", "=", "input_channels", "\n", "for", "i", "in", "range", "(", "self", ".", "n_stacked_convs", ")", ":", "\n", "            ", "layer", "=", "Conv2d", "(", "n_channels", ",", "hidden_dim", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "pad_size", ")", "\n", "layer_name", "=", "self", ".", "_get_layer_name", "(", "i", ")", "\n", "self", ".", "add_module", "(", "layer_name", ",", "layer", ")", "\n", "n_channels", "=", "hidden_dim", "\n", "", "self", ".", "n_out_channels", "=", "n_channels", "\n", "initialize_module_params", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead.forward": [[289, 298], ["range", "densepose_head.DensePoseV1ConvXHead._get_layer_name", "torch.nn.functional.relu", "getattr"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead._get_layer_name"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "x", "=", "features", "\n", "output", "=", "x", "\n", "for", "i", "in", "range", "(", "self", ".", "n_stacked_convs", ")", ":", "\n", "            ", "layer_name", "=", "self", ".", "_get_layer_name", "(", "i", ")", "\n", "x", "=", "getattr", "(", "self", ",", "layer_name", ")", "(", "x", ")", "\n", "x", "=", "F", ".", "relu", "(", "x", ")", "\n", "output", "=", "x", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseV1ConvXHead._get_layer_name": [[299, 302], ["None"], "methods", ["None"], ["", "def", "_get_layer_name", "(", "self", ",", "i", ")", ":", "\n", "        ", "layer_name", "=", "\"body_conv_fcn{}\"", ".", "format", "(", "i", "+", "1", ")", "\n", "return", "layer_name", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePosePredictor.__init__": [[305, 326], ["torch.nn.Module.__init__", "detectron2.layers.ConvTranspose2d", "detectron2.layers.ConvTranspose2d", "detectron2.layers.ConvTranspose2d", "detectron2.layers.ConvTranspose2d", "densepose_head.initialize_module_params", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.initialize_module_params"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "input_channels", ")", ":", "\n", "\n", "        ", "super", "(", "DensePosePredictor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "dim_in", "=", "input_channels", "\n", "n_segm_chan", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "NUM_COARSE_SEGM_CHANNELS", "\n", "dim_out_patches", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "NUM_PATCHES", "+", "1", "\n", "kernel_size", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "DECONV_KERNEL", "\n", "self", ".", "ann_index_lowres", "=", "ConvTranspose2d", "(", "\n", "dim_in", ",", "n_segm_chan", ",", "kernel_size", ",", "stride", "=", "2", ",", "padding", "=", "int", "(", "kernel_size", "/", "2", "-", "1", ")", "\n", ")", "\n", "self", ".", "index_uv_lowres", "=", "ConvTranspose2d", "(", "\n", "dim_in", ",", "dim_out_patches", ",", "kernel_size", ",", "stride", "=", "2", ",", "padding", "=", "int", "(", "kernel_size", "/", "2", "-", "1", ")", "\n", ")", "\n", "self", ".", "u_lowres", "=", "ConvTranspose2d", "(", "\n", "dim_in", ",", "dim_out_patches", ",", "kernel_size", ",", "stride", "=", "2", ",", "padding", "=", "int", "(", "kernel_size", "/", "2", "-", "1", ")", "\n", ")", "\n", "self", ".", "v_lowres", "=", "ConvTranspose2d", "(", "\n", "dim_in", ",", "dim_out_patches", ",", "kernel_size", ",", "stride", "=", "2", ",", "padding", "=", "int", "(", "kernel_size", "/", "2", "-", "1", ")", "\n", ")", "\n", "self", ".", "scale_factor", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "UP_SCALE", "\n", "initialize_module_params", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePosePredictor.forward": [[327, 345], ["densepose_head.DensePosePredictor.ann_index_lowres", "densepose_head.DensePosePredictor.index_uv_lowres", "densepose_head.DensePosePredictor.u_lowres", "densepose_head.DensePosePredictor.v_lowres", "densepose_head.DensePosePredictor.forward.interp2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "head_outputs", ")", ":", "\n", "        ", "ann_index_lowres", "=", "self", ".", "ann_index_lowres", "(", "head_outputs", ")", "\n", "index_uv_lowres", "=", "self", ".", "index_uv_lowres", "(", "head_outputs", ")", "\n", "u_lowres", "=", "self", ".", "u_lowres", "(", "head_outputs", ")", "\n", "v_lowres", "=", "self", ".", "v_lowres", "(", "head_outputs", ")", "\n", "\n", "def", "interp2d", "(", "input", ")", ":", "\n", "            ", "return", "interpolate", "(", "\n", "input", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", "\n", ")", "\n", "\n", "", "ann_index", "=", "interp2d", "(", "ann_index_lowres", ")", "\n", "index_uv", "=", "interp2d", "(", "index_uv_lowres", ")", "\n", "u", "=", "interp2d", "(", "u_lowres", ")", "\n", "v", "=", "interp2d", "(", "v_lowres", ")", "\n", "return", "(", "\n", "(", "ann_index", ",", "index_uv", ",", "u", ",", "v", ")", ",", "\n", "(", "ann_index_lowres", ",", "index_uv_lowres", ",", "u_lowres", ",", "v_lowres", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseDataFilter.__init__": [[349, 351], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "self", ".", "iou_threshold", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "FG_IOU_THRESHOLD", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseDataFilter.__call__": [[352, 386], ["torch.no_grad", "hasattr", "hasattr", "detectron2.structures.boxes.matched_boxlist_iou", "proposals_filtered.append", "hasattr", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "enumerate"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "__call__", "(", "self", ",", "proposals_with_targets", ")", ":", "\n", "        ", "\"\"\"\n        Filters proposals with targets to keep only the ones relevant for\n        DensePose training\n        proposals: list(Instances), each element of the list corresponds to\n            various instances (proposals, GT for boxes and densepose) for one\n            image\n        \"\"\"", "\n", "proposals_filtered", "=", "[", "]", "\n", "for", "proposals_per_image", "in", "proposals_with_targets", ":", "\n", "            ", "if", "not", "hasattr", "(", "proposals_per_image", ",", "\"gt_densepose\"", ")", ":", "\n", "                ", "continue", "\n", "", "assert", "hasattr", "(", "proposals_per_image", ",", "\"gt_boxes\"", ")", "\n", "assert", "hasattr", "(", "proposals_per_image", ",", "\"proposal_boxes\"", ")", "\n", "gt_boxes", "=", "proposals_per_image", ".", "gt_boxes", "\n", "est_boxes", "=", "proposals_per_image", ".", "proposal_boxes", "\n", "# apply match threshold for densepose head", "\n", "iou", "=", "matched_boxlist_iou", "(", "gt_boxes", ",", "est_boxes", ")", "\n", "iou_select", "=", "iou", ">", "self", ".", "iou_threshold", "\n", "proposals_per_image", "=", "proposals_per_image", "[", "iou_select", "]", "\n", "assert", "len", "(", "proposals_per_image", ".", "gt_boxes", ")", "==", "len", "(", "proposals_per_image", ".", "proposal_boxes", ")", "\n", "# filter out any target without densepose annotation", "\n", "gt_densepose", "=", "proposals_per_image", ".", "gt_densepose", "\n", "assert", "len", "(", "proposals_per_image", ".", "gt_boxes", ")", "==", "len", "(", "proposals_per_image", ".", "gt_densepose", ")", "\n", "selected_indices", "=", "[", "\n", "i", "for", "i", ",", "dp_target", "in", "enumerate", "(", "gt_densepose", ")", "if", "dp_target", "is", "not", "None", "\n", "]", "\n", "if", "len", "(", "selected_indices", ")", "!=", "len", "(", "gt_densepose", ")", ":", "\n", "                ", "proposals_per_image", "=", "proposals_per_image", "[", "selected_indices", "]", "\n", "", "assert", "len", "(", "proposals_per_image", ".", "gt_boxes", ")", "==", "len", "(", "proposals_per_image", ".", "proposal_boxes", ")", "\n", "assert", "len", "(", "proposals_per_image", ".", "gt_boxes", ")", "==", "len", "(", "proposals_per_image", ".", "gt_densepose", ")", "\n", "proposals_filtered", ".", "append", "(", "proposals_per_image", ")", "\n", "", "return", "proposals_filtered", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseLosses.__init__": [[753, 760], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ")", ":", "\n", "# fmt: off", "\n", "        ", "self", ".", "heatmap_size", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "HEATMAP_SIZE", "\n", "self", ".", "w_points", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "POINT_REGRESSION_WEIGHTS", "\n", "self", ".", "w_part", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "PART_WEIGHTS", "\n", "self", ".", "w_segm", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "INDEX_WEIGHTS", "\n", "self", ".", "n_segm_chan", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "NUM_COARSE_SEGM_CHANNELS", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.DensePoseLosses.__call__": [[762, 874], ["len", "u.size", "u.size", "densepose_head._grid_sampling_utilities", "densepose_head._extract_at_points_packed", "densepose_head._extract_at_points_packed", "densepose_head._extract_at_points_packed", "u.size", "v.size", "u.size", "v.size", "u.size", "index_uv.size", "u.size", "index_uv.size", "torch.no_grad", "densepose_head._extract_single_tensors_from_matches", "slice", "torch.no_grad", "_resample_data().squeeze", "torch.nn.functional.smooth_l1_loss", "torch.nn.functional.smooth_l1_loss", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "u.sum", "v.sum", "index_uv.sum", "s.sum", "index_uv_gt.long", "_resample_data().squeeze.long", "densepose_head._resample_data", "_resample_data().squeeze.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._grid_sampling_utilities", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_at_points_packed", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_at_points_packed", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_at_points_packed", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_single_tensors_from_matches", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._resample_data"], ["", "def", "__call__", "(", "self", ",", "proposals_with_gt", ",", "densepose_outputs", ")", ":", "\n", "        ", "losses", "=", "{", "}", "\n", "# densepose outputs are computed for all images and all bounding boxes;", "\n", "# i.e. if a batch has 4 images with (3, 1, 2, 1) proposals respectively,", "\n", "# the outputs will have size(0) == 3+1+2+1 == 7", "\n", "s", ",", "index_uv", ",", "u", ",", "v", "=", "densepose_outputs", "\n", "assert", "u", ".", "size", "(", "2", ")", "==", "v", ".", "size", "(", "2", ")", "\n", "assert", "u", ".", "size", "(", "3", ")", "==", "v", ".", "size", "(", "3", ")", "\n", "assert", "u", ".", "size", "(", "2", ")", "==", "index_uv", ".", "size", "(", "2", ")", "\n", "assert", "u", ".", "size", "(", "3", ")", "==", "index_uv", ".", "size", "(", "3", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "index_uv_img", ",", "i_with_dp", ",", "bbox_xywh_est", ",", "bbox_xywh_gt", ",", "index_gt_all", ",", "x_norm", ",", "y_norm", ",", "u_gt_all", ",", "v_gt_all", ",", "s_gt", ",", "index_bbox", "=", "_extract_single_tensors_from_matches", "(", "# noqa", "\n", "proposals_with_gt", "\n", ")", "\n", "", "n_batch", "=", "len", "(", "i_with_dp", ")", "\n", "\n", "# NOTE: we need to keep the same computation graph on all the GPUs to", "\n", "# perform reduction properly. Hence even if we have no data on one", "\n", "# of the GPUs, we still need to generate the computation graph.", "\n", "# Add fake (zero) loss in the form Tensor.sum() * 0", "\n", "if", "not", "n_batch", ":", "\n", "            ", "losses", "[", "\"loss_densepose_U\"", "]", "=", "u", ".", "sum", "(", ")", "*", "0", "\n", "losses", "[", "\"loss_densepose_V\"", "]", "=", "v", ".", "sum", "(", ")", "*", "0", "\n", "losses", "[", "\"loss_densepose_I\"", "]", "=", "index_uv", ".", "sum", "(", ")", "*", "0", "\n", "losses", "[", "\"loss_densepose_S\"", "]", "=", "s", ".", "sum", "(", ")", "*", "0", "\n", "return", "losses", "\n", "\n", "", "zh", "=", "u", ".", "size", "(", "2", ")", "\n", "zw", "=", "u", ".", "size", "(", "3", ")", "\n", "\n", "j_valid", ",", "y_lo", ",", "y_hi", ",", "x_lo", ",", "x_hi", ",", "w_ylo_xlo", ",", "w_ylo_xhi", ",", "w_yhi_xlo", ",", "w_yhi_xhi", "=", "_grid_sampling_utilities", "(", "# noqa", "\n", "zh", ",", "zw", ",", "bbox_xywh_est", ",", "bbox_xywh_gt", ",", "index_gt_all", ",", "x_norm", ",", "y_norm", ",", "index_bbox", "\n", ")", "\n", "\n", "j_valid_fg", "=", "j_valid", "*", "(", "index_gt_all", ">", "0", ")", "\n", "\n", "u_gt", "=", "u_gt_all", "[", "j_valid_fg", "]", "\n", "u_est_all", "=", "_extract_at_points_packed", "(", "\n", "u", "[", "i_with_dp", "]", ",", "\n", "index_bbox", ",", "\n", "index_gt_all", ",", "\n", "y_lo", ",", "\n", "y_hi", ",", "\n", "x_lo", ",", "\n", "x_hi", ",", "\n", "w_ylo_xlo", ",", "\n", "w_ylo_xhi", ",", "\n", "w_yhi_xlo", ",", "\n", "w_yhi_xhi", ",", "\n", ")", "\n", "u_est", "=", "u_est_all", "[", "j_valid_fg", "]", "\n", "\n", "v_gt", "=", "v_gt_all", "[", "j_valid_fg", "]", "\n", "v_est_all", "=", "_extract_at_points_packed", "(", "\n", "v", "[", "i_with_dp", "]", ",", "\n", "index_bbox", ",", "\n", "index_gt_all", ",", "\n", "y_lo", ",", "\n", "y_hi", ",", "\n", "x_lo", ",", "\n", "x_hi", ",", "\n", "w_ylo_xlo", ",", "\n", "w_ylo_xhi", ",", "\n", "w_yhi_xlo", ",", "\n", "w_yhi_xhi", ",", "\n", ")", "\n", "v_est", "=", "v_est_all", "[", "j_valid_fg", "]", "\n", "\n", "index_uv_gt", "=", "index_gt_all", "[", "j_valid", "]", "\n", "index_uv_est_all", "=", "_extract_at_points_packed", "(", "\n", "index_uv", "[", "i_with_dp", "]", ",", "\n", "index_bbox", ",", "\n", "slice", "(", "None", ")", ",", "\n", "y_lo", ",", "\n", "y_hi", ",", "\n", "x_lo", ",", "\n", "x_hi", ",", "\n", "w_ylo_xlo", "[", ":", ",", "None", "]", ",", "\n", "w_ylo_xhi", "[", ":", ",", "None", "]", ",", "\n", "w_yhi_xlo", "[", ":", ",", "None", "]", ",", "\n", "w_yhi_xhi", "[", ":", ",", "None", "]", ",", "\n", ")", "\n", "index_uv_est", "=", "index_uv_est_all", "[", "j_valid", ",", ":", "]", "\n", "\n", "# Resample everything to the estimated data size, no need to resample", "\n", "# S_est then:", "\n", "s_est", "=", "s", "[", "i_with_dp", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "s_gt", "=", "_resample_data", "(", "\n", "s_gt", ".", "unsqueeze", "(", "1", ")", ",", "\n", "bbox_xywh_gt", ",", "\n", "bbox_xywh_est", ",", "\n", "self", ".", "heatmap_size", ",", "\n", "self", ".", "heatmap_size", ",", "\n", "mode", "=", "\"nearest\"", ",", "\n", "padding_mode", "=", "\"zeros\"", ",", "\n", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# add point-based losses:", "\n", "", "u_loss", "=", "F", ".", "smooth_l1_loss", "(", "u_est", ",", "u_gt", ",", "reduction", "=", "\"sum\"", ")", "*", "self", ".", "w_points", "\n", "losses", "[", "\"loss_densepose_U\"", "]", "=", "u_loss", "\n", "v_loss", "=", "F", ".", "smooth_l1_loss", "(", "v_est", ",", "v_gt", ",", "reduction", "=", "\"sum\"", ")", "*", "self", ".", "w_points", "\n", "losses", "[", "\"loss_densepose_V\"", "]", "=", "v_loss", "\n", "index_uv_loss", "=", "F", ".", "cross_entropy", "(", "index_uv_est", ",", "index_uv_gt", ".", "long", "(", ")", ")", "*", "self", ".", "w_part", "\n", "losses", "[", "\"loss_densepose_I\"", "]", "=", "index_uv_loss", "\n", "\n", "if", "self", ".", "n_segm_chan", "==", "2", ":", "\n", "            ", "s_gt", "=", "s_gt", ">", "0", "\n", "", "s_loss", "=", "F", ".", "cross_entropy", "(", "s_est", ",", "s_gt", ".", "long", "(", ")", ")", "*", "self", ".", "w_segm", "\n", "losses", "[", "\"loss_densepose_S\"", "]", "=", "s_loss", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.initialize_module_params": [[16, 22], ["module.named_parameters", "torch.nn.init.constant_", "torch.nn.init.kaiming_normal_"], "function", ["None"], ["def", "initialize_module_params", "(", "module", ")", ":", "\n", "    ", "for", "name", ",", "param", "in", "module", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "\"bias\"", "in", "name", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "param", ",", "0", ")", "\n", "", "elif", "\"weight\"", "in", "name", ":", "\n", "            ", "nn", ".", "init", ".", "kaiming_normal_", "(", "param", ",", "mode", "=", "\"fan_out\"", ",", "nonlinearity", "=", "\"relu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_head": [[388, 391], ["ROI_DENSEPOSE_HEAD_REGISTRY.get"], "function", ["None"], ["", "", "def", "build_densepose_head", "(", "cfg", ",", "input_channels", ")", ":", "\n", "    ", "head_name", "=", "cfg", ".", "MODEL", ".", "ROI_DENSEPOSE_HEAD", ".", "NAME", "\n", "return", "ROI_DENSEPOSE_HEAD_REGISTRY", ".", "get", "(", "head_name", ")", "(", "cfg", ",", "input_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_predictor": [[393, 396], ["densepose_head.DensePosePredictor"], "function", ["None"], ["", "def", "build_densepose_predictor", "(", "cfg", ",", "input_channels", ")", ":", "\n", "    ", "predictor", "=", "DensePosePredictor", "(", "cfg", ",", "input_channels", ")", "\n", "return", "predictor", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_data_filter": [[398, 401], ["densepose_head.DensePoseDataFilter"], "function", ["None"], ["", "def", "build_densepose_data_filter", "(", "cfg", ")", ":", "\n", "    ", "dp_filter", "=", "DensePoseDataFilter", "(", "cfg", ")", "\n", "return", "dp_filter", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.densepose_inference": [[403, 438], ["len", "structures.DensePoseOutput"], "function", ["None"], ["", "def", "densepose_inference", "(", "densepose_outputs", ",", "detections", ")", ":", "\n", "    ", "\"\"\"\n    Infer dense pose estimate based on outputs from the DensePose head\n    and detections. The estimate for each detection instance is stored in its\n    \"pred_densepose\" attribute.\n\n    Args:\n        densepose_outputs (tuple(`torch.Tensor`)): iterable containing 4 elements:\n            - s (:obj: `torch.Tensor`): segmentation tensor of size (N, A, H, W),\n            - i (:obj: `torch.Tensor`): classification tensor of size (N, C, H, W),\n            - u (:obj: `torch.Tensor`): U coordinates for each class of size (N, C, H, W),\n            - v (:obj: `torch.Tensor`): V coordinates for each class of size (N, C, H, W),\n            where N is the total number of detections in a batch,\n                  A is the number of segmentations classes (e.g. 15 for coarse body parts),\n                  C is the number of labels (e.g. 25 for fine body parts),\n                  W is the resolution along the X axis\n                  H is the resolution along the Y axis\n        detections (list[Instances]): A list of N Instances, where N is the number of images\n            in the batch. Instances are modified by this method: \"pred_densepose\" attribute\n            is added to each instance, the attribute contains the corresponding\n            DensePoseOutput object.\n    \"\"\"", "\n", "\n", "# DensePose outputs: segmentation, body part indices, U, V", "\n", "s", ",", "index_uv", ",", "u", ",", "v", "=", "densepose_outputs", "\n", "k", "=", "0", "\n", "for", "detection", "in", "detections", ":", "\n", "        ", "n_i", "=", "len", "(", "detection", ")", "\n", "s_i", "=", "s", "[", "k", ":", "k", "+", "n_i", "]", "\n", "index_uv_i", "=", "index_uv", "[", "k", ":", "k", "+", "n_i", "]", "\n", "u_i", "=", "u", "[", "k", ":", "k", "+", "n_i", "]", "\n", "v_i", "=", "v", "[", "k", ":", "k", "+", "n_i", "]", "\n", "densepose_output_i", "=", "DensePoseOutput", "(", "s_i", ",", "index_uv_i", ",", "u_i", ",", "v_i", ")", "\n", "detection", ".", "pred_densepose", "=", "densepose_output_i", "\n", "k", "+=", "n_i", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._linear_interpolation_utilities": [[440, 484], ["torch.min.floor().long().clamp", "torch.min", "v_hi.float", "v_grid.floor().long().clamp.float", "torch.min.floor().long", "torch.min.floor"], "function", ["None"], ["", "", "def", "_linear_interpolation_utilities", "(", "v_norm", ",", "v0_src", ",", "size_src", ",", "v0_dst", ",", "size_dst", ",", "size_z", ")", ":", "\n", "    ", "\"\"\"\n    Computes utility values for linear interpolation at points v.\n    The points are given as normalized offsets in the source interval\n    (v0_src, v0_src + size_src), more precisely:\n        v = v0_src + v_norm * size_src / 256.0\n    The computed utilities include lower points v_lo, upper points v_hi,\n    interpolation weights v_w and flags j_valid indicating whether the\n    points falls into the destination interval (v0_dst, v0_dst + size_dst).\n\n    Args:\n        v_norm (:obj: `torch.Tensor`): tensor of size N containing\n            normalized point offsets\n        v0_src (:obj: `torch.Tensor`): tensor of size N containing\n            left bounds of source intervals for normalized points\n        size_src (:obj: `torch.Tensor`): tensor of size N containing\n            source interval sizes for normalized points\n        v0_dst (:obj: `torch.Tensor`): tensor of size N containing\n            left bounds of destination intervals\n        size_dst (:obj: `torch.Tensor`): tensor of size N containing\n            destination interval sizes\n        size_z (int): interval size for data to be interpolated\n\n    Returns:\n        v_lo (:obj: `torch.Tensor`): int tensor of size N containing\n            indices of lower values used for interpolation, all values are\n            integers from [0, size_z - 1]\n        v_hi (:obj: `torch.Tensor`): int tensor of size N containing\n            indices of upper values used for interpolation, all values are\n            integers from [0, size_z - 1]\n        v_w (:obj: `torch.Tensor`): float tensor of size N containing\n            interpolation weights\n        j_valid (:obj: `torch.Tensor`): uint8 tensor of size N containing\n            0 for points outside the estimation interval\n            (v0_est, v0_est + size_est) and 1 otherwise\n    \"\"\"", "\n", "v", "=", "v0_src", "+", "v_norm", "*", "size_src", "/", "256.0", "\n", "j_valid", "=", "(", "v", "-", "v0_dst", ">=", "0", ")", "*", "(", "v", "-", "v0_dst", "<", "size_dst", ")", "\n", "v_grid", "=", "(", "v", "-", "v0_dst", ")", "*", "size_z", "/", "size_dst", "\n", "v_lo", "=", "v_grid", ".", "floor", "(", ")", ".", "long", "(", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "size_z", "-", "1", ")", "\n", "v_hi", "=", "(", "v_lo", "+", "1", ")", ".", "clamp", "(", "max", "=", "size_z", "-", "1", ")", "\n", "v_grid", "=", "torch", ".", "min", "(", "v_hi", ".", "float", "(", ")", ",", "v_grid", ")", "\n", "v_w", "=", "v_grid", "-", "v_lo", ".", "float", "(", ")", "\n", "return", "v_lo", ",", "v_hi", ",", "v_w", ",", "j_valid", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._grid_sampling_utilities": [[486, 549], ["bbox_xywh_gt[].unbind", "bbox_xywh_est[].unbind", "densepose_head._linear_interpolation_utilities", "densepose_head._linear_interpolation_utilities"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._linear_interpolation_utilities", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._linear_interpolation_utilities"], ["", "def", "_grid_sampling_utilities", "(", "\n", "zh", ",", "zw", ",", "bbox_xywh_est", ",", "bbox_xywh_gt", ",", "index_gt", ",", "x_norm", ",", "y_norm", ",", "index_bbox", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Prepare tensors used in grid sampling.\n\n    Args:\n        z_est (:obj: `torch.Tensor`): tensor of size (N,C,H,W) with estimated\n            values of Z to be extracted for the points X, Y and channel\n            indices I\n        bbox_xywh_est (:obj: `torch.Tensor`): tensor of size (N, 4) containing\n            estimated bounding boxes in format XYWH\n        bbox_xywh_gt (:obj: `torch.Tensor`): tensor of size (N, 4) containing\n            matched ground truth bounding boxes in format XYWH\n        index_gt (:obj: `torch.Tensor`): tensor of size K with point labels for\n            ground truth points\n        x_norm (:obj: `torch.Tensor`): tensor of size K with X normalized\n            coordinates of ground truth points. Image X coordinates can be\n            obtained as X = Xbbox + x_norm * Wbbox / 255\n        y_norm (:obj: `torch.Tensor`): tensor of size K with Y normalized\n            coordinates of ground truth points. Image Y coordinates can be\n            obtained as Y = Ybbox + y_norm * Hbbox / 255\n        index_bbox (:obj: `torch.Tensor`): tensor of size K with bounding box\n            indices for each ground truth point. The values are thus in\n            [0, N-1]\n\n    Returns:\n        j_valid (:obj: `torch.Tensor`): uint8 tensor of size M containing\n            0 for points to be discarded and 1 for points to be selected\n        y_lo (:obj: `torch.Tensor`): int tensor of indices of upper values\n            in z_est for each point\n        y_hi (:obj: `torch.Tensor`): int tensor of indices of lower values\n            in z_est for each point\n        x_lo (:obj: `torch.Tensor`): int tensor of indices of left values\n            in z_est for each point\n        x_hi (:obj: `torch.Tensor`): int tensor of indices of right values\n            in z_est for each point\n        w_ylo_xlo (:obj: `torch.Tensor`): float tensor of size M;\n            contains upper-left value weight for each point\n        w_ylo_xhi (:obj: `torch.Tensor`): float tensor of size M;\n            contains upper-right value weight for each point\n        w_yhi_xlo (:obj: `torch.Tensor`): float tensor of size M;\n            contains lower-left value weight for each point\n        w_yhi_xhi (:obj: `torch.Tensor`): float tensor of size M;\n            contains lower-right value weight for each point\n    \"\"\"", "\n", "\n", "x0_gt", ",", "y0_gt", ",", "w_gt", ",", "h_gt", "=", "bbox_xywh_gt", "[", "index_bbox", "]", ".", "unbind", "(", "dim", "=", "1", ")", "\n", "x0_est", ",", "y0_est", ",", "w_est", ",", "h_est", "=", "bbox_xywh_est", "[", "index_bbox", "]", ".", "unbind", "(", "dim", "=", "1", ")", "\n", "x_lo", ",", "x_hi", ",", "x_w", ",", "jx_valid", "=", "_linear_interpolation_utilities", "(", "\n", "x_norm", ",", "x0_gt", ",", "w_gt", ",", "x0_est", ",", "w_est", ",", "zw", "\n", ")", "\n", "y_lo", ",", "y_hi", ",", "y_w", ",", "jy_valid", "=", "_linear_interpolation_utilities", "(", "\n", "y_norm", ",", "y0_gt", ",", "h_gt", ",", "y0_est", ",", "h_est", ",", "zh", "\n", ")", "\n", "j_valid", "=", "jx_valid", "*", "jy_valid", "\n", "\n", "w_ylo_xlo", "=", "(", "1.0", "-", "x_w", ")", "*", "(", "1.0", "-", "y_w", ")", "\n", "w_ylo_xhi", "=", "x_w", "*", "(", "1.0", "-", "y_w", ")", "\n", "w_yhi_xlo", "=", "(", "1.0", "-", "x_w", ")", "*", "y_w", "\n", "w_yhi_xhi", "=", "x_w", "*", "y_w", "\n", "\n", "return", "j_valid", ",", "y_lo", ",", "y_hi", ",", "x_lo", ",", "x_hi", ",", "w_ylo_xlo", ",", "w_ylo_xhi", ",", "w_yhi_xlo", ",", "w_yhi_xhi", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_at_points_packed": [[551, 579], ["None"], "function", ["None"], ["", "def", "_extract_at_points_packed", "(", "\n", "z_est", ",", "\n", "index_bbox_valid", ",", "\n", "slice_index_uv", ",", "\n", "y_lo", ",", "\n", "y_hi", ",", "\n", "x_lo", ",", "\n", "x_hi", ",", "\n", "w_ylo_xlo", ",", "\n", "w_ylo_xhi", ",", "\n", "w_yhi_xlo", ",", "\n", "w_yhi_xhi", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Extract ground truth values z_gt for valid point indices and estimated\n    values z_est using bilinear interpolation over top-left (y_lo, x_lo),\n    top-right (y_lo, x_hi), bottom-left (y_hi, x_lo) and bottom-right\n    (y_hi, x_hi) values in z_est with corresponding weights:\n    w_ylo_xlo, w_ylo_xhi, w_yhi_xlo and w_yhi_xhi.\n    Use slice_index_uv to slice dim=1 in z_est\n    \"\"\"", "\n", "z_est_sampled", "=", "(", "\n", "z_est", "[", "index_bbox_valid", ",", "slice_index_uv", ",", "y_lo", ",", "x_lo", "]", "*", "w_ylo_xlo", "\n", "+", "z_est", "[", "index_bbox_valid", ",", "slice_index_uv", ",", "y_lo", ",", "x_hi", "]", "*", "w_ylo_xhi", "\n", "+", "z_est", "[", "index_bbox_valid", ",", "slice_index_uv", ",", "y_hi", ",", "x_lo", "]", "*", "w_yhi_xlo", "\n", "+", "z_est", "[", "index_bbox_valid", ",", "slice_index_uv", ",", "y_hi", ",", "x_hi", "]", "*", "w_yhi_xhi", "\n", ")", "\n", "return", "z_est_sampled", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._resample_data": [[581, 622], ["bbox_xywh_src.size", "bbox_xywh_src.unbind", "bbox_xywh_dst.unbind", "grid_w[].expand", "grid_h[].expand", "[].expand", "[].expand", "x0dst_norm[].expand", "y0dst_norm[].expand", "torch.stack", "torch.nn.functional.grid_sample", "bbox_xywh_dst.size", "bbox_xywh_src.size", "bbox_xywh_dst.size", "torch.arange", "torch.arange"], "function", ["None"], ["", "def", "_resample_data", "(", "\n", "z", ",", "bbox_xywh_src", ",", "bbox_xywh_dst", ",", "wout", ",", "hout", ",", "mode", "=", "\"nearest\"", ",", "padding_mode", "=", "\"zeros\"", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        z (:obj: `torch.Tensor`): tensor of size (N,C,H,W) with data to be\n            resampled\n        bbox_xywh_src (:obj: `torch.Tensor`): tensor of size (N,4) containing\n            source bounding boxes in format XYWH\n        bbox_xywh_dst (:obj: `torch.Tensor`): tensor of size (N,4) containing\n            destination bounding boxes in format XYWH\n    Return:\n        zresampled (:obj: `torch.Tensor`): tensor of size (N, C, Hout, Wout)\n            with resampled values of z, where D is the discretization size\n    \"\"\"", "\n", "n", "=", "bbox_xywh_src", ".", "size", "(", "0", ")", "\n", "assert", "n", "==", "bbox_xywh_dst", ".", "size", "(", "0", ")", ",", "(", "\n", "\"The number of \"", "\n", "\"source ROIs for resampling ({}) should be equal to the number \"", "\n", "\"of destination ROIs ({})\"", ".", "format", "(", "bbox_xywh_src", ".", "size", "(", "0", ")", ",", "bbox_xywh_dst", ".", "size", "(", "0", ")", ")", "\n", ")", "\n", "x0src", ",", "y0src", ",", "wsrc", ",", "hsrc", "=", "bbox_xywh_src", ".", "unbind", "(", "dim", "=", "1", ")", "\n", "x0dst", ",", "y0dst", ",", "wdst", ",", "hdst", "=", "bbox_xywh_dst", ".", "unbind", "(", "dim", "=", "1", ")", "\n", "x0dst_norm", "=", "2", "*", "(", "x0dst", "-", "x0src", ")", "/", "wsrc", "-", "1", "\n", "y0dst_norm", "=", "2", "*", "(", "y0dst", "-", "y0src", ")", "/", "hsrc", "-", "1", "\n", "x1dst_norm", "=", "2", "*", "(", "x0dst", "+", "wdst", "-", "x0src", ")", "/", "wsrc", "-", "1", "\n", "y1dst_norm", "=", "2", "*", "(", "y0dst", "+", "hdst", "-", "y0src", ")", "/", "hsrc", "-", "1", "\n", "grid_w", "=", "torch", ".", "arange", "(", "wout", ",", "device", "=", "z", ".", "device", ",", "dtype", "=", "torch", ".", "float", ")", "/", "wout", "\n", "grid_h", "=", "torch", ".", "arange", "(", "hout", ",", "device", "=", "z", ".", "device", ",", "dtype", "=", "torch", ".", "float", ")", "/", "hout", "\n", "grid_w_expanded", "=", "grid_w", "[", "None", ",", "None", ",", ":", "]", ".", "expand", "(", "n", ",", "hout", ",", "wout", ")", "\n", "grid_h_expanded", "=", "grid_h", "[", "None", ",", ":", ",", "None", "]", ".", "expand", "(", "n", ",", "hout", ",", "wout", ")", "\n", "dx_expanded", "=", "(", "x1dst_norm", "-", "x0dst_norm", ")", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "n", ",", "hout", ",", "wout", ")", "\n", "dy_expanded", "=", "(", "y1dst_norm", "-", "y0dst_norm", ")", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "n", ",", "hout", ",", "wout", ")", "\n", "x0_expanded", "=", "x0dst_norm", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "n", ",", "hout", ",", "wout", ")", "\n", "y0_expanded", "=", "y0dst_norm", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "n", ",", "hout", ",", "wout", ")", "\n", "grid_x", "=", "grid_w_expanded", "*", "dx_expanded", "+", "x0_expanded", "\n", "grid_y", "=", "grid_h_expanded", "*", "dy_expanded", "+", "y0_expanded", "\n", "grid", "=", "torch", ".", "stack", "(", "(", "grid_x", ",", "grid_y", ")", ",", "dim", "=", "3", ")", "\n", "# resample Z from (N, C, H, W) into (N, C, Hout, Wout)", "\n", "zresampled", "=", "F", ".", "grid_sample", "(", "z", ",", "grid", ",", "mode", "=", "mode", ",", "padding_mode", "=", "padding_mode", ",", "align_corners", "=", "True", ")", "\n", "return", "zresampled", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_single_tensors_from_matches_one_image": [[624, 681], ["proposals_targets.proposal_boxes.clone", "proposals_targets.gt_boxes.clone", "len", "len", "hasattr", "zip", "range", "i_gt_all.append", "x_norm_all.append", "y_norm_all.append", "u_gt_all.append", "v_gt_all.append", "s_gt_all.append", "bbox_xywh_gt_all.append", "bbox_xywh_est_all.append", "torch.full_like", "i_bbox_all.append", "i_with_dp.append", "len", "dp_gt.segm.unsqueeze", "box_xywh_gt.view", "box_xywh_est.view", "len"], "function", ["None"], ["", "def", "_extract_single_tensors_from_matches_one_image", "(", "\n", "proposals_targets", ",", "bbox_with_dp_offset", ",", "bbox_global_offset", "\n", ")", ":", "\n", "    ", "i_gt_all", "=", "[", "]", "\n", "x_norm_all", "=", "[", "]", "\n", "y_norm_all", "=", "[", "]", "\n", "u_gt_all", "=", "[", "]", "\n", "v_gt_all", "=", "[", "]", "\n", "s_gt_all", "=", "[", "]", "\n", "bbox_xywh_gt_all", "=", "[", "]", "\n", "bbox_xywh_est_all", "=", "[", "]", "\n", "# Ibbox_all == k should be true for all data that corresponds", "\n", "# to bbox_xywh_gt[k] and bbox_xywh_est[k]", "\n", "# index k here is global wrt images", "\n", "i_bbox_all", "=", "[", "]", "\n", "# at offset k (k is global) contains index of bounding box data", "\n", "# within densepose output tensor", "\n", "i_with_dp", "=", "[", "]", "\n", "\n", "boxes_xywh_est", "=", "proposals_targets", ".", "proposal_boxes", ".", "clone", "(", ")", "\n", "boxes_xywh_gt", "=", "proposals_targets", ".", "gt_boxes", ".", "clone", "(", ")", "\n", "n_i", "=", "len", "(", "boxes_xywh_est", ")", "\n", "assert", "n_i", "==", "len", "(", "boxes_xywh_gt", ")", "\n", "\n", "if", "n_i", ":", "\n", "        ", "boxes_xywh_est", ".", "tensor", "[", ":", ",", "2", "]", "-=", "boxes_xywh_est", ".", "tensor", "[", ":", ",", "0", "]", "\n", "boxes_xywh_est", ".", "tensor", "[", ":", ",", "3", "]", "-=", "boxes_xywh_est", ".", "tensor", "[", ":", ",", "1", "]", "\n", "boxes_xywh_gt", ".", "tensor", "[", ":", ",", "2", "]", "-=", "boxes_xywh_gt", ".", "tensor", "[", ":", ",", "0", "]", "\n", "boxes_xywh_gt", ".", "tensor", "[", ":", ",", "3", "]", "-=", "boxes_xywh_gt", ".", "tensor", "[", ":", ",", "1", "]", "\n", "if", "hasattr", "(", "proposals_targets", ",", "\"gt_densepose\"", ")", ":", "\n", "            ", "densepose_gt", "=", "proposals_targets", ".", "gt_densepose", "\n", "for", "k", ",", "box_xywh_est", ",", "box_xywh_gt", ",", "dp_gt", "in", "zip", "(", "\n", "range", "(", "n_i", ")", ",", "boxes_xywh_est", ".", "tensor", ",", "boxes_xywh_gt", ".", "tensor", ",", "densepose_gt", "\n", ")", ":", "\n", "                ", "if", "(", "dp_gt", "is", "not", "None", ")", "and", "(", "len", "(", "dp_gt", ".", "x", ")", ">", "0", ")", ":", "\n", "                    ", "i_gt_all", ".", "append", "(", "dp_gt", ".", "i", ")", "\n", "x_norm_all", ".", "append", "(", "dp_gt", ".", "x", ")", "\n", "y_norm_all", ".", "append", "(", "dp_gt", ".", "y", ")", "\n", "u_gt_all", ".", "append", "(", "dp_gt", ".", "u", ")", "\n", "v_gt_all", ".", "append", "(", "dp_gt", ".", "v", ")", "\n", "s_gt_all", ".", "append", "(", "dp_gt", ".", "segm", ".", "unsqueeze", "(", "0", ")", ")", "\n", "bbox_xywh_gt_all", ".", "append", "(", "box_xywh_gt", ".", "view", "(", "-", "1", ",", "4", ")", ")", "\n", "bbox_xywh_est_all", ".", "append", "(", "box_xywh_est", ".", "view", "(", "-", "1", ",", "4", ")", ")", "\n", "i_bbox_k", "=", "torch", ".", "full_like", "(", "dp_gt", ".", "i", ",", "bbox_with_dp_offset", "+", "len", "(", "i_with_dp", ")", ")", "\n", "i_bbox_all", ".", "append", "(", "i_bbox_k", ")", "\n", "i_with_dp", ".", "append", "(", "bbox_global_offset", "+", "k", ")", "\n", "", "", "", "", "return", "(", "\n", "i_gt_all", ",", "\n", "x_norm_all", ",", "\n", "y_norm_all", ",", "\n", "u_gt_all", ",", "\n", "v_gt_all", ",", "\n", "s_gt_all", ",", "\n", "bbox_xywh_gt_all", ",", "\n", "bbox_xywh_est_all", ",", "\n", "i_bbox_all", ",", "\n", "i_with_dp", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_single_tensors_from_matches": [[684, 749], ["enumerate", "proposals_targets_per_image.proposal_boxes.tensor.size", "densepose_head._extract_single_tensors_from_matches_one_image", "i_gt_all.extend", "x_norm_all.extend", "y_norm_all.extend", "u_gt_all.extend", "v_gt_all.extend", "s_gt_all.extend", "bbox_xywh_gt_all.extend", "bbox_xywh_est_all.extend", "i_bbox_all.extend", "i_with_dp_all.extend", "i_img.extend", "torch.cat().long", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().long", "len", "len", "len", "torch.cat", "torch.cat"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head._extract_single_tensors_from_matches_one_image"], ["", "def", "_extract_single_tensors_from_matches", "(", "proposals_with_targets", ")", ":", "\n", "    ", "i_img", "=", "[", "]", "\n", "i_gt_all", "=", "[", "]", "\n", "x_norm_all", "=", "[", "]", "\n", "y_norm_all", "=", "[", "]", "\n", "u_gt_all", "=", "[", "]", "\n", "v_gt_all", "=", "[", "]", "\n", "s_gt_all", "=", "[", "]", "\n", "bbox_xywh_gt_all", "=", "[", "]", "\n", "bbox_xywh_est_all", "=", "[", "]", "\n", "i_bbox_all", "=", "[", "]", "\n", "i_with_dp_all", "=", "[", "]", "\n", "n", "=", "0", "\n", "for", "i", ",", "proposals_targets_per_image", "in", "enumerate", "(", "proposals_with_targets", ")", ":", "\n", "        ", "n_i", "=", "proposals_targets_per_image", ".", "proposal_boxes", ".", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "n_i", ":", "\n", "            ", "continue", "\n", "", "i_gt_img", ",", "x_norm_img", ",", "y_norm_img", ",", "u_gt_img", ",", "v_gt_img", ",", "s_gt_img", ",", "bbox_xywh_gt_img", ",", "bbox_xywh_est_img", ",", "i_bbox_img", ",", "i_with_dp_img", "=", "_extract_single_tensors_from_matches_one_image", "(", "# noqa", "\n", "proposals_targets_per_image", ",", "len", "(", "i_with_dp_all", ")", ",", "n", "\n", ")", "\n", "i_gt_all", ".", "extend", "(", "i_gt_img", ")", "\n", "x_norm_all", ".", "extend", "(", "x_norm_img", ")", "\n", "y_norm_all", ".", "extend", "(", "y_norm_img", ")", "\n", "u_gt_all", ".", "extend", "(", "u_gt_img", ")", "\n", "v_gt_all", ".", "extend", "(", "v_gt_img", ")", "\n", "s_gt_all", ".", "extend", "(", "s_gt_img", ")", "\n", "bbox_xywh_gt_all", ".", "extend", "(", "bbox_xywh_gt_img", ")", "\n", "bbox_xywh_est_all", ".", "extend", "(", "bbox_xywh_est_img", ")", "\n", "i_bbox_all", ".", "extend", "(", "i_bbox_img", ")", "\n", "i_with_dp_all", ".", "extend", "(", "i_with_dp_img", ")", "\n", "i_img", ".", "extend", "(", "[", "i", "]", "*", "len", "(", "i_with_dp_img", ")", ")", "\n", "n", "+=", "n_i", "\n", "# concatenate all data into a single tensor", "\n", "", "if", "(", "n", ">", "0", ")", "and", "(", "len", "(", "i_with_dp_all", ")", ">", "0", ")", ":", "\n", "        ", "i_gt", "=", "torch", ".", "cat", "(", "i_gt_all", ",", "0", ")", ".", "long", "(", ")", "\n", "x_norm", "=", "torch", ".", "cat", "(", "x_norm_all", ",", "0", ")", "\n", "y_norm", "=", "torch", ".", "cat", "(", "y_norm_all", ",", "0", ")", "\n", "u_gt", "=", "torch", ".", "cat", "(", "u_gt_all", ",", "0", ")", "\n", "v_gt", "=", "torch", ".", "cat", "(", "v_gt_all", ",", "0", ")", "\n", "s_gt", "=", "torch", ".", "cat", "(", "s_gt_all", ",", "0", ")", "\n", "bbox_xywh_gt", "=", "torch", ".", "cat", "(", "bbox_xywh_gt_all", ",", "0", ")", "\n", "bbox_xywh_est", "=", "torch", ".", "cat", "(", "bbox_xywh_est_all", ",", "0", ")", "\n", "i_bbox", "=", "torch", ".", "cat", "(", "i_bbox_all", ",", "0", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "        ", "i_gt", "=", "None", "\n", "x_norm", "=", "None", "\n", "y_norm", "=", "None", "\n", "u_gt", "=", "None", "\n", "v_gt", "=", "None", "\n", "s_gt", "=", "None", "\n", "bbox_xywh_gt", "=", "None", "\n", "bbox_xywh_est", "=", "None", "\n", "i_bbox", "=", "None", "\n", "", "return", "(", "\n", "i_img", ",", "\n", "i_with_dp_all", ",", "\n", "bbox_xywh_est", ",", "\n", "bbox_xywh_gt", ",", "\n", "i_gt", ",", "\n", "x_norm", ",", "\n", "y_norm", ",", "\n", "u_gt", ",", "\n", "v_gt", ",", "\n", "s_gt", ",", "\n", "i_bbox", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.densepose_head.build_densepose_losses": [[876, 879], ["densepose_head.DensePoseLosses"], "function", ["None"], ["", "", "def", "build_densepose_losses", "(", "cfg", ")", ":", "\n", "    ", "losses", "=", "DensePoseLosses", "(", "cfg", ")", "\n", "return", "losses", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.__init__": [[13, 51], ["print", "train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.load_history"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.load_history"], ["def", "__init__", "(", "self", ",", "losses_to_track", ",", "metrics_to_track", ",", "img_wh", ",", "log_path", ",", "\n", "load_logs", "=", "False", ",", "current_epoch", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "all_per_task_loss_types", "=", "[", "'train_verts_losses'", ",", "'val_verts_losses'", ",", "\n", "'train_shape_params_losses'", ",", "'val_shape_params_losses'", ",", "\n", "'train_pose_params_losses'", ",", "'val_pose_params_losses'", ",", "\n", "'train_joints2D_losses'", ",", "'val_joints2D_losses'", ",", "\n", "'train_joints3D_losses'", ",", "'val_joints3D_losses'", "]", "\n", "\n", "self", ".", "all_metrics_types", "=", "[", "'train_pves'", ",", "'val_pves'", ",", "\n", "'train_pves_sc'", ",", "'val_pves_sc'", ",", "\n", "'train_pves_pa'", ",", "'val_pves_pa'", ",", "\n", "'train_pve-ts'", ",", "'val_pve-ts'", ",", "\n", "'train_pve-ts_sc'", ",", "'val_pve-ts_sc'", ",", "\n", "'train_pve-ts_pa'", ",", "'val_pve-ts_pa'", ",", "\n", "'train_mpjpes'", ",", "'val_mpjpes'", ",", "\n", "'train_mpjpes_sc'", ",", "'val_mpjpes_sc'", ",", "\n", "'train_mpjpes_pa'", ",", "'val_mpjpes_pa'", ",", "\n", "'train_pose_mses'", ",", "'val_pose_mses'", ",", "\n", "'train_shape_mses'", ",", "'val_shape_mses'", ",", "\n", "'train_joints2D_l2es'", ",", "'val_joints2D_l2es'", "]", "\n", "\n", "self", ".", "losses_to_track", "=", "losses_to_track", "\n", "self", ".", "metrics_to_track", "=", "metrics_to_track", "\n", "self", ".", "img_wh", "=", "img_wh", "\n", "self", ".", "log_path", "=", "log_path", "\n", "\n", "if", "load_logs", ":", "\n", "            ", "self", ".", "history", "=", "self", ".", "load_history", "(", "log_path", ",", "current_epoch", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "history", "=", "{", "'train_losses'", ":", "[", "]", ",", "'val_losses'", ":", "[", "]", "}", "\n", "for", "loss_type", "in", "self", ".", "all_per_task_loss_types", ":", "\n", "                ", "self", ".", "history", "[", "loss_type", "]", "=", "[", "]", "\n", "", "for", "metric_type", "in", "self", ".", "all_metrics_types", ":", "\n", "                ", "self", ".", "history", "[", "metric_type", "]", "=", "[", "]", "\n", "\n", "", "", "self", ".", "loss_metric_sums", "=", "None", "\n", "print", "(", "'Metrics tracker initialised.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.load_history": [[52, 91], ["pickle.load.keys", "print", "open", "pickle.load", "pickle.load.keys", "print", "pickle.load.keys", "print", "len", "str", "str", "len"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load"], ["", "def", "load_history", "(", "self", ",", "load_log_path", ",", "current_epoch", ")", ":", "\n", "        ", "\"\"\"\n        Loads loss (total and per-task) and metric history up to current epoch given the path\n        to a log file. If per-task losses or metrics are missing from log file, fill with 0s.\n        This is used if resuming a previous training run that was interrupted.\n        \"\"\"", "\n", "with", "open", "(", "load_log_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "history", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "history", "[", "'train_losses'", "]", "=", "history", "[", "'train_losses'", "]", "[", ":", "current_epoch", "]", "\n", "history", "[", "'val_losses'", "]", "=", "history", "[", "'val_losses'", "]", "[", ":", "current_epoch", "]", "\n", "\n", "# For each task, if per-task loss type in history, load up to current epoch.", "\n", "# Else, fill with 0s up to current epoch.", "\n", "for", "loss_type", "in", "self", ".", "all_per_task_loss_types", ":", "\n", "            ", "if", "loss_type", "in", "history", ".", "keys", "(", ")", ":", "\n", "                ", "history", "[", "loss_type", "]", "=", "history", "[", "loss_type", "]", "[", ":", "current_epoch", "]", "\n", "", "else", ":", "\n", "                ", "history", "[", "loss_type", "]", "=", "[", "0.0", "]", "*", "current_epoch", "\n", "print", "(", "loss_type", ",", "'filled with zeros up to epoch'", ",", "\n", "current_epoch", ")", "\n", "\n", "", "", "for", "metric_type", "in", "self", ".", "all_metrics_types", ":", "\n", "            ", "if", "metric_type", "in", "history", ".", "keys", "(", ")", ":", "\n", "                ", "history", "[", "metric_type", "]", "=", "history", "[", "metric_type", "]", "[", ":", "current_epoch", "]", "\n", "", "else", ":", "\n", "                ", "history", "[", "metric_type", "]", "=", "[", "0.0", "]", "*", "current_epoch", "\n", "print", "(", "metric_type", ",", "'filled with zeros up to epoch'", ",", "\n", "current_epoch", ")", "\n", "\n", "", "", "for", "key", "in", "history", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "len", "(", "history", "[", "key", "]", ")", "==", "current_epoch", ",", "\"{} elements in {} list when current epoch is {}\"", ".", "format", "(", "\n", "str", "(", "len", "(", "history", "[", "key", "]", ")", ")", ",", "\n", "key", ",", "\n", "str", "(", "current_epoch", ")", ")", "\n", "", "print", "(", "'Logs loaded from'", ",", "load_log_path", ")", "\n", "\n", "return", "history", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.initialise_loss_metric_sums": [[92, 101], ["None"], "methods", ["None"], ["", "def", "initialise_loss_metric_sums", "(", "self", ")", ":", "\n", "        ", "self", ".", "loss_metric_sums", "=", "{", "'train_losses'", ":", "0.", ",", "'val_losses'", ":", "0.", ",", "\n", "'train_num_samples'", ":", "0", ",", "'val_num_samples'", ":", "0", "}", "\n", "\n", "for", "loss_type", "in", "self", ".", "all_per_task_loss_types", ":", "\n", "            ", "self", ".", "loss_metric_sums", "[", "loss_type", "]", "=", "0.", "\n", "\n", "", "for", "metric_type", "in", "self", ".", "all_metrics_types", ":", "\n", "            ", "self", ".", "loss_metric_sums", "[", "metric_type", "]", "=", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.update_per_batch": [[102, 214], ["any", "pred_dict.keys", "pred_reposed_vertices.cpu().detach().numpy.cpu().detach().numpy.cpu().detach().numpy", "target_reposed_vertices.cpu().detach().numpy.cpu().detach().numpy.cpu().detach().numpy", "pred_dict[].cpu().detach().numpy", "target_dict[].cpu().detach().numpy", "loss.item", "numpy.linalg.norm", "numpy.sum", "utils.eval_utils.scale_and_translation_transform_batch", "numpy.linalg.norm", "numpy.sum", "utils.eval_utils.procrustes_analysis_batch", "numpy.linalg.norm", "numpy.sum", "numpy.linalg.norm", "numpy.sum", "utils.eval_utils.scale_and_translation_transform_batch", "numpy.linalg.norm", "numpy.sum", "utils.eval_utils.procrustes_analysis_batch", "numpy.linalg.norm", "numpy.sum", "numpy.linalg.norm", "numpy.sum", "utils.eval_utils.scale_and_translation_transform_batch", "numpy.linalg.norm", "numpy.sum", "utils.eval_utils.procrustes_analysis_batch", "numpy.linalg.norm", "numpy.sum", "numpy.sum", "numpy.sum", "utils.joints2d_utils.undo_keypoint_normalisation", "numpy.linalg.norm", "numpy.sum", "task_losses_dict[].item", "pred_reposed_vertices.cpu().detach().numpy.cpu().detach().numpy.cpu().detach", "target_reposed_vertices.cpu().detach().numpy.cpu().detach().numpy.cpu().detach", "pred_dict[].cpu().detach", "target_dict[].cpu().detach", "pred_reposed_vertices.cpu().detach().numpy.cpu().detach().numpy.cpu", "target_reposed_vertices.cpu().detach().numpy.cpu().detach().numpy.cpu", "pred_dict[].cpu", "target_dict[].cpu"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.joints2d_utils.undo_keypoint_normalisation"], ["", "", "def", "update_per_batch", "(", "self", ",", "split", ",", "loss", ",", "task_losses_dict", ",", "pred_dict", ",", "target_dict", ",", "\n", "num_inputs_in_batch", ",", "\n", "pred_reposed_vertices", "=", "None", ",", "target_reposed_vertices", "=", "None", ")", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'val'", "]", ",", "\"Invalid split in metric tracker batch update.\"", "\n", "\n", "if", "any", "(", "[", "'pve-ts'", "in", "metric_type", "for", "metric_type", "in", "self", ".", "metrics_to_track", "]", ")", ":", "\n", "            ", "assert", "(", "pred_reposed_vertices", "is", "not", "None", ")", "and", "(", "target_reposed_vertices", "is", "not", "None", ")", ",", "\"Need to pass reposed vertices to metric tracker batch update.\"", "\n", "pred_reposed_vertices", "=", "pred_reposed_vertices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "target_reposed_vertices", "=", "target_reposed_vertices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "for", "key", "in", "pred_dict", ".", "keys", "(", ")", ":", "\n", "            ", "pred_dict", "[", "key", "]", "=", "pred_dict", "[", "key", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "target_dict", "[", "key", "]", "=", "target_dict", "[", "key", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# -------- Update loss sums --------", "\n", "", "self", ".", "loss_metric_sums", "[", "split", "+", "'_losses'", "]", "+=", "loss", ".", "item", "(", ")", "*", "num_inputs_in_batch", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_num_samples'", "]", "+=", "num_inputs_in_batch", "\n", "\n", "for", "loss_on", "in", "self", ".", "losses_to_track", ":", "\n", "            ", "self", ".", "loss_metric_sums", "[", "split", "+", "'_'", "+", "loss_on", "+", "'_losses'", "]", "+=", "task_losses_dict", "[", "loss_on", "]", ".", "item", "(", ")", "*", "num_inputs_in_batch", "\n", "\n", "# -------- Update metrics sums --------", "\n", "", "if", "'pves'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pve_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_dict", "[", "'verts'", "]", "-", "target_dict", "[", "'verts'", "]", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 6890)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pves'", "]", "+=", "np", ".", "sum", "(", "pve_batch", ")", "# scalar", "\n", "\n", "# Scale and translation correction", "\n", "", "if", "'pves_sc'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_vertices", "=", "pred_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "target_vertices", "=", "target_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "pred_vertices_sc", "=", "scale_and_translation_transform_batch", "(", "pred_vertices", ",", "\n", "target_vertices", ")", "\n", "pve_sc_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_vertices_sc", "-", "target_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bs, 6890)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pves_sc'", "]", "+=", "np", ".", "sum", "(", "pve_sc_batch", ")", "# scalar", "\n", "\n", "# Procrustes analysis", "\n", "", "if", "'pves_pa'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_vertices", "=", "pred_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "target_vertices", "=", "target_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "pred_vertices_pa", "=", "procrustes_analysis_batch", "(", "pred_vertices", ",", "target_vertices", ")", "\n", "pve_pa_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_vertices_pa", "-", "target_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 6890)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pves_pa'", "]", "+=", "np", ".", "sum", "(", "pve_pa_batch", ")", "# scalar", "\n", "\n", "# Reposed", "\n", "", "if", "'pve-ts'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pvet_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_reposed_vertices", "-", "target_reposed_vertices", ",", "\n", "axis", "=", "-", "1", ")", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pve-ts'", "]", "+=", "np", ".", "sum", "(", "pvet_batch", ")", "\n", "\n", "# Reposed + Scale and translation correction", "\n", "", "if", "'pve-ts_sc'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_reposed_vertices_sc", "=", "scale_and_translation_transform_batch", "(", "pred_reposed_vertices", ",", "\n", "target_reposed_vertices", ")", "\n", "pvet_sc_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_reposed_vertices_sc", "-", "target_reposed_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bs, 6890)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pve-ts_sc'", "]", "+=", "np", ".", "sum", "(", "pvet_sc_batch", ")", "# scalar", "\n", "\n", "# Reposed + Procrustes analysis - this doesn't make practical sense for reposed.", "\n", "", "if", "'pve-ts_pa'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_reposed_vertices_pa", "=", "procrustes_analysis_batch", "(", "pred_reposed_vertices", ",", "\n", "target_reposed_vertices", ")", "\n", "pvet_pa_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_reposed_vertices_pa", "-", "target_reposed_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 6890)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pve-ts_pa'", "]", "+=", "np", ".", "sum", "(", "pvet_pa_batch", ")", "# scalar", "\n", "\n", "", "if", "'mpjpes'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "mpjpe_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_dict", "[", "'joints3D'", "]", "-", "target_dict", "[", "'joints3D'", "]", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 14)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_mpjpes'", "]", "+=", "np", ".", "sum", "(", "mpjpe_batch", ")", "# scalar", "\n", "\n", "# Scale and translation correction", "\n", "", "if", "'mpjpes_sc'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_joints3D_h36mlsp", "=", "pred_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "target_joints3D_h36mlsp", "=", "target_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "pred_joints3D_h36mlsp_sc", "=", "scale_and_translation_transform_batch", "(", "pred_joints3D_h36mlsp", ",", "\n", "target_joints3D_h36mlsp", ")", "\n", "mpjpe_sc_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_joints3D_h36mlsp_sc", "-", "target_joints3D_h36mlsp", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 14)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_mpjpes_sc'", "]", "+=", "np", ".", "sum", "(", "mpjpe_sc_batch", ")", "# scalar", "\n", "\n", "# Procrustes analysis", "\n", "", "if", "'mpjpes_pa'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_joints3D_h36mlsp", "=", "pred_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "target_joints3D_h36mlsp", "=", "target_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "pred_joints3D_h36mlsp_pa", "=", "procrustes_analysis_batch", "(", "pred_joints3D_h36mlsp", ",", "\n", "target_joints3D_h36mlsp", ")", "\n", "mpjpe_pa_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_joints3D_h36mlsp_pa", "-", "target_joints3D_h36mlsp", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 14)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_mpjpes_pa'", "]", "+=", "np", ".", "sum", "(", "mpjpe_pa_batch", ")", "# scalar", "\n", "\n", "", "if", "'pose_mses'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "self", ".", "loss_metric_sums", "[", "split", "+", "'_pose_mses'", "]", "+=", "np", ".", "sum", "(", "(", "pred_dict", "[", "'pose_params_rot_matrices'", "]", "-", "\n", "target_dict", "[", "'pose_params_rot_matrices'", "]", ")", "**", "2", ")", "\n", "\n", "", "if", "'shape_mses'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "self", ".", "loss_metric_sums", "[", "split", "+", "'_shape_mses'", "]", "+=", "np", ".", "sum", "(", "(", "pred_dict", "[", "'shape_params'", "]", "-", "\n", "target_dict", "[", "'shape_params'", "]", ")", "**", "2", ")", "\n", "\n", "", "if", "'joints2D_l2es'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_joints2D_coco", "=", "pred_dict", "[", "'joints2D'", "]", "\n", "target_joints2D_coco", "=", "target_dict", "[", "'joints2D'", "]", "\n", "pred_joints2D_coco", "=", "undo_keypoint_normalisation", "(", "pred_joints2D_coco", ",", "\n", "self", ".", "img_wh", ")", "\n", "joints2D_l2e_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_joints2D_coco", "-", "target_joints2D_coco", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, num_joints)", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_joints2D_l2es'", "]", "+=", "np", ".", "sum", "(", "joints2D_l2e_batch", ")", "# scalar", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.update_per_epoch": [[215, 266], ["train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.history[].append", "train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.history[].append", "print", "print", "print", "open", "pickle.dump", "loss_type.split", "train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.history[].append", "train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.history[].append", "metric_type.split", "train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.history[].append", "loss_type.find", "loss_type.find", "metric_type.find"], "methods", ["None"], ["", "", "def", "update_per_epoch", "(", "self", ")", ":", "\n", "        ", "self", ".", "history", "[", "'train_losses'", "]", ".", "append", "(", "self", ".", "loss_metric_sums", "[", "'train_losses'", "]", "/", "\n", "self", ".", "loss_metric_sums", "[", "'train_num_samples'", "]", ")", "\n", "self", ".", "history", "[", "'val_losses'", "]", ".", "append", "(", "self", ".", "loss_metric_sums", "[", "'val_losses'", "]", "/", "\n", "self", ".", "loss_metric_sums", "[", "'val_num_samples'", "]", ")", "\n", "\n", "# For each task, if tracking per-task loss, append loss per sample for current epoch", "\n", "# to loss history. Else, append 0.", "\n", "for", "loss_type", "in", "self", ".", "all_per_task_loss_types", ":", "\n", "            ", "split", "=", "loss_type", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "loss_on", "=", "loss_type", "[", "loss_type", ".", "find", "(", "'_'", ")", "+", "1", ":", "loss_type", ".", "find", "(", "'_losses'", ")", "]", "\n", "if", "loss_on", "in", "self", ".", "losses_to_track", ":", "\n", "                ", "self", ".", "history", "[", "loss_type", "]", ".", "append", "(", "self", ".", "loss_metric_sums", "[", "loss_type", "]", "/", "\n", "self", ".", "loss_metric_sums", "[", "split", "+", "'_num_samples'", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "history", "[", "loss_type", "]", ".", "append", "(", "0.", ")", "\n", "\n", "# For each metric, if tracking metric, append metric per sample for current epoch to", "\n", "# loss history. Else, append 0.", "\n", "", "", "for", "metric_type", "in", "self", ".", "all_metrics_types", ":", "\n", "            ", "split", "=", "metric_type", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "\n", "if", "metric_type", "[", "metric_type", ".", "find", "(", "'_'", ")", "+", "1", ":", "]", "in", "self", ".", "metrics_to_track", ":", "\n", "                ", "if", "'pve'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "6890", "\n", "", "elif", "'mpjpe'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "14", "\n", "", "elif", "'joints2D'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "17", "\n", "", "elif", "'shape_mse'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "10", "\n", "", "elif", "'pose_mse'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "24", "*", "3", "*", "3", "\n", "\n", "", "self", ".", "history", "[", "metric_type", "]", ".", "append", "(", "\n", "self", ".", "loss_metric_sums", "[", "metric_type", "]", "/", "\n", "(", "self", ".", "loss_metric_sums", "[", "split", "+", "'_num_samples'", "]", "*", "num_per_sample", ")", ")", "\n", "\n", "# Print end of epoch losses and metrics.", "\n", "", "", "print", "(", "'Finished epoch.'", ")", "\n", "print", "(", "'Train Loss: {:.5f}, Val Loss: {:.5f}'", ".", "format", "(", "self", ".", "history", "[", "'train_losses'", "]", "[", "-", "1", "]", ",", "\n", "self", ".", "history", "[", "'val_losses'", "]", "[", "-", "1", "]", ")", ")", "\n", "for", "metric", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "print", "(", "'Train {}: {:.5f}, Val {}: {:.5f}'", ".", "format", "(", "metric", ",", "\n", "self", ".", "history", "[", "'train_'", "+", "metric", "]", "[", "-", "1", "]", ",", "\n", "metric", ",", "\n", "self", ".", "history", "[", "'val_'", "+", "metric", "]", "[", "-", "1", "]", ")", ")", "\n", "\n", "# Dump history to log file", "\n", "", "with", "open", "(", "self", ".", "log_path", ",", "'wb'", ")", "as", "f_out", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "history", ",", "f_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.train_loss_and_metrics_tracker.TrainingLossesAndMetricsTracker.determine_save_model_weights_this_epoch": [[267, 275], ["None"], "methods", ["None"], ["", "", "def", "determine_save_model_weights_this_epoch", "(", "self", ",", "save_val_metrics", ",", "best_epoch_val_metrics", ")", ":", "\n", "        ", "save_model_weights_this_epoch", "=", "True", "\n", "for", "metric", "in", "save_val_metrics", ":", "\n", "            ", "if", "self", ".", "history", "[", "'val_'", "+", "metric", "]", "[", "-", "1", "]", ">", "best_epoch_val_metrics", "[", "metric", "]", ":", "\n", "                ", "save_model_weights_this_epoch", "=", "False", "\n", "break", "\n", "\n", "", "", "return", "save_model_weights_this_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.__init__": [[12, 23], ["print"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "metrics_to_track", ",", "img_wh", "=", "None", ",", "save_path", "=", "None", ",", "\n", "save_per_frame_metrics", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "metrics_to_track", "=", "metrics_to_track", "\n", "self", ".", "img_wh", "=", "img_wh", "\n", "\n", "self", ".", "metric_sums", "=", "None", "\n", "self", ".", "total_samples", "=", "0", "\n", "self", ".", "save_per_frame_metrics", "=", "save_per_frame_metrics", "\n", "self", ".", "save_path", "=", "save_path", "\n", "print", "(", "'\\nInitialised metrics tracker.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.initialise_metric_sums": [[24, 34], ["None"], "methods", ["None"], ["", "def", "initialise_metric_sums", "(", "self", ")", ":", "\n", "        ", "self", ".", "metric_sums", "=", "{", "}", "\n", "for", "metric_type", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "if", "metric_type", "==", "'silhouette_ious'", ":", "\n", "                ", "self", ".", "metric_sums", "[", "'num_true_positives'", "]", "=", "0.", "\n", "self", ".", "metric_sums", "[", "'num_false_positives'", "]", "=", "0.", "\n", "self", ".", "metric_sums", "[", "'num_true_negatives'", "]", "=", "0.", "\n", "self", ".", "metric_sums", "[", "'num_false_negatives'", "]", "=", "0.", "\n", "", "else", ":", "\n", "                ", "self", ".", "metric_sums", "[", "metric_type", "]", "=", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.initialise_per_frame_metric_lists": [[35, 39], ["None"], "methods", ["None"], ["", "", "", "def", "initialise_per_frame_metric_lists", "(", "self", ")", ":", "\n", "        ", "self", ".", "per_frame_metrics", "=", "{", "}", "\n", "for", "metric_type", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "self", ".", "per_frame_metrics", "[", "metric_type", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.update_per_batch": [[40, 182], ["numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "utils.eval_utils.scale_and_translation_transform_batch", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "utils.eval_utils.procrustes_analysis_batch", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "utils.eval_utils.scale_and_translation_transform_batch", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "utils.eval_utils.procrustes_analysis_batch", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "utils.eval_utils.scale_and_translation_transform_batch", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "utils.eval_utils.procrustes_analysis_batch", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "numpy.sum", "numpy.sum", "numpy.linalg.norm", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "numpy.logical_and", "numpy.logical_and", "numpy.logical_and", "numpy.logical_and", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "eval_metrics_tracker.EvalMetricsTracker.per_frame_metrics[].append", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.logical_not", "numpy.logical_not", "numpy.logical_not", "numpy.logical_not"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.scale_and_translation_transform_batch", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.utils.eval_utils.procrustes_analysis_batch"], ["", "", "def", "update_per_batch", "(", "self", ",", "pred_dict", ",", "target_dict", ",", "num_input_samples", ",", "\n", "return_transformed_points", "=", "False", ")", ":", "\n", "        ", "self", ".", "total_samples", "+=", "num_input_samples", "\n", "if", "return_transformed_points", ":", "\n", "            ", "return_dict", "=", "{", "}", "\n", "# -------- Update metrics sums --------", "\n", "", "if", "'pves'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pve_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_dict", "[", "'verts'", "]", "-", "target_dict", "[", "'verts'", "]", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 6890)", "\n", "self", ".", "metric_sums", "[", "'pves'", "]", "+=", "np", ".", "sum", "(", "pve_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'pves'", "]", ".", "append", "(", "np", ".", "mean", "(", "pve_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "\n", "# Scale and translation correction", "\n", "", "if", "'pves_sc'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_vertices", "=", "pred_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "target_vertices", "=", "target_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "pred_vertices_sc", "=", "scale_and_translation_transform_batch", "(", "pred_vertices", ",", "\n", "target_vertices", ")", "\n", "pve_sc_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_vertices_sc", "-", "target_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bs, 6890)", "\n", "self", ".", "metric_sums", "[", "'pves_sc'", "]", "+=", "np", ".", "sum", "(", "pve_sc_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'pves_sc'", "]", ".", "append", "(", "np", ".", "mean", "(", "pve_sc_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "if", "return_transformed_points", ":", "\n", "                ", "return_dict", "[", "'pred_vertices_sc'", "]", "=", "pred_vertices_sc", "\n", "\n", "# Procrustes analysis", "\n", "", "", "if", "'pves_pa'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_vertices", "=", "pred_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "target_vertices", "=", "target_dict", "[", "'verts'", "]", "# (bsize, 6890, 3)", "\n", "pred_vertices_pa", "=", "procrustes_analysis_batch", "(", "pred_vertices", ",", "target_vertices", ")", "\n", "pve_pa_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_vertices_pa", "-", "target_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 6890)", "\n", "self", ".", "metric_sums", "[", "'pves_pa'", "]", "+=", "np", ".", "sum", "(", "pve_pa_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'pves_pa'", "]", ".", "append", "(", "np", ".", "mean", "(", "pve_pa_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "if", "return_transformed_points", ":", "\n", "                ", "return_dict", "[", "'pred_vertices_pa'", "]", "=", "pred_vertices_pa", "\n", "\n", "# Reposed", "\n", "", "", "if", "'pve-ts'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pvet_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_dict", "[", "'reposed_verts'", "]", "-", "target_dict", "[", "'reposed_verts'", "]", ",", "\n", "axis", "=", "-", "1", ")", "\n", "self", ".", "metric_sums", "[", "'pve-ts'", "]", "+=", "np", ".", "sum", "(", "pvet_batch", ")", "\n", "self", ".", "per_frame_metrics", "[", "'pve-ts'", "]", ".", "append", "(", "np", ".", "mean", "(", "pvet_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "\n", "# Reposed + Scale and translation correction", "\n", "", "if", "'pve-ts_sc'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_reposed_vertices", "=", "pred_dict", "[", "'reposed_verts'", "]", "\n", "target_reposed_vertices", "=", "target_dict", "[", "'reposed_verts'", "]", "\n", "pred_reposed_vertices_sc", "=", "scale_and_translation_transform_batch", "(", "pred_reposed_vertices", ",", "\n", "target_reposed_vertices", ")", "\n", "pvet_sc_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_reposed_vertices_sc", "-", "target_reposed_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bs, 6890)", "\n", "self", ".", "metric_sums", "[", "'pve-ts_sc'", "]", "+=", "np", ".", "sum", "(", "pvet_sc_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'pve-ts_sc'", "]", ".", "append", "(", "np", ".", "mean", "(", "pvet_sc_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "if", "return_transformed_points", ":", "\n", "                ", "return_dict", "[", "'pred_reposed_vertices_sc'", "]", "=", "pred_reposed_vertices_sc", "\n", "\n", "# Reposed + Procrustes analysis - this doesn't make practical sense for reposed.", "\n", "", "", "if", "'pve-ts_pa'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_reposed_vertices", "=", "pred_dict", "[", "'reposed_verts'", "]", "\n", "target_reposed_vertices", "=", "target_dict", "[", "'reposed_verts'", "]", "\n", "pred_reposed_vertices_pa", "=", "procrustes_analysis_batch", "(", "pred_reposed_vertices", ",", "\n", "target_reposed_vertices", ")", "\n", "pvet_pa_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_reposed_vertices_pa", "-", "target_reposed_vertices", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 6890)", "\n", "self", ".", "metric_sums", "[", "'pve-ts_pa'", "]", "+=", "np", ".", "sum", "(", "pvet_pa_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'pve_ts_pa'", "]", ".", "append", "(", "np", ".", "mean", "(", "pvet_pa_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "if", "return_transformed_points", ":", "\n", "                ", "return_dict", "[", "'pred_reposed_vertices_pa'", "]", "=", "pred_reposed_vertices_pa", "\n", "\n", "", "", "if", "'mpjpes'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "mpjpe_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_dict", "[", "'joints3D'", "]", "-", "target_dict", "[", "'joints3D'", "]", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 14)", "\n", "self", ".", "metric_sums", "[", "'mpjpes'", "]", "+=", "np", ".", "sum", "(", "mpjpe_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'mpjpes'", "]", ".", "append", "(", "np", ".", "mean", "(", "mpjpe_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "\n", "# Scale and translation correction", "\n", "", "if", "'mpjpes_sc'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_joints3D_h36mlsp", "=", "pred_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "target_joints3D_h36mlsp", "=", "target_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "pred_joints3D_h36mlsp_sc", "=", "scale_and_translation_transform_batch", "(", "pred_joints3D_h36mlsp", ",", "\n", "target_joints3D_h36mlsp", ")", "\n", "mpjpe_sc_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_joints3D_h36mlsp_sc", "-", "target_joints3D_h36mlsp", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 14)", "\n", "self", ".", "metric_sums", "[", "'mpjpes_sc'", "]", "+=", "np", ".", "sum", "(", "mpjpe_sc_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'mpjpes_sc'", "]", ".", "append", "(", "np", ".", "mean", "(", "mpjpe_sc_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "if", "return_transformed_points", ":", "\n", "                ", "return_dict", "[", "'pred_joints3D_h36mlsp_sc'", "]", "=", "pred_joints3D_h36mlsp_sc", "\n", "\n", "# Procrustes analysis", "\n", "", "", "if", "'mpjpes_pa'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_joints3D_h36mlsp", "=", "pred_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "target_joints3D_h36mlsp", "=", "target_dict", "[", "'joints3D'", "]", "# (bsize, 14, 3)", "\n", "pred_joints3D_h36mlsp_pa", "=", "procrustes_analysis_batch", "(", "pred_joints3D_h36mlsp", ",", "\n", "target_joints3D_h36mlsp", ")", "\n", "mpjpe_pa_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_joints3D_h36mlsp_pa", "-", "target_joints3D_h36mlsp", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, 14)", "\n", "self", ".", "metric_sums", "[", "'mpjpes_pa'", "]", "+=", "np", ".", "sum", "(", "mpjpe_pa_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'mpjpes_pa'", "]", ".", "append", "(", "np", ".", "mean", "(", "mpjpe_pa_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "if", "return_transformed_points", ":", "\n", "                ", "return_dict", "[", "'pred_joints3D_h36mlsp_pa'", "]", "=", "pred_joints3D_h36mlsp_pa", "\n", "\n", "", "", "if", "'pose_mses'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "self", ".", "metric_sums", "[", "'pose_mses'", "]", "+=", "np", ".", "sum", "(", "(", "pred_dict", "[", "'pose_params_rot_matrices'", "]", "-", "\n", "target_dict", "[", "'pose_params_rot_matrices'", "]", ")", "**", "2", ")", "\n", "\n", "", "if", "'shape_mses'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "self", ".", "metric_sums", "[", "'shape_mses'", "]", "+=", "np", ".", "sum", "(", "(", "pred_dict", "[", "'shape_params'", "]", "-", "\n", "target_dict", "[", "'shape_params'", "]", ")", "**", "2", ")", "\n", "\n", "", "if", "'joints2D_l2es'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_joints2D_coco", "=", "pred_dict", "[", "'joints2D'", "]", "\n", "target_joints2D_coco", "=", "target_dict", "[", "'joints2D'", "]", "\n", "joints2D_l2e_batch", "=", "np", ".", "linalg", ".", "norm", "(", "pred_joints2D_coco", "-", "target_joints2D_coco", ",", "\n", "axis", "=", "-", "1", ")", "# (bsize, num_joints)", "\n", "self", ".", "metric_sums", "[", "'joints2D_l2es'", "]", "+=", "np", ".", "sum", "(", "joints2D_l2e_batch", ")", "# scalar", "\n", "self", ".", "per_frame_metrics", "[", "'joints2D_l2es'", "]", ".", "append", "(", "np", ".", "mean", "(", "joints2D_l2e_batch", ",", "axis", "=", "-", "1", ")", ")", "# (bs,)", "\n", "\n", "", "if", "'silhouette_ious'", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "pred_silhouettes", "=", "pred_dict", "[", "'silhouettes'", "]", "\n", "target_silhouettes", "=", "target_dict", "[", "'silhouettes'", "]", "\n", "true_positive", "=", "np", ".", "logical_and", "(", "pred_silhouettes", ",", "\n", "target_silhouettes", ")", "\n", "false_positive", "=", "np", ".", "logical_and", "(", "pred_silhouettes", ",", "\n", "np", ".", "logical_not", "(", "target_silhouettes", ")", ")", "\n", "true_negative", "=", "np", ".", "logical_and", "(", "np", ".", "logical_not", "(", "pred_silhouettes", ")", ",", "\n", "np", ".", "logical_not", "(", "target_silhouettes", ")", ")", "\n", "false_negative", "=", "np", ".", "logical_and", "(", "np", ".", "logical_not", "(", "pred_silhouettes", ")", ",", "\n", "target_silhouettes", ")", "\n", "num_tp", "=", "np", ".", "sum", "(", "true_positive", ",", "axis", "=", "(", "1", ",", "2", ")", ")", "# (bs,)", "\n", "num_fp", "=", "np", ".", "sum", "(", "false_positive", ",", "axis", "=", "(", "1", ",", "2", ")", ")", "\n", "num_tn", "=", "np", ".", "sum", "(", "true_negative", ",", "axis", "=", "(", "1", ",", "2", ")", ")", "\n", "num_fn", "=", "np", ".", "sum", "(", "false_negative", ",", "axis", "=", "(", "1", ",", "2", ")", ")", "\n", "self", ".", "metric_sums", "[", "'num_true_positives'", "]", "+=", "np", ".", "sum", "(", "num_tp", ")", "# scalar", "\n", "self", ".", "metric_sums", "[", "'num_false_positives'", "]", "+=", "np", ".", "sum", "(", "num_fp", ")", "\n", "self", ".", "metric_sums", "[", "'num_true_negatives'", "]", "+=", "np", ".", "sum", "(", "num_tn", ")", "\n", "self", ".", "metric_sums", "[", "'num_false_negatives'", "]", "+=", "np", ".", "sum", "(", "num_fn", ")", "\n", "iou_per_frame", "=", "num_tp", "/", "(", "num_tp", "+", "num_fp", "+", "num_fn", ")", "\n", "self", ".", "per_frame_metrics", "[", "'silhouette_ious'", "]", ".", "append", "(", "iou_per_frame", ")", "# (bs,)", "\n", "\n", "", "if", "return_transformed_points", ":", "\n", "            ", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.metrics.eval_metrics_tracker.EvalMetricsTracker.compute_final_metrics": [[183, 211], ["print", "numpy.concatenate", "numpy.save", "os.path.join"], "methods", ["None"], ["", "", "def", "compute_final_metrics", "(", "self", ")", ":", "\n", "        ", "final_metrics", "=", "{", "}", "\n", "for", "metric_type", "in", "self", ".", "metrics_to_track", ":", "\n", "            ", "if", "metric_type", "==", "'silhouette_ious'", ":", "\n", "                ", "iou", "=", "self", ".", "metric_sums", "[", "'num_true_positives'", "]", "/", "(", "self", ".", "metric_sums", "[", "'num_true_positives'", "]", "+", "\n", "self", ".", "metric_sums", "[", "'num_false_negatives'", "]", "+", "\n", "self", ".", "metric_sums", "[", "'num_false_positives'", "]", ")", "\n", "final_metrics", "[", "'silhouette_ious'", "]", "=", "iou", "\n", "", "else", ":", "\n", "                ", "if", "'pve'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "6890", "\n", "", "elif", "'mpjpe'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "14", "\n", "", "elif", "'joints2D'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "17", "\n", "", "elif", "'shape_mse'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "10", "\n", "", "elif", "'pose_mse'", "in", "metric_type", ":", "\n", "                    ", "num_per_sample", "=", "24", "*", "3", "*", "3", "\n", "\n", "", "final_metrics", "[", "metric_type", "]", "=", "self", ".", "metric_sums", "[", "metric_type", "]", "/", "(", "self", ".", "total_samples", "*", "num_per_sample", ")", "\n", "", "", "print", "(", "final_metrics", ")", "\n", "\n", "if", "self", ".", "save_per_frame_metrics", ":", "\n", "            ", "for", "metric_type", "in", "self", ".", "metrics_to_track", ":", "\n", "                ", "per_frame", "=", "np", ".", "concatenate", "(", "self", ".", "per_frame_metrics", "[", "metric_type", "]", ",", "axis", "=", "0", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "save_path", ",", "metric_type", "+", "'_per_frame.npy'", ")", ",", "per_frame", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.data.synthetic_training_dataset.SyntheticTrainingDataset.__init__": [[15, 39], ["numpy.load", "enumerate", "enumerate", "x.startswith", "x.startswith", "x.startswith", "x.startswith"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseTransformData.load"], ["def", "__init__", "(", "self", ",", "\n", "npz_path", ",", "\n", "params_from", "=", "'all'", ")", ":", "\n", "\n", "        ", "assert", "params_from", "in", "[", "'all'", ",", "'h36m'", ",", "'up3d'", ",", "'3dpw'", ",", "'not_amass'", "]", "\n", "\n", "data", "=", "np", ".", "load", "(", "npz_path", ")", "\n", "self", ".", "fnames", "=", "data", "[", "'fnames'", "]", "\n", "self", ".", "poses", "=", "data", "[", "'poses'", "]", "\n", "self", ".", "shapes", "=", "data", "[", "'shapes'", "]", "\n", "\n", "if", "params_from", "!=", "'all'", ":", "\n", "            ", "if", "params_from", "==", "'not_amass'", ":", "\n", "                ", "indices", "=", "[", "i", "for", "i", ",", "x", "in", "enumerate", "(", "self", ".", "fnames", ")", "\n", "if", "x", ".", "startswith", "(", "'h36m'", ")", "or", "x", ".", "startswith", "(", "'up3d'", ")", "\n", "or", "x", ".", "startswith", "(", "'3dpw'", ")", "]", "\n", "self", ".", "fnames", "=", "[", "self", ".", "fnames", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "self", ".", "poses", "=", "[", "self", ".", "poses", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "self", ".", "shapes", "=", "[", "self", ".", "shapes", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "", "else", ":", "\n", "                ", "indices", "=", "[", "i", "for", "i", ",", "x", "in", "enumerate", "(", "self", ".", "fnames", ")", "if", "x", ".", "startswith", "(", "params_from", ")", "]", "\n", "self", ".", "fnames", "=", "[", "self", ".", "fnames", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "self", ".", "poses", "=", "[", "self", ".", "poses", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "self", ".", "shapes", "=", "[", "self", ".", "shapes", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.data.synthetic_training_dataset.SyntheticTrainingDataset.__len__": [[40, 42], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "poses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.data.synthetic_training_dataset.SyntheticTrainingDataset.__getitem__": [[43, 58], ["torch.is_tensor", "torch.from_numpy", "torch.from_numpy", "index.tolist.tolist.tolist", "torch.from_numpy.astype", "torch.from_numpy.astype"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "index", ")", ":", "\n", "            ", "index", "=", "index", ".", "tolist", "(", ")", "\n", "\n", "", "pose", "=", "self", ".", "poses", "[", "index", "]", "\n", "shape", "=", "self", ".", "shapes", "[", "index", "]", "\n", "assert", "pose", ".", "shape", "==", "(", "72", ",", ")", "and", "shape", ".", "shape", "==", "(", "10", ",", ")", ",", "\"Poses and shapes are wrong: {}, {}, {}\"", ".", "format", "(", "self", ".", "fnames", "[", "index", "]", ",", "\n", "pose", ".", "shape", ",", "shape", ".", "shape", ")", "\n", "\n", "pose", "=", "torch", ".", "from_numpy", "(", "pose", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "shape", "=", "torch", ".", "from_numpy", "(", "shape", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "\n", "return", "{", "'pose'", ":", "pose", ",", "\n", "'shape'", ":", "shape", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.losses.multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.__init__": [[13, 72], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "numpy.log", "numpy.log", "numpy.log", "numpy.log", "numpy.log", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["def", "__init__", "(", "self", ",", "\n", "losses_on", ",", "\n", "init_loss_weights", "=", "None", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "eps", "=", "1e-6", ")", ":", "\n", "        ", "\"\"\"\n        :param losses_on: List of outputs to apply losses on.\n        Subset of ['verts', 'joints2D', 'joints3D', 'pose_params', 'shape_params'].\n        :param init_loss_weights: Initial multi-task loss weights.\n        :param reduction: 'mean' or 'sum'\n        :param eps: small constant\n        \"\"\"", "\n", "super", "(", "HomoscedasticUncertaintyWeightedMultiTaskLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "losses_on", "=", "losses_on", "\n", "assert", "reduction", "in", "[", "'mean'", ",", "'sum'", "]", ",", "\"Invalid reduction for loss.\"", "\n", "\n", "if", "init_loss_weights", "is", "not", "None", ":", "\n", "# Initialise log variances using given init loss weights for vertices, joints2D,", "\n", "# joints3D, shape, pose", "\n", "            ", "init_verts_log_var", "=", "-", "np", ".", "log", "(", "init_loss_weights", "[", "'verts'", "]", "+", "eps", ")", "\n", "init_joints2D_log_var", "=", "-", "np", ".", "log", "(", "init_loss_weights", "[", "'joints2D'", "]", "+", "eps", ")", "\n", "init_joints3D_log_var", "=", "-", "np", ".", "log", "(", "init_loss_weights", "[", "'joints3D'", "]", "+", "eps", ")", "\n", "init_pose_params_log_var", "=", "-", "np", ".", "log", "(", "init_loss_weights", "[", "'pose_params'", "]", "+", "eps", ")", "\n", "init_shape_params_log_var", "=", "-", "np", ".", "log", "(", "init_loss_weights", "[", "'shape_params'", "]", "+", "eps", ")", "\n", "", "else", ":", "\n", "# Initialise log variances to 0.", "\n", "            ", "init_verts_log_var", "=", "0", "\n", "init_joints2D_log_var", "=", "0", "\n", "init_joints3D_log_var", "=", "0", "\n", "init_pose_params_log_var", "=", "0", "\n", "init_shape_params_log_var", "=", "0", "\n", "\n", "", "self", ".", "verts_log_var", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "init_verts_log_var", ")", ".", "float", "(", ")", ",", "\n", "requires_grad", "=", "False", ")", "\n", "self", ".", "joints2D_log_var", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "init_joints2D_log_var", ")", ".", "float", "(", ")", ",", "\n", "requires_grad", "=", "False", ")", "\n", "self", ".", "joints3D_log_var", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "init_joints3D_log_var", ")", ".", "float", "(", ")", ",", "\n", "requires_grad", "=", "False", ")", "\n", "self", ".", "pose_params_log_var", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "init_pose_params_log_var", ")", ".", "float", "(", ")", ",", "\n", "requires_grad", "=", "False", ")", "\n", "self", ".", "shape_params_log_var", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "init_shape_params_log_var", ")", ".", "float", "(", ")", ",", "\n", "requires_grad", "=", "False", ")", "\n", "\n", "if", "'verts'", "in", "losses_on", ":", "\n", "            ", "self", ".", "verts_log_var", ".", "requires_grad", "=", "True", "\n", "self", ".", "verts_loss", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "reduction", ")", "\n", "", "if", "'joints2D'", "in", "losses_on", ":", "\n", "            ", "self", ".", "joints2D_log_var", ".", "requires_grad", "=", "True", "\n", "self", ".", "joints2D_loss", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "reduction", ")", "\n", "", "if", "'joints3D'", "in", "losses_on", ":", "\n", "            ", "self", ".", "joints3D_log_var", ".", "requires_grad", "=", "True", "\n", "self", ".", "joints3D_loss", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "reduction", ")", "\n", "", "if", "'shape_params'", "in", "losses_on", ":", "\n", "            ", "self", ".", "shape_params_log_var", ".", "requires_grad", "=", "True", "\n", "self", ".", "shape_params_loss", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "reduction", ")", "\n", "", "if", "'pose_params'", "in", "losses_on", ":", "\n", "            ", "self", ".", "pose_params_log_var", ".", "requires_grad", "=", "True", "\n", "self", ".", "pose_params_loss", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.losses.multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.forward": [[73, 120], ["multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.verts_loss", "multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.joints2D_loss", "multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.joints3D_loss", "multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.shape_params_loss", "multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.pose_params_loss", "multi_task_loss.HomoscedasticUncertaintyWeightedMultiTaskLoss.silhouette_loss", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "labels.keys", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "labels", ",", "outputs", ")", ":", "\n", "\n", "        ", "total_loss", "=", "0.", "\n", "loss_dict", "=", "{", "}", "\n", "\n", "if", "'verts'", "in", "self", ".", "losses_on", ":", "\n", "            ", "verts_loss", "=", "self", ".", "verts_loss", "(", "outputs", "[", "'verts'", "]", ",", "labels", "[", "'verts'", "]", ")", "\n", "total_loss", "+=", "verts_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "verts_log_var", ")", "+", "self", ".", "verts_log_var", "\n", "loss_dict", "[", "'verts'", "]", "=", "verts_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "verts_log_var", ")", "\n", "\n", "", "if", "'joints2D'", "in", "self", ".", "losses_on", ":", "\n", "            ", "joints2D_label", "=", "labels", "[", "'joints2D'", "]", "\n", "joints2D_pred", "=", "outputs", "[", "'joints2D'", "]", "\n", "\n", "if", "'vis'", "in", "labels", ".", "keys", "(", ")", ":", "\n", "                ", "vis", "=", "labels", "[", "'vis'", "]", "# joint visibility label - boolean", "\n", "joints2D_label", "=", "joints2D_label", "[", "vis", ",", ":", "]", "\n", "joints2D_pred", "=", "joints2D_pred", "[", "vis", ",", ":", "]", "\n", "\n", "", "joints2D_label", "=", "(", "2.0", "*", "joints2D_label", ")", "/", "config", ".", "REGRESSOR_IMG_WH", "-", "1.0", "# normalising j2d label", "\n", "joints2D_loss", "=", "self", ".", "joints2D_loss", "(", "joints2D_pred", ",", "joints2D_label", ")", "\n", "total_loss", "+=", "joints2D_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "joints2D_log_var", ")", "+", "self", ".", "joints2D_log_var", "\n", "loss_dict", "[", "'joints2D'", "]", "=", "joints2D_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "joints2D_log_var", ")", "\n", "\n", "", "if", "'joints3D'", "in", "self", ".", "losses_on", ":", "\n", "            ", "joints3D_loss", "=", "self", ".", "joints3D_loss", "(", "outputs", "[", "'joints3D'", "]", ",", "labels", "[", "'joints3D'", "]", ")", "\n", "total_loss", "+=", "joints3D_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "joints3D_log_var", ")", "+", "self", ".", "joints3D_log_var", "\n", "loss_dict", "[", "'joints3D'", "]", "=", "joints3D_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "joints3D_log_var", ")", "\n", "\n", "", "if", "'shape_params'", "in", "self", ".", "losses_on", ":", "\n", "            ", "shape_params_loss", "=", "self", ".", "shape_params_loss", "(", "outputs", "[", "'shape_params'", "]", ",", "\n", "labels", "[", "'shape_params'", "]", ")", "\n", "total_loss", "+=", "shape_params_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "shape_params_log_var", ")", "+", "self", ".", "shape_params_log_var", "\n", "loss_dict", "[", "'shape_params'", "]", "=", "shape_params_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "shape_params_log_var", ")", "\n", "\n", "", "if", "'pose_params'", "in", "self", ".", "losses_on", ":", "\n", "            ", "pose_params_loss", "=", "self", ".", "pose_params_loss", "(", "outputs", "[", "'pose_params_rot_matrices'", "]", ",", "\n", "labels", "[", "'pose_params_rot_matrices'", "]", ")", "\n", "total_loss", "+=", "pose_params_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "pose_params_log_var", ")", "+", "self", ".", "pose_params_log_var", "\n", "loss_dict", "[", "'pose_params'", "]", "=", "pose_params_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "pose_params_log_var", ")", "\n", "\n", "", "if", "'silhouette'", "in", "self", ".", "losses_on", ":", "\n", "            ", "silhouette_loss", "=", "self", ".", "silhouette_loss", "(", "outputs", "[", "'silhouettes'", "]", ",", "labels", "[", "'silhouettes'", "]", ")", "\n", "total_loss", "+=", "silhouette_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "silhouette_log_var", ")", "+", "self", ".", "silhouette_log_var", "\n", "loss_dict", "[", "'silhouette'", "]", "=", "silhouette_loss", "*", "torch", ".", "exp", "(", "-", "self", ".", "silhouette_log_var", ")", "\n", "\n", "", "return", "total_loss", ",", "loss_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.config.add_pointrend_config": [[7, 42], ["detectron2.config.CfgNode"], "function", ["None"], ["COCOPLUS_REGRESSOR_PATH", "=", "'additional/cocoplus_regressor.npy'", "\n", "H36M_REGRESSOR_PATH", "=", "'additional/J_regressor_h36m.npy'", "\n", "VERTEX_TEXTURE_PATH", "=", "'additional/vertex_texture.npy'", "\n", "CUBE_PARTS_PATH", "=", "'additional/cube_parts.npy'", "\n", "\n", "# ------------------------ Constants ------------------------", "\n", "FOCAL_LENGTH", "=", "5000.", "\n", "REGRESSOR_IMG_WH", "=", "256", "\n", "\n", "# ------------------------ Joint label conventions ------------------------", "\n", "# The SMPL model (im smpl_official.py) returns a large superset of joints.", "\n", "# Different subsets are used during training - e.g. H36M 3D joints convention and COCO 2D joints convention.", "\n", "# You may wish to use different subsets in accordance with your training data/inference needs.", "\n", "\n", "# The joints superset is broken down into: 45 SMPL joints (24 standard + additional fingers/toes/face),", "\n", "# 9 extra joints, 19 cocoplus joints and 17 H36M joints.", "\n", "# The 45 SMPL joints are converted to COCO joints with the map below.", "\n", "# (Not really sure how coco and cocoplus are related.)", "\n", "\n", "# Indices to get 17 COCO joints and 17 H36M joints from joints superset.", "\n", "ALL_JOINTS_TO_COCO_MAP", "=", "[", "24", ",", "26", ",", "25", ",", "28", ",", "27", ",", "16", ",", "17", ",", "18", ",", "19", ",", "20", ",", "21", ",", "1", ",", "2", ",", "4", ",", "5", ",", "7", ",", "8", "]", "\n", "ALL_JOINTS_TO_H36M_MAP", "=", "list", "(", "range", "(", "73", ",", "90", ")", ")", "\n", "\n", "# Indices to get the 14 LSP joints from the 17 H36M joints", "\n", "H36M_TO_J17", "=", "[", "6", ",", "5", ",", "4", ",", "1", ",", "2", ",", "3", ",", "16", ",", "15", ",", "14", ",", "11", ",", "12", ",", "13", ",", "8", ",", "10", ",", "0", ",", "7", ",", "9", "]", "\n", "H36M_TO_J14", "=", "H36M_TO_J17", "[", ":", "14", "]", "\n", "\n", "\n", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._init_mask_head": [[61, 80], ["numpy.sum", "detectron2.modeling.roi_heads.mask_head.build_mask_head", "roi_heads.PointRendROIHeads._init_point_head", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._init_point_head"], ["def", "_init_mask_head", "(", "self", ",", "cfg", ")", ":", "\n", "# fmt: off", "\n", "        ", "self", ".", "mask_on", "=", "cfg", ".", "MODEL", ".", "MASK_ON", "\n", "if", "not", "self", ".", "mask_on", ":", "\n", "            ", "return", "\n", "", "self", ".", "mask_coarse_in_features", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "IN_FEATURES", "\n", "self", ".", "mask_coarse_side_size", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_RESOLUTION", "\n", "# fmt: on", "\n", "\n", "in_channels", "=", "np", ".", "sum", "(", "[", "self", ".", "feature_channels", "[", "f", "]", "for", "f", "in", "self", ".", "mask_coarse_in_features", "]", ")", "\n", "self", ".", "mask_coarse_head", "=", "build_mask_head", "(", "\n", "cfg", ",", "\n", "ShapeSpec", "(", "\n", "channels", "=", "in_channels", ",", "\n", "width", "=", "self", ".", "mask_coarse_side_size", ",", "\n", "height", "=", "self", ".", "mask_coarse_side_size", ",", "\n", ")", ",", "\n", ")", "\n", "self", ".", "_init_point_head", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._init_point_head": [[81, 99], ["numpy.sum", "point_head.build_point_head", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_head.build_point_head"], ["", "def", "_init_point_head", "(", "self", ",", "cfg", ")", ":", "\n", "# fmt: off", "\n", "        ", "self", ".", "mask_point_on", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POINT_HEAD_ON", "\n", "if", "not", "self", ".", "mask_point_on", ":", "\n", "            ", "return", "\n", "", "assert", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NUM_CLASSES", "==", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "NUM_CLASSES", "\n", "self", ".", "mask_point_in_features", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "IN_FEATURES", "\n", "self", ".", "mask_point_train_num_points", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "TRAIN_NUM_POINTS", "\n", "self", ".", "mask_point_oversample_ratio", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "OVERSAMPLE_RATIO", "\n", "self", ".", "mask_point_importance_sample_ratio", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "IMPORTANCE_SAMPLE_RATIO", "\n", "# next two parameters are use in the adaptive subdivions inference procedure", "\n", "self", ".", "mask_point_subdivision_steps", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "SUBDIVISION_STEPS", "\n", "self", ".", "mask_point_subdivision_num_points", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "SUBDIVISION_NUM_POINTS", "\n", "# fmt: on", "\n", "\n", "in_channels", "=", "np", ".", "sum", "(", "[", "self", ".", "feature_channels", "[", "f", "]", "for", "f", "in", "self", ".", "mask_point_in_features", "]", ")", "\n", "self", ".", "mask_point_head", "=", "build_point_head", "(", "\n", "cfg", ",", "ShapeSpec", "(", "channels", "=", "in_channels", ",", "width", "=", "1", ",", "height", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask": [[101, 133], ["detectron2.modeling.roi_heads.roi_heads.select_foreground_proposals", "roi_heads.PointRendROIHeads._forward_mask_coarse", "losses.update", "roi_heads.PointRendROIHeads._forward_mask_coarse", "roi_heads.PointRendROIHeads._forward_mask_point", "detectron2.modeling.roi_heads.mask_head.mask_rcnn_inference", "detectron2.modeling.roi_heads.mask_head.mask_rcnn_loss", "roi_heads.PointRendROIHeads._forward_mask_point"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask_coarse", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask_coarse", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask_point", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask_point"], ["", "def", "_forward_mask", "(", "self", ",", "features", ",", "instances", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the mask prediction branch.\n\n        Args:\n            features (list[Tensor]): #level input features for mask prediction\n            instances (list[Instances]): the per-image instances to train/predict masks.\n                In training, they can be the proposals.\n                In inference, they can be the predicted boxes.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields \"pred_masks\" and return it.\n        \"\"\"", "\n", "if", "not", "self", ".", "mask_on", ":", "\n", "            ", "return", "{", "}", "if", "self", ".", "training", "else", "instances", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "proposals", ",", "_", "=", "select_foreground_proposals", "(", "instances", ",", "self", ".", "num_classes", ")", "\n", "proposal_boxes", "=", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", "\n", "mask_coarse_logits", "=", "self", ".", "_forward_mask_coarse", "(", "features", ",", "proposal_boxes", ")", "\n", "\n", "losses", "=", "{", "\"loss_mask\"", ":", "mask_rcnn_loss", "(", "mask_coarse_logits", ",", "proposals", ")", "}", "\n", "losses", ".", "update", "(", "self", ".", "_forward_mask_point", "(", "features", ",", "mask_coarse_logits", ",", "proposals", ")", ")", "\n", "return", "losses", "\n", "", "else", ":", "\n", "            ", "pred_boxes", "=", "[", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", "\n", "mask_coarse_logits", "=", "self", ".", "_forward_mask_coarse", "(", "features", ",", "pred_boxes", ")", "\n", "\n", "mask_logits", "=", "self", ".", "_forward_mask_point", "(", "features", ",", "mask_coarse_logits", ",", "instances", ")", "\n", "mask_rcnn_inference", "(", "mask_logits", ",", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask_coarse": [[134, 151], ["point_features.generate_regular_grid_point_coords", "point_features.point_sample_fine_grained_features", "roi_heads.PointRendROIHeads.mask_coarse_head", "numpy.sum", "len", "roi_heads.PointRendROIHeads.in_features.index"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.generate_regular_grid_point_coords", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample_fine_grained_features"], ["", "", "def", "_forward_mask_coarse", "(", "self", ",", "features", ",", "boxes", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the coarse mask head.\n        \"\"\"", "\n", "point_coords", "=", "generate_regular_grid_point_coords", "(", "\n", "np", ".", "sum", "(", "len", "(", "x", ")", "for", "x", "in", "boxes", ")", ",", "self", ".", "mask_coarse_side_size", ",", "boxes", "[", "0", "]", ".", "device", "\n", ")", "\n", "mask_coarse_features_list", "=", "[", "\n", "features", "[", "self", ".", "in_features", ".", "index", "(", "k", ")", "]", "for", "k", "in", "self", ".", "mask_coarse_in_features", "\n", "]", "\n", "features_scales", "=", "[", "1.0", "/", "self", ".", "feature_strides", "[", "k", "]", "for", "k", "in", "self", ".", "mask_coarse_in_features", "]", "\n", "# For regular grids of points, this function is equivalent to `len(features_list)' calls", "\n", "# of `ROIAlign` (with `SAMPLING_RATIO=2`), and concat the results.", "\n", "mask_features", ",", "_", "=", "point_sample_fine_grained_features", "(", "\n", "mask_coarse_features_list", ",", "features_scales", ",", "boxes", ",", "point_coords", "\n", ")", "\n", "return", "self", ".", "mask_coarse_head", "(", "mask_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.PointRendROIHeads._forward_mask_point": [[152, 219], ["detectron2.layers.cat", "point_features.point_sample_fine_grained_features", "point_features.point_sample", "roi_heads.PointRendROIHeads.mask_point_head", "detectron2.layers.cat", "mask_coarse_logits.clone", "range", "torch.no_grad", "point_features.get_uncertain_point_coords_with_randomness", "point_head.roi_mask_point_loss", "len", "detectron2.layers.interpolate", "roi_heads.calculate_uncertainty", "point_features.get_uncertain_point_coords_on_grid", "point_features.point_sample_fine_grained_features", "point_features.point_sample", "roi_heads.PointRendROIHeads.mask_point_head", "point_indices.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "mask_logits.reshape().scatter_().view.reshape().scatter_().view.reshape().scatter_().view", "roi_heads.PointRendROIHeads.in_features.index", "roi_heads.calculate_uncertainty", "point_indices.unsqueeze().expand.unsqueeze().expand.unsqueeze", "mask_logits.reshape().scatter_().view.reshape().scatter_().view.reshape().scatter_", "mask_logits.reshape().scatter_().view.reshape().scatter_().view.reshape"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample_fine_grained_features", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.get_uncertain_point_coords_with_randomness", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_head.roi_mask_point_loss", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.calculate_uncertainty", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.get_uncertain_point_coords_on_grid", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample_fine_grained_features", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.calculate_uncertainty"], ["", "def", "_forward_mask_point", "(", "self", ",", "features", ",", "mask_coarse_logits", ",", "instances", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the mask point head.\n        \"\"\"", "\n", "if", "not", "self", ".", "mask_point_on", ":", "\n", "            ", "return", "{", "}", "if", "self", ".", "training", "else", "mask_coarse_logits", "\n", "\n", "", "mask_features_list", "=", "[", "\n", "features", "[", "self", ".", "in_features", ".", "index", "(", "k", ")", "]", "for", "k", "in", "self", ".", "mask_point_in_features", "\n", "]", "\n", "features_scales", "=", "[", "1.0", "/", "self", ".", "feature_strides", "[", "k", "]", "for", "k", "in", "self", ".", "mask_point_in_features", "]", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "proposal_boxes", "=", "[", "x", ".", "proposal_boxes", "for", "x", "in", "instances", "]", "\n", "gt_classes", "=", "cat", "(", "[", "x", ".", "gt_classes", "for", "x", "in", "instances", "]", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "point_coords", "=", "get_uncertain_point_coords_with_randomness", "(", "\n", "mask_coarse_logits", ",", "\n", "lambda", "logits", ":", "calculate_uncertainty", "(", "logits", ",", "gt_classes", ")", ",", "\n", "self", ".", "mask_point_train_num_points", ",", "\n", "self", ".", "mask_point_oversample_ratio", ",", "\n", "self", ".", "mask_point_importance_sample_ratio", ",", "\n", ")", "\n", "\n", "", "fine_grained_features", ",", "point_coords_wrt_image", "=", "point_sample_fine_grained_features", "(", "\n", "mask_features_list", ",", "features_scales", ",", "proposal_boxes", ",", "point_coords", "\n", ")", "\n", "coarse_features", "=", "point_sample", "(", "mask_coarse_logits", ",", "point_coords", ",", "align_corners", "=", "False", ")", "\n", "point_logits", "=", "self", ".", "mask_point_head", "(", "fine_grained_features", ",", "coarse_features", ")", "\n", "return", "{", "\n", "\"loss_mask_point\"", ":", "roi_mask_point_loss", "(", "\n", "point_logits", ",", "instances", ",", "point_coords_wrt_image", "\n", ")", "\n", "}", "\n", "", "else", ":", "\n", "            ", "pred_boxes", "=", "[", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", "\n", "pred_classes", "=", "cat", "(", "[", "x", ".", "pred_classes", "for", "x", "in", "instances", "]", ")", "\n", "# The subdivision code will fail with the empty list of boxes", "\n", "if", "len", "(", "pred_classes", ")", "==", "0", ":", "\n", "                ", "return", "mask_coarse_logits", "\n", "\n", "", "mask_logits", "=", "mask_coarse_logits", ".", "clone", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "mask_point_subdivision_steps", ")", ":", "\n", "                ", "mask_logits", "=", "interpolate", "(", "\n", "mask_logits", ",", "scale_factor", "=", "2", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", "\n", ")", "\n", "uncertainty_map", "=", "calculate_uncertainty", "(", "mask_logits", ",", "pred_classes", ")", "\n", "point_indices", ",", "point_coords", "=", "get_uncertain_point_coords_on_grid", "(", "\n", "uncertainty_map", ",", "self", ".", "mask_point_subdivision_num_points", "\n", ")", "\n", "fine_grained_features", ",", "_", "=", "point_sample_fine_grained_features", "(", "\n", "mask_features_list", ",", "features_scales", ",", "pred_boxes", ",", "point_coords", "\n", ")", "\n", "coarse_features", "=", "point_sample", "(", "\n", "mask_coarse_logits", ",", "point_coords", ",", "align_corners", "=", "False", "\n", ")", "\n", "point_logits", "=", "self", ".", "mask_point_head", "(", "fine_grained_features", ",", "coarse_features", ")", "\n", "\n", "# put mask point predictions to the right places on the upsampled grid.", "\n", "R", ",", "C", ",", "H", ",", "W", "=", "mask_logits", ".", "shape", "\n", "point_indices", "=", "point_indices", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "C", ",", "-", "1", ")", "\n", "mask_logits", "=", "(", "\n", "mask_logits", ".", "reshape", "(", "R", ",", "C", ",", "H", "*", "W", ")", "\n", ".", "scatter_", "(", "2", ",", "point_indices", ",", "point_logits", ")", "\n", ".", "view", "(", "R", ",", "C", ",", "H", ",", "W", ")", "\n", ")", "\n", "", "return", "mask_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.roi_heads.calculate_uncertainty": [[26, 49], ["logits.clone", "logits[].unsqueeze", "torch.abs", "torch.arange"], "function", ["None"], ["def", "calculate_uncertainty", "(", "logits", ",", "classes", ")", ":", "\n", "    ", "\"\"\"\n    We estimate uncerainty as L1 distance between 0.0 and the logit prediction in 'logits' for the\n        foreground class in `classes`.\n\n    Args:\n        logits (Tensor): A tensor of shape (R, C, ...) or (R, 1, ...) for class-specific or\n            class-agnostic, where R is the total number of predicted masks in all images and C is\n            the number of foreground classes. The values are logits.\n        classes (list): A list of length R that contains either predicted of ground truth class\n            for eash predicted mask.\n\n    Returns:\n        scores (Tensor): A tensor of shape (R, 1, ...) that contains uncertainty scores with\n            the most uncertain locations having the highest uncertainty score.\n    \"\"\"", "\n", "if", "logits", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "        ", "gt_class_logits", "=", "logits", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "        ", "gt_class_logits", "=", "logits", "[", "\n", "torch", ".", "arange", "(", "logits", ".", "shape", "[", "0", "]", ",", "device", "=", "logits", ".", "device", ")", ",", "classes", "\n", "]", ".", "unsqueeze", "(", "1", ")", "\n", "", "return", "-", "torch", ".", "abs", "(", "gt_class_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample": [[19, 43], ["torch.nn.functional.grid_sample", "point_coords.unsqueeze.dim", "point_coords.unsqueeze.unsqueeze", "output.squeeze.squeeze"], "function", ["None"], ["def", "point_sample", "(", "input", ",", "point_coords", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.\n    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside\n    [0, 1] x [0, 1] square.\n\n    Args:\n        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.\n        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains\n        [0, 1] x [0, 1] normalized point coordinates.\n\n    Returns:\n        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains\n            features for points in `point_coords`. The features are obtained via bilinear\n            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.\n    \"\"\"", "\n", "add_dim", "=", "False", "\n", "if", "point_coords", ".", "dim", "(", ")", "==", "3", ":", "\n", "        ", "add_dim", "=", "True", "\n", "point_coords", "=", "point_coords", ".", "unsqueeze", "(", "2", ")", "\n", "", "output", "=", "F", ".", "grid_sample", "(", "input", ",", "2.0", "*", "point_coords", "-", "1.0", ",", "**", "kwargs", ")", "\n", "if", "add_dim", ":", "\n", "        ", "output", "=", "output", ".", "squeeze", "(", "3", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.generate_regular_grid_point_coords": [[45, 61], ["torch.tensor", "torch.nn.functional.affine_grid", "F.affine_grid.view().expand", "torch.Size", "F.affine_grid.view"], "function", ["None"], ["", "def", "generate_regular_grid_point_coords", "(", "R", ",", "side_size", ",", "device", ")", ":", "\n", "    ", "\"\"\"\n    Generate regular square grid of points in [0, 1] x [0, 1] coordinate space.\n\n    Args:\n        R (int): The number of grids to sample, one for each region.\n        side_size (int): The side size of the regular grid.\n        device (torch.device): Desired device of returned tensor.\n\n    Returns:\n        (Tensor): A tensor of shape (R, side_size^2, 2) that contains coordinates\n            for the regular grids.\n    \"\"\"", "\n", "aff", "=", "torch", ".", "tensor", "(", "[", "[", "[", "0.5", ",", "0", ",", "0.5", "]", ",", "[", "0", ",", "0.5", ",", "0.5", "]", "]", "]", ",", "device", "=", "device", ")", "\n", "r", "=", "F", ".", "affine_grid", "(", "aff", ",", "torch", ".", "Size", "(", "(", "1", ",", "1", ",", "side_size", ",", "side_size", ")", ")", ",", "align_corners", "=", "False", ")", "\n", "return", "r", ".", "view", "(", "1", ",", "-", "1", ",", "2", ")", ".", "expand", "(", "R", ",", "-", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.get_uncertain_point_coords_with_randomness": [[63, 116], ["int", "torch.rand", "point_features.point_sample", "uncertainty_func", "int", "[].view", "torch.topk", "torch.arange", "detectron2.layers.cat", "detectron2.layers.cat.view", "torch.rand", "idx.view"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample"], ["", "def", "get_uncertain_point_coords_with_randomness", "(", "\n", "coarse_logits", ",", "uncertainty_func", ",", "num_points", ",", "oversample_ratio", ",", "importance_sample_ratio", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Sample points in [0, 1] x [0, 1] coordinate space based on their uncertainty. The unceratinties\n        are calculated for each point using 'uncertainty_func' function that takes point's logit\n        prediction as input.\n    See PointRend paper for details.\n\n    Args:\n        coarse_logits (Tensor): A tensor of shape (N, C, Hmask, Wmask) or (N, 1, Hmask, Wmask) for\n            class-specific or class-agnostic prediction.\n        uncertainty_func: A function that takes a Tensor of shape (N, C, P) or (N, 1, P) that\n            contains logit predictions for P points and returns their uncertainties as a Tensor of\n            shape (N, 1, P).\n        num_points (int): The number of points P to sample.\n        oversample_ratio (int): Oversampling parameter.\n        importance_sample_ratio (float): Ratio of points that are sampled via importnace sampling.\n\n    Returns:\n        point_coords (Tensor): A tensor of shape (N, P, 2) that contains the coordinates of P\n            sampled points.\n    \"\"\"", "\n", "assert", "oversample_ratio", ">=", "1", "\n", "assert", "importance_sample_ratio", "<=", "1", "and", "importance_sample_ratio", ">=", "0", "\n", "num_boxes", "=", "coarse_logits", ".", "shape", "[", "0", "]", "\n", "num_sampled", "=", "int", "(", "num_points", "*", "oversample_ratio", ")", "\n", "point_coords", "=", "torch", ".", "rand", "(", "num_boxes", ",", "num_sampled", ",", "2", ",", "device", "=", "coarse_logits", ".", "device", ")", "\n", "point_logits", "=", "point_sample", "(", "coarse_logits", ",", "point_coords", ",", "align_corners", "=", "False", ")", "\n", "# It is crucial to calculate uncertanty based on the sampled prediction value for the points.", "\n", "# Calculating uncertainties of the coarse predictions first and sampling them for points leads", "\n", "# to worse results. To illustrate the difference: a sampled point between two coarse predictions", "\n", "# with -1 and 1 logits has 0 logit prediction and therefore 0 uncertainty value, however, if one", "\n", "# calculates uncertainties for the coarse predictions first (-1 and -1) and sampe it for the", "\n", "# center point, they will get -1 unceratinty.", "\n", "point_uncertainties", "=", "uncertainty_func", "(", "point_logits", ")", "\n", "num_uncertain_points", "=", "int", "(", "importance_sample_ratio", "*", "num_points", ")", "\n", "num_random_points", "=", "num_points", "-", "num_uncertain_points", "\n", "idx", "=", "torch", ".", "topk", "(", "point_uncertainties", "[", ":", ",", "0", ",", ":", "]", ",", "k", "=", "num_uncertain_points", ",", "dim", "=", "1", ")", "[", "1", "]", "\n", "shift", "=", "num_sampled", "*", "torch", ".", "arange", "(", "num_boxes", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "coarse_logits", ".", "device", ")", "\n", "idx", "+=", "shift", "[", ":", ",", "None", "]", "\n", "point_coords", "=", "point_coords", ".", "view", "(", "-", "1", ",", "2", ")", "[", "idx", ".", "view", "(", "-", "1", ")", ",", ":", "]", ".", "view", "(", "\n", "num_boxes", ",", "num_uncertain_points", ",", "2", "\n", ")", "\n", "if", "num_random_points", ">", "0", ":", "\n", "        ", "point_coords", "=", "cat", "(", "\n", "[", "\n", "point_coords", ",", "\n", "torch", ".", "rand", "(", "num_boxes", ",", "num_random_points", ",", "2", ",", "device", "=", "coarse_logits", ".", "device", ")", ",", "\n", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "", "return", "point_coords", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.get_uncertain_point_coords_on_grid": [[118, 143], ["min", "torch.zeros", "float", "float", "torch.topk", "uncertainty_map.view"], "function", ["None"], ["", "def", "get_uncertain_point_coords_on_grid", "(", "uncertainty_map", ",", "num_points", ")", ":", "\n", "    ", "\"\"\"\n    Find `num_points` most uncertain points from `uncertainty_map` grid.\n\n    Args:\n        uncertainty_map (Tensor): A tensor of shape (N, 1, H, W) that contains uncertainty\n            values for a set of points on a regular H x W grid.\n        num_points (int): The number of points P to select.\n\n    Returns:\n        point_indices (Tensor): A tensor of shape (N, P) that contains indices from\n            [0, H x W) of the most uncertain points.\n        point_coords (Tensor): A tensor of shape (N, P, 2) that contains [0, 1] x [0, 1] normalized\n            coordinates of the most uncertain points from the H x W grid.\n    \"\"\"", "\n", "R", ",", "_", ",", "H", ",", "W", "=", "uncertainty_map", ".", "shape", "\n", "h_step", "=", "1.0", "/", "float", "(", "H", ")", "\n", "w_step", "=", "1.0", "/", "float", "(", "W", ")", "\n", "\n", "num_points", "=", "min", "(", "H", "*", "W", ",", "num_points", ")", "\n", "point_indices", "=", "torch", ".", "topk", "(", "uncertainty_map", ".", "view", "(", "R", ",", "H", "*", "W", ")", ",", "k", "=", "num_points", ",", "dim", "=", "1", ")", "[", "1", "]", "\n", "point_coords", "=", "torch", ".", "zeros", "(", "R", ",", "num_points", ",", "2", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "uncertainty_map", ".", "device", ")", "\n", "point_coords", "[", ":", ",", ":", ",", "0", "]", "=", "w_step", "/", "2.0", "+", "(", "point_indices", "%", "W", ")", ".", "to", "(", "torch", ".", "float", ")", "*", "w_step", "\n", "point_coords", "[", ":", ",", ":", ",", "1", "]", "=", "h_step", "/", "2.0", "+", "(", "point_indices", "//", "W", ")", ".", "to", "(", "torch", ".", "float", ")", "*", "h_step", "\n", "return", "point_indices", ",", "point_coords", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample_fine_grained_features": [[145, 189], ["detectron2.structures.Boxes.cat", "point_features.get_point_coords_wrt_image", "torch.split", "enumerate", "len", "enumerate", "point_features.append", "detectron2.layers.cat", "point_features_per_image.append", "detectron2.layers.cat", "torch.tensor", "point_sample().squeeze().transpose", "point_sample().squeeze", "point_features.point_sample", "feature_map[].unsqueeze", "point_coords_scaled.unsqueeze"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.get_point_coords_wrt_image", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample"], ["", "def", "point_sample_fine_grained_features", "(", "features_list", ",", "feature_scales", ",", "boxes", ",", "point_coords", ")", ":", "\n", "    ", "\"\"\"\n    Get features from feature maps in `features_list` that correspond to specific point coordinates\n        inside each bounding box from `boxes`.\n\n    Args:\n        features_list (list[Tensor]): A list of feature map tensors to get features from.\n        feature_scales (list[float]): A list of scales for tensors in `features_list`.\n        boxes (list[Boxes]): A list of I Boxes  objects that contain R_1 + ... + R_I = R boxes all\n            together.\n        point_coords (Tensor): A tensor of shape (R, P, 2) that contains\n            [0, 1] x [0, 1] box-normalized coordinates of the P sampled points.\n\n    Returns:\n        point_features (Tensor): A tensor of shape (R, C, P) that contains features sampled\n            from all features maps in feature_list for P sampled points for all R boxes in `boxes`.\n        point_coords_wrt_image (Tensor): A tensor of shape (R, P, 2) that contains image-level\n            coordinates of P points.\n    \"\"\"", "\n", "cat_boxes", "=", "Boxes", ".", "cat", "(", "boxes", ")", "\n", "num_boxes", "=", "[", "len", "(", "b", ")", "for", "b", "in", "boxes", "]", "\n", "\n", "point_coords_wrt_image", "=", "get_point_coords_wrt_image", "(", "cat_boxes", ".", "tensor", ",", "point_coords", ")", "\n", "split_point_coords_wrt_image", "=", "torch", ".", "split", "(", "point_coords_wrt_image", ",", "num_boxes", ")", "\n", "\n", "point_features", "=", "[", "]", "\n", "for", "idx_img", ",", "point_coords_wrt_image_per_image", "in", "enumerate", "(", "split_point_coords_wrt_image", ")", ":", "\n", "        ", "point_features_per_image", "=", "[", "]", "\n", "for", "idx_feature", ",", "feature_map", "in", "enumerate", "(", "features_list", ")", ":", "\n", "            ", "h", ",", "w", "=", "feature_map", ".", "shape", "[", "-", "2", ":", "]", "\n", "scale", "=", "torch", ".", "tensor", "(", "[", "w", ",", "h", "]", ",", "device", "=", "feature_map", ".", "device", ")", "/", "feature_scales", "[", "idx_feature", "]", "\n", "point_coords_scaled", "=", "point_coords_wrt_image_per_image", "/", "scale", "\n", "point_features_per_image", ".", "append", "(", "\n", "point_sample", "(", "\n", "feature_map", "[", "idx_img", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "point_coords_scaled", ".", "unsqueeze", "(", "0", ")", ",", "\n", "align_corners", "=", "False", ",", "\n", ")", "\n", ".", "squeeze", "(", "0", ")", "\n", ".", "transpose", "(", "1", ",", "0", ")", "\n", ")", "\n", "", "point_features", ".", "append", "(", "cat", "(", "point_features_per_image", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "return", "cat", "(", "point_features", ",", "dim", "=", "0", ")", ",", "point_coords_wrt_image", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.get_point_coords_wrt_image": [[191, 216], ["torch.no_grad", "point_coords.clone"], "function", ["None"], ["", "def", "get_point_coords_wrt_image", "(", "boxes_coords", ",", "point_coords", ")", ":", "\n", "    ", "\"\"\"\n    Convert box-normalized [0, 1] x [0, 1] point cooordinates to image-level coordinates.\n\n    Args:\n        boxes_coords (Tensor): A tensor of shape (R, 4) that contains bounding boxes.\n            coordinates.\n        point_coords (Tensor): A tensor of shape (R, P, 2) that contains\n            [0, 1] x [0, 1] box-normalized coordinates of the P sampled points.\n\n    Returns:\n        point_coords_wrt_image (Tensor): A tensor of shape (R, P, 2) that contains\n            image-normalized coordinates of P sampled points.\n    \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "point_coords_wrt_image", "=", "point_coords", ".", "clone", "(", ")", "\n", "point_coords_wrt_image", "[", ":", ",", ":", ",", "0", "]", "=", "point_coords_wrt_image", "[", ":", ",", ":", ",", "0", "]", "*", "(", "\n", "boxes_coords", "[", ":", ",", "None", ",", "2", "]", "-", "boxes_coords", "[", ":", ",", "None", ",", "0", "]", "\n", ")", "\n", "point_coords_wrt_image", "[", ":", ",", ":", ",", "1", "]", "=", "point_coords_wrt_image", "[", ":", ",", ":", ",", "1", "]", "*", "(", "\n", "boxes_coords", "[", ":", ",", "None", ",", "3", "]", "-", "boxes_coords", "[", ":", ",", "None", ",", "1", "]", "\n", ")", "\n", "point_coords_wrt_image", "[", ":", ",", ":", ",", "0", "]", "+=", "boxes_coords", "[", ":", ",", "None", ",", "0", "]", "\n", "point_coords_wrt_image", "[", ":", ",", ":", ",", "1", "]", "+=", "boxes_coords", "[", ":", ",", "None", ",", "1", "]", "\n", "", "return", "point_coords_wrt_image", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_head.StandardPointHead.__init__": [[103, 139], ["torch.nn.Module.__init__", "range", "torch.nn.Conv1d", "torch.nn.init.normal_", "torch.nn.Conv1d", "point_head.StandardPointHead.add_module", "point_head.StandardPointHead.fc_layers.append", "fvcore.c2_msra_fill", "torch.nn.init.constant_"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["def", "__init__", "(", "self", ",", "cfg", ",", "input_shape", ":", "ShapeSpec", ")", ":", "\n", "        ", "\"\"\"\n        The following attributes are parsed from config:\n            fc_dim: the output dimension of each FC layers\n            num_fc: the number of FC layers\n            coarse_pred_each_layer: if True, coarse prediction features are concatenated to each\n                layer's input\n        \"\"\"", "\n", "super", "(", "StandardPointHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# fmt: off", "\n", "num_classes", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "NUM_CLASSES", "\n", "fc_dim", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "FC_DIM", "\n", "num_fc", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "NUM_FC", "\n", "cls_agnostic_mask", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "CLS_AGNOSTIC_MASK", "\n", "self", ".", "coarse_pred_each_layer", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "COARSE_PRED_EACH_LAYER", "\n", "input_channels", "=", "input_shape", ".", "channels", "\n", "# fmt: on", "\n", "\n", "fc_dim_in", "=", "input_channels", "+", "num_classes", "\n", "self", ".", "fc_layers", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_fc", ")", ":", "\n", "            ", "fc", "=", "nn", ".", "Conv1d", "(", "fc_dim_in", ",", "fc_dim", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "True", ")", "\n", "self", ".", "add_module", "(", "\"fc{}\"", ".", "format", "(", "k", "+", "1", ")", ",", "fc", ")", "\n", "self", ".", "fc_layers", ".", "append", "(", "fc", ")", "\n", "fc_dim_in", "=", "fc_dim", "\n", "fc_dim_in", "+=", "num_classes", "if", "self", ".", "coarse_pred_each_layer", "else", "0", "\n", "\n", "", "num_mask_classes", "=", "1", "if", "cls_agnostic_mask", "else", "num_classes", "\n", "self", ".", "predictor", "=", "nn", ".", "Conv1d", "(", "fc_dim_in", ",", "num_mask_classes", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "\n", "for", "layer", "in", "self", ".", "fc_layers", ":", "\n", "            ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "# use normal distribution initialization for mask prediction layer", "\n", "", "nn", ".", "init", ".", "normal_", "(", "self", ".", "predictor", ".", "weight", ",", "std", "=", "0.001", ")", "\n", "if", "self", ".", "predictor", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "predictor", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_head.StandardPointHead.forward": [[140, 147], ["torch.cat", "point_head.StandardPointHead.predictor", "torch.nn.functional.relu", "layer", "detectron2.layers.cat"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "fine_grained_features", ",", "coarse_features", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "(", "fine_grained_features", ",", "coarse_features", ")", ",", "dim", "=", "1", ")", "\n", "for", "layer", "in", "self", ".", "fc_layers", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "layer", "(", "x", ")", ")", "\n", "if", "self", ".", "coarse_pred_each_layer", ":", "\n", "                ", "x", "=", "cat", "(", "(", "x", ",", "coarse_features", ")", ",", "dim", "=", "1", ")", "\n", "", "", "return", "self", ".", "predictor", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_head.roi_mask_point_loss": [[22, 94], ["detectron2.utils.events.get_event_storage().put_scalar", "torch.nn.functional.binary_cross_entropy_with_logits", "isinstance", "torch.no_grad", "mask_logits.size", "detectron2.layers.cat", "detectron2.layers.cat.numel", "torch.arange", "detectron2.layers.cat", "detectron2.layers.cat.to", "mask_accurate.nonzero().size", "mask_accurate.numel", "detectron2.layers.cat.to", "len", "mask_logits.size", "torch.tensor", "len", "detectron2.layers.cat.append", "mask_logits.sum", "detectron2.utils.events.get_event_storage", "instances_per_image.gt_classes.to", "detectron2.layers.cat.append", "point_features.point_sample().squeeze", "mask_accurate.nonzero", "point_features.point_sample", "len", "gt_bit_masks.to().unsqueeze", "gt_bit_masks.to"], "function", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_features.point_sample", "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.densepose.structures.DensePoseList.to"], ["def", "roi_mask_point_loss", "(", "mask_logits", ",", "instances", ",", "points_coord", ")", ":", "\n", "    ", "\"\"\"\n    Compute the point-based loss for instance segmentation mask predictions.\n\n    Args:\n        mask_logits (Tensor): A tensor of shape (R, C, P) or (R, 1, P) for class-specific or\n            class-agnostic, where R is the total number of predicted masks in all images, C is the\n            number of foreground classes, and P is the number of points sampled for each mask.\n            The values are logits.\n        instances (list[Instances]): A list of N Instances, where N is the number of images\n            in the batch. These instances are in 1:1 correspondence with the `mask_logits`. So, i_th\n            elememt of the list contains R_i objects and R_1 + ... + R_N is equal to R.\n            The ground-truth labels (class, box, mask, ...) associated with each instance are stored\n            in fields.\n        points_coords (Tensor): A tensor of shape (R, P, 2), where R is the total number of\n            predicted masks and P is the number of points for each mask. The coordinates are in\n            the image pixel coordinate space, i.e. [0, H] x [0, W].\n    Returns:\n        point_loss (Tensor): A scalar tensor containing the loss.\n    \"\"\"", "\n", "assert", "len", "(", "instances", ")", "==", "0", "or", "isinstance", "(", "\n", "instances", "[", "0", "]", ".", "gt_masks", ",", "BitMasks", "\n", ")", ",", "\"Point head works with GT in 'bitmask' format only. Set INPUT.MASK_FORMAT to 'bitmask'.\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "cls_agnostic_mask", "=", "mask_logits", ".", "size", "(", "1", ")", "==", "1", "\n", "total_num_masks", "=", "mask_logits", ".", "size", "(", "0", ")", "\n", "\n", "gt_classes", "=", "[", "]", "\n", "gt_mask_logits", "=", "[", "]", "\n", "idx", "=", "0", "\n", "for", "instances_per_image", "in", "instances", ":", "\n", "            ", "if", "not", "cls_agnostic_mask", ":", "\n", "                ", "gt_classes_per_image", "=", "instances_per_image", ".", "gt_classes", ".", "to", "(", "dtype", "=", "torch", ".", "int64", ")", "\n", "gt_classes", ".", "append", "(", "gt_classes_per_image", ")", "\n", "\n", "", "gt_bit_masks", "=", "instances_per_image", ".", "gt_masks", ".", "tensor", "\n", "h", ",", "w", "=", "instances_per_image", ".", "gt_masks", ".", "image_size", "\n", "scale", "=", "torch", ".", "tensor", "(", "[", "w", ",", "h", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "gt_bit_masks", ".", "device", ")", "\n", "points_coord_grid_sample_format", "=", "(", "\n", "points_coord", "[", "idx", ":", "idx", "+", "len", "(", "instances_per_image", ")", "]", "/", "scale", "\n", ")", "\n", "idx", "+=", "len", "(", "instances_per_image", ")", "\n", "gt_mask_logits", ".", "append", "(", "\n", "point_sample", "(", "\n", "gt_bit_masks", ".", "to", "(", "torch", ".", "float32", ")", ".", "unsqueeze", "(", "1", ")", ",", "\n", "points_coord_grid_sample_format", ",", "\n", "align_corners", "=", "False", ",", "\n", ")", ".", "squeeze", "(", "1", ")", "\n", ")", "\n", "", "gt_mask_logits", "=", "cat", "(", "gt_mask_logits", ")", "\n", "\n", "# torch.mean (in binary_cross_entropy_with_logits) doesn't", "\n", "# accept empty tensors, so handle it separately", "\n", "", "if", "gt_mask_logits", ".", "numel", "(", ")", "==", "0", ":", "\n", "        ", "return", "mask_logits", ".", "sum", "(", ")", "*", "0", "\n", "\n", "", "if", "cls_agnostic_mask", ":", "\n", "        ", "mask_logits", "=", "mask_logits", "[", ":", ",", "0", "]", "\n", "", "else", ":", "\n", "        ", "indices", "=", "torch", ".", "arange", "(", "total_num_masks", ")", "\n", "gt_classes", "=", "cat", "(", "gt_classes", ",", "dim", "=", "0", ")", "\n", "mask_logits", "=", "mask_logits", "[", "indices", ",", "gt_classes", "]", "\n", "\n", "# Log the training accuracy (using gt classes and 0.0 threshold for the logits)", "\n", "", "mask_accurate", "=", "(", "mask_logits", ">", "0.0", ")", "==", "gt_mask_logits", ".", "to", "(", "dtype", "=", "torch", ".", "uint8", ")", "\n", "mask_accuracy", "=", "mask_accurate", ".", "nonzero", "(", ")", ".", "size", "(", "0", ")", "/", "mask_accurate", ".", "numel", "(", ")", "\n", "get_event_storage", "(", ")", ".", "put_scalar", "(", "\"point_rend/accuracy\"", ",", "mask_accuracy", ")", "\n", "\n", "point_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "mask_logits", ",", "gt_mask_logits", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ",", "reduction", "=", "\"mean\"", "\n", ")", "\n", "return", "point_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.point_head.build_point_head": [[149, 155], ["POINT_HEAD_REGISTRY.get"], "function", ["None"], ["", "", "def", "build_point_head", "(", "cfg", ",", "input_channels", ")", ":", "\n", "    ", "\"\"\"\n    Build a point head defined by `cfg.MODEL.POINT_HEAD.NAME`.\n    \"\"\"", "\n", "head_name", "=", "cfg", ".", "MODEL", ".", "POINT_HEAD", ".", "NAME", "\n", "return", "POINT_HEAD_REGISTRY", ".", "get", "(", "head_name", ")", "(", "cfg", ",", "input_channels", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__": [[19, 79], ["torch.nn.Module.__init__", "detectron2.layers.Conv2d", "coarse_mask_head.CoarseMaskHead.conv_layers.append", "range", "torch.nn.Linear", "torch.nn.init.normal_", "torch.nn.init.constant_", "detectron2.layers.Conv2d", "coarse_mask_head.CoarseMaskHead.conv_layers.append", "torch.nn.Linear", "coarse_mask_head.CoarseMaskHead.add_module", "coarse_mask_head.CoarseMaskHead.fcs.append", "fvcore.c2_msra_fill", "fvcore.c2_xavier_fill"], "methods", ["home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.__init__"], ["def", "__init__", "(", "self", ",", "cfg", ",", "input_shape", ":", "ShapeSpec", ")", ":", "\n", "        ", "\"\"\"\n        The following attributes are parsed from config:\n            conv_dim: the output dimension of the conv layers\n            fc_dim: the feature dimenstion of the FC layers\n            num_fc: the number of FC layers\n            output_side_resolution: side resolution of the output square mask prediction\n        \"\"\"", "\n", "super", "(", "CoarseMaskHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# fmt: off", "\n", "self", ".", "num_classes", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NUM_CLASSES", "\n", "conv_dim", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "CONV_DIM", "\n", "self", ".", "fc_dim", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "FC_DIM", "\n", "num_fc", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NUM_FC", "\n", "self", ".", "output_side_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "OUTPUT_SIDE_RESOLUTION", "\n", "self", ".", "input_channels", "=", "input_shape", ".", "channels", "\n", "self", ".", "input_h", "=", "input_shape", ".", "height", "\n", "self", ".", "input_w", "=", "input_shape", ".", "width", "\n", "# fmt: on", "\n", "\n", "self", ".", "conv_layers", "=", "[", "]", "\n", "if", "self", ".", "input_channels", ">", "conv_dim", ":", "\n", "            ", "self", ".", "reduce_channel_dim_conv", "=", "Conv2d", "(", "\n", "self", ".", "input_channels", ",", "\n", "conv_dim", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "True", ",", "\n", "activation", "=", "F", ".", "relu", ",", "\n", ")", "\n", "self", ".", "conv_layers", ".", "append", "(", "self", ".", "reduce_channel_dim_conv", ")", "\n", "\n", "", "self", ".", "reduce_spatial_dim_conv", "=", "Conv2d", "(", "\n", "conv_dim", ",", "conv_dim", ",", "kernel_size", "=", "2", ",", "stride", "=", "2", ",", "padding", "=", "0", ",", "bias", "=", "True", ",", "activation", "=", "F", ".", "relu", "\n", ")", "\n", "self", ".", "conv_layers", ".", "append", "(", "self", ".", "reduce_spatial_dim_conv", ")", "\n", "\n", "input_dim", "=", "conv_dim", "*", "self", ".", "input_h", "*", "self", ".", "input_w", "\n", "input_dim", "//=", "4", "\n", "\n", "self", ".", "fcs", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_fc", ")", ":", "\n", "            ", "fc", "=", "nn", ".", "Linear", "(", "input_dim", ",", "self", ".", "fc_dim", ")", "\n", "self", ".", "add_module", "(", "\"coarse_mask_fc{}\"", ".", "format", "(", "k", "+", "1", ")", ",", "fc", ")", "\n", "self", ".", "fcs", ".", "append", "(", "fc", ")", "\n", "input_dim", "=", "self", ".", "fc_dim", "\n", "\n", "", "output_dim", "=", "self", ".", "num_classes", "*", "self", ".", "output_side_resolution", "*", "self", ".", "output_side_resolution", "\n", "\n", "self", ".", "prediction", "=", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "output_dim", ")", "\n", "# use normal distribution initialization for mask prediction layer", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "prediction", ".", "weight", ",", "std", "=", "0.001", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "prediction", ".", "bias", ",", "0", ")", "\n", "\n", "for", "layer", "in", "self", ".", "conv_layers", ":", "\n", "            ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "", "for", "layer", "in", "self", ".", "fcs", ":", "\n", "            ", "weight_init", ".", "c2_xavier_fill", "(", "layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.akashsengupta1997_STRAPS-3DHumanShapePose.point_rend.coarse_mask_head.CoarseMaskHead.forward": [[80, 90], ["torch.nn.functional.relu.view", "torch.flatten", "coarse_mask_head.CoarseMaskHead.prediction().view", "layer", "torch.nn.functional.relu", "layer", "coarse_mask_head.CoarseMaskHead.prediction"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "N", "=", "x", ".", "shape", "[", "0", "]", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "self", ".", "input_channels", ",", "self", ".", "input_h", ",", "self", ".", "input_w", ")", "\n", "for", "layer", "in", "self", ".", "conv_layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "x", "=", "torch", ".", "flatten", "(", "x", ",", "start_dim", "=", "1", ")", "\n", "for", "layer", "in", "self", ".", "fcs", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "layer", "(", "x", ")", ")", "\n", "", "return", "self", ".", "prediction", "(", "x", ")", ".", "view", "(", "\n", "N", ",", "self", ".", "num_classes", ",", "self", ".", "output_side_resolution", ",", "self", ".", "output_side_resolution", "\n", ")", "\n"]]}