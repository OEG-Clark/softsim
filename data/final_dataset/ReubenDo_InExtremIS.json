{"home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.inference.infinite_iterable": [[44, 47], ["None"], "function", ["None"], ["def", "infinite_iterable", "(", "i", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "yield", "from", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.inference.inference": [[48, 80], ["dict", "dict", "os.path.join", "checkpoint_path.format.format", "os.path.isfile", "print", "model.to.load_state_dict", "model.to.to", "model.to.eval", "torch.load", "monai.data.Dataset", "monai.data.DataLoader", "torch.no_grad", "monai.data.NiftiSaver", "tqdm.tqdm", "batch[].to", "monai.inferers.sliding_window_inference", "pred.argmax().detach.argmax().detach", "monai.data.NiftiSaver.save_batch", "os.path.join", "pred.argmax().detach.argmax"], "function", ["None"], ["", "", "def", "inference", "(", "paths_dict", ",", "model", ",", "transform_inference", ",", "device", ",", "opt", ")", ":", "\n", "\n", "# Define transforms for data normalization and augmentation", "\n", "    ", "dataloaders", "=", "dict", "(", ")", "\n", "subjects_dataset", "=", "dict", "(", ")", "\n", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "model_dir", ",", "'models'", ",", "'./CP_{}.pth'", ")", "\n", "checkpoint_path", "=", "checkpoint_path", ".", "format", "(", "opt", ".", "epoch_inf", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ",", "'no checkpoint found'", "\n", "print", "(", "checkpoint_path", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "checkpoint_path", ")", ")", "\n", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "for", "phase", "in", "[", "'inference'", "]", ":", "\n", "        ", "subjects_dataset", "[", "phase", "]", "=", "Dataset", "(", "paths_dict", ",", "transform", "=", "transform_inference", ")", "\n", "dataloaders", "[", "phase", "]", "=", "DataLoader", "(", "subjects_dataset", "[", "phase", "]", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "\n", "", "model", ".", "eval", "(", ")", "# Set model to evaluate mode", "\n", "\n", "fold_name", "=", "'output_pred'", "\n", "# Iterate over data", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "saver", "=", "NiftiSaver", "(", "output_dir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "model_dir", ",", "fold_name", ")", ")", "\n", "for", "batch", "in", "tqdm", "(", "dataloaders", "[", "'inference'", "]", ")", ":", "\n", "            ", "inputs", "=", "batch", "[", "'img'", "]", ".", "to", "(", "device", ")", "\n", "\n", "pred", "=", "sliding_window_inference", "(", "inputs", ",", "opt", ".", "spatial_shape", ",", "1", ",", "model", ",", "mode", "=", "'gaussian'", ")", "\n", "\n", "pred", "=", "pred", ".", "argmax", "(", "1", ",", "keepdim", "=", "True", ")", ".", "detach", "(", ")", "\n", "saver", ".", "save_batch", "(", "pred", ",", "batch", "[", "\"img_meta_dict\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.inference.main": [[82, 148], ["inference.parsing_data", "monai.utils.set_determinism", "torch.cuda.is_available", "print", "os.path.join", "pandas.read_csv", "dict", "print", "print", "print", "monai.transforms.Compose", "network.unet2d5.UNet2D5().to", "print", "inference.inference", "print", "torch.device", "Exception", "[].tolist", "print", "dict", "os.path.exists", "monai.transforms.LoadNiftid", "monai.transforms.AddChanneld", "monai.transforms.Orientationd", "monai.transforms.NormalizeIntensityd", "monai.transforms.ToTensord", "network.unet2d5.UNet2D5", "torch.cuda.is_available", "os.path.join", "os.path.join", "paths_dict[].append", "len", "df_split[].isin"], "function", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.write_geodesics.parsing_data", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.inference.inference"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "opt", "=", "parsing_data", "(", ")", "\n", "\n", "set_determinism", "(", "seed", "=", "2", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "print", "(", "'[INFO] GPU available.'", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\n", "\"[INFO] No GPU found.\"", ")", "\n", "\n", "\n", "", "print", "(", "\"[INFO] Reading data\"", ")", "\n", "# PHASES", "\n", "split_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "dataset_split", ")", "\n", "df_split", "=", "pd", ".", "read_csv", "(", "split_path", ",", "header", "=", "None", ")", "\n", "list_file", "=", "dict", "(", ")", "\n", "for", "phase", "in", "PHASES", ":", "# list of patient name associated to each phase", "\n", "        ", "list_file", "[", "phase", "]", "=", "df_split", "[", "df_split", "[", "1", "]", ".", "isin", "(", "[", "phase", "]", ")", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "\n", "# CREATING DICT FOR DATASET", "\n", "", "mod_ext", "=", "\"_T2.nii.gz\"", "\n", "paths_dict", "=", "{", "split", ":", "[", "]", "for", "split", "in", "PHASES", "}", "\n", "for", "split", "in", "PHASES", ":", "\n", "        ", "for", "subject", "in", "list_file", "[", "split", "]", ":", "\n", "            ", "subject_data", "=", "dict", "(", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "path_data", ",", "subject", "+", "mod_ext", ")", ")", ":", "\n", "                ", "subject_data", "[", "\"img\"", "]", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_data", ",", "subject", "+", "mod_ext", ")", "\n", "paths_dict", "[", "split", "]", ".", "append", "(", "subject_data", ")", "\n", "", "", "print", "(", "f\"Nb patients in {split} data: {len(paths_dict[split])}\"", ")", "\n", "\n", "# Logging hyperparameters", "\n", "", "print", "(", "\"[INFO] Hyperparameters\"", ")", "\n", "print", "(", "'Spatial shape: {}'", ".", "format", "(", "opt", ".", "spatial_shape", ")", ")", "\n", "print", "(", "f\"Inference on the {opt.phase} set\"", ")", "\n", "\n", "# PREPROCESSING", "\n", "all_keys", "=", "[", "\"img\"", "]", "\n", "test_transforms", "=", "Compose", "(", "\n", "(", "\n", "LoadNiftid", "(", "keys", "=", "all_keys", ")", ",", "\n", "AddChanneld", "(", "keys", "=", "all_keys", ")", ",", "\n", "Orientationd", "(", "keys", "=", "all_keys", ",", "axcodes", "=", "\"RAS\"", ")", ",", "\n", "NormalizeIntensityd", "(", "keys", "=", "all_keys", ")", ",", "\n", "ToTensord", "(", "keys", "=", "all_keys", ")", "\n", ")", "\n", ")", "\n", "\n", "# MODEL", "\n", "norm_op_kwargs", "=", "{", "\"eps\"", ":", "1e-5", ",", "\"affine\"", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "\"negative_slope\"", ":", "1e-2", ",", "\"inplace\"", ":", "True", "}", "\n", "\n", "model", "=", "UNet2D5", "(", "input_channels", "=", "1", ",", "\n", "base_num_features", "=", "16", ",", "\n", "num_classes", "=", "NB_CLASSES", ",", "\n", "num_pool", "=", "4", ",", "\n", "conv_op", "=", "nn", ".", "Conv3d", ",", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", ",", "\n", "norm_op_kwargs", "=", "norm_op_kwargs", ",", "\n", "nonlin", "=", "net_nonlin", ",", "\n", "nonlin_kwargs", "=", "net_nonlin_kwargs", ")", ".", "to", "(", "device", ")", "\n", "\n", "print", "(", "\"[INFO] Inference\"", ")", "\n", "inference", "(", "paths_dict", "[", "opt", ".", "phase", "]", ",", "model", ",", "test_transforms", ",", "device", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.inference.parsing_data": [[150, 182], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parsing_data", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Performing inference'", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--model_dir'", ",", "\n", "type", "=", "str", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--dataset_split\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"splits/split_inextremis_budget1.csv\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--path_data\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"data/VS_MICCAI21/T2/\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--phase'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'inference'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--spatial_shape'", ",", "\n", "type", "=", "int", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "(", "224", ",", "224", ",", "48", ")", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epoch_inf'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'best'", ")", "\n", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.train.train": [[49, 241], ["time.time", "monai.data.Dataset", "monai.data.Dataset", "dict", "utilities.utils.infinite_iterable", "utilities.utils.infinite_iterable", "os.path.join", "os.path.isfile", "torch.optim.SGD", "ScribbleDA.scribbleDALoss.CRFLoss", "logger.info", "logger.info", "monai.data.DataLoader", "monai.data.DataLoader", "len", "pandas.read_csv", "model.load_state_dict", "pandas.DataFrame", "model.parameters", "logger.info", "logger.info", "time.time", "torch.load", "logger.info", "tqdm.tqdm", "logger.info", "logger.info", "save_path.format", "model.train", "model.eval", "range", "next", "batch[].to", "torch.optim.SGD.zero_grad", "loss.item", "logger.info", "df.append.append", "df.append.to_csv", "utilities.utils.poly_lr", "torch.save", "torch.save", "batch[].to", "batch[].to", "batch[].to", "torch.set_grad_enabled", "criterion", "torch.save", "torch.save", "model.state_dict", "save_path.format", "model.state_dict", "save_path.format", "model", "monai.inferers.sliding_window_inference", "time.time", "range", "torch.cat", "loss.backward", "torch.optim.SGD.step", "model.state_dict", "save_path.format", "model.state_dict", "save_path.format", "utilities.geodesics.generate_geodesics", "geodesics.append", "time.time", "ScribbleDA.scribbleDALoss.CRFLoss.", "utilities.geodesics.generate_geodesics.to", "numpy.prod"], "function", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.infinite_iterable", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.infinite_iterable", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.train.train", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.poly_lr", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.geodesics.generate_geodesics"], ["def", "train", "(", "paths_dict", ",", "model", ",", "transformation", ",", "criterion", ",", "device", ",", "save_path", ",", "logger", ",", "opt", ")", ":", "\n", "\n", "    ", "since", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Define transforms for data normalization and augmentation", "\n", "subjects_train", "=", "Dataset", "(", "\n", "paths_dict", "[", "\"training\"", "]", ",", "\n", "transform", "=", "transformation", "[", "\"training\"", "]", ")", "\n", "\n", "subjects_val", "=", "Dataset", "(", "\n", "paths_dict", "[", "\"validation\"", "]", ",", "\n", "transform", "=", "transformation", "[", "\"validation\"", "]", ")", "\n", "\n", "# Dataloaders", "\n", "dataloaders", "=", "dict", "(", ")", "\n", "dataloaders", "[", "\"training\"", "]", "=", "infinite_iterable", "(", "\n", "DataLoader", "(", "subjects_train", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "num_workers", "=", "2", ",", "shuffle", "=", "True", ")", "\n", ")", "\n", "dataloaders", "[", "\"validation\"", "]", "=", "infinite_iterable", "(", "\n", "DataLoader", "(", "subjects_val", ",", "batch_size", "=", "1", ",", "num_workers", "=", "2", ")", "\n", ")", "\n", "\n", "nb_batches", "=", "{", "\n", "\"training\"", ":", "30", ",", "# One image patch per epoch for the full dataset", "\n", "\"validation\"", ":", "len", "(", "paths_dict", "[", "\"validation\"", "]", ")", "\n", "}", "\n", "\n", "# Training parameters are saved ", "\n", "df_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "model_dir", ",", "\"log.csv\"", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "df_path", ")", ":", "# If the training already started", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "df_path", ",", "index_col", "=", "False", ")", "\n", "epoch", "=", "df", ".", "iloc", "[", "-", "1", "]", "[", "\"epoch\"", "]", "\n", "best_epoch", "=", "df", ".", "iloc", "[", "-", "1", "]", "[", "\"best_epoch\"", "]", "\n", "best_val", "=", "df", ".", "iloc", "[", "-", "1", "]", "[", "\"best_val\"", "]", "\n", "initial_lr", "=", "df", ".", "iloc", "[", "-", "1", "]", "[", "\"lr\"", "]", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", ".", "format", "(", "\"best\"", ")", ")", ")", "\n", "\n", "", "else", ":", "# If training from scratch", "\n", "        ", "columns", "=", "[", "\"epoch\"", ",", "\"best_epoch\"", ",", "\"MA\"", ",", "\"best_MA\"", ",", "\"lr\"", ",", "\"timeit\"", "]", "\n", "df", "=", "pd", ".", "DataFrame", "(", "columns", "=", "columns", ")", "\n", "best_val", "=", "None", "\n", "best_epoch", "=", "0", "\n", "epoch", "=", "0", "\n", "initial_lr", "=", "opt", ".", "learning_rate", "\n", "\n", "\n", "# Optimisation policy mimicking nnUnet training policy", "\n", "", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "initial_lr", ",", "\n", "weight_decay", "=", "weight_decay", ",", "momentum", "=", "0.99", ",", "nesterov", "=", "True", ")", "\n", "\n", "# CRF Loss initialisation", "\n", "crf_l", "=", "CRFLoss", "(", "alpha", "=", "opt", ".", "alpha", ",", "beta", "=", "opt", ".", "beta", ",", "is_da", "=", "False", ",", "use_norm", "=", "False", ")", "\n", "\n", "# Training loop", "\n", "continue_training", "=", "True", "\n", "while", "continue_training", ":", "\n", "        ", "epoch", "+=", "1", "\n", "logger", ".", "info", "(", "\"-\"", "*", "10", ")", "\n", "logger", ".", "info", "(", "\"Epoch {}/\"", ".", "format", "(", "epoch", ")", ")", "\n", "logger", ".", "info", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "logger", ".", "info", "(", "\"Current learning rate is: {}\"", ".", "format", "(", "param_group", "[", "\"lr\"", "]", ")", ")", "\n", "\n", "# Each epoch has a training and validation phase", "\n", "", "for", "phase", "in", "PHASES", ":", "\n", "            ", "if", "phase", "==", "\"training\"", ":", "\n", "                ", "model", ".", "train", "(", ")", "# Set model to training mode", "\n", "", "else", ":", "\n", "                ", "model", ".", "eval", "(", ")", "# Set model to evaluate mode ", "\n", "\n", "# Initializing the statistics", "\n", "", "running_loss", "=", "0.0", "\n", "running_loss_reg", "=", "0.0", "\n", "running_loss_seg", "=", "0.0", "\n", "epoch_samples", "=", "0", "\n", "running_time", "=", "0.0", "\n", "\n", "# Iterate over data", "\n", "for", "_", "in", "tqdm", "(", "range", "(", "nb_batches", "[", "phase", "]", ")", ")", ":", "\n", "                ", "batch", "=", "next", "(", "dataloaders", "[", "phase", "]", ")", "\n", "inputs", "=", "batch", "[", "\"img\"", "]", ".", "to", "(", "device", ")", "# T2 images", "\n", "if", "opt", ".", "mode", "==", "\"extreme_points\"", ":", "\n", "                    ", "extremes", "=", "batch", "[", "\"label\"", "]", ".", "to", "(", "device", ")", "# Extreme points", "\n", "img_gradients", "=", "batch", "[", "\"img_gradient\"", "]", ".", "to", "(", "device", ")", "# Pre-Computed Sobel map", "\n", "", "else", ":", "\n", "                    ", "labels", "=", "batch", "[", "\"label\"", "]", ".", "to", "(", "device", ")", "\n", "\n", "# zero the parameter gradients", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "with", "torch", ".", "set_grad_enabled", "(", "phase", "==", "\"training\"", ")", ":", "\n", "                    ", "if", "phase", "==", "\"training\"", ":", "# Random patch predictions", "\n", "                        ", "outputs", "=", "model", "(", "inputs", ")", "\n", "", "else", ":", "# if validation, Inference on the full image", "\n", "                        ", "outputs", "=", "sliding_window_inference", "(", "\n", "inputs", "=", "inputs", ",", "\n", "roi_size", "=", "opt", ".", "spatial_shape", ",", "\n", "sw_batch_size", "=", "1", ",", "\n", "predictor", "=", "model", ",", "\n", "mode", "=", "\"gaussian\"", ",", "\n", ")", "\n", "\n", "", "if", "opt", ".", "mode", "==", "\"extreme_points\"", ":", "# Generate geodesics", "\n", "                        ", "init_time_geodesics", "=", "time", ".", "time", "(", ")", "\n", "geodesics", "=", "[", "]", "\n", "nb_target", "=", "outputs", ".", "shape", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "nb_target", ")", ":", "\n", "                            ", "geodesics_i", "=", "generate_geodesics", "(", "\n", "extreme", "=", "extremes", "[", "i", ",", "...", "]", ",", "\n", "img_gradient", "=", "img_gradients", "[", "i", ",", "...", "]", ",", "\n", "prob", "=", "outputs", "[", "i", ",", "...", "]", ",", "\n", "with_prob", "=", "opt", ".", "with_prob", ",", "\n", "with_euclidean", "=", "opt", ".", "with_euclidean", "\n", ")", "\n", "geodesics", ".", "append", "(", "geodesics_i", ".", "to", "(", "device", ")", ")", "\n", "", "labels", "=", "torch", ".", "cat", "(", "geodesics", ",", "0", ")", "\n", "time_geodesics", "=", "time", ".", "time", "(", ")", "-", "init_time_geodesics", "\n", "", "else", ":", "\n", "                        ", "time_geodesics", "=", "0.", "\n", "\n", "# Segmentation loss", "\n", "", "loss_seg", "=", "criterion", "(", "outputs", ",", "labels", ",", "phase", ")", "\n", "\n", "# CRF regularisation (training only)", "\n", "if", "(", "opt", ".", "beta", ">", "0", "or", "opt", ".", "alpha", ">", "0", ")", "and", "phase", "==", "\"training\"", "and", "opt", ".", "mode", "==", "\"extreme_points\"", ":", "\n", "                        ", "reg", "=", "opt", ".", "weight_crf", "/", "np", ".", "prod", "(", "opt", ".", "spatial_shape", ")", "*", "crf_l", "(", "inputs", ",", "outputs", ")", "\n", "loss", "=", "loss_seg", "+", "reg", "\n", "", "else", ":", "\n", "                        ", "reg", "=", "0.0", "\n", "loss", "=", "loss_seg", "\n", "\n", "", "if", "phase", "==", "\"training\"", ":", "\n", "                        ", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Iteration statistics", "\n", "", "", "epoch_samples", "+=", "1", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "\n", "running_loss_reg", "+=", "reg", "\n", "running_loss_seg", "+=", "loss_seg", "\n", "running_time", "+=", "time_geodesics", "\n", "\n", "# Epoch statistcs", "\n", "", "epoch_loss", "=", "running_loss", "/", "epoch_samples", "\n", "epoch_loss_reg", "=", "running_loss_reg", "/", "epoch_samples", "\n", "epoch_loss_seg", "=", "running_loss_seg", "/", "epoch_samples", "\n", "if", "phase", "==", "\"training\"", ":", "\n", "                ", "epoch_time", "=", "running_time", "/", "epoch_samples", "\n", "\n", "", "logger", ".", "info", "(", "\"{}  Loss Reg: {:.4f}\"", ".", "format", "(", "\n", "phase", ",", "epoch_loss_reg", ")", ")", "\n", "logger", ".", "info", "(", "\"{}  Loss Seg: {:.4f}\"", ".", "format", "(", "\n", "phase", ",", "epoch_loss_seg", ")", ")", "\n", "if", "phase", "==", "\"training\"", ":", "\n", "                ", "logger", ".", "info", "(", "\"{}  Time Geodesics: {:.4f}\"", ".", "format", "(", "\n", "phase", ",", "epoch_time", ")", ")", "\n", "\n", "# Saving best model on the validation set", "\n", "", "if", "phase", "==", "\"validation\"", ":", "\n", "                ", "if", "best_val", "is", "None", ":", "# first iteration", "\n", "                    ", "best_val", "=", "epoch_loss", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "save_path", ".", "format", "(", "\"best\"", ")", ")", "\n", "\n", "", "if", "epoch_loss", "<=", "best_val", ":", "\n", "                    ", "best_val", "=", "epoch_loss", "\n", "best_epoch", "=", "epoch", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "save_path", ".", "format", "(", "\"best\"", ")", ")", "\n", "\n", "", "df", "=", "df", ".", "append", "(", "\n", "{", "\"epoch\"", ":", "epoch", ",", "\n", "\"best_epoch\"", ":", "best_epoch", ",", "\n", "\"best_val\"", ":", "best_val", ",", "\n", "\"lr\"", ":", "param_group", "[", "\"lr\"", "]", ",", "\n", "\"timeit\"", ":", "epoch_time", "}", ",", "\n", "ignore_index", "=", "True", ")", "\n", "df", ".", "to_csv", "(", "df_path", ",", "index", "=", "False", ")", "\n", "\n", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", "=", "poly_lr", "(", "epoch", ",", "MAX_EPOCHS", ",", "opt", ".", "learning_rate", ",", "0.9", ")", "\n", "\n", "# Early stopping performed when full annotations are used (training set may be small)", "\n", "", "if", "opt", ".", "mode", "==", "\"full_annotations\"", "and", "epoch", "-", "best_epoch", ">", "70", ":", "\n", "                ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "save_path", ".", "format", "(", "\"final\"", ")", ")", "\n", "continue_training", "=", "False", "\n", "\n", "", "if", "epoch", "==", "MAX_EPOCHS", ":", "\n", "                ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "save_path", ".", "format", "(", "\"final\"", ")", ")", "\n", "continue_training", "=", "False", "\n", "\n", "", "", "", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "since", "\n", "logger", ".", "info", "(", "\"[INFO] Training completed in {:.0f}m {:.0f}s\"", ".", "format", "(", "\n", "time_elapsed", "//", "60", ",", "time_elapsed", "%", "60", ")", ")", "\n", "logger", ".", "info", "(", "f\"[INFO] Best validation epoch is {best_epoch}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.train.main": [[243, 377], ["monai.utils.set_determinism", "train.parsing_data", "os.path.join", "os.path.join", "utilities.utils.create_logger", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "utilities.utils.create_logger.info", "torch.cuda.is_available", "os.path.isfile", "utilities.utils.create_logger.error", "pandas.read_csv", "dict", "dict", "monai.transforms.Compose", "monai.transforms.Compose", "utilities.utils.create_logger.info", "network.unet2d5.UNet2D5().to", "utilities.utils.create_logger.info", "train.train", "os.path.exists", "os.makedirs", "utilities.utils.create_logger.info", "torch.device", "utilities.utils.create_logger.error", "[].tolist", "utilities.utils.create_logger.info", "all_keys.append", "monai.transforms.LoadNiftid", "monai.transforms.AddChanneld", "monai.transforms.Orientationd", "monai.transforms.NormalizeIntensityd", "monai.transforms.SpatialPadd", "monai.transforms.RandFlipd", "monai.transforms.RandSpatialCropd", "monai.transforms.ToTensord", "monai.transforms.LoadNiftid", "monai.transforms.AddChanneld", "monai.transforms.Orientationd", "monai.transforms.NormalizeIntensityd", "monai.transforms.SpatialPadd", "monai.transforms.ToTensord", "utilities.losses.DC", "dict", "os.path.join", "os.path.join", "os.path.join", "network.unet2d5.UNet2D5", "utilities.losses.DC.", "utilities.losses.DC_CE_Focal", "utilities.losses.PartialLoss", "torch.cuda.is_available", "os.path.exists", "os.path.exists", "os.path.exists", "paths_dict[].append", "len", "paths_dict[].append", "df_split[].isin"], "function", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.write_geodesics.parsing_data", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.create_logger", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.train.train"], ["", "def", "main", "(", ")", ":", "\n", "    ", "set_determinism", "(", "seed", "=", "2", ")", "\n", "\n", "opt", "=", "parsing_data", "(", ")", "\n", "\n", "# FOLDERS", "\n", "fold_dir", "=", "opt", ".", "model_dir", "\n", "fold_dir_model", "=", "os", ".", "path", ".", "join", "(", "fold_dir", ",", "\"models\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "fold_dir_model", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "fold_dir_model", ")", "\n", "", "save_path", "=", "os", ".", "path", ".", "join", "(", "fold_dir_model", ",", "\"./CP_{}.pth\"", ")", "\n", "\n", "if", "opt", ".", "path_labels", "is", "None", ":", "\n", "        ", "opt", ".", "path_labels", "=", "opt", ".", "path_data", "\n", "\n", "", "logger", "=", "create_logger", "(", "fold_dir", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Hyperparameters\"", ")", "\n", "logger", ".", "info", "(", "f\"Alpha: {opt.alpha}\"", ")", "\n", "logger", ".", "info", "(", "f\"Beta: {opt.beta}\"", ")", "\n", "logger", ".", "info", "(", "f\"Weight Reg: {opt.weight_crf}\"", ")", "\n", "logger", ".", "info", "(", "f\"Batch size: {opt.batch_size}\"", ")", "\n", "logger", ".", "info", "(", "f\"Spatial shape: {opt.spatial_shape}\"", ")", "\n", "logger", ".", "info", "(", "f\"Initial lr: {opt.learning_rate}\"", ")", "\n", "logger", ".", "info", "(", "f\"Postfix img gradients: {opt.img_gradient_postfix}\"", ")", "\n", "logger", ".", "info", "(", "f\"Postfix labels: {opt.label_postfix}\"", ")", "\n", "logger", ".", "info", "(", "f\"With euclidean: {opt.with_euclidean}\"", ")", "\n", "logger", ".", "info", "(", "f\"With probs: {opt.with_prob}\"", ")", "\n", "\n", "# GPU CHECKING", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"[INFO] GPU available.\"", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "        ", "raise", "logger", ".", "error", "(", "\n", "\"[INFO] No GPU found\"", ")", "\n", "\n", "# SPLIT", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "opt", ".", "dataset_split", ")", ",", "logger", ".", "error", "(", "\"[ERROR] Invalid split\"", ")", "\n", "df_split", "=", "pd", ".", "read_csv", "(", "opt", ".", "dataset_split", ",", "header", "=", "None", ")", "\n", "list_file", "=", "dict", "(", ")", "\n", "for", "split", "in", "PHASES", ":", "\n", "        ", "list_file", "[", "split", "]", "=", "df_split", "[", "df_split", "[", "1", "]", ".", "isin", "(", "[", "split", "]", ")", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "\n", "\n", "# CREATING DICT FOR CACHEDATASET", "\n", "", "mod_ext", "=", "\"_T2.nii.gz\"", "\n", "grad_ext", "=", "f\"_{opt.img_gradient_postfix}.nii.gz\"", "\n", "extreme_ext", "=", "f\"_{opt.label_postfix}.nii.gz\"", "\n", "paths_dict", "=", "{", "split", ":", "[", "]", "for", "split", "in", "PHASES", "}", "\n", "\n", "for", "split", "in", "PHASES", ":", "\n", "        ", "for", "subject", "in", "list_file", "[", "split", "]", ":", "\n", "            ", "subject_data", "=", "dict", "(", ")", "\n", "\n", "img_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_data", ",", "subject", "+", "mod_ext", ")", "\n", "img_grad_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_labels", ",", "subject", "+", "grad_ext", ")", "\n", "lab_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_labels", ",", "subject", "+", "extreme_ext", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "img_path", ")", "and", "os", ".", "path", ".", "exists", "(", "lab_path", ")", ":", "\n", "                ", "subject_data", "[", "\"img\"", "]", "=", "img_path", "\n", "subject_data", "[", "\"label\"", "]", "=", "lab_path", "\n", "\n", "if", "opt", ".", "mode", "==", "\"extreme_points\"", ":", "\n", "                    ", "if", "os", ".", "path", ".", "exists", "(", "img_grad_path", ")", ":", "\n", "                        ", "subject_data", "[", "\"img_gradient\"", "]", "=", "img_grad_path", "\n", "paths_dict", "[", "split", "]", ".", "append", "(", "subject_data", ")", "\n", "", "", "else", ":", "\n", "                     ", "paths_dict", "[", "split", "]", ".", "append", "(", "subject_data", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "f\"Nb patients in {split} data: {len(paths_dict[split])}\"", ")", "\n", "\n", "\n", "# PREPROCESSING", "\n", "", "transforms", "=", "dict", "(", ")", "\n", "all_keys", "=", "[", "\"img\"", ",", "\"label\"", "]", "\n", "if", "opt", ".", "mode", "==", "\"extreme_points\"", ":", "\n", "        ", "all_keys", ".", "append", "(", "\"img_gradient\"", ")", "\n", "\n", "", "transforms_training", "=", "(", "\n", "LoadNiftid", "(", "keys", "=", "all_keys", ")", ",", "\n", "AddChanneld", "(", "keys", "=", "all_keys", ")", ",", "\n", "Orientationd", "(", "keys", "=", "all_keys", ",", "axcodes", "=", "\"RAS\"", ")", ",", "\n", "NormalizeIntensityd", "(", "keys", "=", "[", "\"img\"", "]", ")", ",", "\n", "SpatialPadd", "(", "keys", "=", "all_keys", ",", "spatial_size", "=", "opt", ".", "spatial_shape", ")", ",", "\n", "RandFlipd", "(", "keys", "=", "all_keys", ",", "prob", "=", "0.5", ",", "spatial_axis", "=", "0", ")", ",", "\n", "RandSpatialCropd", "(", "keys", "=", "all_keys", ",", "roi_size", "=", "opt", ".", "spatial_shape", ",", "random_center", "=", "True", ",", "random_size", "=", "False", ")", ",", "\n", "ToTensord", "(", "keys", "=", "all_keys", ")", ",", "\n", ")", "\n", "transforms", "[", "\"training\"", "]", "=", "Compose", "(", "transforms_training", ")", "\n", "\n", "transforms_validation", "=", "(", "\n", "LoadNiftid", "(", "keys", "=", "all_keys", ")", ",", "\n", "AddChanneld", "(", "keys", "=", "all_keys", ")", ",", "\n", "Orientationd", "(", "keys", "=", "all_keys", ",", "axcodes", "=", "\"RAS\"", ")", ",", "\n", "NormalizeIntensityd", "(", "keys", "=", "[", "\"img\"", "]", ")", ",", "\n", "SpatialPadd", "(", "keys", "=", "all_keys", ",", "spatial_size", "=", "opt", ".", "spatial_shape", ")", ",", "\n", "ToTensord", "(", "keys", "=", "all_keys", ")", "\n", ")", "\n", "transforms", "[", "\"validation\"", "]", "=", "Compose", "(", "transforms_validation", ")", "\n", "\n", "# MODEL", "\n", "logger", ".", "info", "(", "\"[INFO] Building model\"", ")", "\n", "norm_op_kwargs", "=", "{", "\"eps\"", ":", "1e-5", ",", "\"affine\"", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "\"negative_slope\"", ":", "1e-2", ",", "\"inplace\"", ":", "True", "}", "\n", "\n", "model", "=", "UNet2D5", "(", "input_channels", "=", "1", ",", "\n", "base_num_features", "=", "16", ",", "\n", "num_classes", "=", "NB_CLASSES", ",", "\n", "num_pool", "=", "4", ",", "\n", "conv_op", "=", "nn", ".", "Conv3d", ",", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", ",", "\n", "norm_op_kwargs", "=", "norm_op_kwargs", ",", "\n", "nonlin", "=", "net_nonlin", ",", "\n", "nonlin_kwargs", "=", "net_nonlin_kwargs", ")", ".", "to", "(", "device", ")", "\n", "\n", "\n", "logger", ".", "info", "(", "\"[INFO] Training\"", ")", "\n", "if", "opt", ".", "mode", "==", "\"full_annotations\"", ":", "\n", "        ", "dice", "=", "DC", "(", "NB_CLASSES", ")", "\n", "criterion", "=", "lambda", "pred", ",", "grnd", ",", "phase", ":", "dice", "(", "pred", ",", "grnd", ")", "\n", "\n", "", "elif", "opt", ".", "mode", "==", "\"extreme_points\"", "or", "opt", ".", "mode", "==", "\"geodesics\"", ":", "\n", "        ", "dice_ce_focal", "=", "DC_CE_Focal", "(", "NB_CLASSES", ")", "\n", "criterion", "=", "PartialLoss", "(", "dice_ce_focal", ")", "\n", "\n", "", "train", "(", "paths_dict", ",", "\n", "model", ",", "\n", "transforms", ",", "\n", "criterion", ",", "\n", "device", ",", "\n", "save_path", ",", "\n", "logger", ",", "\n", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.None.train.parsing_data": [[378, 456], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parsing_data", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Script to train the models using extreme points as supervision\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--model_dir\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Path to the model directory\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--mode\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Choice of the supervision mode\"", ",", "\n", "choices", "=", "[", "\"full_annotations\"", ",", "\"extreme_points\"", ",", "\"geodesics\"", "]", ",", "\n", "default", "=", "\"extreme_points\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--weight_crf\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.1", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--alpha\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "15", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--beta\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.05", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "6", ",", "\n", "help", "=", "\"Size of the batch size (default: 6)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--dataset_split\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"splits/split_inextremis_budget1.csv\"", ",", "\n", "help", "=", "\"Path to split file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--path_data\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"data/VS_MICCAI21/T2/\"", ",", "\n", "help", "=", "\"Path to the T2 scans\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--path_labels\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"Path to the extreme points\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "1e-2", ",", "\n", "help", "=", "\"Initial learning rate\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--label_postfix\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Postfix of the Labels points\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--img_gradient_postfix\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Postfix of the gradient images\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--spatial_shape\"", ",", "\n", "type", "=", "int", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "(", "224", ",", "224", ",", "48", ")", ",", "\n", "help", "=", "\"Size of the window patch\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--with_prob\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Add Deep probabilities\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--with_euclidean\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Add Euclidean distance\"", ")", "\n", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.InitWeights_He.__init__": [[10, 12], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "neg_slope", "=", "1e-2", ")", ":", "\n", "        ", "self", ".", "neg_slope", "=", "neg_slope", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.InitWeights_He.__call__": [[13, 18], ["isinstance", "isinstance", "isinstance", "isinstance", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose3d", ")", ":", "\n", "            ", "module", ".", "weight", "=", "nn", ".", "init", ".", "kaiming_normal_", "(", "module", ".", "weight", ",", "a", "=", "self", ".", "neg_slope", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", "=", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.ConvNormNonlinBlock.__init__": [[21, 61], ["torch.nn.Module.__init__", "unet2d5.ConvNormNonlinBlock.conv_op", "unet2d5.ConvNormNonlinBlock.norm_op", "unet2d5.ConvNormNonlinBlock.nonlin", "unet2d5.ConvNormNonlinBlock.conv_op", "unet2d5.ConvNormNonlinBlock.norm_op", "unet2d5.ConvNormNonlinBlock.nonlin", "torch.nn.Sequential", "torch.nn.Sequential"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_channels", ",", "\n", "output_channels", ",", "\n", "conv_op", "=", "nn", ".", "Conv3d", ",", "\n", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", ",", "\n", "norm_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "\n", "nonlin_kwargs", "=", "None", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Block: Conv->Norm->Activation->Conv->Norm->Activation\n        \"\"\"", "\n", "\n", "super", "(", "ConvNormNonlinBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "output_channels", "=", "output_channels", "\n", "\n", "self", ".", "first_conv", "=", "self", ".", "conv_op", "(", "input_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "self", ".", "first_norm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "first_acti", "=", "self", ".", "nonlin", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n", "self", ".", "second_conv", "=", "self", ".", "conv_op", "(", "output_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "self", ".", "second_norm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "second_acti", "=", "self", ".", "nonlin", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n", "self", ".", "block", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "first_conv", ",", "\n", "self", ".", "first_norm", ",", "\n", "self", ".", "first_acti", ",", "\n", "self", ".", "second_conv", ",", "\n", "self", ".", "second_norm", ",", "\n", "self", ".", "second_acti", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.ConvNormNonlinBlock.forward": [[64, 66], ["unet2d5.ConvNormNonlinBlock.block"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "block", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.Upsample.__init__": [[70, 76], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "'nearest'", ",", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.Upsample.forward": [[77, 79], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "self", ".", "size", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "self", ".", "mode", ",", "align_corners", "=", "self", ".", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.UNet2D5.__init__": [[83, 208], ["unet2d5.InitWeights_He", "torch.nn.Module.__init__", "numpy.prod", "range", "unet2d5.UNet2D5.conv_blocks_context.append", "range", "conv_op", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "unet2d5.UNet2D5.conv_pad_sizes.append", "unet2d5.UNet2D5.conv_blocks_context.append", "unet2d5.UNet2D5.td.append", "unet2d5.ConvNormNonlinBlock", "unet2d5.UNet2D5.tu.append", "unet2d5.UNet2D5.conv_blocks_localization.append", "unet2d5.UNet2D5.apply", "unet2d5.ConvNormNonlinBlock", "pool_op", "unet2d5.Upsample", "unet2d5.ConvNormNonlinBlock"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_channels", ",", "\n", "base_num_features", ",", "\n", "num_classes", ",", "\n", "num_pool", ",", "\n", "conv_op", "=", "nn", ".", "Conv3d", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", ",", "\n", "norm_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "\n", "nonlin_kwargs", "=", "None", ",", "\n", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ")", ":", "\n", "        ", "\"\"\"\n        2.5D CNN combining 2D and 3D convolutions to dealwith the low through-plane resolution.\n        The first two stages have 2D convolutions while the others have 3D convolutions. \n\n        Architecture inspired by: \n        Wang,et al: Automatic segmentation of  vestibular  schwannoma  from  t2-weighted  mri  \n        by  deep  spatial  attention  with hardness-weighted loss. MICCAI 2019. \n        \"\"\"", "\n", "super", "(", "UNet2D5", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "             ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "\n", "", "self", ".", "conv_kwargs", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "weightInitializer", "=", "weightInitializer", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "upsample_mode", "=", "'trilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool3d", "\n", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "            ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ",", "1", ")", "]", "*", "2", "+", "[", "(", "3", ",", "3", ",", "3", ")", "]", "*", "(", "num_pool", "-", "1", ")", "\n", "\n", "\n", "", "self", ".", "input_shape_must_be_divisible_by", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "pool_op_kernel_sizes", "=", "pool_op_kernel_sizes", "\n", "self", ".", "conv_kernel_sizes", "=", "conv_kernel_sizes", "\n", "\n", "self", ".", "conv_pad_sizes", "=", "[", "]", "\n", "for", "krnl", "in", "self", ".", "conv_kernel_sizes", ":", "\n", "            ", "self", ".", "conv_pad_sizes", ".", "append", "(", "[", "1", "if", "i", "==", "3", "else", "0", "for", "i", "in", "krnl", "]", ")", "\n", "\n", "\n", "\n", "", "self", ".", "conv_blocks_context", "=", "[", "]", "\n", "self", ".", "conv_blocks_localization", "=", "[", "]", "\n", "self", ".", "td", "=", "[", "]", "\n", "self", ".", "tu", "=", "[", "]", "\n", "self", ".", "seg_outputs", "=", "[", "]", "\n", "\n", "\n", "input_features", "=", "input_channels", "\n", "output_features", "=", "base_num_features", "\n", "\n", "\n", "for", "d", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "d", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "d", "]", "\n", "# add convolutions", "\n", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "ConvNormNonlinBlock", "(", "input_features", ",", "output_features", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", ")", "\n", "\n", "self", ".", "td", ".", "append", "(", "pool_op", "(", "pool_op_kernel_sizes", "[", "d", "]", ")", ")", "\n", "input_features", "=", "output_features", "\n", "output_features", "=", "2", "*", "output_features", "# Number of kernel increases by a factor 2 after each pooling", "\n", "\n", "\n", "", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", ".", "output_channels", "\n", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "ConvNormNonlinBlock", "(", "input_features", ",", "final_num_features", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", ")", "\n", "\n", "\n", "# now lets build the localization pathway", "\n", "for", "u", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "nfeatures_from_skip", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "2", "+", "u", ")", "]", ".", "output_channels", "# self.conv_blocks_context[-1] is bottleneck, so start with -2", "\n", "n_features_after_tu_and_concat", "=", "nfeatures_from_skip", "*", "2", "\n", "\n", "# the first conv reduces the number of features to match those of skip", "\n", "# the following convs work on that number of features", "\n", "# if not convolutional upsampling then the final conv reduces the num of features again", "\n", "if", "u", "!=", "num_pool", "-", "1", ":", "\n", "                ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "3", "+", "u", ")", "]", ".", "output_channels", "\n", "", "else", ":", "\n", "                ", "final_num_features", "=", "nfeatures_from_skip", "\n", "\n", "", "self", ".", "tu", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "mode", "=", "upsample_mode", ")", ")", "\n", "\n", "\n", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_blocks_localization", ".", "append", "(", "ConvNormNonlinBlock", "(", "n_features_after_tu_and_concat", ",", "final_num_features", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", ")", "\n", "\n", "\n", "", "self", ".", "final_conv", "=", "conv_op", "(", "self", ".", "conv_blocks_localization", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "False", ")", "\n", "\n", "\n", "\n", "# register all modules properly", "\n", "self", ".", "conv_blocks_localization", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_localization", ")", "\n", "self", ".", "conv_blocks_context", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_context", ")", "\n", "self", ".", "td", "=", "nn", ".", "ModuleList", "(", "self", ".", "td", ")", "\n", "self", ".", "tu", "=", "nn", ".", "ModuleList", "(", "self", ".", "tu", ")", "\n", "\n", "if", "self", ".", "weightInitializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "self", ".", "weightInitializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.network.unet2d5.UNet2D5.forward": [[209, 226], ["range", "range", "unet2d5.UNet2D5.final_conv", "skips.append", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skips", "=", "[", "]", "\n", "seg_outputs", "=", "[", "]", "\n", "for", "d", "in", "range", "(", "len", "(", "self", ".", "conv_blocks_context", ")", "-", "1", ")", ":", "\n", "            ", "x", "=", "self", ".", "conv_blocks_context", "[", "d", "]", "(", "x", ")", "\n", "skips", ".", "append", "(", "x", ")", "\n", "x", "=", "self", ".", "td", "[", "d", "]", "(", "x", ")", "\n", "\n", "", "x", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", "(", "x", ")", "\n", "\n", "for", "u", "in", "range", "(", "len", "(", "self", ".", "tu", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "tu", "[", "u", "]", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "skips", "[", "-", "(", "u", "+", "1", ")", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "conv_blocks_localization", "[", "u", "]", "(", "x", ")", "\n", "\n", "", "output", "=", "self", ".", "final_conv", "(", "x", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.geodesics.normalize": [[5, 7], ["data.min", "data.max", "data.min"], "function", ["None"], ["def", "normalize", "(", "data", ")", ":", "\n", "    ", "return", "(", "data", "-", "data", ".", "min", "(", ")", ")", "/", "(", "data", ".", "max", "(", ")", "-", "data", ".", "min", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.geodesics.generate_geodesics": [[8, 77], ["extreme.squeeze.squeeze", "img_gradient.squeeze.squeeze", "prob.squeeze.squeeze", "[].min().item", "[].min().item", "[].min().item", "img_gradient[].cpu().numpy", "prob[].detach", "[].cpu().numpy", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.zeros", "torch.from_numpy", "range", "[].max().item", "[].max().item", "[].max().item", "couples.append", "couples.append", "couples.append", "numpy.zeros", "img_gradient[].cpu().numpy.copy", "dijkstra3d.dijkstra", "output_crop.astype", "[].min", "[].min", "[].min", "img_gradient[].cpu", "[].cpu", "numpy.sqrt", "geodesics.normalize", "[].max", "[].max", "[].max", "k[].item", "k[].item", "k[].item", "k[].item", "k[].item", "k[].item", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.nn.Softmax"], "function", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.geodesics.normalize"], ["", "def", "generate_geodesics", "(", "extreme", ",", "img_gradient", ",", "prob", ",", "inside_bb_value", "=", "12", ",", "with_prob", "=", "True", ",", "with_euclidean", "=", "True", ")", ":", "\n", "# ", "\n", "    ", "extreme", "=", "extreme", ".", "squeeze", "(", ")", "\n", "img_gradient", "=", "img_gradient", ".", "squeeze", "(", ")", "\n", "prob", "=", "prob", ".", "squeeze", "(", ")", "\n", "\n", "if", "(", "extreme", "==", "inside_bb_value", ")", ".", "sum", "(", ")", ">", "0", ":", "# If the image patch is not only background", "\n", "# Corners of the bounding boxes", "\n", "        ", "bb_x_min", "=", "torch", ".", "where", "(", "extreme", "==", "inside_bb_value", ")", "[", "0", "]", ".", "min", "(", ")", ".", "item", "(", ")", "\n", "bb_x_max", "=", "torch", ".", "where", "(", "extreme", "==", "inside_bb_value", ")", "[", "0", "]", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "bb_y_min", "=", "torch", ".", "where", "(", "extreme", "==", "inside_bb_value", ")", "[", "1", "]", ".", "min", "(", ")", ".", "item", "(", ")", "\n", "bb_y_max", "=", "torch", ".", "where", "(", "extreme", "==", "inside_bb_value", ")", "[", "1", "]", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "bb_z_min", "=", "torch", ".", "where", "(", "extreme", "==", "inside_bb_value", ")", "[", "2", "]", ".", "min", "(", ")", ".", "item", "(", ")", "\n", "bb_z_max", "=", "torch", ".", "where", "(", "extreme", "==", "inside_bb_value", ")", "[", "2", "]", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "\n", "# Only paths within the non-relaxed bounding box are considered", "\n", "img_gradient_crop", "=", "img_gradient", "[", "bb_x_min", ":", "bb_x_max", ",", "bb_y_min", ":", "bb_y_max", ",", "bb_z_min", ":", "bb_z_max", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "prob_crop", "=", "prob", "[", ":", ",", "bb_x_min", ":", "bb_x_max", ",", "bb_y_min", ":", "bb_y_max", ",", "bb_z_min", ":", "bb_z_max", "]", ".", "detach", "(", ")", "\n", "prob_crop", "=", "torch", ".", "nn", ".", "Softmax", "(", "0", ")", "(", "prob_crop", ")", "[", "0", ",", "...", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "# Probability of the background", "\n", "\n", "# Extreme points", "\n", "ex_x_min", "=", "torch", ".", "where", "(", "extreme", "==", "1", ")", "\n", "ex_x_max", "=", "torch", ".", "where", "(", "extreme", "==", "2", ")", "\n", "ex_y_min", "=", "torch", ".", "where", "(", "extreme", "==", "3", ")", "\n", "ex_y_max", "=", "torch", ".", "where", "(", "extreme", "==", "4", ")", "\n", "ex_z_min", "=", "torch", ".", "where", "(", "extreme", "==", "5", ")", "\n", "ex_z_max", "=", "torch", ".", "where", "(", "extreme", "==", "6", ")", "\n", "\n", "# Identifying the pairs of extreme points to join (Extreme points may miss --> patch based approach)", "\n", "couples", "=", "[", "]", "\n", "if", "ex_x_min", "[", "0", "]", ".", "shape", "[", "0", "]", ">", "0", "and", "ex_x_max", "[", "0", "]", ".", "shape", "[", "0", "]", ">", "0", ":", "# Extreme points in the x dimension", "\n", "            ", "couples", ".", "append", "(", "[", "[", "k", "[", "0", "]", ".", "item", "(", ")", "for", "k", "in", "ex_x_min", "]", ",", "[", "k", "[", "0", "]", ".", "item", "(", ")", "for", "k", "in", "ex_x_max", "]", "]", ")", "\n", "\n", "", "if", "ex_y_min", "[", "0", "]", ".", "shape", "[", "0", "]", ">", "0", "and", "ex_y_max", "[", "0", "]", ".", "shape", "[", "0", "]", ">", "0", ":", "# Extreme points in the y dimension", "\n", "            ", "couples", ".", "append", "(", "[", "[", "k", "[", "0", "]", ".", "item", "(", ")", "for", "k", "in", "ex_y_min", "]", ",", "[", "k", "[", "0", "]", ".", "item", "(", ")", "for", "k", "in", "ex_y_max", "]", "]", ")", "\n", "\n", "", "if", "ex_z_min", "[", "0", "]", ".", "shape", "[", "0", "]", ">", "0", "and", "ex_z_max", "[", "0", "]", ".", "shape", "[", "0", "]", ">", "0", ":", "# Extreme points in the z dimension", "\n", "            ", "couples", ".", "append", "(", "[", "[", "k", "[", "0", "]", ".", "item", "(", ")", "for", "k", "in", "ex_z_min", "]", ",", "[", "k", "[", "0", "]", ".", "item", "(", ")", "for", "k", "in", "ex_z_max", "]", "]", ")", "\n", "\n", "", "couples_crop", "=", "[", "[", "[", "k", "[", "0", "]", "-", "bb_x_min", ",", "k", "[", "1", "]", "-", "bb_y_min", ",", "k", "[", "2", "]", "-", "bb_z_min", "]", "for", "k", "in", "couple", "]", "for", "couple", "in", "couples", "]", "\n", "\n", "# Calculating the geodesics using the dijkstra3d", "\n", "output_crop", "=", "inside_bb_value", "+", "np", ".", "zeros", "(", "img_gradient_crop", ".", "shape", ")", "\n", "for", "source", ",", "target", "in", "couples_crop", ":", "\n", "            ", "weights", "=", "img_gradient_crop", ".", "copy", "(", ")", "# Image gradient term", "\n", "\n", "if", "with_prob", ":", "\n", "                ", "weights", "+=", "prob_crop", "# Deep background probability term", "\n", "\n", "", "if", "with_euclidean", ":", "# Normalized distance map to the target", "\n", "                ", "x", ",", "y", ",", "z", "=", "np", ".", "ogrid", "[", "0", ":", "img_gradient_crop", ".", "shape", "[", "0", "]", ",", "0", ":", "img_gradient_crop", ".", "shape", "[", "1", "]", ",", "0", ":", "img_gradient_crop", ".", "shape", "[", "2", "]", "]", "\n", "distances", "=", "np", ".", "sqrt", "(", "(", "x", "-", "target", "[", "0", "]", ")", "**", "2", "+", "(", "y", "-", "target", "[", "1", "]", ")", "**", "2", "+", "(", "z", "-", "target", "[", "2", "]", ")", "**", "2", ")", "\n", "distances", "=", "normalize", "(", "distances", ")", "\n", "weights", "+=", "distances", "\n", "\n", "", "path", "=", "dijkstra3d", ".", "dijkstra", "(", "weights", ",", "source", ",", "target", ",", "connectivity", "=", "26", ")", "\n", "for", "k", "in", "path", ":", "\n", "                ", "x", ",", "y", ",", "z", "=", "k", "\n", "output_crop", "[", "x", ",", "y", ",", "z", "]", "=", "1", "\n", "\n", "\n", "", "", "output", "=", "torch", ".", "zeros", "(", "extreme", ".", "shape", ")", "\n", "output", "[", "bb_x_min", ":", "bb_x_max", ",", "bb_y_min", ":", "bb_y_max", ",", "bb_z_min", ":", "bb_z_max", "]", "=", "torch", ".", "from_numpy", "(", "output_crop", ".", "astype", "(", "int", ")", ")", "\n", "return", "output", "[", "None", ",", "None", ",", "...", "]", "#Adding batch and channel", "\n", "", "else", ":", "\n", "# No geodesics", "\n", "        ", "for", "k", "in", "range", "(", "1", ",", "7", ")", ":", "\n", "            ", "extreme", "[", "extreme", "==", "k", "]", "=", "1", "\n", "", "return", "extreme", "[", "None", ",", "None", ",", "...", "]", "#Adding batch and channel", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.focal.FocalLoss.__init__": [[32, 65], ["torch.nn.modules.loss._WeightedLoss.__init__", "monai.utils.LossReduction"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "gamma", ":", "float", "=", "2.0", ",", "\n", "weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "reduction", ":", "Union", "[", "LossReduction", ",", "str", "]", "=", "LossReduction", ".", "MEAN", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Args:\n            gamma: value of the exponent gamma in the definition of the Focal loss.\n            weight: weights to apply to the voxels of each class. If None no weights are applied.\n                This corresponds to the weights `\\alpha` in [1].\n            reduction: {``\"none\"``, ``\"mean\"``, ``\"sum\"``}\n                Specifies the reduction to apply to the output. Defaults to ``\"mean\"``.\n\n                - ``\"none\"``: no reduction will be applied.\n                - ``\"mean\"``: the sum of the output will be divided by the number of elements in the output.\n                - ``\"sum\"``: the output will be summed.\n\n        Example:\n            .. code-block:: python\n\n                import torch\n                from monai.losses import FocalLoss\n\n                pred = torch.tensor([[1, 0], [0, 1], [1, 0]], dtype=torch.float32)\n                grnd = torch.tensor([[0], [1], [0]], dtype=torch.int64)\n                fl = FocalLoss()\n                fl(pred, grnd)\n\n        \"\"\"", "\n", "super", "(", "FocalLoss", ",", "self", ")", ".", "__init__", "(", "weight", "=", "weight", ",", "reduction", "=", "LossReduction", "(", "reduction", ")", ".", "value", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.focal.FocalLoss.forward": [[66, 143], ["torch.log_softmax", "torch.log_softmax", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "ValueError", "i.unsqueeze.unsqueeze.ndimension", "t.unsqueeze.unsqueeze.ndimension", "ValueError", "ValueError", "NotImplementedError", "i.unsqueeze.unsqueeze.dim", "i.unsqueeze.unsqueeze.view", "t.unsqueeze.unsqueeze.view", "i.unsqueeze.unsqueeze.unsqueeze", "t.unsqueeze.unsqueeze.unsqueeze", "torch.squeeze.gather", "torch.squeeze.gather", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "focal.FocalLoss.weight.to", "torch.squeeze.expand", "torch.squeeze.expand", "loss.sum", "loss.mean", "i.unsqueeze.unsqueeze.size", "i.unsqueeze.unsqueeze.size", "t.unsqueeze.unsqueeze.size", "t.unsqueeze.unsqueeze.size", "t.unsqueeze.unsqueeze.long", "torch.squeeze.gather", "torch.squeeze.gather", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "t.unsqueeze.unsqueeze.size", "t.unsqueeze.unsqueeze.size", "t.unsqueeze.unsqueeze.long", "i.unsqueeze.unsqueeze.ndimension", "t.unsqueeze.unsqueeze.ndimension"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "logits", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Args:\n            logits: the shape should be BCH[WD].\n                where C (greater than 1) is the number of classes.\n                Softmax over the logits is integrated in this module for improved numerical stability.\n            target: the shape should be B1H[WD] or BCH[WD].\n                If the target's shape is B1H[WD], the target that this loss expects should be a class index\n                in the range [0, C-1] where C is the number of classes.\n\n        Raises:\n            ValueError: When ``target`` ndim differs from ``logits``.\n            ValueError: When ``target`` channel is not 1 and ``target`` shape differs from ``logits``.\n            ValueError: When ``self.reduction`` is not one of [\"mean\", \"sum\", \"none\"].\n\n        \"\"\"", "\n", "i", "=", "logits", "\n", "t", "=", "target", "\n", "\n", "if", "i", ".", "ndimension", "(", ")", "!=", "t", ".", "ndimension", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f\"logits and target ndim must match, got logits={i.ndimension()} target={t.ndimension()}.\"", ")", "\n", "\n", "", "if", "t", ".", "shape", "[", "1", "]", "!=", "1", "and", "t", ".", "shape", "[", "1", "]", "!=", "i", ".", "shape", "[", "1", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"target must have one channel or have the same shape as the logits. \"", "\n", "\"If it has one channel, it should be a class index in the range [0, C-1] \"", "\n", "f\"where C is the number of classes inferred from 'logits': C={i.shape[1]}. \"", "\n", ")", "\n", "", "if", "i", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Single-channel predictions not supported.\"", ")", "\n", "\n", "# Change the shape of logits and target to", "\n", "# num_batch x num_class x num_voxels.", "\n", "", "if", "i", ".", "dim", "(", ")", ">", "2", ":", "\n", "            ", "i", "=", "i", ".", "view", "(", "i", ".", "size", "(", "0", ")", ",", "i", ".", "size", "(", "1", ")", ",", "-", "1", ")", "# N,C,H,W => N,C,H*W", "\n", "t", "=", "t", ".", "view", "(", "t", ".", "size", "(", "0", ")", ",", "t", ".", "size", "(", "1", ")", ",", "-", "1", ")", "# N,1,H,W => N,1,H*W or N,C,H*W", "\n", "", "else", ":", "# Compatibility with classification.", "\n", "            ", "i", "=", "i", ".", "unsqueeze", "(", "2", ")", "# N,C => N,C,1", "\n", "t", "=", "t", ".", "unsqueeze", "(", "2", ")", "# N,1 => N,1,1 or N,C,1", "\n", "\n", "# Compute the log proba (more stable numerically than softmax).", "\n", "", "logpt", "=", "F", ".", "log_softmax", "(", "i", ",", "dim", "=", "1", ")", "# N,C,H*W", "\n", "# Keep only log proba values of the ground truth class for each voxel.", "\n", "if", "target", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "            ", "logpt", "=", "logpt", ".", "gather", "(", "1", ",", "t", ".", "long", "(", ")", ")", "# N,C,H*W => N,1,H*W", "\n", "logpt", "=", "torch", ".", "squeeze", "(", "logpt", ",", "dim", "=", "1", ")", "# N,1,H*W => N,H*W", "\n", "\n", "# Get the proba", "\n", "", "pt", "=", "torch", ".", "exp", "(", "logpt", ")", "# N,H*W or N,C,H*W", "\n", "\n", "if", "self", ".", "weight", "is", "not", "None", ":", "\n", "            ", "self", ".", "weight", "=", "self", ".", "weight", ".", "to", "(", "i", ")", "\n", "# Convert the weight to a map in which each voxel", "\n", "# has the weight associated with the ground-truth label", "\n", "# associated with this voxel in target.", "\n", "at", "=", "self", ".", "weight", "[", "None", ",", ":", ",", "None", "]", "# C => 1,C,1", "\n", "at", "=", "at", ".", "expand", "(", "(", "t", ".", "size", "(", "0", ")", ",", "-", "1", ",", "t", ".", "size", "(", "2", ")", ")", ")", "# 1,C,1 => N,C,H*W", "\n", "if", "target", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "                ", "at", "=", "at", ".", "gather", "(", "1", ",", "t", ".", "long", "(", ")", ")", "# selection of the weights  => N,1,H*W", "\n", "at", "=", "torch", ".", "squeeze", "(", "at", ",", "dim", "=", "1", ")", "# N,1,H*W => N,H*W", "\n", "# Multiply the log proba by their weights.", "\n", "", "logpt", "=", "logpt", "*", "at", "\n", "\n", "# Compute the loss mini-batch.", "\n", "", "weight", "=", "torch", ".", "pow", "(", "-", "pt", "+", "1.0", ",", "self", ".", "gamma", ")", "\n", "if", "target", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "            ", "loss", "=", "-", "weight", "*", "logpt", "# N", "\n", "", "else", ":", "\n", "            ", "loss", "=", "-", "weight", "*", "t", "*", "logpt", "# N,C", "\n", "\n", "", "if", "self", ".", "reduction", "==", "LossReduction", ".", "SUM", ".", "value", ":", "\n", "            ", "return", "loss", ".", "sum", "(", ")", "\n", "", "if", "self", ".", "reduction", "==", "LossReduction", ".", "NONE", ".", "value", ":", "\n", "            ", "return", "loss", "\n", "", "if", "self", ".", "reduction", "==", "LossReduction", ".", "MEAN", ".", "value", ":", "\n", "            ", "return", "loss", ".", "mean", "(", ")", "\n", "", "raise", "ValueError", "(", "f'Unsupported reduction: {self.reduction}, available options are [\"mean\", \"sum\", \"none\"].'", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.write_geodesics.main": [[11, 57], ["write_geodesics.parsing_data", "os.path.isfile", "print", "pandas.read_csv", "dict", "print", "os.path.exists", "os.makedirs", "[].tolist", "tqdm.tqdm", "dict", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.path.exists", "os.path.exists", "nibabel.load", "torch.from_numpy", "torch.from_numpy", "geodesics.generate_geodesics().numpy().squeeze", "nibabel.Nifti1Image().to_filename", "nib.load.get_fdata", "nibabel.load().get_fdata", "geodesics.generate_geodesics().numpy", "nibabel.Nifti1Image", "df_split[].isin", "nibabel.load", "geodesics.generate_geodesics"], "function", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.write_geodesics.parsing_data", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.geodesics.generate_geodesics"], ["def", "main", "(", ")", ":", "\n", "    ", "opt", "=", "parsing_data", "(", ")", "\n", "\n", "# FOLDERS", "\n", "fold_dir", "=", "opt", ".", "output_folder", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "fold_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "fold_dir", ")", "\n", "\n", "# SPLIT", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "opt", ".", "dataset_split", ")", ",", "print", "(", "\"[ERROR] Invalid split file\"", ")", "\n", "df_split", "=", "pd", ".", "read_csv", "(", "opt", ".", "dataset_split", ",", "header", "=", "None", ")", "\n", "list_file", "=", "dict", "(", ")", "\n", "for", "split", "in", "PHASES", ":", "\n", "        ", "list_file", "[", "split", "]", "=", "df_split", "[", "df_split", "[", "1", "]", ".", "isin", "(", "[", "split", "]", ")", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "\n", "", "mod_ext", "=", "\"_T2.nii.gz\"", "\n", "grad_ext", "=", "f\"_{opt.img_gradient_postfix}.nii.gz\"", "\n", "extreme_ext", "=", "f\"_{opt.label_postfix}.nii.gz\"", "\n", "paths_dict", "=", "{", "split", ":", "[", "]", "for", "split", "in", "PHASES", "}", "\n", "\n", "print", "(", "f\"Using the Euclidean distance: {opt.with_euclidean}\"", ")", "\n", "for", "split", "in", "PHASES", ":", "\n", "        ", "score", "=", "[", "]", "\n", "for", "subject", "in", "tqdm", "(", "list_file", "[", "split", "]", ")", ":", "\n", "            ", "subject_data", "=", "dict", "(", ")", "\n", "\n", "img_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_data", ",", "subject", "+", "mod_ext", ")", "\n", "img_grad_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_extremes", ",", "subject", "+", "grad_ext", ")", "\n", "lab_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "path_extremes", ",", "subject", "+", "extreme_ext", ")", "\n", "output_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "output_folder", ",", "subject", "+", "'_PartLabel.nii.gz'", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "img_path", ")", "and", "os", ".", "path", ".", "exists", "(", "lab_path", ")", "and", "os", ".", "path", ".", "exists", "(", "img_grad_path", ")", ":", "\n", "                ", "extreme", "=", "nib", ".", "load", "(", "lab_path", ")", "\n", "affine", "=", "extreme", ".", "affine", "\n", "extreme_data", "=", "torch", ".", "from_numpy", "(", "extreme", ".", "get_fdata", "(", ")", ")", "\n", "\n", "grad_data", "=", "torch", ".", "from_numpy", "(", "nib", ".", "load", "(", "img_grad_path", ")", ".", "get_fdata", "(", ")", ")", "\n", "\n", "geodesics", "=", "generate_geodesics", "(", "\n", "extreme", "=", "extreme_data", ",", "\n", "img_gradient", "=", "grad_data", ",", "\n", "prob", "=", "None", ",", "\n", "with_prob", "=", "False", ",", "\n", "with_euclidean", "=", "opt", ".", "with_euclidean", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "nib", ".", "Nifti1Image", "(", "geodesics", ",", "affine", ")", ".", "to_filename", "(", "output_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.write_geodesics.parsing_data": [[59, 100], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "", "", "", "def", "parsing_data", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Script to generate (non-deep) geodesics using extreme points\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_folder\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"geodesics_folder\"", ",", "\n", "help", "=", "\"Path to the model directory\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--dataset_split\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"splits/split_inextremis_budget1.csv\"", ",", "\n", "help", "=", "\"Path to split file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--path_data\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"../data/VS_MICCAI21/T2/\"", ",", "\n", "help", "=", "\"Path to the T2 scans\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--path_extremes\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"../data/VS_MICCAI21/extremes_manual/\"", ",", "\n", "help", "=", "\"Path to the extreme points\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--label_postfix\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"Extremes_man\"", ",", "\n", "help", "=", "\"Postfix of the Labels points\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--img_gradient_postfix\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"Sobel_man\"", ",", "\n", "help", "=", "\"Postfix of the gradient images\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--with_euclidean\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Add Euclidean distance\"", ")", "\n", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.create_logger": [[5, 22], ["os.path.exists", "os.path.join", "logging.getLogger", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger.addHandler", "logging.getLogger.addHandler", "logging.Formatter", "logging.FileHandler.setFormatter", "logging.StreamHandler.setFormatter", "logging.getLogger.setLevel", "os.path.join"], "function", ["None"], ["def", "create_logger", "(", "folder", ")", ":", "\n", "    ", "\"\"\"Create a logger to save logs.\"\"\"", "\n", "compt", "=", "0", "\n", "while", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "f\"logs_{compt}.txt\"", ")", ")", ":", "\n", "        ", "compt", "+=", "1", "\n", "", "logname", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "f\"logs_{compt}.txt\"", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "fileHandler", "=", "logging", ".", "FileHandler", "(", "logname", ",", "mode", "=", "\"w\"", ")", "\n", "consoleHandler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "logger", ".", "addHandler", "(", "fileHandler", ")", "\n", "logger", ".", "addHandler", "(", "consoleHandler", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "\"%(message)s\"", ")", "\n", "fileHandler", ".", "setFormatter", "(", "formatter", ")", "\n", "consoleHandler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.poly_lr": [[24, 27], ["None"], "function", ["None"], ["", "def", "poly_lr", "(", "epoch", ",", "max_epochs", ",", "initial_lr", ",", "exponent", "=", "0.9", ")", ":", "\n", "    ", "\"\"\"Learning rate policy used in nnUNet.\"\"\"", "\n", "return", "initial_lr", "*", "(", "1", "-", "epoch", "/", "max_epochs", ")", "**", "exponent", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.utils.infinite_iterable": [[29, 32], ["None"], "function", ["None"], ["", "def", "infinite_iterable", "(", "i", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "yield", "from", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.PartialLoss.__init__": [[7, 12], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "criterion", ")", ":", "\n", "        ", "super", "(", "PartialLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "criterion", "=", "criterion", "\n", "self", ".", "nb_classes", "=", "self", ".", "criterion", ".", "nb_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.PartialLoss.forward": [[13, 34], ["range", "partial_target[].reshape", "outputs[].reshape().unsqueeze", "outputs_i.reshape.reshape.reshape", "partial_i[].reshape", "losses.PartialLoss.criterion", "outputs[].reshape", "partial_i[].reshape.type"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape"], ["", "def", "forward", "(", "self", ",", "outputs", ",", "partial_target", ",", "phase", "=", "'training'", ")", ":", "\n", "        ", "nb_target", "=", "outputs", ".", "shape", "[", "0", "]", "\n", "loss_target", "=", "0.0", "\n", "total", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "nb_target", ")", ":", "\n", "            ", "partial_i", "=", "partial_target", "[", "i", ",", "...", "]", ".", "reshape", "(", "-", "1", ")", "\n", "outputs_i", "=", "outputs", "[", "i", ",", "...", "]", ".", "reshape", "(", "self", ".", "nb_classes", ",", "-", "1", ")", ".", "unsqueeze", "(", "0", ")", "\n", "outputs_i", "=", "outputs_i", "[", ":", ",", ":", ",", "partial_i", "<", "self", ".", "nb_classes", "]", "\n", "\n", "nb_annotated", "=", "outputs_i", ".", "shape", "[", "-", "1", "]", "\n", "if", "nb_annotated", ">", "0", ":", "\n", "                ", "outputs_i", "=", "outputs_i", ".", "reshape", "(", "1", ",", "self", ".", "nb_classes", ",", "1", ",", "1", ",", "nb_annotated", ")", "# Reshape to a 5D tensor", "\n", "partial_i", "=", "partial_i", "[", "partial_i", "<", "self", ".", "nb_classes", "]", ".", "reshape", "(", "1", ",", "1", ",", "1", ",", "1", ",", "nb_annotated", ")", "# Reshape to a 5D tensor", "\n", "loss_target", "+=", "self", ".", "criterion", "(", "outputs_i", ",", "partial_i", ".", "type", "(", "torch", ".", "cuda", ".", "IntTensor", ")", ",", "phase", ")", "\n", "total", "+=", "1", "\n", "\n", "", "", "if", "total", ">", "0", ":", "\n", "            ", "return", "loss_target", "/", "total", "\n", "", "else", ":", "\n", "            ", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.__init__": [[37, 42], ["torch.nn.Module.__init__", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nb_classes", ")", ":", "\n", "        ", "super", "(", "DC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "1", ")", "\n", "self", ".", "nb_classes", "=", "nb_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.onehot": [[43, 51], ["gt.long.long.long", "torch.zeros", "y_onehot.cuda.cuda.cuda", "y_onehot.cuda.cuda.scatter_"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "onehot", "(", "gt", ",", "shape", ")", ":", "\n", "        ", "shp_y", "=", "gt", ".", "shape", "\n", "gt", "=", "gt", ".", "long", "(", ")", "\n", "y_onehot", "=", "torch", ".", "zeros", "(", "shape", ")", "\n", "y_onehot", "=", "y_onehot", ".", "cuda", "(", ")", "\n", "y_onehot", ".", "scatter_", "(", "1", ",", "gt", ",", "1", ")", "\n", "return", "y_onehot", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape": [[52, 62], ["losses.DC.permute", "output.permute.permute.permute", "print", "all", "losses.DC.onehot", "zip"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.onehot"], ["", "def", "reshape", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "batch_size", "=", "output", ".", "shape", "[", "0", "]", "\n", "\n", "if", "not", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "output", ".", "shape", ",", "target", ".", "shape", ")", "]", ")", ":", "\n", "            ", "target", "=", "self", ".", "onehot", "(", "target", ",", "output", ".", "shape", ")", "\n", "\n", "", "target", "=", "target", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", "\n", "output", "=", "output", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", "\n", "print", "(", "target", ".", "shape", ",", "output", ".", "shape", ")", "\n", "return", "output", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.dice": [[64, 76], ["losses.DC.softmax", "list", "torch.sum", "all", "losses.DC.onehot", "range", "dice.mean", "len", "torch.sum", "torch.sum", "zip"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.onehot"], ["", "def", "dice", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "output", "=", "self", ".", "softmax", "(", "output", ")", "\n", "if", "not", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "output", ".", "shape", ",", "target", ".", "shape", ")", "]", ")", ":", "\n", "            ", "target", "=", "self", ".", "onehot", "(", "target", ",", "output", ".", "shape", ")", "\n", "\n", "", "sum_axis", "=", "list", "(", "range", "(", "2", ",", "len", "(", "target", ".", "shape", ")", ")", ")", "\n", "\n", "s", "=", "(", "10e-20", ")", "\n", "intersect", "=", "torch", ".", "sum", "(", "output", "*", "target", ",", "sum_axis", ")", "\n", "dice", "=", "(", "2", "*", "intersect", ")", "/", "(", "torch", ".", "sum", "(", "output", ",", "sum_axis", ")", "+", "torch", ".", "sum", "(", "target", ",", "sum_axis", ")", "+", "s", ")", "\n", "#dice shape is (batch_size, nb_classes)", "\n", "return", "1.0", "-", "dice", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.forward": [[78, 81], ["losses.DC.dice"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.dice"], ["", "def", "forward", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "result", "=", "self", ".", "dice", "(", "output", ",", "target", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__": [[84, 89], ["losses.DC.__init__", "torch.nn.CrossEntropyLoss", "utilities.focal.FocalLoss"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nb_classes", ")", ":", "\n", "        ", "super", "(", "DC_CE_Focal", ",", "self", ")", ".", "__init__", "(", "nb_classes", ")", "\n", "\n", "self", ".", "ce", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'mean'", ")", "\n", "self", ".", "fl", "=", "FocalLoss", "(", "reduction", "=", "\"none\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.focal": [[90, 108], ["losses.DC_CE_Focal.fl().reshape", "range", "losses.DC_CE_Focal.mean", "losses.DC_CE_Focal.fl", "score[].mean", "grnd.reshape"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.reshape"], ["", "def", "focal", "(", "self", ",", "pred", ",", "grnd", ",", "phase", "=", "\"training\"", ")", ":", "\n", "        ", "score", "=", "self", ".", "fl", "(", "pred", ",", "grnd", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n", "if", "phase", "==", "\"training\"", ":", "# class-balanced focal loss", "\n", "            ", "output", "=", "0.0", "\n", "nb_classes", "=", "0", "\n", "for", "cl", "in", "range", "(", "self", ".", "nb_classes", ")", ":", "\n", "                ", "if", "(", "grnd", "==", "cl", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ">", "0", ":", "\n", "                    ", "output", "+=", "score", "[", "grnd", ".", "reshape", "(", "-", "1", ")", "==", "cl", "]", ".", "mean", "(", ")", "\n", "nb_classes", "+=", "1", "\n", "\n", "", "", "if", "nb_classes", ">", "0", ":", "\n", "                ", "return", "output", "/", "nb_classes", "\n", "", "else", ":", "\n", "                ", "return", "0.0", "\n", "\n", "", "", "else", ":", "# class-balanced focal loss", "\n", "            ", "return", "score", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.forward": [[109, 123], ["losses.DC_CE_Focal.dice", "losses.DC_CE_Focal.focal", "output.permute().contiguous().view.permute().contiguous().view.permute().contiguous().view", "target.view().long().cuda.view().long().cuda.view().long().cuda", "losses.DC_CE_Focal.ce", "output.permute().contiguous().view.permute().contiguous().view.permute().contiguous", "target.view().long().cuda.view().long().cuda.view().long", "output.permute().contiguous().view.permute().contiguous().view.permute", "target.view().long().cuda.view().long().cuda.view"], "methods", ["home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC.dice", "home.repos.pwc.inspect_result.ReubenDo_InExtremIS.utilities.losses.DC_CE_Focal.focal"], ["", "", "def", "forward", "(", "self", ",", "output", ",", "target", ",", "phase", "=", "\"training\"", ")", ":", "\n", "# Dice term", "\n", "        ", "dc_loss", "=", "self", ".", "dice", "(", "output", ",", "target", ")", "\n", "\n", "# Focal term", "\n", "focal_loss", "=", "self", ".", "focal", "(", "output", ",", "target", ",", "phase", ")", "\n", "\n", "# Cross entropy", "\n", "output", "=", "output", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "nb_classes", ")", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ",", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "ce_loss", "=", "self", ".", "ce", "(", "output", ",", "target", ")", "\n", "\n", "result", "=", "ce_loss", "+", "dc_loss", "+", "focal_loss", "\n", "return", "result", "\n", "\n"]]}