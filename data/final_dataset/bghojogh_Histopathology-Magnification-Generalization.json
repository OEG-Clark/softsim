{"home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.main": [[113, 176], ["domain_list.copy", "domain_list.remove", "int", "deep_all.DeepAll.construct_model_train", "deep_all.DeepAll.construct_model_test", "tensorflow.summary.merge_all", "tensorflow.train.Saver", "tensorflow.InteractiveSession", "tensorflow.global_variables_initializer().run", "tensorflow.train.start_queue_runners", "print", "deep_all.DeepAll.load_initial_weights", "os.path.exists", "os.makedirs", "masf_func.MASF", "deep_all.DeepAll", "tensorflow.get_collection", "main.load_network_model", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "main.evaluation_after_training", "str", "str", "tensorflow.global_variables_initializer", "main.train_MASF", "main.train_DeepAll", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_model_train", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_model_test", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.load_initial_weights", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.load_network_model", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_after_training", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.train_MASF", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.train_DeepAll"], ["def", "main", "(", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "FLAGS", ".", "logdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "FLAGS", ".", "logdir", ")", "\n", "\n", "", "if", "FLAGS", ".", "dataset", "==", "\"pacs\"", ":", "\n", "        ", "domain_list", "=", "[", "'art_painting'", ",", "'cartoon'", ",", "'photo'", ",", "'sketch'", "]", "\n", "", "elif", "FLAGS", ".", "dataset", "==", "\"pathology\"", "or", "FLAGS", ".", "dataset", "==", "\"pathology_binary\"", ":", "\n", "        ", "domain_list", "=", "[", "'40'", ",", "'100'", ",", "'200'", ",", "'400'", "]", "\n", "", "all_domains_list", "=", "domain_list", ".", "copy", "(", ")", "\n", "domain_list", ".", "remove", "(", "FLAGS", ".", "target_domain", ")", "\n", "source_domains_list", "=", "domain_list", "\n", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "exp_string", "=", "'MASF_'", "+", "FLAGS", ".", "target_domain", "+", "'.mbs_'", "+", "str", "(", "FLAGS", ".", "meta_batch_size", ")", "+", "'.inner'", "+", "str", "(", "FLAGS", ".", "inner_lr", ")", "+", "'.outer'", "+", "str", "(", "FLAGS", ".", "outer_lr", ")", "+", "'.clipNorm'", "+", "str", "(", "\n", "FLAGS", ".", "gradients_clip_value", ")", "+", "'.metric'", "+", "str", "(", "FLAGS", ".", "metric_lr", ")", "+", "'.margin'", "+", "str", "(", "FLAGS", ".", "margin", ")", "\n", "", "else", ":", "\n", "        ", "exp_string", "=", "'DeepAll_'", "+", "FLAGS", ".", "target_domain", "+", "'.mbs_'", "+", "str", "(", "FLAGS", ".", "deep_all_batch_size", ")", "+", "'.lr'", "+", "str", "(", "FLAGS", ".", "deep_all_lr", ")", "+", "'.margin'", "+", "str", "(", "FLAGS", ".", "margin", ")", "\n", "\n", "# Constructing model", "\n", "", "FLAGS", ".", "feature_space_dimension", "=", "int", "(", "FLAGS", ".", "feature_space_dimension", ")", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "model", "=", "MASF", "(", "FLAGS", ".", "WEIGHTS_PATH", ",", "FLAGS", ".", "feature_space_dimension", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DeepAll", "(", "FLAGS", ".", "WEIGHTS_PATH", ",", "FLAGS", ".", "feature_space_dimension", ")", "\n", "", "model", ".", "construct_model_train", "(", ")", "\n", "model", ".", "construct_model_test", "(", ")", "\n", "\n", "model", ".", "summ_op", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ")", ",", "\n", "max_to_keep", "=", "None", ")", "# https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver", "\n", "sess", "=", "tf", ".", "InteractiveSession", "(", ")", "\n", "\n", "tf", ".", "global_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "tf", ".", "train", ".", "start_queue_runners", "(", ")", "\n", "\n", "print", "(", "'Loading pretrained weights'", ")", "\n", "model", ".", "load_initial_weights", "(", "sess", ")", "\n", "\n", "resume_itr", "=", "0", "\n", "if", "FLAGS", ".", "resume", "or", "not", "FLAGS", ".", "train", ":", "\n", "        ", "checkpoint_dir", "=", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", "+", "'/'", "+", "str", "(", "\n", "FLAGS", ".", "which_checkpoint_to_load", ")", "+", "'/'", "\n", "load_network_model", "(", "saver_", "=", "saver", ",", "session_", "=", "sess", ",", "checkpoint_dir", "=", "checkpoint_dir", ")", "\n", "resume_itr", "=", "FLAGS", ".", "which_checkpoint_to_load", "+", "1", "\n", "\n", "", "train_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "source_domain", "+", "'_train_kfold.txt'", ")", "for", "source_domain", "in", "\n", "source_domains_list", "]", "\n", "test_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "FLAGS", ".", "target_domain", "+", "'_test_kfold.txt'", ")", "]", "\n", "val_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "FLAGS", ".", "target_domain", "+", "'_crossval_kfold.txt'", ")", "]", "\n", "evaluation_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "domain_", "+", "'_test_kfold.txt'", ")", "for", "domain_", "in", "\n", "all_domains_list", "]", "\n", "\n", "if", "FLAGS", ".", "train", ":", "\n", "        ", "if", "FLAGS", ".", "masf_mode", ":", "\n", "            ", "train_MASF", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file_list", "[", "0", "]", ",", "val_file_list", "[", "0", "]", ",", "resume_itr", ")", "\n", "", "else", ":", "\n", "            ", "train_DeepAll", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file_list", "[", "0", "]", ",", "val_file_list", "[", "0", "]", ",", "\n", "resume_itr", ")", "\n", "", "", "else", ":", "\n", "        ", "evaluation_after_training", "(", "model", ",", "sess", ",", "evaluation_file_list", ",", "evaluation_batch_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.train_MASF": [[180, 329], ["range", "tf.data.Iterator.from_structure.make_initializer", "int", "tf.data.Iterator.from_structure.make_initializer", "int", "range", "tensorflow.summary.FileWriter", "tensorflow.device", "range", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "len", "training_init_op.append", "train_batches_per_epoch.append", "numpy.floor", "numpy.floor", "len", "numpy.random.permutation", "range", "range", "range", "numpy.unique", "numpy.unique", "range", "numpy.unique", "numpy.unique", "range", "int", "numpy.concatenate", "numpy.concatenate", "numpy.sum", "numpy.argmax", "output_tensors.extend", "sess.run", "source_losses.append", "source_accuracies.append", "len", "data_generator.ImageDataGenerator", "tr_data_list.append", "train_iterator_list.append", "train_next_list.append", "train_iterator_list[].make_initializer", "int", "len", "numpy.argmax", "numpy.argmax", "numpy.argmax", "numpy.argmax", "print", "print", "print", "print", "print", "print", "os.path.join", "tf.summary.FileWriter.add_summary", "main.save_network_model", "main.evaluation_during_training", "main.evaluation_during_training", "tensorflow.data.Iterator.from_structure", "train_iterator_list[].get_next", "numpy.floor", "sess.run", "sess.run", "sess.run", "RuntimeError", "os.path.exists", "os.makedirs", "open", "fle.write", "sess.run", "RuntimeError", "str", "str", "str", "numpy.mean", "numpy.mean", "str", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.save_network_model", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training"], ["", "", "def", "train_MASF", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file", ",", "val_file", ",", "resume_itr", "=", "0", ")", ":", "\n", "    ", "if", "FLAGS", ".", "log", ":", "\n", "        ", "train_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", ",", "sess", ".", "graph", ")", "\n", "", "source_losses", ",", "target_losses", ",", "source_accuracies", ",", "target_accuracies", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Data loaders", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "        ", "tr_data_list", ",", "train_iterator_list", ",", "train_next_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "tr_data", "=", "ImageDataGenerator", "(", "train_file_list", "[", "i", "]", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'training'", ",", "batch_size", "=", "FLAGS", ".", "meta_batch_size", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "True", ")", "\n", "tr_data_list", ".", "append", "(", "tr_data", ")", "\n", "train_iterator_list", ".", "append", "(", "\n", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "tr_data", ".", "data", ".", "output_types", ",", "tr_data", ".", "data", ".", "output_shapes", ")", ")", "\n", "train_next_list", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "get_next", "(", ")", ")", "\n", "\n", "", "test_data", "=", "ImageDataGenerator", "(", "test_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "test_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "test_data", ".", "data", ".", "output_types", ",", "test_data", ".", "data", ".", "output_shapes", ")", "\n", "test_next_batch", "=", "test_iterator", ".", "get_next", "(", ")", "\n", "\n", "val_data", "=", "ImageDataGenerator", "(", "val_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "val_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "val_data", ".", "data", ".", "output_types", ",", "val_data", ".", "data", ".", "output_shapes", ")", "\n", "val_next_batch", "=", "val_iterator", ".", "get_next", "(", ")", "\n", "\n", "# Ops for initializing different iterators", "\n", "", "training_init_op", "=", "[", "]", "\n", "train_batches_per_epoch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "        ", "training_init_op", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "make_initializer", "(", "tr_data_list", "[", "i", "]", ".", "data", ")", ")", "\n", "train_batches_per_epoch", ".", "append", "(", "int", "(", "np", ".", "floor", "(", "tr_data_list", "[", "i", "]", ".", "data_size", "/", "FLAGS", ".", "meta_batch_size", ")", ")", ")", "\n", "\n", "", "test_init_op", "=", "test_iterator", ".", "make_initializer", "(", "test_data", ".", "data", ")", "\n", "test_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "test_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "val_init_op", "=", "val_iterator", ".", "make_initializer", "(", "val_data", ".", "data", ")", "\n", "val_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "val_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "# Training begins", "\n", "best_test_acc", ",", "best_val_acc", "=", "0", ",", "0", "\n", "for", "itr", "in", "range", "(", "resume_itr", ",", "FLAGS", ".", "train_iterations", ")", ":", "\n", "\n", "# Sampling training and test tasks", "\n", "        ", "num_training_tasks", "=", "len", "(", "train_file_list", ")", "\n", "num_meta_train", "=", "num_training_tasks", "-", "1", "\n", "num_meta_test", "=", "num_training_tasks", "-", "num_meta_train", "# as setting num_meta_test = 1", "\n", "\n", "# Randomly choosing meta train and meta test domains", "\n", "task_list", "=", "np", ".", "random", ".", "permutation", "(", "num_training_tasks", ")", "\n", "meta_train_index_list", "=", "task_list", "[", ":", "num_meta_train", "]", "\n", "meta_test_index_list", "=", "task_list", "[", "num_meta_train", ":", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "if", "itr", "%", "train_batches_per_epoch", "[", "i", "]", "==", "0", "or", "resume_itr", "!=", "0", ":", "\n", "                ", "sess", ".", "run", "(", "training_init_op", "[", "i", "]", ")", "# initialize training sample generator at itr=0", "\n", "\n", "# Sampling meta-train, meta-test data", "\n", "", "", "for", "i", "in", "range", "(", "num_meta_train", ")", ":", "\n", "            ", "task_ind", "=", "meta_train_index_list", "[", "i", "]", "\n", "if", "i", "==", "0", ":", "\n", "                ", "inputa", ",", "labela", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "elif", "i", "==", "1", ":", "\n", "                ", "inputa1", ",", "labela1", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "'check number of meta-train domains.'", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "num_meta_test", ")", ":", "\n", "            ", "task_ind", "=", "meta_test_index_list", "[", "i", "]", "\n", "if", "i", "==", "0", ":", "\n", "                ", "inputb", ",", "labelb", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "'check number of meta-test domains.'", ")", "\n", "\n", "# to avoid a certain un-sampled class affect stability of global class alignment", "\n", "# i.e., mask-out the un-sampled class from computing kd-loss", "\n", "", "", "sampledb", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labelb", ",", "axis", "=", "1", ")", ")", "\n", "sampleda", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labela", ",", "axis", "=", "1", ")", ")", "\n", "bool_indicator_b_a", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "for", "i", "in", "range", "(", "FLAGS", ".", "num_classes", ")", ":", "\n", "# only count class that are sampled in both source domains", "\n", "            ", "if", "(", "i", "in", "sampledb", ")", "and", "(", "i", "in", "sampleda", ")", ":", "\n", "                ", "bool_indicator_b_a", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "sampledb", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labelb", ",", "axis", "=", "1", ")", ")", "\n", "sampleda1", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labela1", ",", "axis", "=", "1", ")", ")", "\n", "bool_indicator_b_a1", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "for", "i", "in", "range", "(", "FLAGS", ".", "num_classes", ")", ":", "\n", "            ", "if", "(", "i", "in", "sampledb", ")", "and", "(", "i", "in", "sampleda1", ")", ":", "\n", "                ", "bool_indicator_b_a1", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "part", "=", "FLAGS", ".", "meta_batch_size", "/", "3", "\n", "part", "=", "int", "(", "part", ")", "\n", "input_group", "=", "np", ".", "concatenate", "(", "(", "inputa", "[", ":", "part", "]", ",", "inputa1", "[", ":", "part", "]", ",", "inputb", "[", ":", "part", "]", ")", ",", "axis", "=", "0", ")", "\n", "label_group", "=", "np", ".", "concatenate", "(", "(", "labela", "[", ":", "part", "]", ",", "labela1", "[", ":", "part", "]", ",", "labelb", "[", ":", "part", "]", ")", ",", "axis", "=", "0", ")", "\n", "group_list", "=", "np", ".", "sum", "(", "label_group", ",", "axis", "=", "0", ")", "\n", "label_group", "=", "np", ".", "argmax", "(", "label_group", ",", "axis", "=", "1", ")", "# transform one-hot labels into class-wise integer", "\n", "\n", "feed_dict", "=", "{", "model", ".", "inputa", ":", "inputa", ",", "model", ".", "labela", ":", "labela", ",", "model", ".", "inputa1", ":", "inputa1", ",", "model", ".", "labela1", ":", "labela1", ",", "model", ".", "inputb", ":", "inputb", ",", "model", ".", "labelb", ":", "labelb", ",", "model", ".", "input_group", ":", "input_group", ",", "model", ".", "label_group", ":", "label_group", ",", "model", ".", "bool_indicator_b_a", ":", "bool_indicator_b_a", ",", "model", ".", "bool_indicator_b_a1", ":", "bool_indicator_b_a1", ",", "\n", "model", ".", "KEEP_PROB", ":", "0.5", "}", "\n", "\n", "output_tensors", "=", "[", "model", ".", "task_train_op", ",", "model", ".", "meta_train_op", ",", "model", ".", "metric_train_op", "]", "\n", "output_tensors", ".", "extend", "(", "\n", "[", "model", ".", "summ_op", ",", "model", ".", "global_loss", ",", "model", ".", "source_loss", ",", "model", ".", "source_accuracy", ",", "model", ".", "metric_loss", "]", ")", "\n", "_", ",", "_", ",", "_", ",", "summ_writer", ",", "global_loss", ",", "source_loss", ",", "source_accuracy", ",", "metric_loss", "=", "sess", ".", "run", "(", "output_tensors", ",", "\n", "feed_dict", ")", "\n", "\n", "source_losses", ".", "append", "(", "source_loss", ")", "\n", "source_accuracies", ".", "append", "(", "source_accuracy", ")", "\n", "\n", "if", "itr", "%", "FLAGS", ".", "print_interval", "==", "0", ":", "\n", "            ", "print", "(", "'---'", "*", "10", "+", "'\\n%s'", "%", "exp_string", ")", "\n", "print", "(", "'number of samples per category:'", ",", "group_list", ")", "\n", "print", "(", "'global loss: %.7f'", "%", "global_loss", ")", "\n", "print", "(", "'metric loss: %.7f '", "%", "metric_loss", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Loss '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_losses", ")", ")", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Accuracy '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "# log loss and accuracy:", "\n", "path_save_train_acc", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "FLAGS", ".", "dataset", ",", "exp_string", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save_train_acc", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "path_save_train_acc", ")", "\n", "", "with", "open", "(", "path_save_train_acc", "+", "'/eva_'", "+", "'train'", "+", "'.txt'", ",", "'a'", ")", "as", "fle", ":", "\n", "                ", "fle", ".", "write", "(", "\n", "'Train results: Iteration %d, global loss: %.7f, metric loss: %.7f, Loss: %f, Accuracy: %f \\n'", "%", "(", "\n", "itr", ",", "global_loss", ",", "metric_loss", ",", "np", ".", "mean", "(", "source_losses", ")", ",", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "", "source_losses", ",", "target_losses", "=", "[", "]", ",", "[", "]", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "summary_interval", "==", "0", "and", "FLAGS", ".", "log", ":", "\n", "            ", "train_writer", ".", "add_summary", "(", "summ_writer", ",", "itr", ")", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "save_interval", "==", "0", ":", "\n", "# saver.save(sess, FLAGS.logdir + '/' + FLAGS.dataset + '/' + exp_string + '/model' + str(itr))", "\n", "            ", "checkpoint_dir", "=", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", "+", "\"/\"", "+", "str", "(", "itr", ")", "+", "\"/\"", "\n", "save_network_model", "(", "saver_", "=", "saver", ",", "session_", "=", "sess", ",", "checkpoint_dir", "=", "checkpoint_dir", ",", "\n", "model_name", "=", "\"model_itr\"", "+", "str", "(", "itr", ")", ")", "\n", "\n", "# Testing periodically:", "\n", "", "if", "itr", "%", "FLAGS", ".", "val_print_interval", "==", "0", ":", "\n", "            ", "val_acc", ",", "best_val_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "val_batches_per_epoch", ",", "\n", "val_init_op", ",", "val_next_batch", ",", "itr", ",", "best_val_acc", ",", "saver", ",", "\n", "is_val", "=", "True", ")", "\n", "", "if", "itr", "%", "FLAGS", ".", "test_print_interval", "==", "0", ":", "\n", "            ", "test_acc", ",", "best_test_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "test_batches_per_epoch", ",", "\n", "test_init_op", ",", "test_next_batch", ",", "itr", ",", "best_test_acc", ",", "\n", "saver", ",", "is_val", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.train_DeepAll": [[331, 434], ["range", "tf.data.Iterator.from_structure.make_initializer", "int", "tf.data.Iterator.from_structure.make_initializer", "int", "range", "tensorflow.summary.FileWriter", "tensorflow.device", "range", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "len", "training_init_op.append", "train_batches_per_epoch.append", "numpy.floor", "numpy.floor", "len", "range", "range", "output_tensors.extend", "sess.run", "source_losses.append", "source_accuracies.append", "len", "data_generator.ImageDataGenerator", "tr_data_list.append", "train_iterator_list.append", "train_next_list.append", "train_iterator_list[].make_initializer", "int", "len", "print", "print", "print", "os.path.join", "tf.summary.FileWriter.add_summary", "main.save_network_model", "main.evaluation_during_training", "main.evaluation_during_training", "tensorflow.data.Iterator.from_structure", "train_iterator_list[].get_next", "numpy.floor", "sess.run", "sess.run", "os.path.exists", "os.makedirs", "open", "fle.write", "sess.run", "sess.run", "str", "str", "str", "numpy.mean", "numpy.mean", "str", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.save_network_model", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training"], ["", "", "", "def", "train_DeepAll", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file", ",", "val_file", ",", "resume_itr", "=", "0", ")", ":", "\n", "    ", "if", "FLAGS", ".", "log", ":", "\n", "        ", "train_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", ",", "sess", ".", "graph", ")", "\n", "", "source_losses", ",", "target_losses", ",", "source_accuracies", ",", "target_accuracies", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Data loaders", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "        ", "tr_data_list", ",", "train_iterator_list", ",", "train_next_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "tr_data", "=", "ImageDataGenerator", "(", "train_file_list", "[", "i", "]", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'training'", ",", "batch_size", "=", "FLAGS", ".", "deep_all_batch_size", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "\n", "shuffle", "=", "True", ")", "\n", "tr_data_list", ".", "append", "(", "tr_data", ")", "\n", "train_iterator_list", ".", "append", "(", "\n", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "tr_data", ".", "data", ".", "output_types", ",", "tr_data", ".", "data", ".", "output_shapes", ")", ")", "\n", "train_next_list", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "get_next", "(", ")", ")", "\n", "\n", "", "test_data", "=", "ImageDataGenerator", "(", "test_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "test_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "test_data", ".", "data", ".", "output_types", ",", "test_data", ".", "data", ".", "output_shapes", ")", "\n", "test_next_batch", "=", "test_iterator", ".", "get_next", "(", ")", "\n", "\n", "val_data", "=", "ImageDataGenerator", "(", "val_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "val_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "val_data", ".", "data", ".", "output_types", ",", "val_data", ".", "data", ".", "output_shapes", ")", "\n", "val_next_batch", "=", "val_iterator", ".", "get_next", "(", ")", "\n", "\n", "# Ops for initializing different iterators", "\n", "", "training_init_op", "=", "[", "]", "\n", "train_batches_per_epoch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "        ", "training_init_op", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "make_initializer", "(", "tr_data_list", "[", "i", "]", ".", "data", ")", ")", "\n", "train_batches_per_epoch", ".", "append", "(", "int", "(", "np", ".", "floor", "(", "tr_data_list", "[", "i", "]", ".", "data_size", "/", "FLAGS", ".", "deep_all_batch_size", ")", ")", ")", "\n", "\n", "", "test_init_op", "=", "test_iterator", ".", "make_initializer", "(", "test_data", ".", "data", ")", "\n", "test_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "test_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "val_init_op", "=", "val_iterator", ".", "make_initializer", "(", "val_data", ".", "data", ")", "\n", "val_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "val_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "# Training begins", "\n", "best_test_acc", ",", "best_val_acc", "=", "0", ",", "0", "\n", "for", "itr", "in", "range", "(", "resume_itr", ",", "FLAGS", ".", "train_iterations", ")", ":", "\n", "\n", "# Sampling training and test tasks", "\n", "        ", "num_training_tasks", "=", "len", "(", "train_file_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "if", "itr", "%", "train_batches_per_epoch", "[", "i", "]", "==", "0", "or", "resume_itr", "!=", "0", ":", "\n", "                ", "sess", ".", "run", "(", "training_init_op", "[", "i", "]", ")", "# initialize training sample generator at itr=0", "\n", "\n", "# Sampling", "\n", "", "", "for", "i", "in", "range", "(", "num_training_tasks", ")", ":", "\n", "            ", "task_ind", "=", "i", "\n", "if", "i", "==", "0", ":", "\n", "                ", "inputa", ",", "labela", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "elif", "i", "==", "1", ":", "\n", "                ", "inputa1", ",", "labela1", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "inputb", ",", "labelb", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "", "feed_dict", "=", "{", "model", ".", "inputa", ":", "inputa", ",", "model", ".", "labela", ":", "labela", ",", "model", ".", "inputa1", ":", "inputa1", ",", "model", ".", "labela1", ":", "labela1", ",", "model", ".", "inputb", ":", "inputb", ",", "model", ".", "labelb", ":", "labelb", ",", "model", ".", "KEEP_PROB", ":", "0.5", "}", "\n", "\n", "output_tensors", "=", "[", "model", ".", "task_train_op", "]", "\n", "output_tensors", ".", "extend", "(", "[", "model", ".", "summ_op", ",", "model", ".", "source_loss", ",", "model", ".", "source_accuracy", "]", ")", "\n", "_", ",", "summ_writer", ",", "source_loss", ",", "source_accuracy", "=", "sess", ".", "run", "(", "output_tensors", ",", "feed_dict", ")", "\n", "\n", "source_losses", ".", "append", "(", "source_loss", ")", "\n", "source_accuracies", ".", "append", "(", "source_accuracy", ")", "\n", "\n", "if", "itr", "%", "FLAGS", ".", "print_interval", "==", "0", ":", "\n", "            ", "print", "(", "'---'", "*", "10", "+", "'\\n%s'", "%", "exp_string", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Loss '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_losses", ")", ")", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Accuracy '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "# log loss and accuracy:", "\n", "path_save_train_acc", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "FLAGS", ".", "dataset", ",", "exp_string", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save_train_acc", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "path_save_train_acc", ")", "\n", "", "with", "open", "(", "path_save_train_acc", "+", "'/eva_'", "+", "'train'", "+", "'.txt'", ",", "'a'", ")", "as", "fle", ":", "\n", "                ", "fle", ".", "write", "(", "'Train results: Iteration %d, Loss: %f, Accuracy: %f \\n'", "%", "(", "\n", "itr", ",", "np", ".", "mean", "(", "source_losses", ")", ",", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "", "source_losses", ",", "target_losses", "=", "[", "]", ",", "[", "]", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "summary_interval", "==", "0", "and", "FLAGS", ".", "log", ":", "\n", "            ", "train_writer", ".", "add_summary", "(", "summ_writer", ",", "itr", ")", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "save_interval", "==", "0", ":", "\n", "# saver.save(sess, FLAGS.logdir + '/' + FLAGS.dataset + '/' + exp_string + '/model' + str(itr))", "\n", "            ", "checkpoint_dir", "=", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", "+", "\"/\"", "+", "str", "(", "itr", ")", "+", "\"/\"", "\n", "save_network_model", "(", "saver_", "=", "saver", ",", "session_", "=", "sess", ",", "checkpoint_dir", "=", "checkpoint_dir", ",", "\n", "model_name", "=", "\"model_itr\"", "+", "str", "(", "itr", ")", ")", "\n", "\n", "# Testing periodically", "\n", "", "if", "itr", "%", "FLAGS", ".", "val_print_interval", "==", "0", ":", "\n", "            ", "val_acc", ",", "best_val_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "val_batches_per_epoch", ",", "\n", "val_init_op", ",", "val_next_batch", ",", "itr", ",", "best_val_acc", ",", "saver", ",", "\n", "is_val", "=", "True", ")", "\n", "", "if", "itr", "%", "FLAGS", ".", "test_print_interval", "==", "0", ":", "\n", "            ", "test_acc", ",", "best_test_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "test_batches_per_epoch", ",", "\n", "test_init_op", ",", "test_next_batch", ",", "itr", ",", "best_test_acc", ",", "\n", "saver", ",", "is_val", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.evaluation_during_training": [[436, 496], ["numpy.zeros", "numpy.zeros", "sess.run", "range", "print", "main.plot_embedding_of_points", "matplotlib.savefig", "matplotlib.clf", "matplotlib.close", "sess.run", "sess.run", "print", "print", "os.path.exists", "os.makedirs", "open", "fle.write", "numpy.argmax", "numpy.argmax", "os.path.join", "str"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.plot_embedding_of_points"], ["", "", "", "def", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "test_batches_per_epoch", ",", "\n", "test_init_op", ",", "test_next_batch", ",", "itr", ",", "best_test_acc", ",", "saver", ",", "is_val", "=", "True", ")", ":", "\n", "    ", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "method_", "=", "\"MASF\"", "\n", "", "else", ":", "\n", "        ", "method_", "=", "\"DeepAll\"", "\n", "", "if", "is_val", ":", "\n", "        ", "case_", "=", "\"val\"", "\n", "", "else", ":", "\n", "        ", "case_", "=", "\"test\"", "\n", "", "class_accs", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "class_samples", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "test_embeddings", "=", "np", ".", "zeros", "(", "(", "test_batches_per_epoch", ",", "FLAGS", ".", "feature_space_dimension", ")", ")", "\n", "test_labels", "=", "np", ".", "zeros", "(", "(", "test_batches_per_epoch", ",", ")", ")", "\n", "test_acc", ",", "test_loss", ",", "test_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "sess", ".", "run", "(", "test_init_op", ")", "# initialize testing data generator", "\n", "for", "it", "in", "range", "(", "test_batches_per_epoch", ")", ":", "\n", "        ", "test_input", ",", "test_label", "=", "sess", ".", "run", "(", "test_next_batch", ")", "\n", "feed_dict", "=", "{", "model", ".", "test_input", ":", "test_input", ",", "model", ".", "test_label", ":", "test_label", ",", "model", ".", "KEEP_PROB", ":", "1.", "}", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "            ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "semantic_feature", ",", "model", ".", "outputs", ",", "\n", "model", ".", "metric_embedding", "]", "\n", "", "else", ":", "\n", "            ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "embedding_feature", ",", "model", ".", "outputs", "]", "\n", "# --> model.semantic_feature: 4096-dimensional embedding with psi weights (for semantic features)", "\n", "# --> model.outputs: n_classes-dimensional embedding with theta weights (after softmax for cross-entropy)", "\n", "# --> model.metric_embedding: 256-dimensional embedding with phi weights (for metric features used in triplet loss)", "\n", "", "result", "=", "sess", ".", "run", "(", "output_tensors", ",", "feed_dict", ")", "\n", "test_loss", "+=", "result", "[", "0", "]", "\n", "test_acc", "+=", "result", "[", "1", "]", "\n", "test_count", "+=", "1", "\n", "this_class", "=", "np", ".", "argmax", "(", "test_label", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "class_accs", "[", "this_class", "]", "+=", "result", "[", "1", "]", "# added for debug", "\n", "class_samples", "[", "this_class", "]", "+=", "1", "\n", "test_embeddings", "[", "it", ",", ":", "]", "=", "result", "[", "2", "]", "\n", "test_labels", "[", "it", "]", "=", "np", ".", "argmax", "(", "test_label", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "", "test_acc", "=", "test_acc", "/", "test_count", "\n", "test_acc", "*=", "100", "\n", "if", "test_acc", ">", "best_test_acc", ":", "\n", "        ", "best_test_acc", "=", "test_acc", "\n", "# saver.save(sess, FLAGS.logdir + '/' + FLAGS.dataset + '/' + exp_string + '/itr' + str(itr) + '_model_acc' + str(best_test_acc))", "\n", "", "print", "(", "'Unseen Target %s results: Iteration %d, Loss: %f, Accuracy: %f'", "%", "(", "\n", "case_", ",", "itr", ",", "test_loss", "/", "test_count", ",", "test_acc", ")", ")", "\n", "if", "case_", "==", "\"test\"", ":", "\n", "        ", "print", "(", "'Current best test accuracy {}'", ".", "format", "(", "best_test_acc", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Current best val accuracy {}'", ".", "format", "(", "best_test_acc", ")", ")", "\n", "# plot the test embedding:", "\n", "", "path_save", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "\"/target_domain/plots/\"", "+", "case_", "+", "\"/\"", "\n", "_", ",", "_", "=", "plot_embedding_of_points", "(", "embedding", "=", "test_embeddings", ",", "labels", "=", "test_labels", ",", "n_samples_plot", "=", "None", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_save", ")", "\n", "", "plt", ".", "savefig", "(", "path_save", "+", "'epoch'", "+", "str", "(", "itr", ")", "+", "'.png'", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "# log loss and accuracy:", "\n", "with", "open", "(", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "FLAGS", ".", "dataset", ",", "exp_string", ",", "'eva_'", "+", "case_", "+", "'.txt'", ")", ")", ",", "'a'", ")", "as", "fle", ":", "\n", "        ", "fle", ".", "write", "(", "'Unseen Target %s results: Iteration %d, Loss: %f, Accuracy: %f \\n'", "%", "(", "\n", "case_", ",", "itr", ",", "test_loss", "/", "test_count", ",", "test_acc", ")", ")", "\n", "", "return", "test_acc", ",", "best_test_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.evaluation_after_training": [[498, 597], ["range", "len", "range", "numpy.empty", "range", "main.plot_embedding_of_points_2PLOTS", "tensorflow.device", "range", "len", "evaluation_init_op.append", "evaluation_batches_per_epoch.append", "len", "sess.run", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "print", "numpy.vstack", "total_evaluation_labels_classwise.extend", "total_evaluation_labels_domainwise.extend", "os.path.exists", "os.makedirs", "open", "fle.write", "len", "data_generator.ImageDataGenerator", "evaluation_data_list.append", "evaluation_iterator_list.append", "evaluation_next_list.append", "evaluation_iterator_list[].make_initializer", "int", "sess.run", "sess.run", "os.path.exists", "os.makedirs", "open", "fle.write", "tensorflow.data.Iterator.from_structure", "evaluation_iterator_list[].get_next", "numpy.floor", "numpy.argmax", "numpy.argmax", "str"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.plot_embedding_of_points_2PLOTS"], ["", "def", "evaluation_after_training", "(", "model", ",", "sess", ",", "evaluation_file_list", ",", "evaluation_batch_size", "=", "1", ")", ":", "\n", "    ", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "method_", "=", "\"MASF\"", "\n", "", "else", ":", "\n", "        ", "method_", "=", "\"DeepAll\"", "\n", "\n", "# Data loaders:", "\n", "", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "        ", "evaluation_data_list", ",", "evaluation_iterator_list", ",", "evaluation_next_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "evaluation_file_list", ")", ")", ":", "\n", "            ", "evaluation_data", "=", "ImageDataGenerator", "(", "evaluation_file_list", "[", "i", "]", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "evaluation_batch_size", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "\n", "shuffle", "=", "False", ")", "\n", "evaluation_data_list", ".", "append", "(", "evaluation_data", ")", "\n", "evaluation_iterator_list", ".", "append", "(", "\n", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "evaluation_data", ".", "data", ".", "output_types", ",", "evaluation_data", ".", "data", ".", "output_shapes", ")", ")", "\n", "evaluation_next_list", ".", "append", "(", "evaluation_iterator_list", "[", "i", "]", ".", "get_next", "(", ")", ")", "\n", "\n", "# Ops for initializing different iterators:", "\n", "", "", "evaluation_init_op", "=", "[", "]", "\n", "evaluation_batches_per_epoch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "evaluation_file_list", ")", ")", ":", "\n", "        ", "evaluation_init_op", ".", "append", "(", "evaluation_iterator_list", "[", "i", "]", ".", "make_initializer", "(", "evaluation_data_list", "[", "i", "]", ".", "data", ")", ")", "\n", "evaluation_batches_per_epoch", ".", "append", "(", "int", "(", "np", ".", "floor", "(", "evaluation_data_list", "[", "i", "]", ".", "data_size", "/", "evaluation_batch_size", ")", ")", ")", "\n", "\n", "# Initialize iterator:", "\n", "", "num_total_domains", "=", "len", "(", "evaluation_file_list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "evaluation_file_list", ")", ")", ":", "\n", "        ", "sess", ".", "run", "(", "evaluation_init_op", "[", "i", "]", ")", "\n", "\n", "# Embedding:", "\n", "", "total_evaluation_embeddings", "=", "np", ".", "empty", "(", "(", "0", ",", "FLAGS", ".", "feature_space_dimension", ")", ")", "\n", "total_evaluation_labels_classwise", "=", "[", "]", "\n", "total_evaluation_labels_domainwise", "=", "[", "]", "\n", "total_evaluation_acc", ",", "total_evaluation_loss", ",", "total_evaluation_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "for", "domain_index", "in", "range", "(", "num_total_domains", ")", ":", "\n", "        ", "class_accs", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "class_samples", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "evaluation_embeddings", "=", "np", ".", "zeros", "(", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ",", "FLAGS", ".", "feature_space_dimension", ")", ")", "\n", "evaluation_labels_classwise", "=", "np", ".", "zeros", "(", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ",", ")", ")", "\n", "evaluation_labels_domainwise", "=", "np", ".", "zeros", "(", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ",", ")", ")", "\n", "evaluation_acc", ",", "evaluation_loss", ",", "evaluation_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "for", "it", "in", "range", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ")", ":", "\n", "            ", "evaluation_input", ",", "evaluation_label_classwise", "=", "sess", ".", "run", "(", "evaluation_next_list", "[", "domain_index", "]", ")", "\n", "feed_dict", "=", "{", "model", ".", "test_input", ":", "evaluation_input", ",", "model", ".", "test_label", ":", "evaluation_label_classwise", ",", "\n", "model", ".", "KEEP_PROB", ":", "1.", "}", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "                ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "semantic_feature", ",", "model", ".", "outputs", ",", "\n", "model", ".", "metric_embedding", "]", "\n", "", "else", ":", "\n", "                ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "embedding_feature", ",", "model", ".", "outputs", "]", "\n", "# --> model.semantic_feature: 4096-dimensional embedding with psi weights (for semantic features)", "\n", "# --> model.outputs: n_classes-dimensional embedding with theta weights (after softmax for cross-entropy)", "\n", "# --> model.metric_embedding: 256-dimensional embedding with phi weights (for metric features used in triplet loss)", "\n", "", "result", "=", "sess", ".", "run", "(", "output_tensors", ",", "feed_dict", ")", "\n", "evaluation_loss", "+=", "result", "[", "0", "]", "\n", "total_evaluation_loss", "+=", "result", "[", "0", "]", "\n", "evaluation_acc", "+=", "result", "[", "1", "]", "\n", "total_evaluation_acc", "+=", "result", "[", "1", "]", "\n", "evaluation_count", "+=", "1", "\n", "total_evaluation_count", "+=", "1", "\n", "this_class", "=", "np", ".", "argmax", "(", "evaluation_label_classwise", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "class_accs", "[", "this_class", "]", "+=", "result", "[", "1", "]", "# added for debug", "\n", "class_samples", "[", "this_class", "]", "+=", "1", "\n", "evaluation_embeddings", "[", "it", ",", ":", "]", "=", "result", "[", "2", "]", "\n", "evaluation_labels_classwise", "[", "it", "]", "=", "np", ".", "argmax", "(", "evaluation_label_classwise", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "evaluation_labels_domainwise", "[", "it", "]", "=", "domain_index", "\n", "", "evaluation_acc", "=", "evaluation_acc", "/", "evaluation_count", "\n", "evaluation_acc", "*=", "100", "\n", "evaluation_loss", "=", "evaluation_loss", "/", "evaluation_count", "\n", "print", "(", "'Evaluation results: Domain %d, Loss: %f, Accuracy: %f'", "%", "(", "domain_index", ",", "evaluation_loss", ",", "evaluation_acc", ")", ")", "\n", "total_evaluation_embeddings", "=", "np", ".", "vstack", "(", "(", "total_evaluation_embeddings", ",", "evaluation_embeddings", ")", ")", "\n", "total_evaluation_labels_classwise", ".", "extend", "(", "evaluation_labels_classwise", ")", "\n", "total_evaluation_labels_domainwise", ".", "extend", "(", "evaluation_labels_domainwise", ")", "\n", "# save accuracy results:", "\n", "path_", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "'/total_evaluation/accuracy/'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "path_", ")", "\n", "", "with", "open", "(", "(", "path_", "+", "'domain'", "+", "str", "(", "domain_index", ")", "+", "'_accuracy.txt'", ")", ",", "'a'", ")", "as", "fle", ":", "\n", "            ", "fle", ".", "write", "(", "'Evaluation results: Domain %d, Loss: %f, Accuracy: %f'", "%", "(", "\n", "domain_index", ",", "evaluation_loss", ",", "evaluation_acc", ")", ")", "\n", "# save total average accuracy results (average over the whole evaulation data):", "\n", "", "", "total_evaluation_acc", "=", "total_evaluation_acc", "/", "total_evaluation_count", "\n", "total_evaluation_acc", "*=", "100", "\n", "total_evaluation_loss_avg", "=", "total_evaluation_loss", "/", "total_evaluation_count", "\n", "path_", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "'/total_evaluation/accuracy/'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_", ")", "\n", "", "with", "open", "(", "(", "path_", "+", "\"total_accuracy.txt\"", ")", ",", "'a'", ")", "as", "fle", ":", "\n", "        ", "fle", ".", "write", "(", "'Evaluation results: Loss: %f, Accuracy: %f'", "%", "(", "total_evaluation_loss", ",", "total_evaluation_acc", ")", ")", "\n", "\n", "# plot the classwise evaluation embedding:", "\n", "", "path_save1", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "\"/total_evaluation/plots/classwise/\"", "\n", "path_save2", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "\"/total_evaluation/plots/domainwise/\"", "\n", "name_save1", ",", "name_save2", "=", "\"embedding_classwise\"", ",", "\"embedding_domainwise\"", "\n", "plot_embedding_of_points_2PLOTS", "(", "embedding", "=", "total_evaluation_embeddings", ",", "labels1", "=", "total_evaluation_labels_classwise", ",", "\n", "labels2", "=", "total_evaluation_labels_domainwise", ",", "\n", "path_save1", "=", "path_save1", ",", "path_save2", "=", "path_save2", ",", "\n", "name_save1", "=", "name_save1", ",", "name_save2", "=", "name_save2", ",", "n_samples_plot", "=", "2000", ",", "method", "=", "'TSNE'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.plot_embedding_of_points": [[599, 623], ["matplotlib.subplots", "matplotlib.scatter", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "range", "min", "range", "sklearn.manifold.TSNE().fit_transform", "range", "umap.UMAP().fit_transform", "str", "len", "numpy.arange", "sklearn.manifold.TSNE", "umap.UMAP"], "function", ["None"], ["", "def", "plot_embedding_of_points", "(", "embedding", ",", "labels", ",", "n_samples_plot", "=", "None", ",", "method", "=", "'TSNE'", ")", ":", "\n", "    ", "n_samples", "=", "embedding", ".", "shape", "[", "0", "]", "\n", "if", "n_samples_plot", "!=", "None", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "min", "(", "n_samples_plot", ",", "n_samples", ")", ",", "replace", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "n_samples", ",", "replace", "=", "False", ")", "\n", "", "embedding_sampled", "=", "embedding", "[", "indices_to_plot", ",", ":", "]", "\n", "if", "embedding", ".", "shape", "[", "1", "]", "==", "2", ":", "\n", "\n", "        ", "embedding_sampled", "=", "embedding", "\n", "", "else", ":", "\n", "        ", "if", "method", "==", "'TSNE'", ":", "\n", "            ", "embedding_sampled", "=", "TSNE", "(", "n_components", "=", "2", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "elif", "method", "==", "'UMAP'", ":", "\n", "            ", "embedding_sampled", "=", "umap", ".", "UMAP", "(", "n_neighbors", "=", "500", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "", "labels_sampled", "=", "labels", "[", "indices_to_plot", "]", "\n", "_", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "figsize", "=", "(", "14", ",", "10", ")", ")", "\n", "n_classes", "=", "FLAGS", ".", "num_classes", "\n", "class_names", "=", "[", "class_list", "[", "str", "(", "i", ")", "]", "for", "i", "in", "range", "(", "len", "(", "class_list", ")", ")", "]", "\n", "plt", ".", "scatter", "(", "embedding_sampled", "[", ":", ",", "0", "]", ",", "embedding_sampled", "[", ":", ",", "1", "]", ",", "s", "=", "10", ",", "c", "=", "labels_sampled", ",", "cmap", "=", "'Spectral'", ",", "alpha", "=", "1.0", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "boundaries", "=", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", "+", "1", ")", "-", "0.5", ")", "\n", "cbar", ".", "set_ticks", "(", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", ")", ")", "\n", "cbar", ".", "set_ticklabels", "(", "class_names", ")", "\n", "return", "plt", ",", "indices_to_plot", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.plot_embedding_of_points_2PLOTS": [[625, 679], ["numpy.asarray", "numpy.asarray", "labels1.astype.astype", "labels2.astype.astype", "matplotlib.subplots", "matplotlib.scatter", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "matplotlib.savefig", "matplotlib.clf", "matplotlib.close", "matplotlib.subplots", "len", "matplotlib.scatter", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "matplotlib.savefig", "matplotlib.clf", "matplotlib.close", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "os.path.exists", "os.makedirs", "numpy.arange", "os.path.exists", "os.makedirs", "range", "min", "range", "sklearn.manifold.TSNE().fit_transform", "range", "umap.UMAP().fit_transform", "str", "len", "numpy.arange", "numpy.arange", "sklearn.manifold.TSNE", "umap.UMAP"], "function", ["None"], ["", "def", "plot_embedding_of_points_2PLOTS", "(", "embedding", ",", "labels1", ",", "labels2", ",", "path_save1", ",", "path_save2", ",", "\n", "name_save1", ",", "name_save2", ",", "n_samples_plot", "=", "None", ",", "method", "=", "'TSNE'", ")", ":", "\n", "    ", "n_samples", "=", "embedding", ".", "shape", "[", "0", "]", "\n", "if", "n_samples_plot", "!=", "None", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "min", "(", "n_samples_plot", ",", "n_samples", ")", ",", "replace", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "n_samples", ",", "replace", "=", "False", ")", "\n", "", "embedding_sampled", "=", "embedding", "[", "indices_to_plot", ",", ":", "]", "\n", "if", "embedding", ".", "shape", "[", "1", "]", "==", "2", ":", "\n", "\n", "        ", "embedding_sampled", "=", "embedding", "\n", "", "else", ":", "\n", "        ", "if", "method", "==", "'TSNE'", ":", "\n", "            ", "embedding_sampled", "=", "TSNE", "(", "n_components", "=", "2", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "elif", "method", "==", "'UMAP'", ":", "\n", "            ", "embedding_sampled", "=", "umap", ".", "UMAP", "(", "n_neighbors", "=", "500", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "", "labels1", "=", "np", ".", "asarray", "(", "labels1", ")", "\n", "labels2", "=", "np", ".", "asarray", "(", "labels2", ")", "\n", "labels1", "=", "labels1", ".", "astype", "(", "int", ")", "\n", "labels2", "=", "labels2", ".", "astype", "(", "int", ")", "\n", "labels1_sampled", "=", "labels1", "[", "indices_to_plot", "]", "\n", "labels2_sampled", "=", "labels2", "[", "indices_to_plot", "]", "\n", "#### plot1 (for classwise):", "\n", "_", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "figsize", "=", "(", "14", ",", "10", ")", ")", "\n", "n_classes", "=", "FLAGS", ".", "num_classes", "\n", "class_names", "=", "[", "class_list", "[", "str", "(", "i", ")", "]", "for", "i", "in", "range", "(", "len", "(", "class_list", ")", ")", "]", "\n", "plt", ".", "scatter", "(", "embedding_sampled", "[", ":", ",", "0", "]", ",", "embedding_sampled", "[", ":", ",", "1", "]", ",", "s", "=", "10", ",", "c", "=", "labels1_sampled", ",", "cmap", "=", "'Spectral'", ",", "alpha", "=", "1.0", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "boundaries", "=", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", "+", "1", ")", "-", "0.5", ")", "\n", "cbar", ".", "set_ticks", "(", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", ")", ")", "\n", "cbar", ".", "set_ticklabels", "(", "class_names", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save1", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_save1", ")", "\n", "", "plt", ".", "savefig", "(", "path_save1", "+", "name_save1", "+", "'.png'", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "#### plot2 (for domainwise):", "\n", "_", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "figsize", "=", "(", "14", ",", "10", ")", ")", "\n", "n_classes", "=", "len", "(", "domain_list", ")", "\n", "class_names", "=", "domain_list", "\n", "plt", ".", "scatter", "(", "embedding_sampled", "[", ":", ",", "0", "]", ",", "embedding_sampled", "[", ":", ",", "1", "]", ",", "s", "=", "10", ",", "c", "=", "labels2_sampled", ",", "cmap", "=", "'Spectral'", ",", "alpha", "=", "1.0", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "boundaries", "=", "np", ".", "arange", "(", "n_classes", "+", "1", ")", "-", "0.5", ")", "\n", "cbar", ".", "set_ticks", "(", "np", ".", "arange", "(", "n_classes", ")", ")", "\n", "cbar", ".", "set_ticklabels", "(", "class_names", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save2", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_save2", ")", "\n", "", "plt", ".", "savefig", "(", "path_save2", "+", "name_save2", "+", "'.png'", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "np", ".", "save", "(", "'embedding_sampled.npy'", ",", "embedding_sampled", ")", "\n", "np", ".", "save", "(", "'labels1_sampled.npy'", ",", "labels1_sampled", ")", "\n", "np", ".", "save", "(", "'labels2_sampled.npy'", ",", "labels2_sampled", ")", "\n", "np", ".", "save", "(", "'embedding.npy'", ",", "embedding", ")", "\n", "np", ".", "save", "(", "'labels1.npy'", ",", "labels1", ")", "\n", "np", ".", "save", "(", "'labels2.npy'", ",", "labels2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.save_network_model": [[681, 687], ["saver_.save", "os.path.exists", "os.makedirs", "os.path.join"], "function", ["None"], ["", "def", "save_network_model", "(", "saver_", ",", "session_", ",", "checkpoint_dir", ",", "model_name", "=", "\"model_\"", ")", ":", "\n", "# https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model", "\n", "# https://github.com/taki0112/ResNet-Tensorflow/blob/master/ResNet.py", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "checkpoint_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "checkpoint_dir", ")", "\n", "", "saver_", ".", "save", "(", "session_", ",", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "model_name", "+", "'.model'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.main.load_network_model": [[689, 701], ["print", "tensorflow.train.get_checkpoint_state", "os.path.basename", "saver_.restore", "print", "print", "os.path.join"], "function", ["None"], ["", "def", "load_network_model", "(", "saver_", ",", "session_", ",", "checkpoint_dir", ")", ":", "\n", "# https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model", "\n", "    ", "print", "(", "\" [*] Reading checkpoints...\"", ")", "\n", "ckpt", "=", "tf", ".", "train", ".", "get_checkpoint_state", "(", "checkpoint_dir", ")", "\n", "if", "ckpt", "and", "ckpt", ".", "model_checkpoint_path", ":", "\n", "        ", "ckpt_name", "=", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "model_checkpoint_path", ")", "\n", "saver_", ".", "restore", "(", "session_", ",", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "ckpt_name", ")", ")", "\n", "print", "(", "\" [*] Success to read {}\"", ".", "format", "(", "ckpt_name", ")", ")", "\n", "return", "True", "\n", "", "else", ":", "\n", "        ", "print", "(", "\" [*] Failed to find a checkpoint\"", ")", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator.__init__": [[16, 73], ["data_generator.ImageDataGenerator._read_txt_file", "len", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.data.Dataset.from_tensor_slices", "data.map.map.batch", "data_generator.ImageDataGenerator._shuffle_lists", "data.map.map.map", "data.map.map.shuffle", "data.map.map.map", "ValueError"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._read_txt_file", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._shuffle_lists"], ["    ", "def", "__init__", "(", "self", ",", "txt_file", ",", "dataroot", ",", "mode", ",", "batch_size", ",", "num_classes", ",", "shuffle", "=", "True", ",", "buffer_size", "=", "1000", ")", ":", "\n", "\n", "        ", "\"\"\"Create a new ImageDataGenerator.\n        Receives a path string to a text file, where each line has a path string to an image and\n        separated by a space, then with an integer referring to the class number.\n\n        Args:\n            txt_file: path to the text file.\n            mode: either 'training' or 'validation'. Depending on this value, different parsing functions will be used.\n            batch_size: number of images per batch.\n            num_classes: number of classes in the dataset.\n            shuffle: wether or not to shuffle the data in the dataset and the initial file list.\n            buffer_size: number of images used as buffer for TensorFlows shuffling of the dataset.\n\n        Raises:\n            ValueError: If an invalid mode is passed.\n        \"\"\"", "\n", "\n", "self", ".", "dataroot", "=", "dataroot", "\n", "self", ".", "txt_file", "=", "txt_file", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "# retrieve the data from the text file", "\n", "self", ".", "_read_txt_file", "(", ")", "\n", "\n", "# number of samples in the dataset", "\n", "self", ".", "data_size", "=", "len", "(", "self", ".", "labels", ")", "\n", "\n", "# initial shuffling of the file and label lists together", "\n", "if", "shuffle", ":", "\n", "            ", "self", ".", "_shuffle_lists", "(", ")", "\n", "\n", "# convert lists to TF tensor", "\n", "", "self", ".", "img_paths", "=", "convert_to_tensor", "(", "self", ".", "img_paths", ",", "dtype", "=", "dtypes", ".", "string", ")", "\n", "self", ".", "labels", "=", "convert_to_tensor", "(", "self", ".", "labels", ",", "dtype", "=", "dtypes", ".", "int32", ")", "\n", "\n", "# create dataset", "\n", "data", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "(", "self", ".", "img_paths", ",", "self", ".", "labels", ")", ")", "\n", "\n", "# distinguish between train/infer. when calling the parsing functions", "\n", "if", "mode", "==", "'training'", ":", "\n", "            ", "data", "=", "data", ".", "map", "(", "self", ".", "_parse_function_train", ",", "num_parallel_calls", "=", "8", ")", "\n", "\n", "", "elif", "mode", "==", "'inference'", ":", "\n", "            ", "data", "=", "data", ".", "map", "(", "self", ".", "_parse_function_inference", ",", "num_parallel_calls", "=", "8", ")", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid mode '%s'.\"", "%", "(", "mode", ")", ")", "\n", "\n", "# shuffle the first `buffer_size` elements of the dataset", "\n", "", "if", "shuffle", ":", "\n", "            ", "data", "=", "data", ".", "shuffle", "(", "buffer_size", "=", "buffer_size", ")", "\n", "\n", "# create a new dataset with batches of images", "\n", "", "data", "=", "data", ".", "batch", "(", "batch_size", ")", "\n", "\n", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator._read_txt_file": [[74, 84], ["open", "f.readlines", "line.split", "data_generator.ImageDataGenerator.img_paths.append", "data_generator.ImageDataGenerator.labels.append", "os.path.join", "int"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "_read_txt_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"Read the content of the text file and store it into lists.\"\"\"", "\n", "self", ".", "img_paths", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "txt_file", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "items", "=", "line", ".", "split", "(", "' '", ")", "\n", "self", ".", "img_paths", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataroot", ",", "items", "[", "0", "]", ")", ")", "\n", "self", ".", "labels", ".", "append", "(", "int", "(", "items", "[", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator._shuffle_lists": [[85, 95], ["numpy.random.permutation", "data_generator.ImageDataGenerator.img_paths.append", "data_generator.ImageDataGenerator.labels.append"], "methods", ["None"], ["", "", "", "def", "_shuffle_lists", "(", "self", ")", ":", "\n", "        ", "\"\"\"Conjoined shuffling of the list of paths and labels.\"\"\"", "\n", "path", "=", "self", ".", "img_paths", "\n", "labels", "=", "self", ".", "labels", "\n", "permutation", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "data_size", ")", "\n", "self", ".", "img_paths", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "for", "i", "in", "permutation", ":", "\n", "            ", "self", ".", "img_paths", ".", "append", "(", "path", "[", "i", "]", ")", "\n", "self", ".", "labels", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator._parse_function_train": [[96, 115], ["tensorflow.one_hot", "tensorflow.read_file", "tensorflow.image.decode_png", "tensorflow.image.resize_images", "tensorflow.subtract", "tensorflow.cond", "tensorflow.random_uniform", "data_generator.ImageDataGenerator.augment"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator.augment"], ["", "", "def", "_parse_function_train", "(", "self", ",", "filename", ",", "label", ")", ":", "\n", "        ", "\"\"\"Input parser for samples of the training set.\"\"\"", "\n", "# convert label number into one-hot-encoding", "\n", "one_hot", "=", "tf", ".", "one_hot", "(", "label", ",", "self", ".", "num_classes", ")", "\n", "\n", "# load and pre-process the image", "\n", "img_string", "=", "tf", ".", "read_file", "(", "filename", ")", "\n", "img_decoded", "=", "tf", ".", "image", ".", "decode_png", "(", "img_string", ",", "channels", "=", "3", ")", "\n", "img_resized", "=", "tf", ".", "image", ".", "resize_images", "(", "img_decoded", ",", "[", "227", ",", "227", "]", ")", "\n", "\n", "img_centered", "=", "tf", ".", "subtract", "(", "img_resized", ",", "IMAGENET_MEAN", ")", "\n", "\n", "# RGB -> BGR", "\n", "img_bgr", "=", "img_centered", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "\n", "# Data augmentation comes here, with a chance of 50%", "\n", "img_bgr", "=", "tf", ".", "cond", "(", "tf", ".", "random_uniform", "(", "[", "]", ",", "0", ",", "1", ")", "<", "0.5", ",", "lambda", ":", "self", ".", "augment", "(", "img_bgr", ")", ",", "lambda", ":", "img_bgr", ")", "\n", "\n", "return", "img_bgr", ",", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator._parse_function_inference": [[116, 132], ["tensorflow.one_hot", "tensorflow.read_file", "tensorflow.image.decode_png", "tensorflow.image.resize_images", "tensorflow.subtract"], "methods", ["None"], ["", "def", "_parse_function_inference", "(", "self", ",", "filename", ",", "label", ")", ":", "\n", "        ", "\"\"\"Input parser for samples of the validation/test set.\"\"\"", "\n", "# convert label number into one-hot-encoding", "\n", "one_hot", "=", "tf", ".", "one_hot", "(", "label", ",", "self", ".", "num_classes", ")", "\n", "\n", "# load and preprocess the image", "\n", "img_string", "=", "tf", ".", "read_file", "(", "filename", ")", "\n", "img_decoded", "=", "tf", ".", "image", ".", "decode_png", "(", "img_string", ",", "channels", "=", "3", ")", "\n", "img_resized", "=", "tf", ".", "image", ".", "resize_images", "(", "img_decoded", ",", "[", "227", ",", "227", "]", ")", "\n", "\n", "img_centered", "=", "tf", ".", "subtract", "(", "img_resized", ",", "IMAGENET_MEAN", ")", "\n", "\n", "# RGB -> BGR", "\n", "img_bgr", "=", "img_centered", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "\n", "return", "img_bgr", ",", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator.augment": [[133, 140], ["tensorflow.cond", "tensorflow.random_uniform", "f"], "methods", ["None"], ["", "def", "augment", "(", "self", ",", "x", ")", ":", "\n", "# add more types of augmentations here", "\n", "        ", "augmentations", "=", "[", "self", ".", "flip", "]", "\n", "for", "f", "in", "augmentations", ":", "\n", "            ", "x", "=", "tf", ".", "cond", "(", "tf", ".", "random_uniform", "(", "[", "]", ",", "0", ",", "1", ")", "<", "0.25", ",", "lambda", ":", "f", "(", "x", ")", ",", "lambda", ":", "x", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.data_generator.ImageDataGenerator.flip": [[141, 152], ["tensorflow.image.random_flip_left_right"], "methods", ["None"], ["", "def", "flip", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Flip augmentation\n        Args:\n            x: Image to flip\n        Returns:\n            Augmented image\n        \"\"\"", "\n", "x", "=", "tf", ".", "image", ".", "random_flip_left_right", "(", "x", ")", "\n", "# x = tf.image.random_flip_up_down(x)", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.deep_all.DeepAll.__init__": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "WEIGHTS_PATH", ",", "feature_space_dimension", ")", ":", "\n", "        ", "\"\"\" Call construct_model_*() after initializing deep_all\"\"\"", "\n", "self", ".", "deep_all_lr", "=", "FLAGS", ".", "deep_all_lr", "\n", "self", ".", "SKIP_LAYER", "=", "[", "'fc8'", "]", "\n", "self", ".", "forward", "=", "self", ".", "forward_alexnet", "\n", "self", ".", "construct_weights", "=", "self", ".", "construct_alexnet_weights", "\n", "self", ".", "loss_func", "=", "xent", "\n", "self", ".", "WEIGHTS_PATH", "=", "WEIGHTS_PATH", "\n", "self", ".", "feature_space_dimension", "=", "feature_space_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.deep_all.DeepAll.construct_model_train": [[27, 92], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "tensorflow.Variable", "deep_all.DeepAll.construct_model_train.cross_entropy"], "methods", ["None"], ["", "def", "construct_model_train", "(", "self", ",", "prefix", "=", "'deep_all_train_'", ")", ":", "\n", "        ", "self", ".", "inputa", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputa1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labelb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "bool_indicator_b_a", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "self", ".", "bool_indicator_b_a1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "\n", "self", ".", "KEEP_PROB", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "training_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "print", "(", "'weights already defined'", ")", "\n", "training_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "self", ".", "weights", "=", "weights", "=", "self", ".", "construct_weights", "(", ")", "\n", "\n", "", "def", "cross_entropy", "(", "inp", ",", "reuse", "=", "True", ")", ":", "\n", "# Function to perform meta learning update \"\"\"", "\n", "                ", "inputa", ",", "inputa1", ",", "inputb", ",", "labela", ",", "labela1", ",", "labelb", "=", "inp", "\n", "\n", "# Obtaining the conventional task loss on meta-train", "\n", "_", ",", "task_outputa", "=", "self", ".", "forward", "(", "inputa", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa", "=", "self", ".", "loss_func", "(", "task_outputa", ",", "labela", ")", "\n", "_", ",", "task_outputa1", "=", "self", ".", "forward", "(", "inputa1", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa1", "=", "self", ".", "loss_func", "(", "task_outputa1", ",", "labela1", ")", "\n", "_", ",", "task_outputb", "=", "self", ".", "forward", "(", "inputb", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossb", "=", "self", ".", "loss_func", "(", "task_outputb", ",", "labelb", ")", "\n", "\n", "task_accuracya", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela", ",", "1", ")", ")", "#this accuracy already gathers batch size", "\n", "task_accuracya1", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa1", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela1", ",", "1", ")", ")", "\n", "task_accuracyb", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputb", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labelb", ",", "1", ")", ")", "\n", "task_output", "=", "[", "task_lossa", ",", "task_lossa1", ",", "task_lossb", ",", "task_accuracya", ",", "task_accuracya1", ",", "task_accuracyb", "]", "\n", "\n", "return", "task_output", "\n", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "\n", "input_tensors", "=", "(", "self", ".", "inputa", ",", "self", ".", "inputa1", ",", "self", ".", "inputb", ",", "self", ".", "labela", ",", "self", ".", "labela1", ",", "self", ".", "labelb", ")", "\n", "result", "=", "cross_entropy", "(", "inp", "=", "input_tensors", ")", "\n", "self", ".", "lossa_raw", ",", "self", ".", "lossa1_raw", ",", "self", ".", "lossb_raw", ",", "accuracya", ",", "accuracya1", ",", "accuracyb", "=", "result", "\n", "\n", "## Performance & Optimization", "\n", "", "if", "'train'", "in", "prefix", ":", "\n", "            ", "self", ".", "lossa", "=", "avg_lossa", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa_raw", ")", "\n", "self", ".", "lossa1", "=", "avg_lossa1", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa1_raw", ")", "\n", "self", ".", "lossb", "=", "avg_lossb", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossb_raw", ")", "\n", "self", ".", "source_loss", "=", "(", "avg_lossa", "+", "avg_lossa1", "+", "avg_lossb", ")", "/", "3.0", "#--> because we have three training source domains", "\n", "self", ".", "task_train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "deep_all_lr", ")", ".", "minimize", "(", "self", ".", "source_loss", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "\n", "self", ".", "accuracya", "=", "accuracya", "*", "100.", "\n", "self", ".", "accuracya1", "=", "accuracya1", "*", "100.", "\n", "self", ".", "accuracyb", "=", "accuracyb", "*", "100.", "\n", "self", ".", "source_accuracy", "=", "(", "self", ".", "accuracya", "+", "self", ".", "accuracya1", "+", "self", ".", "accuracyb", ")", "/", "3.0", "\n", "\n", "## Summaries", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 loss'", ",", "self", ".", "lossa", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 loss'", ",", "self", ".", "lossa1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_3 loss'", ",", "self", ".", "lossb", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 accuracy'", ",", "self", ".", "accuracya", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 accuracy'", ",", "self", ".", "accuracya1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_3 accuracy'", ",", "self", ".", "accuracyb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.deep_all.DeepAll.construct_model_test": [[93, 113], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.variable_scope", "deep_all.DeepAll.forward", "deep_all.DeepAll.loss_func", "tensorflow.contrib.metrics.accuracy", "tensorflow.nn.softmax", "dir", "testing_scope.reuse_variables", "ValueError", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.nn.softmax"], "methods", ["None"], ["", "def", "construct_model_test", "(", "self", ",", "prefix", "=", "'test'", ")", ":", "\n", "\n", "        ", "self", ".", "test_input", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "test_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "testing_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "testing_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Weights not initilized. Create training model before testing model'", ")", "\n", "\n", "", "self", ".", "embedding_feature", ",", "outputs", "=", "self", ".", "forward", "(", "self", ".", "test_input", ",", "weights", ")", "\n", "losses", "=", "self", ".", "loss_func", "(", "outputs", ",", "self", ".", "test_label", ")", "\n", "accuracies", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "self", ".", "test_label", ",", "1", ")", ")", "\n", "self", ".", "pred_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", "\n", "self", ".", "outputs", "=", "outputs", "\n", "\n", "", "self", ".", "test_loss", "=", "losses", "\n", "self", ".", "test_acc", "=", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.deep_all.DeepAll.load_initial_weights": [[119, 144], ["numpy.load().item", "numpy.load", "tensorflow.variable_scope", "tensorflow.variable_scope", "len", "tensorflow.get_variable", "session.run", "tensorflow.get_variable", "session.run", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["None"], ["", "def", "load_initial_weights", "(", "self", ",", "session", ")", ":", "\n", "        ", "\"\"\"Load weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n        The weights come as a dict of lists (e.g. weights['conv1'] is a list)\n        Load the weights into the model\n        \"\"\"", "\n", "weights_dict", "=", "np", ".", "load", "(", "self", ".", "WEIGHTS_PATH", ",", "allow_pickle", "=", "True", ",", "encoding", "=", "'bytes'", ")", ".", "item", "(", ")", "\n", "\n", "# Loop over all layer names stored in the weights dict", "\n", "for", "op_name", "in", "weights_dict", ":", "\n", "\n", "# Check if layer should be trained from scratch", "\n", "            ", "if", "op_name", "not", "in", "self", ".", "SKIP_LAYER", ":", "\n", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "op_name", ",", "reuse", "=", "True", ")", ":", "\n", "\n", "                        ", "for", "data", "in", "weights_dict", "[", "op_name", "]", ":", "\n", "# Biases", "\n", "                            ", "if", "len", "(", "data", ".", "shape", ")", "==", "1", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "# Weights", "\n", "", "else", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.deep_all.DeepAll.construct_alexnet_weights": [[145, 196], ["tensorflow.contrib.layers.xavier_initializer_conv2d", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable"], "methods", ["None"], ["", "", "", "", "", "", "", "def", "construct_alexnet_weights", "(", "self", ")", ":", "\n", "\n", "        ", "weights", "=", "{", "}", "\n", "conv_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer_conv2d", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "fc_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'conv1'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv1_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "11", ",", "11", ",", "3", ",", "96", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv1_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "96", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv2'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv2_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "5", ",", "5", ",", "48", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv2_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv3'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv3_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "256", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv3_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv4'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv4_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv4_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv5'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv5_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv5_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "# with tf.variable_scope('fc6') as scope:", "\n", "#     weights['fc6_weights'] = tf.get_variable('weights', shape=[9216, 4096], initializer=conv_initializer)", "\n", "#     weights['fc6_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc6'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc6_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "9216", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc6_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc7') as scope:", "\n", "#     weights['fc7_weights'] = tf.get_variable('weights', shape=[4096, 4096], initializer=conv_initializer)", "\n", "#     weights['fc7_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc7'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc7_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc7_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc8') as scope:", "\n", "#     weights['fc8_weights'] = tf.get_variable('weights', shape=[4096, FLAGS.num_classes], initializer=fc_initializer)", "\n", "#     weights['fc8_biases'] = tf.get_variable('biases', [FLAGS.num_classes])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc8'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc8_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "FLAGS", ".", "num_classes", "]", ",", "initializer", "=", "fc_initializer", ")", "\n", "weights", "[", "'fc8_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "FLAGS", ".", "num_classes", "]", ")", "\n", "\n", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.deep_all.DeepAll.forward_alexnet": [[197, 232], ["utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.conv_block", "utils.conv_block", "utils.max_pool", "tensorflow.reshape", "utils.fc", "utils.dropout", "utils.fc", "utils.dropout", "utils.fc"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc"], ["", "def", "forward_alexnet", "(", "self", ",", "inp", ",", "weights", ",", "reuse", "=", "False", ")", ":", "\n", "# reuse is for the normalization parameters.", "\n", "\n", "        ", "conv1", "=", "conv_block", "(", "inp", ",", "weights", "[", "'conv1_weights'", "]", ",", "weights", "[", "'conv1_biases'", "]", ",", "stride_y", "=", "4", ",", "stride_x", "=", "4", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv1'", ")", "\n", "norm1", "=", "lrn", "(", "conv1", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool1", "=", "max_pool", "(", "norm1", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups", "\n", "conv2", "=", "conv_block", "(", "pool1", ",", "weights", "[", "'conv2_weights'", "]", ",", "weights", "[", "'conv2_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv2'", ")", "\n", "norm2", "=", "lrn", "(", "conv2", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool2", "=", "max_pool", "(", "norm2", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 3rd Layer: Conv (w ReLu)", "\n", "conv3", "=", "conv_block", "(", "pool2", ",", "weights", "[", "'conv3_weights'", "]", ",", "weights", "[", "'conv3_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv3'", ")", "\n", "\n", "# 4th Layer: Conv (w ReLu) splitted into two groups", "\n", "conv4", "=", "conv_block", "(", "conv3", ",", "weights", "[", "'conv4_weights'", "]", ",", "weights", "[", "'conv4_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv4'", ")", "\n", "\n", "# 5th Layer: Conv (w ReLu) -> Pool splitted into two groups", "\n", "conv5", "=", "conv_block", "(", "conv4", ",", "weights", "[", "'conv5_weights'", "]", ",", "weights", "[", "'conv5_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv5'", ")", "\n", "pool5", "=", "max_pool", "(", "conv5", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 6th Layer: Flatten -> FC (w ReLu) -> Dropout", "\n", "flattened", "=", "tf", ".", "reshape", "(", "pool5", ",", "[", "-", "1", ",", "6", "*", "6", "*", "256", "]", ")", "\n", "fc6", "=", "fc", "(", "flattened", ",", "weights", "[", "'fc6_weights'", "]", ",", "weights", "[", "'fc6_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout6", "=", "dropout", "(", "fc6", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 7th Layer: FC (w ReLu) -> Dropout", "\n", "fc7", "=", "fc", "(", "dropout6", ",", "weights", "[", "'fc7_weights'", "]", ",", "weights", "[", "'fc7_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout7", "=", "dropout", "(", "fc7", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 8th Layer: FC and return unscaled activations", "\n", "fc8", "=", "fc", "(", "dropout7", ",", "weights", "[", "'fc8_weights'", "]", ",", "weights", "[", "'fc8_biases'", "]", ")", "\n", "\n", "return", "fc7", ",", "fc8", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.get_images": [[15, 26], ["random.shuffle", "random.sample", "os.path.join", "zip", "sampler", "os.listdir"], "function", ["None"], ["def", "get_images", "(", "paths", ",", "labels", ",", "nb_samples", "=", "None", ",", "shuffle", "=", "True", ")", ":", "\n", "    ", "if", "nb_samples", "is", "not", "None", ":", "\n", "        ", "sampler", "=", "lambda", "x", ":", "random", ".", "sample", "(", "x", ",", "nb_samples", ")", "\n", "", "else", ":", "\n", "        ", "sampler", "=", "lambda", "x", ":", "x", "\n", "", "images", "=", "[", "(", "i", ",", "os", ".", "path", ".", "join", "(", "path", ",", "image", ")", ")", "for", "i", ",", "path", "in", "zip", "(", "labels", ",", "paths", ")", "for", "image", "in", "sampler", "(", "os", ".", "listdir", "(", "path", ")", ")", "]", "\n", "if", "shuffle", ":", "\n", "        ", "random", ".", "shuffle", "(", "images", ")", "\n", "", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.conv_block": [[28, 45], ["tensorflow.nn.relu", "tensorflow.nn.conv2d", "tensorflow.nn.bias_add", "tensorflow.split", "tensorflow.split", "tensorflow.concat", "tensorflow.nn.bias_add", "convolve", "convolve", "zip"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "conv_block", "(", "inp", ",", "cweight", ",", "bweight", ",", "stride_y", "=", "2", ",", "stride_x", "=", "2", ",", "groups", "=", "1", ",", "reuse", "=", "False", ",", "scope", "=", "''", ")", ":", "\n", "    ", "stride", "=", "[", "1", ",", "stride_y", ",", "stride_x", ",", "1", "]", "\n", "convolve", "=", "lambda", "i", ",", "k", ":", "tf", ".", "nn", ".", "conv2d", "(", "i", ",", "k", ",", "strides", "=", "stride", ",", "padding", "=", "'SAME'", ")", "\n", "\n", "if", "groups", "==", "1", ":", "\n", "        ", "conv_output", "=", "tf", ".", "nn", ".", "bias_add", "(", "convolve", "(", "inp", ",", "cweight", ")", ",", "bweight", ")", "\n", "", "else", ":", "\n", "        ", "input_groups", "=", "tf", ".", "split", "(", "axis", "=", "3", ",", "num_or_size_splits", "=", "groups", ",", "value", "=", "inp", ")", "\n", "weight_groups", "=", "tf", ".", "split", "(", "axis", "=", "3", ",", "num_or_size_splits", "=", "groups", ",", "value", "=", "cweight", ")", "\n", "output_groups", "=", "[", "convolve", "(", "i", ",", "k", ")", "for", "i", ",", "k", "in", "zip", "(", "input_groups", ",", "weight_groups", ")", "]", "\n", "\n", "conv", "=", "tf", ".", "concat", "(", "axis", "=", "3", ",", "values", "=", "output_groups", ")", "\n", "conv_output", "=", "tf", ".", "nn", ".", "bias_add", "(", "conv", ",", "bweight", ")", "\n", "\n", "", "relu", "=", "tf", ".", "nn", ".", "relu", "(", "conv_output", ")", "\n", "\n", "return", "relu", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.normalize": [[46, 48], ["tensorflow.contrib.layers.python.layers.batch_norm"], "function", ["None"], ["", "def", "normalize", "(", "inp", ",", "activation", ",", "reuse", ",", "scope", ")", ":", "\n", "    ", "return", "tf_layers", ".", "batch_norm", "(", "inp", ",", "activation_fn", "=", "activation", ",", "reuse", "=", "reuse", ",", "scope", "=", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.max_pool": [[49, 52], ["tensorflow.nn.max_pool"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool"], ["", "def", "max_pool", "(", "x", ",", "filter_height", ",", "filter_width", ",", "stride_y", ",", "stride_x", ",", "padding", "=", "'SAME'", ")", ":", "\n", "    ", "\"\"\"Create a max pooling layer.\"\"\"", "\n", "return", "tf", ".", "nn", ".", "max_pool", "(", "x", ",", "ksize", "=", "[", "1", ",", "filter_height", ",", "filter_width", ",", "1", "]", ",", "strides", "=", "[", "1", ",", "stride_y", ",", "stride_x", ",", "1", "]", ",", "padding", "=", "padding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.lrn": [[53, 56], ["tensorflow.nn.local_response_normalization"], "function", ["None"], ["", "def", "lrn", "(", "x", ",", "radius", ",", "alpha", ",", "beta", ",", "bias", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"Create a local response normalization layer.\"\"\"", "\n", "return", "tf", ".", "nn", ".", "local_response_normalization", "(", "x", ",", "depth_radius", "=", "radius", ",", "alpha", "=", "alpha", ",", "beta", "=", "beta", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.dropout": [[57, 60], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout"], ["", "def", "dropout", "(", "x", ",", "keep_prob", ")", ":", "\n", "    ", "\"\"\"Create a dropout layer.\"\"\"", "\n", "return", "tf", ".", "nn", ".", "dropout", "(", "x", ",", "keep_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.fc": [[61, 74], ["tensorflow.nn.xw_plus_b", "tensorflow.nn.relu", "tensorflow.nn.leaky_relu"], "function", ["None"], ["", "def", "fc", "(", "x", ",", "wweight", ",", "bweight", ",", "activation", "=", "None", ")", ":", "\n", "    ", "\"\"\"Create a fully connected layer.\"\"\"", "\n", "\n", "act", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "x", ",", "wweight", ",", "bweight", ")", "\n", "\n", "if", "activation", "is", "'relu'", ":", "\n", "        ", "return", "tf", ".", "nn", ".", "relu", "(", "act", ")", "\n", "", "elif", "activation", "is", "'leaky_relu'", ":", "\n", "        ", "return", "tf", ".", "nn", ".", "leaky_relu", "(", "act", ")", "\n", "", "elif", "activation", "is", "None", ":", "\n", "        ", "return", "act", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.mse": [[76, 80], ["tensorflow.reshape", "tensorflow.reshape", "tensorflow.reduce_mean", "tensorflow.square"], "function", ["None"], ["", "", "def", "mse", "(", "pred", ",", "label", ")", ":", "\n", "    ", "pred", "=", "tf", ".", "reshape", "(", "pred", ",", "[", "-", "1", "]", ")", "\n", "label", "=", "tf", ".", "reshape", "(", "label", ",", "[", "-", "1", "]", ")", "\n", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "pred", "-", "label", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.xent": [[81, 83], ["tensorflow.nn.softmax_cross_entropy_with_logits_v2"], "function", ["None"], ["", "def", "xent", "(", "pred", ",", "label", ")", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "logits", "=", "pred", ",", "labels", "=", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.kd": [[84, 116], ["range", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "prob1s.append", "prob2s.append", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.log", "tensorflow.log"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log"], ["", "def", "kd", "(", "data1", ",", "label1", ",", "data2", ",", "label2", ",", "bool_indicator", ",", "n_class", "=", "7", ",", "temperature", "=", "2.0", ")", ":", "\n", "\n", "    ", "kd_loss", "=", "0.0", "\n", "eps", "=", "1e-16", "\n", "\n", "prob1s", "=", "[", "]", "\n", "prob2s", "=", "[", "]", "\n", "\n", "for", "cls", "in", "range", "(", "n_class", ")", ":", "\n", "        ", "mask1", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label1", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum1", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data1", ",", "mask1", ")", ",", "axis", "=", "0", ")", "\n", "num1", "=", "tf", ".", "reduce_sum", "(", "label1", "[", ":", ",", "cls", "]", ")", "\n", "activations1", "=", "logits_sum1", "*", "1.0", "/", "(", "num1", "+", "eps", ")", "# add eps for prevent un-sampled class resulting in NAN", "\n", "prob1", "=", "tf", ".", "nn", ".", "softmax", "(", "activations1", "/", "temperature", ")", "\n", "prob1", "=", "tf", ".", "clip_by_value", "(", "prob1", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "# for preventing prob=0 resulting in NAN", "\n", "\n", "mask2", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label2", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum2", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data2", ",", "mask2", ")", ",", "axis", "=", "0", ")", "\n", "num2", "=", "tf", ".", "reduce_sum", "(", "label2", "[", ":", ",", "cls", "]", ")", "\n", "activations2", "=", "logits_sum2", "*", "1.0", "/", "(", "num2", "+", "eps", ")", "\n", "prob2", "=", "tf", ".", "nn", ".", "softmax", "(", "activations2", "/", "temperature", ")", "\n", "prob2", "=", "tf", ".", "clip_by_value", "(", "prob2", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "\n", "\n", "KL_div", "=", "(", "tf", ".", "reduce_sum", "(", "prob1", "*", "tf", ".", "log", "(", "prob1", "/", "prob2", ")", ")", "+", "tf", ".", "reduce_sum", "(", "prob2", "*", "tf", ".", "log", "(", "prob2", "/", "prob1", ")", ")", ")", "/", "2.0", "\n", "kd_loss", "+=", "KL_div", "*", "bool_indicator", "[", "cls", "]", "\n", "\n", "prob1s", ".", "append", "(", "prob1", ")", "\n", "prob2s", ".", "append", "(", "prob2", ")", "\n", "\n", "", "kd_loss", "=", "kd_loss", "/", "n_class", "\n", "\n", "return", "kd_loss", ",", "prob1s", ",", "prob2s", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.JS": [[117, 151], ["range", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "prob1s.append", "prob2s.append", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.log", "tensorflow.log"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log"], ["", "def", "JS", "(", "data1", ",", "label1", ",", "data2", ",", "label2", ",", "bool_indicator", ",", "n_class", "=", "7", ",", "temperature", "=", "2.0", ")", ":", "\n", "\n", "    ", "kd_loss", "=", "0.0", "\n", "eps", "=", "1e-16", "\n", "\n", "prob1s", "=", "[", "]", "\n", "prob2s", "=", "[", "]", "\n", "\n", "for", "cls", "in", "range", "(", "n_class", ")", ":", "\n", "        ", "mask1", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label1", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum1", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data1", ",", "mask1", ")", ",", "axis", "=", "0", ")", "\n", "num1", "=", "tf", ".", "reduce_sum", "(", "label1", "[", ":", ",", "cls", "]", ")", "\n", "activations1", "=", "logits_sum1", "*", "1.0", "/", "(", "num1", "+", "eps", ")", "# add eps for prevent un-sampled class resulting in NAN", "\n", "prob1", "=", "tf", ".", "nn", ".", "softmax", "(", "activations1", "/", "temperature", ")", "\n", "prob1", "=", "tf", ".", "clip_by_value", "(", "prob1", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "# for preventing prob=0 resulting in NAN", "\n", "\n", "mask2", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label2", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum2", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data2", ",", "mask2", ")", ",", "axis", "=", "0", ")", "\n", "num2", "=", "tf", ".", "reduce_sum", "(", "label2", "[", ":", ",", "cls", "]", ")", "\n", "activations2", "=", "logits_sum2", "*", "1.0", "/", "(", "num2", "+", "eps", ")", "\n", "prob2", "=", "tf", ".", "nn", ".", "softmax", "(", "activations2", "/", "temperature", ")", "\n", "prob2", "=", "tf", ".", "clip_by_value", "(", "prob2", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "\n", "\n", "mean_prob", "=", "(", "prob1", "+", "prob2", ")", "/", "2", "\n", "\n", "JS_div", "=", "(", "tf", ".", "reduce_sum", "(", "prob1", "*", "tf", ".", "log", "(", "prob1", "/", "mean_prob", ")", ")", "+", "tf", ".", "reduce_sum", "(", "prob2", "*", "tf", ".", "log", "(", "prob2", "/", "mean_prob", ")", ")", ")", "/", "2.0", "\n", "kd_loss", "+=", "JS_div", "*", "bool_indicator", "[", "cls", "]", "\n", "\n", "prob1s", ".", "append", "(", "prob1", ")", "\n", "prob2s", ".", "append", "(", "prob2", ")", "\n", "\n", "", "kd_loss", "=", "kd_loss", "/", "n_class", "\n", "\n", "return", "kd_loss", ",", "prob1s", ",", "prob2s", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.contrastive": [[152, 173], ["tensorflow.argmax", "tensorflow.argmax", "tensorflow.to_float", "tensorflow.sqrt", "tensorflow.square", "tensorflow.equal", "tensorflow.reduce_sum", "tensorflow.nn.relu", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.reduce_sum", "tensorflow.reduce_sum"], "function", ["None"], ["", "def", "contrastive", "(", "feature1", ",", "label1", ",", "feature2", ",", "label2", ",", "bool_indicator", "=", "None", ",", "margin", "=", "50", ")", ":", "\n", "\n", "    ", "l1", "=", "tf", ".", "argmax", "(", "label1", ",", "axis", "=", "1", ")", "\n", "l2", "=", "tf", ".", "argmax", "(", "label2", ",", "axis", "=", "1", ")", "\n", "pair", "=", "tf", ".", "to_float", "(", "tf", ".", "equal", "(", "l1", ",", "l2", ")", ")", "\n", "\n", "delta", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "feature1", "-", "feature2", ")", ",", "1", ")", "+", "1e-10", "\n", "match_loss", "=", "delta", "\n", "\n", "delta_sqrt", "=", "tf", ".", "sqrt", "(", "delta", "+", "1e-10", ")", "\n", "mismatch_loss", "=", "tf", ".", "square", "(", "tf", ".", "nn", ".", "relu", "(", "margin", "-", "delta_sqrt", ")", ")", "\n", "\n", "if", "bool_indicator", "is", "None", ":", "\n", "        ", "loss", "=", "tf", ".", "reduce_mean", "(", "0.5", "*", "(", "pair", "*", "match_loss", "+", "(", "1", "-", "pair", ")", "*", "mismatch_loss", ")", ")", "\n", "", "else", ":", "\n", "        ", "loss", "=", "0.5", "*", "tf", ".", "reduce_sum", "(", "match_loss", "*", "pair", ")", "/", "tf", ".", "reduce_sum", "(", "pair", ")", "\n", "\n", "", "debug_dist_positive", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "pair", ")", "/", "tf", ".", "reduce_sum", "(", "pair", ")", "\n", "debug_dist_negative", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "(", "1", "-", "pair", ")", ")", "/", "tf", ".", "reduce_sum", "(", "1", "-", "pair", ")", "\n", "\n", "return", "loss", ",", "pair", ",", "delta", ",", "debug_dist_positive", ",", "debug_dist_negative", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.utils.compute_distance": [[174, 186], ["tensorflow.argmax", "tensorflow.argmax", "tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.sqrt", "tensorflow.equal", "tensorflow.square", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum"], "function", ["None"], ["", "def", "compute_distance", "(", "feature1", ",", "label1", ",", "feature2", ",", "label2", ")", ":", "\n", "    ", "l1", "=", "tf", ".", "argmax", "(", "label1", ",", "axis", "=", "1", ")", "\n", "l2", "=", "tf", ".", "argmax", "(", "label2", ",", "axis", "=", "1", ")", "\n", "pair", "=", "tf", ".", "to_float", "(", "tf", ".", "equal", "(", "l1", ",", "l2", ")", ")", "\n", "\n", "delta", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "feature1", "-", "feature2", ")", ",", "1", ")", "\n", "delta_sqrt", "=", "tf", ".", "sqrt", "(", "delta", "+", "1e-16", ")", "\n", "\n", "dist_positive_pair", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "pair", ")", "/", "tf", ".", "reduce_sum", "(", "pair", ")", "\n", "dist_negative_pair", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "(", "1", "-", "pair", ")", ")", "/", "tf", ".", "reduce_sum", "(", "1", "-", "pair", ")", "\n", "\n", "return", "dist_positive_pair", ",", "dist_negative_pair", "\n", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.__init__": [[17, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "WEIGHTS_PATH", ",", "feature_space_dimension", ")", ":", "\n", "        ", "\"\"\" Call construct_model_*() after initializing MASF\"\"\"", "\n", "self", ".", "inner_lr", "=", "FLAGS", ".", "inner_lr", "\n", "self", ".", "outer_lr", "=", "FLAGS", ".", "outer_lr", "\n", "self", ".", "metric_lr", "=", "FLAGS", ".", "metric_lr", "\n", "self", ".", "SKIP_LAYER", "=", "[", "'fc8'", "]", "\n", "self", ".", "forward", "=", "self", ".", "forward_alexnet", "\n", "self", ".", "forward_metric_net", "=", "self", ".", "forward_metric_net", "\n", "self", ".", "construct_weights", "=", "self", ".", "construct_alexnet_weights", "\n", "self", ".", "loss_func", "=", "xent", "\n", "self", ".", "global_loss_func", "=", "kd", "\n", "self", ".", "WEIGHTS_PATH", "=", "WEIGHTS_PATH", "\n", "self", ".", "feature_space_dimension", "=", "feature_space_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.construct_model_train": [[31, 147], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "tensorflow.Variable", "masf_func.MASF.construct_model_train.task_metalearn"], "methods", ["None"], ["", "def", "construct_model_train", "(", "self", ",", "prefix", "=", "'metatrain_'", ")", ":", "\n", "# a: meta-train for inner update, b: meta-test for meta loss", "\n", "        ", "self", ".", "inputa", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputa1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labelb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "bool_indicator_b_a", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "self", ".", "bool_indicator_b_a1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "\n", "meta_sample_num", "=", "(", "FLAGS", ".", "meta_batch_size", "/", "3", ")", "*", "3", "\n", "self", ".", "input_group", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "label_group", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "meta_sample_num", ",", ")", ")", "\n", "\n", "self", ".", "clip_value", "=", "FLAGS", ".", "gradients_clip_value", "\n", "self", ".", "margin", "=", "FLAGS", ".", "margin", "\n", "self", ".", "KEEP_PROB", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "training_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "print", "(", "'weights already defined'", ")", "\n", "training_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "self", ".", "weights", "=", "weights", "=", "self", ".", "construct_weights", "(", ")", "\n", "\n", "", "def", "task_metalearn", "(", "inp", ",", "global_bool_indicator", ",", "reuse", "=", "True", ")", ":", "\n", "# Function to perform meta learning update \"\"\"", "\n", "                ", "inputa", ",", "inputa1", ",", "inputb", ",", "input_group", ",", "labela", ",", "labela1", ",", "labelb", ",", "label_group", "=", "inp", "\n", "global_bool_indicator_b_a", ",", "global_bool_indicator_b_a1", "=", "global_bool_indicator", "\n", "\n", "# Obtaining the conventional task loss on meta-train", "\n", "_", ",", "task_outputa", "=", "self", ".", "forward", "(", "inputa", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa", "=", "self", ".", "loss_func", "(", "task_outputa", ",", "labela", ")", "\n", "_", ",", "task_outputa1", "=", "self", ".", "forward", "(", "inputa1", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa1", "=", "self", ".", "loss_func", "(", "task_outputa1", ",", "labela1", ")", "\n", "\n", "## perform inner update with plain gradient descent on meta-train", "\n", "grads", "=", "tf", ".", "gradients", "(", "(", "task_lossa", "+", "task_lossa1", ")", "/", "2.0", ",", "list", "(", "weights", ".", "values", "(", ")", ")", ")", "\n", "grads", "=", "[", "tf", ".", "stop_gradient", "(", "grad", ")", "for", "grad", "in", "grads", "]", "# first-order gradients approximation", "\n", "gradients", "=", "dict", "(", "zip", "(", "weights", ".", "keys", "(", ")", ",", "grads", ")", ")", "\n", "fast_weights", "=", "dict", "(", "zip", "(", "weights", ".", "keys", "(", ")", ",", "[", "weights", "[", "key", "]", "-", "self", ".", "inner_lr", "*", "tf", ".", "clip_by_norm", "(", "gradients", "[", "key", "]", ",", "clip_norm", "=", "self", ".", "clip_value", ")", "for", "key", "in", "weights", ".", "keys", "(", ")", "]", ")", ")", "\n", "\n", "## compute global loss", "\n", "_", ",", "new_task_outputa", "=", "self", ".", "forward", "(", "inputa", ",", "fast_weights", ",", "reuse", "=", "reuse", ")", "\n", "_", ",", "new_task_outputa1", "=", "self", ".", "forward", "(", "inputa1", ",", "fast_weights", ",", "reuse", "=", "reuse", ")", "\n", "_", ",", "task_outputb", "=", "self", ".", "forward", "(", "inputb", ",", "fast_weights", ",", "reuse", "=", "reuse", ")", "\n", "global_loss_b_a", ",", "_", ",", "_", "=", "self", ".", "global_loss_func", "(", "task_outputb", ",", "labelb", ",", "new_task_outputa", ",", "labela", ",", "global_bool_indicator_b_a", ",", "n_class", "=", "FLAGS", ".", "num_classes", ")", "\n", "global_loss_b_a1", ",", "_", ",", "_", "=", "self", ".", "global_loss_func", "(", "task_outputb", ",", "labelb", ",", "new_task_outputa1", ",", "labela1", ",", "global_bool_indicator_b_a1", ",", "n_class", "=", "FLAGS", ".", "num_classes", ")", "\n", "global_loss", "=", "(", "global_loss_b_a", "+", "global_loss_b_a1", ")", "/", "2.0", "\n", "\n", "## compute local loss", "\n", "embeddings", ",", "_", "=", "self", ".", "forward", "(", "input_group", ",", "fast_weights", ",", "reuse", "=", "True", ")", "\n", "embeddings", "=", "self", ".", "forward_metric_net", "(", "embeddings", ")", "\n", "metric_loss", "=", "tf", ".", "contrib", ".", "losses", ".", "metric_learning", ".", "triplet_semihard_loss", "(", "labels", "=", "label_group", ",", "embeddings", "=", "embeddings", ",", "margin", "=", "self", ".", "margin", ")", "\n", "\n", "task_output", "=", "[", "global_loss", ",", "task_lossa", ",", "task_lossa1", ",", "metric_loss", "]", "\n", "task_accuracya", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela", ",", "1", ")", ")", "#this accuracy already gathers batch size", "\n", "task_accuracya1", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa1", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela1", ",", "1", ")", ")", "\n", "task_output", ".", "extend", "(", "[", "task_accuracya", ",", "task_accuracya1", "]", ")", "\n", "\n", "return", "task_output", "\n", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "# self.inner_lr = tf.train.exponential_decay(learning_rate=FLAGS.inner_lr, global_step=self.global_step, decay_steps=FLAGS.decay_steps, decay_rate=FLAGS.decay_rate)", "\n", "\n", "input_tensors", "=", "(", "self", ".", "inputa", ",", "self", ".", "inputa1", ",", "self", ".", "inputb", ",", "self", ".", "input_group", ",", "self", ".", "labela", ",", "self", ".", "labela1", ",", "self", ".", "labelb", ",", "self", ".", "label_group", ")", "\n", "global_bool_indicator", "=", "(", "self", ".", "bool_indicator_b_a", ",", "self", ".", "bool_indicator_b_a1", ")", "\n", "result", "=", "task_metalearn", "(", "inp", "=", "input_tensors", ",", "global_bool_indicator", "=", "global_bool_indicator", ")", "\n", "global_loss", ",", "self", ".", "lossa_raw", ",", "self", ".", "lossa1_raw", ",", "metric_loss", ",", "accuracya", ",", "accuracya1", "=", "result", "\n", "self", ".", "global_loss", "=", "global_loss", "*", "1.0", "\n", "self", ".", "metric_loss", "=", "metric_loss", "*", "0.005", "\n", "\n", "## Performance & Optimization", "\n", "", "if", "'train'", "in", "prefix", ":", "\n", "            ", "self", ".", "lossa", "=", "avg_lossa", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa_raw", ")", "\n", "self", ".", "lossa1", "=", "avg_lossa1", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa1_raw", ")", "\n", "self", ".", "source_loss", "=", "(", "avg_lossa", "+", "avg_lossa1", ")", "/", "2.0", "\n", "self", ".", "task_train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "outer_lr", ")", ".", "minimize", "(", "self", ".", "source_loss", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "\n", "self", ".", "accuracya", "=", "accuracya", "*", "100.", "\n", "self", ".", "accuracya1", "=", "accuracya1", "*", "100.", "\n", "self", ".", "source_accuracy", "=", "(", "self", ".", "accuracya", "+", "self", ".", "accuracya1", ")", "/", "2.0", "\n", "\n", "var_list_feature_extractor", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "(", "v", ".", "name", ".", "split", "(", "'/'", ")", "[", "1", "]", "not", "in", "self", ".", "SKIP_LAYER", "and", "'meta'", "not", "in", "v", ".", "name", ".", "split", "(", "'/'", ")", ")", "]", "\n", "var_list_classifier", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "v", ".", "name", ".", "split", "(", "'/'", ")", "[", "1", "]", "in", "self", ".", "SKIP_LAYER", "]", "\n", "var_list_metric", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "'metric'", "in", "v", ".", "name", ".", "split", "(", "'/'", ")", "]", "\n", "\n", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "outer_lr", ")", "\n", "gvs", "=", "optimizer", ".", "compute_gradients", "(", "self", ".", "global_loss", "+", "self", ".", "metric_loss", ",", "var_list", "=", "var_list_feature_extractor", "+", "var_list_classifier", ")", "\n", "\n", "# observe stability of gradients for meta loss", "\n", "l2_norm", "=", "lambda", "t", ":", "tf", ".", "sqrt", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "pow", "(", "t", ",", "2", ")", ")", ")", "\n", "for", "grad", ",", "var", "in", "gvs", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "\"gradients_norm/\"", "+", "var", ".", "name", ",", "l2_norm", "(", "grad", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "\"feature_extractor_var_norm/\"", "+", "var", ".", "name", ",", "l2_norm", "(", "var", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'gradients/'", "+", "var", ".", "name", ",", "var", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "\"feature_extractor_var/\"", "+", "var", ".", "name", ",", "var", ")", "\n", "\n", "", "gvs", "=", "[", "(", "tf", ".", "clip_by_norm", "(", "grad", ",", "clip_norm", "=", "self", ".", "clip_value", ")", ",", "var", ")", "for", "grad", ",", "var", "in", "gvs", "]", "\n", "\n", "for", "grad", ",", "var", "in", "gvs", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "\"gradients_norm_clipped/\"", "+", "var", ".", "name", ",", "l2_norm", "(", "grad", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'gradients_clipped/'", "+", "var", ".", "name", ",", "var", ")", "\n", "\n", "", "self", ".", "meta_train_op", "=", "optimizer", ".", "apply_gradients", "(", "gvs", ")", "\n", "self", ".", "metric_train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "metric_lr", ")", ".", "minimize", "(", "self", ".", "metric_loss", ",", "var_list", "=", "var_list_metric", ")", "\n", "\n", "## Summaries", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 loss'", ",", "self", ".", "lossa", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 loss'", ",", "self", ".", "lossa1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 accuracy'", ",", "self", ".", "accuracya", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 accuracy'", ",", "self", ".", "accuracya1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'global loss'", ",", "self", ".", "global_loss", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'metric loss'", ",", "self", ".", "metric_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.construct_model_test": [[148, 169], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.variable_scope", "masf_func.MASF.forward", "masf_func.MASF.forward_metric_net", "masf_func.MASF.loss_func", "tensorflow.contrib.metrics.accuracy", "tensorflow.nn.softmax", "dir", "testing_scope.reuse_variables", "ValueError", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.nn.softmax"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.forward_metric_net"], ["", "def", "construct_model_test", "(", "self", ",", "prefix", "=", "'test'", ")", ":", "\n", "\n", "        ", "self", ".", "test_input", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "test_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "testing_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "testing_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Weights not initilized. Create training model before testing model'", ")", "\n", "\n", "", "self", ".", "semantic_feature", ",", "outputs", "=", "self", ".", "forward", "(", "self", ".", "test_input", ",", "weights", ")", "\n", "self", ".", "metric_embedding", "=", "self", ".", "forward_metric_net", "(", "self", ".", "semantic_feature", ")", "\n", "losses", "=", "self", ".", "loss_func", "(", "outputs", ",", "self", ".", "test_label", ")", "\n", "accuracies", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "self", ".", "test_label", ",", "1", ")", ")", "\n", "self", ".", "pred_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", "\n", "self", ".", "outputs", "=", "outputs", "\n", "\n", "", "self", ".", "test_loss", "=", "losses", "\n", "self", ".", "test_acc", "=", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.load_initial_weights": [[175, 200], ["numpy.load().item", "numpy.load", "tensorflow.variable_scope", "tensorflow.variable_scope", "len", "tensorflow.get_variable", "session.run", "tensorflow.get_variable", "session.run", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["None"], ["", "def", "load_initial_weights", "(", "self", ",", "session", ")", ":", "\n", "        ", "\"\"\"Load weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n        The weights come as a dict of lists (e.g. weights['conv1'] is a list)\n        Load the weights into the model\n        \"\"\"", "\n", "weights_dict", "=", "np", ".", "load", "(", "self", ".", "WEIGHTS_PATH", ",", "allow_pickle", "=", "True", ",", "encoding", "=", "'bytes'", ")", ".", "item", "(", ")", "\n", "\n", "# Loop over all layer names stored in the weights dict", "\n", "for", "op_name", "in", "weights_dict", ":", "\n", "\n", "# Check if layer should be trained from scratch", "\n", "            ", "if", "op_name", "not", "in", "self", ".", "SKIP_LAYER", ":", "\n", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "op_name", ",", "reuse", "=", "True", ")", ":", "\n", "\n", "                        ", "for", "data", "in", "weights_dict", "[", "op_name", "]", ":", "\n", "# Biases", "\n", "                            ", "if", "len", "(", "data", ".", "shape", ")", "==", "1", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "# Weights", "\n", "", "else", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.forward_metric_net": [[201, 213], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "utils.fc", "tensorflow.get_variable", "tensorflow.get_variable", "utils.fc"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc"], ["", "", "", "", "", "", "", "def", "forward_metric_net", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'metric'", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "scope", ":", "\n", "\n", "            ", "w1", "=", "tf", ".", "get_variable", "(", "'w1'", ",", "shape", "=", "[", "4096", ",", "1024", "]", ")", "\n", "b1", "=", "tf", ".", "get_variable", "(", "'b1'", ",", "shape", "=", "[", "1024", "]", ")", "\n", "out", "=", "fc", "(", "x", ",", "w1", ",", "b1", ",", "activation", "=", "'leaky_relu'", ")", "\n", "w2", "=", "tf", ".", "get_variable", "(", "'w2'", ",", "shape", "=", "[", "1024", ",", "256", "]", ")", "\n", "b2", "=", "tf", ".", "get_variable", "(", "'b2'", ",", "shape", "=", "[", "256", "]", ")", "\n", "out", "=", "fc", "(", "out", ",", "w2", ",", "b2", ",", "activation", "=", "'leaky_relu'", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.construct_alexnet_weights": [[214, 265], ["tensorflow.contrib.layers.xavier_initializer_conv2d", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable"], "methods", ["None"], ["", "def", "construct_alexnet_weights", "(", "self", ")", ":", "\n", "\n", "        ", "weights", "=", "{", "}", "\n", "conv_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer_conv2d", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "fc_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'conv1'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv1_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "11", ",", "11", ",", "3", ",", "96", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv1_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "96", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv2'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv2_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "5", ",", "5", ",", "48", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv2_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv3'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv3_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "256", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv3_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv4'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv4_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv4_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv5'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv5_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv5_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "# with tf.variable_scope('fc6') as scope:", "\n", "#     weights['fc6_weights'] = tf.get_variable('weights', shape=[9216, 4096], initializer=conv_initializer)", "\n", "#     weights['fc6_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc6'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc6_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "9216", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc6_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc7') as scope:", "\n", "#     weights['fc7_weights'] = tf.get_variable('weights', shape=[4096, 4096], initializer=conv_initializer)", "\n", "#     weights['fc7_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc7'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc7_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc7_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc8') as scope:", "\n", "#     weights['fc8_weights'] = tf.get_variable('weights', shape=[4096, FLAGS.num_classes], initializer=fc_initializer)", "\n", "#     weights['fc8_biases'] = tf.get_variable('biases', [FLAGS.num_classes])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc8'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc8_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "FLAGS", ".", "num_classes", "]", ",", "initializer", "=", "fc_initializer", ")", "\n", "weights", "[", "'fc8_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "FLAGS", ".", "num_classes", "]", ")", "\n", "\n", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Malignancy_binary_classification.masf_func.MASF.forward_alexnet": [[266, 301], ["utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.conv_block", "utils.conv_block", "utils.max_pool", "tensorflow.reshape", "utils.fc", "utils.dropout", "utils.fc", "utils.dropout", "utils.fc"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc"], ["", "def", "forward_alexnet", "(", "self", ",", "inp", ",", "weights", ",", "reuse", "=", "False", ")", ":", "\n", "# reuse is for the normalization parameters.", "\n", "\n", "        ", "conv1", "=", "conv_block", "(", "inp", ",", "weights", "[", "'conv1_weights'", "]", ",", "weights", "[", "'conv1_biases'", "]", ",", "stride_y", "=", "4", ",", "stride_x", "=", "4", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv1'", ")", "\n", "norm1", "=", "lrn", "(", "conv1", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool1", "=", "max_pool", "(", "norm1", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups", "\n", "conv2", "=", "conv_block", "(", "pool1", ",", "weights", "[", "'conv2_weights'", "]", ",", "weights", "[", "'conv2_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv2'", ")", "\n", "norm2", "=", "lrn", "(", "conv2", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool2", "=", "max_pool", "(", "norm2", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 3rd Layer: Conv (w ReLu)", "\n", "conv3", "=", "conv_block", "(", "pool2", ",", "weights", "[", "'conv3_weights'", "]", ",", "weights", "[", "'conv3_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv3'", ")", "\n", "\n", "# 4th Layer: Conv (w ReLu) splitted into two groups", "\n", "conv4", "=", "conv_block", "(", "conv3", ",", "weights", "[", "'conv4_weights'", "]", ",", "weights", "[", "'conv4_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv4'", ")", "\n", "\n", "# 5th Layer: Conv (w ReLu) -> Pool splitted into two groups", "\n", "conv5", "=", "conv_block", "(", "conv4", ",", "weights", "[", "'conv5_weights'", "]", ",", "weights", "[", "'conv5_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv5'", ")", "\n", "pool5", "=", "max_pool", "(", "conv5", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 6th Layer: Flatten -> FC (w ReLu) -> Dropout", "\n", "flattened", "=", "tf", ".", "reshape", "(", "pool5", ",", "[", "-", "1", ",", "6", "*", "6", "*", "256", "]", ")", "\n", "fc6", "=", "fc", "(", "flattened", ",", "weights", "[", "'fc6_weights'", "]", ",", "weights", "[", "'fc6_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout6", "=", "dropout", "(", "fc6", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 7th Layer: FC (w ReLu) -> Dropout", "\n", "fc7", "=", "fc", "(", "dropout6", ",", "weights", "[", "'fc7_weights'", "]", ",", "weights", "[", "'fc7_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout7", "=", "dropout", "(", "fc7", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 8th Layer: FC and return unscaled activations", "\n", "fc8", "=", "fc", "(", "dropout7", ",", "weights", "[", "'fc8_weights'", "]", ",", "weights", "[", "'fc8_biases'", "]", ")", "\n", "\n", "return", "fc7", ",", "fc8", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.patch_organizer_masf.main": [[7, 41], ["BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer", "BreakHisOrganizer.BreakHisOrganizer.build_organized_dataframe", "BreakHisOrganizer.BreakHisOrganizer.write_dataframe"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.build_organized_dataframe", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.write_dataframe"], ["def", "main", "(", ")", ":", "\n", "# dataset_root = 'D:/Datasets/square_breast/'", "\n", "# reinhard_normalizer = ReinhardNormalizer.ReinhardNormalizer()    ", "\n", "# normalization_ref_image = StainingUtils.read_image('reference.png')", "\n", "# path_extension = '*/*/*/*/*/*'", "\n", "# image_extension = '.png'", "\n", "# data_frame_name = 'for_test'", "\n", "# break_his_organizer = BreakHisOrganizer.BreakHisOrganizer(dataset_root,", "\n", "#                                                         reinhard_normalizer,", "\n", "#                                                         normalization_ref_image,", "\n", "#                                                         path_extension,", "\n", "#                                                         image_extension,", "\n", "#                                                         data_frame_name)", "\n", "\n", "# break_his_organizer.build_dataframe()", "\n", "# break_his_organizer.write_dataframe()", "\n", "# break_his_organizer.save_dataset(new_dataset_name= 'custom_dataset')", "\n", "\n", "# del break_his_organizer, reinhard_normalizer, normalization_ref_image", "\n", "\n", "    ", "dataset_root", "=", "'D:/Datasets/masf_organized_breakhis_dataset/Best_after_normalization/'", "\n", "reinhard_normalizer", "=", "None", "\n", "normalization_ref_image", "=", "None", "\n", "path_extension", "=", "'*/*/*'", "\n", "image_extension", "=", "'.png'", "\n", "\n", "break_his_organizer_new_dataset", "=", "BreakHisOrganizer", ".", "BreakHisOrganizer", "(", "dataset_root", ",", "\n", "reinhard_normalizer", ",", "\n", "normalization_ref_image", ",", "\n", "path_extension", ",", "\n", "image_extension", ")", "\n", "\n", "break_his_organizer_new_dataset", ".", "build_organized_dataframe", "(", ")", "\n", "break_his_organizer_new_dataset", ".", "write_dataframe", "(", "dataframe_name", "=", "'organized_dataframe'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.read_image": [[9, 19], ["cv2.imread"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "read_image", "(", "path", ")", ":", "\n", "        ", "\"\"\"\n        Read an image to RGB uint8\n        :param path:\n        :return:\n        \"\"\"", "\n", "im", "=", "cv", ".", "imread", "(", "path", ")", "\n", "# im = cv.cvtColor(im, cv.COLOR_BGR2RGB)", "\n", "return", "im", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.show_colors": [[20, 35], ["range", "matplotlib.axis", "matplotlib.axis", "C[].max", "matplotlib.plot", "matplotlib.plot"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "show_colors", "(", "C", ")", ":", "\n", "        ", "\"\"\"\n        Shows rows of C as colors (RGB)\n        :param C:\n        :return:\n        \"\"\"", "\n", "n", "=", "C", ".", "shape", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "if", "C", "[", "i", "]", ".", "max", "(", ")", ">", "1.0", ":", "\n", "                ", "plt", ".", "plot", "(", "[", "0", ",", "1", "]", ",", "[", "n", "-", "1", "-", "i", ",", "n", "-", "1", "-", "i", "]", ",", "c", "=", "C", "[", "i", "]", "/", "255", ",", "linewidth", "=", "20", ")", "\n", "", "else", ":", "\n", "                ", "plt", ".", "plot", "(", "[", "0", ",", "1", "]", ",", "[", "n", "-", "1", "-", "i", ",", "n", "-", "1", "-", "i", "]", ",", "c", "=", "C", "[", "i", "]", ",", "linewidth", "=", "20", ")", "\n", "", "plt", ".", "axis", "(", "'off'", ")", "\n", "plt", ".", "axis", "(", "[", "0", ",", "1", ",", "-", "1", ",", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.show": [[36, 54], ["image.astype.astype.astype", "matplotlib.imshow", "matplotlib.axis", "image.astype.astype.min", "image.astype.astype.max", "matplotlib.show"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.show"], ["", "", "@", "staticmethod", "\n", "def", "show", "(", "image", ",", "now", "=", "True", ",", "fig_size", "=", "(", "10", ",", "10", ")", ")", ":", "\n", "        ", "\"\"\"\n        Show an image (np.array).\n        Caution! Rescales image to be in range [0,1].\n        :param image:\n        :param now:\n        :param fig_size:\n        :return:\n        \"\"\"", "\n", "image", "=", "image", ".", "astype", "(", "np", ".", "float32", ")", "\n", "m", ",", "M", "=", "image", ".", "min", "(", ")", ",", "image", ".", "max", "(", ")", "\n", "if", "fig_size", "!=", "None", ":", "\n", "            ", "plt", ".", "rcParams", "[", "'figure.figsize'", "]", "=", "(", "fig_size", "[", "0", "]", ",", "fig_size", "[", "1", "]", ")", "\n", "", "plt", ".", "imshow", "(", "(", "image", "-", "m", ")", "/", "(", "M", "-", "m", ")", ",", "cmap", "=", "'gray'", ")", "\n", "plt", ".", "axis", "(", "'off'", ")", "\n", "if", "now", "==", "True", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.build_stack": [[55, 72], ["len", "range", "len", "numpy.zeros", "len", "numpy.zeros"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "build_stack", "(", "tup", ")", ":", "\n", "        ", "\"\"\"\n        Build a stack of images from a tuple of images\n        :param tup:\n        :return:\n        \"\"\"", "\n", "N", "=", "len", "(", "tup", ")", "\n", "if", "len", "(", "tup", "[", "0", "]", ".", "shape", ")", "==", "3", ":", "\n", "            ", "h", ",", "w", ",", "c", "=", "tup", "[", "0", "]", ".", "shape", "\n", "stack", "=", "np", ".", "zeros", "(", "(", "N", ",", "h", ",", "w", ",", "c", ")", ")", "\n", "", "if", "len", "(", "tup", "[", "0", "]", ".", "shape", ")", "==", "2", ":", "\n", "            ", "h", ",", "w", "=", "tup", "[", "0", "]", ".", "shape", "\n", "stack", "=", "np", ".", "zeros", "(", "(", "N", ",", "h", ",", "w", ")", ")", "\n", "", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "stack", "[", "i", "]", "=", "tup", "[", "i", "]", "\n", "", "return", "stack", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.patch_grid": [[73, 104], ["numpy.ceil().astype", "matplotlib.figure", "range", "matplotlib.show", "numpy.shape", "matplotlib.subplot", "StainingUtils.StainingUtils.show"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.show", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.show"], ["", "@", "staticmethod", "\n", "def", "patch_grid", "(", "ims", ",", "width", "=", "5", ",", "sub_sample", "=", "None", ",", "rand", "=", "False", ",", "save_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Display a grid of patches\n        :param ims:\n        :param width:\n        :param sub_sample:\n        :param rand:\n        :return:\n        \"\"\"", "\n", "N0", "=", "np", ".", "shape", "(", "ims", ")", "[", "0", "]", "\n", "if", "sub_sample", "==", "None", ":", "\n", "            ", "N", "=", "N0", "\n", "stack", "=", "ims", "\n", "", "elif", "sub_sample", "!=", "None", "and", "rand", "==", "False", ":", "\n", "            ", "N", "=", "sub_sample", "\n", "stack", "=", "ims", "[", ":", "N", "]", "\n", "", "elif", "sub_sample", "!=", "None", "and", "rand", "==", "True", ":", "\n", "            ", "N", "=", "sub_sample", "\n", "idx", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "N", ")", ",", "sub_sample", ",", "replace", "=", "False", ")", "\n", "stack", "=", "ims", "[", "idx", "]", "\n", "", "height", "=", "np", ".", "ceil", "(", "float", "(", "N", ")", "/", "width", ")", ".", "astype", "(", "np", ".", "uint16", ")", "\n", "plt", ".", "rcParams", "[", "'figure.figsize'", "]", "=", "(", "18", ",", "(", "18", "/", "width", ")", "*", "height", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "plt", ".", "subplot", "(", "height", ",", "width", ",", "i", "+", "1", ")", "\n", "im", "=", "stack", "[", "i", "]", "\n", "show", "(", "im", ",", "now", "=", "False", ",", "fig_size", "=", "None", ")", "\n", "", "if", "save_name", "!=", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "save_name", ")", "\n", "", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.remove_zeros": [[105, 115], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "remove_zeros", "(", "I", ")", ":", "\n", "        ", "\"\"\"\n        Remove zeros, replace with 1's.\n        :param I: uint8 array\n        :return:\n        \"\"\"", "\n", "mask", "=", "(", "I", "==", "0", ")", "\n", "I", "[", "mask", "]", "=", "1", "\n", "return", "I", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.RGB_to_OD": [[116, 125], ["StainingUtils.remove_zeros", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.remove_zeros", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log"], ["", "@", "staticmethod", "\n", "def", "RGB_to_OD", "(", "I", ")", ":", "\n", "        ", "\"\"\"\n        Convert from RGB to optical density\n        :param I:\n        :return:\n        \"\"\"", "\n", "I", "=", "StainingUtils", ".", "remove_zeros", "(", "I", ")", "\n", "return", "-", "1", "*", "np", ".", "log", "(", "I", "/", "255", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.OD_to_RGB": [[126, 134], ["numpy.exp"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "OD_to_RGB", "(", "OD", ")", ":", "\n", "        ", "\"\"\"\n        Convert from optical density to RGB\n        :param OD:\n        :return:\n        \"\"\"", "\n", "return", "(", "255", "*", "np", ".", "exp", "(", "-", "1", "*", "OD", ")", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.normalize_rows": [[135, 143], ["numpy.linalg.norm"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "normalize_rows", "(", "A", ")", ":", "\n", "        ", "\"\"\"\n        Normalize rows of an array\n        :param A:\n        :return:\n        \"\"\"", "\n", "return", "A", "/", "np", ".", "linalg", ".", "norm", "(", "A", ",", "axis", "=", "1", ")", "[", ":", ",", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.notwhite_mask": [[144, 155], ["cv2.cvtColor"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "notwhite_mask", "(", "I", ",", "thresh", "=", "0.8", ")", ":", "\n", "        ", "\"\"\"\n        Get a binary mask where true denotes 'not white'\n        :param I:\n        :param thresh:\n        :return:\n        \"\"\"", "\n", "I_LAB", "=", "cv", ".", "cvtColor", "(", "I", ",", "cv", ".", "COLOR_RGB2LAB", ")", "\n", "L", "=", "I_LAB", "[", ":", ",", ":", ",", "0", "]", "/", "255.0", "\n", "return", "(", "L", "<", "thresh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.sign": [[156, 169], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "sign", "(", "x", ")", ":", "\n", "        ", "\"\"\"\n        Returns the sign of x\n        :param x:\n        :return:\n        \"\"\"", "\n", "if", "x", ">", "0", ":", "\n", "            ", "return", "+", "1", "\n", "", "elif", "x", "<", "0", ":", "\n", "            ", "return", "-", "1", "\n", "", "elif", "x", "==", "0", ":", "\n", "            ", "return", "0", "", "", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser_test.TestJSONParser.test_JSONParser_constructible": [[6, 10], ["JSONParser.JSONParser.JSONParser", "JSONParser_test.TestJSONParser.assertIsNotNone"], "methods", ["None"], ["    ", "def", "test_JSONParser_constructible", "(", "self", ")", ":", "\n", "        ", "path", "=", "'c:/dummy_path/'", "\n", "json_parser", "=", "JSONParser", "(", "path", ")", "\n", "self", ".", "assertIsNotNone", "(", "json_parser", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser_test.TestJSONParser.test_if_json_is_loaded": [[11, 18], ["JSONParser.JSONParser.JSONParser", "JSONParser.JSONParser.JSONParser.parse", "JSONParser.JSONParser.JSONParser.get_setting", "JSONParser_test.TestJSONParser.assertIsNotNone"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.parse", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_setting"], ["", "def", "test_if_json_is_loaded", "(", "self", ")", ":", "\n", "        ", "path", "=", "'./JSONParser/test.json'", "\n", "json_parser", "=", "JSONParser", "(", "path", ")", "\n", "json_parser", ".", "parse", "(", ")", "\n", "setting", "=", "json_parser", ".", "get_setting", "(", ")", "\n", "self", ".", "assertIsNotNone", "(", "setting", ",", "\"There is a json file and \\\n            have been loaded\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser_test.TestJSONParser.test_if_item_can_be_retrieved": [[19, 25], ["JSONParser.JSONParser.JSONParser", "JSONParser.JSONParser.JSONParser.parse", "JSONParser.JSONParser.JSONParser.get_item", "JSONParser_test.TestJSONParser.assertEqual"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.parse", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_item"], ["", "def", "test_if_item_can_be_retrieved", "(", "self", ")", ":", "\n", "        ", "path", "=", "'./JSONParser/test.json'", "\n", "json_parser", "=", "JSONParser", "(", "path", ")", "\n", "json_parser", ".", "parse", "(", ")", "\n", "item", "=", "json_parser", ".", "get_item", "(", "'parent'", ",", "'child'", ")", "\n", "self", ".", "assertEqual", "(", "item", ",", "'item'", ",", "\"The item has been retrieved succefully!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser_test.TestJSONParser.test_if_can_work_with_nested_dict": [[26, 33], ["JSONParser.JSONParser.JSONParser", "JSONParser.JSONParser.JSONParser.parse", "JSONParser.JSONParser.JSONParser.get_item", "JSONParser_test.TestJSONParser.assertEqual"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.parse", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_item"], ["", "def", "test_if_can_work_with_nested_dict", "(", "self", ")", ":", "\n", "        ", "path", "=", "'./JSONParser/nested.json'", "\n", "json_parser", "=", "JSONParser", "(", "path", ")", "\n", "json_parser", ".", "parse", "(", ")", "\n", "item", "=", "json_parser", ".", "get_item", "(", "'breakhis_label_to_integer'", ",", "'benign_tubular_adenoma'", ")", "\n", "\n", "self", ".", "assertEqual", "(", "item", ",", "4", ",", "\"The item has been retrieved succefully!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser_test.TestJSONParser.test_if_can_get_a_dict": [[34, 41], ["JSONParser.JSONParser.JSONParser", "JSONParser.JSONParser.JSONParser.parse", "JSONParser.JSONParser.JSONParser.get_dict", "JSONParser_test.TestJSONParser.assertEqual"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.parse", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_dict"], ["", "def", "test_if_can_get_a_dict", "(", "self", ")", ":", "\n", "        ", "path", "=", "'./JSONParser/nested.json'", "\n", "json_parser", "=", "JSONParser", "(", "path", ")", "\n", "json_parser", ".", "parse", "(", ")", "\n", "dict_retrieved", "=", "json_parser", ".", "get_dict", "(", "'breakhis_label_to_integer'", ")", "\n", "item", "=", "dict_retrieved", "[", "'benign_tubular_adenoma'", "]", "\n", "self", ".", "assertEqual", "(", "item", ",", "4", ",", "\"The item has been retrieved succefully!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.__init__": [[5, 8], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "self", ".", "__file_path", "=", "file_path", "\n", "self", ".", "setting", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.parse": [[9, 12], ["open", "json.load"], "methods", ["None"], ["", "def", "parse", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "__file_path", ")", "as", "json_file", ":", "\n", "            ", "self", ".", "setting", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_setting": [[13, 15], ["None"], "methods", ["None"], ["", "", "def", "get_setting", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "setting", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_item": [[16, 19], ["None"], "methods", ["None"], ["", "def", "get_item", "(", "self", ",", "parent", ",", "child", ")", ":", "\n", "        ", "if", "self", ".", "setting", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "setting", "[", "parent", "]", "[", "child", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_dict": [[20, 23], ["None"], "methods", ["None"], ["", "", "def", "get_dict", "(", "self", ",", "parent", ")", ":", "\n", "        ", "if", "self", ".", "setting", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "setting", "[", "parent", "]", "", "", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer_test.TestReinhardNormalizer.test_if_constructible": [[9, 12], ["ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer", "ReinhardNormalizer_test.TestReinhardNormalizer.assertIsNotNone"], "methods", ["None"], ["    ", "def", "test_if_constructible", "(", "self", ")", ":", "\n", "        ", "reinhard_normalizer", "=", "ReinhardNormalizer", "(", ")", "\n", "self", ".", "assertIsNotNone", "(", "reinhard_normalizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.__init__": [[16, 19], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_means", "=", "None", "\n", "self", ".", "target_stds", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.lab_split": [[20, 33], ["cv2.cvtColor", "I.astype.astype.astype", "cv2.split"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "lab_split", "(", "self", ",", "I", ")", ":", "\n", "        ", "\"\"\"\n        Convert from RGB uint8 to LAB and split into channels\n        :param I: uint8\n        :return:s\n        \"\"\"", "\n", "I", "=", "cv", ".", "cvtColor", "(", "I", ",", "cv", ".", "COLOR_RGB2LAB", ")", "\n", "I", "=", "I", ".", "astype", "(", "np", ".", "float32", ")", "\n", "I1", ",", "I2", ",", "I3", "=", "cv", ".", "split", "(", "I", ")", "\n", "I1", "/=", "2.55", "\n", "I2", "-=", "128.0", "\n", "I3", "-=", "128.0", "\n", "return", "I1", ",", "I2", ",", "I3", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.standardize_brightness": [[34, 41], ["numpy.percentile", "numpy.clip().astype", "numpy.clip"], "methods", ["None"], ["", "def", "standardize_brightness", "(", "self", ",", "I", ")", ":", "\n", "        ", "\"\"\"\n        :param I:\n        :return:\n        \"\"\"", "\n", "p", "=", "np", ".", "percentile", "(", "I", ",", "90", ")", "\n", "return", "np", ".", "clip", "(", "I", "*", "255.0", "/", "p", ",", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.get_mean_std": [[42, 55], ["ReinhardNormalizer.ReinhardNormalizer.lab_split", "cv2.meanStdDev", "cv2.meanStdDev", "cv2.meanStdDev"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.lab_split"], ["", "def", "get_mean_std", "(", "self", ",", "I", ")", ":", "\n", "        ", "\"\"\"\n        Get mean and standard deviation of each channel\n        :param I: uint8\n        :return:\n        \"\"\"", "\n", "I1", ",", "I2", ",", "I3", "=", "self", ".", "lab_split", "(", "I", ")", "\n", "m1", ",", "sd1", "=", "cv", ".", "meanStdDev", "(", "I1", ")", "\n", "m2", ",", "sd2", "=", "cv", ".", "meanStdDev", "(", "I2", ")", "\n", "m3", ",", "sd3", "=", "cv", ".", "meanStdDev", "(", "I3", ")", "\n", "means", "=", "m1", ",", "m2", ",", "m3", "\n", "stds", "=", "sd1", ",", "sd2", ",", "sd3", "\n", "return", "means", ",", "stds", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.merge_back": [[56, 69], ["numpy.clip().astype", "cv2.cvtColor", "numpy.clip", "cv2.merge"], "methods", ["None"], ["", "def", "merge_back", "(", "self", ",", "I1", ",", "I2", ",", "I3", ")", ":", "\n", "        ", "\"\"\"\n        Take seperate LAB channels and merge back to give RGB uint8\n        :param I1:\n        :param I2:\n        :param I3:\n        :return:\n        \"\"\"", "\n", "I1", "*=", "2.55", "\n", "I2", "+=", "128.0", "\n", "I3", "+=", "128.0", "\n", "I", "=", "np", ".", "clip", "(", "cv", ".", "merge", "(", "(", "I1", ",", "I2", ",", "I3", ")", ")", ",", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "return", "cv", ".", "cvtColor", "(", "I", ",", "cv", ".", "COLOR_LAB2RGB", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.fit": [[71, 76], ["ReinhardNormalizer.ReinhardNormalizer.standardize_brightness", "ReinhardNormalizer.ReinhardNormalizer.get_mean_std"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.standardize_brightness", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.get_mean_std"], ["", "def", "fit", "(", "self", ",", "target", ")", ":", "\n", "        ", "target", "=", "self", ".", "standardize_brightness", "(", "target", ")", "\n", "means", ",", "stds", "=", "self", ".", "get_mean_std", "(", "target", ")", "\n", "self", ".", "target_means", "=", "means", "\n", "self", ".", "target_stds", "=", "stds", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.transform": [[77, 85], ["ReinhardNormalizer.ReinhardNormalizer.standardize_brightness", "ReinhardNormalizer.ReinhardNormalizer.lab_split", "ReinhardNormalizer.ReinhardNormalizer.get_mean_std", "ReinhardNormalizer.ReinhardNormalizer.merge_back"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.standardize_brightness", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.lab_split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.get_mean_std", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.merge_back"], ["", "def", "transform", "(", "self", ",", "I", ")", ":", "\n", "        ", "I", "=", "self", ".", "standardize_brightness", "(", "I", ")", "\n", "I1", ",", "I2", ",", "I3", "=", "self", ".", "lab_split", "(", "I", ")", "\n", "means", ",", "stds", "=", "self", ".", "get_mean_std", "(", "I", ")", "\n", "norm1", "=", "(", "(", "I1", "-", "means", "[", "0", "]", ")", "*", "(", "self", ".", "target_stds", "[", "0", "]", "/", "stds", "[", "0", "]", ")", ")", "+", "self", ".", "target_means", "[", "0", "]", "\n", "norm2", "=", "(", "(", "I2", "-", "means", "[", "1", "]", ")", "*", "(", "self", ".", "target_stds", "[", "1", "]", "/", "stds", "[", "1", "]", ")", ")", "+", "self", ".", "target_means", "[", "1", "]", "\n", "norm3", "=", "(", "(", "I3", "-", "means", "[", "2", "]", ")", "*", "(", "self", ".", "target_stds", "[", "2", "]", "/", "stds", "[", "2", "]", ")", ")", "+", "self", ".", "target_means", "[", "2", "]", "\n", "return", "self", ".", "merge_back", "(", "norm1", ",", "norm2", ",", "norm3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.TextFIleGenerator.TextFIleGenerator_test.TestTextFileGenerator.test_if_constructible": [[5, 8], ["TextFileGenerator.TextFileGenerator.TextFileGenerator"], "methods", ["None"], ["    ", "def", "test_if_constructible", "(", "self", ")", ":", "\n", "        ", "csv_path", "=", "'D:/Datasets/square_breast/dataset_dataframe.csv'", "\n", "text_file_generator", "=", "TextFileGenerator", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.__init__": [[6, 12], ["JSONParser.JSONParser.JSONParser.JSONParser", "DataframeSplitter.DataframeSplitter.json_parser.parse", "DataframeSplitter.DataframeSplitter.json_parser.get_dict"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.parse", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.JSONParser.JSONParser.JSONParser.get_dict"], ["    ", "def", "__init__", "(", "self", ",", "split_ratio", ",", "dataframe", ")", ":", "\n", "        ", "self", ".", "test_prop", ",", "self", ".", "train_prop", ",", "self", ".", "validation_prop", "=", "split_ratio", "\n", "self", ".", "dataframe", "=", "dataframe", "\n", "self", ".", "json_parser", "=", "JSONParser", ".", "JSONParser", "(", "'setting.json'", ")", "\n", "self", ".", "json_parser", ".", "parse", "(", ")", "\n", "self", ".", "label_to_int_dict", "=", "self", ".", "json_parser", ".", "get_dict", "(", "'breakhis_label_to_integer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split": [[13, 27], ["DataframeSplitter.DataframeSplitter.dataframe.sample", "DataframeSplitter.DataframeSplitter.sort_values", "DataframeSplitter.DataframeSplitter.sort_values.magnification.unique", "numpy.split", "test.sort_values.sort_values.sort_values", "train.sort_values.sort_values.sort_values", "validation.sort_values.sort_values.sort_values", "DataframeSplitter.DataframeSplitter.log", "DataframeSplitter.DataframeSplitter.log", "DataframeSplitter.DataframeSplitter.log", "int", "int", "str", "str", "str", "len", "len"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log"], ["", "def", "split", "(", "self", ")", ":", "\n", "        ", "shuffled_dataframe", "=", "self", ".", "dataframe", ".", "sample", "(", "frac", "=", "1", ")", "\n", "reorganized", "=", "shuffled_dataframe", ".", "sort_values", "(", "by", "=", "[", "'magnification'", "]", ")", "\n", "magnification_list", "=", "reorganized", ".", "magnification", ".", "unique", "(", ")", "\n", "for", "magnification", "in", "magnification_list", ":", "\n", "\n", "            ", "magnification_dataframe", "=", "reorganized", "[", "reorganized", "[", "'magnification'", "]", "==", "magnification", "]", "\n", "test", ",", "train", ",", "validation", "=", "np", ".", "split", "(", "magnification_dataframe", ",", "[", "int", "(", "self", ".", "test_prop", "*", "len", "(", "magnification_dataframe", ")", ")", ",", "int", "(", "(", "self", ".", "test_prop", "+", "self", ".", "train_prop", ")", "*", "len", "(", "magnification_dataframe", ")", ")", "]", ")", "\n", "test", "=", "test", ".", "sort_values", "(", "by", "=", "[", "'label'", "]", ")", "\n", "train", "=", "train", ".", "sort_values", "(", "by", "=", "[", "'label'", "]", ")", "\n", "validation", "=", "validation", ".", "sort_values", "(", "by", "=", "[", "'label'", "]", ")", "\n", "self", ".", "log", "(", "str", "(", "magnification", ")", "+", "'_test.txt'", ",", "test", ")", "\n", "self", ".", "log", "(", "str", "(", "magnification", ")", "+", "'_train.txt'", ",", "train", ")", "\n", "self", ".", "log", "(", "str", "(", "magnification", ")", "+", "'_validation.txt'", ",", "validation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log": [[29, 34], ["open", "dataframe.iterrows", "open.close", "open.write", "str", "row.path.split"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "", "def", "log", "(", "self", ",", "logger_name", ",", "dataframe", ")", ":", "\n", "        ", "logger", "=", "open", "(", "logger_name", ",", "\"w\"", ")", "\n", "for", "_", ",", "row", "in", "dataframe", ".", "iterrows", "(", ")", ":", "\n", "            ", "logger", ".", "write", "(", "'/'", ".", "join", "(", "row", ".", "path", ".", "split", "(", "'/'", ")", "[", "-", "3", ":", "]", ")", "+", "' '", "+", "str", "(", "self", ".", "label_to_int_dict", "[", "row", ".", "label", "]", ")", "+", "\"\\n\"", ")", "\n", "", "logger", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter_test.TestDataframeSplitter.test_if_constructible": [[15, 20], ["pandas.read_csv", "DataframeSplitter.DataframeSplitter.DataframeSplitter", "DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["    ", "def", "test_if_constructible", "(", "self", ")", ":", "\n", "        ", "dataframe_path", "=", "'D:/Datasets/masf_organized_breakhis_dataset/Best_after_normalization/organized_dataframe.csv'", "\n", "df", "=", "pd", ".", "read_csv", "(", "dataframe_path", ")", "\n", "data_frame_splitter", "=", "DataframeSplitter", "(", "[", "0.45", ",", "0.45", ",", "0.1", "]", ",", "df", ")", "\n", "data_frame_splitter", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.__init__": [[17, 24], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset_root", ",", "normalizer", ",", "normalization_reference_image", ",", "path_extension", ",", "image_extension", ")", ":", "\n", "        ", "self", ".", "normalizer", "=", "normalizer", "\n", "self", ".", "dataset_root", "=", "dataset_root", "\n", "self", ".", "normalization_reference_image", "=", "normalization_reference_image", "\n", "self", ".", "path_extension", "=", "path_extension", "\n", "self", ".", "img_extension", "=", "image_extension", "\n", "self", ".", "dataframe", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.build_dataframe": [[25, 35], ["pandas.DataFrame", "img_file.replace", "list", "glob.glob.glob", "[].split", "[].split", "zip", "image_path.split", "image_path.split", "image_path.split", "image_path.split"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "build_dataframe", "(", "self", ")", ":", "\n", "        ", "images_path", "=", "[", "img_file", ".", "replace", "(", "\"\\\\\"", ",", "\"/\"", ")", "for", "img_file", "in", "glob", "(", "self", ".", "dataset_root", "+", "self", ".", "path_extension", "+", "self", ".", "img_extension", ")", "]", "\n", "\"\"\"\n        you will face with some magic numbers. These are specific to BreakHis dataset. The class has been designed for working with this dataset.\n        \"\"\"", "\n", "images_magnification", "=", "[", "(", "image_path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ")", ".", "split", "(", "'-'", ")", "[", "-", "2", "]", "for", "image_path", "in", "images_path", "]", "\n", "images_class", "=", "[", "image_path", ".", "split", "(", "'/'", ")", "[", "3", "]", "+", "'_'", "+", "image_path", ".", "split", "(", "'/'", ")", "[", "5", "]", "for", "image_path", "in", "images_path", "]", "\n", "slides_name", "=", "[", "(", "image_path", ".", "split", "(", "'/'", ")", "[", "6", "]", ")", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", "for", "image_path", "in", "images_path", "]", "\n", "self", ".", "dataframe", "=", "pd", ".", "DataFrame", "(", "list", "(", "zip", "(", "images_path", ",", "images_magnification", ",", "images_class", ",", "slides_name", ")", ")", ",", "\n", "columns", "=", "[", "'path'", ",", "'magnification'", ",", "'label'", ",", "'slide_name'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.build_organized_dataframe": [[36, 46], ["pandas.DataFrame", "img_file.replace", "list", "glob.glob.glob", "[].split", "image_path.split", "zip", "[].split", "image_path.split", "[].split", "image_path.split"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "build_organized_dataframe", "(", "self", ")", ":", "\n", "        ", "images_path", "=", "[", "img_file", ".", "replace", "(", "\"\\\\\"", ",", "\"/\"", ")", "for", "img_file", "in", "glob", "(", "self", ".", "dataset_root", "+", "self", ".", "path_extension", "+", "self", ".", "img_extension", ")", "]", "\n", "\"\"\"\n        you will face with some magic numbers. These are specific to BreakHis dataset. The class has been designed for working with this dataset.\n        \"\"\"", "\n", "images_magnification", "=", "[", "(", "image_path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ")", ".", "split", "(", "'-'", ")", "[", "-", "2", "]", "for", "image_path", "in", "images_path", "]", "\n", "images_class", "=", "[", "image_path", ".", "split", "(", "'/'", ")", "[", "5", "]", "for", "image_path", "in", "images_path", "]", "\n", "slides_name", "=", "[", "'-'", ".", "join", "(", "(", "(", "(", "image_path", ".", "split", "(", "'/'", ")", "[", "6", "]", ")", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", ".", "split", "(", "'-'", ")", "[", ":", "-", "2", "]", ")", ")", "for", "image_path", "in", "images_path", "]", "\n", "self", ".", "dataframe", "=", "pd", ".", "DataFrame", "(", "list", "(", "zip", "(", "images_path", ",", "images_magnification", ",", "images_class", ",", "slides_name", ")", ")", ",", "\n", "columns", "=", "[", "'path'", ",", "'magnification'", ",", "'label'", ",", "'slide_name'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.set_dataframe": [[47, 49], ["None"], "methods", ["None"], ["", "def", "set_dataframe", "(", "self", ",", "dataframe", ")", ":", "\n", "        ", "self", ".", "dataframe", "=", "dataframe", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.write_dataframe": [[50, 52], ["BreakHisOrganizer.BreakHisOrganizer.dataframe.to_csv"], "methods", ["None"], ["", "def", "write_dataframe", "(", "self", ",", "dataframe_name", ")", ":", "\n", "        ", "self", ".", "dataframe", ".", "to_csv", "(", "self", ".", "dataset_root", "+", "dataframe_name", "+", "'.csv'", ",", "index", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.__create_new_dataset_directory": [[53, 56], ["pathlib.Path().mkdir", "pathlib.Path"], "methods", ["None"], ["", "def", "__create_new_dataset_directory", "(", "self", ",", "new_dataset_name", ")", ":", "\n", "        ", "Path", "(", "self", ".", "dataset_root", "+", "new_dataset_name", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "return", "self", ".", "dataset_root", "+", "new_dataset_name", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.save_dataset": [[57, 77], ["BreakHisOrganizer.BreakHisOrganizer.normalizer.fit", "BreakHisOrganizer.BreakHisOrganizer.__create_new_dataset_directory", "pandas.read_csv.iterrows", "pandas.read_csv", "pathlib.Path().mkdir", "row.path.split", "os.path.exists", "str", "pathlib.Path", "StainingUtils.StainingUtils.StainingUtils.read_image", "BreakHisOrganizer.BreakHisOrganizer.normalizer.transform", "cv2.imwrite", "shutil.copy", "str"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.fit", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.__create_new_dataset_directory", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.breakhis_data_preparator.StainingUtils.StainingUtils.read_image", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.ReinhardNormalizer.ReinhardNormalizer.ReinhardNormalizer.transform"], ["", "def", "save_dataset", "(", "self", ",", "new_dataset_name", ",", "stain_normalization", "=", "True", ",", "load_from_csv", "=", "False", ",", "dataframe_name", "=", "'dataset_dataframe.csv'", ")", ":", "\n", "        ", "self", ".", "normalizer", ".", "fit", "(", "self", ".", "normalization_reference_image", ")", "\n", "new_dataset_directory", "=", "self", ".", "__create_new_dataset_directory", "(", "new_dataset_name", ")", "\n", "\n", "if", "load_from_csv", ":", "\n", "            ", "df", "=", "pd", ".", "read_csv", "(", "self", ".", "dataset_root", "+", "dataframe_name", ")", "\n", "", "else", ":", "\n", "            ", "df", "=", "self", ".", "dataframe", "\n", "\n", "", "for", "_", ",", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "            ", "image_name", "=", "row", ".", "path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "current_directory", "=", "new_dataset_directory", "+", "'/'", "+", "str", "(", "row", ".", "magnification", ")", "+", "'/'", "+", "str", "(", "row", ".", "label", ")", "+", "'/'", "\n", "Path", "(", "current_directory", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "if", "not", "path", ".", "exists", "(", "current_directory", "+", "image_name", ")", ":", "\n", "                ", "if", "stain_normalization", ":", "\n", "                    ", "source_image", "=", "StainingUtils", ".", "read_image", "(", "row", ".", "path", ")", "\n", "output_image", "=", "self", ".", "normalizer", ".", "transform", "(", "source_image", ")", "\n", "cv2", ".", "imwrite", "(", "current_directory", "+", "image_name", ",", "output_image", ")", "\n", "", "else", ":", "\n", "                    ", "shutil", ".", "copy", "(", "row", ".", "path", ",", "current_directory", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer_test.TestBreakHisOrganizer.test_if_constructible": [[23, 26], ["BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer", "BreakHisOrganizer_test.TestBreakHisOrganizer.assertIsNotNone"], "methods", ["None"], ["    ", "def", "test_if_constructible", "(", "self", ")", ":", "\n", "        ", "break_his_organizer", "=", "BreakHisOrganizer", "(", "'.'", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", ")", "\n", "self", ".", "assertIsNotNone", "(", "break_his_organizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer_test.TestBreakHisOrganizer.test_how_dataframe_is_loaded": [[27, 41], ["BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer", "BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.build_organized_dataframe"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer.BreakHisOrganizer.build_organized_dataframe"], ["", "def", "test_how_dataframe_is_loaded", "(", "self", ")", ":", "\n", "        ", "dataset_root", "=", "'D:/Datasets/masf_organized_breakhis_dataset/Best_after_normalization/'", "\n", "reinhard_normalizer", "=", "None", "\n", "normalization_ref_image", "=", "None", "\n", "path_extension", "=", "'*/*/*'", "\n", "image_extension", "=", "'.png'", "\n", "\n", "break_his_organizer_new_dataset", "=", "BreakHisOrganizer", "(", "dataset_root", ",", "\n", "reinhard_normalizer", ",", "\n", "normalization_ref_image", ",", "\n", "path_extension", ",", "\n", "image_extension", ")", "\n", "\n", "break_his_organizer_new_dataset", ".", "build_organized_dataframe", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.BreakHisOrganizer.BreakHisOrganizer_test.mock_ReinhardNormalizer": [[17, 21], ["unittest.mock.patch", "print"], "function", ["None"], ["@", "mock", ".", "patch", "(", "\"ReinhardNormalizer.ReinhardNormalizer\"", ")", "\n", "def", "mock_ReinhardNormalizer", "(", "mock_class", ")", ":", "\n", "\n", "    ", "print", "(", "mock_class", ".", "return_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.lib.construct_unet_weights": [[7, 66], ["tensorflow.contrib.layers.xavier_initializer_conv2d", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable"], "function", ["None"], ["def", "construct_unet_weights", "(", "self", ")", ":", "\n", "\n", "    ", "weights", "=", "{", "}", "\n", "conv_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer_conv2d", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'conv1'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv1_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "5", ",", "5", ",", "3", ",", "16", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv1_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "16", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv2'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv2_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "5", ",", "5", ",", "16", ",", "32", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv2_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "32", "]", ")", "\n", "\n", "## Network has downsample here", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv3'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv3_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "32", ",", "64", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv3_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "64", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv4'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv4_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "64", ",", "64", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv4_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "64", "]", ")", "\n", "\n", "## Network has downsample here", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv5'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv5_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "64", ",", "128", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv5_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "128", "]", ")", "\n", "\n", "# with tf.variable_scope('conv6') as scope:", "\n", "#     weights['conv6_weights'] = tf.get_variable('weights', shape=[3, 3, 128, 128], initializer=conv_initializer)", "\n", "#     weights['conv6_biases'] = tf.get_variable('biases', [128])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'deconv1'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'deconv1_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "64", ",", "128", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv7'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv7_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "64", ",", "64", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv7_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "64", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv8'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv8_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "64", ",", "64", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv8_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "64", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'deconv2'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'deconv2_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "32", ",", "64", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "\n", "# with tf.variable_scope('conv9') as scope:", "\n", "#     weights['conv9_weights'] = tf.get_variable('weights', shape=[3, 3, 32, 32], initializer=conv_initializer)", "\n", "#     weights['conv9_biases'] = tf.get_variable('biases', [32])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv10'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'conv10_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "32", ",", "32", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv10_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "32", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'output'", ")", "as", "scope", ":", "\n", "        ", "weights", "[", "'output_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "32", ",", "4", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "\n", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.lib.forward_unet": [[68, 101], ["lib.conv_block", "lib.conv_block", "max_pool", "lib.conv_block", "lib.conv_block", "max_pool", "lib.conv_block", "lib.deconv_block", "lib.conv_block", "lib.conv_block", "lib.deconv_block", "lib.conv_block", "tensorflow.nn.conv2d", "tensorflow.nn.softmax", "tensorflow.argmax"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.lib.deconv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.lib.deconv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block"], ["", "def", "forward_unet", "(", "self", ",", "inp", ",", "weights", ")", ":", "\n", "\n", "    ", "self", ".", "conv1", "=", "conv_block", "(", "inp", ",", "weights", "[", "'conv1_weights'", "]", ",", "weights", "[", "'conv1_biases'", "]", ")", "\n", "self", ".", "conv2", "=", "conv_block", "(", "self", ".", "conv1", ",", "weights", "[", "'conv2_weights'", "]", ",", "weights", "[", "'conv2_biases'", "]", ")", "\n", "self", ".", "pool2", "=", "max_pool", "(", "self", ".", "conv2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "self", ".", "conv3", "=", "conv_block", "(", "self", ".", "pool2", ",", "weights", "[", "'conv3_weights'", "]", ",", "weights", "[", "'conv3_biases'", "]", ")", "\n", "self", ".", "conv4", "=", "conv_block", "(", "self", ".", "conv3", ",", "weights", "[", "'conv4_weights'", "]", ",", "weights", "[", "'conv4_biases'", "]", ")", "\n", "self", ".", "pool4", "=", "max_pool", "(", "self", ".", "conv4", ",", "2", ",", "2", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "self", ".", "conv5", "=", "conv_block", "(", "self", ".", "pool4", ",", "weights", "[", "'conv5_weights'", "]", ",", "weights", "[", "'conv5_biases'", "]", ")", "\n", "# self.conv6 = conv_block(self.conv5, weights['conv6_weights'], weights['conv6_biases'])", "\n", "\n", "## add upsampling, meanwhile, channel number is reduced to half", "\n", "# self.up1 = deconv_block(self.conv6, weights['deconv1_weights'])", "\n", "self", ".", "up1", "=", "deconv_block", "(", "self", ".", "conv5", ",", "weights", "[", "'deconv1_weights'", "]", ")", "\n", "\n", "self", ".", "conv7", "=", "conv_block", "(", "self", ".", "up1", ",", "weights", "[", "'conv7_weights'", "]", ",", "weights", "[", "'conv7_biases'", "]", ")", "\n", "self", ".", "conv8", "=", "conv_block", "(", "self", ".", "conv7", ",", "weights", "[", "'conv8_weights'", "]", ",", "weights", "[", "'conv8_biases'", "]", ")", "\n", "\n", "## add upsampling, meanwhile, channel number is reduced to half", "\n", "self", ".", "up2", "=", "deconv_block", "(", "self", ".", "conv8", ",", "weights", "[", "'deconv2_weights'", "]", ")", "\n", "\n", "# self.conv9 = conv_block(self.up2, weights['conv9_weights'], weights['conv9_biases'])", "\n", "# self.conv10 = conv_block(self.conv9, weights['conv10_weights'], weights['conv10_biases'])", "\n", "self", ".", "conv10", "=", "conv_block", "(", "self", ".", "up2", ",", "weights", "[", "'conv10_weights'", "]", ",", "weights", "[", "'conv10_biases'", "]", ")", "\n", "\n", "self", ".", "logits", "=", "tf", ".", "nn", ".", "conv2d", "(", "self", ".", "conv10", ",", "weights", "[", "'output_weights'", "]", ",", "strides", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "padding", "=", "'SAME'", ")", "\n", "\n", "self", ".", "pred_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "logits", ")", "# shape [batch, w, h, num_classes]", "\n", "self", ".", "pred_compact", "=", "tf", ".", "argmax", "(", "self", ".", "pred_prob", ",", "axis", "=", "-", "1", ")", "# shape [batch, w, h]", "\n", "\n", "return", "self", ".", "conv10", ",", "self", ".", "logits", ",", "self", ".", "pred_prob", ",", "self", ".", "pred_compact", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.lib.conv_block": [[104, 110], ["tensorflow.nn.conv2d", "tensorflow.nn.bias_add", "tensorflow.nn.leaky_relu"], "function", ["None"], ["", "def", "conv_block", "(", "inp", ",", "cweight", ",", "bweight", ")", ":", "\n", "    ", "\"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"", "\n", "conv", "=", "tf", ".", "nn", ".", "conv2d", "(", "inp", ",", "cweight", ",", "strides", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "padding", "=", "'SAME'", ")", "\n", "conv", "=", "tf", ".", "nn", ".", "bias_add", "(", "conv", ",", "bweight", ")", "\n", "relu", "=", "tf", ".", "nn", ".", "leaky_relu", "(", "conv", ")", "\n", "return", "relu", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.lib.deconv_block": [[112, 118], ["inp.get_shape().as_list", "tensorflow.stack", "tensorflow.nn.conv2d_transpose", "inp.get_shape"], "function", ["None"], ["", "def", "deconv_block", "(", "inp", ",", "cweight", ")", ":", "\n", "# x_shape = tf.shape(inp)", "\n", "    ", "x_shape", "=", "inp", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "output_shape", "=", "tf", ".", "stack", "(", "[", "x_shape", "[", "0", "]", ",", "x_shape", "[", "1", "]", "*", "2", ",", "x_shape", "[", "2", "]", "*", "2", ",", "x_shape", "[", "3", "]", "//", "2", "]", ")", "\n", "deconv", "=", "tf", ".", "nn", ".", "conv2d_transpose", "(", "inp", ",", "cweight", ",", "output_shape", ",", "strides", "=", "[", "1", ",", "2", ",", "2", ",", "1", "]", ",", "padding", "=", "'SAME'", ")", "\n", "return", "deconv", "\n", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.data_generator_brain.ImageDataGenerator.__init__": [[14, 71], ["data_generator_brain.ImageDataGenerator._read_txt_file", "tensorflow.data.Dataset.from_tensor_slices", "data.map.map.batch", "data_generator_brain.ImageDataGenerator._shuffle_lists", "tensorflow.py_func", "tensorflow.py_func", "data.map.map.map", "data.map.map.shuffle", "data.map.map.map", "ValueError"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._read_txt_file", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._shuffle_lists"], ["def", "__init__", "(", "self", ",", "txt_file", ",", "mode", ",", "batch_size", ",", "num_classes", ",", "shuffle", "=", "True", ",", "buffer_size", "=", "300", ")", ":", "\n", "        ", "\"\"\"Create a new ImageDataGenerator.\n\n        Recieves a path string to a text file, which consists of many lines,\n        where each line has first a path string to an image and separated by\n        a space an integer, referring to the class number. Using this data,\n        this class will create TensorFlow datasets, that can be used to train\n        e.g. a convolutional neural network.\n\n        Args:\n            txt_file: Path to the text file.\n            mode: Either 'training' or 'validation'. Depending on this value, different parsing functions will be used.\n            batch_size: Number of images per batch.\n            num_classes: Number of classes in the dataset.\n            shuffle: Wether or not to shuffle the data in the dataset and the initial file list.\n            buffer_size: Number of images used as buffer for TensorFlows shuffling of the dataset.\n        Raises:\n            ValueError: If an invalid mode is passed.\n        \"\"\"", "\n", "\n", "self", ".", "txt_file", "=", "txt_file", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "# retrieve the data from the text file", "\n", "self", ".", "_read_txt_file", "(", ")", "\n", "self", ".", "img_paths", "=", "self", ".", "img_paths", "[", ":", "-", "1", "]", "\n", "self", ".", "data_size", "=", "self", ".", "data_size", "-", "1", "\n", "\n", "# initial shuffling of the file and label lists (together!)", "\n", "if", "shuffle", ":", "\n", "            ", "self", ".", "_shuffle_lists", "(", ")", "\n", "\n", "# convert lists to TF tensor", "\n", "# self.img_paths = convert_to_tensor(self.img_paths, dtype=dtypes.string)", "\n", "\n", "# create dataset", "\n", "", "data", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "self", ".", "img_paths", ")", "\n", "\n", "# patch_size = [128,128]", "\n", "self", ".", "_parse_function_train", "=", "lambda", "filename", ":", "tf", ".", "py_func", "(", "self", ".", "_extract_patch", ",", "[", "filename", "]", ",", "[", "tf", ".", "float32", ",", "tf", ".", "float32", "]", ")", "\n", "self", ".", "_parse_function_inference", "=", "lambda", "filename", ":", "tf", ".", "py_func", "(", "self", ".", "_extract_patch", ",", "[", "filename", "]", ",", "[", "tf", ".", "float32", ",", "tf", ".", "float32", "]", ")", "\n", "# distinguish between train/infer. when calling the parsing functions", "\n", "if", "mode", "==", "'training'", ":", "\n", "            ", "data", "=", "data", ".", "map", "(", "self", ".", "_parse_function_train", ",", "num_parallel_calls", "=", "8", ")", "\n", "", "elif", "mode", "==", "'inference'", ":", "\n", "            ", "data", "=", "data", ".", "map", "(", "self", ".", "_parse_function_inference", ",", "num_parallel_calls", "=", "8", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid mode '%s'.\"", "%", "(", "mode", ")", ")", "\n", "\n", "# shuffle the first `buffer_size` elements of the dataset", "\n", "", "if", "shuffle", ":", "\n", "            ", "data", "=", "data", ".", "shuffle", "(", "buffer_size", "=", "buffer_size", ")", "\n", "\n", "# create a new dataset with batches of images", "\n", "", "data", "=", "data", ".", "batch", "(", "batch_size", ")", "\n", "\n", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.data_generator_brain.ImageDataGenerator._read_txt_file": [[72, 81], ["open", "f.readlines", "data_generator_brain.ImageDataGenerator.img_paths.append"], "methods", ["None"], ["", "def", "_read_txt_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"Read the content of the text file and store it into lists.\"\"\"", "\n", "self", ".", "img_paths", "=", "[", "]", "\n", "self", ".", "data_size", "=", "0", "\n", "with", "open", "(", "self", ".", "txt_file", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "self", ".", "img_paths", ".", "append", "(", "line", "[", ":", "-", "1", "]", ")", "\n", "self", ".", "data_size", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.data_generator_brain.ImageDataGenerator._shuffle_lists": [[82, 89], ["numpy.random.permutation", "data_generator_brain.ImageDataGenerator.img_paths.append"], "methods", ["None"], ["", "", "", "def", "_shuffle_lists", "(", "self", ")", ":", "\n", "        ", "\"\"\"Conjoined shuffling of the list of paths and labels.\"\"\"", "\n", "path", "=", "self", ".", "img_paths", "\n", "permutation", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "data_size", ")", "\n", "self", ".", "img_paths", "=", "[", "]", "\n", "for", "i", "in", "permutation", ":", "\n", "            ", "self", ".", "img_paths", ".", "append", "(", "path", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.data_generator_brain.ImageDataGenerator._extract_patch": [[90, 99], ["scipy.loadmat", "data_generator_brain.ImageDataGenerator._label_decomp", "image.astype", "data_generator_brain.ImageDataGenerator.astype"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.data_generator_brain.ImageDataGenerator._label_decomp"], ["", "", "def", "_extract_patch", "(", "self", ",", "filename", ")", ":", "\n", "\n", "        ", "mat_content", "=", "sio", ".", "loadmat", "(", "filename", ")", "\n", "image", "=", "mat_content", "[", "'image'", "]", "\n", "image", "=", "image", "[", "34", ":", "-", "35", ",", "16", ":", "-", "17", ",", ":", "]", "# [18:-19,1:,:] is 192*192", "\n", "label", "=", "mat_content", "[", "'label'", "]", "\n", "label", "=", "label", "[", "34", ":", "-", "35", ",", "16", ":", "-", "17", ",", "0", "]", "\n", "mask", "=", "self", ".", "_label_decomp", "(", "label", ")", "\n", "return", "image", ".", "astype", "(", "np", ".", "float32", ")", ",", "mask", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.medical.data_generator_brain.ImageDataGenerator._label_decomp": [[100, 114], ["xrange", "numpy.stack", "numpy.zeros", "one_hot.append"], "methods", ["None"], ["", "def", "_label_decomp", "(", "self", ",", "label_vol", ")", ":", "\n", "        ", "\"\"\"\n        decompose label for softmax classifier\n        original labels are batchsize * W * H * 1, with label values 0,1,2,3...\n        this function decompse it to one hot, e.g.: 0,0,0,1,0,0 in channel dimension\n        numpy version of tf.one_hot\n        \"\"\"", "\n", "one_hot", "=", "[", "]", "\n", "for", "i", "in", "xrange", "(", "self", ".", "num_classes", ")", ":", "\n", "            ", "_vol", "=", "np", ".", "zeros", "(", "label_vol", ".", "shape", ")", "\n", "_vol", "[", "label_vol", "==", "i", "]", "=", "1", "\n", "one_hot", ".", "append", "(", "_vol", ")", "\n", "\n", "", "return", "np", ".", "stack", "(", "one_hot", ",", "axis", "=", "-", "1", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.main": [[101, 164], ["domain_list.copy", "domain_list.remove", "int", "deep_all.DeepAll.construct_model_train", "deep_all.DeepAll.construct_model_test", "tensorflow.summary.merge_all", "tensorflow.train.Saver", "tensorflow.InteractiveSession", "tensorflow.global_variables_initializer().run", "tensorflow.train.start_queue_runners", "print", "deep_all.DeepAll.load_initial_weights", "os.path.exists", "os.makedirs", "masf_func.MASF", "deep_all.DeepAll", "tensorflow.get_collection", "main.load_network_model", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "main.evaluation_after_training", "str", "str", "tensorflow.global_variables_initializer", "main.train_MASF", "main.train_DeepAll", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_model_train", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_model_test", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.load_initial_weights", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.load_network_model", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_after_training", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.train_MASF", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.train_DeepAll"], ["flags", ".", "DEFINE_bool", "(", "'resume'", ",", "False", ",", "'resume training if there is a model available'", ")", "\n", "flags", ".", "DEFINE_integer", "(", "'which_checkpoint_to_load'", ",", "2300", ",", "\n", "'the checkpoint to be loaded for resuming training or the test phase'", ")", "\n", "flags", ".", "DEFINE_integer", "(", "'summary_interval'", ",", "50", ",", "'frequency for logging training summaries'", ")", "\n", "flags", ".", "DEFINE_integer", "(", "'save_interval'", ",", "50", ",", "'intervals to save model'", ")", "\n", "flags", ".", "DEFINE_integer", "(", "'print_interval'", ",", "50", ",", "'intervals to print out training info'", ")", "\n", "flags", ".", "DEFINE_integer", "(", "'test_print_interval'", ",", "50", ",", "'intervals to test the model'", ")", "\n", "flags", ".", "DEFINE_integer", "(", "'val_print_interval'", ",", "50", ",", "'intervals to test the model'", ")", "\n", "\n", "\n", "##-------------------- main:", "\n", "\n", "def", "main", "(", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "FLAGS", ".", "logdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "FLAGS", ".", "logdir", ")", "\n", "\n", "", "if", "FLAGS", ".", "dataset", "==", "\"pacs\"", ":", "\n", "        ", "domain_list", "=", "[", "'art_painting'", ",", "'cartoon'", ",", "'photo'", ",", "'sketch'", "]", "\n", "", "elif", "FLAGS", ".", "dataset", "==", "\"pathology\"", "or", "FLAGS", ".", "dataset", "==", "\"pathology_binary\"", ":", "\n", "        ", "domain_list", "=", "[", "'40'", ",", "'100'", ",", "'200'", ",", "'400'", "]", "\n", "", "all_domains_list", "=", "domain_list", ".", "copy", "(", ")", "\n", "domain_list", ".", "remove", "(", "FLAGS", ".", "target_domain", ")", "\n", "source_domains_list", "=", "domain_list", "\n", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "exp_string", "=", "'MASF_'", "+", "FLAGS", ".", "target_domain", "+", "'.mbs_'", "+", "str", "(", "FLAGS", ".", "meta_batch_size", ")", "+", "'.inner'", "+", "str", "(", "FLAGS", ".", "inner_lr", ")", "+", "'.outer'", "+", "str", "(", "FLAGS", ".", "outer_lr", ")", "+", "'.clipNorm'", "+", "str", "(", "\n", "FLAGS", ".", "gradients_clip_value", ")", "+", "'.metric'", "+", "str", "(", "FLAGS", ".", "metric_lr", ")", "+", "'.margin'", "+", "str", "(", "FLAGS", ".", "margin", ")", "\n", "", "else", ":", "\n", "        ", "exp_string", "=", "'DeepAll_'", "+", "FLAGS", ".", "target_domain", "+", "'.mbs_'", "+", "str", "(", "FLAGS", ".", "deep_all_batch_size", ")", "+", "'.lr'", "+", "str", "(", "FLAGS", ".", "deep_all_lr", ")", "+", "'.margin'", "+", "str", "(", "FLAGS", ".", "margin", ")", "\n", "\n", "# Constructing model", "\n", "", "FLAGS", ".", "feature_space_dimension", "=", "int", "(", "FLAGS", ".", "feature_space_dimension", ")", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "model", "=", "MASF", "(", "FLAGS", ".", "WEIGHTS_PATH", ",", "FLAGS", ".", "feature_space_dimension", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DeepAll", "(", "FLAGS", ".", "WEIGHTS_PATH", ",", "FLAGS", ".", "feature_space_dimension", ")", "\n", "", "model", ".", "construct_model_train", "(", ")", "\n", "model", ".", "construct_model_test", "(", ")", "\n", "\n", "model", ".", "summ_op", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ")", ",", "\n", "max_to_keep", "=", "None", ")", "# https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver", "\n", "sess", "=", "tf", ".", "InteractiveSession", "(", ")", "\n", "\n", "tf", ".", "global_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "tf", ".", "train", ".", "start_queue_runners", "(", ")", "\n", "\n", "print", "(", "'Loading pretrained weights'", ")", "\n", "model", ".", "load_initial_weights", "(", "sess", ")", "\n", "\n", "resume_itr", "=", "0", "\n", "if", "FLAGS", ".", "resume", "or", "not", "FLAGS", ".", "train", ":", "\n", "        ", "checkpoint_dir", "=", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", "+", "'/'", "+", "str", "(", "\n", "FLAGS", ".", "which_checkpoint_to_load", ")", "+", "'/'", "\n", "load_network_model", "(", "saver_", "=", "saver", ",", "session_", "=", "sess", ",", "checkpoint_dir", "=", "checkpoint_dir", ")", "\n", "resume_itr", "=", "FLAGS", ".", "which_checkpoint_to_load", "+", "1", "\n", "\n", "", "train_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "source_domain", "+", "'_train_kfold.txt'", ")", "for", "source_domain", "in", "\n", "source_domains_list", "]", "\n", "test_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "FLAGS", ".", "target_domain", "+", "'_test_kfold.txt'", ")", "]", "\n", "val_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "filelist_root", ",", "FLAGS", ".", "target_domain", "+", "'_crossval_kfold.txt'", ")", "]", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.train_MASF": [[168, 317], ["range", "tf.data.Iterator.from_structure.make_initializer", "int", "tf.data.Iterator.from_structure.make_initializer", "int", "range", "tensorflow.summary.FileWriter", "tensorflow.device", "range", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "len", "training_init_op.append", "train_batches_per_epoch.append", "numpy.floor", "numpy.floor", "len", "numpy.random.permutation", "range", "range", "range", "numpy.unique", "numpy.unique", "range", "numpy.unique", "numpy.unique", "range", "int", "numpy.concatenate", "numpy.concatenate", "numpy.sum", "numpy.argmax", "output_tensors.extend", "sess.run", "source_losses.append", "source_accuracies.append", "len", "data_generator.ImageDataGenerator", "tr_data_list.append", "train_iterator_list.append", "train_next_list.append", "train_iterator_list[].make_initializer", "int", "len", "numpy.argmax", "numpy.argmax", "numpy.argmax", "numpy.argmax", "print", "print", "print", "print", "print", "print", "os.path.join", "tf.summary.FileWriter.add_summary", "main.save_network_model", "main.evaluation_during_training", "main.evaluation_during_training", "tensorflow.data.Iterator.from_structure", "train_iterator_list[].get_next", "numpy.floor", "sess.run", "sess.run", "sess.run", "RuntimeError", "os.path.exists", "os.makedirs", "open", "fle.write", "sess.run", "RuntimeError", "str", "str", "str", "numpy.mean", "numpy.mean", "str", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.save_network_model", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training"], ["if", "FLAGS", ".", "train", ":", "\n", "        ", "if", "FLAGS", ".", "masf_mode", ":", "\n", "            ", "train_MASF", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file_list", "[", "0", "]", ",", "val_file_list", "[", "0", "]", ",", "resume_itr", ")", "\n", "", "else", ":", "\n", "            ", "train_DeepAll", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file_list", "[", "0", "]", ",", "val_file_list", "[", "0", "]", ",", "\n", "resume_itr", ")", "\n", "", "", "else", ":", "\n", "        ", "evaluation_after_training", "(", "model", ",", "sess", ",", "evaluation_file_list", ",", "evaluation_batch_size", "=", "1", ")", "\n", "\n", "\n", "##-------------------- functions:", "\n", "\n", "", "", "def", "train_MASF", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file", ",", "val_file", ",", "resume_itr", "=", "0", ")", ":", "\n", "    ", "if", "FLAGS", ".", "log", ":", "\n", "        ", "train_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", ",", "sess", ".", "graph", ")", "\n", "", "source_losses", ",", "target_losses", ",", "source_accuracies", ",", "target_accuracies", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Data loaders", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "        ", "tr_data_list", ",", "train_iterator_list", ",", "train_next_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "tr_data", "=", "ImageDataGenerator", "(", "train_file_list", "[", "i", "]", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'training'", ",", "batch_size", "=", "FLAGS", ".", "meta_batch_size", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "True", ")", "\n", "tr_data_list", ".", "append", "(", "tr_data", ")", "\n", "train_iterator_list", ".", "append", "(", "\n", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "tr_data", ".", "data", ".", "output_types", ",", "tr_data", ".", "data", ".", "output_shapes", ")", ")", "\n", "train_next_list", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "get_next", "(", ")", ")", "\n", "\n", "", "test_data", "=", "ImageDataGenerator", "(", "test_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "test_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "test_data", ".", "data", ".", "output_types", ",", "test_data", ".", "data", ".", "output_shapes", ")", "\n", "test_next_batch", "=", "test_iterator", ".", "get_next", "(", ")", "\n", "\n", "val_data", "=", "ImageDataGenerator", "(", "val_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "val_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "val_data", ".", "data", ".", "output_types", ",", "val_data", ".", "data", ".", "output_shapes", ")", "\n", "val_next_batch", "=", "val_iterator", ".", "get_next", "(", ")", "\n", "\n", "# Ops for initializing different iterators", "\n", "", "training_init_op", "=", "[", "]", "\n", "train_batches_per_epoch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "        ", "training_init_op", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "make_initializer", "(", "tr_data_list", "[", "i", "]", ".", "data", ")", ")", "\n", "train_batches_per_epoch", ".", "append", "(", "int", "(", "np", ".", "floor", "(", "tr_data_list", "[", "i", "]", ".", "data_size", "/", "FLAGS", ".", "meta_batch_size", ")", ")", ")", "\n", "\n", "", "test_init_op", "=", "test_iterator", ".", "make_initializer", "(", "test_data", ".", "data", ")", "\n", "test_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "test_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "val_init_op", "=", "val_iterator", ".", "make_initializer", "(", "val_data", ".", "data", ")", "\n", "val_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "val_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "# Training begins", "\n", "best_test_acc", ",", "best_val_acc", "=", "0", ",", "0", "\n", "for", "itr", "in", "range", "(", "resume_itr", ",", "FLAGS", ".", "train_iterations", ")", ":", "\n", "\n", "# Sampling training and test tasks", "\n", "        ", "num_training_tasks", "=", "len", "(", "train_file_list", ")", "\n", "num_meta_train", "=", "num_training_tasks", "-", "1", "\n", "num_meta_test", "=", "num_training_tasks", "-", "num_meta_train", "# as setting num_meta_test = 1", "\n", "\n", "# Randomly choosing meta train and meta test domains", "\n", "task_list", "=", "np", ".", "random", ".", "permutation", "(", "num_training_tasks", ")", "\n", "meta_train_index_list", "=", "task_list", "[", ":", "num_meta_train", "]", "\n", "meta_test_index_list", "=", "task_list", "[", "num_meta_train", ":", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "if", "itr", "%", "train_batches_per_epoch", "[", "i", "]", "==", "0", "or", "resume_itr", "!=", "0", ":", "\n", "                ", "sess", ".", "run", "(", "training_init_op", "[", "i", "]", ")", "# initialize training sample generator at itr=0", "\n", "\n", "# Sampling meta-train, meta-test data", "\n", "", "", "for", "i", "in", "range", "(", "num_meta_train", ")", ":", "\n", "            ", "task_ind", "=", "meta_train_index_list", "[", "i", "]", "\n", "if", "i", "==", "0", ":", "\n", "                ", "inputa", ",", "labela", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "elif", "i", "==", "1", ":", "\n", "                ", "inputa1", ",", "labela1", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "'check number of meta-train domains.'", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "num_meta_test", ")", ":", "\n", "            ", "task_ind", "=", "meta_test_index_list", "[", "i", "]", "\n", "if", "i", "==", "0", ":", "\n", "                ", "inputb", ",", "labelb", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "'check number of meta-test domains.'", ")", "\n", "\n", "# to avoid a certain un-sampled class affect stability of global class alignment", "\n", "# i.e., mask-out the un-sampled class from computing kd-loss", "\n", "", "", "sampledb", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labelb", ",", "axis", "=", "1", ")", ")", "\n", "sampleda", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labela", ",", "axis", "=", "1", ")", ")", "\n", "bool_indicator_b_a", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "for", "i", "in", "range", "(", "FLAGS", ".", "num_classes", ")", ":", "\n", "# only count class that are sampled in both source domains", "\n", "            ", "if", "(", "i", "in", "sampledb", ")", "and", "(", "i", "in", "sampleda", ")", ":", "\n", "                ", "bool_indicator_b_a", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "sampledb", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labelb", ",", "axis", "=", "1", ")", ")", "\n", "sampleda1", "=", "np", ".", "unique", "(", "np", ".", "argmax", "(", "labela1", ",", "axis", "=", "1", ")", ")", "\n", "bool_indicator_b_a1", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "for", "i", "in", "range", "(", "FLAGS", ".", "num_classes", ")", ":", "\n", "            ", "if", "(", "i", "in", "sampledb", ")", "and", "(", "i", "in", "sampleda1", ")", ":", "\n", "                ", "bool_indicator_b_a1", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "part", "=", "FLAGS", ".", "meta_batch_size", "/", "3", "\n", "part", "=", "int", "(", "part", ")", "\n", "input_group", "=", "np", ".", "concatenate", "(", "(", "inputa", "[", ":", "part", "]", ",", "inputa1", "[", ":", "part", "]", ",", "inputb", "[", ":", "part", "]", ")", ",", "axis", "=", "0", ")", "\n", "label_group", "=", "np", ".", "concatenate", "(", "(", "labela", "[", ":", "part", "]", ",", "labela1", "[", ":", "part", "]", ",", "labelb", "[", ":", "part", "]", ")", ",", "axis", "=", "0", ")", "\n", "group_list", "=", "np", ".", "sum", "(", "label_group", ",", "axis", "=", "0", ")", "\n", "label_group", "=", "np", ".", "argmax", "(", "label_group", ",", "axis", "=", "1", ")", "# transform one-hot labels into class-wise integer", "\n", "\n", "feed_dict", "=", "{", "model", ".", "inputa", ":", "inputa", ",", "model", ".", "labela", ":", "labela", ",", "model", ".", "inputa1", ":", "inputa1", ",", "model", ".", "labela1", ":", "labela1", ",", "model", ".", "inputb", ":", "inputb", ",", "model", ".", "labelb", ":", "labelb", ",", "model", ".", "input_group", ":", "input_group", ",", "model", ".", "label_group", ":", "label_group", ",", "model", ".", "bool_indicator_b_a", ":", "bool_indicator_b_a", ",", "model", ".", "bool_indicator_b_a1", ":", "bool_indicator_b_a1", ",", "\n", "model", ".", "KEEP_PROB", ":", "0.5", "}", "\n", "\n", "output_tensors", "=", "[", "model", ".", "task_train_op", ",", "model", ".", "meta_train_op", ",", "model", ".", "metric_train_op", "]", "\n", "output_tensors", ".", "extend", "(", "\n", "[", "model", ".", "summ_op", ",", "model", ".", "global_loss", ",", "model", ".", "source_loss", ",", "model", ".", "source_accuracy", ",", "model", ".", "metric_loss", "]", ")", "\n", "_", ",", "_", ",", "_", ",", "summ_writer", ",", "global_loss", ",", "source_loss", ",", "source_accuracy", ",", "metric_loss", "=", "sess", ".", "run", "(", "output_tensors", ",", "\n", "feed_dict", ")", "\n", "\n", "source_losses", ".", "append", "(", "source_loss", ")", "\n", "source_accuracies", ".", "append", "(", "source_accuracy", ")", "\n", "\n", "if", "itr", "%", "FLAGS", ".", "print_interval", "==", "0", ":", "\n", "            ", "print", "(", "'---'", "*", "10", "+", "'\\n%s'", "%", "exp_string", ")", "\n", "print", "(", "'number of samples per category:'", ",", "group_list", ")", "\n", "print", "(", "'global loss: %.7f'", "%", "global_loss", ")", "\n", "print", "(", "'metric loss: %.7f '", "%", "metric_loss", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Loss '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_losses", ")", ")", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Accuracy '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "# log loss and accuracy:", "\n", "path_save_train_acc", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "FLAGS", ".", "dataset", ",", "exp_string", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save_train_acc", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "path_save_train_acc", ")", "\n", "", "with", "open", "(", "path_save_train_acc", "+", "'/eva_'", "+", "'train'", "+", "'.txt'", ",", "'a'", ")", "as", "fle", ":", "\n", "                ", "fle", ".", "write", "(", "\n", "'Train results: Iteration %d, global loss: %.7f, metric loss: %.7f, Loss: %f, Accuracy: %f \\n'", "%", "(", "\n", "itr", ",", "global_loss", ",", "metric_loss", ",", "np", ".", "mean", "(", "source_losses", ")", ",", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "", "source_losses", ",", "target_losses", "=", "[", "]", ",", "[", "]", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "summary_interval", "==", "0", "and", "FLAGS", ".", "log", ":", "\n", "            ", "train_writer", ".", "add_summary", "(", "summ_writer", ",", "itr", ")", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "save_interval", "==", "0", ":", "\n", "# saver.save(sess, FLAGS.logdir + '/' + FLAGS.dataset + '/' + exp_string + '/model' + str(itr))", "\n", "            ", "checkpoint_dir", "=", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", "+", "\"/\"", "+", "str", "(", "itr", ")", "+", "\"/\"", "\n", "save_network_model", "(", "saver_", "=", "saver", ",", "session_", "=", "sess", ",", "checkpoint_dir", "=", "checkpoint_dir", ",", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.train_DeepAll": [[319, 422], ["range", "tf.data.Iterator.from_structure.make_initializer", "int", "tf.data.Iterator.from_structure.make_initializer", "int", "range", "tensorflow.summary.FileWriter", "tensorflow.device", "range", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "data_generator.ImageDataGenerator", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "len", "training_init_op.append", "train_batches_per_epoch.append", "numpy.floor", "numpy.floor", "len", "range", "range", "output_tensors.extend", "sess.run", "source_losses.append", "source_accuracies.append", "len", "data_generator.ImageDataGenerator", "tr_data_list.append", "train_iterator_list.append", "train_next_list.append", "train_iterator_list[].make_initializer", "int", "len", "print", "print", "print", "os.path.join", "tf.summary.FileWriter.add_summary", "main.save_network_model", "main.evaluation_during_training", "main.evaluation_during_training", "tensorflow.data.Iterator.from_structure", "train_iterator_list[].get_next", "numpy.floor", "sess.run", "sess.run", "os.path.exists", "os.makedirs", "open", "fle.write", "sess.run", "sess.run", "str", "str", "str", "numpy.mean", "numpy.mean", "str", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.save_network_model", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training"], ["\n", "# Testing periodically:", "\n", "", "if", "itr", "%", "FLAGS", ".", "val_print_interval", "==", "0", ":", "\n", "            ", "val_acc", ",", "best_val_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "val_batches_per_epoch", ",", "\n", "val_init_op", ",", "val_next_batch", ",", "itr", ",", "best_val_acc", ",", "saver", ",", "\n", "is_val", "=", "True", ")", "\n", "", "if", "itr", "%", "FLAGS", ".", "test_print_interval", "==", "0", ":", "\n", "            ", "test_acc", ",", "best_test_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "test_batches_per_epoch", ",", "\n", "test_init_op", ",", "test_next_batch", ",", "itr", ",", "best_test_acc", ",", "\n", "saver", ",", "is_val", "=", "False", ")", "\n", "\n", "\n", "", "", "", "def", "train_DeepAll", "(", "model", ",", "saver", ",", "sess", ",", "exp_string", ",", "train_file_list", ",", "test_file", ",", "val_file", ",", "resume_itr", "=", "0", ")", ":", "\n", "    ", "if", "FLAGS", ".", "log", ":", "\n", "        ", "train_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", ",", "sess", ".", "graph", ")", "\n", "", "source_losses", ",", "target_losses", ",", "source_accuracies", ",", "target_accuracies", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Data loaders", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "        ", "tr_data_list", ",", "train_iterator_list", ",", "train_next_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "tr_data", "=", "ImageDataGenerator", "(", "train_file_list", "[", "i", "]", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'training'", ",", "batch_size", "=", "FLAGS", ".", "deep_all_batch_size", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "\n", "shuffle", "=", "True", ")", "\n", "tr_data_list", ".", "append", "(", "tr_data", ")", "\n", "train_iterator_list", ".", "append", "(", "\n", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "tr_data", ".", "data", ".", "output_types", ",", "tr_data", ".", "data", ".", "output_shapes", ")", ")", "\n", "train_next_list", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "get_next", "(", ")", ")", "\n", "\n", "", "test_data", "=", "ImageDataGenerator", "(", "test_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "test_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "test_data", ".", "data", ".", "output_types", ",", "test_data", ".", "data", ".", "output_shapes", ")", "\n", "test_next_batch", "=", "test_iterator", ".", "get_next", "(", ")", "\n", "\n", "val_data", "=", "ImageDataGenerator", "(", "val_file", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "1", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "shuffle", "=", "False", ")", "\n", "val_iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "val_data", ".", "data", ".", "output_types", ",", "val_data", ".", "data", ".", "output_shapes", ")", "\n", "val_next_batch", "=", "val_iterator", ".", "get_next", "(", ")", "\n", "\n", "# Ops for initializing different iterators", "\n", "", "training_init_op", "=", "[", "]", "\n", "train_batches_per_epoch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "        ", "training_init_op", ".", "append", "(", "train_iterator_list", "[", "i", "]", ".", "make_initializer", "(", "tr_data_list", "[", "i", "]", ".", "data", ")", ")", "\n", "train_batches_per_epoch", ".", "append", "(", "int", "(", "np", ".", "floor", "(", "tr_data_list", "[", "i", "]", ".", "data_size", "/", "FLAGS", ".", "deep_all_batch_size", ")", ")", ")", "\n", "\n", "", "test_init_op", "=", "test_iterator", ".", "make_initializer", "(", "test_data", ".", "data", ")", "\n", "test_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "test_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "val_init_op", "=", "val_iterator", ".", "make_initializer", "(", "val_data", ".", "data", ")", "\n", "val_batches_per_epoch", "=", "int", "(", "np", ".", "floor", "(", "val_data", ".", "data_size", "/", "1", ")", ")", "\n", "\n", "# Training begins", "\n", "best_test_acc", ",", "best_val_acc", "=", "0", ",", "0", "\n", "for", "itr", "in", "range", "(", "resume_itr", ",", "FLAGS", ".", "train_iterations", ")", ":", "\n", "\n", "# Sampling training and test tasks", "\n", "        ", "num_training_tasks", "=", "len", "(", "train_file_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "train_file_list", ")", ")", ":", "\n", "            ", "if", "itr", "%", "train_batches_per_epoch", "[", "i", "]", "==", "0", "or", "resume_itr", "!=", "0", ":", "\n", "                ", "sess", ".", "run", "(", "training_init_op", "[", "i", "]", ")", "# initialize training sample generator at itr=0", "\n", "\n", "# Sampling", "\n", "", "", "for", "i", "in", "range", "(", "num_training_tasks", ")", ":", "\n", "            ", "task_ind", "=", "i", "\n", "if", "i", "==", "0", ":", "\n", "                ", "inputa", ",", "labela", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "elif", "i", "==", "1", ":", "\n", "                ", "inputa1", ",", "labela1", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "inputb", ",", "labelb", "=", "sess", ".", "run", "(", "train_next_list", "[", "task_ind", "]", ")", "\n", "", "", "feed_dict", "=", "{", "model", ".", "inputa", ":", "inputa", ",", "model", ".", "labela", ":", "labela", ",", "model", ".", "inputa1", ":", "inputa1", ",", "model", ".", "labela1", ":", "labela1", ",", "model", ".", "inputb", ":", "inputb", ",", "model", ".", "labelb", ":", "labelb", ",", "model", ".", "KEEP_PROB", ":", "0.5", "}", "\n", "\n", "output_tensors", "=", "[", "model", ".", "task_train_op", "]", "\n", "output_tensors", ".", "extend", "(", "[", "model", ".", "summ_op", ",", "model", ".", "source_loss", ",", "model", ".", "source_accuracy", "]", ")", "\n", "_", ",", "summ_writer", ",", "source_loss", ",", "source_accuracy", "=", "sess", ".", "run", "(", "output_tensors", ",", "feed_dict", ")", "\n", "\n", "source_losses", ".", "append", "(", "source_loss", ")", "\n", "source_accuracies", ".", "append", "(", "source_accuracy", ")", "\n", "\n", "if", "itr", "%", "FLAGS", ".", "print_interval", "==", "0", ":", "\n", "            ", "print", "(", "'---'", "*", "10", "+", "'\\n%s'", "%", "exp_string", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Loss '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_losses", ")", ")", ")", "\n", "print", "(", "'Iteration %d'", "%", "itr", "+", "': Accuracy '", "+", "'training domains '", "+", "str", "(", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "# log loss and accuracy:", "\n", "path_save_train_acc", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "FLAGS", ".", "dataset", ",", "exp_string", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save_train_acc", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "path_save_train_acc", ")", "\n", "", "with", "open", "(", "path_save_train_acc", "+", "'/eva_'", "+", "'train'", "+", "'.txt'", ",", "'a'", ")", "as", "fle", ":", "\n", "                ", "fle", ".", "write", "(", "'Train results: Iteration %d, Loss: %f, Accuracy: %f \\n'", "%", "(", "\n", "itr", ",", "np", ".", "mean", "(", "source_losses", ")", ",", "np", ".", "mean", "(", "source_accuracies", ")", ")", ")", "\n", "", "source_losses", ",", "target_losses", "=", "[", "]", ",", "[", "]", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "summary_interval", "==", "0", "and", "FLAGS", ".", "log", ":", "\n", "            ", "train_writer", ".", "add_summary", "(", "summ_writer", ",", "itr", ")", "\n", "\n", "", "if", "itr", "%", "FLAGS", ".", "save_interval", "==", "0", ":", "\n", "# saver.save(sess, FLAGS.logdir + '/' + FLAGS.dataset + '/' + exp_string + '/model' + str(itr))", "\n", "            ", "checkpoint_dir", "=", "FLAGS", ".", "logdir", "+", "'/'", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "exp_string", "+", "\"/\"", "+", "str", "(", "itr", ")", "+", "\"/\"", "\n", "save_network_model", "(", "saver_", "=", "saver", ",", "session_", "=", "sess", ",", "checkpoint_dir", "=", "checkpoint_dir", ",", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_during_training": [[424, 484], ["numpy.zeros", "numpy.zeros", "sess.run", "range", "print", "main.plot_embedding_of_points", "matplotlib.savefig", "matplotlib.clf", "matplotlib.close", "sess.run", "sess.run", "print", "print", "os.path.exists", "os.makedirs", "open", "fle.write", "numpy.argmax", "numpy.argmax", "os.path.join", "str"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.plot_embedding_of_points"], ["\n", "# Testing periodically", "\n", "", "if", "itr", "%", "FLAGS", ".", "val_print_interval", "==", "0", ":", "\n", "            ", "val_acc", ",", "best_val_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "val_batches_per_epoch", ",", "\n", "val_init_op", ",", "val_next_batch", ",", "itr", ",", "best_val_acc", ",", "saver", ",", "\n", "is_val", "=", "True", ")", "\n", "", "if", "itr", "%", "FLAGS", ".", "test_print_interval", "==", "0", ":", "\n", "            ", "test_acc", ",", "best_test_acc", "=", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "test_batches_per_epoch", ",", "\n", "test_init_op", ",", "test_next_batch", ",", "itr", ",", "best_test_acc", ",", "\n", "saver", ",", "is_val", "=", "False", ")", "\n", "\n", "\n", "", "", "", "def", "evaluation_during_training", "(", "sess", ",", "model", ",", "exp_string", ",", "test_batches_per_epoch", ",", "\n", "test_init_op", ",", "test_next_batch", ",", "itr", ",", "best_test_acc", ",", "saver", ",", "is_val", "=", "True", ")", ":", "\n", "    ", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "method_", "=", "\"MASF\"", "\n", "", "else", ":", "\n", "        ", "method_", "=", "\"DeepAll\"", "\n", "", "if", "is_val", ":", "\n", "        ", "case_", "=", "\"val\"", "\n", "", "else", ":", "\n", "        ", "case_", "=", "\"test\"", "\n", "", "class_accs", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "class_samples", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "test_embeddings", "=", "np", ".", "zeros", "(", "(", "test_batches_per_epoch", ",", "FLAGS", ".", "feature_space_dimension", ")", ")", "\n", "test_labels", "=", "np", ".", "zeros", "(", "(", "test_batches_per_epoch", ",", ")", ")", "\n", "test_acc", ",", "test_loss", ",", "test_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "sess", ".", "run", "(", "test_init_op", ")", "# initialize testing data generator", "\n", "for", "it", "in", "range", "(", "test_batches_per_epoch", ")", ":", "\n", "        ", "test_input", ",", "test_label", "=", "sess", ".", "run", "(", "test_next_batch", ")", "\n", "feed_dict", "=", "{", "model", ".", "test_input", ":", "test_input", ",", "model", ".", "test_label", ":", "test_label", ",", "model", ".", "KEEP_PROB", ":", "1.", "}", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "            ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "semantic_feature", ",", "model", ".", "outputs", ",", "\n", "model", ".", "metric_embedding", "]", "\n", "", "else", ":", "\n", "            ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "embedding_feature", ",", "model", ".", "outputs", "]", "\n", "# --> model.semantic_feature: 4096-dimensional embedding with psi weights (for semantic features)", "\n", "# --> model.outputs: n_classes-dimensional embedding with theta weights (after softmax for cross-entropy)", "\n", "# --> model.metric_embedding: 256-dimensional embedding with phi weights (for metric features used in triplet loss)", "\n", "", "result", "=", "sess", ".", "run", "(", "output_tensors", ",", "feed_dict", ")", "\n", "test_loss", "+=", "result", "[", "0", "]", "\n", "test_acc", "+=", "result", "[", "1", "]", "\n", "test_count", "+=", "1", "\n", "this_class", "=", "np", ".", "argmax", "(", "test_label", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "class_accs", "[", "this_class", "]", "+=", "result", "[", "1", "]", "# added for debug", "\n", "class_samples", "[", "this_class", "]", "+=", "1", "\n", "test_embeddings", "[", "it", ",", ":", "]", "=", "result", "[", "2", "]", "\n", "test_labels", "[", "it", "]", "=", "np", ".", "argmax", "(", "test_label", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "", "test_acc", "=", "test_acc", "/", "test_count", "\n", "test_acc", "*=", "100", "\n", "if", "test_acc", ">", "best_test_acc", ":", "\n", "        ", "best_test_acc", "=", "test_acc", "\n", "# saver.save(sess, FLAGS.logdir + '/' + FLAGS.dataset + '/' + exp_string + '/itr' + str(itr) + '_model_acc' + str(best_test_acc))", "\n", "", "print", "(", "'Unseen Target %s results: Iteration %d, Loss: %f, Accuracy: %f'", "%", "(", "\n", "case_", ",", "itr", ",", "test_loss", "/", "test_count", ",", "test_acc", ")", ")", "\n", "if", "case_", "==", "\"test\"", ":", "\n", "        ", "print", "(", "'Current best test accuracy {}'", ".", "format", "(", "best_test_acc", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Current best val accuracy {}'", ".", "format", "(", "best_test_acc", ")", ")", "\n", "# plot the test embedding:", "\n", "", "path_save", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "\"/target_domain/plots/\"", "+", "case_", "+", "\"/\"", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.evaluation_after_training": [[486, 585], ["range", "len", "range", "numpy.empty", "range", "main.plot_embedding_of_points_2PLOTS", "tensorflow.device", "range", "len", "evaluation_init_op.append", "evaluation_batches_per_epoch.append", "len", "sess.run", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "print", "numpy.vstack", "total_evaluation_labels_classwise.extend", "total_evaluation_labels_domainwise.extend", "os.path.exists", "os.makedirs", "open", "fle.write", "len", "data_generator.ImageDataGenerator", "evaluation_data_list.append", "evaluation_iterator_list.append", "evaluation_next_list.append", "evaluation_iterator_list[].make_initializer", "int", "sess.run", "sess.run", "os.path.exists", "os.makedirs", "open", "fle.write", "tensorflow.data.Iterator.from_structure", "evaluation_iterator_list[].get_next", "numpy.floor", "numpy.argmax", "numpy.argmax", "str"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.plot_embedding_of_points_2PLOTS"], ["if", "not", "os", ".", "path", ".", "exists", "(", "path_save", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_save", ")", "\n", "", "plt", ".", "savefig", "(", "path_save", "+", "'epoch'", "+", "str", "(", "itr", ")", "+", "'.png'", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "# log loss and accuracy:", "\n", "with", "open", "(", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "FLAGS", ".", "dataset", ",", "exp_string", ",", "'eva_'", "+", "case_", "+", "'.txt'", ")", ")", ",", "'a'", ")", "as", "fle", ":", "\n", "        ", "fle", ".", "write", "(", "'Unseen Target %s results: Iteration %d, Loss: %f, Accuracy: %f \\n'", "%", "(", "\n", "case_", ",", "itr", ",", "test_loss", "/", "test_count", ",", "test_acc", ")", ")", "\n", "", "return", "test_acc", ",", "best_test_acc", "\n", "\n", "\n", "", "def", "evaluation_after_training", "(", "model", ",", "sess", ",", "evaluation_file_list", ",", "evaluation_batch_size", "=", "1", ")", ":", "\n", "    ", "if", "FLAGS", ".", "masf_mode", ":", "\n", "        ", "method_", "=", "\"MASF\"", "\n", "", "else", ":", "\n", "        ", "method_", "=", "\"DeepAll\"", "\n", "\n", "# Data loaders:", "\n", "", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "        ", "evaluation_data_list", ",", "evaluation_iterator_list", ",", "evaluation_next_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "evaluation_file_list", ")", ")", ":", "\n", "            ", "evaluation_data", "=", "ImageDataGenerator", "(", "evaluation_file_list", "[", "i", "]", ",", "dataroot", "=", "FLAGS", ".", "dataroot", ",", "mode", "=", "'inference'", ",", "batch_size", "=", "evaluation_batch_size", ",", "num_classes", "=", "FLAGS", ".", "num_classes", ",", "\n", "shuffle", "=", "False", ")", "\n", "evaluation_data_list", ".", "append", "(", "evaluation_data", ")", "\n", "evaluation_iterator_list", ".", "append", "(", "\n", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "evaluation_data", ".", "data", ".", "output_types", ",", "evaluation_data", ".", "data", ".", "output_shapes", ")", ")", "\n", "evaluation_next_list", ".", "append", "(", "evaluation_iterator_list", "[", "i", "]", ".", "get_next", "(", ")", ")", "\n", "\n", "# Ops for initializing different iterators:", "\n", "", "", "evaluation_init_op", "=", "[", "]", "\n", "evaluation_batches_per_epoch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "evaluation_file_list", ")", ")", ":", "\n", "        ", "evaluation_init_op", ".", "append", "(", "evaluation_iterator_list", "[", "i", "]", ".", "make_initializer", "(", "evaluation_data_list", "[", "i", "]", ".", "data", ")", ")", "\n", "evaluation_batches_per_epoch", ".", "append", "(", "int", "(", "np", ".", "floor", "(", "evaluation_data_list", "[", "i", "]", ".", "data_size", "/", "evaluation_batch_size", ")", ")", ")", "\n", "\n", "# Initialize iterator:", "\n", "", "num_total_domains", "=", "len", "(", "evaluation_file_list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "evaluation_file_list", ")", ")", ":", "\n", "        ", "sess", ".", "run", "(", "evaluation_init_op", "[", "i", "]", ")", "\n", "\n", "# Embedding:", "\n", "", "total_evaluation_embeddings", "=", "np", ".", "empty", "(", "(", "0", ",", "FLAGS", ".", "feature_space_dimension", ")", ")", "\n", "total_evaluation_labels_classwise", "=", "[", "]", "\n", "total_evaluation_labels_domainwise", "=", "[", "]", "\n", "total_evaluation_acc", ",", "total_evaluation_loss", ",", "total_evaluation_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "for", "domain_index", "in", "range", "(", "num_total_domains", ")", ":", "\n", "        ", "class_accs", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "class_samples", "=", "[", "0.0", "]", "*", "FLAGS", ".", "num_classes", "\n", "evaluation_embeddings", "=", "np", ".", "zeros", "(", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ",", "FLAGS", ".", "feature_space_dimension", ")", ")", "\n", "evaluation_labels_classwise", "=", "np", ".", "zeros", "(", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ",", ")", ")", "\n", "evaluation_labels_domainwise", "=", "np", ".", "zeros", "(", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ",", ")", ")", "\n", "evaluation_acc", ",", "evaluation_loss", ",", "evaluation_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "for", "it", "in", "range", "(", "evaluation_batches_per_epoch", "[", "domain_index", "]", ")", ":", "\n", "            ", "evaluation_input", ",", "evaluation_label_classwise", "=", "sess", ".", "run", "(", "evaluation_next_list", "[", "domain_index", "]", ")", "\n", "feed_dict", "=", "{", "model", ".", "test_input", ":", "evaluation_input", ",", "model", ".", "test_label", ":", "evaluation_label_classwise", ",", "\n", "model", ".", "KEEP_PROB", ":", "1.", "}", "\n", "if", "FLAGS", ".", "masf_mode", ":", "\n", "                ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "semantic_feature", ",", "model", ".", "outputs", ",", "\n", "model", ".", "metric_embedding", "]", "\n", "", "else", ":", "\n", "                ", "output_tensors", "=", "[", "model", ".", "test_loss", ",", "model", ".", "test_acc", ",", "model", ".", "embedding_feature", ",", "model", ".", "outputs", "]", "\n", "# --> model.semantic_feature: 4096-dimensional embedding with psi weights (for semantic features)", "\n", "# --> model.outputs: n_classes-dimensional embedding with theta weights (after softmax for cross-entropy)", "\n", "# --> model.metric_embedding: 256-dimensional embedding with phi weights (for metric features used in triplet loss)", "\n", "", "result", "=", "sess", ".", "run", "(", "output_tensors", ",", "feed_dict", ")", "\n", "evaluation_loss", "+=", "result", "[", "0", "]", "\n", "total_evaluation_loss", "+=", "result", "[", "0", "]", "\n", "evaluation_acc", "+=", "result", "[", "1", "]", "\n", "total_evaluation_acc", "+=", "result", "[", "1", "]", "\n", "evaluation_count", "+=", "1", "\n", "total_evaluation_count", "+=", "1", "\n", "this_class", "=", "np", ".", "argmax", "(", "evaluation_label_classwise", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "class_accs", "[", "this_class", "]", "+=", "result", "[", "1", "]", "# added for debug", "\n", "class_samples", "[", "this_class", "]", "+=", "1", "\n", "evaluation_embeddings", "[", "it", ",", ":", "]", "=", "result", "[", "2", "]", "\n", "evaluation_labels_classwise", "[", "it", "]", "=", "np", ".", "argmax", "(", "evaluation_label_classwise", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "evaluation_labels_domainwise", "[", "it", "]", "=", "domain_index", "\n", "", "evaluation_acc", "=", "evaluation_acc", "/", "evaluation_count", "\n", "evaluation_acc", "*=", "100", "\n", "evaluation_loss", "=", "evaluation_loss", "/", "evaluation_count", "\n", "print", "(", "'Evaluation results: Domain %d, Loss: %f, Accuracy: %f'", "%", "(", "domain_index", ",", "evaluation_loss", ",", "evaluation_acc", ")", ")", "\n", "total_evaluation_embeddings", "=", "np", ".", "vstack", "(", "(", "total_evaluation_embeddings", ",", "evaluation_embeddings", ")", ")", "\n", "total_evaluation_labels_classwise", ".", "extend", "(", "evaluation_labels_classwise", ")", "\n", "total_evaluation_labels_domainwise", ".", "extend", "(", "evaluation_labels_domainwise", ")", "\n", "# save accuracy results:", "\n", "path_", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "'/total_evaluation/accuracy/'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "path_", ")", "\n", "", "with", "open", "(", "(", "path_", "+", "'domain'", "+", "str", "(", "domain_index", ")", "+", "'_accuracy.txt'", ")", ",", "'a'", ")", "as", "fle", ":", "\n", "            ", "fle", ".", "write", "(", "'Evaluation results: Domain %d, Loss: %f, Accuracy: %f'", "%", "(", "\n", "domain_index", ",", "evaluation_loss", ",", "evaluation_acc", ")", ")", "\n", "# save total average accuracy results (average over the whole evaulation data):", "\n", "", "", "total_evaluation_acc", "=", "total_evaluation_acc", "/", "total_evaluation_count", "\n", "total_evaluation_acc", "*=", "100", "\n", "total_evaluation_loss_avg", "=", "total_evaluation_loss", "/", "total_evaluation_count", "\n", "path_", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "'/total_evaluation/accuracy/'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_", ")", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.plot_embedding_of_points": [[587, 611], ["matplotlib.subplots", "matplotlib.scatter", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "range", "min", "range", "sklearn.manifold.TSNE().fit_transform", "range", "umap.UMAP().fit_transform", "str", "len", "numpy.arange", "sklearn.manifold.TSNE", "umap.UMAP"], "function", ["None"], ["        ", "fle", ".", "write", "(", "'Evaluation results: Loss: %f, Accuracy: %f'", "%", "(", "total_evaluation_loss", ",", "total_evaluation_acc", ")", ")", "\n", "\n", "# plot the classwise evaluation embedding:", "\n", "", "path_save1", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "\"/total_evaluation/plots/classwise/\"", "\n", "path_save2", "=", "FLAGS", ".", "logdir", "+", "FLAGS", ".", "dataset", "+", "'/'", "+", "method_", "+", "\"/total_evaluation/plots/domainwise/\"", "\n", "name_save1", ",", "name_save2", "=", "\"embedding_classwise\"", ",", "\"embedding_domainwise\"", "\n", "plot_embedding_of_points_2PLOTS", "(", "embedding", "=", "total_evaluation_embeddings", ",", "labels1", "=", "total_evaluation_labels_classwise", ",", "\n", "labels2", "=", "total_evaluation_labels_domainwise", ",", "\n", "path_save1", "=", "path_save1", ",", "path_save2", "=", "path_save2", ",", "\n", "name_save1", "=", "name_save1", ",", "name_save2", "=", "name_save2", ",", "n_samples_plot", "=", "2000", ",", "method", "=", "'TSNE'", ")", "\n", "\n", "\n", "", "def", "plot_embedding_of_points", "(", "embedding", ",", "labels", ",", "n_samples_plot", "=", "None", ",", "method", "=", "'TSNE'", ")", ":", "\n", "    ", "n_samples", "=", "embedding", ".", "shape", "[", "0", "]", "\n", "if", "n_samples_plot", "!=", "None", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "min", "(", "n_samples_plot", ",", "n_samples", ")", ",", "replace", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "n_samples", ",", "replace", "=", "False", ")", "\n", "", "embedding_sampled", "=", "embedding", "[", "indices_to_plot", ",", ":", "]", "\n", "if", "embedding", ".", "shape", "[", "1", "]", "==", "2", ":", "\n", "\n", "        ", "embedding_sampled", "=", "embedding", "\n", "", "else", ":", "\n", "        ", "if", "method", "==", "'TSNE'", ":", "\n", "            ", "embedding_sampled", "=", "TSNE", "(", "n_components", "=", "2", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.plot_embedding_of_points_2PLOTS": [[613, 661], ["numpy.asarray", "numpy.asarray", "labels1.astype.astype", "labels2.astype.astype", "matplotlib.subplots", "matplotlib.scatter", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "matplotlib.savefig", "matplotlib.clf", "matplotlib.close", "matplotlib.subplots", "len", "matplotlib.scatter", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "matplotlib.savefig", "matplotlib.clf", "matplotlib.close", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "os.path.exists", "os.makedirs", "numpy.arange", "os.path.exists", "os.makedirs", "range", "min", "range", "sklearn.manifold.TSNE().fit_transform", "range", "umap.UMAP().fit_transform", "str", "len", "numpy.arange", "numpy.arange", "sklearn.manifold.TSNE", "umap.UMAP"], "function", ["None"], ["            ", "embedding_sampled", "=", "umap", ".", "UMAP", "(", "n_neighbors", "=", "500", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "", "labels_sampled", "=", "labels", "[", "indices_to_plot", "]", "\n", "_", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "figsize", "=", "(", "14", ",", "10", ")", ")", "\n", "n_classes", "=", "FLAGS", ".", "num_classes", "\n", "class_names", "=", "[", "class_list", "[", "str", "(", "i", ")", "]", "for", "i", "in", "range", "(", "len", "(", "class_list", ")", ")", "]", "\n", "plt", ".", "scatter", "(", "embedding_sampled", "[", ":", ",", "0", "]", ",", "embedding_sampled", "[", ":", ",", "1", "]", ",", "s", "=", "10", ",", "c", "=", "labels_sampled", ",", "cmap", "=", "'Spectral'", ",", "alpha", "=", "1.0", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "boundaries", "=", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", "+", "1", ")", "-", "0.5", ")", "\n", "cbar", ".", "set_ticks", "(", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", ")", ")", "\n", "cbar", ".", "set_ticklabels", "(", "class_names", ")", "\n", "return", "plt", ",", "indices_to_plot", "\n", "\n", "\n", "", "def", "plot_embedding_of_points_2PLOTS", "(", "embedding", ",", "labels1", ",", "labels2", ",", "path_save1", ",", "path_save2", ",", "\n", "name_save1", ",", "name_save2", ",", "n_samples_plot", "=", "None", ",", "method", "=", "'TSNE'", ")", ":", "\n", "    ", "n_samples", "=", "embedding", ".", "shape", "[", "0", "]", "\n", "if", "n_samples_plot", "!=", "None", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "min", "(", "n_samples_plot", ",", "n_samples", ")", ",", "replace", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "indices_to_plot", "=", "np", ".", "random", ".", "choice", "(", "range", "(", "n_samples", ")", ",", "n_samples", ",", "replace", "=", "False", ")", "\n", "", "embedding_sampled", "=", "embedding", "[", "indices_to_plot", ",", ":", "]", "\n", "if", "embedding", ".", "shape", "[", "1", "]", "==", "2", ":", "\n", "\n", "        ", "embedding_sampled", "=", "embedding", "\n", "", "else", ":", "\n", "        ", "if", "method", "==", "'TSNE'", ":", "\n", "            ", "embedding_sampled", "=", "TSNE", "(", "n_components", "=", "2", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "elif", "method", "==", "'UMAP'", ":", "\n", "            ", "embedding_sampled", "=", "umap", ".", "UMAP", "(", "n_neighbors", "=", "500", ")", ".", "fit_transform", "(", "embedding_sampled", ")", "\n", "", "", "labels1", "=", "np", ".", "asarray", "(", "labels1", ")", "\n", "labels2", "=", "np", ".", "asarray", "(", "labels2", ")", "\n", "labels1", "=", "labels1", ".", "astype", "(", "int", ")", "\n", "labels2", "=", "labels2", ".", "astype", "(", "int", ")", "\n", "labels1_sampled", "=", "labels1", "[", "indices_to_plot", "]", "\n", "labels2_sampled", "=", "labels2", "[", "indices_to_plot", "]", "\n", "#### plot1 (for classwise):", "\n", "_", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "figsize", "=", "(", "14", ",", "10", ")", ")", "\n", "n_classes", "=", "FLAGS", ".", "num_classes", "\n", "class_names", "=", "[", "class_list", "[", "str", "(", "i", ")", "]", "for", "i", "in", "range", "(", "len", "(", "class_list", ")", ")", "]", "\n", "plt", ".", "scatter", "(", "embedding_sampled", "[", ":", ",", "0", "]", ",", "embedding_sampled", "[", ":", ",", "1", "]", ",", "s", "=", "10", ",", "c", "=", "labels1_sampled", ",", "cmap", "=", "'Spectral'", ",", "alpha", "=", "1.0", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "boundaries", "=", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", "+", "1", ")", "-", "0.5", ")", "\n", "cbar", ".", "set_ticks", "(", "np", ".", "arange", "(", "FLAGS", ".", "num_classes", ")", ")", "\n", "cbar", ".", "set_ticklabels", "(", "class_names", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save1", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_save1", ")", "\n", "", "plt", ".", "savefig", "(", "path_save1", "+", "name_save1", "+", "'.png'", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "#### plot2 (for domainwise):", "\n", "_", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "figsize", "=", "(", "14", ",", "10", ")", ")", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.save_network_model": [[663, 669], ["saver_.save", "os.path.exists", "os.makedirs", "os.path.join"], "function", ["None"], ["class_names", "=", "domain_list", "\n", "plt", ".", "scatter", "(", "embedding_sampled", "[", ":", ",", "0", "]", ",", "embedding_sampled", "[", ":", ",", "1", "]", ",", "s", "=", "10", ",", "c", "=", "labels2_sampled", ",", "cmap", "=", "'Spectral'", ",", "alpha", "=", "1.0", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "boundaries", "=", "np", ".", "arange", "(", "n_classes", "+", "1", ")", "-", "0.5", ")", "\n", "cbar", ".", "set_ticks", "(", "np", ".", "arange", "(", "n_classes", ")", ")", "\n", "cbar", ".", "set_ticklabels", "(", "class_names", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_save2", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_save2", ")", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.main.load_network_model": [[671, 683], ["print", "tensorflow.train.get_checkpoint_state", "os.path.basename", "saver_.restore", "print", "print", "os.path.join"], "function", ["None"], ["plt", ".", "clf", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "np", ".", "save", "(", "'embedding_sampled.npy'", ",", "embedding_sampled", ")", "\n", "np", ".", "save", "(", "'labels1_sampled.npy'", ",", "labels1_sampled", ")", "\n", "np", ".", "save", "(", "'labels2_sampled.npy'", ",", "labels2_sampled", ")", "\n", "np", ".", "save", "(", "'embedding.npy'", ",", "embedding", ")", "\n", "np", ".", "save", "(", "'labels1.npy'", ",", "labels1", ")", "\n", "np", ".", "save", "(", "'labels2.npy'", ",", "labels2", ")", "\n", "\n", "\n", "", "def", "save_network_model", "(", "saver_", ",", "session_", ",", "checkpoint_dir", ",", "model_name", "=", "\"model_\"", ")", ":", "\n", "# https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model", "\n", "# https://github.com/taki0112/ResNet-Tensorflow/blob/master/ResNet.py", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator.__init__": [[16, 73], ["data_generator.ImageDataGenerator._read_txt_file", "len", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.data.Dataset.from_tensor_slices", "data.map.map.batch", "data_generator.ImageDataGenerator._shuffle_lists", "data.map.map.map", "data.map.map.shuffle", "data.map.map.map", "ValueError"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._read_txt_file", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._shuffle_lists"], ["    ", "def", "__init__", "(", "self", ",", "txt_file", ",", "dataroot", ",", "mode", ",", "batch_size", ",", "num_classes", ",", "shuffle", "=", "True", ",", "buffer_size", "=", "1000", ")", ":", "\n", "\n", "        ", "\"\"\"Create a new ImageDataGenerator.\n        Receives a path string to a text file, where each line has a path string to an image and\n        separated by a space, then with an integer referring to the class number.\n\n        Args:\n            txt_file: path to the text file.\n            mode: either 'training' or 'validation'. Depending on this value, different parsing functions will be used.\n            batch_size: number of images per batch.\n            num_classes: number of classes in the dataset.\n            shuffle: wether or not to shuffle the data in the dataset and the initial file list.\n            buffer_size: number of images used as buffer for TensorFlows shuffling of the dataset.\n\n        Raises:\n            ValueError: If an invalid mode is passed.\n        \"\"\"", "\n", "\n", "self", ".", "dataroot", "=", "dataroot", "\n", "self", ".", "txt_file", "=", "txt_file", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "# retrieve the data from the text file", "\n", "self", ".", "_read_txt_file", "(", ")", "\n", "\n", "# number of samples in the dataset", "\n", "self", ".", "data_size", "=", "len", "(", "self", ".", "labels", ")", "\n", "\n", "# initial shuffling of the file and label lists together", "\n", "if", "shuffle", ":", "\n", "            ", "self", ".", "_shuffle_lists", "(", ")", "\n", "\n", "# convert lists to TF tensor", "\n", "", "self", ".", "img_paths", "=", "convert_to_tensor", "(", "self", ".", "img_paths", ",", "dtype", "=", "dtypes", ".", "string", ")", "\n", "self", ".", "labels", "=", "convert_to_tensor", "(", "self", ".", "labels", ",", "dtype", "=", "dtypes", ".", "int32", ")", "\n", "\n", "# create dataset", "\n", "data", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "(", "self", ".", "img_paths", ",", "self", ".", "labels", ")", ")", "\n", "\n", "# distinguish between train/infer. when calling the parsing functions", "\n", "if", "mode", "==", "'training'", ":", "\n", "            ", "data", "=", "data", ".", "map", "(", "self", ".", "_parse_function_train", ",", "num_parallel_calls", "=", "8", ")", "\n", "\n", "", "elif", "mode", "==", "'inference'", ":", "\n", "            ", "data", "=", "data", ".", "map", "(", "self", ".", "_parse_function_inference", ",", "num_parallel_calls", "=", "8", ")", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid mode '%s'.\"", "%", "(", "mode", ")", ")", "\n", "\n", "# shuffle the first `buffer_size` elements of the dataset", "\n", "", "if", "shuffle", ":", "\n", "            ", "data", "=", "data", ".", "shuffle", "(", "buffer_size", "=", "buffer_size", ")", "\n", "\n", "# create a new dataset with batches of images", "\n", "", "data", "=", "data", ".", "batch", "(", "batch_size", ")", "\n", "\n", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._read_txt_file": [[74, 84], ["open", "f.readlines", "line.split", "data_generator.ImageDataGenerator.img_paths.append", "data_generator.ImageDataGenerator.labels.append", "os.path.join", "int"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "_read_txt_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"Read the content of the text file and store it into lists.\"\"\"", "\n", "self", ".", "img_paths", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "txt_file", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "items", "=", "line", ".", "split", "(", "' '", ")", "\n", "self", ".", "img_paths", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataroot", ",", "items", "[", "0", "]", ")", ")", "\n", "self", ".", "labels", ".", "append", "(", "int", "(", "items", "[", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._shuffle_lists": [[85, 95], ["numpy.random.permutation", "data_generator.ImageDataGenerator.img_paths.append", "data_generator.ImageDataGenerator.labels.append"], "methods", ["None"], ["", "", "", "def", "_shuffle_lists", "(", "self", ")", ":", "\n", "        ", "\"\"\"Conjoined shuffling of the list of paths and labels.\"\"\"", "\n", "path", "=", "self", ".", "img_paths", "\n", "labels", "=", "self", ".", "labels", "\n", "permutation", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "data_size", ")", "\n", "self", ".", "img_paths", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "for", "i", "in", "permutation", ":", "\n", "            ", "self", ".", "img_paths", ".", "append", "(", "path", "[", "i", "]", ")", "\n", "self", ".", "labels", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._parse_function_train": [[96, 115], ["tensorflow.one_hot", "tensorflow.read_file", "tensorflow.image.decode_png", "tensorflow.image.resize_images", "tensorflow.subtract", "tensorflow.cond", "tensorflow.random_uniform", "data_generator.ImageDataGenerator.augment"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator.augment"], ["", "", "def", "_parse_function_train", "(", "self", ",", "filename", ",", "label", ")", ":", "\n", "        ", "\"\"\"Input parser for samples of the training set.\"\"\"", "\n", "# convert label number into one-hot-encoding", "\n", "one_hot", "=", "tf", ".", "one_hot", "(", "label", ",", "self", ".", "num_classes", ")", "\n", "\n", "# load and pre-process the image", "\n", "img_string", "=", "tf", ".", "read_file", "(", "filename", ")", "\n", "img_decoded", "=", "tf", ".", "image", ".", "decode_png", "(", "img_string", ",", "channels", "=", "3", ")", "\n", "img_resized", "=", "tf", ".", "image", ".", "resize_images", "(", "img_decoded", ",", "[", "227", ",", "227", "]", ")", "\n", "\n", "img_centered", "=", "tf", ".", "subtract", "(", "img_resized", ",", "IMAGENET_MEAN", ")", "\n", "\n", "# RGB -> BGR", "\n", "img_bgr", "=", "img_centered", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "\n", "# Data augmentation comes here, with a chance of 50%", "\n", "img_bgr", "=", "tf", ".", "cond", "(", "tf", ".", "random_uniform", "(", "[", "]", ",", "0", ",", "1", ")", "<", "0.5", ",", "lambda", ":", "self", ".", "augment", "(", "img_bgr", ")", ",", "lambda", ":", "img_bgr", ")", "\n", "\n", "return", "img_bgr", ",", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator._parse_function_inference": [[116, 132], ["tensorflow.one_hot", "tensorflow.read_file", "tensorflow.image.decode_png", "tensorflow.image.resize_images", "tensorflow.subtract"], "methods", ["None"], ["", "def", "_parse_function_inference", "(", "self", ",", "filename", ",", "label", ")", ":", "\n", "        ", "\"\"\"Input parser for samples of the validation/test set.\"\"\"", "\n", "# convert label number into one-hot-encoding", "\n", "one_hot", "=", "tf", ".", "one_hot", "(", "label", ",", "self", ".", "num_classes", ")", "\n", "\n", "# load and preprocess the image", "\n", "img_string", "=", "tf", ".", "read_file", "(", "filename", ")", "\n", "img_decoded", "=", "tf", ".", "image", ".", "decode_png", "(", "img_string", ",", "channels", "=", "3", ")", "\n", "img_resized", "=", "tf", ".", "image", ".", "resize_images", "(", "img_decoded", ",", "[", "227", ",", "227", "]", ")", "\n", "\n", "img_centered", "=", "tf", ".", "subtract", "(", "img_resized", ",", "IMAGENET_MEAN", ")", "\n", "\n", "# RGB -> BGR", "\n", "img_bgr", "=", "img_centered", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "\n", "return", "img_bgr", ",", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator.augment": [[133, 140], ["tensorflow.cond", "tensorflow.random_uniform", "f"], "methods", ["None"], ["", "def", "augment", "(", "self", ",", "x", ")", ":", "\n", "# add more types of augmentations here", "\n", "        ", "augmentations", "=", "[", "self", ".", "flip", "]", "\n", "for", "f", "in", "augmentations", ":", "\n", "            ", "x", "=", "tf", ".", "cond", "(", "tf", ".", "random_uniform", "(", "[", "]", ",", "0", ",", "1", ")", "<", "0.25", ",", "lambda", ":", "f", "(", "x", ")", ",", "lambda", ":", "x", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.data_generator.ImageDataGenerator.flip": [[141, 152], ["tensorflow.image.random_flip_left_right"], "methods", ["None"], ["", "def", "flip", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Flip augmentation\n        Args:\n            x: Image to flip\n        Returns:\n            Augmented image\n        \"\"\"", "\n", "x", "=", "tf", ".", "image", ".", "random_flip_left_right", "(", "x", ")", "\n", "# x = tf.image.random_flip_up_down(x)", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.deep_all.DeepAll.__init__": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "WEIGHTS_PATH", ",", "feature_space_dimension", ")", ":", "\n", "        ", "\"\"\" Call construct_model_*() after initializing deep_all\"\"\"", "\n", "self", ".", "deep_all_lr", "=", "FLAGS", ".", "deep_all_lr", "\n", "self", ".", "SKIP_LAYER", "=", "[", "'fc8'", "]", "\n", "self", ".", "forward", "=", "self", ".", "forward_alexnet", "\n", "self", ".", "construct_weights", "=", "self", ".", "construct_alexnet_weights", "\n", "self", ".", "loss_func", "=", "xent", "\n", "self", ".", "WEIGHTS_PATH", "=", "WEIGHTS_PATH", "\n", "self", ".", "feature_space_dimension", "=", "feature_space_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.deep_all.DeepAll.construct_model_train": [[27, 92], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "tensorflow.Variable", "deep_all.DeepAll.construct_model_train.cross_entropy"], "methods", ["None"], ["", "def", "construct_model_train", "(", "self", ",", "prefix", "=", "'deep_all_train_'", ")", ":", "\n", "        ", "self", ".", "inputa", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputa1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labelb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "bool_indicator_b_a", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "self", ".", "bool_indicator_b_a1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "\n", "self", ".", "KEEP_PROB", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "training_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "print", "(", "'weights already defined'", ")", "\n", "training_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "self", ".", "weights", "=", "weights", "=", "self", ".", "construct_weights", "(", ")", "\n", "\n", "", "def", "cross_entropy", "(", "inp", ",", "reuse", "=", "True", ")", ":", "\n", "# Function to perform meta learning update \"\"\"", "\n", "                ", "inputa", ",", "inputa1", ",", "inputb", ",", "labela", ",", "labela1", ",", "labelb", "=", "inp", "\n", "\n", "# Obtaining the conventional task loss on meta-train", "\n", "_", ",", "task_outputa", "=", "self", ".", "forward", "(", "inputa", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa", "=", "self", ".", "loss_func", "(", "task_outputa", ",", "labela", ")", "\n", "_", ",", "task_outputa1", "=", "self", ".", "forward", "(", "inputa1", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa1", "=", "self", ".", "loss_func", "(", "task_outputa1", ",", "labela1", ")", "\n", "_", ",", "task_outputb", "=", "self", ".", "forward", "(", "inputb", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossb", "=", "self", ".", "loss_func", "(", "task_outputb", ",", "labelb", ")", "\n", "\n", "task_accuracya", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela", ",", "1", ")", ")", "#this accuracy already gathers batch size", "\n", "task_accuracya1", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa1", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela1", ",", "1", ")", ")", "\n", "task_accuracyb", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputb", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labelb", ",", "1", ")", ")", "\n", "task_output", "=", "[", "task_lossa", ",", "task_lossa1", ",", "task_lossb", ",", "task_accuracya", ",", "task_accuracya1", ",", "task_accuracyb", "]", "\n", "\n", "return", "task_output", "\n", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "\n", "input_tensors", "=", "(", "self", ".", "inputa", ",", "self", ".", "inputa1", ",", "self", ".", "inputb", ",", "self", ".", "labela", ",", "self", ".", "labela1", ",", "self", ".", "labelb", ")", "\n", "result", "=", "cross_entropy", "(", "inp", "=", "input_tensors", ")", "\n", "self", ".", "lossa_raw", ",", "self", ".", "lossa1_raw", ",", "self", ".", "lossb_raw", ",", "accuracya", ",", "accuracya1", ",", "accuracyb", "=", "result", "\n", "\n", "## Performance & Optimization", "\n", "", "if", "'train'", "in", "prefix", ":", "\n", "            ", "self", ".", "lossa", "=", "avg_lossa", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa_raw", ")", "\n", "self", ".", "lossa1", "=", "avg_lossa1", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa1_raw", ")", "\n", "self", ".", "lossb", "=", "avg_lossb", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossb_raw", ")", "\n", "self", ".", "source_loss", "=", "(", "avg_lossa", "+", "avg_lossa1", "+", "avg_lossb", ")", "/", "3.0", "#--> because we have three training source domains", "\n", "self", ".", "task_train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "deep_all_lr", ")", ".", "minimize", "(", "self", ".", "source_loss", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "\n", "self", ".", "accuracya", "=", "accuracya", "*", "100.", "\n", "self", ".", "accuracya1", "=", "accuracya1", "*", "100.", "\n", "self", ".", "accuracyb", "=", "accuracyb", "*", "100.", "\n", "self", ".", "source_accuracy", "=", "(", "self", ".", "accuracya", "+", "self", ".", "accuracya1", "+", "self", ".", "accuracyb", ")", "/", "3.0", "\n", "\n", "## Summaries", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 loss'", ",", "self", ".", "lossa", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 loss'", ",", "self", ".", "lossa1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_3 loss'", ",", "self", ".", "lossb", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 accuracy'", ",", "self", ".", "accuracya", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 accuracy'", ",", "self", ".", "accuracya1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_3 accuracy'", ",", "self", ".", "accuracyb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.deep_all.DeepAll.construct_model_test": [[93, 113], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.variable_scope", "deep_all.DeepAll.forward", "deep_all.DeepAll.loss_func", "tensorflow.contrib.metrics.accuracy", "tensorflow.nn.softmax", "dir", "testing_scope.reuse_variables", "ValueError", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.nn.softmax"], "methods", ["None"], ["", "def", "construct_model_test", "(", "self", ",", "prefix", "=", "'test'", ")", ":", "\n", "\n", "        ", "self", ".", "test_input", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "test_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "testing_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "testing_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Weights not initilized. Create training model before testing model'", ")", "\n", "\n", "", "self", ".", "embedding_feature", ",", "outputs", "=", "self", ".", "forward", "(", "self", ".", "test_input", ",", "weights", ")", "\n", "losses", "=", "self", ".", "loss_func", "(", "outputs", ",", "self", ".", "test_label", ")", "\n", "accuracies", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "self", ".", "test_label", ",", "1", ")", ")", "\n", "self", ".", "pred_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", "\n", "self", ".", "outputs", "=", "outputs", "\n", "\n", "", "self", ".", "test_loss", "=", "losses", "\n", "self", ".", "test_acc", "=", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.deep_all.DeepAll.load_initial_weights": [[119, 144], ["numpy.load().item", "numpy.load", "tensorflow.variable_scope", "tensorflow.variable_scope", "len", "tensorflow.get_variable", "session.run", "tensorflow.get_variable", "session.run", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["None"], ["", "def", "load_initial_weights", "(", "self", ",", "session", ")", ":", "\n", "        ", "\"\"\"Load weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n        The weights come as a dict of lists (e.g. weights['conv1'] is a list)\n        Load the weights into the model\n        \"\"\"", "\n", "weights_dict", "=", "np", ".", "load", "(", "self", ".", "WEIGHTS_PATH", ",", "allow_pickle", "=", "True", ",", "encoding", "=", "'bytes'", ")", ".", "item", "(", ")", "\n", "\n", "# Loop over all layer names stored in the weights dict", "\n", "for", "op_name", "in", "weights_dict", ":", "\n", "\n", "# Check if layer should be trained from scratch", "\n", "            ", "if", "op_name", "not", "in", "self", ".", "SKIP_LAYER", ":", "\n", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "op_name", ",", "reuse", "=", "True", ")", ":", "\n", "\n", "                        ", "for", "data", "in", "weights_dict", "[", "op_name", "]", ":", "\n", "# Biases", "\n", "                            ", "if", "len", "(", "data", ".", "shape", ")", "==", "1", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "# Weights", "\n", "", "else", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.deep_all.DeepAll.construct_alexnet_weights": [[145, 196], ["tensorflow.contrib.layers.xavier_initializer_conv2d", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable"], "methods", ["None"], ["", "", "", "", "", "", "", "def", "construct_alexnet_weights", "(", "self", ")", ":", "\n", "\n", "        ", "weights", "=", "{", "}", "\n", "conv_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer_conv2d", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "fc_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'conv1'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv1_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "11", ",", "11", ",", "3", ",", "96", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv1_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "96", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv2'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv2_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "5", ",", "5", ",", "48", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv2_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv3'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv3_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "256", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv3_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv4'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv4_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv4_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv5'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv5_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv5_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "# with tf.variable_scope('fc6') as scope:", "\n", "#     weights['fc6_weights'] = tf.get_variable('weights', shape=[9216, 4096], initializer=conv_initializer)", "\n", "#     weights['fc6_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc6'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc6_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "9216", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc6_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc7') as scope:", "\n", "#     weights['fc7_weights'] = tf.get_variable('weights', shape=[4096, 4096], initializer=conv_initializer)", "\n", "#     weights['fc7_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc7'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc7_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc7_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc8') as scope:", "\n", "#     weights['fc8_weights'] = tf.get_variable('weights', shape=[4096, FLAGS.num_classes], initializer=fc_initializer)", "\n", "#     weights['fc8_biases'] = tf.get_variable('biases', [FLAGS.num_classes])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc8'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc8_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "FLAGS", ".", "num_classes", "]", ",", "initializer", "=", "fc_initializer", ")", "\n", "weights", "[", "'fc8_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "FLAGS", ".", "num_classes", "]", ")", "\n", "\n", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.deep_all.DeepAll.forward_alexnet": [[197, 232], ["utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.conv_block", "utils.conv_block", "utils.max_pool", "tensorflow.reshape", "utils.fc", "utils.dropout", "utils.fc", "utils.dropout", "utils.fc"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc"], ["", "def", "forward_alexnet", "(", "self", ",", "inp", ",", "weights", ",", "reuse", "=", "False", ")", ":", "\n", "# reuse is for the normalization parameters.", "\n", "\n", "        ", "conv1", "=", "conv_block", "(", "inp", ",", "weights", "[", "'conv1_weights'", "]", ",", "weights", "[", "'conv1_biases'", "]", ",", "stride_y", "=", "4", ",", "stride_x", "=", "4", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv1'", ")", "\n", "norm1", "=", "lrn", "(", "conv1", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool1", "=", "max_pool", "(", "norm1", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups", "\n", "conv2", "=", "conv_block", "(", "pool1", ",", "weights", "[", "'conv2_weights'", "]", ",", "weights", "[", "'conv2_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv2'", ")", "\n", "norm2", "=", "lrn", "(", "conv2", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool2", "=", "max_pool", "(", "norm2", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 3rd Layer: Conv (w ReLu)", "\n", "conv3", "=", "conv_block", "(", "pool2", ",", "weights", "[", "'conv3_weights'", "]", ",", "weights", "[", "'conv3_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv3'", ")", "\n", "\n", "# 4th Layer: Conv (w ReLu) splitted into two groups", "\n", "conv4", "=", "conv_block", "(", "conv3", ",", "weights", "[", "'conv4_weights'", "]", ",", "weights", "[", "'conv4_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv4'", ")", "\n", "\n", "# 5th Layer: Conv (w ReLu) -> Pool splitted into two groups", "\n", "conv5", "=", "conv_block", "(", "conv4", ",", "weights", "[", "'conv5_weights'", "]", ",", "weights", "[", "'conv5_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv5'", ")", "\n", "pool5", "=", "max_pool", "(", "conv5", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 6th Layer: Flatten -> FC (w ReLu) -> Dropout", "\n", "flattened", "=", "tf", ".", "reshape", "(", "pool5", ",", "[", "-", "1", ",", "6", "*", "6", "*", "256", "]", ")", "\n", "fc6", "=", "fc", "(", "flattened", ",", "weights", "[", "'fc6_weights'", "]", ",", "weights", "[", "'fc6_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout6", "=", "dropout", "(", "fc6", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 7th Layer: FC (w ReLu) -> Dropout", "\n", "fc7", "=", "fc", "(", "dropout6", ",", "weights", "[", "'fc7_weights'", "]", ",", "weights", "[", "'fc7_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout7", "=", "dropout", "(", "fc7", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 8th Layer: FC and return unscaled activations", "\n", "fc8", "=", "fc", "(", "dropout7", ",", "weights", "[", "'fc8_weights'", "]", ",", "weights", "[", "'fc8_biases'", "]", ")", "\n", "\n", "return", "fc7", ",", "fc8", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.get_images": [[15, 26], ["random.shuffle", "random.sample", "os.path.join", "zip", "sampler", "os.listdir"], "function", ["None"], ["def", "get_images", "(", "paths", ",", "labels", ",", "nb_samples", "=", "None", ",", "shuffle", "=", "True", ")", ":", "\n", "    ", "if", "nb_samples", "is", "not", "None", ":", "\n", "        ", "sampler", "=", "lambda", "x", ":", "random", ".", "sample", "(", "x", ",", "nb_samples", ")", "\n", "", "else", ":", "\n", "        ", "sampler", "=", "lambda", "x", ":", "x", "\n", "", "images", "=", "[", "(", "i", ",", "os", ".", "path", ".", "join", "(", "path", ",", "image", ")", ")", "for", "i", ",", "path", "in", "zip", "(", "labels", ",", "paths", ")", "for", "image", "in", "sampler", "(", "os", ".", "listdir", "(", "path", ")", ")", "]", "\n", "if", "shuffle", ":", "\n", "        ", "random", ".", "shuffle", "(", "images", ")", "\n", "", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block": [[28, 45], ["tensorflow.nn.relu", "tensorflow.nn.conv2d", "tensorflow.nn.bias_add", "tensorflow.split", "tensorflow.split", "tensorflow.concat", "tensorflow.nn.bias_add", "convolve", "convolve", "zip"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.split"], ["", "def", "conv_block", "(", "inp", ",", "cweight", ",", "bweight", ",", "stride_y", "=", "2", ",", "stride_x", "=", "2", ",", "groups", "=", "1", ",", "reuse", "=", "False", ",", "scope", "=", "''", ")", ":", "\n", "    ", "stride", "=", "[", "1", ",", "stride_y", ",", "stride_x", ",", "1", "]", "\n", "convolve", "=", "lambda", "i", ",", "k", ":", "tf", ".", "nn", ".", "conv2d", "(", "i", ",", "k", ",", "strides", "=", "stride", ",", "padding", "=", "'SAME'", ")", "\n", "\n", "if", "groups", "==", "1", ":", "\n", "        ", "conv_output", "=", "tf", ".", "nn", ".", "bias_add", "(", "convolve", "(", "inp", ",", "cweight", ")", ",", "bweight", ")", "\n", "", "else", ":", "\n", "        ", "input_groups", "=", "tf", ".", "split", "(", "axis", "=", "3", ",", "num_or_size_splits", "=", "groups", ",", "value", "=", "inp", ")", "\n", "weight_groups", "=", "tf", ".", "split", "(", "axis", "=", "3", ",", "num_or_size_splits", "=", "groups", ",", "value", "=", "cweight", ")", "\n", "output_groups", "=", "[", "convolve", "(", "i", ",", "k", ")", "for", "i", ",", "k", "in", "zip", "(", "input_groups", ",", "weight_groups", ")", "]", "\n", "\n", "conv", "=", "tf", ".", "concat", "(", "axis", "=", "3", ",", "values", "=", "output_groups", ")", "\n", "conv_output", "=", "tf", ".", "nn", ".", "bias_add", "(", "conv", ",", "bweight", ")", "\n", "\n", "", "relu", "=", "tf", ".", "nn", ".", "relu", "(", "conv_output", ")", "\n", "\n", "return", "relu", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.normalize": [[46, 48], ["tensorflow.contrib.layers.python.layers.batch_norm"], "function", ["None"], ["", "def", "normalize", "(", "inp", ",", "activation", ",", "reuse", ",", "scope", ")", ":", "\n", "    ", "return", "tf_layers", ".", "batch_norm", "(", "inp", ",", "activation_fn", "=", "activation", ",", "reuse", "=", "reuse", ",", "scope", "=", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool": [[49, 52], ["tensorflow.nn.max_pool"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool"], ["", "def", "max_pool", "(", "x", ",", "filter_height", ",", "filter_width", ",", "stride_y", ",", "stride_x", ",", "padding", "=", "'SAME'", ")", ":", "\n", "    ", "\"\"\"Create a max pooling layer.\"\"\"", "\n", "return", "tf", ".", "nn", ".", "max_pool", "(", "x", ",", "ksize", "=", "[", "1", ",", "filter_height", ",", "filter_width", ",", "1", "]", ",", "strides", "=", "[", "1", ",", "stride_y", ",", "stride_x", ",", "1", "]", ",", "padding", "=", "padding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn": [[53, 56], ["tensorflow.nn.local_response_normalization"], "function", ["None"], ["", "def", "lrn", "(", "x", ",", "radius", ",", "alpha", ",", "beta", ",", "bias", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"Create a local response normalization layer.\"\"\"", "\n", "return", "tf", ".", "nn", ".", "local_response_normalization", "(", "x", ",", "depth_radius", "=", "radius", ",", "alpha", "=", "alpha", ",", "beta", "=", "beta", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout": [[57, 60], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout"], ["", "def", "dropout", "(", "x", ",", "keep_prob", ")", ":", "\n", "    ", "\"\"\"Create a dropout layer.\"\"\"", "\n", "return", "tf", ".", "nn", ".", "dropout", "(", "x", ",", "keep_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc": [[61, 74], ["tensorflow.nn.xw_plus_b", "tensorflow.nn.relu", "tensorflow.nn.leaky_relu"], "function", ["None"], ["", "def", "fc", "(", "x", ",", "wweight", ",", "bweight", ",", "activation", "=", "None", ")", ":", "\n", "    ", "\"\"\"Create a fully connected layer.\"\"\"", "\n", "\n", "act", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "x", ",", "wweight", ",", "bweight", ")", "\n", "\n", "if", "activation", "is", "'relu'", ":", "\n", "        ", "return", "tf", ".", "nn", ".", "relu", "(", "act", ")", "\n", "", "elif", "activation", "is", "'leaky_relu'", ":", "\n", "        ", "return", "tf", ".", "nn", ".", "leaky_relu", "(", "act", ")", "\n", "", "elif", "activation", "is", "None", ":", "\n", "        ", "return", "act", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.mse": [[76, 80], ["tensorflow.reshape", "tensorflow.reshape", "tensorflow.reduce_mean", "tensorflow.square"], "function", ["None"], ["", "", "def", "mse", "(", "pred", ",", "label", ")", ":", "\n", "    ", "pred", "=", "tf", ".", "reshape", "(", "pred", ",", "[", "-", "1", "]", ")", "\n", "label", "=", "tf", ".", "reshape", "(", "label", ",", "[", "-", "1", "]", ")", "\n", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "pred", "-", "label", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.xent": [[81, 83], ["tensorflow.nn.softmax_cross_entropy_with_logits_v2"], "function", ["None"], ["", "def", "xent", "(", "pred", ",", "label", ")", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "logits", "=", "pred", ",", "labels", "=", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.kd": [[84, 116], ["range", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "prob1s.append", "prob2s.append", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.log", "tensorflow.log"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log"], ["", "def", "kd", "(", "data1", ",", "label1", ",", "data2", ",", "label2", ",", "bool_indicator", ",", "n_class", "=", "7", ",", "temperature", "=", "2.0", ")", ":", "\n", "\n", "    ", "kd_loss", "=", "0.0", "\n", "eps", "=", "1e-16", "\n", "\n", "prob1s", "=", "[", "]", "\n", "prob2s", "=", "[", "]", "\n", "\n", "for", "cls", "in", "range", "(", "n_class", ")", ":", "\n", "        ", "mask1", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label1", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum1", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data1", ",", "mask1", ")", ",", "axis", "=", "0", ")", "\n", "num1", "=", "tf", ".", "reduce_sum", "(", "label1", "[", ":", ",", "cls", "]", ")", "\n", "activations1", "=", "logits_sum1", "*", "1.0", "/", "(", "num1", "+", "eps", ")", "# add eps for prevent un-sampled class resulting in NAN", "\n", "prob1", "=", "tf", ".", "nn", ".", "softmax", "(", "activations1", "/", "temperature", ")", "\n", "prob1", "=", "tf", ".", "clip_by_value", "(", "prob1", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "# for preventing prob=0 resulting in NAN", "\n", "\n", "mask2", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label2", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum2", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data2", ",", "mask2", ")", ",", "axis", "=", "0", ")", "\n", "num2", "=", "tf", ".", "reduce_sum", "(", "label2", "[", ":", ",", "cls", "]", ")", "\n", "activations2", "=", "logits_sum2", "*", "1.0", "/", "(", "num2", "+", "eps", ")", "\n", "prob2", "=", "tf", ".", "nn", ".", "softmax", "(", "activations2", "/", "temperature", ")", "\n", "prob2", "=", "tf", ".", "clip_by_value", "(", "prob2", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "\n", "\n", "KL_div", "=", "(", "tf", ".", "reduce_sum", "(", "prob1", "*", "tf", ".", "log", "(", "prob1", "/", "prob2", ")", ")", "+", "tf", ".", "reduce_sum", "(", "prob2", "*", "tf", ".", "log", "(", "prob2", "/", "prob1", ")", ")", ")", "/", "2.0", "\n", "kd_loss", "+=", "KL_div", "*", "bool_indicator", "[", "cls", "]", "\n", "\n", "prob1s", ".", "append", "(", "prob1", ")", "\n", "prob2s", ".", "append", "(", "prob2", ")", "\n", "\n", "", "kd_loss", "=", "kd_loss", "/", "n_class", "\n", "\n", "return", "kd_loss", ",", "prob1s", ",", "prob2s", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.JS": [[117, 151], ["range", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.softmax", "tensorflow.clip_by_value", "prob1s.append", "prob2s.append", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.log", "tensorflow.log"], "function", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.DataframeSplitter.DataframeSplitter.DataframeSplitter.log"], ["", "def", "JS", "(", "data1", ",", "label1", ",", "data2", ",", "label2", ",", "bool_indicator", ",", "n_class", "=", "7", ",", "temperature", "=", "2.0", ")", ":", "\n", "\n", "    ", "kd_loss", "=", "0.0", "\n", "eps", "=", "1e-16", "\n", "\n", "prob1s", "=", "[", "]", "\n", "prob2s", "=", "[", "]", "\n", "\n", "for", "cls", "in", "range", "(", "n_class", ")", ":", "\n", "        ", "mask1", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label1", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum1", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data1", ",", "mask1", ")", ",", "axis", "=", "0", ")", "\n", "num1", "=", "tf", ".", "reduce_sum", "(", "label1", "[", ":", ",", "cls", "]", ")", "\n", "activations1", "=", "logits_sum1", "*", "1.0", "/", "(", "num1", "+", "eps", ")", "# add eps for prevent un-sampled class resulting in NAN", "\n", "prob1", "=", "tf", ".", "nn", ".", "softmax", "(", "activations1", "/", "temperature", ")", "\n", "prob1", "=", "tf", ".", "clip_by_value", "(", "prob1", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "# for preventing prob=0 resulting in NAN", "\n", "\n", "mask2", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "label2", "[", ":", ",", "cls", "]", ",", "-", "1", ")", ",", "[", "1", ",", "n_class", "]", ")", "\n", "logits_sum2", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "data2", ",", "mask2", ")", ",", "axis", "=", "0", ")", "\n", "num2", "=", "tf", ".", "reduce_sum", "(", "label2", "[", ":", ",", "cls", "]", ")", "\n", "activations2", "=", "logits_sum2", "*", "1.0", "/", "(", "num2", "+", "eps", ")", "\n", "prob2", "=", "tf", ".", "nn", ".", "softmax", "(", "activations2", "/", "temperature", ")", "\n", "prob2", "=", "tf", ".", "clip_by_value", "(", "prob2", ",", "clip_value_min", "=", "1e-8", ",", "clip_value_max", "=", "1.0", ")", "\n", "\n", "mean_prob", "=", "(", "prob1", "+", "prob2", ")", "/", "2", "\n", "\n", "JS_div", "=", "(", "tf", ".", "reduce_sum", "(", "prob1", "*", "tf", ".", "log", "(", "prob1", "/", "mean_prob", ")", ")", "+", "tf", ".", "reduce_sum", "(", "prob2", "*", "tf", ".", "log", "(", "prob2", "/", "mean_prob", ")", ")", ")", "/", "2.0", "\n", "kd_loss", "+=", "JS_div", "*", "bool_indicator", "[", "cls", "]", "\n", "\n", "prob1s", ".", "append", "(", "prob1", ")", "\n", "prob2s", ".", "append", "(", "prob2", ")", "\n", "\n", "", "kd_loss", "=", "kd_loss", "/", "n_class", "\n", "\n", "return", "kd_loss", ",", "prob1s", ",", "prob2s", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.contrastive": [[152, 173], ["tensorflow.argmax", "tensorflow.argmax", "tensorflow.to_float", "tensorflow.sqrt", "tensorflow.square", "tensorflow.equal", "tensorflow.reduce_sum", "tensorflow.nn.relu", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.reduce_sum", "tensorflow.reduce_sum"], "function", ["None"], ["", "def", "contrastive", "(", "feature1", ",", "label1", ",", "feature2", ",", "label2", ",", "bool_indicator", "=", "None", ",", "margin", "=", "50", ")", ":", "\n", "\n", "    ", "l1", "=", "tf", ".", "argmax", "(", "label1", ",", "axis", "=", "1", ")", "\n", "l2", "=", "tf", ".", "argmax", "(", "label2", ",", "axis", "=", "1", ")", "\n", "pair", "=", "tf", ".", "to_float", "(", "tf", ".", "equal", "(", "l1", ",", "l2", ")", ")", "\n", "\n", "delta", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "feature1", "-", "feature2", ")", ",", "1", ")", "+", "1e-10", "\n", "match_loss", "=", "delta", "\n", "\n", "delta_sqrt", "=", "tf", ".", "sqrt", "(", "delta", "+", "1e-10", ")", "\n", "mismatch_loss", "=", "tf", ".", "square", "(", "tf", ".", "nn", ".", "relu", "(", "margin", "-", "delta_sqrt", ")", ")", "\n", "\n", "if", "bool_indicator", "is", "None", ":", "\n", "        ", "loss", "=", "tf", ".", "reduce_mean", "(", "0.5", "*", "(", "pair", "*", "match_loss", "+", "(", "1", "-", "pair", ")", "*", "mismatch_loss", ")", ")", "\n", "", "else", ":", "\n", "        ", "loss", "=", "0.5", "*", "tf", ".", "reduce_sum", "(", "match_loss", "*", "pair", ")", "/", "tf", ".", "reduce_sum", "(", "pair", ")", "\n", "\n", "", "debug_dist_positive", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "pair", ")", "/", "tf", ".", "reduce_sum", "(", "pair", ")", "\n", "debug_dist_negative", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "(", "1", "-", "pair", ")", ")", "/", "tf", ".", "reduce_sum", "(", "1", "-", "pair", ")", "\n", "\n", "return", "loss", ",", "pair", ",", "delta", ",", "debug_dist_positive", ",", "debug_dist_negative", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.compute_distance": [[174, 186], ["tensorflow.argmax", "tensorflow.argmax", "tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.sqrt", "tensorflow.equal", "tensorflow.square", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum"], "function", ["None"], ["", "def", "compute_distance", "(", "feature1", ",", "label1", ",", "feature2", ",", "label2", ")", ":", "\n", "    ", "l1", "=", "tf", ".", "argmax", "(", "label1", ",", "axis", "=", "1", ")", "\n", "l2", "=", "tf", ".", "argmax", "(", "label2", ",", "axis", "=", "1", ")", "\n", "pair", "=", "tf", ".", "to_float", "(", "tf", ".", "equal", "(", "l1", ",", "l2", ")", ")", "\n", "\n", "delta", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "feature1", "-", "feature2", ")", ",", "1", ")", "\n", "delta_sqrt", "=", "tf", ".", "sqrt", "(", "delta", "+", "1e-16", ")", "\n", "\n", "dist_positive_pair", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "pair", ")", "/", "tf", ".", "reduce_sum", "(", "pair", ")", "\n", "dist_negative_pair", "=", "tf", ".", "reduce_sum", "(", "delta_sqrt", "*", "(", "1", "-", "pair", ")", ")", "/", "tf", ".", "reduce_sum", "(", "1", "-", "pair", ")", "\n", "\n", "return", "dist_positive_pair", ",", "dist_negative_pair", "\n", "", ""]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.__init__": [[17, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "WEIGHTS_PATH", ",", "feature_space_dimension", ")", ":", "\n", "        ", "\"\"\" Call construct_model_*() after initializing MASF\"\"\"", "\n", "self", ".", "inner_lr", "=", "FLAGS", ".", "inner_lr", "\n", "self", ".", "outer_lr", "=", "FLAGS", ".", "outer_lr", "\n", "self", ".", "metric_lr", "=", "FLAGS", ".", "metric_lr", "\n", "self", ".", "SKIP_LAYER", "=", "[", "'fc8'", "]", "\n", "self", ".", "forward", "=", "self", ".", "forward_alexnet", "\n", "self", ".", "forward_metric_net", "=", "self", ".", "forward_metric_net", "\n", "self", ".", "construct_weights", "=", "self", ".", "construct_alexnet_weights", "\n", "self", ".", "loss_func", "=", "xent", "\n", "self", ".", "global_loss_func", "=", "kd", "\n", "self", ".", "WEIGHTS_PATH", "=", "WEIGHTS_PATH", "\n", "self", ".", "feature_space_dimension", "=", "feature_space_dimension", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_model_train": [[31, 147], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "tensorflow.Variable", "masf_func.MASF.construct_model_train.task_metalearn"], "methods", ["None"], ["", "def", "construct_model_train", "(", "self", ",", "prefix", "=", "'metatrain_'", ")", ":", "\n", "# a: meta-train for inner update, b: meta-test for meta loss", "\n", "        ", "self", ".", "inputa", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputa1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labela1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "inputb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "labelb", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "bool_indicator_b_a", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "self", ".", "bool_indicator_b_a1", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "FLAGS", ".", "num_classes", ",", ")", ")", "\n", "\n", "meta_sample_num", "=", "(", "FLAGS", ".", "meta_batch_size", "/", "3", ")", "*", "3", "\n", "self", ".", "input_group", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "label_group", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "meta_sample_num", ",", ")", ")", "\n", "\n", "self", ".", "clip_value", "=", "FLAGS", ".", "gradients_clip_value", "\n", "self", ".", "margin", "=", "FLAGS", ".", "margin", "\n", "self", ".", "KEEP_PROB", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "training_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "print", "(", "'weights already defined'", ")", "\n", "training_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "self", ".", "weights", "=", "weights", "=", "self", ".", "construct_weights", "(", ")", "\n", "\n", "", "def", "task_metalearn", "(", "inp", ",", "global_bool_indicator", ",", "reuse", "=", "True", ")", ":", "\n", "# Function to perform meta learning update \"\"\"", "\n", "                ", "inputa", ",", "inputa1", ",", "inputb", ",", "input_group", ",", "labela", ",", "labela1", ",", "labelb", ",", "label_group", "=", "inp", "\n", "global_bool_indicator_b_a", ",", "global_bool_indicator_b_a1", "=", "global_bool_indicator", "\n", "\n", "# Obtaining the conventional task loss on meta-train", "\n", "_", ",", "task_outputa", "=", "self", ".", "forward", "(", "inputa", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa", "=", "self", ".", "loss_func", "(", "task_outputa", ",", "labela", ")", "\n", "_", ",", "task_outputa1", "=", "self", ".", "forward", "(", "inputa1", ",", "weights", ",", "reuse", "=", "reuse", ")", "\n", "task_lossa1", "=", "self", ".", "loss_func", "(", "task_outputa1", ",", "labela1", ")", "\n", "\n", "## perform inner update with plain gradient descent on meta-train", "\n", "grads", "=", "tf", ".", "gradients", "(", "(", "task_lossa", "+", "task_lossa1", ")", "/", "2.0", ",", "list", "(", "weights", ".", "values", "(", ")", ")", ")", "\n", "grads", "=", "[", "tf", ".", "stop_gradient", "(", "grad", ")", "for", "grad", "in", "grads", "]", "# first-order gradients approximation", "\n", "gradients", "=", "dict", "(", "zip", "(", "weights", ".", "keys", "(", ")", ",", "grads", ")", ")", "\n", "fast_weights", "=", "dict", "(", "zip", "(", "weights", ".", "keys", "(", ")", ",", "[", "weights", "[", "key", "]", "-", "self", ".", "inner_lr", "*", "tf", ".", "clip_by_norm", "(", "gradients", "[", "key", "]", ",", "clip_norm", "=", "self", ".", "clip_value", ")", "for", "key", "in", "weights", ".", "keys", "(", ")", "]", ")", ")", "\n", "\n", "## compute global loss", "\n", "_", ",", "new_task_outputa", "=", "self", ".", "forward", "(", "inputa", ",", "fast_weights", ",", "reuse", "=", "reuse", ")", "\n", "_", ",", "new_task_outputa1", "=", "self", ".", "forward", "(", "inputa1", ",", "fast_weights", ",", "reuse", "=", "reuse", ")", "\n", "_", ",", "task_outputb", "=", "self", ".", "forward", "(", "inputb", ",", "fast_weights", ",", "reuse", "=", "reuse", ")", "\n", "global_loss_b_a", ",", "_", ",", "_", "=", "self", ".", "global_loss_func", "(", "task_outputb", ",", "labelb", ",", "new_task_outputa", ",", "labela", ",", "global_bool_indicator_b_a", ",", "n_class", "=", "FLAGS", ".", "num_classes", ")", "\n", "global_loss_b_a1", ",", "_", ",", "_", "=", "self", ".", "global_loss_func", "(", "task_outputb", ",", "labelb", ",", "new_task_outputa1", ",", "labela1", ",", "global_bool_indicator_b_a1", ",", "n_class", "=", "FLAGS", ".", "num_classes", ")", "\n", "global_loss", "=", "(", "global_loss_b_a", "+", "global_loss_b_a1", ")", "/", "2.0", "\n", "\n", "## compute local loss", "\n", "embeddings", ",", "_", "=", "self", ".", "forward", "(", "input_group", ",", "fast_weights", ",", "reuse", "=", "True", ")", "\n", "embeddings", "=", "self", ".", "forward_metric_net", "(", "embeddings", ")", "\n", "metric_loss", "=", "tf", ".", "contrib", ".", "losses", ".", "metric_learning", ".", "triplet_semihard_loss", "(", "labels", "=", "label_group", ",", "embeddings", "=", "embeddings", ",", "margin", "=", "self", ".", "margin", ")", "\n", "\n", "task_output", "=", "[", "global_loss", ",", "task_lossa", ",", "task_lossa1", ",", "metric_loss", "]", "\n", "task_accuracya", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela", ",", "1", ")", ")", "#this accuracy already gathers batch size", "\n", "task_accuracya1", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "task_outputa1", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "labela1", ",", "1", ")", ")", "\n", "task_output", ".", "extend", "(", "[", "task_accuracya", ",", "task_accuracya1", "]", ")", "\n", "\n", "return", "task_output", "\n", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "# self.inner_lr = tf.train.exponential_decay(learning_rate=FLAGS.inner_lr, global_step=self.global_step, decay_steps=FLAGS.decay_steps, decay_rate=FLAGS.decay_rate)", "\n", "\n", "input_tensors", "=", "(", "self", ".", "inputa", ",", "self", ".", "inputa1", ",", "self", ".", "inputb", ",", "self", ".", "input_group", ",", "self", ".", "labela", ",", "self", ".", "labela1", ",", "self", ".", "labelb", ",", "self", ".", "label_group", ")", "\n", "global_bool_indicator", "=", "(", "self", ".", "bool_indicator_b_a", ",", "self", ".", "bool_indicator_b_a1", ")", "\n", "result", "=", "task_metalearn", "(", "inp", "=", "input_tensors", ",", "global_bool_indicator", "=", "global_bool_indicator", ")", "\n", "global_loss", ",", "self", ".", "lossa_raw", ",", "self", ".", "lossa1_raw", ",", "metric_loss", ",", "accuracya", ",", "accuracya1", "=", "result", "\n", "self", ".", "global_loss", "=", "global_loss", "*", "1.0", "\n", "self", ".", "metric_loss", "=", "metric_loss", "*", "0.005", "\n", "\n", "## Performance & Optimization", "\n", "", "if", "'train'", "in", "prefix", ":", "\n", "            ", "self", ".", "lossa", "=", "avg_lossa", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa_raw", ")", "\n", "self", ".", "lossa1", "=", "avg_lossa1", "=", "tf", ".", "reduce_mean", "(", "self", ".", "lossa1_raw", ")", "\n", "self", ".", "source_loss", "=", "(", "avg_lossa", "+", "avg_lossa1", ")", "/", "2.0", "\n", "self", ".", "task_train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "outer_lr", ")", ".", "minimize", "(", "self", ".", "source_loss", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "\n", "self", ".", "accuracya", "=", "accuracya", "*", "100.", "\n", "self", ".", "accuracya1", "=", "accuracya1", "*", "100.", "\n", "self", ".", "source_accuracy", "=", "(", "self", ".", "accuracya", "+", "self", ".", "accuracya1", ")", "/", "2.0", "\n", "\n", "var_list_feature_extractor", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "(", "v", ".", "name", ".", "split", "(", "'/'", ")", "[", "1", "]", "not", "in", "self", ".", "SKIP_LAYER", "and", "'meta'", "not", "in", "v", ".", "name", ".", "split", "(", "'/'", ")", ")", "]", "\n", "var_list_classifier", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "v", ".", "name", ".", "split", "(", "'/'", ")", "[", "1", "]", "in", "self", ".", "SKIP_LAYER", "]", "\n", "var_list_metric", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "'metric'", "in", "v", ".", "name", ".", "split", "(", "'/'", ")", "]", "\n", "\n", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "outer_lr", ")", "\n", "gvs", "=", "optimizer", ".", "compute_gradients", "(", "self", ".", "global_loss", "+", "self", ".", "metric_loss", ",", "var_list", "=", "var_list_feature_extractor", "+", "var_list_classifier", ")", "\n", "\n", "# observe stability of gradients for meta loss", "\n", "l2_norm", "=", "lambda", "t", ":", "tf", ".", "sqrt", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "pow", "(", "t", ",", "2", ")", ")", ")", "\n", "for", "grad", ",", "var", "in", "gvs", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "\"gradients_norm/\"", "+", "var", ".", "name", ",", "l2_norm", "(", "grad", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "\"feature_extractor_var_norm/\"", "+", "var", ".", "name", ",", "l2_norm", "(", "var", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'gradients/'", "+", "var", ".", "name", ",", "var", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "\"feature_extractor_var/\"", "+", "var", ".", "name", ",", "var", ")", "\n", "\n", "", "gvs", "=", "[", "(", "tf", ".", "clip_by_norm", "(", "grad", ",", "clip_norm", "=", "self", ".", "clip_value", ")", ",", "var", ")", "for", "grad", ",", "var", "in", "gvs", "]", "\n", "\n", "for", "grad", ",", "var", "in", "gvs", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "\"gradients_norm_clipped/\"", "+", "var", ".", "name", ",", "l2_norm", "(", "grad", ")", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'gradients_clipped/'", "+", "var", ".", "name", ",", "var", ")", "\n", "\n", "", "self", ".", "meta_train_op", "=", "optimizer", ".", "apply_gradients", "(", "gvs", ")", "\n", "self", ".", "metric_train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "metric_lr", ")", ".", "minimize", "(", "self", ".", "metric_loss", ",", "var_list", "=", "var_list_metric", ")", "\n", "\n", "## Summaries", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 loss'", ",", "self", ".", "lossa", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 loss'", ",", "self", ".", "lossa1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_1 accuracy'", ",", "self", ".", "accuracya", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'source_2 accuracy'", ",", "self", ".", "accuracya1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'global loss'", ",", "self", ".", "global_loss", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "prefix", "+", "'metric loss'", ",", "self", ".", "metric_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_model_test": [[148, 169], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.variable_scope", "masf_func.MASF.forward", "masf_func.MASF.forward_metric_net", "masf_func.MASF.loss_func", "tensorflow.contrib.metrics.accuracy", "tensorflow.nn.softmax", "dir", "testing_scope.reuse_variables", "ValueError", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.nn.softmax"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.forward_metric_net"], ["", "def", "construct_model_test", "(", "self", ",", "prefix", "=", "'test'", ")", ":", "\n", "\n", "        ", "self", ".", "test_input", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "self", ".", "test_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "None", ")", "as", "testing_scope", ":", "\n", "            ", "if", "'weights'", "in", "dir", "(", "self", ")", ":", "\n", "                ", "testing_scope", ".", "reuse_variables", "(", ")", "\n", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Weights not initilized. Create training model before testing model'", ")", "\n", "\n", "", "self", ".", "semantic_feature", ",", "outputs", "=", "self", ".", "forward", "(", "self", ".", "test_input", ",", "weights", ")", "\n", "self", ".", "metric_embedding", "=", "self", ".", "forward_metric_net", "(", "self", ".", "semantic_feature", ")", "\n", "losses", "=", "self", ".", "loss_func", "(", "outputs", ",", "self", ".", "test_label", ")", "\n", "accuracies", "=", "tf", ".", "contrib", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", ",", "1", ")", ",", "tf", ".", "argmax", "(", "self", ".", "test_label", ",", "1", ")", ")", "\n", "self", ".", "pred_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "outputs", ")", "\n", "self", ".", "outputs", "=", "outputs", "\n", "\n", "", "self", ".", "test_loss", "=", "losses", "\n", "self", ".", "test_acc", "=", "accuracies", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.load_initial_weights": [[175, 200], ["numpy.load().item", "numpy.load", "tensorflow.variable_scope", "tensorflow.variable_scope", "len", "tensorflow.get_variable", "session.run", "tensorflow.get_variable", "session.run", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["None"], ["", "def", "load_initial_weights", "(", "self", ",", "session", ")", ":", "\n", "        ", "\"\"\"Load weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n        The weights come as a dict of lists (e.g. weights['conv1'] is a list)\n        Load the weights into the model\n        \"\"\"", "\n", "weights_dict", "=", "np", ".", "load", "(", "self", ".", "WEIGHTS_PATH", ",", "allow_pickle", "=", "True", ",", "encoding", "=", "'bytes'", ")", ".", "item", "(", ")", "\n", "\n", "# Loop over all layer names stored in the weights dict", "\n", "for", "op_name", "in", "weights_dict", ":", "\n", "\n", "# Check if layer should be trained from scratch", "\n", "            ", "if", "op_name", "not", "in", "self", ".", "SKIP_LAYER", ":", "\n", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'model'", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "op_name", ",", "reuse", "=", "True", ")", ":", "\n", "\n", "                        ", "for", "data", "in", "weights_dict", "[", "op_name", "]", ":", "\n", "# Biases", "\n", "                            ", "if", "len", "(", "data", ".", "shape", ")", "==", "1", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "# Weights", "\n", "", "else", ":", "\n", "                                ", "var", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "trainable", "=", "True", ")", "\n", "session", ".", "run", "(", "var", ".", "assign", "(", "data", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.forward_metric_net": [[201, 213], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "utils.fc", "tensorflow.get_variable", "tensorflow.get_variable", "utils.fc"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc"], ["", "", "", "", "", "", "", "def", "forward_metric_net", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'metric'", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "scope", ":", "\n", "\n", "            ", "w1", "=", "tf", ".", "get_variable", "(", "'w1'", ",", "shape", "=", "[", "4096", ",", "1024", "]", ")", "\n", "b1", "=", "tf", ".", "get_variable", "(", "'b1'", ",", "shape", "=", "[", "1024", "]", ")", "\n", "out", "=", "fc", "(", "x", ",", "w1", ",", "b1", ",", "activation", "=", "'leaky_relu'", ")", "\n", "w2", "=", "tf", ".", "get_variable", "(", "'w2'", ",", "shape", "=", "[", "1024", ",", "256", "]", ")", "\n", "b2", "=", "tf", ".", "get_variable", "(", "'b2'", ",", "shape", "=", "[", "256", "]", ")", "\n", "out", "=", "fc", "(", "out", ",", "w2", ",", "b2", ",", "activation", "=", "'leaky_relu'", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.construct_alexnet_weights": [[214, 265], ["tensorflow.contrib.layers.xavier_initializer_conv2d", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable"], "methods", ["None"], ["", "def", "construct_alexnet_weights", "(", "self", ")", ":", "\n", "\n", "        ", "weights", "=", "{", "}", "\n", "conv_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer_conv2d", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "fc_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'conv1'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv1_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "11", ",", "11", ",", "3", ",", "96", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv1_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "96", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv2'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv2_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "5", ",", "5", ",", "48", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv2_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv3'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv3_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "256", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv3_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv4'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv4_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "384", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv4_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "384", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'conv5'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'conv5_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "3", ",", "3", ",", "192", ",", "256", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'conv5_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "256", "]", ")", "\n", "\n", "# with tf.variable_scope('fc6') as scope:", "\n", "#     weights['fc6_weights'] = tf.get_variable('weights', shape=[9216, 4096], initializer=conv_initializer)", "\n", "#     weights['fc6_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc6'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc6_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "9216", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc6_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc7') as scope:", "\n", "#     weights['fc7_weights'] = tf.get_variable('weights', shape=[4096, 4096], initializer=conv_initializer)", "\n", "#     weights['fc7_biases'] = tf.get_variable('biases', [4096])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc7'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc7_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "self", ".", "feature_space_dimension", "]", ",", "initializer", "=", "conv_initializer", ")", "\n", "weights", "[", "'fc7_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "self", ".", "feature_space_dimension", "]", ")", "\n", "\n", "# with tf.variable_scope('fc8') as scope:", "\n", "#     weights['fc8_weights'] = tf.get_variable('weights', shape=[4096, FLAGS.num_classes], initializer=fc_initializer)", "\n", "#     weights['fc8_biases'] = tf.get_variable('biases', [FLAGS.num_classes])", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc8'", ")", "as", "scope", ":", "\n", "            ", "weights", "[", "'fc8_weights'", "]", "=", "tf", ".", "get_variable", "(", "'weights'", ",", "shape", "=", "[", "self", ".", "feature_space_dimension", ",", "FLAGS", ".", "num_classes", "]", ",", "initializer", "=", "fc_initializer", ")", "\n", "weights", "[", "'fc8_biases'", "]", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "FLAGS", ".", "num_classes", "]", ")", "\n", "\n", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.masf_func.MASF.forward_alexnet": [[266, 301], ["utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.lrn", "utils.max_pool", "utils.conv_block", "utils.conv_block", "utils.conv_block", "utils.max_pool", "tensorflow.reshape", "utils.fc", "utils.dropout", "utils.fc", "utils.dropout", "utils.fc"], "methods", ["home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.lrn", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.conv_block", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.max_pool", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.dropout", "home.repos.pwc.inspect_result.bghojogh_Histopathology-Magnification-Generalization.Tumor_type_classification.utils.fc"], ["", "def", "forward_alexnet", "(", "self", ",", "inp", ",", "weights", ",", "reuse", "=", "False", ")", ":", "\n", "# reuse is for the normalization parameters.", "\n", "\n", "        ", "conv1", "=", "conv_block", "(", "inp", ",", "weights", "[", "'conv1_weights'", "]", ",", "weights", "[", "'conv1_biases'", "]", ",", "stride_y", "=", "4", ",", "stride_x", "=", "4", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv1'", ")", "\n", "norm1", "=", "lrn", "(", "conv1", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool1", "=", "max_pool", "(", "norm1", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups", "\n", "conv2", "=", "conv_block", "(", "pool1", ",", "weights", "[", "'conv2_weights'", "]", ",", "weights", "[", "'conv2_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv2'", ")", "\n", "norm2", "=", "lrn", "(", "conv2", ",", "2", ",", "1e-05", ",", "0.75", ")", "\n", "pool2", "=", "max_pool", "(", "norm2", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 3rd Layer: Conv (w ReLu)", "\n", "conv3", "=", "conv_block", "(", "pool2", ",", "weights", "[", "'conv3_weights'", "]", ",", "weights", "[", "'conv3_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "1", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv3'", ")", "\n", "\n", "# 4th Layer: Conv (w ReLu) splitted into two groups", "\n", "conv4", "=", "conv_block", "(", "conv3", ",", "weights", "[", "'conv4_weights'", "]", ",", "weights", "[", "'conv4_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv4'", ")", "\n", "\n", "# 5th Layer: Conv (w ReLu) -> Pool splitted into two groups", "\n", "conv5", "=", "conv_block", "(", "conv4", ",", "weights", "[", "'conv5_weights'", "]", ",", "weights", "[", "'conv5_biases'", "]", ",", "stride_y", "=", "1", ",", "stride_x", "=", "1", ",", "groups", "=", "2", ",", "reuse", "=", "reuse", ",", "scope", "=", "'conv5'", ")", "\n", "pool5", "=", "max_pool", "(", "conv5", ",", "3", ",", "3", ",", "2", ",", "2", ",", "padding", "=", "'VALID'", ")", "\n", "\n", "# 6th Layer: Flatten -> FC (w ReLu) -> Dropout", "\n", "flattened", "=", "tf", ".", "reshape", "(", "pool5", ",", "[", "-", "1", ",", "6", "*", "6", "*", "256", "]", ")", "\n", "fc6", "=", "fc", "(", "flattened", ",", "weights", "[", "'fc6_weights'", "]", ",", "weights", "[", "'fc6_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout6", "=", "dropout", "(", "fc6", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 7th Layer: FC (w ReLu) -> Dropout", "\n", "fc7", "=", "fc", "(", "dropout6", ",", "weights", "[", "'fc7_weights'", "]", ",", "weights", "[", "'fc7_biases'", "]", ",", "activation", "=", "'relu'", ")", "\n", "dropout7", "=", "dropout", "(", "fc7", ",", "self", ".", "KEEP_PROB", ")", "\n", "\n", "# 8th Layer: FC and return unscaled activations", "\n", "fc8", "=", "fc", "(", "dropout7", ",", "weights", "[", "'fc8_weights'", "]", ",", "weights", "[", "'fc8_biases'", "]", ")", "\n", "\n", "return", "fc7", ",", "fc8", "\n", "", "", ""]]}