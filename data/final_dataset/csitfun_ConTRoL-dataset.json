{"home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.NLIDataset.__init__": [[197, 202], ["torch.utils.data.Dataset.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_list", ",", "transform", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_list", "=", "data_list", "\n", "self", ".", "len", "=", "len", "(", "self", ".", "d_list", ")", "\n", "self", ".", "transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.NLIDataset.__getitem__": [[203, 205], ["training.NLIDataset.transform"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "transform", "(", "self", ".", "d_list", "[", "index", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.NLIDataset.__len__": [[208, 210], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "len", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.NLITransform.__init__": [[213, 217], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "tokenizer", ",", "max_length", "=", "None", ")", ":", "\n", "        ", "self", ".", "model_name", "=", "model_name", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.NLITransform.__call__": [[218, 245], ["dict", "training.NLITransform.tokenizer.encode_plus", "training.NLITransform.tokenizer", "dict.update", "premise.strip", "hypothesis.strip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "sample", ")", ":", "\n", "        ", "processed_sample", "=", "dict", "(", ")", "\n", "processed_sample", "[", "'uid'", "]", "=", "sample", "[", "'uid'", "]", "\n", "processed_sample", "[", "'gold_label'", "]", "=", "sample", "[", "'label'", "]", "\n", "processed_sample", "[", "'y'", "]", "=", "nli_label2index", "[", "sample", "[", "'label'", "]", "]", "\n", "\n", "# premise: str = sample['premise']", "\n", "premise", ":", "str", "=", "sample", "[", "'context'", "]", "if", "'context'", "in", "sample", "else", "sample", "[", "'premise'", "]", "\n", "hypothesis", ":", "str", "=", "sample", "[", "'hypothesis'", "]", "\n", "\n", "if", "premise", ".", "strip", "(", ")", "==", "''", ":", "\n", "            ", "premise", "=", "'empty'", "\n", "\n", "", "if", "hypothesis", ".", "strip", "(", ")", "==", "''", ":", "\n", "            ", "hypothesis", "=", "'empty'", "\n", "# premise = \" \"", "\n", "# hypothesis = \" \"", "\n", "\n", "", "tokenized_input_seq_pair", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "premise", ",", "hypothesis", ",", "\n", "max_length", "=", "self", ".", "max_length", ",", "\n", "return_token_type_ids", "=", "True", ",", "truncation", "=", "True", ")", "\n", "inputs", "=", "self", ".", "tokenizer", "(", "\"Hello, my dog is cute\"", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "\n", "\n", "processed_sample", ".", "update", "(", "tokenized_input_seq_pair", ")", "\n", "\n", "return", "processed_sample", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.set_seed": [[190, 194], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.build_eval_dataset_loader_and_sampler": [[247, 258], ["training.NLIDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "data_utils.batchbuilder.BaseBatchBuilder"], "function", ["None"], ["", "", "def", "build_eval_dataset_loader_and_sampler", "(", "d_list", ",", "data_transformer", ",", "batching_schema", ",", "batch_size_per_gpu_eval", ")", ":", "\n", "    ", "d_dataset", "=", "NLIDataset", "(", "d_list", ",", "data_transformer", ")", "\n", "d_sampler", "=", "SequentialSampler", "(", "d_dataset", ")", "\n", "d_dataloader", "=", "DataLoader", "(", "dataset", "=", "d_dataset", ",", "\n", "batch_size", "=", "batch_size_per_gpu_eval", ",", "\n", "shuffle", "=", "False", ",", "#", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "d_sampler", ",", "\n", "collate_fn", "=", "BaseBatchBuilder", "(", "batching_schema", ")", ")", "#", "\n", "return", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.sample_data_list": [[260, 276], ["int", "ValueError", "math.ceil", "range", "numpy.isclose", "sampled_d_list.extend", "int", "random.shuffle", "copy.deepcopy", "len"], "function", ["None"], ["", "def", "sample_data_list", "(", "d_list", ",", "ratio", ")", ":", "\n", "    ", "if", "ratio", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid training weight ratio. Please change --train_weights.\"", ")", "\n", "", "upper_int", "=", "int", "(", "math", ".", "ceil", "(", "ratio", ")", ")", "\n", "if", "upper_int", "==", "1", ":", "\n", "        ", "return", "d_list", "# if ratio is 1 then we just return the data list", "\n", "", "else", ":", "\n", "        ", "sampled_d_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "upper_int", ")", ":", "\n", "            ", "sampled_d_list", ".", "extend", "(", "copy", ".", "deepcopy", "(", "d_list", ")", ")", "\n", "", "if", "np", ".", "isclose", "(", "ratio", ",", "upper_int", ")", ":", "\n", "            ", "return", "sampled_d_list", "\n", "", "else", ":", "\n", "            ", "sampled_length", "=", "int", "(", "ratio", "*", "len", "(", "d_list", ")", ")", "\n", "random", ".", "shuffle", "(", "sampled_d_list", ")", "\n", "return", "sampled_d_list", "[", ":", "sampled_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.main": [[278, 381], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "training.train", "training.train", "torch.spawn"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.train", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.train"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--cpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use CPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--single_gpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use single GPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we will use fp16.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--transfer\"", ",", "default", "=", "False", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16_opt_level\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"O1\"", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ",", "\n", ")", "\n", "\n", "# environment arguments", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--seed'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'manual random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_nodes'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of nodes'", ")", "\n", "parser", ".", "add_argument", "(", "'-g'", ",", "'--gpus_per_node'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of gpus per node'", ")", "\n", "parser", ".", "add_argument", "(", "'-nr'", ",", "'--node_rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'ranking within the nodes'", ")", "\n", "\n", "# experiments specific arguments", "\n", "parser", ".", "add_argument", "(", "'--debug_mode'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "dest", "=", "'debug_mode'", ",", "\n", "help", "=", "'weather this is debug mode or normal'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_class_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the model class of the experiment.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--experiment_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the name of the experiment. [model_name]/[data]/[task]/[other]\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--save_prediction\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "dest", "=", "'save_prediction'", ",", "\n", "help", "=", "'Do we want to save prediction'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "default", "=", "2", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of total epochs to run'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "16", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for training.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gradient_accumulation_steps\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "64", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "default", "=", "160", ",", "type", "=", "int", ",", "help", "=", "\"Max length of the sequences.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval_frequency\"", ",", "default", "=", "2000", ",", "type", "=", "int", ",", "help", "=", "\"set the evaluation frequency, evaluate every X global step.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_weights\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data weights used in the experiments.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--eval_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "args", ".", "world_size", "=", "1", "\n", "train", "(", "-", "1", ",", "args", ")", "\n", "", "elif", "args", ".", "single_gpu", ":", "\n", "        ", "args", ".", "world_size", "=", "1", "\n", "train", "(", "0", ",", "args", ")", "\n", "", "else", ":", "# distributed multiGPU training", "\n", "#########################################################", "\n", "        ", "args", ".", "world_size", "=", "args", ".", "gpus_per_node", "*", "args", ".", "num_nodes", "#", "\n", "# os.environ['MASTER_ADDR'] = '152.2.142.184'  # This is the IP address for nlp5", "\n", "# maybe we will automatically retrieve the IP later.", "\n", "os", ".", "environ", "[", "'MASTER_PORT'", "]", "=", "'88888'", "#", "\n", "mp", ".", "spawn", "(", "train", ",", "nprocs", "=", "args", ".", "gpus_per_node", ",", "args", "=", "(", "args", ",", ")", ")", "# spawn how many process in this node", "\n", "# remember train is called as train(i, args).", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.train": [[385, 765], ["training.set_seed", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained", "train_data_str.split", "eval_data_str.split", "training.NLITransform", "range", "len", "print", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "dict", "tqdm.tqdm", "print", "nn.parallel.DistributedDataParallel.load_state_dict", "model_class_item[].from_pretrained.convert_tokens_to_ids", "torch.init_process_group", "train_data_weights_str.split", "named_path.find", "train_data_name.append", "train_data_path.append", "train_data_list.append", "range", "named_path.find", "eval_data_name.append", "eval_data_path.append", "eval_data_list.append", "data_utils.fields.RawFlintField", "data_utils.fields.LabelFlintField", "data_utils.fields.ArrayIndexFlintField", "data_utils.fields.ArrayIndexFlintField", "data_utils.fields.ArrayIndexFlintField", "training.build_eval_dataset_loader_and_sampler", "eval_data_loaders.append", "len", "print", "training.sample_data_list", "print", "training_list.extend", "int", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "nn.parallel.DistributedDataParallel.cuda", "amp.initialize", "torch.parallel.DistributedDataParallel", "vars", "print", "print", "print", "print", "range", "range", "random.shuffle", "training.NLIDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "print", "enumerate", "str", "str", "torch.load", "torch.load", "torch.load", "torch.load", "utils.common.load_jsonl", "utils.common.load_jsonl", "train_data_weights.append", "len", "train_data_weights.append", "utils.common.load_jsonl", "utils.common.load_jsonl", "pp.pprint", "utils.save_tool.gen_file_prefix", "os.path.basename", "utils.common.save_json", "len", "print", "training.sample_data_list", "print", "training_list.extend", "print", "torch.utils.data.DistributedSampler", "training.debug_node_info", "torch.utils.data.DistributedSampler.set_epoch", "tqdm.tqdm", "nn.parallel.DistributedDataParallel.train", "data_utils.batchbuilder.move_to_device", "dict", "range", "range", "float", "ImportError", "open", "open", "out_f.write", "out_f.flush", "os.path.join", "pathlib.Path", "checkpoints_path.exists", "checkpoints_path.mkdir", "pathlib.Path", "prediction_path.exists", "prediction_path.mkdir", "data_utils.batchbuilder.BaseBatchBuilder", "nn.parallel.DistributedDataParallel.", "nn.parallel.DistributedDataParallel.", "loss.backward", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "nn.parallel.DistributedDataParallel.zero_grad", "len", "training.evaluation_dataset", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "dict.items", "dict.items", "utils.common.save_json", "len", "len", "nn.parallel.DistributedDataParallel.named_parameters", "nn.parallel.DistributedDataParallel.named_parameters", "any", "os.path.join", "it.read", "amp.scale_loss", "scaled_loss.backward", "dict", "range", "range", "model_output_dir.exists", "model_output_dir.mkdir", "hasattr", "model_to_save.state_dict", "str", "transformers.AdamW.state_dict", "str", "transformers.get_linear_schedule_with_warmup.state_dict", "str", "cur_results_path.exists", "cur_results_path.mkdir", "utils.common.save_jsonl", "any", "len", "len", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "len", "training.evaluation_dataset", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "dict.items", "dict.items", "utils.common.save_json", "round", "amp.master_params", "nn.parallel.DistributedDataParallel.parameters", "model_output_dir.exists", "model_output_dir.mkdir", "hasattr", "model_to_save.state_dict", "str", "transformers.AdamW.state_dict", "str", "transformers.get_linear_schedule_with_warmup.state_dict", "str", "cur_results_path.exists", "cur_results_path.mkdir", "utils.common.save_jsonl", "round"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.set_seed", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.build_eval_dataset_loader_and_sampler", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.sample_data_list", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_json", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.sample_data_list", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.debug_node_info", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.train", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.evaluation_dataset", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_json", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.evaluation_dataset", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_json", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_jsonl"], ["", "", "def", "train", "(", "local_rank", ",", "args", ")", ":", "\n", "# debug = False", "\n", "# print(\"GPU:\", gpu)", "\n", "# world_size = args.world_size", "\n", "    ", "args", ".", "global_rank", "=", "args", ".", "node_rank", "*", "args", ".", "gpus_per_node", "+", "local_rank", "\n", "args", ".", "local_rank", "=", "local_rank", "\n", "# args.warmup_steps = 20", "\n", "debug_count", "=", "1000", "\n", "num_epoch", "=", "args", ".", "epochs", "\n", "\n", "actual_train_batch_size", "=", "args", ".", "world_size", "*", "args", ".", "per_gpu_train_batch_size", "*", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "actual_train_batch_size", "=", "actual_train_batch_size", "\n", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "num_labels", "=", "3", "# we are doing NLI so we set num_labels = 3, for other task we can change this value.", "\n", "\n", "max_length", "=", "args", ".", "max_length", "\n", "\n", "model_class_item", "=", "MODEL_CLASSES", "[", "args", ".", "model_class_name", "]", "\n", "model_name", "=", "model_class_item", "[", "'model_name'", "]", "\n", "do_lower_case", "=", "model_class_item", "[", "'do_lower_case'", "]", "if", "'do_lower_case'", "in", "model_class_item", "else", "False", "\n", "\n", "tokenizer", "=", "model_class_item", "[", "'tokenizer'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "\n", "if", "args", ".", "transfer", ":", "\n", "        ", "print", "(", "'--------'", ")", "\n", "model_checkpoint_path", "=", "'saved_models/transfer/checkpoints/eval/model.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_checkpoint_path", ")", ")", "\n", "\n", "", "padding_token_value", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", "\n", "padding_segement_value", "=", "model_class_item", "[", "\"padding_segement_value\"", "]", "\n", "padding_att_value", "=", "model_class_item", "[", "\"padding_att_value\"", "]", "\n", "left_pad", "=", "model_class_item", "[", "'left_pad'", "]", "if", "'left_pad'", "in", "model_class_item", "else", "False", "\n", "\n", "batch_size_per_gpu_train", "=", "args", ".", "per_gpu_train_batch_size", "\n", "batch_size_per_gpu_eval", "=", "args", ".", "per_gpu_eval_batch_size", "\n", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "        ", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "'nccl'", ",", "\n", "init_method", "=", "'env://'", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "\n", "rank", "=", "args", ".", "global_rank", "\n", ")", "\n", "\n", "", "train_data_str", "=", "args", ".", "train_data", "\n", "train_data_weights_str", "=", "args", ".", "train_weights", "\n", "eval_data_str", "=", "args", ".", "eval_data", "\n", "\n", "train_data_name", "=", "[", "]", "\n", "train_data_path", "=", "[", "]", "\n", "train_data_list", "=", "[", "]", "\n", "train_data_weights", "=", "[", "]", "\n", "\n", "eval_data_name", "=", "[", "]", "\n", "eval_data_path", "=", "[", "]", "\n", "eval_data_list", "=", "[", "]", "\n", "\n", "train_data_named_path", "=", "train_data_str", ".", "split", "(", "','", ")", "\n", "weights_str", "=", "train_data_weights_str", ".", "split", "(", "','", ")", "if", "train_data_weights_str", "is", "not", "None", "else", "None", "\n", "\n", "eval_data_named_path", "=", "eval_data_str", ".", "split", "(", "','", ")", "\n", "\n", "for", "named_path", "in", "train_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "name", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "\n", "", "train_data_name", ".", "append", "(", "name", ")", "\n", "train_data_path", ".", "append", "(", "path", ")", "\n", "\n", "train_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "if", "weights_str", "is", "not", "None", ":", "\n", "        ", "for", "weights", "in", "weights_str", ":", "\n", "            ", "train_data_weights", ".", "append", "(", "float", "(", "weights", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "            ", "train_data_weights", ".", "append", "(", "1", ")", "\n", "\n", "", "", "for", "named_path", "in", "eval_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "name", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "", "eval_data_name", ".", "append", "(", "name", ")", "\n", "eval_data_path", ".", "append", "(", "path", ")", "\n", "\n", "eval_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "# assert len(train_data_weights) == len(train_data_list)", "\n", "\n", "", "batching_schema", "=", "{", "\n", "'uid'", ":", "RawFlintField", "(", ")", ",", "\n", "'y'", ":", "LabelFlintField", "(", ")", ",", "\n", "'input_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_token_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'token_type_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_segement_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'attention_mask'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_att_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "}", "\n", "\n", "data_transformer", "=", "NLITransform", "(", "model_name", ",", "tokenizer", ",", "max_length", ")", "\n", "# data_transformer = NLITransform(model_name, tokenizer, max_length, with_element=True)", "\n", "\n", "eval_data_loaders", "=", "[", "]", "\n", "for", "eval_d_list", "in", "eval_data_list", ":", "\n", "        ", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "=", "build_eval_dataset_loader_and_sampler", "(", "eval_d_list", ",", "data_transformer", ",", "\n", "batching_schema", ",", "\n", "batch_size_per_gpu_eval", ")", "\n", "eval_data_loaders", ".", "append", "(", "d_dataloader", ")", "\n", "\n", "# Estimate the training size:", "\n", "", "training_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "        ", "print", "(", "\"Build Training Data ...\"", ")", "\n", "train_d_list", "=", "train_data_list", "[", "i", "]", "\n", "train_d_name", "=", "train_data_name", "[", "i", "]", "\n", "train_d_weight", "=", "train_data_weights", "[", "i", "]", "\n", "cur_train_list", "=", "sample_data_list", "(", "train_d_list", ",", "train_d_weight", ")", "# change later  # we can apply different sample strategy here.", "\n", "print", "(", "f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"", "\n", "f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\"", ")", "\n", "training_list", ".", "extend", "(", "cur_train_list", ")", "\n", "", "estimated_training_size", "=", "len", "(", "training_list", ")", "\n", "print", "(", "\"Estimated training size:\"", ",", "estimated_training_size", ")", "\n", "# Estimate the training size ends:", "\n", "\n", "# t_total = estimated_training_size // args.gradient_accumulation_steps * num_epoch", "\n", "t_total", "=", "estimated_training_size", "*", "num_epoch", "//", "args", ".", "actual_train_batch_size", "\n", "if", "args", ".", "warmup_steps", "<=", "0", ":", "# set the warmup steps to 0.1 * total step if the given warmup step is -1.", "\n", "        ", "args", ".", "warmup_steps", "=", "int", "(", "t_total", "*", "0.1", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "model", ".", "cuda", "(", "args", ".", "local_rank", ")", "\n", "\n", "", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "        ", "model", "=", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "local_rank", "]", ",", "\n", "output_device", "=", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "\n", "", "args_dict", "=", "dict", "(", "vars", "(", "args", ")", ")", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "print", "(", "\"Total Steps:\"", ",", "t_total", ")", "\n", "args", ".", "total_step", "=", "t_total", "\n", "print", "(", "\"Warmup Steps:\"", ",", "args", ".", "warmup_steps", ")", "\n", "print", "(", "\"Actual Training Batch Size:\"", ",", "actual_train_batch_size", ")", "\n", "print", "(", "\"Arguments\"", ",", "pp", ".", "pprint", "(", "args", ")", ")", "\n", "\n", "# Let build the logger and log everything before the start of the first training epoch.", "\n", "", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "# only do logging if we use cpu or global_rank=0", "\n", "        ", "if", "not", "args", ".", "debug_mode", ":", "\n", "            ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{args.experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "                ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "\n", "# Save option file", "\n", "", "common", ".", "save_json", "(", "args_dict", ",", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "\"args.json\"", ")", ")", "\n", "checkpoints_path", "=", "Path", "(", "file_path_prefix", ")", "/", "\"checkpoints\"", "\n", "if", "not", "checkpoints_path", ".", "exists", "(", ")", ":", "\n", "                ", "checkpoints_path", ".", "mkdir", "(", ")", "\n", "", "prediction_path", "=", "Path", "(", "file_path_prefix", ")", "/", "\"predictions\"", "\n", "if", "not", "prediction_path", ".", "exists", "(", ")", ":", "\n", "                ", "prediction_path", ".", "mkdir", "(", ")", "\n", "\n", "", "", "", "global_step", "=", "0", "\n", "\n", "# print(f\"Global Rank:{args.global_rank} ### \", 'Init!')", "\n", "\n", "for", "epoch", "in", "tqdm", "(", "range", "(", "num_epoch", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "global_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", ":", "\n", "# Let's build up training dataset for this epoch", "\n", "        ", "training_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "            ", "print", "(", "\"Build Training Data ...\"", ")", "\n", "train_d_list", "=", "train_data_list", "[", "i", "]", "\n", "train_d_name", "=", "train_data_name", "[", "i", "]", "\n", "train_d_weight", "=", "train_data_weights", "[", "i", "]", "\n", "cur_train_list", "=", "sample_data_list", "(", "train_d_list", ",", "train_d_weight", ")", "# change later  # we can apply different sample strategy here.", "\n", "print", "(", "f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"", "\n", "f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\"", ")", "\n", "training_list", ".", "extend", "(", "cur_train_list", ")", "\n", "\n", "", "random", ".", "shuffle", "(", "training_list", ")", "\n", "train_dataset", "=", "NLIDataset", "(", "training_list", ",", "data_transformer", ")", "\n", "\n", "train_sampler", "=", "SequentialSampler", "(", "train_dataset", ")", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "            ", "print", "(", "\"Use distributed sampler.\"", ")", "\n", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ",", "args", ".", "world_size", ",", "args", ".", "global_rank", ",", "\n", "shuffle", "=", "True", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "batch_size_per_gpu_train", ",", "\n", "shuffle", "=", "False", ",", "#", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "train_sampler", ",", "\n", "collate_fn", "=", "BaseBatchBuilder", "(", "batching_schema", ")", ")", "#", "\n", "# training build finished.", "\n", "\n", "print", "(", "debug_node_info", "(", "args", ")", ",", "\"epoch: \"", ",", "epoch", ")", "\n", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "            ", "train_sampler", ".", "set_epoch", "(", "epoch", ")", "# setup the epoch to ensure random sampling at each epoch", "\n", "\n", "", "for", "forward_step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ",", "\n", "disable", "=", "args", ".", "global_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", ",", "0", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "local_rank", ")", "\n", "# print(batch['input_ids'], batch['y'])", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "# print(debug_node_info(args), loss, logits, batch['uid'])", "\n", "# print(debug_node_info(args), loss, batch['uid'])", "\n", "\n", "# Accumulated loss", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "# if this forward step need model updates", "\n", "# handle fp16", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient clip: if max_grad_norm < 0", "\n", "", "if", "(", "forward_step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "if", "args", ".", "fp16", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "eval_frequency", ">", "0", "and", "global_step", "%", "args", ".", "eval_frequency", "==", "0", ":", "\n", "                    ", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# saving checkpoints", "\n", "", "current_checkpoint_filename", "=", "f'e({epoch})|i({global_step})'", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "current_checkpoint_filename", "+=", "f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'", "\n", "\n", "", "if", "not", "args", ".", "debug_mode", ":", "\n", "# save model:", "\n", "                        ", "model_output_dir", "=", "checkpoints_path", "/", "current_checkpoint_filename", "\n", "if", "not", "model_output_dir", ".", "exists", "(", ")", ":", "\n", "                            ", "model_output_dir", ".", "mkdir", "(", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "not", "args", ".", "debug_mode", "and", "args", ".", "save_prediction", ":", "\n", "                        ", "cur_results_path", "=", "prediction_path", "/", "current_checkpoint_filename", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "                            ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                            ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                            ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n", "# End of epoch evaluation.", "\n", "", "", "", "", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# saving checkpoints", "\n", "", "current_checkpoint_filename", "=", "f'e({epoch})|i({global_step})'", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "current_checkpoint_filename", "+=", "f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'", "\n", "\n", "", "if", "not", "args", ".", "debug_mode", ":", "\n", "# save model:", "\n", "                ", "model_output_dir", "=", "checkpoints_path", "/", "current_checkpoint_filename", "\n", "if", "not", "model_output_dir", ".", "exists", "(", ")", ":", "\n", "                    ", "model_output_dir", ".", "mkdir", "(", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "not", "args", ".", "debug_mode", "and", "args", ".", "save_prediction", ":", "\n", "                ", "cur_results_path", "=", "prediction_path", "/", "current_checkpoint_filename", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "                    ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                    ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                    ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.count_acc": [[775, 791], ["utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "print", "list_dict_data_tool.list_to_dict.items", "len", "len", "precision_recall_fscore_support", "list_dict_data_tool.list_to_dict.items", "list_dict_data_tool.list_to_dict.items"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.list_to_dict"], ["def", "count_acc", "(", "gt_list", ",", "pred_list", ")", ":", "\n", "    ", "assert", "len", "(", "gt_list", ")", "==", "len", "(", "pred_list", ")", "\n", "gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "gt_list", ",", "'uid'", ")", "\n", "pred_list", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "pred_list", ",", "'uid'", ")", "\n", "total_count", "=", "0", "\n", "hit", "=", "0", "\n", "predict", "=", "[", "value", "[", "'predicted_label'", "]", "for", "key", ",", "value", "in", "pred_list", ".", "items", "(", ")", "]", "\n", "label", "=", "[", "gt_dict", "[", "key", "]", "[", "'label'", "]", "for", "key", ",", "value", "in", "pred_list", ".", "items", "(", ")", "]", "\n", "# from sklearn.metrics import confusion_matrix", "\n", "from", "sklearn", ".", "metrics", "import", "precision_recall_fscore_support", "\n", "print", "(", "precision_recall_fscore_support", "(", "label", ",", "predict", ")", ")", "\n", "for", "key", ",", "value", "in", "pred_list", ".", "items", "(", ")", ":", "\n", "        ", "if", "gt_dict", "[", "key", "]", "[", "'label'", "]", "==", "value", "[", "'predicted_label'", "]", ":", "\n", "            ", "hit", "+=", "1", "\n", "", "total_count", "+=", "1", "\n", "", "return", "hit", ",", "total_count", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.evaluation_dataset": [[793, 805], ["training.eval_model", "training.count_acc", "print", "training.debug_node_info"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.eval_model", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.count_acc", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.debug_node_info"], ["", "def", "evaluation_dataset", "(", "args", ",", "eval_dataloader", ",", "eval_list", ",", "model", ",", "r_dict", ",", "eval_name", ")", ":", "\n", "# r_dict = dict()", "\n", "    ", "pred_output_list", "=", "eval_model", "(", "model", ",", "eval_dataloader", ",", "args", ".", "global_rank", ",", "args", ")", "\n", "predictions", "=", "pred_output_list", "\n", "hit", ",", "total", "=", "count_acc", "(", "eval_list", ",", "pred_output_list", ")", "\n", "print", "(", "debug_node_info", "(", "args", ")", ",", "f\"{eval_name} Acc:\"", ",", "hit", ",", "total", ",", "hit", "/", "total", ")", "\n", "\n", "r_dict", "[", "f'{eval_name}'", "]", "=", "{", "\n", "'acc'", ":", "hit", "/", "total", ",", "\n", "'correct_count'", ":", "hit", ",", "\n", "'total_count'", ":", "total", ",", "\n", "'predictions'", ":", "predictions", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.eval_model": [[808, 850], ["model.eval", "range", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "data_utils.batchbuilder.move_to_device", "uid_list.extend", "y_list.extend", "pred_list.extend", "logits_list.extend", "model", "model", "list", "batch[].tolist", "[].view().tolist", "logits.tolist", "[].view", "logits.size", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device"], ["", "def", "eval_model", "(", "model", ",", "dev_dataloader", ",", "device_num", ",", "args", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "uid_list", "=", "[", "]", "\n", "y_list", "=", "[", "]", "\n", "pred_list", "=", "[", "]", "\n", "logits_list", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "dev_dataloader", ",", "0", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "\n", "", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "\n", "uid_list", ".", "extend", "(", "list", "(", "batch", "[", "'uid'", "]", ")", ")", "\n", "y_list", ".", "extend", "(", "batch", "[", "'y'", "]", ".", "tolist", "(", ")", ")", "\n", "pred_list", ".", "extend", "(", "torch", ".", "max", "(", "logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "logits_list", ".", "extend", "(", "logits", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "\n", "result_items_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "uid_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'uid'", "]", "=", "uid_list", "[", "i", "]", "\n", "r_item", "[", "'logits'", "]", "=", "logits_list", "[", "i", "]", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "pred_list", "[", "i", "]", "]", "\n", "\n", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.debug_node_info": [[852, 863], ["values.append", "getattr", "zip"], "function", ["None"], ["", "def", "debug_node_info", "(", "args", ")", ":", "\n", "    ", "names", "=", "[", "'global_rank'", ",", "'local_rank'", ",", "'node_rank'", "]", "\n", "values", "=", "[", "]", "\n", "\n", "for", "name", "in", "names", ":", "\n", "        ", "if", "name", "in", "args", ":", "\n", "            ", "values", ".", "append", "(", "getattr", "(", "args", ",", "name", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "\"Pro:No node info \"", "\n", "\n", "", "", "return", "\"Pro:\"", "+", "'|'", ".", "join", "(", "[", "f\"{name}:{value}\"", "for", "name", ",", "value", "in", "zip", "(", "names", ",", "values", ")", "]", ")", "+", "\"||Print:\"", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.evaluation.evaluation": [[18, 146], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained.load_state_dict", "eval_data_str.split", "src.training.NLITransform", "dict", "range", "torch.load", "model_class_item[].from_pretrained.convert_tokens_to_ids", "named_path.find", "eval_data_name.append", "eval_data_path.append", "eval_data_list.append", "data_utils.fields.RawFlintField", "data_utils.fields.LabelFlintField", "data_utils.fields.ArrayIndexFlintField", "data_utils.fields.ArrayIndexFlintField", "data_utils.fields.ArrayIndexFlintField", "src.training.build_eval_dataset_loader_and_sampler", "eval_data_loaders.append", "torch.cuda.set_device", "model_class_item[].from_pretrained.cuda", "len", "src.training.evaluation_dataset", "pathlib.Path", "dict.items", "dict.items", "utils.common.save_json", "str", "str", "utils.common.load_jsonl", "utils.common.load_jsonl", "pathlib.Path.exists", "pathlib.Path.mkdir", "utils.common.save_jsonl"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.build_eval_dataset_loader_and_sampler", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.src.training.evaluation_dataset", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_json", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_jsonl"], ["def", "evaluation", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--cpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use CPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_class_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the model class of the experiment.\"", ",", "\n", "required", "=", "True", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_checkpoint_path\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'Set the path to save the prediction.'", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output_prediction_path\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'Set the path to save the prediction.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "16", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "default", "=", "156", ",", "type", "=", "int", ",", "help", "=", "\"Max length of the sequences.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--eval_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "args", ".", "global_rank", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "args", ".", "global_rank", "=", "0", "\n", "\n", "", "model_checkpoint_path", "=", "args", ".", "model_checkpoint_path", "\n", "num_labels", "=", "3", "\n", "# we are doing NLI so we set num_labels = 3, for other task we can change this value.", "\n", "\n", "max_length", "=", "args", ".", "max_length", "\n", "\n", "model_class_item", "=", "MODEL_CLASSES", "[", "args", ".", "model_class_name", "]", "\n", "model_name", "=", "model_class_item", "[", "'model_name'", "]", "\n", "do_lower_case", "=", "model_class_item", "[", "'do_lower_case'", "]", "if", "'do_lower_case'", "in", "model_class_item", "else", "False", "\n", "\n", "tokenizer", "=", "model_class_item", "[", "'tokenizer'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_checkpoint_path", ")", ")", "\n", "\n", "padding_token_value", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", "\n", "padding_segement_value", "=", "model_class_item", "[", "\"padding_segement_value\"", "]", "\n", "padding_att_value", "=", "model_class_item", "[", "\"padding_att_value\"", "]", "\n", "left_pad", "=", "model_class_item", "[", "'left_pad'", "]", "if", "'left_pad'", "in", "model_class_item", "else", "False", "\n", "\n", "batch_size_per_gpu_eval", "=", "args", ".", "per_gpu_eval_batch_size", "\n", "\n", "eval_data_str", "=", "args", ".", "eval_data", "\n", "eval_data_name", "=", "[", "]", "\n", "eval_data_path", "=", "[", "]", "\n", "eval_data_list", "=", "[", "]", "\n", "\n", "eval_data_named_path", "=", "eval_data_str", ".", "split", "(", "','", ")", "\n", "\n", "for", "named_path", "in", "eval_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "name", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "", "eval_data_name", ".", "append", "(", "name", ")", "\n", "eval_data_path", ".", "append", "(", "path", ")", "\n", "\n", "eval_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "batching_schema", "=", "{", "\n", "'uid'", ":", "RawFlintField", "(", ")", ",", "\n", "'y'", ":", "LabelFlintField", "(", ")", ",", "\n", "'input_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_token_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'token_type_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_segement_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'attention_mask'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_att_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "}", "\n", "\n", "data_transformer", "=", "NLITransform", "(", "model_name", ",", "tokenizer", ",", "max_length", ")", "\n", "eval_data_loaders", "=", "[", "]", "\n", "for", "eval_d_list", "in", "eval_data_list", ":", "\n", "        ", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "=", "build_eval_dataset_loader_and_sampler", "(", "eval_d_list", ",", "data_transformer", ",", "\n", "batching_schema", ",", "\n", "batch_size_per_gpu_eval", ")", "\n", "eval_data_loaders", ".", "append", "(", "d_dataloader", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "0", ")", "\n", "model", ".", "cuda", "(", "0", ")", "\n", "\n", "", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "args", ".", "output_prediction_path", "is", "not", "None", ":", "\n", "        ", "cur_results_path", "=", "Path", "(", "args", ".", "output_prediction_path", ")", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "            ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "            ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "            ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.save_tool.ScoreLogger.__init__": [[11, 16], ["object.__init__", "dict", "save_tool.ScoreLogger.score_tracker.update"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "init_tracking_dict", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "logging_item_list", "=", "[", "]", "\n", "self", ".", "score_tracker", "=", "dict", "(", ")", "\n", "self", ".", "score_tracker", ".", "update", "(", "init_tracking_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.save_tool.ScoreLogger.incorporate_results": [[17, 31], ["score_dict.keys", "score_dict.items", "save_tool.ScoreLogger.logging_item_list.append", "len", "len", "score_dict.keys", "save_tool.ScoreLogger.score_tracker.keys"], "methods", ["None"], ["", "def", "incorporate_results", "(", "self", ",", "score_dict", ",", "save_key", ",", "item", "=", "None", ")", "->", "bool", ":", "\n", "        ", "assert", "len", "(", "score_dict", ".", "keys", "(", ")", ")", "==", "len", "(", "self", ".", "score_tracker", ".", "keys", "(", ")", ")", "\n", "for", "fieldname", "in", "score_dict", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "fieldname", "in", "self", ".", "score_tracker", "\n", "\n", "", "valid_improvement", "=", "False", "\n", "for", "fieldname", ",", "value", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "score_dict", "[", "fieldname", "]", ">=", "self", ".", "score_tracker", "[", "fieldname", "]", ":", "\n", "                ", "self", ".", "score_tracker", "[", "fieldname", "]", "=", "score_dict", "[", "fieldname", "]", "\n", "valid_improvement", "=", "True", "\n", "\n", "", "", "self", ".", "logging_item_list", ".", "append", "(", "{", "'k'", ":", "save_key", ",", "'v'", ":", "item", "}", ")", "\n", "\n", "return", "valid_improvement", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.save_tool.ScoreLogger.logging_to_file": [[32, 45], ["pathlib.Path().is_file", "utils.common.save_json", "utils.common.load_json", "set", "pathlib.Path", "set.add", "ValueError"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_json", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_json"], ["", "def", "logging_to_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "if", "Path", "(", "filename", ")", ".", "is_file", "(", ")", ":", "\n", "            ", "old_logging_list", "=", "common", ".", "load_json", "(", "filename", ")", "\n", "current_saved_key", "=", "set", "(", ")", "\n", "\n", "for", "item", "in", "self", ".", "logging_item_list", ":", "\n", "                ", "current_saved_key", ".", "add", "(", "item", "[", "'k'", "]", ")", "\n", "\n", "", "for", "item", "in", "old_logging_list", ":", "\n", "                ", "if", "item", "[", "'k'", "]", "not", "in", "current_saved_key", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Previous logged item can not be found!\"", ")", "\n", "\n", "", "", "", "common", ".", "save_json", "(", "self", ".", "logging_item_list", ",", "filename", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.save_tool.gen_file_prefix": [[47, 53], ["os.path.join", "datetime.datetime.now().strftime", "os.path.exists", "os.makedirs", "datetime.datetime.now"], "function", ["None"], ["", "", "def", "gen_file_prefix", "(", "model_name", ",", "directory_name", "=", "'saved_models'", ",", "date", "=", "None", ")", ":", "\n", "    ", "date_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d-%H:%M:%S\"", ")", "if", "not", "date", "else", "date", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "config", ".", "PRO_ROOT", "/", "directory_name", "/", "'_'", ".", "join", "(", "(", "date_now", ",", "model_name", ")", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "file_path", ")", "\n", "", "return", "file_path", ",", "date_now", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.save_tool.get_cur_time_str": [[55, 58], ["datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["None"], ["", "def", "get_cur_time_str", "(", ")", ":", "\n", "    ", "date_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d[%H:%M:%S]\"", ")", "\n", "return", "date_now", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.JsonableObjectEncoder.default": [[23, 30], ["isinstance", "d.update", "super().default", "vars", "type"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.JsonableObjectEncoder.default"], ["    ", "def", "default", "(", "self", ",", "o", ")", ":", "\n", "        ", "if", "isinstance", "(", "o", ",", "JsonableObj", ")", ":", "\n", "            ", "d", "=", "{", "'_jcls_'", ":", "type", "(", "o", ")", ".", "__name__", "}", "\n", "d", ".", "update", "(", "vars", "(", "o", ")", ")", "\n", "return", "d", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "default", "(", "o", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.register_class": [[12, 16], ["registered_jsonabl_classes.update"], "function", ["None"], ["def", "register_class", "(", "cls", ")", ":", "\n", "    ", "global", "registered_jsonabl_classes", "\n", "if", "cls", "not", "in", "registered_jsonabl_classes", ":", "\n", "        ", "registered_jsonabl_classes", ".", "update", "(", "{", "cls", ".", "__name__", ":", "cls", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.unserialize_JsonableObject": [[32, 43], ["d.pop", "cls.__new__", "d.items", "setattr"], "function", ["None"], ["", "", "", "def", "unserialize_JsonableObject", "(", "d", ")", ":", "\n", "    ", "global", "registered_jsonabl_classes", "\n", "classname", "=", "d", ".", "pop", "(", "'_jcls_'", ",", "None", ")", "\n", "if", "classname", ":", "\n", "        ", "cls", "=", "registered_jsonabl_classes", "[", "classname", "]", "\n", "obj", "=", "cls", ".", "__new__", "(", "cls", ")", "# Make instance without calling __init__", "\n", "for", "key", ",", "value", "in", "d", ".", "items", "(", ")", ":", "\n", "            ", "setattr", "(", "obj", ",", "key", ",", "value", ")", "\n", "", "return", "obj", "\n", "", "else", ":", "\n", "        ", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.json_dumps": [[45, 47], ["json.dumps"], "function", ["None"], ["", "", "def", "json_dumps", "(", "item", ")", ":", "\n", "    ", "return", "json", ".", "dumps", "(", "item", ",", "cls", "=", "JsonableObjectEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.json_loads": [[49, 51], ["json.loads"], "function", ["None"], ["", "def", "json_loads", "(", "item_str", ")", ":", "\n", "    ", "return", "json", ".", "loads", "(", "item_str", ",", "object_hook", "=", "unserialize_JsonableObject", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_jsonl": [[55, 60], ["print", "open", "out_f.write", "json.dumps"], "function", ["None"], ["", "def", "save_jsonl", "(", "d_list", ",", "filename", ")", ":", "\n", "    ", "print", "(", "\"Save to Jsonl:\"", ",", "filename", ")", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "for", "item", "in", "d_list", ":", "\n", "            ", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ",", "cls", "=", "JsonableObjectEncoder", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_jsonl": [[62, 73], ["open", "print", "tqdm.tqdm", "json.loads", "d_list.append", "line.strip", "len"], "function", ["None"], ["", "", "", "def", "load_jsonl", "(", "filename", ",", "debug_num", "=", "None", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "print", "(", "\"Load Jsonl:\"", ",", "filename", ")", "\n", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ",", "object_hook", "=", "unserialize_JsonableObject", ")", "\n", "d_list", ".", "append", "(", "item", ")", "\n", "if", "debug_num", "is", "not", "None", "and", "0", "<", "debug_num", "==", "len", "(", "d_list", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.load_json": [[75, 78], ["open", "json.load"], "function", ["None"], ["", "def", "load_json", "(", "filename", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "return", "json", ".", "load", "(", "in_f", ",", "object_hook", "=", "unserialize_JsonableObject", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.common.save_json": [[80, 84], ["open", "json.dump", "out_f.close"], "function", ["None"], ["", "", "def", "save_json", "(", "obj", ",", "filename", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "obj", ",", "out_f", ",", "cls", "=", "JsonableObjectEncoder", ",", "**", "kwargs", ")", "\n", "out_f", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.list_to_dict": [[4, 10], ["dict"], "function", ["None"], ["def", "list_to_dict", "(", "d_list", ",", "key_fields", ")", ":", "#   '_id' or 'pid'", "\n", "    ", "d_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "assert", "key_fields", "in", "item", "\n", "d_dict", "[", "item", "[", "key_fields", "]", "]", "=", "item", "\n", "", "return", "d_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.dict_to_list": [[12, 17], ["d_dict.items", "d_list.append"], "function", ["None"], ["", "def", "dict_to_list", "(", "d_dict", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "for", "key", ",", "value", "in", "d_dict", ".", "items", "(", ")", ":", "\n", "        ", "d_list", ".", "append", "(", "value", ")", "\n", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.append_item_from_dict_to_list": [[19, 32], ["isinstance", "print"], "function", ["None"], ["", "def", "append_item_from_dict_to_list", "(", "d_list", ",", "d_dict", ",", "key_fieldname", ",", "append_fieldnames", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "append_fieldnames", ",", "list", ")", ":", "\n", "        ", "append_fieldnames", "=", "[", "append_fieldnames", "]", "\n", "", "for", "item", "in", "d_list", ":", "\n", "        ", "key", "=", "item", "[", "key_fieldname", "]", "\n", "if", "key", "in", "d_dict", ":", "\n", "            ", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "d_dict", "[", "key", "]", "[", "append_fieldname", "]", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "f\"Potential Error: {key} not in scored_dict. Maybe bc all forward items are empty.\"", ")", "\n", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "[", "]", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style": [[34, 47], ["isinstance", "print"], "function", ["None"], ["", "def", "append_item_from_dict_to_list_hotpot_style", "(", "d_list", ",", "d_dict", ",", "key_fieldname", ",", "append_fieldnames", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "append_fieldnames", ",", "list", ")", ":", "\n", "        ", "append_fieldnames", "=", "[", "append_fieldnames", "]", "\n", "", "for", "item", "in", "d_list", ":", "\n", "        ", "key", "=", "item", "[", "key_fieldname", "]", "\n", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "            ", "if", "key", "in", "d_dict", "[", "append_fieldname", "]", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "d_dict", "[", "append_fieldname", "]", "[", "key", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "f\"Potential Error: {key} not in scored_dict. Maybe bc all forward items are empty.\"", ")", "\n", "# for append_fieldname in append_fieldnames:", "\n", "item", "[", "append_fieldname", "]", "=", "[", "]", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.utils.list_dict_data_tool.append_subfield_from_list_to_dict": [[49, 86], ["d_dict.keys", "dict", "print"], "function", ["None"], ["", "def", "append_subfield_from_list_to_dict", "(", "subf_list", ",", "d_dict", ",", "o_key_field_name", ",", "subfield_key_name", ",", "\n", "subfield_name", "=", "'merged_field'", ",", "check", "=", "False", ")", ":", "\n", "# Often times, we will need to split the one data point to multiple items to be feeded into neural networks", "\n", "# and after we obtain the results we will need to map the results back to original data point with some keys.", "\n", "\n", "# This method is used for this purpose.", "\n", "# The method can be invoke multiple times, (in practice usually one batch per time.)", "\n", "    ", "\"\"\"\n    :param subf_list:               The forward list.\n    :param d_dict:                  The dict that contain keys mapping to original data point.\n    :param o_key_field_name:        The fieldname of original data point key. 'pid'\n    :param subfield_key_name:       The fieldname of the sub item. 'fid'\n    :param subfield_name:           The merge field name.       'merged_field'\n    :param check:\n    :return:\n    \"\"\"", "\n", "for", "key", "in", "d_dict", ".", "keys", "(", ")", ":", "\n", "        ", "d_dict", "[", "key", "]", "[", "subfield_name", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "item", "in", "subf_list", ":", "\n", "        ", "assert", "o_key_field_name", "in", "item", "\n", "assert", "subfield_key_name", "in", "item", "\n", "map_id", "=", "item", "[", "o_key_field_name", "]", "\n", "sub_filed_id", "=", "item", "[", "subfield_key_name", "]", "\n", "assert", "map_id", "in", "d_dict", "\n", "\n", "# if subfield_name not in d_dict[map_id]:", "\n", "#     d_dict[map_id][subfield_name] = dict()", "\n", "\n", "if", "sub_filed_id", "not", "in", "d_dict", "[", "map_id", "]", "[", "subfield_name", "]", ":", "\n", "            ", "if", "check", ":", "\n", "                ", "assert", "item", "[", "o_key_field_name", "]", "==", "map_id", "\n", "", "d_dict", "[", "map_id", "]", "[", "subfield_name", "]", "[", "sub_filed_id", "]", "=", "item", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Duplicate forward item with key:\"", ",", "sub_filed_id", ")", "\n", "\n", "", "", "return", "d_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.BaseBatchBuilder.__init__": [[8, 11], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batching_schema", ":", "Dict", "[", "str", ",", "FlintField", "]", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "batching_schema", ":", "Dict", "[", "str", ",", "FlintField", "]", "=", "batching_schema", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.BaseBatchBuilder.__call__": [[12, 25], ["batch[].keys", "dict", "flint.data_utils.fields.RawFlintField.batching", "batchbuilder.BaseBatchBuilder.batching_schema[].batching"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.batching", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.batching"], ["", "def", "__call__", "(", "self", ",", "batch", ")", ":", "\n", "        ", "field_names", "=", "batch", "[", "0", "]", ".", "keys", "(", ")", "\n", "batched_data", "=", "dict", "(", ")", "\n", "\n", "for", "field_name", "in", "field_names", ":", "\n", "            ", "if", "field_name", "not", "in", "self", ".", "batching_schema", ":", "\n", "# default is RawFlintField", "\n", "                ", "batched_data", "[", "field_name", "]", "=", "RawFlintField", ".", "batching", "(", "[", "item", "[", "field_name", "]", "for", "item", "in", "batch", "]", ")", "\n", "\n", "", "else", ":", "\n", "                ", "batched_data", "[", "field_name", "]", "=", "self", ".", "batching_schema", "[", "field_name", "]", ".", "batching", "(", "[", "item", "[", "field_name", "]", "for", "item", "in", "batch", "]", ")", "\n", "\n", "", "", "return", "batched_data", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.has_tensor": [[27, 40], ["isinstance", "isinstance", "any", "isinstance", "any", "batchbuilder.has_tensor", "obj.values", "batchbuilder.has_tensor"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.has_tensor", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.has_tensor"], ["", "", "def", "has_tensor", "(", "obj", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Given a possibly complex data structure,\n    check if it has any torch.Tensors in it.\n    \"\"\"", "\n", "if", "isinstance", "(", "obj", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "any", "(", "has_tensor", "(", "value", ")", "for", "value", "in", "obj", ".", "values", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "return", "any", "(", "has_tensor", "(", "item", ")", "for", "item", "in", "obj", ")", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device": [[42, 63], ["isinstance", "batchbuilder.has_tensor", "obj.cuda", "isinstance", "isinstance", "batchbuilder.move_to_device", "obj.items", "batchbuilder.move_to_device", "isinstance", "hasattr", "obj.__class__", "isinstance", "tuple", "batchbuilder.move_to_device", "batchbuilder.move_to_device"], "function", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.has_tensor", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.batchbuilder.move_to_device"], ["", "", "def", "move_to_device", "(", "obj", ",", "cuda_device", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Given a structure (possibly) containing Tensors on the CPU,\n    move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).\n    \"\"\"", "\n", "\n", "if", "cuda_device", "<", "0", "or", "not", "has_tensor", "(", "obj", ")", ":", "\n", "        ", "return", "obj", "\n", "", "elif", "isinstance", "(", "obj", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "obj", ".", "cuda", "(", "cuda_device", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "{", "key", ":", "move_to_device", "(", "value", ",", "cuda_device", ")", "for", "key", ",", "value", "in", "obj", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "obj", ",", "list", ")", ":", "\n", "        ", "return", "[", "move_to_device", "(", "item", ",", "cuda_device", ")", "for", "item", "in", "obj", "]", "\n", "", "elif", "isinstance", "(", "obj", ",", "tuple", ")", "and", "hasattr", "(", "obj", ",", "\"_fields\"", ")", ":", "\n", "# This is the best way to detect a NamedTuple, it turns out.", "\n", "        ", "return", "obj", ".", "__class__", "(", "*", "(", "move_to_device", "(", "item", ",", "cuda_device", ")", "for", "item", "in", "obj", ")", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "tuple", ")", ":", "\n", "        ", "return", "tuple", "(", "move_to_device", "(", "item", ",", "cuda_device", ")", "for", "item", "in", "obj", ")", "\n", "", "else", ":", "\n", "        ", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.FlintField.batching": [[5, 8], ["NotImplemented"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "batching", "(", "cls", ",", "batched_data", ")", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.RawFlintField.batching": [[11, 14], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "batching", "(", "cls", ",", "batched_data", ")", ":", "\n", "        ", "return", "batched_data", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.LabelFlintField.batching": [[17, 19], ["torch.tensor"], "methods", ["None"], ["    ", "def", "batching", "(", "self", ",", "batched_data", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "batched_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.__init__": [[22, 28], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pad_idx", ",", "eos_idx", "=", "None", ",", "left_pad", "=", "False", ",", "move_eos_to_beginning", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "eos_idx", "=", "eos_idx", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "move_eos_to_beginning", "=", "move_eos_to_beginning", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.collate_tokens": [[29, 51], ["max", "values[].new().fill_", "enumerate", "torch.is_tensor", "fields.ArrayIndexFlintField.collate_tokens.copy_tensor"], "methods", ["None"], ["", "def", "collate_tokens", "(", "self", ",", "values", ",", "pad_idx", ",", "eos_idx", "=", "None", ",", "left_pad", "=", "False", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Convert a list of 1d tensors into a padded 2d tensor.\n        \"\"\"", "\n", "if", "not", "torch", ".", "is_tensor", "(", "values", "[", "0", "]", ")", ":", "\n", "            ", "values", "=", "[", "torch", ".", "tensor", "(", "v", ")", "for", "v", "in", "values", "]", "\n", "\n", "", "size", "=", "max", "(", "v", ".", "size", "(", "0", ")", "for", "v", "in", "values", ")", "\n", "res", "=", "values", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "size", ")", ".", "fill_", "(", "pad_idx", ")", "\n", "\n", "def", "copy_tensor", "(", "src", ",", "dst", ")", ":", "\n", "            ", "assert", "dst", ".", "numel", "(", ")", "==", "src", ".", "numel", "(", ")", "\n", "if", "move_eos_to_beginning", ":", "\n", "                ", "assert", "src", "[", "-", "1", "]", "==", "eos_idx", "\n", "dst", "[", "0", "]", "=", "eos_idx", "\n", "dst", "[", "1", ":", "]", "=", "src", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "dst", ".", "copy_", "(", "src", ")", "\n", "\n", "", "", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "            ", "copy_tensor", "(", "v", ",", "res", "[", "i", "]", "[", "size", "-", "len", "(", "v", ")", ":", "]", "if", "left_pad", "else", "res", "[", "i", "]", "[", ":", "len", "(", "v", ")", "]", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.batching": [[52, 58], ["fields.ArrayIndexFlintField.collate_tokens"], "methods", ["home.repos.pwc.inspect_result.csitfun_ConTRoL-dataset.data_utils.fields.ArrayIndexFlintField.collate_tokens"], ["", "def", "batching", "(", "self", ",", "batched_data", ")", ":", "\n", "        ", "return", "self", ".", "collate_tokens", "(", "batched_data", ",", "\n", "self", ".", "pad_idx", ",", "\n", "self", ".", "eos_idx", ",", "\n", "self", ".", "left_pad", ",", "\n", "self", ".", "move_eos_to_beginning", ")", "\n", "", "", ""]]}