{"home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.add_model_arguments": [[30, 49], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["def", "add_model_arguments", "(", "parser", ")", ":", "\n", "###decoder", "\n", "#parser.add_argument('--de_model', type=str, default='LSTM',", "\n", "    ", "parser", ".", "add_argument", "(", "'--de_model'", ",", "type", "=", "str", ",", "default", "=", "'TRANS'", ",", "\n", "help", "=", "'type of decoder model (LSTM, LSTM+TRANS, TRANS+LSTM, TRANS)'", ")", "\n", "parser", ".", "add_argument", "(", "'--trans_layers'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'How many layers we have in transformer. Do not have effect if de_model is LSTM'", ")", "\n", "parser", ".", "add_argument", "(", "'--de_en_connection'", ",", "type", "=", "str2bool", ",", "nargs", "=", "'?'", ",", "default", "=", "False", ",", "\n", "help", "=", "'If True, using Transformer decoder in our decoder. Otherwise, using Transformer encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--nhidlast2'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "\n", "help", "=", "'hidden embedding size of the second LSTM'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_basis'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'number of basis we want to predict'", ")", "\n", "parser", ".", "add_argument", "(", "'--positional_option'", ",", "type", "=", "str", ",", "default", "=", "'linear'", ",", "\n", "help", "=", "'options of encode positional embedding into models (linear, cat, add)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropoutp'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "'dropout of positional embedding or input embedding after linear transformation (when linear_mapping_dim != 0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout_prob_trans'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "'hidden_dropout_prob and attention_probs_dropout_prob in Transformer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.compute_freq_prob_idx2word": [[50, 55], ["list", "float", "enumerate", "zip", "sum", "idx2word_freq[].append"], "function", ["None"], ["", "def", "compute_freq_prob_idx2word", "(", "idx2word_freq", ")", ":", "\n", "    ", "all_word", ",", "all_freq", "=", "list", "(", "zip", "(", "*", "idx2word_freq", ")", ")", "\n", "freq_sum", "=", "float", "(", "sum", "(", "all_freq", ")", ")", "\n", "for", "i", ",", "(", "w", ",", "freq", ")", "in", "enumerate", "(", "idx2word_freq", ")", ":", "\n", "        ", "idx2word_freq", "[", "i", "]", ".", "append", "(", "freq", "/", "freq_sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.compute_freq_prob": [[56, 62], ["list", "float", "zip", "sum", "word_d2_idx_freq[].append", "word_d2_idx_freq.values"], "function", ["None"], ["", "", "def", "compute_freq_prob", "(", "word_d2_idx_freq", ")", ":", "\n", "    ", "all_idx", ",", "all_freq", "=", "list", "(", "zip", "(", "*", "word_d2_idx_freq", ".", "values", "(", ")", ")", ")", "\n", "freq_sum", "=", "float", "(", "sum", "(", "all_freq", ")", ")", "\n", "for", "w", "in", "word_d2_idx_freq", ":", "\n", "        ", "idx", ",", "freq", "=", "word_d2_idx_freq", "[", "w", "]", "\n", "word_d2_idx_freq", "[", "w", "]", ".", "append", "(", "freq", "/", "freq_sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_gpt2_vocab": [[63, 71], ["json.load", "max", "json.load.values", "range"], "function", ["None"], ["", "", "def", "load_gpt2_vocab", "(", "f_in", ")", ":", "\n", "    ", "w_d2_idx", "=", "json", ".", "load", "(", "f_in", ")", "\n", "max_w_idx", "=", "max", "(", "w_d2_idx", ".", "values", "(", ")", ")", "\n", "idx_l2_w_gpt2", "=", "[", "''", "for", "i", "in", "range", "(", "max_w_idx", "+", "1", ")", "]", "\n", "for", "w", "in", "w_d2_idx", ":", "\n", "        ", "idx", "=", "w_d2_idx", "[", "w", "]", "\n", "idx_l2_w_gpt2", "[", "idx", "]", "=", "w", "\n", "", "return", "idx_l2_w_gpt2", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.predict_batch_simple": [[72, 91], ["parallel_encoder", "output_emb.size", "feature.size", "inner_idx_tensor.size", "output_emb.gather", "output_emb.gather.view", "output_emb.gather.size", "output_emb.unsqueeze().expand", "output_emb_masked.reshape.reshape", "future_mask.view.view", "parallel_decoder", "parallel_decoder", "inner_idx_tensor.unsqueeze().expand", "parallel_decoder.norm", "output_emb.unsqueeze", "inner_idx_tensor.unsqueeze"], "function", ["None"], ["", "def", "predict_batch_simple", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "de_en_connection", ")", ":", "\n", "    ", "output_emb", ",", "past", "=", "parallel_encoder", "(", "feature", ")", "\n", "hidden_size", "=", "output_emb", ".", "size", "(", "2", ")", "\n", "#batch_size, num_head, seq_len = future_mask.size()", "\n", "batch_size", ",", "seq_len", "=", "feature", ".", "size", "(", ")", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "output_emb_head", "=", "output_emb", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "inner_idx_tensor", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "num_head", ",", "hidden_size", ")", ")", "\n", "output_emb_last", "=", "output_emb_head", ".", "view", "(", "-", "1", ",", "output_emb_head", ".", "size", "(", "2", ")", ")", "\n", "if", "de_en_connection", ":", "\n", "        ", "output_emb_masked", "=", "output_emb", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "batch_size", ",", "num_head", ",", "seq_len", ",", "hidden_size", ")", "\n", "output_emb_masked", "=", "output_emb_masked", ".", "reshape", "(", "-", "1", ",", "seq_len", ",", "hidden_size", ")", "\n", "future_mask", "=", "future_mask", ".", "view", "(", "-", "1", ",", "seq_len", ")", "\n", "\n", "basis_pred", "=", "parallel_decoder", "(", "output_emb_last", ",", "output_emb_masked", ",", "memory_attention_mask", "=", "future_mask", ")", "\n", "", "else", ":", "\n", "        ", "basis_pred", "=", "parallel_decoder", "(", "output_emb_last", ")", "\n", "\n", "", "basis_norm_pred", "=", "basis_pred", "/", "(", "0.000000000001", "+", "basis_pred", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ")", "\n", "return", "basis_norm_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.predict_batch": [[92, 98], ["utils_testing.predict_batch_simple", "basis_norm_pred.permute.permute", "torch.matmul", "torch.matmul", "torch.matmul", "torch.topk", "torch.topk", "torch.topk", "word_norm_emb.unsqueeze"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.predict_batch_simple"], ["", "def", "predict_batch", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ")", ":", "\n", "    ", "basis_norm_pred", "=", "predict_batch_simple", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "de_en_connection", ")", "\n", "basis_norm_pred", "=", "basis_norm_pred", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "sim_pairwise", "=", "torch", ".", "matmul", "(", "word_norm_emb", ".", "unsqueeze", "(", "dim", "=", "0", ")", ",", "basis_norm_pred", ")", "\n", "top_value", ",", "top_index", "=", "torch", ".", "topk", "(", "sim_pairwise", ",", "top_k", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "return", "basis_norm_pred", ",", "top_value", ",", "top_index", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.convert_feature_to_text": [[99, 109], ["feature.tolist", "range", "feature.size", "feature_text.append", "current_sent.append"], "function", ["None"], ["", "def", "convert_feature_to_text", "(", "feature", ",", "idx_l2_w_gpt2", ")", ":", "\n", "    ", "feature_list", "=", "feature", ".", "tolist", "(", ")", "\n", "feature_text", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "feature", ".", "size", "(", "0", ")", ")", ":", "\n", "        ", "current_sent", "=", "[", "]", "\n", "for", "w_ind", "in", "feature_list", "[", "i", "]", ":", "\n", "            ", "w", "=", "idx_l2_w_gpt2", "[", "w_ind", "]", "\n", "current_sent", ".", "append", "(", "w", ")", "\n", "", "feature_text", ".", "append", "(", "current_sent", ")", "\n", "", "return", "feature_text", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_basis_text": [[112, 150], ["top_index_arr[].size", "inner_idx_tensor.size", "range", "range", "tokenizer_GPT2._convert_id_to_token", "range", "inner_idx_tensor[].item", "tokenizer_GPT2.convert_tokens_to_string", "outf.write", "enumerate", "feature[].tolist", "feature.size", "utils_testing.preprocessing_context", "outf.write", "range", "outf.write", "range", "outf.write", "outf.write", "top_index[].item", "top_value[].item"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context"], ["", "def", "print_basis_text", "(", "feature", ",", "idx2word_freq", ",", "top_value_arr", ",", "top_index_arr", ",", "method_name_arr", ",", "i_batch", ",", "outf", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ",", "readable_context", "=", "False", ")", ":", "\n", "#n_basis = coeff_order.shape[1]", "\n", "    ", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", "=", "top_index_arr", "[", "0", "]", ".", "size", "(", ")", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "#feature_text = convert_feature_to_text(feature, idx_l2_w_gpt2)", "\n", "feature_text", "=", "[", "[", "tokenizer_GPT2", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "feature", "[", "i", ",", ":", "]", ".", "tolist", "(", ")", "]", "for", "i", "in", "range", "(", "feature", ".", "size", "(", "0", ")", ")", "]", "\n", "#print(feature_text)", "\n", "#for i_sent in range(len(feature_text)):", "\n", "for", "i_sent", "in", "range", "(", "batch_size", ")", ":", "\n", "#outf.write('{} batch, {}th sent: '.format(i_batch, i_sent)+' '.join(feature_text[i_sent])+'\\n')", "\n", "        ", "last_end", "=", "-", "1", "\n", "for", "m", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "end", "=", "inner_idx_tensor", "[", "i_sent", ",", "m", "]", ".", "item", "(", ")", "\n", "if", "end", "==", "last_end", ":", "\n", "                ", "continue", "\n", "", "last_end", "=", "end", "\n", "#outf.write(''.join(feature_text[i_sent][:end]).replace('\u0120',' ')+'\\n')", "\n", "\n", "context", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "feature_text", "[", "i_sent", "]", "[", ":", "end", "]", ")", "\n", "if", "readable_context", ":", "\n", "                ", "context", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ")", "\n", "if", "bad_context", ":", "\n", "                    ", "continue", "\n", "", "", "outf", ".", "write", "(", "context", "+", "'\\n\\n'", ")", "\n", "\n", "for", "q", ",", "method_name", "in", "enumerate", "(", "method_name_arr", ")", ":", "\n", "                ", "top_index", "=", "top_index_arr", "[", "q", "]", "#.view(batch_size, num_head, top_k, n_basis)", "\n", "top_value", "=", "top_value_arr", "[", "q", "]", "#.view(batch_size, num_head, top_k, n_basis)", "\n", "outf", ".", "write", "(", "method_name", "+", "': \\n'", ")", "\n", "for", "j", "in", "range", "(", "n_basis", ")", ":", "\n", "#org_ind = coeff_order[i_sent, j]", "\n", "#outf.write(str(j)+', org '+str(org_ind)+', '+str( coeff_sum[i_sent,org_ind,0] )+' - '+str( coeff_sum[i_sent,org_ind,1] )+': ')", "\n", "\n", "                    ", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "                        ", "word_nn", "=", "idx2word_freq", "[", "top_index", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "outf", ".", "write", "(", "word_nn", "+", "' {:5.3f}'", ".", "format", "(", "top_value", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", ")", "+", "' '", ")", "\n", "", "outf", ".", "write", "(", "'\\n'", ")", "\n", "", "outf", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.visualize_topics_val": [[152, 165], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "utils_testing.predict_batch", "inner_idx_tensor.size", "utils_testing.print_basis_text", "top_value.view", "top_index.view"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.predict_batch", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_basis_text"], ["", "", "", "", "def", "visualize_topics_val", "(", "dataloader", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "idx2word_freq", ",", "outf", ",", "n_basis", ",", "max_batch_num", ",", "de_en_connection", ",", "tokenizer_GPT2", ")", ":", "\n", "#topics_num = 0", "\n", "    ", "top_k", "=", "5", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "            ", "if", "i_batch", ">=", "max_batch_num", ":", "\n", "                ", "break", "\n", "", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "=", "sample_batched", "\n", "\n", "basis_norm_pred", ",", "top_value", ",", "top_index", "=", "predict_batch", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ")", "\n", "#print_basis_text(feature, idx2word_freq, top_value, top_index, i_batch, outf, idx_l2_w_gpt2, inner_idx_tensor)", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "print_basis_text", "(", "feature", ",", "idx2word_freq", ",", "[", "top_value", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "]", ",", "[", "top_index", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "]", ",", "[", "'NSD'", "]", ",", "i_batch", ",", "outf", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_sampled_sent": [[168, 236], ["len", "top_index_im.size", "range", "outf.write", "range", "range", "outf.write", "find_all"], "function", ["None"], ["", "", "", "def", "print_sampled_sent", "(", "selected_topic_idx", ",", "generated_sent", ",", "top_index_im", ",", "idx2word_freq", ",", "outf", ",", "print_prefix", ",", "selected_word_idx", "=", "None", ")", ":", "\n", "    ", "def", "insert_substring", "(", "sent", ",", "insert_loc", ",", "insert_substring", ")", ":", "\n", "        ", "return", "sent", "[", ":", "insert_loc", "]", "+", "insert_substring", "+", "sent", "[", "insert_loc", ":", "]", "\n", "\n", "", "def", "highlight_words", "(", "word_nn", ",", "generated_sent", ",", "topic_l2_word_d2_count_t", ")", ":", "\n", "        ", "def", "find_all", "(", "a_str", ",", "sub", ")", ":", "\n", "            ", "start", "=", "0", "\n", "while", "True", ":", "\n", "                ", "start", "=", "a_str", ".", "find", "(", "sub", ",", "start", ")", "\n", "if", "start", "==", "-", "1", ":", "return", "\n", "yield", "start", "\n", "start", "+=", "len", "(", "sub", ")", "# use start += 1 to find overlapping matches", "\n", "", "", "index_shift", "=", "0", "\n", "\n", "#for m in re.finditer(word_nn, generated_sent):", "\n", "#    start = m.start() + index_shift", "\n", "#    end = m.end() + index_shift", "\n", "for", "m_start", "in", "find_all", "(", "generated_sent", ",", "word_nn", ")", ":", "\n", "            ", "start", "=", "m_start", "+", "index_shift", "\n", "end", "=", "m_start", "+", "len", "(", "word_nn", ")", "+", "index_shift", "\n", "#print(generated_sent)", "\n", "#print(word_nn)", "\n", "#print(start, end)", "\n", "#if start != 0 and generated_sent[start-1] != ' ' and end >= len(generated_sent) - 1 and generated_sent[end+1] != ' ':", "\n", "if", "end", "<", "len", "(", "generated_sent", ")", "-", "1", "and", "generated_sent", "[", "end", "+", "1", "]", "!=", "' '", "and", "start", "!=", "0", "and", "generated_sent", "[", "start", "-", "1", "]", "!=", "' '", ":", "\n", "                ", "continue", "\n", "", "if", "word_nn", "not", "in", "topic_l2_word_d2_count_t", ":", "\n", "                ", "topic_l2_word_d2_count_t", "[", "word_nn", "]", "=", "0", "\n", "", "topic_l2_word_d2_count_t", "[", "word_nn", "]", "+=", "1", "\n", "prev_start", "=", "generated_sent", "[", ":", "start", "]", ".", "rfind", "(", "colorama", ".", "Fore", ".", "RED", ")", "\n", "prev_end", "=", "generated_sent", "[", ":", "start", "]", ".", "rfind", "(", "colorama", ".", "Style", ".", "RESET_ALL", ")", "\n", "if", "prev_start", ">", "prev_end", ":", "\n", "                ", "continue", "\n", "", "generated_sent", "=", "insert_substring", "(", "generated_sent", ",", "end", ",", "colorama", ".", "Style", ".", "RESET_ALL", ")", "\n", "generated_sent", "=", "insert_substring", "(", "generated_sent", ",", "start", ",", "colorama", ".", "Fore", ".", "RED", ")", "\n", "index_shift", "+=", "len", "(", "colorama", ".", "Style", ".", "RESET_ALL", ")", "+", "len", "(", "colorama", ".", "Fore", ".", "RED", ")", "\n", "", "return", "generated_sent", "\n", "\n", "", "num_selected", "=", "len", "(", "selected_topic_idx", ")", "\n", "top_k", "=", "top_index_im", ".", "size", "(", "0", ")", "\n", "topic_l2_word_d2_count", "=", "[", "{", "}", "for", "t", "in", "range", "(", "num_selected", ")", "]", "\n", "for", "t", "in", "range", "(", "num_selected", ")", ":", "\n", "        ", "topic_idx", "=", "selected_topic_idx", "[", "t", "]", "\n", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "#word_nn = idx2word_freq[top_index[i_sent,m,k,topic_idx].item()][0]", "\n", "#print(top_index_im.size())", "\n", "#print(topic_idx)", "\n", "\n", "            ", "word_nn", "=", "idx2word_freq", "[", "top_index_im", "[", "k", ",", "topic_idx", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "generated_sent", "=", "highlight_words", "(", "word_nn", ",", "generated_sent", ",", "topic_l2_word_d2_count", "[", "t", "]", ")", "\n", "", "", "num_word", "=", "0", "\n", "if", "selected_word_idx", "is", "not", "None", ":", "\n", "        ", "num_word", "=", "len", "(", "selected_word_idx", ")", "\n", "word_l2_word_d2_count", "=", "[", "{", "}", "for", "t", "in", "range", "(", "num_word", ")", "]", "\n", "for", "t", "in", "range", "(", "num_word", ")", ":", "\n", "            ", "word_nn", "=", "idx2word_freq", "[", "selected_word_idx", "[", "t", "]", "]", "[", "0", "]", "\n", "generated_sent", "=", "highlight_words", "(", "word_nn", ",", "generated_sent", ",", "word_l2_word_d2_count", "[", "t", "]", ")", "\n", "", "", "outf", ".", "write", "(", "print_prefix", "+", "': '", "+", "generated_sent", "+", "'\\n'", ")", "\n", "for", "t", "in", "range", "(", "num_selected", ")", ":", "\n", "        ", "if", "len", "(", "topic_l2_word_d2_count", "[", "t", "]", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "topic_idx", "=", "selected_topic_idx", "[", "t", "]", "\n", "outf", ".", "write", "(", "str", "(", "topic_idx", ")", "+", "' topic: '", "+", "str", "(", "topic_l2_word_d2_count", "[", "t", "]", ")", "+", "'\\n'", ")", "\n", "", "for", "t", "in", "range", "(", "num_word", ")", ":", "\n", "        ", "if", "len", "(", "word_l2_word_d2_count", "[", "t", "]", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "outf", ".", "write", "(", "'word: '", "+", "str", "(", "word_l2_word_d2_count", "[", "t", "]", ")", "+", "'\\n'", ")", "\n", "", "outf", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.saparateParagraph": [[237, 248], ["paragraph.rindex", "paragraph.find"], "function", ["None"], ["", "def", "saparateParagraph", "(", "paragraph", ")", ":", "\n", "    ", "i", "=", "0", "\n", "res", "=", "''", "\n", "while", "i", "<", "30", ":", "\n", "        ", "if", "paragraph", ".", "find", "(", "' '", ")", "<", "0", ":", "\n", "            ", "return", "''", ",", "paragraph", "+", "res", "\n", "", "last", "=", "paragraph", ".", "rindex", "(", "' '", ")", "\n", "res", "=", "paragraph", "[", "last", ":", "]", "+", "res", "\n", "paragraph", "=", "paragraph", "[", ":", "last", "]", "\n", "i", "+=", "1", "\n", "", "return", "paragraph", ",", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString": [[249, 251], ["None"], "function", ["None"], ["", "def", "generateString", "(", "topic", ")", ":", "\n", "    ", "return", "topic", "[", "\"name\"", "]", "+", "\": \"", "+", "', '", ".", "join", "(", "[", "each", "for", "each", "in", "topic", "[", "\"keywords\"", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.exclude_condition": [[252, 267], ["text.encode", "text.count", "len"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "exclude_condition", "(", "text", ",", "num_not_ascii", ",", "method_idx", ")", ":", "\n", "    ", "exclude_this", "=", "False", "\n", "try", ":", "\n", "        ", "text", ".", "encode", "(", "'ascii'", ",", "'strict'", ")", "\n", "", "except", ":", "\n", "        ", "num_not_ascii", "[", "method_idx", "]", "+=", "1", "\n", "exclude_this", "=", "True", "\n", "", "if", "not", "exclude_this", "and", "text", ".", "count", "(", "\" \"", ")", ">", "len", "(", "text", ")", "*", "0.3", ":", "\n", "        ", "num_not_ascii", "[", "method_idx", "]", "+=", "1", "\n", "exclude_this", "=", "True", "\n", "", "if", "not", "exclude_this", "and", "'<|endoftext|>'", "in", "text", ":", "\n", "        ", "num_not_ascii", "[", "method_idx", "]", "+=", "1", "\n", "exclude_this", "=", "True", "\n", "\n", "", "return", "exclude_this", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.filter_generated_sents": [[269, 290], ["set", "range", "len", "cond_sent_list_out.append", "org_sent_list_out.append", "pplm_sent_list_out.append", "utils_testing.exclude_condition", "utils_testing.exclude_condition", "utils_testing.exclude_condition", "cond_sent_list[].replace().replace().replace", "org_sent_list[].replace().replace().replace", "pplm_sent_list[].replace().replace().replace", "print", "set.add", "cond_sent_list[].replace().replace", "org_sent_list[].replace().replace", "pplm_sent_list[].replace().replace", "cond_sent_list[].replace", "org_sent_list[].replace", "pplm_sent_list[].replace"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.exclude_condition", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.exclude_condition", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.exclude_condition", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add"], ["", "def", "filter_generated_sents", "(", "cond_sent_list", ",", "org_sent_list", ",", "pplm_sent_list", ")", ":", "\n", "    ", "cond_sent_list_out", "=", "[", "]", "\n", "org_sent_list_out", "=", "[", "]", "\n", "pplm_sent_list_out", "=", "[", "]", "\n", "excluding_j_set", "=", "set", "(", ")", "\n", "num_not_ascii", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "cond_sent_list", ")", ")", ":", "\n", "        ", "cond_sent_list_out", ".", "append", "(", "cond_sent_list", "[", "j", "]", ".", "replace", "(", "'\u00e2\u0080\u0099'", ",", "\"'\"", ")", ".", "replace", "(", "'\u00e2\u0080\u0093'", ",", "'-'", ")", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", ")", "\n", "org_sent_list_out", ".", "append", "(", "org_sent_list", "[", "j", "]", ".", "replace", "(", "'\u00e2\u0080\u0099'", ",", "\"'\"", ")", ".", "replace", "(", "'\u00e2\u0080\u0093'", ",", "'-'", ")", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", ")", "\n", "pplm_sent_list_out", ".", "append", "(", "pplm_sent_list", "[", "j", "]", ".", "replace", "(", "'\u00e2\u0080\u0099'", ",", "\"'\"", ")", ".", "replace", "(", "'\u00e2\u0080\u0093'", ",", "'-'", ")", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", ")", "\n", "exclude_this_0", "=", "exclude_condition", "(", "cond_sent_list_out", "[", "j", "]", ",", "num_not_ascii", ",", "0", ")", "\n", "exclude_this_1", "=", "exclude_condition", "(", "org_sent_list_out", "[", "j", "]", ",", "num_not_ascii", ",", "1", ")", "\n", "exclude_this_2", "=", "exclude_condition", "(", "pplm_sent_list_out", "[", "j", "]", ",", "num_not_ascii", ",", "2", ")", "\n", "exclude_this", "=", "exclude_this_0", "or", "exclude_this_1", "or", "exclude_this_2", "\n", "\n", "if", "exclude_this", ":", "\n", "            ", "print", "(", "\"Skip generated sentences due to special token\"", ")", "\n", "excluding_j_set", ".", "add", "(", "j", ")", "\n", "#continue", "\n", "\n", "", "", "return", "cond_sent_list_out", ",", "org_sent_list_out", ",", "pplm_sent_list_out", ",", "excluding_j_set", ",", "num_not_ascii", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context": [[291, 311], ["context.replace().replace().replace.replace().replace().replace", "len", "len", "context.replace().replace().replace.encode", "context.replace().replace().replace.replace().replace", "context.replace().replace().replace.split", "outf.write", "context.replace().replace().replace.split", "outf.write", "outf.write", "context.replace().replace().replace.replace"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "preprocessing_context", "(", "context", ",", "outf", "=", "None", ",", "shortest_context", "=", "50", ",", "longest_context", "=", "150", ")", ":", "\n", "    ", "bad_context", "=", "False", "\n", "context", "=", "context", ".", "replace", "(", "'\u00e2\u0080\u0099'", ",", "\"'\"", ")", ".", "replace", "(", "'\u00e2\u0080\u0093'", ",", "'-'", ")", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", "\n", "if", "len", "(", "context", ".", "split", "(", ")", ")", "<", "shortest_context", ":", "\n", "#if len(context.split()) < 5:", "\n", "        ", "if", "outf", "is", "not", "None", ":", "\n", "            ", "outf", ".", "write", "(", "\"Skip due to short context\\n\"", ")", "\n", "", "bad_context", "=", "True", "\n", "#if len(context.split()) > 50:", "\n", "", "if", "len", "(", "context", ".", "split", "(", ")", ")", ">", "longest_context", ":", "\n", "        ", "if", "outf", "is", "not", "None", ":", "\n", "            ", "outf", ".", "write", "(", "\"Skip due to long context\\n\"", ")", "\n", "", "bad_context", "=", "True", "\n", "", "try", ":", "\n", "        ", "context", ".", "encode", "(", "'ascii'", ",", "'strict'", ")", "\n", "", "except", ":", "\n", "        ", "if", "outf", "is", "not", "None", ":", "\n", "            ", "outf", ".", "write", "(", "\"Skip due to special token\\n\"", ")", "\n", "", "bad_context", "=", "True", "\n", "", "return", "context", ",", "bad_context", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.eval_basis_conditional_text_single": [[312, 390], ["top_index.size", "gen_sent_tensor.size", "range", "range", "inner_idx_tensor[].item", "tokenizer_GPT2.decode", "outf.write", "dict", "range", "utils_testing.saparateParagraph", "range", "range", "range", "result_stats.update_self_BLEU", "result_stats.renew_ngram", "outf.write", "str", "range", "tokenizer_GPT2.decode", "sent_list.append", "csvOutf.writerow", "result_stats.update", "len", "utils_testing.preprocessing_context", "utils_testing.preprocessing_context", "[].append", "[].append", "utils_testing.generateString", "len", "str", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "top_value[].item", "type_idx_tensor[].item", "top_index[].item"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.saparateParagraph", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update_self_BLEU", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.renew_ngram", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString"], ["", "def", "eval_basis_conditional_text_single", "(", "feature", ",", "idx2word_freq", ",", "top_value", ",", "top_index", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ",", "gen_sent_tensor", ",", "selected_topic_idx_arr", ",", "result_stats", ",", "word_raw_list", ",", "word_raw_rest_list", ",", "outf", ",", "csvOutf", ",", "readable_context", ",", "topic_mode", ",", "run_eval", ",", "type_idx_tensor", ",", "idx_l2_type", ")", ":", "\n", "    ", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", "=", "top_index", ".", "size", "(", ")", "\n", "num_sent_gen", "=", "gen_sent_tensor", ".", "size", "(", "2", ")", "\n", "#for i_sent in range(1):", "\n", "for", "i_sent", "in", "range", "(", "batch_size", ")", ":", "\n", "#outf.write('batch number: ' + str(i_sent) + '\\n')", "\n", "        ", "last_end", "=", "-", "1", "\n", "#for m in range(3):", "\n", "for", "m", "in", "range", "(", "num_head", ")", ":", "\n", "#outf.write('number of head: ' + str(m) + '\\n')", "\n", "            ", "end", "=", "inner_idx_tensor", "[", "i_sent", ",", "m", "]", ".", "item", "(", ")", "\n", "if", "end", "==", "last_end", ":", "\n", "                ", "continue", "\n", "", "last_end", "=", "end", "\n", "#outf.write(tokenizer_GPT2.convert_tokens_to_string(feature_text[i_sent][:end])+'\\n')", "\n", "context", "=", "tokenizer_GPT2", ".", "decode", "(", "feature", "[", "i_sent", ",", ":", "end", "]", ")", "\n", "if", "readable_context", ":", "\n", "                ", "if", "len", "(", "idx_l2_type", ")", ">", "0", ":", "\n", "                    ", "context", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ",", "0", ",", "10000000", ")", "\n", "", "else", ":", "\n", "                    ", "context", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ")", "\n", "#context, bad_context = preprocessing_context(context, outf)", "\n", "", "if", "bad_context", ":", "\n", "                    ", "continue", "\n", "", "", "outf", ".", "write", "(", "context", "+", "'\\n'", ")", "\n", "\n", "topics", "=", "dict", "(", ")", "\n", "for", "j", "in", "range", "(", "n_basis", ")", ":", "\n", "                ", "outf", ".", "write", "(", "str", "(", "j", ")", "+", "', '", ")", "\n", "topics", "[", "j", "]", "=", "{", "}", "\n", "topics", "[", "j", "]", "[", "\"name\"", "]", "=", "str", "(", "j", ")", "\n", "topics", "[", "j", "]", "[", "\"keywords\"", "]", "=", "[", "]", "\n", "topics", "[", "j", "]", "[", "\"weights\"", "]", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "                    ", "word_nn", "=", "idx2word_freq", "[", "top_index", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "topics", "[", "j", "]", "[", "\"keywords\"", "]", ".", "append", "(", "word_nn", ")", "\n", "topics", "[", "j", "]", "[", "\"weights\"", "]", ".", "append", "(", "' {:5.3f}'", ".", "format", "(", "top_value", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", ")", ")", "\n", "", "", "selected_topic_idx", "=", "selected_topic_idx_arr", "[", "i_sent", "]", "[", "m", "]", "\n", "\n", "prev", ",", "last", "=", "saparateParagraph", "(", "context", ")", "\n", "selected_topics", "=", "' | '", ".", "join", "(", "[", "generateString", "(", "topics", "[", "x", "]", ")", "for", "x", "in", "selected_topic_idx", "]", ")", "\n", "\n", "# if len(pplm_sent[i_sent][m][0]) == 0:", "\n", "#     outf.write('Skipping this context because PPLM cannot condition on any word.\\n')", "\n", "#     continue", "\n", "\n", "sent_list", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "                ", "generated_sent", "=", "tokenizer_GPT2", ".", "decode", "(", "gen_sent_tensor", "[", "i_sent", ",", "m", ",", "j", ",", ":", "]", ")", "\n", "sent_list", ".", "append", "(", "generated_sent", ")", "\n", "\n", "# cond_sent_list_out, org_sent_list_out, pplm_sent_list_out, excluding_j_set, num_not_ascii = filter_generated_sents(cond_sent_list, org_sent_list, pplm_sent[i_sent][m])", "\n", "# not_ascii_conditional += num_not_ascii[0]", "\n", "# not_ascii_org += num_not_ascii[1] ", "\n", "# not_ascii_PPLM += num_not_ascii[2]", "\n", "\n", "", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "# if j in excluding_j_set:", "\n", "#     continue", "\n", "                ", "if", "len", "(", "idx_l2_type", ")", ">", "0", ":", "\n", "                    ", "prompt_type_info", "=", "idx_l2_type", "[", "type_idx_tensor", "[", "j", "]", ".", "item", "(", ")", "]", "\n", "", "else", ":", "\n", "                    ", "prompt_type_info", "=", "''", "\n", "", "csvOutf", ".", "writerow", "(", "[", "prev", ",", "last", ",", "generateString", "(", "topics", "[", "0", "]", ")", ",", "generateString", "(", "topics", "[", "1", "]", ")", ",", "generateString", "(", "topics", "[", "2", "]", ")", ",", "generateString", "(", "topics", "[", "3", "]", ")", ",", "generateString", "(", "topics", "[", "4", "]", ")", ",", "generateString", "(", "topics", "[", "5", "]", ")", ",", "generateString", "(", "topics", "[", "6", "]", ")", ",", "generateString", "(", "topics", "[", "7", "]", ")", ",", "generateString", "(", "topics", "[", "8", "]", ")", ",", "generateString", "(", "topics", "[", "9", "]", ")", ",", "selected_topics", ",", "sent_list", "[", "j", "]", ",", "topic_mode", ",", "prompt_type_info", "]", ")", "\n", "# if gen_sent_tensor_org.size(0) > 0:", "\n", "#     csvOutf.writerow([prev, last, generateString(topics[0]), generateString(topics[1]), generateString(topics[2]), generateString(topics[3]), generateString(topics[4]), generateString(topics[5]), generateString(topics[6]), generateString(topics[7]), generateString(topics[8]), generateString(topics[9]), selected_topics, org_sent_list_out[j], 'Original '+ str(j)])", "\n", "# csvOutf.writerow([prev, last, generateString(topics[0]), generateString(topics[1]), generateString(topics[2]), generateString(topics[3]), generateString(topics[4]), generateString(topics[5]), generateString(topics[6]), generateString(topics[7]), generateString(topics[8]), generateString(topics[9]), selected_topics, pplm_sent_list_out[j], 'PPLM '+ str(j)])", "\n", "\n", "", "if", "not", "run_eval", ":", "\n", "                ", "continue", "\n", "", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "#generated_sent = cond_sent_list[j]", "\n", "#generated_sent = tokenizer_GPT2.decode( gen_sent_tensor[i_sent, m, j, :] )", "\n", "#print_sampled_sent(selected_topic_idx, generated_sent, top_index[i_sent,m,:,:], idx2word_freq, outf, 'conditional '+ str(j))", "\n", "                ", "result_stats", ".", "update", "(", "\"Model condition\"", ",", "gen_sent_tensor", "[", "i_sent", ",", "m", ",", "j", ",", ":", "]", ",", "feature", "[", "i_sent", ",", ":", "end", "]", ",", "selected_topic_idx", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "tokenizer_GPT2", ",", "word_raw_list", "[", "i_sent", "]", "[", "m", "]", ",", "word_raw_rest_list", "[", "i_sent", "]", "[", "m", "]", ",", "m", ")", "\n", "", "result_stats", ".", "update_self_BLEU", "(", "\"Model condition\"", ")", "\n", "", "if", "run_eval", ":", "\n", "            ", "result_stats", ".", "renew_ngram", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_basis_conditional_text": [[391, 518], ["top_index.size", "gen_sent_tensor.size", "range", "outf.write", "outf.write", "range", "outf.write", "inner_idx_tensor[].item", "tokenizer_GPT2.decode", "outf.write", "dict", "range", "outf.write", "utils_testing.saparateParagraph", "outf.write", "range", "utils_testing.filter_generated_sents", "range", "outf.write", "range", "result_stats.update_self_BLEU", "range", "result_stats.update_self_BLEU", "result_stats.renew_ngram", "outf.write", "str", "range", "outf.write", "len", "outf.write", "tokenizer_GPT2.decode", "cond_sent_list.append", "csvOutf.writerow", "utils_testing.print_sampled_sent", "csvOutf.writerow", "utils_testing.print_sampled_sent", "result_stats.update", "gen_sent_tensor_org.size", "range", "result_stats.update_self_BLEU", "torch.tensor", "torch.tensor", "torch.tensor", "result_stats.update", "str", "len", "utils_testing.preprocessing_context", "utils_testing.preprocessing_context", "outf.write", "[].append", "[].append", "utils_testing.generateString", "gen_sent_tensor_org.size", "tokenizer_GPT2.decode", "org_sent_list.append", "len", "gen_sent_tensor_org.size", "csvOutf.writerow", "utils_testing.print_sampled_sent", "result_stats.update", "tokenizer_GPT2.encode", "str", "str", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "str", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "str", "top_value[].item", "type_idx_tensor[].item", "str", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "utils_testing.generateString", "str", "str", "top_index[].item", "str", "str", "top_value[].item"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.saparateParagraph", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.filter_generated_sents", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update_self_BLEU", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update_self_BLEU", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.renew_ngram", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_sampled_sent", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_sampled_sent", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update_self_BLEU", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_sampled_sent", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.generateString"], ["", "", "", "def", "print_basis_conditional_text", "(", "feature", ",", "pplm_sent", ",", "idx2word_freq", ",", "top_value", ",", "top_index", ",", "i_batch", ",", "outf", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ",", "gen_sent_tensor", ",", "gen_sent_tensor_org", ",", "selected_topic_idx_arr", ",", "result_stats", ",", "word_raw_list", ",", "word_raw_rest_list", ",", "csvOutf", ",", "readable_context", ",", "run_eval", ",", "type_idx_tensor", ",", "idx_l2_type", ")", ":", "\n", "    ", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", "=", "top_index", ".", "size", "(", ")", "\n", "num_sent_gen", "=", "gen_sent_tensor", ".", "size", "(", "2", ")", "\n", "#feature_text = [ [tokenizer_GPT2._convert_id_to_token(x) for x in feature[i,:].tolist()] for i in range(feature.size(0))]", "\n", "# for i_sent in range(1):", "\n", "not_ascii_conditional", "=", "0", "\n", "not_ascii_org", "=", "0", "\n", "not_ascii_PPLM", "=", "0", "\n", "for", "i_sent", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "outf", ".", "write", "(", "'batch number: '", "+", "str", "(", "i_sent", ")", "+", "'\\n'", ")", "\n", "last_end", "=", "-", "1", "\n", "# for m in range(1):", "\n", "for", "m", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "outf", ".", "write", "(", "'number of head: '", "+", "str", "(", "m", ")", "+", "'\\n'", ")", "\n", "end", "=", "inner_idx_tensor", "[", "i_sent", ",", "m", "]", ".", "item", "(", ")", "\n", "if", "end", "==", "last_end", ":", "\n", "                ", "continue", "\n", "", "last_end", "=", "end", "\n", "\n", "#outf.write(tokenizer_GPT2.convert_tokens_to_string(feature_text[i_sent][:end])+'\\n')", "\n", "context", "=", "tokenizer_GPT2", ".", "decode", "(", "feature", "[", "i_sent", ",", ":", "end", "]", ")", "\n", "#.replace('\u00e2\u0080\u0099',\"'\").replace('\\n',\" \")", "\n", "if", "readable_context", ":", "\n", "                ", "if", "len", "(", "idx_l2_type", ")", ">", "0", ":", "\n", "                    ", "context", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ",", "0", ",", "10000000", ")", "\n", "", "else", ":", "\n", "                    ", "context", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ")", "\n", "", "if", "bad_context", ":", "\n", "                    ", "continue", "\n", "\n", "#if len(context.split()) < 50:", "\n", "#    outf.write(\"Skip {} sent {} head due to short context\\n\".format(i_sent, m))", "\n", "#    continue", "\n", "#try:", "\n", "#    context.encode('ascii', 'strict')", "\n", "#except:", "\n", "#    outf.write(\"Skip {} sent {} head due to special token\\n\".format(i_sent, m))", "\n", "#    continue", "\n", "\n", "#outf.write(tokenizer_GPT2.decode(feature[i_sent,:end])+'\\n')", "\n", "", "", "outf", ".", "write", "(", "context", "+", "'\\n'", ")", "\n", "\n", "topics", "=", "dict", "(", ")", "\n", "for", "j", "in", "range", "(", "n_basis", ")", ":", "\n", "\n", "#org_ind = coeff_order[i_sent, j]", "\n", "#outf.write(str(j)+', org '+str(org_ind)+', '+str( coeff_sum[i_sent,org_ind,0] )+' - '+str( coeff_sum[i_sent,org_ind,1] )+': ')", "\n", "                ", "outf", ".", "write", "(", "str", "(", "j", ")", "+", "', '", ")", "\n", "topics", "[", "j", "]", "=", "{", "}", "\n", "topics", "[", "j", "]", "[", "\"name\"", "]", "=", "str", "(", "j", ")", "\n", "topics", "[", "j", "]", "[", "\"keywords\"", "]", "=", "[", "]", "\n", "topics", "[", "j", "]", "[", "\"weights\"", "]", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "#print(i_sent,m,k,j, top_index.size())", "\n", "#print(top_index[i_sent,m,k,j].item(), len(idx2word_freq))", "\n", "                    ", "word_nn", "=", "idx2word_freq", "[", "top_index", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "outf", ".", "write", "(", "word_nn", "+", "' {:5.3f}'", ".", "format", "(", "top_value", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", ")", "+", "', '", ")", "\n", "topics", "[", "j", "]", "[", "\"keywords\"", "]", ".", "append", "(", "word_nn", ")", "\n", "topics", "[", "j", "]", "[", "\"weights\"", "]", ".", "append", "(", "' {:5.3f}'", ".", "format", "(", "top_value", "[", "i_sent", ",", "m", ",", "k", ",", "j", "]", ".", "item", "(", ")", ")", ")", "\n", "", "outf", ".", "write", "(", "'\\n'", ")", "\n", "", "outf", ".", "write", "(", "'\\n'", ")", "\n", "prev", ",", "last", "=", "saparateParagraph", "(", "context", ")", "\n", "selected_topic_idx", "=", "selected_topic_idx_arr", "[", "i_sent", "]", "[", "m", "]", "\n", "# selected_topic = '|'.join(generateString(topics[x]) for x in selected_topic_idx)", "\n", "selected_topics", "=", "' | '", ".", "join", "(", "[", "generateString", "(", "topics", "[", "x", "]", ")", "for", "x", "in", "selected_topic_idx", "]", ")", "\n", "outf", ".", "write", "(", "'Select these topics '", "+", "' '", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "selected_topic_idx", "]", ")", "+", "'\\n'", ")", "\n", "\n", "if", "len", "(", "pplm_sent", "[", "i_sent", "]", "[", "m", "]", "[", "0", "]", ")", "==", "0", ":", "\n", "                ", "outf", ".", "write", "(", "'Skipping this context because PPLM cannot condition on any word.\\n'", ")", "\n", "continue", "\n", "\n", "", "cond_sent_list", "=", "[", "]", "\n", "org_sent_list", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "                ", "generated_sent", "=", "tokenizer_GPT2", ".", "decode", "(", "gen_sent_tensor", "[", "i_sent", ",", "m", ",", "j", ",", ":", "]", ")", "\n", "cond_sent_list", ".", "append", "(", "generated_sent", ")", "\n", "if", "gen_sent_tensor_org", ".", "size", "(", "0", ")", ">", "0", ":", "\n", "                    ", "generated_sent_org", "=", "tokenizer_GPT2", ".", "decode", "(", "gen_sent_tensor_org", "[", "i_sent", ",", "m", ",", "j", ",", ":", "]", ")", "\n", "org_sent_list", ".", "append", "(", "generated_sent_org", ")", "\n", "\n", "", "", "cond_sent_list_out", ",", "org_sent_list_out", ",", "pplm_sent_list_out", ",", "excluding_j_set", ",", "num_not_ascii", "=", "filter_generated_sents", "(", "cond_sent_list", ",", "org_sent_list", ",", "pplm_sent", "[", "i_sent", "]", "[", "m", "]", ")", "\n", "not_ascii_conditional", "+=", "num_not_ascii", "[", "0", "]", "\n", "not_ascii_org", "+=", "num_not_ascii", "[", "1", "]", "\n", "not_ascii_PPLM", "+=", "num_not_ascii", "[", "2", "]", "\n", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "                ", "if", "j", "in", "excluding_j_set", ":", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "idx_l2_type", ")", ">", "0", ":", "\n", "                    ", "prompt_type_info", "=", "idx_l2_type", "[", "type_idx_tensor", "[", "j", "]", ".", "item", "(", ")", "]", "\n", "", "else", ":", "\n", "                    ", "prompt_type_info", "=", "''", "\n", "", "csvOutf", ".", "writerow", "(", "[", "prev", ",", "last", ",", "generateString", "(", "topics", "[", "0", "]", ")", ",", "generateString", "(", "topics", "[", "1", "]", ")", ",", "generateString", "(", "topics", "[", "2", "]", ")", ",", "generateString", "(", "topics", "[", "3", "]", ")", ",", "generateString", "(", "topics", "[", "4", "]", ")", ",", "generateString", "(", "topics", "[", "5", "]", ")", ",", "generateString", "(", "topics", "[", "6", "]", ")", ",", "generateString", "(", "topics", "[", "7", "]", ")", ",", "generateString", "(", "topics", "[", "8", "]", ")", ",", "generateString", "(", "topics", "[", "9", "]", ")", ",", "selected_topics", ",", "cond_sent_list_out", "[", "j", "]", ",", "'model condition '", "+", "str", "(", "j", ")", ",", "prompt_type_info", "]", ")", "\n", "print_sampled_sent", "(", "selected_topic_idx", ",", "cond_sent_list", "[", "j", "]", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "outf", ",", "'conditional '", "+", "str", "(", "j", ")", ")", "\n", "if", "gen_sent_tensor_org", ".", "size", "(", "0", ")", ">", "0", ":", "\n", "                    ", "csvOutf", ".", "writerow", "(", "[", "prev", ",", "last", ",", "generateString", "(", "topics", "[", "0", "]", ")", ",", "generateString", "(", "topics", "[", "1", "]", ")", ",", "generateString", "(", "topics", "[", "2", "]", ")", ",", "generateString", "(", "topics", "[", "3", "]", ")", ",", "generateString", "(", "topics", "[", "4", "]", ")", ",", "generateString", "(", "topics", "[", "5", "]", ")", ",", "generateString", "(", "topics", "[", "6", "]", ")", ",", "generateString", "(", "topics", "[", "7", "]", ")", ",", "generateString", "(", "topics", "[", "8", "]", ")", ",", "generateString", "(", "topics", "[", "9", "]", ")", ",", "selected_topics", ",", "org_sent_list_out", "[", "j", "]", ",", "'Original '", "+", "str", "(", "j", ")", ",", "prompt_type_info", "]", ")", "\n", "print_sampled_sent", "(", "selected_topic_idx", ",", "generated_sent_org", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "outf", ",", "'original '", "+", "str", "(", "j", ")", ")", "\n", "", "csvOutf", ".", "writerow", "(", "[", "prev", ",", "last", ",", "generateString", "(", "topics", "[", "0", "]", ")", ",", "generateString", "(", "topics", "[", "1", "]", ")", ",", "generateString", "(", "topics", "[", "2", "]", ")", ",", "generateString", "(", "topics", "[", "3", "]", ")", ",", "generateString", "(", "topics", "[", "4", "]", ")", ",", "generateString", "(", "topics", "[", "5", "]", ")", ",", "generateString", "(", "topics", "[", "6", "]", ")", ",", "generateString", "(", "topics", "[", "7", "]", ")", ",", "generateString", "(", "topics", "[", "8", "]", ")", ",", "generateString", "(", "topics", "[", "9", "]", ")", ",", "selected_topics", ",", "pplm_sent_list_out", "[", "j", "]", ",", "'PPLM '", "+", "str", "(", "j", ")", ",", "prompt_type_info", "]", ")", "\n", "print_sampled_sent", "(", "selected_topic_idx", ",", "pplm_sent", "[", "i_sent", "]", "[", "m", "]", "[", "j", "]", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "outf", ",", "'pplm model '", "+", "str", "(", "j", ")", ")", "\n", "", "outf", ".", "write", "(", "'\\n\\n'", ")", "\n", "if", "not", "run_eval", ":", "\n", "                ", "continue", "\n", "", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "#During the print, highlight the words which occur in generated sentences", "\n", "#search directly without tokenization", "\n", "#make this a function", "\n", "#generated_sent = tokenizer_GPT2.convert_tokens_to_string( [tokenizer_GPT2._convert_id_to_token(x) for x in gen_sent_tensor[i_sent, m, j, :].tolist()] )", "\n", "#generated_sent = tokenizer_GPT2.decode( gen_sent_tensor[i_sent, m, j, :] )", "\n", "                ", "generated_sent", "=", "cond_sent_list", "[", "j", "]", "\n", "#print_sampled_sent(selected_topic_idx, generated_sent, top_index[i_sent,m,:,:], idx2word_freq, outf, 'conditional '+ str(j))", "\n", "result_stats", ".", "update", "(", "\"Model condition\"", ",", "gen_sent_tensor", "[", "i_sent", ",", "m", ",", "j", ",", ":", "]", ",", "feature", "[", "i_sent", ",", ":", "end", "]", ",", "selected_topic_idx", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "tokenizer_GPT2", ",", "word_raw_list", "[", "i_sent", "]", "[", "m", "]", ",", "word_raw_rest_list", "[", "i_sent", "]", "[", "m", "]", ",", "m", ")", "\n", "", "result_stats", ".", "update_self_BLEU", "(", "\"Model condition\"", ")", "\n", "if", "gen_sent_tensor_org", ".", "size", "(", "0", ")", ">", "0", ":", "\n", "                ", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "#generated_sent_org = tokenizer_GPT2.convert_tokens_to_string( [tokenizer_GPT2._convert_id_to_token(x) for x in gen_sent_tensor_org[i_sent, m, j, :].tolist()] )", "\n", "#generated_sent_org = tokenizer_GPT2.decode( gen_sent_tensor_org[i_sent, m, j, :] )", "\n", "                    ", "generated_sent_org", "=", "org_sent_list", "[", "j", "]", "\n", "#print_sampled_sent(selected_topic_idx, generated_sent_org, top_index[i_sent,m,:,:], idx2word_freq, outf, 'original '+ str(j))", "\n", "result_stats", ".", "update", "(", "\"Original\"", ",", "gen_sent_tensor_org", "[", "i_sent", ",", "m", ",", "j", ",", ":", "]", ",", "feature", "[", "i_sent", ",", ":", "end", "]", ",", "selected_topic_idx", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "tokenizer_GPT2", ",", "word_raw_list", "[", "i_sent", "]", "[", "m", "]", ",", "word_raw_rest_list", "[", "i_sent", "]", "[", "m", "]", ",", "m", ")", "\n", "", "result_stats", ".", "update_self_BLEU", "(", "\"Original\"", ")", "\n", "", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "#print_sampled_sent(selected_topic_idx, pplm_sent[i_sent][m][j], top_index[i_sent,m,:,:], idx2word_freq, outf, 'pplm model '+ str(j))", "\n", "                ", "sentence", "=", "torch", ".", "tensor", "(", "tokenizer_GPT2", ".", "encode", "(", "pplm_sent", "[", "i_sent", "]", "[", "m", "]", "[", "j", "]", ")", ",", "device", "=", "\"cuda\"", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "result_stats", ".", "update", "(", "\"PPLM\"", ",", "sentence", ",", "feature", "[", "i_sent", ",", ":", "end", "]", ",", "selected_topic_idx", ",", "top_index", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "tokenizer_GPT2", ",", "word_raw_list", "[", "i_sent", "]", "[", "m", "]", ",", "word_raw_rest_list", "[", "i_sent", "]", "[", "m", "]", ",", "m", ")", "\n", "", "result_stats", ".", "update_self_BLEU", "(", "\"PPLM\"", ")", "\n", "", "if", "run_eval", ":", "\n", "            ", "result_stats", ".", "renew_ngram", "(", ")", "\n", "", "", "outf", ".", "write", "(", "'Number of not ascii code in coditional: {}, in org: {}, in PPLM: {}\\n'", ".", "format", "(", "not_ascii_conditional", ",", "not_ascii_org", ",", "not_ascii_PPLM", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.top_k_logits": [[519, 526], ["torch.topk", "torch.topk", "torch.topk", "values[].view().expand_as", "torch.where", "torch.where", "torch.where", "values[].view", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "function", ["None"], ["", "def", "top_k_logits", "(", "logits", ",", "k", ")", ":", "\n", "#modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/sample.py", "\n", "    ", "if", "k", "==", "0", ":", "\n", "        ", "return", "logits", "\n", "", "values", ",", "_", "=", "torch", ".", "topk", "(", "logits", ",", "k", ")", "\n", "min_values", "=", "values", "[", ":", ",", "-", "1", "]", ".", "view", "(", "-", "1", ",", "1", ")", ".", "expand_as", "(", "logits", ")", "\n", "return", "torch", ".", "where", "(", "logits", "<", "min_values", ",", "torch", ".", "ones_like", "(", "logits", ",", "dtype", "=", "logits", ".", "dtype", ")", "*", "-", "1e10", ",", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq": [[527, 570], ["torch.multinomial.size", "torch.zeros", "torch.zeros", "torch.zeros", "range", "utils_testing.top_k_logits", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "model_condition", "model_condition", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.topk", "torch.topk", "torch.topk", "torch.isnan().sum", "torch.isnan().sum", "torch.isnan().sum", "print", "print", "print", "print", "print", "print", "sys.exit", "torch.isnan", "torch.isnan", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.top_k_logits", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "sample_seq", "(", "model_condition", ",", "context", ",", "insert_loc", ",", "future_emb_chosen_arr", ",", "gen_sent_len", ",", "device", ",", "temperature", "=", "1", ",", "top_k", "=", "40", ",", "sample", "=", "True", ")", ":", "\n", "#def sample_seq(model_condition, context, insert_loc, future_emb_chosen_arr, gen_sent_len, device, temperature=1, top_k = 5, sample=True):", "\n", "#modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/sample.py", "\n", "    ", "prev", "=", "context", "\n", "batch_size", "=", "prev", ".", "size", "(", "0", ")", "\n", "output", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "0", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "past", "=", "None", "\n", "#sample = False", "\n", "for", "i", "in", "range", "(", "gen_sent_len", ")", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "outputs_condition", "=", "model_condition", "(", "prev", ",", "past", "=", "past", ",", "insert_loc", "=", "insert_loc", ",", "future_emb_chosen_arr", "=", "future_emb_chosen_arr", ")", "# lm_logits, presents, (all hidden_states), (attentions)", "\n", "", "else", ":", "\n", "            ", "outputs_condition", "=", "model_condition", "(", "prev", ",", "past", "=", "past", ")", "\n", "", "logits", "=", "outputs_condition", "[", "0", "]", "\n", "past", "=", "outputs_condition", "[", "1", "]", "\n", "logits", "=", "logits", "[", ":", ",", "-", "1", ",", ":", "]", "/", "temperature", "\n", "logits", "=", "top_k_logits", "(", "logits", ",", "k", "=", "top_k", ")", "\n", "#max_val, _ = torch.max(logits,dim=1,keepdim=True)", "\n", "#logits_max_too_small = (max_val < -50)", "\n", "#logits[logits_max_too_small * (logits != -1e10 )] = 0", "\n", "#exp_logits = torch.exp(logits)", "\n", "#probs = exp_logits / torch.sum(exp_logits, dim = -1, keepdim=True)", "\n", "\n", "#if de.nonzero().size(0) != de.numel():", "\n", "#    print(logits)", "\n", "#    sys.exit(0)", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "#probs = torch.exp(F.log_softmax(logits, dim=-1))", "\n", "if", "sample", ":", "\n", "#probs[probs < 0] = 0", "\n", "            ", "if", "torch", ".", "isnan", "(", "probs", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "                ", "print", "(", "past", ")", "\n", "print", "(", "prev", ")", "\n", "print", "(", "insert_loc", ")", "\n", "print", "(", "future_emb_chosen_arr", ")", "\n", "print", "(", "logits", ")", "\n", "print", "(", "probs", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "", "prev", "=", "torch", ".", "multinomial", "(", "probs", ",", "num_samples", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "_", ",", "prev", "=", "torch", ".", "topk", "(", "probs", ",", "k", "=", "1", ",", "dim", "=", "-", "1", ")", "\n", "", "output", "=", "torch", ".", "cat", "(", "(", "output", ",", "prev", ")", ",", "dim", "=", "1", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.visualize_interactive_LM": [[572, 747], ["spacy.lang.en.English", "range", "torch.sum", "torch.sum", "torch.sum", "torch.nonzero().squeeze().cpu().tolist", "torch.nonzero().squeeze().cpu().tolist", "torch.nonzero().squeeze().cpu().tolist", "print", "print", "set", "len", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "torch.nonzero().squeeze().cpu", "torch.nonzero().squeeze().cpu", "torch.nonzero().squeeze().cpu", "len", "result_statistics.result_statistics", "result_statistics.result_statistics.add_model", "result_statistics.result_statistics.add_model", "result_statistics.result_statistics.add_model", "print", "sys.stdout.flush", "utils_testing.predict_batch", "inner_idx_tensor.unsqueeze.size", "top_index.view.view", "top_value.view.view", "word_w_sum_norm.to.to", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "utils_testing.get_word_list_spacy", "range", "utils_testing.print_basis_conditional_text", "result_statistics.result_statistics.print", "result_statistics.result_statistics.generate_report", "inner_idx_tensor.unsqueeze.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "top_value.view.unsqueeze().sum", "print", "range", "pplm_sent.append", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "str", "tokenizer_GPT2._convert_id_to_token", "range", "word_norm_emb_w_sum.norm", "range", "print", "tokenizer_GPT2.convert_tokens_to_string", "end.item", "insert_loc_list.append", "random.randint", "numpy.sort", "torch.tensor.tolist", "torch.tensor.tolist", "time.time", "torch.tensor", "torch.tensor", "torch.tensor", "feature[].unsqueeze().expand().to", "word_w_sum_norm[].unsqueeze().expand", "future_emb_chosen_arr.append", "utils_testing.sample_seq", "time.time", "time.time", "tokenizer_GPT2.convert_tokens_to_string", "temp.append", "feature[].tolist", "feature.size", "top_value.view.unsqueeze", "top_value.view.unsqueeze", "range", "str", "temp.append", "numpy.random.choice", "range", "word_norm_emb.size", "numpy.array", "time.time", "utils_testing.sample_seq", "utils_testing.sample_seq", "time.time", "pplm_model.run_pplm_example", "time.time", "torch.nonzero", "torch.nonzero", "torch.nonzero", "str", "utils_testing.preprocessing_context", "temp.append", "bag_of_words.append", "feature[].unsqueeze().expand", "word_w_sum_norm[].unsqueeze", "print", "len", "utils_testing.preprocessing_context", "torch.zeros", "torch.zeros", "torch.zeros", "feature[].unsqueeze", "range", "top_index[].item", "word_norm_emb.size"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.add_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.add_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.add_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.predict_batch", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_word_list_spacy", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_basis_conditional_text", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.generate_report", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.run_pplm_example", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.preprocessing_context"], ["", "def", "visualize_interactive_LM", "(", "model_condition", ",", "pplm_model", ",", "gpt2_model", ",", "device_conditional", ",", "num_sent_gen", ",", "gen_sent_len", ",", "dataloader", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "idx2word_freq", ",", "outf", ",", "n_basis", ",", "max_batch_num", ",", "de_en_connection", ",", "tokenizer_GPT2", ",", "stop_word_set", ",", "bptt_conditional", ",", "csvOutf", ",", "model_org", ",", "readable_context", "=", "False", ",", "run_eval", "=", "True", ",", "use_corpus", "=", "'wiki'", ",", "idx_l2_type", "=", "[", "]", ")", ":", "\n", "    ", "top_k", "=", "5", "\n", "nlp", "=", "English", "(", ")", "\n", "word_d2_idx", "=", "{", "}", "\n", "for", "idx", "in", "range", "(", "len", "(", "idx2word_freq", ")", ")", ":", "\n", "        ", "word", "=", "idx2word_freq", "[", "idx", "]", "[", "0", "]", "\n", "word_d2_idx", "[", "word", "]", "=", "idx", "\n", "\n", "", "emb_sum", "=", "torch", ".", "sum", "(", "word_norm_emb", ",", "dim", "=", "1", ")", "\n", "OOV_list", "=", "torch", ".", "nonzero", "(", "emb_sum", "==", "0", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "print", "(", "\"OOV number = {}\"", ".", "format", "(", "len", "(", "OOV_list", ")", ")", ")", "\n", "print", "(", "\"OOV index examples {}\"", ".", "format", "(", "OOV_list", "[", ":", "10", "]", ")", ")", "\n", "OOV_set", "=", "set", "(", "OOV_list", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "if", "run_eval", ":", "\n", "            ", "result_stats", "=", "result_statistics", "(", "gpt2_model", ")", "\n", "result_stats", ".", "add_model", "(", "\"Model condition\"", ")", "\n", "result_stats", ".", "add_model", "(", "\"Original\"", ")", "\n", "result_stats", ".", "add_model", "(", "\"PPLM\"", ")", "\n", "", "else", ":", "\n", "            ", "result_stats", "=", "[", "]", "\n", "", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "# if i_batch == 0:", "\n", "#     continue", "\n", "            ", "print", "(", "\"batch\"", "+", "str", "(", "i_batch", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "if", "use_corpus", "==", "'wiki'", ":", "\n", "                ", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "=", "sample_batched", "\n", "type_idx_tensor", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "feature", ",", "inner_idx_tensor", ",", "type_idx_tensor", "=", "sample_batched", "\n", "inner_idx_tensor", "=", "inner_idx_tensor", ".", "unsqueeze", "(", "1", ")", "\n", "future_mask", "=", "[", "]", "\n", "\n", "", "feature_text", "=", "[", "[", "tokenizer_GPT2", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "feature", "[", "i", ",", ":", "]", ".", "tolist", "(", ")", "]", "for", "i", "in", "range", "(", "feature", ".", "size", "(", "0", ")", ")", "]", "\n", "\n", "basis_norm_pred", ",", "top_value", ",", "top_index", "=", "predict_batch", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ")", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "\n", "# the index of each words in the vocab list", "\n", "top_index", "=", "top_index", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "\n", "# the value of each words", "\n", "top_value", "=", "top_value", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "\n", "\n", "word_norm_emb_top", "=", "word_norm_emb", "[", "top_index", ",", ":", "]", "\n", "word_norm_emb_w_sum", "=", "torch", ".", "sum", "(", "word_norm_emb_top", "*", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", ")", "/", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "word_w_sum_norm", "=", "word_norm_emb_w_sum", "/", "(", "0.000000000001", "+", "word_norm_emb_w_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "word_w_sum_norm", "=", "word_w_sum_norm", ".", "to", "(", "device", "=", "device_conditional", ")", "\n", "gen_sent_tensor", "=", "torch", ".", "empty", "(", "(", "batch_size", ",", "num_head", ",", "num_sent_gen", ",", "gen_sent_len", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "gen_sent_tensor_org", "=", "torch", ".", "empty", "(", "(", "batch_size", ",", "num_head", ",", "num_sent_gen", ",", "gen_sent_len", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "selected_topic_idx_arr", "=", "[", "[", "[", "]", "for", "j", "in", "range", "(", "num_head", ")", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "pplm_sent", "=", "[", "]", "\n", "\n", "#word_idx_list, word_idx_rest_list, word_raw_list, word_raw_rest_list, word_full_list = get_word_list_spacy(inner_idx_tensor, feature_text, tokenizer_GPT2, nlp, word_d2_idx, stop_word_set, OOV_set)", "\n", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "word_raw_rest_list", "=", "get_word_list_spacy", "(", "inner_idx_tensor", ",", "feature_text", ",", "tokenizer_GPT2", ",", "nlp", ",", "word_d2_idx", ",", "stop_word_set", ",", "OOV_set", ")", "\n", "# for i_sent in range(1):", "\n", "for", "i_sent", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "print", "(", "\"sent\"", "+", "str", "(", "i_sent", ")", ")", "\n", "insert_loc_list", "=", "[", "]", "\n", "future_emb_chosen_arr", "=", "[", "]", "\n", "last_end", "=", "-", "1", "\n", "temp", "=", "[", "]", "\n", "# for m in range(1):", "\n", "for", "m", "in", "range", "(", "num_head", ")", ":", "\n", "                    ", "print", "(", "\"head\"", "+", "str", "(", "m", ")", ")", "\n", "\n", "\n", "end", "=", "inner_idx_tensor", "[", "i_sent", ",", "m", "]", "\n", "if", "end", "==", "last_end", ":", "\n", "                        ", "gen_text", "=", "[", "''", "]", "*", "num_sent_gen", "\n", "temp", ".", "append", "(", "gen_text", ")", "\n", "continue", "\n", "", "last_end", "=", "end", "\n", "\n", "context", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "feature_text", "[", "i_sent", "]", "[", ":", "end", "]", ")", "\n", "if", "readable_context", ":", "\n", "                        ", "if", "use_corpus", "==", "'wiki'", ":", "\n", "                            ", "context_proc", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ")", "\n", "", "elif", "use_corpus", "==", "'STS'", ":", "\n", "                            ", "context_proc", ",", "bad_context", "=", "preprocessing_context", "(", "context", ",", "outf", ",", "0", ",", "1000000", ")", "\n", "", "if", "bad_context", ":", "\n", "                            ", "gen_text", "=", "[", "''", "]", "*", "num_sent_gen", "\n", "temp", ".", "append", "(", "gen_text", ")", "\n", "continue", "\n", "\n", "#if len(context.split()) < 50:", "\n", "#    gen_text = ['']*num_sent_gen   ", "\n", "#    temp.append(gen_text)", "\n", "#    continue", "\n", "\n", "#try:", "\n", "#    context.replace('\u00e2\u0080\u0099',\"'\").encode('ascii', 'strict')", "\n", "#except:", "\n", "#    gen_text = ['']*num_sent_gen   ", "\n", "#    temp.append(gen_text)", "\n", "#    continue", "\n", "\n", "", "", "end_int", "=", "end", ".", "item", "(", ")", "\n", "max_prompt_len", "=", "bptt_conditional", "-", "gen_sent_len", "\n", "start_int", "=", "0", "\n", "if", "end_int", ">", "max_prompt_len", ":", "\n", "                        ", "start_int", "=", "end_int", "-", "max_prompt_len", "\n", "", "insert_loc_list", ".", "append", "(", "end_int", "-", "1", ")", "\n", "\n", "num_selection", "=", "random", ".", "randint", "(", "1", ",", "n_basis", ")", "\n", "selected_topic_idx", "=", "np", ".", "sort", "(", "np", ".", "random", ".", "choice", "(", "n_basis", ",", "size", "=", "num_selection", ",", "replace", "=", "False", ")", ")", "\n", "#num_selection = n_basis", "\n", "#selected_topic_idx = np.arange(n_basis)", "\n", "selected_topic_idx_arr", "[", "i_sent", "]", "[", "m", "]", "=", "selected_topic_idx", ".", "tolist", "(", ")", "\n", "# generate bag-of-words", "\n", "bag_of_words", "=", "[", "]", "\n", "for", "each", "in", "selected_topic_idx", ".", "tolist", "(", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "                            ", "word_nn", "=", "idx2word_freq", "[", "top_index", "[", "i_sent", ",", "m", ",", "k", ",", "each", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "bag_of_words", ".", "append", "(", "word_nn", ")", "\n", "\n", "", "", "t", "=", "time", ".", "time", "(", ")", "\n", "selected_topic_idx", "=", "torch", ".", "tensor", "(", "selected_topic_idx", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "feature_expanded", "=", "feature", "[", "i_sent", ",", "start_int", ":", "end", "]", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_sent_gen", ",", "end_int", "-", "start_int", ")", ".", "to", "(", "device", "=", "device_conditional", ")", "\n", "future_emb_chosen", "=", "word_w_sum_norm", "[", "i_sent", ",", "m", ",", "selected_topic_idx", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_sent_gen", ",", "num_selection", ",", "word_norm_emb", ".", "size", "(", "-", "1", ")", ")", "\n", "future_emb_chosen_arr", ".", "append", "(", "future_emb_chosen", ")", "\n", "insert_loc_truncated", "=", "np", ".", "array", "(", "insert_loc_list", ")", "-", "start_int", "\n", "#truncate_idx = 0", "\n", "#while( insert_loc_truncated[truncate_idx] < 0 ):", "\n", "#    truncate_idx += 1", "\n", "truncate_idx", "=", "-", "1", "\n", "output", "=", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ",", "future_emb_chosen_arr", "[", "truncate_idx", ":", "]", ",", "gen_sent_len", ",", "device_conditional", ",", ")", "\n", "condition_elapsed", "=", "time", ".", "time", "(", ")", "-", "t", "\n", "gen_sent_tensor", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", "=", "output", "\n", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "if", "model_org", "is", "None", ":", "\n", "                        ", "output_org", "=", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ",", "[", "torch", ".", "zeros", "(", "(", "num_sent_gen", ",", "0", ",", "word_norm_emb", ".", "size", "(", "-", "1", ")", ")", ",", "device", "=", "device_conditional", ")", "for", "x", "in", "range", "(", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ".", "size", ")", "]", ",", "gen_sent_len", ",", "device_conditional", ",", ")", "\n", "", "else", ":", "\n", "                        ", "output_org", "=", "sample_seq", "(", "model_org", ",", "feature_expanded", ",", "None", ",", "None", ",", "gen_sent_len", ",", "device_conditional", ",", ")", "\n", "", "org_elapsed", "=", "time", ".", "time", "(", ")", "-", "t", "\n", "gen_sent_tensor_org", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", "=", "output_org", "\n", "\n", "#gen_text = tokenizer_GPT2.decode(output)", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "context_pplm", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "feature_text", "[", "i_sent", "]", "[", "start_int", ":", "end", "]", ")", "\n", "try", ":", "\n", "                        ", "top_k_sampling", "=", "40", "\n", "#gen_text, _ = pplm_model.run_pplm_example(context_pplm, False, num_sent_gen, bag_of_words, gen_sent_len, 0.05, 1.0, top_k_sampling, True, 1, 10000, 1, 0, False, 1.5, 0.9, 0.01, True) #original", "\n", "gen_text", ",", "_", "=", "pplm_model", ".", "run_pplm_example", "(", "context_pplm", ",", "False", ",", "num_sent_gen", ",", "bag_of_words", ",", "gen_sent_len", ",", "0.03", ",", "1.0", ",", "top_k_sampling", ",", "True", ",", "3", ",", "10000", ",", "1", ",", "5", ",", "False", ",", "1.5", ",", "0.99", ",", "0.01", ",", "True", ")", "#default", "\n", "#gen_text, _ = pplm_model.run_pplm_example(context_pplm, False, num_sent_gen, bag_of_words, gen_sent_len, 0.03, 1.0, top_k_sampling, True, 3, 10000, 1, 5, False, 1.5, 0.9, 0.01, True)", "\n", "", "except", ":", "\n", "                        ", "print", "(", "\"Skipping {} batch, {} paragraph, and {} head because PPLM cannot condition on any word\"", ".", "format", "(", "i_batch", ",", "i_sent", ",", "m", ")", ")", "\n", "gen_text", "=", "[", "''", "]", "*", "num_sent_gen", "\n", "#gen_text = ['']*num_sent_gen", "\n", "#print(context)", "\n", "#print(num_sent_gen)", "\n", "#print(bag_of_words)", "\n", "#print(gen_sent_len)", "\n", "#sys.exit(1)", "\n", "#print(gen_text,perplexity)", "\n", "#gen_text, _ = pplm_model.run_pplm_example(context, False, num_sent_gen, bag_of_words, gen_sent_len, 0.05, 1.0, top_k, True, 1, 10000, 1, 0, False, 1.5, 0.9, 0.01, True)", "\n", "", "pplm_elapsed", "=", "time", ".", "time", "(", ")", "-", "t", "\n", "temp", ".", "append", "(", "gen_text", ")", "\n", "\n", "if", "run_eval", "and", "len", "(", "gen_text", "[", "0", "]", ")", ">", "0", ":", "\n", "#result_stats.model_results[\"time_count\"] += 1", "\n", "                        ", "for", "method_name", ",", "time_spent", "in", "[", "(", "\"Model condition\"", ",", "condition_elapsed", ")", ",", "(", "\"Original\"", ",", "org_elapsed", ")", ",", "(", "\"PPLM\"", ",", "pplm_elapsed", ")", "]", ":", "\n", "                            ", "result_stats", ".", "model_results", "[", "method_name", "]", "[", "\"time_sum\"", "]", "+=", "time_spent", "\n", "result_stats", ".", "model_results", "[", "method_name", "]", "[", "\"time_count\"", "]", "+=", "1", "\n", "\n", "", "", "", "pplm_sent", ".", "append", "(", "temp", ")", "\n", "", "print_basis_conditional_text", "(", "feature", ",", "pplm_sent", ",", "idx2word_freq", ",", "top_value", ",", "top_index", ",", "i_batch", ",", "outf", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ",", "gen_sent_tensor", ",", "gen_sent_tensor_org", ",", "selected_topic_idx_arr", ",", "result_stats", ",", "word_raw_list", ",", "word_raw_rest_list", ",", "csvOutf", ",", "readable_context", ",", "run_eval", ",", "type_idx_tensor", ",", "idx_l2_type", ")", "\n", "#result_stats.renew_ngram()", "\n", "if", "i_batch", "+", "1", ">=", "max_batch_num", ":", "\n", "                ", "break", "\n", "", "", "if", "run_eval", ":", "\n", "            ", "result_stats", ".", "print", "(", ")", "\n", "result_stats", ".", "generate_report", "(", "outf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_word_list_spacy": [[748, 808], ["inner_idx_tensor.size", "inner_idx_tensor.cpu().numpy", "enumerate", "tokenizer_GPT2.convert_tokens_to_string", "nlp.tokenizer", "range", "word_idx_list.append", "word_idx_rest_list.append", "word_raw_list.append", "word_raw_rest_list.append", "word_raw_list_i_j.append", "word_idx_list_i_j.append", "inner_idx_tensor.cpu", "utils_testing.get_word_list_spacy.get_word_list_from_text"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.tokenizer"], ["", "", "", "def", "get_word_list_spacy", "(", "inner_idx_tensor", ",", "feature_text", ",", "tokenizer_GPT2", ",", "nlp", ",", "word_d2_idx", ",", "stop_word_set", ",", "OOV_set", ")", ":", "\n", "    ", "def", "get_word_list_from_text", "(", "feature_text_i", ")", ":", "\n", "        ", "feature_text_i_str", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "feature_text_i", ")", "\n", "tokens", "=", "nlp", ".", "tokenizer", "(", "feature_text_i_str", ")", "\n", "word_idx_list_i_j", "=", "[", "]", "\n", "word_raw_list_i_j", "=", "[", "]", "\n", "#word_full_list_i_j = []", "\n", "\n", "for", "tok", "in", "tokens", ":", "\n", "            ", "w", "=", "tok", ".", "text", "\n", "word_raw_list_i_j", ".", "append", "(", "w", ")", "\n", "#word_full_list_i_j.append(w)", "\n", "#print(w, )", "\n", "if", "w", "not", "in", "word_d2_idx", ":", "\n", "                ", "continue", "\n", "", "w_idx", "=", "word_d2_idx", "[", "w", "]", "\n", "if", "w_idx", "in", "stop_word_set", "or", "w_idx", "in", "OOV_set", ":", "\n", "                ", "continue", "\n", "", "word_idx_list_i_j", ".", "append", "(", "w_idx", ")", "\n", "#word_raw_list_i_j.append(w)", "\n", "#return word_idx_list_i_j, word_raw_list_i_j, word_full_list_i_j", "\n", "", "return", "word_idx_list_i_j", ",", "word_raw_list_i_j", "\n", "", "word_idx_list", "=", "[", "]", "\n", "word_idx_rest_list", "=", "[", "]", "\n", "word_raw_list", "=", "[", "]", "\n", "#word_full_list = []", "\n", "word_raw_rest_list", "=", "[", "]", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "inner_idx_tensor_np", "=", "inner_idx_tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "b", ",", "feature_text_i", "in", "enumerate", "(", "feature_text", ")", ":", "\n", "        ", "word_idx_list_i", "=", "[", "]", "\n", "word_idx_rest_list_i", "=", "[", "]", "\n", "word_raw_list_i", "=", "[", "]", "\n", "#word_full_list_i = []", "\n", "word_raw_rest_list_i", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "end_idx", "=", "inner_idx_tensor_np", "[", "b", ",", "j", "]", "\n", "#word_idx_list_i_j, word_raw_list_i_j, word_full_list_i_j = get_word_list_from_text(feature_text_i[:end_idx])", "\n", "word_idx_list_i_j", ",", "word_raw_list_i_j", "=", "get_word_list_from_text", "(", "feature_text_i", "[", ":", "end_idx", "]", ")", "\n", "#assert len(word_idx_list_i_j) > 0, print(feature_text_i[:end_idx])", "\n", "word_idx_list_i", ".", "append", "(", "word_idx_list_i_j", ")", "\n", "word_raw_list_i", ".", "append", "(", "word_raw_list_i_j", ")", "\n", "#word_full_list_i.append(word_full_list_i_j)", "\n", "if", "end_idx", "==", "len", "(", "feature_text_i", ")", ":", "\n", "                ", "word_idx_rest_list_i", ".", "append", "(", "[", "]", ")", "\n", "word_raw_rest_list_i", ".", "append", "(", "[", "]", ")", "\n", "", "else", ":", "\n", "#word_idx_rest_list_i_j, word_raw_rest_list_i_j, word_full_rest_list_i_j = get_word_list_from_text(feature_text_i[end_idx:])", "\n", "                ", "word_idx_rest_list_i_j", ",", "word_raw_rest_list_i_j", "=", "get_word_list_from_text", "(", "feature_text_i", "[", "end_idx", ":", "]", ")", "\n", "word_idx_rest_list_i", ".", "append", "(", "word_idx_rest_list_i_j", ")", "\n", "word_raw_rest_list_i", ".", "append", "(", "word_raw_rest_list_i_j", ")", "\n", "#count = word_idx_d2_count.get(w_idx,0)", "\n", "#word_idx_d2_count[w_idx] += 1", "\n", "", "", "word_idx_list", ".", "append", "(", "word_idx_list_i", ")", "\n", "word_idx_rest_list", ".", "append", "(", "word_idx_rest_list_i", ")", "\n", "word_raw_list", ".", "append", "(", "word_raw_list_i", ")", "\n", "#word_full_list.append(word_full_list_i)", "\n", "word_raw_rest_list", ".", "append", "(", "word_raw_rest_list_i", ")", "\n", "#return word_idx_list, word_idx_rest_list, word_raw_list, word_raw_rest_list, word_full_list", "\n", "", "return", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "word_raw_rest_list", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_topic_emb": [[809, 822], ["basis_norm_pred.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.topk", "torch.topk", "torch.topk", "top_index.view.view", "top_value.view.view", "word_norm_emb.unsqueeze", "basis_norm_pred.permute", "torch.sum", "torch.sum", "torch.sum", "top_value.view.unsqueeze().sum", "word_norm_emb_w_sum.norm", "top_value.view.unsqueeze", "top_value.view.unsqueeze"], "function", ["None"], ["", "def", "get_topic_emb", "(", "basis_norm_pred", ",", "word_norm_emb", ",", "top_k", ",", "batch_size", ",", "num_head", ")", ":", "\n", "    ", "n_basis", "=", "basis_norm_pred", ".", "size", "(", "1", ")", "\n", "sim_pairwise", "=", "torch", ".", "matmul", "(", "word_norm_emb", ".", "unsqueeze", "(", "dim", "=", "0", ")", ",", "basis_norm_pred", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "top_value", ",", "top_index", "=", "torch", ".", "topk", "(", "sim_pairwise", ",", "top_k", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "# the index of each words in the vocab list", "\n", "top_index", "=", "top_index", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "\n", "# the value of each words", "\n", "top_value", "=", "top_value", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "\n", "\n", "word_norm_emb_top", "=", "word_norm_emb", "[", "top_index", ",", ":", "]", "\n", "word_norm_emb_w_sum", "=", "torch", ".", "sum", "(", "word_norm_emb_top", "*", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", ")", "/", "(", "0.000000000001", "+", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "dim", "=", "2", ")", ")", "\n", "word_w_sum_norm", "=", "word_norm_emb_w_sum", "/", "(", "0.000000000001", "+", "word_norm_emb_w_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.random_vocab_sampling": [[823, 831], ["numpy.empty", "range", "utils_testing.get_topic_emb", "range", "numpy.random.choice", "np.empty.reshape"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_topic_emb"], ["", "def", "random_vocab_sampling", "(", "could_sample_list", ",", "word_norm_emb", ",", "batch_size", ",", "num_head", ",", "n_basis", ",", "top_k", ")", ":", "\n", "    ", "idx_chosen", "=", "np", ".", "empty", "(", "[", "batch_size", ",", "num_head", ",", "n_basis", "]", ",", "dtype", "=", "int", ")", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "idx_chosen", "[", "b", ",", "j", ",", ":", "]", "=", "np", ".", "random", ".", "choice", "(", "could_sample_list", ",", "n_basis", ",", "replace", "=", "False", ")", "\n", "", "", "basis_norm_pred", "=", "word_norm_emb", "[", "idx_chosen", ".", "reshape", "(", "batch_size", "*", "num_head", ",", "n_basis", ")", ",", ":", "]", "\n", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "get_topic_emb", "(", "basis_norm_pred", ",", "word_norm_emb", ",", "top_k", ",", "batch_size", ",", "num_head", ")", "\n", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.random_word_sampling": [[832, 853], ["len", "len", "numpy.zeros", "range", "utils_testing.get_topic_emb", "range", "len", "len", "numpy.random.choice", "numpy.random.choice", "np.zeros.reshape"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_topic_emb"], ["", "def", "random_word_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ")", ":", "\n", "    ", "batch_size", "=", "len", "(", "word_idx_list", ")", "\n", "num_head", "=", "len", "(", "word_idx_list", "[", "0", "]", ")", "\n", "idx_chosen", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "num_head", ",", "n_basis", "]", ",", "dtype", "=", "int", ")", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "#end_idx = inner_idx_tensor_np[b,j]", "\n", "            ", "if", "len", "(", "word_idx_list", "[", "b", "]", "[", "j", "]", ")", "<=", "1", ":", "\n", "                ", "continue", "\n", "", "if", "len", "(", "word_idx_list", "[", "b", "]", "[", "j", "]", ")", "<", "n_basis", ":", "\n", "                ", "idx_chosen", "[", "b", ",", "j", ",", ":", "]", "=", "np", ".", "random", ".", "choice", "(", "word_idx_list", "[", "b", "]", "[", "j", "]", ",", "n_basis", ",", "replace", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "idx_chosen", "[", "b", ",", "j", ",", ":", "]", "=", "np", ".", "random", ".", "choice", "(", "word_idx_list", "[", "b", "]", "[", "j", "]", ",", "n_basis", ",", "replace", "=", "False", ")", "\n", "\n", "#idx_chosen = []", "\n", "#for word_idx_list_i in word_idx_list:", "\n", "#    idx_chosen_i = np.random.choice(word_idx_list_i, n_basis, replace=True)", "\n", "#    idx_chosen.append(idx_chosen_i)", "\n", "", "", "", "basis_norm_pred", "=", "word_norm_emb", "[", "idx_chosen", ".", "reshape", "(", "batch_size", "*", "num_head", ",", "n_basis", ")", ",", ":", "]", "\n", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "get_topic_emb", "(", "basis_norm_pred", ",", "word_norm_emb", ",", "top_k", ",", "batch_size", ",", "num_head", ")", "\n", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.SC_clustering": [[854, 876], ["torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "model.SparseCoding", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.optim.RMSprop", "torch.optim.RMSprop", "torch.optim.RMSprop", "range", "cluster_feature.size", "cluster_feature.size", "model.SparseCoding.parameters", "torch.optim.RMSprop.zero_grad", "model.SparseCoding.", "loss.backward", "torch.optim.RMSprop.step", "model.SparseCoding.compute_coeff_pos", "torch.nn.MSELoss.", "model.SparseCoding.coeff.abs().sum", "model.SparseCoding.coeff.abs"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.step", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.MatrixReconstruction.compute_coeff_pos"], ["", "def", "SC_clustering", "(", "cluster_feature", ",", "n_basis", ",", "max_iter", ")", ":", "\n", "    ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "        ", "lr", "=", "0.1", "\n", "L1_losss_B", "=", "0.2", "\n", "SC", "=", "SparseCoding", "(", "n_basis", ",", "cluster_feature", ".", "size", "(", "0", ")", ",", "cluster_feature", ".", "size", "(", "1", ")", ",", "device", "=", "cluster_feature", ".", "device", ")", "\n", "loss_func", "=", "torch", ".", "nn", ".", "MSELoss", "(", "reduction", "=", "'sum'", ")", "\n", "opt", "=", "torch", ".", "optim", ".", "RMSprop", "(", "SC", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "alpha", "=", "0.99", ",", "eps", "=", "1e-08", ",", "weight_decay", "=", "0", ",", "momentum", "=", "0", ",", "centered", "=", "False", ")", "\n", "#print(\"\\nloss: \")", "\n", "for", "i", "in", "range", "(", "max_iter", ")", ":", "\n", "            ", "opt", ".", "zero_grad", "(", ")", "\n", "pred", "=", "SC", "(", ")", "\n", "loss", "=", "loss_func", "(", "pred", ",", "cluster_feature", ")", "/", "2", "\n", "#print(\"{:5.2f}\".format(loss.item()), end =\" \") ", "\n", "# loss += L1_losss_B * mr.coeff.abs().sum()", "\n", "#loss += L1_losss_B * (mr.coeff.abs().sum() + mr.coeff.diagonal(dim1=1, dim2=2).abs().sum())", "\n", "loss", "+=", "L1_losss_B", "*", "SC", ".", "coeff", ".", "abs", "(", ")", ".", "sum", "(", ")", "\n", "# print('loss:', loss.item())", "\n", "loss", ".", "backward", "(", ")", "\n", "opt", ".", "step", "(", ")", "\n", "SC", ".", "compute_coeff_pos", "(", ")", "\n", "\n", "", "return", "SC", ".", "code_book", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.cluster_sampling": [[877, 916], ["len", "len", "word_norm_emb.size", "torch.zeros", "torch.zeros", "torch.zeros", "range", "basis_pred.view.view", "utils_testing.get_topic_emb", "sklearn.cluster.KMeans", "range", "torch.isnan().sum", "torch.isnan().sum", "torch.isnan().sum", "print", "print", "print", "print", "sys.exit", "basis_pred.view.norm", "len", "len", "numpy.random.choice().tolist", "word_norm_emb[].cpu().numpy", "torch.isnan", "torch.isnan", "torch.isnan", "sklearn.cluster.KMeans.fit", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.random.choice", "word_norm_emb[].cpu", "utils_testing.SC_clustering", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_topic_emb", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.SC_clustering"], ["", "", "def", "cluster_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "cluster_method", "=", "\"KMeans\"", ")", ":", "\n", "    ", "batch_size", "=", "len", "(", "word_idx_list", ")", "\n", "num_head", "=", "len", "(", "word_idx_list", "[", "0", "]", ")", "\n", "device_topic", "=", "word_norm_emb", ".", "device", "\n", "if", "cluster_method", "==", "\"KMeans\"", ":", "\n", "        ", "kmeans_model", "=", "KMeans", "(", "n_clusters", "=", "n_basis", ",", "n_init", "=", "1", ",", "random_state", "=", "0", ",", "init", "=", "'random'", ")", "\n", "", "emb_size", "=", "word_norm_emb", ".", "size", "(", "-", "1", ")", "\n", "basis_pred", "=", "torch", ".", "zeros", "(", "[", "batch_size", ",", "num_head", ",", "n_basis", ",", "emb_size", "]", ",", "device", "=", "device_topic", ")", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "word_index_context", "=", "word_idx_list", "[", "b", "]", "[", "j", "]", "\n", "if", "len", "(", "word_index_context", ")", "<=", "1", ":", "\n", "                ", "continue", "\n", "", "if", "len", "(", "word_index_context", ")", "<", "n_basis", ":", "\n", "                ", "word_index_context", "+=", "np", ".", "random", ".", "choice", "(", "word_index_context", ",", "n_basis", "-", "len", "(", "word_index_context", ")", ",", "replace", "=", "True", ")", ".", "tolist", "(", ")", "\n", "basis_pred", "[", "b", ",", "j", ",", ":", ",", ":", "]", "=", "word_norm_emb", "[", "word_index_context", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "cluster_feature", "=", "word_norm_emb", "[", "word_index_context", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "cluster_method", "==", "\"KMeans\"", ":", "\n", "                    ", "kmeans_model", ".", "fit", "(", "cluster_feature", ")", "\n", "basis_pred", "[", "b", ",", "j", ",", ":", ",", ":", "]", "=", "torch", ".", "tensor", "(", "kmeans_model", ".", "cluster_centers_", ",", "device", "=", "device_topic", ")", "\n", "", "elif", "cluster_method", "==", "\"Sparse_coding\"", ":", "\n", "#print(cluster_feature)", "\n", "                    ", "basis_pred", "[", "b", ",", "j", ",", ":", ",", ":", "]", "=", "SC_clustering", "(", "torch", ".", "tensor", "(", "cluster_feature", ",", "device", "=", "device_topic", ")", ",", "n_basis", ",", "max_iter", "=", "2000", ")", "\n", "", "", "", "", "basis_pred", "=", "basis_pred", ".", "view", "(", "batch_size", "*", "num_head", ",", "n_basis", ",", "emb_size", ")", "\n", "basis_norm_pred", "=", "basis_pred", "/", "(", "0.000000000001", "+", "basis_pred", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ")", "\n", "#if torch.isnan(basis_norm_pred).sum() > 0:", "\n", "#    print(basis_norm_pred)", "\n", "#    print(word_index_context)", "\n", "#    print(cluster_feature)", "\n", "#    sys.exit(0)", "\n", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "get_topic_emb", "(", "basis_norm_pred", ",", "word_norm_emb", ",", "top_k", ",", "batch_size", ",", "num_head", ")", "\n", "if", "torch", ".", "isnan", "(", "word_w_sum_norm", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "        ", "print", "(", "basis_norm_pred", ")", "\n", "print", "(", "word_index_context", ")", "\n", "print", "(", "cluster_feature", ")", "\n", "print", "(", "word_w_sum_norm", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_lda_model": [[917, 953], ["gensim.models.ldamodel.LdaModel.load", "ldamodel.LdaModel.load.get_topics", "word_norm_emb.size", "numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.topk", "torch.topk", "torch.topk", "torch.sum", "torch.sum", "torch.sum", "torch.matmul", "torch.matmul", "torch.matmul", "torch.topk", "torch.topk", "torch.topk", "top_index.permute", "top_value.permute", "lda_fixed_topic_emb.permute", "torch.sum", "torch.sum", "torch.sum", "top_value.permute.unsqueeze().sum", "top_value.permute.unsqueeze", "torch.sum.norm", "word_norm_emb_w_sum.norm", "top_value.permute.unsqueeze", "top_value.permute.unsqueeze"], "function", ["None"], ["", "def", "load_lda_model", "(", "LDA_model_path", ",", "word_d2_idx", ",", "word_norm_emb", ",", "top_k", ")", ":", "\n", "    ", "lda_model", "=", "ldamodel", ".", "LdaModel", ".", "load", "(", "LDA_model_path", ",", "mmap", "=", "'r'", ")", "\n", "lda_word_prob_raw", "=", "lda_model", ".", "get_topics", "(", ")", "\n", "lda_id2word", "=", "lda_model", ".", "id2word", "\n", "#id_lda_d2_id_dict = {}", "\n", "word_num", ",", "emb_size", "=", "word_norm_emb", ".", "size", "(", ")", "\n", "lda_topic_num", ",", "num_vocab_lda", "=", "lda_word_prob_raw", ".", "shape", "\n", "lda_word_prob", "=", "np", ".", "zeros", "(", "(", "lda_topic_num", ",", "word_num", ")", ")", "\n", "for", "lda_id", "in", "lda_id2word", ":", "\n", "        ", "word", "=", "lda_id2word", "[", "lda_id", "]", "\n", "if", "word", "not", "in", "word_d2_idx", ":", "\n", "            ", "continue", "\n", "", "id_dict", "=", "word_d2_idx", "[", "word", "]", "\n", "lda_word_prob", "[", ":", ",", "id_dict", "]", "=", "lda_word_prob_raw", "[", ":", ",", "lda_id", "]", "\n", "#id_lda_d2_id_dict[lda_id] = id_dict", "\n", "", "lda_word_prob_tensor", "=", "torch", ".", "tensor", "(", "lda_word_prob", ",", "device", "=", "word_norm_emb", ".", "device", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "lda_top_word_value", ",", "lda_top_word_index", "=", "torch", ".", "topk", "(", "lda_word_prob_tensor", ",", "top_k", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "top_word_emb", "=", "word_norm_emb", "[", "lda_top_word_index", ",", ":", "]", "\n", "#top_word_emb should have size (lda_topic_num, top_k, emb_size)", "\n", "top_word_emb_sum", "=", "torch", ".", "sum", "(", "top_word_emb", "*", "lda_top_word_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "\n", "lda_fixed_topic_emb", "=", "top_word_emb_sum", "/", "(", "0.000000000001", "+", "top_word_emb_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "#lda_top_word_value, lda_top_word_index, lda_fixed_topic_emb = get_topic_emb(lda_fixed_topic_emb, word_norm_emb, top_k, batch_size, num_head)", "\n", "\n", "sim_pairwise", "=", "torch", ".", "matmul", "(", "word_norm_emb", ",", "lda_fixed_topic_emb", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "top_value", ",", "top_index", "=", "torch", ".", "topk", "(", "sim_pairwise", ",", "top_k", ",", "dim", "=", "0", ",", "sorted", "=", "True", ")", "\n", "# the index of each words in the vocab list", "\n", "lda_top_word_index", "=", "top_index", ".", "permute", "(", "1", ",", "0", ")", "\n", "# the value of each words", "\n", "lda_top_word_value", "=", "top_value", ".", "permute", "(", "1", ",", "0", ")", "\n", "\n", "word_norm_emb_top", "=", "word_norm_emb", "[", "lda_top_word_index", ",", ":", "]", "\n", "word_norm_emb_w_sum", "=", "torch", ".", "sum", "(", "word_norm_emb_top", "*", "lda_top_word_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "/", "lda_top_word_value", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "lda_fixed_topic_emb", "=", "word_norm_emb_w_sum", "/", "(", "0.000000000001", "+", "word_norm_emb_w_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "return", "lda_word_prob_tensor", ",", "lda_top_word_value", ",", "lda_top_word_index", ",", "lda_fixed_topic_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_global_centers": [[954, 964], ["numpy.loadtxt", "torch.tensor", "torch.tensor", "torch.tensor", "torch.mm", "torch.mm", "torch.mm", "torch.topk", "torch.topk", "torch.topk", "word_norm_emb.permute", "torch.sum", "torch.sum", "torch.sum", "w_emb_top_word_value.unsqueeze().sum", "torch.tensor.norm", "word_norm_emb_w_sum.norm", "w_emb_top_word_value.unsqueeze", "w_emb_top_word_value.unsqueeze"], "function", ["None"], ["", "def", "load_global_centers", "(", "word_emb_center_path", ",", "word_d2_idx", ",", "word_norm_emb", ",", "top_k", ")", ":", "\n", "    ", "word_emb_centers", "=", "np", ".", "loadtxt", "(", "word_emb_center_path", ")", "\n", "word_emb_centers", "=", "torch", ".", "tensor", "(", "word_emb_centers", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "word_norm_emb", ".", "device", ")", "\n", "word_norm_emb_centers", "=", "word_emb_centers", "/", "(", "0.000000000001", "+", "word_emb_centers", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "sim_pairwise", "=", "torch", ".", "mm", "(", "word_norm_emb_centers", ",", "word_norm_emb", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "w_emb_top_word_value", ",", "w_emb_top_word_index", "=", "torch", ".", "topk", "(", "sim_pairwise", ",", "top_k", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "word_norm_emb_top", "=", "word_norm_emb", "[", "w_emb_top_word_index", ",", ":", "]", "\n", "word_norm_emb_w_sum", "=", "torch", ".", "sum", "(", "word_norm_emb_top", "*", "w_emb_top_word_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "/", "w_emb_top_word_value", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "word_norm_emb_w_sum_global", "=", "word_norm_emb_w_sum", "/", "(", "0.000000000001", "+", "word_norm_emb_w_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "return", "w_emb_top_word_value", ",", "w_emb_top_word_index", ",", "word_norm_emb_w_sum_global", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.compose_context_emb": [[966, 980], ["len", "len", "word_norm_emb.size", "torch.zeros", "torch.zeros", "torch.zeros", "range", "range", "word_norm_emb[].sum", "torch.zeros.norm", "len"], "function", ["None"], ["", "def", "compose_context_emb", "(", "word_idx_list", ",", "word_norm_emb", ")", ":", "\n", "    ", "batch_size", "=", "len", "(", "word_idx_list", ")", "\n", "num_head", "=", "len", "(", "word_idx_list", "[", "0", "]", ")", "\n", "emb_size", "=", "word_norm_emb", ".", "size", "(", "1", ")", "\n", "context_emb", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "num_head", ",", "emb_size", ")", ",", "device", "=", "word_norm_emb", ".", "device", ")", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "word_index_context", "=", "word_idx_list", "[", "b", "]", "[", "j", "]", "\n", "if", "len", "(", "word_index_context", ")", "<=", "1", ":", "\n", "                ", "continue", "\n", "", "context_emb", "[", "b", ",", "j", ",", ":", "]", "=", "word_norm_emb", "[", "word_index_context", ",", ":", "]", ".", "sum", "(", "dim", "=", "0", ")", "\n", "", "", "context_norm_emb", "=", "context_emb", "/", "(", "0.000000000001", "+", "context_emb", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "return", "context_norm_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_fixed_lda_topics": [[981, 990], ["context_norm_emb.size", "torch.mm", "torch.mm", "torch.mm", "torch.topk", "torch.topk", "torch.topk", "selected_topic_idx.view.view", "lda_top_word_value[].permute", "lda_top_word_index[].permute", "context_norm_emb.view", "lda_fixed_topic_emb.permute"], "function", ["None"], ["", "def", "select_fixed_lda_topics", "(", "context_norm_emb", ",", "lda_top_word_value", ",", "lda_top_word_index", ",", "lda_fixed_topic_emb", ",", "n_basis", ")", ":", "\n", "    ", "batch_size", ",", "num_head", ",", "emb_size", "=", "context_norm_emb", ".", "size", "(", ")", "\n", "sim_pairwise", "=", "torch", ".", "mm", "(", "context_norm_emb", ".", "view", "(", "batch_size", "*", "num_head", ",", "emb_size", ")", ",", "lda_fixed_topic_emb", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "_", ",", "selected_topic_idx", "=", "torch", ".", "topk", "(", "sim_pairwise", ",", "n_basis", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "selected_topic_idx", "=", "selected_topic_idx", ".", "view", "(", "batch_size", ",", "num_head", ",", "n_basis", ")", "\n", "top_value", "=", "lda_top_word_value", "[", "selected_topic_idx", ",", ":", "]", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "\n", "top_index", "=", "lda_top_word_index", "[", "selected_topic_idx", ",", ":", "]", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "\n", "word_w_sum_norm", "=", "lda_fixed_topic_emb", "[", "selected_topic_idx", ",", ":", "]", "\n", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_dynamic_lda_topics": [[991, 1020], ["word_norm_emb.size", "context_norm_emb.size", "torch.mm", "torch.mm", "torch.mm", "sim_pairwise.view.view", "torch.zeros", "torch.zeros", "torch.zeros", "range", "gc.collect", "utils_testing.get_topic_emb", "context_norm_emb.view", "word_norm_emb.permute", "range", "torch.zeros.view", "torch.topk", "torch.topk", "torch.topk", "torch.sum", "torch.sum", "torch.sum", "torch.mm", "torch.mm", "torch.mm", "torch.topk", "torch.topk", "torch.topk", "sim_pairwise[].unsqueeze", "context_norm_emb[].unsqueeze", "lda_dynamic_topic_emb.permute", "torch.mm.squeeze", "lda_top_word_value.unsqueeze", "torch.sum.norm"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_topic_emb"], ["", "def", "select_dynamic_lda_topics", "(", "context_norm_emb", ",", "word_norm_emb", ",", "lda_word_prob_tensor", ",", "n_basis", ",", "top_k", ")", ":", "\n", "    ", "num_words", "=", "word_norm_emb", ".", "size", "(", "0", ")", "\n", "batch_size", ",", "num_head", ",", "emb_size", "=", "context_norm_emb", ".", "size", "(", ")", "\n", "sim_pairwise", "=", "torch", ".", "mm", "(", "context_norm_emb", ".", "view", "(", "batch_size", "*", "num_head", ",", "emb_size", ")", ",", "word_norm_emb", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "#sim_pairwise should have size (batch_size*num_head, num_words)", "\n", "sim_pairwise", "=", "sim_pairwise", ".", "view", "(", "batch_size", ",", "num_head", ",", "num_words", ")", "\n", "#lda_word_prob_tensor should have size (lda_topic_num, num_words)", "\n", "#top_value = torch.zeros((batch_size, num_head, top_k, n_basis), device = word_norm_emb.device)", "\n", "#top_index = torch.zeros((batch_size, num_head, top_k, n_basis), dtype=torch.long, device = word_norm_emb.device)", "\n", "word_w_sum_norm", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "num_head", ",", "n_basis", ",", "emb_size", ")", ",", "device", "=", "word_norm_emb", ".", "device", ")", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "            ", "lda_word_prob_local", "=", "lda_word_prob_tensor", "*", "sim_pairwise", "[", "b", ",", "j", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", "\n", "lda_top_word_value", ",", "lda_top_word_index", "=", "torch", ".", "topk", "(", "lda_word_prob_local", ",", "top_k", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "top_word_emb", "=", "word_norm_emb", "[", "lda_top_word_index", ",", ":", "]", "\n", "top_word_emb_sum", "=", "torch", ".", "sum", "(", "top_word_emb", "*", "lda_top_word_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "\n", "lda_dynamic_topic_emb", "=", "top_word_emb_sum", "/", "(", "0.000000000001", "+", "top_word_emb_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "sim_pairwise_bj", "=", "torch", ".", "mm", "(", "context_norm_emb", "[", "b", ",", "j", ",", ":", "]", ".", "unsqueeze", "(", "dim", "=", "0", ")", ",", "lda_dynamic_topic_emb", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "_", ",", "selected_topic_idx", "=", "torch", ".", "topk", "(", "sim_pairwise_bj", ".", "squeeze", "(", "dim", "=", "0", ")", ",", "n_basis", ",", "dim", "=", "0", ",", "sorted", "=", "True", ")", "\n", "#print(lda_top_word_value)", "\n", "#print(selected_topic_idx)", "\n", "#top_value[b,j,:,:] = lda_top_word_value[selected_topic_idx,:].permute(1,0)", "\n", "#top_index[b,j,:,:] = lda_top_word_index[selected_topic_idx,:].permute(1,0)", "\n", "word_w_sum_norm", "[", "b", ",", "j", ",", ":", ",", ":", "]", "=", "lda_dynamic_topic_emb", "[", "selected_topic_idx", ",", ":", "]", "\n", "", "", "gc", ".", "collect", "(", ")", "\n", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "get_topic_emb", "(", "word_w_sum_norm", ".", "view", "(", "batch_size", "*", "num_head", ",", "n_basis", ",", "emb_size", ")", ",", "word_norm_emb", ",", "top_k", ",", "batch_size", ",", "num_head", ")", "\n", "\n", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.NSD_prediction": [[1021, 1032], ["utils_testing.predict_batch", "top_index.view.view", "top_value.view.view", "torch.sum", "torch.sum", "torch.sum", "top_value.view.unsqueeze().sum", "word_norm_emb_w_sum.norm", "top_value.view.unsqueeze", "top_value.view.unsqueeze"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.predict_batch"], ["", "def", "NSD_prediction", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ",", "batch_size", ",", "num_head", ")", ":", "\n", "    ", "basis_norm_pred", ",", "top_value", ",", "top_index", "=", "predict_batch", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ")", "\n", "# the index of each words in the vocab list", "\n", "top_index", "=", "top_index", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "\n", "# the value of each words", "\n", "top_value", "=", "top_value", ".", "view", "(", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", ")", "\n", "\n", "word_norm_emb_top", "=", "word_norm_emb", "[", "top_index", ",", ":", "]", "\n", "word_norm_emb_w_sum", "=", "torch", ".", "sum", "(", "word_norm_emb_top", "*", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", ")", "/", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "word_w_sum_norm", "=", "word_norm_emb_w_sum", "/", "(", "0.000000000001", "+", "word_norm_emb_w_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "return", "top_value", ",", "top_index", ",", "word_w_sum_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.testing_all_topic_baselines": [[1034, 1165], ["torch.sum", "torch.sum", "torch.sum", "torch.nonzero().squeeze().cpu().tolist", "torch.nonzero().squeeze().cpu().tolist", "torch.nonzero().squeeze().cpu().tolist", "print", "print", "set", "list", "spacy.lang.en.English", "range", "utils_testing.load_lda_model", "utils_testing.load_global_centers", "torch.no_grad", "torch.no_grad", "torch.no_grad", "topic_result_statistics.topic_result_statistics", "topic_models.split", "enumerate", "topic_result_statistics.topic_result_statistics.generate_report", "topic_result_statistics.topic_result_statistics.generate_report", "torch.nonzero().squeeze().cpu", "torch.nonzero().squeeze().cpu", "torch.nonzero().squeeze().cpu", "len", "set", "len", "topic_result_statistics.topic_result_statistics.add_model", "print", "sys.stdout.flush", "inner_idx_tensor.size", "inner_idx_tensor.cpu().numpy", "inner_idx_tensor.size", "utils_testing.print_basis_text", "list", "utils_testing.get_word_list_spacy", "utils_testing.random_vocab_sampling", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.random_word_sampling", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.cluster_sampling", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.cluster_sampling", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.compose_context_emb", "utils_testing.select_fixed_lda_topics", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.select_dynamic_lda_topics", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.select_fixed_lda_topics", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "utils_testing.NSD_prediction", "topic_result_statistics.topic_result_statistics.evaluate_topic_models", "method_name_arr.append", "top_value_arr.append", "top_index_arr.append", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "range", "str", "tokenizer_GPT2._convert_id_to_token", "range", "inner_idx_tensor.cpu", "len", "feature[].tolist", "feature.size", "torch.nonzero", "torch.nonzero", "torch.nonzero"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_lda_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_global_centers", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.generate_report", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.generate_report", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.add_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_basis_text", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_word_list_spacy", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.random_vocab_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.random_word_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.cluster_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.cluster_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.compose_context_emb", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_fixed_lda_topics", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_dynamic_lda_topics", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_fixed_lda_topics", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.NSD_prediction", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token"], ["", "def", "testing_all_topic_baselines", "(", "dataloader", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "idx2word_freq", ",", "outf", ",", "n_basis", ",", "max_batch_num", ",", "tokenizer_GPT2", ",", "stop_word_set", ",", "topic_models", ",", "de_en_connection", ",", "LDA_model_path", ",", "word_emb_center_path", ",", "readable_context", ")", ":", "\n", "    ", "top_k", "=", "5", "\n", "emb_sum", "=", "torch", ".", "sum", "(", "word_norm_emb", ",", "dim", "=", "1", ")", "\n", "OOV_list", "=", "torch", ".", "nonzero", "(", "emb_sum", "==", "0", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "print", "(", "\"OOV number = {}\"", ".", "format", "(", "len", "(", "OOV_list", ")", ")", ")", "\n", "print", "(", "\"OOV index examples {}\"", ".", "format", "(", "OOV_list", "[", ":", "10", "]", ")", ")", "\n", "OOV_set", "=", "set", "(", "OOV_list", ")", "\n", "could_sample_list", "=", "list", "(", "set", "(", "list", "(", "range", "(", "len", "(", "idx2word_freq", ")", ")", ")", ")", "-", "(", "OOV_set", "|", "stop_word_set", ")", ")", "\n", "if", "topic_models", "!=", "'random_vocab'", ":", "\n", "        ", "nlp", "=", "English", "(", ")", "\n", "word_d2_idx", "=", "{", "}", "\n", "for", "idx", "in", "range", "(", "len", "(", "idx2word_freq", ")", ")", ":", "\n", "            ", "word", "=", "idx2word_freq", "[", "idx", "]", "[", "0", "]", "\n", "word_d2_idx", "[", "word", "]", "=", "idx", "\n", "", "", "if", "'LDA'", "in", "topic_models", ":", "\n", "        ", "lda_word_prob_tensor", ",", "lda_top_word_value", ",", "lda_top_word_index", ",", "lda_fixed_topic_emb", "=", "load_lda_model", "(", "LDA_model_path", ",", "word_d2_idx", ",", "word_norm_emb", ",", "top_k", ")", "\n", "", "if", "'global_centers'", "in", "topic_models", ":", "\n", "        ", "w_emb_top_word_value", ",", "w_emb_top_word_index", ",", "word_norm_emb_w_sum_global", "=", "load_global_centers", "(", "word_emb_center_path", ",", "word_d2_idx", ",", "word_norm_emb", ",", "top_k", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "topic_result_stats", "=", "topic_result_statistics", "(", ")", "\n", "for", "t_model", "in", "topic_models", ".", "split", "(", "'+'", ")", ":", "\n", "            ", "topic_result_stats", ".", "add_model", "(", "t_model", ")", "\n", "#topic_result_stats.add_model(\"Model condition\")", "\n", "#result_stats.add_model(\"Original\")", "\n", "#topic_result_stats.add_model(\"PPLM\")", "\n", "\n", "", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "# if i_batch == 0:", "\n", "#     continue", "\n", "            ", "if", "i_batch", ">=", "max_batch_num", ":", "\n", "                ", "break", "\n", "", "print", "(", "\"batch\"", "+", "str", "(", "i_batch", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "=", "sample_batched", "\n", "feature_text", "=", "[", "[", "tokenizer_GPT2", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "feature", "[", "i", ",", ":", "]", ".", "tolist", "(", ")", "]", "for", "i", "in", "range", "(", "feature", ".", "size", "(", "0", ")", ")", "]", "\n", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "inner_idx_tensor_np", "=", "inner_idx_tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#word_raw_org_list = []", "\n", "#for b, feature_text_i in enumerate(feature_text):", "\n", "#    word_raw_org_list_i = []", "\n", "#    for j in range(num_head):", "\n", "#        end_idx = inner_idx_tensor_np[b,j]", "\n", "#        word_raw_org_list_i.append( tokenizer_GPT2.convert_tokens_to_string(feature_text_i[:end_idx]) )", "\n", "#    word_raw_org_list.append(word_raw_org_list_i)", "\n", "\n", "#context = tokenizer_GPT2.decode(feature[i_sent,:end]).replace('\u00e2\u0080\u0099',\"'\").replace('\\n',\" \")", "\n", "#if len(context.split()) < 50:", "\n", "#    #outf.write(\"Skip {} sent {} head due to short context\\n\".format(i_sent, m))", "\n", "#    continue", "\n", "#try:", "\n", "#    context.encode('ascii', 'strict')", "\n", "#except:", "\n", "#    #outf.write(\"Skip {} sent {} head due to special token\\n\".format(i_sent, m))", "\n", "#    continue", "\n", "#tokenized_feature = [nlp(feature_text_i).text for feature_text_i in feature_text]", "\n", "#word_idx_d2_count = {}", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "\n", "top_value_arr", "=", "[", "]", "\n", "top_index_arr", "=", "[", "]", "\n", "method_name_arr", "=", "[", "]", "\n", "if", "topic_models", "!=", "'random_vocab'", ":", "\n", "#word_idx_list, word_idx_rest_list, word_raw_list, word_raw_rest_list, word_full_list = get_word_list_spacy(inner_idx_tensor, feature_text, tokenizer_GPT2, nlp, word_d2_idx, stop_word_set, OOV_set)", "\n", "                ", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "word_raw_rest_list", "=", "get_word_list_spacy", "(", "inner_idx_tensor", ",", "feature_text", ",", "tokenizer_GPT2", ",", "nlp", ",", "word_d2_idx", ",", "stop_word_set", ",", "OOV_set", ")", "\n", "\n", "", "if", "'random_vocab'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "random_vocab_sampling", "(", "could_sample_list", ",", "word_norm_emb", ",", "batch_size", ",", "num_head", ",", "n_basis", ",", "top_k", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"random_vocab\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"random_vocab\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'random_vocab'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'random_word'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "random_word_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"random_word\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"random_word\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'random_word'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'SC_cluster'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "cluster_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "cluster_method", "=", "\"Sparse_coding\"", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"SC_cluster\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"SC_cluster\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'SC_cluster'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'kmeans_cluster'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "cluster_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "cluster_method", "=", "\"KMeans\"", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"kmeans_cluster\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"kmeans_cluster\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'kmeans_cluster'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'LDA'", "in", "topic_models", "or", "'global_centers'", "in", "topic_models", ":", "\n", "                ", "context_norm_emb", "=", "compose_context_emb", "(", "word_idx_list", ",", "word_norm_emb", ")", "\n", "\n", "", "if", "'LDA_org'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "select_fixed_lda_topics", "(", "context_norm_emb", ",", "lda_top_word_value", ",", "lda_top_word_index", ",", "lda_fixed_topic_emb", ",", "n_basis", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"LDA_org\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"LDA_org\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'LDA_org'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'LDA_plus'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "select_dynamic_lda_topics", "(", "context_norm_emb", ",", "word_norm_emb", ",", "lda_word_prob_tensor", ",", "n_basis", ",", "top_k", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"LDA_plus\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"LDA_plus\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'LDA_plus'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'global_centers'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "select_fixed_lda_topics", "(", "context_norm_emb", ",", "w_emb_top_word_value", ",", "w_emb_top_word_index", ",", "word_norm_emb_w_sum_global", ",", "n_basis", ")", "\n", "#topic_result_stats.evaluate_topic_models(\"global_centers\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"global_centers\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'global_centers'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "if", "'NSD'", "in", "topic_models", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "NSD_prediction", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ",", "batch_size", ",", "num_head", ")", "\n", "#basis_norm_pred, top_value, top_index = predict_batch(feature, inner_idx_tensor, future_mask, parallel_encoder, parallel_decoder, word_norm_emb, n_basis, top_k, de_en_connection)", "\n", "## the index of each words in the vocab list", "\n", "#top_index = top_index.view(batch_size, num_head, top_k, n_basis)", "\n", "## the value of each words", "\n", "#top_value = top_value.view(batch_size, num_head, top_k, n_basis)", "\n", "\n", "#word_norm_emb_top = word_norm_emb[top_index,:]", "\n", "#word_norm_emb_w_sum = torch.sum( word_norm_emb_top * top_value.unsqueeze(-1), dim = 2)/ top_value.unsqueeze(-1).sum(dim = 2)", "\n", "#word_w_sum_norm = word_norm_emb_w_sum / (0.000000000001 + word_norm_emb_w_sum.norm(dim = -1, keepdim=True))", "\n", "#topic_result_stats.evaluate_topic_models(\"NSD\", top_value, top_index, word_w_sum_norm, word_idx_list, word_idx_rest_list, word_full_list, idx2word_freq, word_norm_emb)", "\n", "topic_result_stats", ".", "evaluate_topic_models", "(", "\"NSD\"", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", "\n", "method_name_arr", ".", "append", "(", "'NSD'", ")", ";", "top_value_arr", ".", "append", "(", "top_value", ")", ";", "top_index_arr", ".", "append", "(", "top_index", ")", "\n", "\n", "", "print_basis_text", "(", "feature", ",", "idx2word_freq", ",", "top_value_arr", ",", "top_index_arr", ",", "method_name_arr", ",", "i_batch", ",", "outf", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ",", "readable_context", ")", "\n", "\n", "", "topic_result_stats", ".", "generate_report", "(", "outf", ")", "\n", "topic_result_stats", ".", "generate_report", "(", "sys", ".", "stdout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.testing_topic_baseline": [[1167, 1337], ["spacy.lang.en.English", "range", "torch.sum", "torch.sum", "torch.sum", "torch.nonzero().squeeze().cpu().tolist", "torch.nonzero().squeeze().cpu().tolist", "torch.nonzero().squeeze().cpu().tolist", "print", "print", "set", "len", "list", "utils_testing.load_lda_model", "utils_testing.load_global_centers", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "torch.nonzero().squeeze().cpu", "torch.nonzero().squeeze().cpu", "torch.nonzero().squeeze().cpu", "len", "result_statistics.result_statistics", "result_statistics.result_statistics.add_model", "print", "sys.stdout.flush", "inner_idx_tensor.unsqueeze.size", "utils_testing.get_word_list_spacy", "word_w_sum_norm.to.to", "torch.empty", "torch.empty", "torch.empty", "torch.zeros", "torch.zeros", "torch.zeros", "range", "utils_testing.eval_basis_conditional_text_single", "result_statistics.result_statistics.print", "result_statistics.result_statistics.generate_report", "set", "inner_idx_tensor.unsqueeze.unsqueeze", "utils_testing.compose_context_emb", "utils_testing.random_vocab_sampling", "print", "range", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "list", "str", "tokenizer_GPT2._convert_id_to_token", "range", "utils_testing.random_word_sampling", "range", "print", "end.item", "insert_loc_list.append", "numpy.arange", "torch.tensor.tolist", "torch.tensor.tolist", "tokenizer_GPT2.convert_tokens_to_string", "torch.tensor", "torch.tensor", "torch.tensor", "feature[].unsqueeze().expand().to", "word_w_sum_norm[].unsqueeze().expand", "future_emb_chosen_arr.append", "range", "feature[].tolist", "feature.size", "utils_testing.cluster_sampling", "range", "str", "range", "word_norm_emb.size", "numpy.array", "utils_testing.sample_seq", "torch.nonzero", "torch.nonzero", "torch.nonzero", "len", "utils_testing.cluster_sampling", "str", "bag_of_words.append", "feature[].unsqueeze().expand", "word_w_sum_norm[].unsqueeze", "utils_testing.sample_seq", "utils_testing.sample_seq", "utils_testing.select_fixed_lda_topics", "torch.zeros", "torch.zeros", "torch.zeros", "utils_testing.NSD_prediction", "feature[].unsqueeze", "range", "utils_testing.select_fixed_lda_topics", "top_index[].item", "word_norm_emb.size"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_lda_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.load_global_centers", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.add_model", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.get_word_list_spacy", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.eval_basis_conditional_text_single", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.generate_report", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.compose_context_emb", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.random_vocab_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.random_word_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.cluster_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.cluster_sampling", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_fixed_lda_topics", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.NSD_prediction", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.select_fixed_lda_topics"], ["", "", "def", "testing_topic_baseline", "(", "model_condition", ",", "gpt2_model", ",", "device_conditional", ",", "num_sent_gen", ",", "gen_sent_len", ",", "dataloader", ",", "word_norm_emb", ",", "idx2word_freq", ",", "outf", ",", "n_basis", ",", "max_batch_num", ",", "tokenizer_GPT2", ",", "bptt_conditional", ",", "topic_mode", ",", "stop_word_set", ",", "parallel_encoder", ",", "parallel_decoder", ",", "de_en_connection", ",", "LDA_model_path", ",", "word_emb_center_path", ",", "csvOutf", ",", "readable_context", "=", "False", ",", "run_eval", "=", "True", ",", "use_corpus", "=", "'wiki'", ",", "idx_l2_type", "=", "[", "]", ")", ":", "\n", "    ", "top_k", "=", "5", "\n", "nlp", "=", "English", "(", ")", "\n", "word_d2_idx", "=", "{", "}", "\n", "for", "idx", "in", "range", "(", "len", "(", "idx2word_freq", ")", ")", ":", "\n", "        ", "word", "=", "idx2word_freq", "[", "idx", "]", "[", "0", "]", "\n", "word_d2_idx", "[", "word", "]", "=", "idx", "\n", "\n", "", "emb_sum", "=", "torch", ".", "sum", "(", "word_norm_emb", ",", "dim", "=", "1", ")", "\n", "OOV_list", "=", "torch", ".", "nonzero", "(", "emb_sum", "==", "0", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "print", "(", "\"OOV number = {}\"", ".", "format", "(", "len", "(", "OOV_list", ")", ")", ")", "\n", "print", "(", "\"OOV index examples {}\"", ".", "format", "(", "OOV_list", "[", ":", "10", "]", ")", ")", "\n", "OOV_set", "=", "set", "(", "OOV_list", ")", "\n", "if", "topic_mode", "==", "'random_vocab'", ":", "\n", "        ", "could_sample_list", "=", "list", "(", "set", "(", "list", "(", "range", "(", "len", "(", "idx2word_freq", ")", ")", ")", ")", "-", "(", "OOV_set", "|", "stop_word_set", ")", ")", "\n", "", "if", "'LDA'", "in", "topic_mode", ":", "\n", "        ", "lda_word_prob_tensor", ",", "lda_top_word_value", ",", "lda_top_word_index", ",", "lda_fixed_topic_emb", "=", "load_lda_model", "(", "LDA_model_path", ",", "word_d2_idx", ",", "word_norm_emb", ",", "top_k", ")", "\n", "", "if", "'global_centers'", "in", "topic_mode", ":", "\n", "        ", "w_emb_top_word_value", ",", "w_emb_top_word_index", ",", "word_norm_emb_w_sum_global", "=", "load_global_centers", "(", "word_emb_center_path", ",", "word_d2_idx", ",", "word_norm_emb", ",", "top_k", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "if", "run_eval", ":", "\n", "            ", "result_stats", "=", "result_statistics", "(", "gpt2_model", ")", "\n", "result_stats", ".", "add_model", "(", "\"Model condition\"", ")", "\n", "", "else", ":", "\n", "            ", "result_stats", "=", "[", "]", "\n", "#result_stats.add_model(\"Original\")", "\n", "#result_stats.add_model(\"PPLM\")", "\n", "", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "# if i_batch == 0:", "\n", "#     continue", "\n", "            ", "print", "(", "\"batch\"", "+", "str", "(", "i_batch", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "#feature, target_unfold, inner_idx_tensor, future_mask = sample_batched", "\n", "if", "use_corpus", "==", "'wiki'", ":", "\n", "                ", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "=", "sample_batched", "\n", "type_idx_tensor", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "feature", ",", "inner_idx_tensor", ",", "type_idx_tensor", "=", "sample_batched", "\n", "inner_idx_tensor", "=", "inner_idx_tensor", ".", "unsqueeze", "(", "1", ")", "\n", "future_mask", "=", "[", "]", "\n", "", "feature_text", "=", "[", "[", "tokenizer_GPT2", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "feature", "[", "i", ",", ":", "]", ".", "tolist", "(", ")", "]", "for", "i", "in", "range", "(", "feature", ".", "size", "(", "0", ")", ")", "]", "\n", "\n", "#tokenized_feature = [nlp(feature_text_i).text for feature_text_i in feature_text]", "\n", "#word_idx_d2_count = {}", "\n", "batch_size", ",", "num_head", "=", "inner_idx_tensor", ".", "size", "(", ")", "\n", "\n", "#if topic_mode != 'random_vocab':", "\n", "#word_idx_list, word_idx_rest_list, word_raw_list, word_raw_rest_list, word_full_list = get_word_list_spacy(inner_idx_tensor, feature_text, tokenizer_GPT2, nlp, word_d2_idx, stop_word_set, OOV_set)", "\n", "word_idx_list", ",", "word_idx_rest_list", ",", "word_raw_list", ",", "word_raw_rest_list", "=", "get_word_list_spacy", "(", "inner_idx_tensor", ",", "feature_text", ",", "tokenizer_GPT2", ",", "nlp", ",", "word_d2_idx", ",", "stop_word_set", ",", "OOV_set", ")", "\n", "\n", "if", "'LDA'", "in", "topic_mode", "or", "'global_centers'", "in", "topic_mode", ":", "\n", "                ", "context_norm_emb", "=", "compose_context_emb", "(", "word_idx_list", ",", "word_norm_emb", ")", "\n", "\n", "", "if", "topic_mode", "==", "'random_vocab'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "random_vocab_sampling", "(", "could_sample_list", ",", "word_norm_emb", ",", "batch_size", ",", "num_head", ",", "n_basis", ",", "top_k", ")", "\n", "\n", "", "elif", "topic_mode", "==", "'random_word'", "or", "topic_mode", "==", "'no_condition'", "or", "topic_mode", "==", "'no_condition_org'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "random_word_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ")", "\n", "\n", "", "elif", "topic_mode", "==", "'SC_cluster'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "cluster_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "cluster_method", "=", "\"Sparse_coding\"", ")", "\n", "\n", "", "elif", "topic_mode", "==", "'kmeans_cluster'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "cluster_sampling", "(", "word_idx_list", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "cluster_method", "=", "\"KMeans\"", ")", "\n", "\n", "", "elif", "topic_mode", "==", "'LDA_org'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "select_fixed_lda_topics", "(", "context_norm_emb", ",", "lda_top_word_value", ",", "lda_top_word_index", ",", "lda_fixed_topic_emb", ",", "n_basis", ")", "\n", "\n", "", "elif", "topic_mode", "==", "'NSD_topic'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "NSD_prediction", "(", "feature", ",", "inner_idx_tensor", ",", "future_mask", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "de_en_connection", ",", "batch_size", ",", "num_head", ")", "\n", "", "elif", "topic_mode", "==", "'global_centers'", ":", "\n", "                ", "top_value", ",", "top_index", ",", "word_w_sum_norm", "=", "select_fixed_lda_topics", "(", "context_norm_emb", ",", "w_emb_top_word_value", ",", "w_emb_top_word_index", ",", "word_norm_emb_w_sum_global", ",", "n_basis", ")", "\n", "#elif topic_mode == 'no_condition':", "\n", "#    word_w_sum_norm = torch.zeros(0)", "\n", "#basis_norm_pred, top_value, top_index = predict_batch(feature, inner_idx_tensor, future_mask, parallel_encoder, parallel_decoder, word_norm_emb, n_basis, top_k, de_en_connection)", "\n", "", "word_w_sum_norm", "=", "word_w_sum_norm", ".", "to", "(", "device", "=", "device_conditional", ")", "\n", "\n", "gen_sent_tensor", "=", "torch", ".", "empty", "(", "(", "batch_size", ",", "num_head", ",", "num_sent_gen", ",", "gen_sent_len", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "#gen_sent_tensor_org = torch.empty( (batch_size, num_head, num_sent_gen, gen_sent_len), dtype=torch.long, device=device_conditional )", "\n", "gen_sent_tensor_org", "=", "torch", ".", "zeros", "(", "0", ")", "\n", "selected_topic_idx_arr", "=", "[", "[", "[", "]", "for", "j", "in", "range", "(", "num_head", ")", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "pplm_sent", "=", "[", "]", "\n", "\n", "#for i_sent in range(1):", "\n", "for", "i_sent", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "print", "(", "\"sent\"", "+", "str", "(", "i_sent", ")", ")", "\n", "#if i_sent == 0:", "\n", "#    continue", "\n", "insert_loc_list", "=", "[", "]", "\n", "future_emb_chosen_arr", "=", "[", "]", "\n", "last_end", "=", "-", "1", "\n", "#temp = []", "\n", "#for m in range(3):", "\n", "for", "m", "in", "range", "(", "num_head", ")", ":", "\n", "                    ", "print", "(", "\"head\"", "+", "str", "(", "m", ")", ")", "\n", "#if m <4:", "\n", "#    continue", "\n", "end", "=", "inner_idx_tensor", "[", "i_sent", ",", "m", "]", "\n", "if", "end", "==", "last_end", ":", "\n", "                        ", "continue", "\n", "\n", "", "last_end", "=", "end", "\n", "end_int", "=", "end", ".", "item", "(", ")", "\n", "max_prompt_len", "=", "bptt_conditional", "-", "gen_sent_len", "\n", "start_int", "=", "0", "\n", "if", "end_int", ">", "max_prompt_len", ":", "\n", "                        ", "start_int", "=", "end_int", "-", "max_prompt_len", "\n", "\n", "", "insert_loc_list", ".", "append", "(", "end_int", "-", "1", ")", "\n", "#num_selection = random.randint(1, n_basis)", "\n", "num_selection", "=", "n_basis", "\n", "#selected_topic_idx = np.sort(np.random.choice(n_basis, size=num_selection, replace = False))", "\n", "selected_topic_idx", "=", "np", ".", "arange", "(", "n_basis", ")", "\n", "selected_topic_idx_arr", "[", "i_sent", "]", "[", "m", "]", "=", "selected_topic_idx", ".", "tolist", "(", ")", "\n", "# generate bag-of-words", "\n", "bag_of_words", "=", "[", "]", "\n", "for", "each", "in", "selected_topic_idx", ".", "tolist", "(", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "                            ", "word_nn", "=", "idx2word_freq", "[", "top_index", "[", "i_sent", ",", "m", ",", "k", ",", "each", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "bag_of_words", ".", "append", "(", "word_nn", ")", "\n", "\n", "", "", "context", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "feature_text", "[", "i_sent", "]", "[", ":", "end", "]", ")", "\n", "\n", "selected_topic_idx", "=", "torch", ".", "tensor", "(", "selected_topic_idx", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "feature_expanded", "=", "feature", "[", "i_sent", ",", "start_int", ":", "end", "]", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_sent_gen", ",", "end_int", "-", "start_int", ")", ".", "to", "(", "device", "=", "device_conditional", ")", "\n", "future_emb_chosen", "=", "word_w_sum_norm", "[", "i_sent", ",", "m", ",", "selected_topic_idx", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_sent_gen", ",", "num_selection", ",", "word_norm_emb", ".", "size", "(", "-", "1", ")", ")", "\n", "future_emb_chosen_arr", ".", "append", "(", "future_emb_chosen", ")", "\n", "insert_loc_truncated", "=", "np", ".", "array", "(", "insert_loc_list", ")", "-", "start_int", "\n", "#truncate_idx = 0", "\n", "#while( insert_loc_truncated[truncate_idx] < 0 ):", "\n", "#    truncate_idx += 1", "\n", "truncate_idx", "=", "-", "1", "\n", "if", "topic_mode", "==", "'no_condition'", ":", "\n", "#feature_expanded = feature[i_sent,start_int:end].unsqueeze(0).expand(num_sent_gen,end_int - start_int).to(device = device_conditional)", "\n", "#output_org = sample_seq(model_condition, feature_expanded, None, None, gen_sent_len, device_conditional)", "\n", "#truncate_idx = -1", "\n", "                        ", "output", "=", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ",", "[", "torch", ".", "zeros", "(", "(", "num_sent_gen", ",", "0", ",", "word_norm_emb", ".", "size", "(", "-", "1", ")", ")", ",", "device", "=", "device_conditional", ")", "for", "x", "in", "range", "(", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ".", "size", ")", "]", ",", "gen_sent_len", ",", "device_conditional", ",", ")", "\n", "#gen_sent_tensor[i_sent, m, :, :] = output_org", "\n", "#continue", "\n", "", "elif", "topic_mode", "==", "'no_condition_org'", ":", "\n", "                        ", "output", "=", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "None", ",", "None", ",", "gen_sent_len", ",", "device_conditional", ",", ")", "\n", "", "else", ":", "\n", "                        ", "output", "=", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ",", "future_emb_chosen_arr", "[", "truncate_idx", ":", "]", ",", "gen_sent_len", ",", "device_conditional", ",", ")", "\n", "", "gen_sent_tensor", "[", "i_sent", ",", "m", ",", ":", ",", ":", "]", "=", "output", "\n", "\n", "#gen_text = tokenizer_GPT2.decode(output)", "\n", "#try:", "\n", "#    gen_text, _ = pplm_model.run_pplm_example(context, False, num_sent_gen, bag_of_words, gen_sent_len, 0.05, 1.0, top_k, True, 1, 10000, 1, 0, False, 1.5, 0.9, 0.01, True)", "\n", "#except:", "\n", "#    print(context)", "\n", "#    print(num_sent_gen)", "\n", "#    print(bag_of_words)", "\n", "#    print(gen_sent_len)", "\n", "#    sys.exit(1)", "\n", "#    print(gen_text,perplexity)", "\n", "#    gen_text, _ = pplm_model.run_pplm_example(context, False, num_sent_gen, bag_of_words, gen_sent_len, 0.05, 1.0, top_k, True, 1, 10000, 1, 0, False, 1.5, 0.9, 0.01, True)", "\n", "#temp.append(gen_text)", "\n", "#output_org = sample_seq(model_condition, feature_expanded, None, None, gen_sent_len, device_conditional,)", "\n", "#gen_sent_tensor_org[i_sent, m, :, :] = output_org", "\n", "#pplm_sent.append(temp)", "\n", "#print_basis_conditional_text(feature, pplm_sent, idx2word_freq, top_value, top_index, i_batch, outf, tokenizer_GPT2, inner_idx_tensor, gen_sent_tensor, gen_sent_tensor_org, selected_topic_idx_arr, result_stats, csvOutf)", "\n", "", "", "eval_basis_conditional_text_single", "(", "feature", ",", "idx2word_freq", ",", "top_value", ",", "top_index", ",", "tokenizer_GPT2", ",", "inner_idx_tensor", ",", "gen_sent_tensor", ",", "selected_topic_idx_arr", ",", "result_stats", ",", "word_raw_list", ",", "word_raw_rest_list", ",", "outf", ",", "csvOutf", ",", "readable_context", ",", "topic_mode", ",", "run_eval", ",", "type_idx_tensor", ",", "idx_l2_type", ")", "\n", "#if run_eval:", "\n", "#   result_stats.renew_ngram()", "\n", "if", "i_batch", "+", "1", ">=", "max_batch_num", ":", "\n", "                ", "break", "\n", "", "", "if", "run_eval", ":", "\n", "            ", "result_stats", ".", "print", "(", ")", "\n", "result_stats", ".", "generate_report", "(", "outf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertConfig.__init__": [[24, 70], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "hidden_size", ",", "\n", "max_position_embeddings", ",", "\n", "num_hidden_layers", "=", "2", ",", "\n", "num_attention_heads", "=", "10", ",", "\n", "#intermediate_size=3072,", "\n", "intermediate_factor", "=", "4", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "#type_vocab_size=2,", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n        Args:\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            intermediate_factor: intermediate_size = hidden_size * intermediate_factor:\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "#self.intermediate_size = intermediate_size", "\n", "self", ".", "intermediate_size", "=", "intermediate_factor", "*", "hidden_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "#self.type_vocab_size = type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertLayerNorm.__init__": [[77, 84], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BertLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertLayerNorm.forward": [[85, 90], ["x.mean", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "weight", "*", "x", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertEmbeddings.__init__": [[95, 106], ["torch.nn.Module.__init__", "model_trans.BertLayerNorm", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_position_emb", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)", "\n", "if", "add_position_emb", ":", "\n", "            ", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "", "self", ".", "add_position_emb", "=", "add_position_emb", "\n", "#self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "#self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertEmbeddings.forward": [[108, 129], ["model_trans.BertEmbeddings.LayerNorm", "words_embeddings.size", "words_embeddings.size", "torch.arange", "position_ids.expand.expand.expand", "model_trans.BertEmbeddings.position_embeddings"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "words_embeddings", ")", ":", "\n", "        ", "if", "self", ".", "add_position_emb", ":", "\n", "            ", "seq_length", "=", "words_embeddings", ".", "size", "(", "1", ")", "\n", "bsz", "=", "words_embeddings", ".", "size", "(", "0", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "words_embeddings", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "expand", "(", "bsz", ",", "seq_length", ")", "\n", "#position_ids = position_ids.unsqueeze(0).expand_as(input_ids)", "\n", "#if token_type_ids is None:", "\n", "#    token_type_ids = torch.zeros_like(input_ids)", "\n", "\n", "#words_embeddings = self.word_embeddings(input_ids)", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "#token_type_embeddings = self.token_type_embeddings(token_type_ids)", "\n", "\n", "#embeddings = words_embeddings + position_embeddings + token_type_embeddings", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "\n", "", "else", ":", "\n", "            ", "embeddings", "=", "words_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "#embeddings = self.dropout(embeddings)", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.MultiHeadedAttention.__init__": [[131, 146], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "MultiHeadedAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.MultiHeadedAttention.transpose_for_scores": [[147, 151], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.MultiHeadedAttention.forward": [[153, 181], ["model_trans.MultiHeadedAttention.query", "model_trans.MultiHeadedAttention.key", "model_trans.MultiHeadedAttention.value", "model_trans.MultiHeadedAttention.transpose_for_scores", "model_trans.MultiHeadedAttention.transpose_for_scores", "model_trans.MultiHeadedAttention.transpose_for_scores", "torch.matmul", "model_trans.MultiHeadedAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "model_trans.MultiHeadedAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.MultiHeadedAttention.transpose_for_scores", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.MultiHeadedAttention.transpose_for_scores", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.MultiHeadedAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "query_in", ",", "key_in", ",", "value_in", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "query_in", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "key_in", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "value_in", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertSelfOutput.__init__": [[183, 188], ["torch.nn.Module.__init__", "torch.nn.Linear", "model_trans.BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertSelfOutput.forward": [[189, 194], ["model_trans.BertSelfOutput.dense", "model_trans.BertSelfOutput.dropout", "model_trans.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertAttention.__init__": [[197, 201], ["torch.nn.Module.__init__", "model_trans.MultiHeadedAttention", "model_trans.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "MultiHeadedAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertAttention.forward": [[202, 210], ["model_trans.BertAttention.output", "model_trans.BertAttention.self", "model_trans.BertAttention.self"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "memory_tensors", ",", "attention_mask", ")", ":", "\n", "#self_output = self.self(input_tensor, attention_mask)", "\n", "        ", "if", "memory_tensors", "is", "None", ":", "\n", "            ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "input_tensor", ",", "input_tensor", ",", "attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "memory_tensors", ",", "memory_tensors", ",", "attention_mask", ")", "\n", "", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertIntermediate.__init__": [[213, 220], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertIntermediate.forward": [[221, 225], ["model_trans.BertIntermediate.dense", "model_trans.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertOutput.__init__": [[228, 233], ["torch.nn.Module.__init__", "torch.nn.Linear", "model_trans.BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertOutput.forward": [[234, 239], ["model_trans.BertOutput.dense", "model_trans.BertOutput.dropout", "model_trans.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertLayer.__init__": [[241, 246], ["torch.nn.Module.__init__", "model_trans.BertAttention", "model_trans.BertIntermediate", "model_trans.BertOutput"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertLayer.forward": [[247, 252], ["model_trans.BertLayer.attention", "model_trans.BertLayer.intermediate", "model_trans.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "None", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.Decode_Layer.__init__": [[254, 260], ["torch.nn.Module.__init__", "model_trans.BertAttention", "model_trans.BertAttention", "model_trans.BertIntermediate", "model_trans.BertOutput"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Decode_Layer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "memory_attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.Decode_Layer.forward": [[261, 267], ["model_trans.Decode_Layer.self_attention", "model_trans.Decode_Layer.memory_attention", "model_trans.Decode_Layer.intermediate", "model_trans.Decode_Layer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "memory_tensors", ",", "attention_mask", ")", ":", "\n", "        ", "self_attention_output", "=", "self", ".", "self_attention", "(", "hidden_states", ",", "None", ",", "None", ")", "\n", "memory_attention_output", "=", "self", ".", "memory_attention", "(", "self_attention_output", ",", "memory_tensors", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "memory_attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "memory_attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.Transformer.__init__": [[270, 283], ["torch.nn.Module.__init__", "model_trans.BertConfig", "model_trans.BertEmbeddings", "torch.nn.ModuleList", "model_trans.Decode_Layer", "print", "model_trans.BertLayer", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["    ", "def", "__init__", "(", "self", ",", "model_type", ",", "hidden_size", ",", "max_position_embeddings", ",", "num_hidden_layers", "=", "2", ",", "add_position_emb", "=", "False", ",", "decoder", "=", "False", ",", "dropout_prob", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "Transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "config", "=", "BertConfig", "(", "hidden_size", "=", "hidden_size", ",", "max_position_embeddings", "=", "max_position_embeddings", ",", "num_hidden_layers", "=", "num_hidden_layers", ",", "hidden_dropout_prob", "=", "dropout_prob", ",", "attention_probs_dropout_prob", "=", "dropout_prob", ")", "\n", "self", ".", "emb_layer", "=", "BertEmbeddings", "(", "config", ",", "add_position_emb", ")", "\n", "if", "decoder", ":", "\n", "            ", "layer", "=", "Decode_Layer", "(", "config", ")", "\n", "print", "(", "\"Using Tranformer decoder\"", ")", "\n", "", "else", ":", "\n", "            ", "layer", "=", "BertLayer", "(", "config", ")", "\n", "", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "self", ".", "model_type", "=", "model_type", "\n", "#self.add_position_emb = add_position_emb", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.Transformer.forward": [[285, 316], ["model_trans.Transformer.emb_layer", "attention_mask.to.to.unsqueeze().unsqueeze", "attention_mask.to.to.to", "all_encoder_layers.append", "layer_module", "layer_module", "all_encoder_layers.append", "attention_mask.to.to.unsqueeze", "next", "model_trans.Transformer.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "memory_tensors", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "False", ")", ":", "\n", "#print(\"hidden state size\", hidden_states.size())", "\n", "        ", "hidden_states", "=", "self", ".", "emb_layer", "(", "hidden_states", ")", "\n", "#hidden_states should have dimension [n_batch, n_seq_len, emb_size]", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "            ", "attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_mask", "=", "attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "attention_mask", "=", "(", "1.0", "-", "attention_mask", ")", "*", "-", "10000.0", "\n", "#attention_mask should have the same dimension and 0 means not masking while -10000.0 means masking", "\n", "", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "if", "self", ".", "decoder", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "memory_tensors", ",", "attention_mask", ")", "\n", "", "else", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "", "if", "output_all_encoded_layers", ":", "\n", "                ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertPooler.__init__": [[319, 323], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.BertPooler.forward": [[324, 331], ["model_trans.BertPooler.dense", "model_trans.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.gelu": [[8, 15], ["torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model_trans.swish": [[16, 18], ["torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_condition.logging": [[108, 115], ["print", "sys.stdout.flush", "open", "f_log.write", "os.path.join"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["def", "logging", "(", "s", ",", "print_", "=", "True", ",", "log_", "=", "True", ")", ":", "\n", "    ", "if", "print_", ":", "\n", "        ", "print", "(", "s", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "if", "log_", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "save", ",", "'log.txt'", ")", ",", "'a+'", ")", "as", "f_log", ":", "\n", "            ", "f_log", ".", "write", "(", "s", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_condition.counter_to_tensor": [[137, 146], ["len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range"], "function", ["None"], ["def", "counter_to_tensor", "(", "idx2word_freq", ",", "device", ")", ":", "\n", "    ", "total", "=", "len", "(", "idx2word_freq", ")", "\n", "w_freq", "=", "torch", ".", "zeros", "(", "total", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "total", ")", ":", "\n", "        ", "w_freq", "[", "i", "]", "=", "1", "\n", "#w_freq[i] = math.sqrt(idx2word_freq[x][1])", "\n", "#w_freq[i] = idx2word_freq[x][1]", "\n", "", "w_freq", "[", "0", "]", "=", "-", "1", "\n", "return", "w_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_condition.evaluate": [[191, 222], ["encoder.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "total_loss.item", "len", "feature.size", "encoder", "numpy.random.choice", "np.random.choice.sort", "numpy.random.randint", "range", "encoder", "future_toks.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "future_toks.gather", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "future_toks.gather.gather", "future_emb_chosen_arr.append", "list", "range", "range"], "function", ["None"], ["def", "evaluate", "(", "dataloader", ",", "external_emb", ")", ":", "\n", "# Turn on evaluation mode which disables dropout.", "\n", "    ", "encoder", ".", "eval", "(", ")", "\n", "total_loss", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "            ", "feature", ",", "future_toks", ",", "idx_gpt2_to_spacy_small", "=", "sample_batched", "\n", "if", "args", ".", "turn_off_condition", ":", "\n", "                ", "outputs", "=", "encoder", "(", "feature", ",", "labels", "=", "feature", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "insert_loc", "=", "np", ".", "random", ".", "choice", "(", "args", ".", "bptt", "-", "args", ".", "min_rest_seq_len", ",", "size", "=", "args", ".", "num_insert", ",", "replace", "=", "False", ")", "\n", "insert_loc", ".", "sort", "(", ")", "\n", "chosen_topic_num", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "args", ".", "max_chosen_topics", "+", "1", ",", "size", "=", "(", "args", ".", "num_insert", ")", ")", "\n", "future_emb_chosen_arr", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "args", ".", "num_insert", ")", ":", "\n", "                    ", "start", "=", "idx_gpt2_to_spacy_small", "[", ":", ",", "insert_loc", "[", "j", "]", "]", "\n", "batch_size", "=", "future_toks", ".", "size", "(", "0", ")", "\n", "idx_space", "=", "torch", ".", "tensor", "(", "[", "list", "(", "range", "(", "start", "[", "k", "]", ",", "start", "[", "k", "]", "+", "args", ".", "n_further", ")", ")", "for", "k", "in", "range", "(", "batch_size", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "#future_sample_space = future_toks[:,start:start+args.n_further]", "\n", "future_sample_space", "=", "future_toks", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "idx_space", ")", "\n", "select_idx", "=", "torch", ".", "randint", "(", "0", ",", "args", ".", "n_further", ",", "size", "=", "(", "batch_size", ",", "chosen_topic_num", "[", "j", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "future_toks_chosen", "=", "future_sample_space", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "select_idx", ")", "\n", "future_emb_chosen_arr", ".", "append", "(", "external_emb", "[", "future_toks_chosen", ",", ":", "]", ")", "\n", "", "outputs", "=", "encoder", "(", "feature", ",", "labels", "=", "feature", ",", "insert_loc", "=", "insert_loc", ",", "future_emb_chosen_arr", "=", "future_emb_chosen_arr", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "\n", "", "batch_size", "=", "feature", ".", "size", "(", "0", ")", "\n", "total_loss", "+=", "loss", "*", "batch_size", "\n", "\n", "", "", "return", "total_loss", ".", "item", "(", ")", "/", "len", "(", "dataloader", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_condition.train_one_epoch": [[224, 331], ["time.time", "encoder.train", "enumerate", "optimizer_e.zero_grad", "loss.item", "loss.backward", "gc.collect", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer_e.step", "parallel_encoder", "numpy.random.choice", "np.random.choice.sort", "numpy.random.randint", "range", "parallel_encoder", "encoder.parameters", "main_train_condition.logging", "time.time", "future_toks.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "future_toks.gather", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "future_toks.gather.gather", "future_emb_chosen_arr.append", "time.time", "list", "len", "range", "range"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.step", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_topics.logging"], ["", "def", "train_one_epoch", "(", "dataloader_train", ",", "external_emb", ",", "lr", ",", "split_i", ")", ":", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "total_loss", "=", "0.", "\n", "\n", "encoder", ".", "train", "(", ")", "\n", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader_train", ")", ":", "\n", "#init_head_posi = random.randint(0, args.dilated_head_span - 1)", "\n", "#dilated_heads_posi = list(range(init_head_posi, len(feature), args.dilated_head_span))", "\n", "#if dilated_heads_posi[-1] != len(feature)-1:", "\n", "#    dilated_heads_posi.append(len(feature)-1)", "\n", "        ", "feature", ",", "future_toks", ",", "idx_gpt2_to_spacy_small", "=", "sample_batched", "\n", "if", "args", ".", "turn_off_condition", ":", "\n", "            ", "outputs", "=", "parallel_encoder", "(", "feature", ",", "labels", "=", "feature", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "", "else", ":", "\n", "#insert_loc = random.sample( range(args.bptt - args.min_rest_seq_len), args.num_insert) #without replacement", "\n", "            ", "insert_loc", "=", "np", ".", "random", ".", "choice", "(", "args", ".", "bptt", "-", "args", ".", "min_rest_seq_len", ",", "size", "=", "args", ".", "num_insert", ",", "replace", "=", "False", ")", "\n", "insert_loc", ".", "sort", "(", ")", "\n", "chosen_topic_num", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "args", ".", "max_chosen_topics", "+", "1", ",", "size", "=", "(", "args", ".", "num_insert", ")", ")", "\n", "future_emb_chosen_arr", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "args", ".", "num_insert", ")", ":", "\n", "                ", "start", "=", "idx_gpt2_to_spacy_small", "[", ":", ",", "insert_loc", "[", "j", "]", "]", "\n", "batch_size", "=", "future_toks", ".", "size", "(", "0", ")", "\n", "idx_space", "=", "torch", ".", "tensor", "(", "[", "list", "(", "range", "(", "start", "[", "k", "]", ",", "start", "[", "k", "]", "+", "args", ".", "n_further", ")", ")", "for", "k", "in", "range", "(", "batch_size", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "#future_sample_space = future_toks[:,start:start+args.n_further]", "\n", "future_sample_space", "=", "future_toks", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "idx_space", ")", "\n", "select_idx", "=", "torch", ".", "randint", "(", "0", ",", "args", ".", "n_further", ",", "size", "=", "(", "batch_size", ",", "chosen_topic_num", "[", "j", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "future_toks_chosen", "=", "future_sample_space", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "select_idx", ")", "\n", "future_emb_chosen_arr", ".", "append", "(", "external_emb", "[", "future_toks_chosen", ",", ":", "]", ")", "\n", "#future_toks_chosen_emb = external_emb[future_toks_chosen,:]", "\n", "#if args.avg_word_num == 1 or chosen_topic_num[j]<=1:", "\n", "#    future_emb_chosen_arr.append( future_toks_chosen_emb )", "\n", "#else:", "\n", "#    future_toks_all_emb = external_emb[future_sample_space,:]", "\n", "#    merge_num = int(chosen_topic_num[j]/2)", "\n", "#    emb_sim = torch.bmm(future_toks_chosen_emb[:,:merge_num,:], future_toks_all_emb.permute(0,2,1))", "\n", "#    top_value, top_index = torch.topk(emb_sim, args.avg_word_num, dim = 2, sorted=False)", "\n", "#    #top_index should have size (batch_size, merge_num, args.avg_word_num)", "\n", "#    emb_size = future_toks_all_emb.size(2)", "\n", "#    future_toks_all_emb_expanded = future_toks_all_emb.unsqueeze(dim=1).expand(batch_size, merge_num, future_toks_all_emb.size(1), future_toks_all_emb.size(2))", "\n", "#    future_toks_topk_emb = future_toks_all_emb_expanded.gather(dim = 2, index = top_index.unsqueeze(dim=-1).expand(batch_size, merge_num, args.avg_word_num, emb_size))", "\n", "#    #future_toks_topk_emb should have size (batch_size, merge_num, args.avg_word_num, emb_size)", "\n", "#    future_toks_topk_sum_emb = torch.sum( future_toks_topk_emb * top_value.unsqueeze(-1), dim = 2) ", "\n", "#    future_toks_topk_sum_norm_emb = future_toks_topk_sum_emb / (0.000000000001 + future_toks_topk_sum_emb.norm(dim = -1, keepdim=True))", "\n", "#    future_toks_merge_emb = torch.cat( (future_toks_chosen_emb[:,merge_num:,:], future_toks_topk_sum_norm_emb), dim = 1)", "\n", "#    future_toks_merge_perm_emb = future_toks_merge_emb[:, torch.randperm(chosen_topic_num[j].item(),device=device),:]", "\n", "#    future_emb_chosen_arr.append( future_toks_merge_perm_emb )", "\n", "", "outputs", "=", "parallel_encoder", "(", "feature", ",", "labels", "=", "feature", ",", "insert_loc", "=", "insert_loc", ",", "future_emb_chosen_arr", "=", "future_emb_chosen_arr", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "#feature.size(0) % args.dilated_head_span", "\n", "#target_idx = idx_feature_to_target[:,init_head_posi:].unfold(dim=1, args.n_further, args.dilated_head_span)", "\n", "#target_idx = torch.cat( (target_idx, idx_feature_to_target[:,-1]), dim=1)", "\n", "\n", "#print(feature.size())", "\n", "#print(target_unfold.size()) ", "\n", "#print(future_mask.size())", "\n", "#target should have size (batch, num_head, n_further)", "\n", "#target = target_unfold.view(-1, target_unfold.size(2) )", "\n", "\n", "", "optimizer_e", ".", "zero_grad", "(", ")", "\n", "#output_emb, hidden, output_emb_last = parallel_encoder(feature.t())", "\n", "#output_emb, hidden, output_emb_last = parallel_encoder(feature)", "\n", "#output_emb_last, output_emb = parallel_encoder(feature)", "\n", "#output_emb, past = parallel_encoder(feature)", "\n", "#output_emb should have size (batch, seq_len, hidden_size)", "\n", "#hidden_size = output_emb.size(2)", "\n", "#batch_size, num_head, seq_len = future_mask.size()", "\n", "\n", "#print(inner_idx_tensor)", "\n", "#output_emb_head = output_emb.gather(dim=1,index=inner_idx_tensor.unsqueeze(dim=-1).expand(batch_size,num_head,hidden_size))", "\n", "#output_emb_head should have size (batch, num_head, hidden_size)", "\n", "#output_emb_last = output_emb_head.view(-1, output_emb_head.size(2))", "\n", "\n", "\n", "loss", "*=", "args", ".", "small_batch_size", "/", "args", ".", "batch_size", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "gc", ".", "collect", "(", ")", "\n", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "encoder", ".", "parameters", "(", ")", ",", "args", ".", "clip", ")", "\n", "optimizer_e", ".", "step", "(", ")", "\n", "\n", "#del sample_batched", "\n", "#break", "\n", "\n", "#if args.update_target_emb:", "\n", "#    if args.optimizer == 'SGD':", "\n", "#        external_emb.data -= lr/10.0 * external_emb.grad.data", "\n", "#    else:", "\n", "#        external_emb.data -= 10/10.0 * external_emb.grad.data", "\n", "#    external_emb.data[0,:] = 0", "\n", "#    external_emb.grad.data.zero_()", "\n", "#    #with torch.no_grad():", "\n", "#    external_emb.data = external_emb.data / (0.000000000001 + external_emb.data.norm(dim = 1, keepdim=True))", "\n", "\n", "if", "i_batch", "%", "args", ".", "log_interval", "==", "0", "and", "i_batch", ">", "0", ":", "\n", "            ", "cur_loss", "=", "total_loss", "/", "args", ".", "log_interval", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "logging", "(", "'| e {:3d} {:3d} | {:5d}/{:5d} b | lr {:02.2f} | ms/batch {:5.2f} | '", "\n", "'l {:5.5f}  '", ".", "format", "(", "\n", "epoch", ",", "split_i", ",", "i_batch", ",", "len", "(", "dataloader_train", ".", "dataset", ")", "//", "args", ".", "batch_size", ",", "optimizer_e", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "\n", "elapsed", "*", "1000", "/", "args", ".", "log_interval", ",", "cur_loss", ")", ")", "\n", "\n", "total_loss", "=", "0.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.SparseCoding.__init__": [[11, 16], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "ntopic", ",", "nbow", ",", "emb_size", ",", "device", ")", ":", "\n", "        ", "super", "(", "SparseCoding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "code_book", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "ntopic", ",", "emb_size", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "coeff", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "nbow", ",", "ntopic", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.SparseCoding.compute_coeff_pos": [[17, 19], ["model.SparseCoding.coeff.clamp"], "methods", ["None"], ["", "def", "compute_coeff_pos", "(", "self", ")", ":", "\n", "        ", "self", ".", "coeff", ".", "data", "=", "self", ".", "coeff", ".", "clamp", "(", "0.0", ",", "1.0", ")", "\n", "#self.coeff.data = self.coeff.clamp(0.0)", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.SparseCoding.forward": [[21, 24], ["model.SparseCoding.coeff.matmul"], "methods", ["None"], ["", "def", "forward", "(", "self", ")", ":", "\n", "        ", "result", "=", "self", ".", "coeff", ".", "matmul", "(", "self", ".", "code_book", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.MatrixReconstruction.__init__": [[27, 31], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "ntopic", ",", "nbow", ",", "device", ")", ":", "\n", "        ", "super", "(", "MatrixReconstruction", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "coeff", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "batch_size", ",", "ntopic", ",", "nbow", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.MatrixReconstruction.compute_coeff_pos": [[32, 34], ["model.MatrixReconstruction.coeff.clamp"], "methods", ["None"], ["", "def", "compute_coeff_pos", "(", "self", ")", ":", "\n", "        ", "self", ".", "coeff", ".", "data", "=", "self", ".", "coeff", ".", "clamp", "(", "0.0", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.MatrixReconstruction.forward": [[35, 38], ["model.MatrixReconstruction.coeff.matmul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "result", "=", "self", ".", "coeff", ".", "matmul", "(", "input", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.RNN_decoder.__init__": [[40, 67], ["torch.Module.__init__", "torch.RNN", "torch.RNN", "torch.RNN", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "getattr", "model.RNN_decoder.init_hid_linear_1[].weight.data.uniform_", "model.RNN_decoder.init_hid_linear_1[].bias.data.uniform_", "model.RNN_decoder.init_hid_linear_2[].weight.data.uniform_", "model.RNN_decoder.init_hid_linear_2[].bias.data.uniform_", "ValueError", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "range", "range"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_type", ",", "emb_dim", ",", "ninp", ",", "nhid", ",", "nlayers", ")", ":", "\n", "        ", "super", "(", "RNN_decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "model_type", "in", "[", "'LSTM'", ",", "'GRU'", "]", ":", "\n", "            ", "self", ".", "rnn", "=", "getattr", "(", "nn", ",", "model_type", ")", "(", "emb_dim", ",", "nhid", ",", "nlayers", ",", "dropout", "=", "0", ")", "\n", "#if linear_mapping_dim > 0:", "\n", "#    self.rnn = getattr(nn, model_type)(linear_mapping_dim, nhid, nlayers, dropout=0)", "\n", "#else:", "\n", "#    self.rnn = getattr(nn, model_type)(ninp+position_emb_size, nhid, nlayers, dropout=0)", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "nonlinearity", "=", "{", "'RNN_TANH'", ":", "'tanh'", ",", "'RNN_RELU'", ":", "'relu'", "}", "[", "model_type", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "raise", "ValueError", "(", "\"\"\"An invalid option for `--model` was supplied,\n                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\"", ")", "\n", "#self.rnn = nn.RNN(ninp+position_emb_size, nhid, nlayers, nonlinearity=nonlinearity, dropout=0)", "\n", "", "self", ".", "rnn", "=", "nn", ".", "RNN", "(", "emb_dim", ",", "nhid", ",", "nlayers", ",", "nonlinearity", "=", "nonlinearity", ",", "dropout", "=", "0", ")", "\n", "\n", "", "if", "model_type", "==", "'LSTM'", ":", "\n", "            ", "self", ".", "init_hid_linear_1", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "ninp", ",", "nhid", ")", "for", "i", "in", "range", "(", "nlayers", ")", "]", ")", "\n", "self", ".", "init_hid_linear_2", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "ninp", ",", "nhid", ")", "for", "i", "in", "range", "(", "nlayers", ")", "]", ")", "\n", "for", "i", "in", "range", "(", "nlayers", ")", ":", "\n", "                ", "self", ".", "init_hid_linear_1", "[", "i", "]", ".", "weight", ".", "data", ".", "uniform_", "(", "-", ".1", ",", ".1", ")", "\n", "self", ".", "init_hid_linear_1", "[", "i", "]", ".", "bias", ".", "data", ".", "uniform_", "(", "-", ".5", ",", ".5", ")", "\n", "self", ".", "init_hid_linear_2", "[", "i", "]", ".", "weight", ".", "data", ".", "uniform_", "(", "-", ".1", ",", ".1", ")", "\n", "self", ".", "init_hid_linear_2", "[", "i", "]", ".", "bias", ".", "data", ".", "uniform_", "(", "-", ".5", ",", ".5", ")", "\n", "", "", "self", ".", "nlayers", "=", "nlayers", "\n", "self", ".", "model_type", "=", "model_type", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.RNN_decoder.forward": [[68, 75], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.RNN_decoder.rnn", "range", "range"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_init", ",", "emb", ")", ":", "\n", "        ", "hidden_1", "=", "torch", ".", "cat", "(", "[", "self", ".", "init_hid_linear_1", "[", "i", "]", "(", "input_init", ")", ".", "unsqueeze", "(", "dim", "=", "0", ")", "for", "i", "in", "range", "(", "self", ".", "nlayers", ")", "]", ",", "dim", "=", "0", ")", "\n", "hidden_2", "=", "torch", ".", "cat", "(", "[", "self", ".", "init_hid_linear_2", "[", "i", "]", "(", "input_init", ")", ".", "unsqueeze", "(", "dim", "=", "0", ")", "for", "i", "in", "range", "(", "self", ".", "nlayers", ")", "]", ",", "dim", "=", "0", ")", "\n", "hidden", "=", "(", "hidden_1", ",", "hidden_2", ")", "\n", "\n", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "emb", ",", "hidden", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.ext_emb_to_seq.__init__": [[86, 107], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "model_trans.Transformer.ext_emb_to_seq.decoder_array.append", "model_trans.Transformer.RNN_decoder", "model_trans.Transformer", "print", "sys.exit"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["    ", "def", "__init__", "(", "self", ",", "model_type_list", ",", "emb_dim", ",", "ninp", ",", "nhid", ",", "nlayers", ",", "n_basis", ",", "trans_layers", ",", "using_memory", ",", "add_position_emb", ",", "dropout_prob_trans", ")", ":", "\n", "        ", "super", "(", "ext_emb_to_seq", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "decoder_array", "=", "nn", ".", "ModuleList", "(", ")", "\n", "input_dim", "=", "emb_dim", "\n", "self", ".", "trans_dim", "=", "None", "\n", "for", "model_type", "in", "model_type_list", ":", "\n", "            ", "if", "model_type", "==", "'LSTM'", ":", "\n", "                ", "model", "=", "RNN_decoder", "(", "model_type", ",", "input_dim", ",", "ninp", ",", "nhid", ",", "nlayers", ")", "\n", "input_dim", "=", "nhid", "\n", "#output_dim = nhid", "\n", "", "elif", "model_type", "==", "'TRANS'", ":", "\n", "#model = model_trans.BertEncoder(model_type = model_type, hidden_size = input_dim, max_position_embeddings = n_basis, num_hidden_layers=trans_layers)", "\n", "#print(\"input_dim\", input_dim)", "\n", "                ", "model", "=", "model_trans", ".", "Transformer", "(", "model_type", "=", "model_type", ",", "hidden_size", "=", "input_dim", ",", "max_position_embeddings", "=", "n_basis", ",", "num_hidden_layers", "=", "trans_layers", ",", "add_position_emb", "=", "add_position_emb", ",", "decoder", "=", "using_memory", ",", "dropout_prob", "=", "dropout_prob_trans", ")", "\n", "self", ".", "trans_dim", "=", "input_dim", "\n", "#output_dim = input_dim", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"model type must be either LSTM or TRANS\"", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "self", ".", "decoder_array", ".", "append", "(", "model", ")", "\n", "", "self", ".", "output_dim", "=", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.ext_emb_to_seq.forward": [[108, 120], ["model", "hidden_states[].permute.permute", "model", "hidden_states[].permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_init", ",", "emb", ",", "memory", "=", "None", ",", "memory_attention_mask", "=", "None", ")", ":", "\n", "        ", "hidden_states", "=", "emb", "\n", "for", "model", "in", "self", ".", "decoder_array", ":", "\n", "            ", "model_type", "=", "model", ".", "model_type", "\n", "if", "model_type", "==", "'LSTM'", ":", "\n", "                ", "hidden_states", "=", "model", "(", "input_init", ",", "hidden_states", ")", "\n", "", "elif", "model_type", "==", "'TRANS'", ":", "\n", "#If we want to use transformer by default at the end, we will want to reconsider reducing the number of permutes", "\n", "                ", "hidden_states", "=", "hidden_states", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "hidden_states", "=", "model", "(", "hidden_states", ",", "memory", ",", "memory_attention_mask", ")", "\n", "hidden_states", "=", "hidden_states", "[", "0", "]", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.EMB2SEQ.__init__": [[125, 179], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "model.ext_emb_to_seq", "torch.Linear", "torch.Linear", "torch.Linear", "model.EMB2SEQ.init_weights", "torch.Linear", "torch.Linear", "torch.Linear", "model.EMB2SEQ.in_linear_mem.bias.data.uniform_", "model.EMB2SEQ.in_linear_mem.weight.data.uniform_", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "model.EMB2SEQ.init_linear_arr[].bias.data.uniform_", "model.EMB2SEQ.init_linear_arr[].weight.data.uniform_", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "range", "torch.Embedding", "torch.Embedding", "torch.Embedding", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "model_type_list", ",", "ninp", ",", "nhid", ",", "outd", ",", "nlayers", ",", "n_basis", ",", "positional_option", ",", "dropoutp", "=", "0.5", ",", "trans_layers", "=", "2", ",", "using_memory", "=", "False", ",", "dropout_prob_trans", "=", "0.1", ")", ":", "\n", "#super(RNNModel_decoder, self).__init__()", "\n", "        ", "super", "(", "EMB2SEQ", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropoutp", ")", "\n", "self", ".", "n_basis", "=", "n_basis", "\n", "#self.layernorm = nn.InstanceNorm1d(n_basis, affine =False)", "\n", "#self.outd_sqrt = math.sqrt(outd)", "\n", "#self.linear_mapping_dim = linear_mapping_dim", "\n", "#position_emb_size = 0", "\n", "#if n_basis > 0:", "\n", "#if linear_mapping_dim > 0:", "\n", "input_size", "=", "ninp", "\n", "#self.in_linear = nn.Linear(ninp, nhid)", "\n", "if", "using_memory", ":", "\n", "            ", "self", ".", "in_linear_mem", "=", "nn", ".", "Linear", "(", "ninp", ",", "nhid", ")", "\n", "self", ".", "in_linear_mem", ".", "bias", ".", "data", ".", "uniform_", "(", "-", ".00005", ",", ".00005", ")", "\n", "self", ".", "in_linear_mem", ".", "weight", ".", "data", ".", "uniform_", "(", "-", ".00001", ",", ".00001", ")", "\n", "#input_size = nhid", "\n", "", "add_position_emb", "=", "False", "\n", "if", "positional_option", "==", "'linear'", ":", "\n", "            ", "linear_mapping_dim", "=", "nhid", "\n", "self", ".", "init_linear_arr", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "input_size", ",", "linear_mapping_dim", ")", "for", "i", "in", "range", "(", "n_basis", ")", "]", ")", "\n", "for", "i", "in", "range", "(", "n_basis", ")", ":", "\n", "#It seems that the LSTM only learns well when bias is larger than weights at the beginning", "\n", "#If setting std in weight to be too large (e.g., 1), the loss might explode", "\n", "                ", "self", ".", "init_linear_arr", "[", "i", "]", ".", "bias", ".", "data", ".", "uniform_", "(", "-", ".5", ",", ".5", ")", "\n", "self", ".", "init_linear_arr", "[", "i", "]", ".", "weight", ".", "data", ".", "uniform_", "(", "-", ".1", ",", ".1", ")", "\n", "", "input_size", "=", "linear_mapping_dim", "\n", "", "elif", "positional_option", "==", "'cat'", ":", "\n", "            ", "position_emb_size", "=", "100", "\n", "self", ".", "poistion_emb", "=", "nn", ".", "Embedding", "(", "n_basis", ",", "position_emb_size", ")", "\n", "self", ".", "linear_keep_same_dim", "=", "nn", ".", "Linear", "(", "position_emb_size", "+", "input_size", ",", "nhid", ")", "\n", "#input_size = position_emb_size + ninp", "\n", "#input_size = ninp", "\n", "", "elif", "positional_option", "==", "'add'", ":", "\n", "#input_size = ninp", "\n", "            ", "add_position_emb", "=", "True", "\n", "if", "model_type_list", "[", "0", "]", "==", "'LSTM'", ":", "\n", "                ", "self", ".", "poistion_emb", "=", "nn", ".", "Embedding", "(", "n_basis", ",", "input_size", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "scale_factor", "=", "math", ".", "sqrt", "(", "input_size", ")", "\n", "\n", "#self.relu_layer = nn.ReLU()", "\n", "\n", "", "", "self", ".", "positional_option", "=", "positional_option", "\n", "self", ".", "dep_learner", "=", "ext_emb_to_seq", "(", "model_type_list", ",", "nhid", ",", "input_size", ",", "nhid", ",", "nlayers", ",", "n_basis", ",", "trans_layers", ",", "using_memory", ",", "add_position_emb", ",", "dropout_prob_trans", ")", "\n", "\n", "self", ".", "trans_dim", "=", "self", ".", "dep_learner", ".", "trans_dim", "\n", "\n", "\n", "#self.out_linear = nn.Linear(nhid, outd, bias=False)", "\n", "self", ".", "out_linear", "=", "nn", ".", "Linear", "(", "self", ".", "dep_learner", ".", "output_dim", ",", "outd", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.EMB2SEQ.init_weights": [[183, 188], ["model.EMB2SEQ.out_linear.bias.data.zero_", "model.EMB2SEQ.out_linear.weight.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "#necessary?", "\n", "        ", "initrange", "=", "0.1", "\n", "self", ".", "out_linear", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "self", ".", "out_linear", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.EMB2SEQ.forward": [[189, 238], ["input_init.expand", "model.EMB2SEQ.dep_learner", "model.EMB2SEQ.out_linear", "model.EMB2SEQ.permute", "input_init.expand.size", "input_init.expand.size", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "torch.arange().expand().permute", "poistion_emb", "drop", "input_init.size", "input_init.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.EMB2SEQ.drop", "model.EMB2SEQ.in_linear_mem", "model.EMB2SEQ.forward.prepare_posi_emb"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_init", ",", "memory", "=", "None", ",", "memory_attention_mask", "=", "None", ")", ":", "\n", "#print(input_init.size())", "\n", "        ", "def", "prepare_posi_emb", "(", "input", ",", "poistion_emb", ",", "drop", ")", ":", "\n", "            ", "batch_size", "=", "input", ".", "size", "(", "1", ")", "\n", "n_basis", "=", "input", ".", "size", "(", "0", ")", "\n", "input_pos", "=", "torch", ".", "arange", "(", "n_basis", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input", ".", "get_device", "(", ")", ")", ".", "expand", "(", "batch_size", ",", "n_basis", ")", ".", "permute", "(", "1", ",", "0", ")", "\n", "poistion_emb_input", "=", "poistion_emb", "(", "input_pos", ")", "\n", "poistion_emb_input", "=", "drop", "(", "poistion_emb_input", ")", "\n", "return", "poistion_emb_input", "\n", "\n", "#input_init_small = in_linear(input_init)", "\n", "#input = input_init_small.expand(self.n_basis, input_init_small.size(0), input_init_small.size(1) )", "\n", "", "input", "=", "input_init", ".", "expand", "(", "self", ".", "n_basis", ",", "input_init", ".", "size", "(", "0", ")", ",", "input_init", ".", "size", "(", "1", ")", ")", "\n", "#if self.n_basis == 0:", "\n", "#    emb = input", "\n", "#else:", "\n", "#    if self.linear_mapping_dim > 0:", "\n", "if", "self", ".", "positional_option", "==", "'linear'", ":", "\n", "            ", "emb_raw", "=", "torch", ".", "cat", "(", "[", "self", ".", "init_linear_arr", "[", "i", "]", "(", "input_init", ")", ".", "unsqueeze", "(", "dim", "=", "0", ")", "for", "i", "in", "range", "(", "self", ".", "n_basis", ")", "]", ",", "dim", "=", "0", ")", "\n", "#emb = emb_raw", "\n", "emb", "=", "self", ".", "drop", "(", "emb_raw", ")", "\n", "", "elif", "self", ".", "positional_option", "==", "'cat'", ":", "\n", "#batch_size = input.size(1)", "\n", "#input_pos = torch.arange(self.n_basis,dtype=torch.long,device = input.get_device()).expand(batch_size,self.n_basis).permute(1,0)", "\n", "\n", "#poistion_emb_input = self.poistion_emb(input_pos)", "\n", "#poistion_emb_input = self.drop(poistion_emb_input)", "\n", "            ", "poistion_emb_input", "=", "prepare_posi_emb", "(", "input", ",", "self", ".", "poistion_emb", ",", "self", ".", "drop", ")", "\n", "emb", "=", "torch", ".", "cat", "(", "(", "poistion_emb_input", ",", "input", ")", ",", "dim", "=", "2", ")", "\n", "emb", "=", "self", ".", "linear_keep_same_dim", "(", "emb", ")", "\n", "", "elif", "self", ".", "positional_option", "==", "'add'", ":", "\n", "            ", "if", "self", ".", "dep_learner", ".", "decoder_array", "[", "0", "]", ".", "model_type", "==", "\"LSTM\"", ":", "\n", "                ", "poistion_emb_input", "=", "prepare_posi_emb", "(", "input", ",", "self", ".", "poistion_emb", ",", "self", ".", "drop", ")", "\n", "emb", "=", "input", "+", "poistion_emb_input", "\n", "", "else", ":", "\n", "                ", "emb", "=", "input", "*", "self", ".", "scale_factor", "\n", "\n", "", "", "if", "memory", "is", "not", "None", ":", "\n", "            ", "memory", "=", "self", ".", "in_linear_mem", "(", "memory", ")", "\n", "", "output", "=", "self", ".", "dep_learner", "(", "input_init", ",", "emb", ",", "memory", ",", "memory_attention_mask", ")", "\n", "#output = self.drop(output)", "\n", "#output = self.out_linear(self.relu_layer(output))", "\n", "#output = self.layernorm(output.permute(1,0,2)).permute(1,0,2)", "\n", "#output /= self.outd_sqrt", "\n", "output", "=", "self", ".", "out_linear", "(", "output", ")", "\n", "#output = output / (0.000000000001 + output.norm(dim = 2, keepdim=True) )", "\n", "output_batch_first", "=", "output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "\n", "return", "output_batch_first", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Dictionary.__init__": [[21, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "byte_mode", "=", "False", ")", ":", "\n", "        ", "self", ".", "w_d2_ind", "=", "w_d2_ind_init", "\n", "self", ".", "ind_l2_w_freq", "=", "ind_l2_w_freq_init", "\n", "self", ".", "num_special_token", "=", "num_special_token", "\n", "self", ".", "UNK_IND", "=", "UNK_IND", "\n", "self", ".", "EOS_IND", "=", "EOS_IND", "\n", "self", ".", "byte_mode", "=", "byte_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Dictionary.dict_check_add": [[29, 41], ["len", "utils.Dictionary.ind_l2_w_freq.append", "utils.Dictionary.ind_l2_w_freq.append", "w.decode"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "dict_check_add", "(", "self", ",", "w", ")", ":", "\n", "        ", "if", "w", "not", "in", "self", ".", "w_d2_ind", ":", "\n", "            ", "w_ind", "=", "len", "(", "self", ".", "w_d2_ind", ")", "\n", "self", ".", "w_d2_ind", "[", "w", "]", "=", "w_ind", "\n", "if", "self", ".", "byte_mode", ":", "\n", "                ", "self", ".", "ind_l2_w_freq", ".", "append", "(", "[", "w", ".", "decode", "(", "'utf-8'", ")", ",", "1", ",", "w_ind", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "ind_l2_w_freq", ".", "append", "(", "[", "w", ",", "1", ",", "w_ind", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "w_ind", "=", "self", ".", "w_d2_ind", "[", "w", "]", "\n", "self", ".", "ind_l2_w_freq", "[", "w_ind", "]", "[", "1", "]", "+=", "1", "\n", "", "return", "w_ind", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Dictionary.append_eos": [[42, 45], ["w_ind_list.append"], "methods", ["None"], ["", "def", "append_eos", "(", "self", ",", "w_ind_list", ")", ":", "\n", "        ", "w_ind_list", ".", "append", "(", "self", ".", "EOS_IND", ")", "# append <eos>", "\n", "self", ".", "ind_l2_w_freq", "[", "self", ".", "EOS_IND", "]", "[", "1", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Dictionary.densify_index": [[46, 78], ["len", "range", "range", "print", "utils.Dictionary.ind_l2_w_freq[].append"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "densify_index", "(", "self", ",", "min_freq", ")", ":", "\n", "        ", "vocab_size", "=", "len", "(", "self", ".", "ind_l2_w_freq", ")", "\n", "compact_mapping", "=", "[", "0", "]", "*", "vocab_size", "\n", "for", "i", "in", "range", "(", "self", ".", "num_special_token", ")", ":", "\n", "            ", "compact_mapping", "[", "i", "]", "=", "i", "\n", "#compact_mapping[1] = 1", "\n", "#compact_mapping[2] = 2", "\n", "\n", "#total_num_filtering = 0", "\n", "", "total_freq_filtering", "=", "0", "\n", "current_new_idx", "=", "self", ".", "num_special_token", "\n", "\n", "#for i, (w, w_freq, w_ind_org) in enumerate(self.ind_l2_w_freq[self.num_special_token:]):", "\n", "for", "i", "in", "range", "(", "self", ".", "num_special_token", ",", "vocab_size", ")", ":", "\n", "            ", "w", ",", "w_freq", ",", "w_ind_org", "=", "self", ".", "ind_l2_w_freq", "[", "i", "]", "\n", "if", "w_freq", "<", "min_freq", ":", "\n", "                ", "compact_mapping", "[", "i", "]", "=", "self", ".", "UNK_IND", "\n", "self", ".", "ind_l2_w_freq", "[", "i", "]", "[", "-", "1", "]", "=", "self", ".", "UNK_IND", "\n", "self", ".", "ind_l2_w_freq", "[", "i", "]", ".", "append", "(", "'unk'", ")", "\n", "#total_num_filtering += 1", "\n", "total_freq_filtering", "+=", "w_freq", "\n", "", "else", ":", "\n", "                ", "compact_mapping", "[", "i", "]", "=", "current_new_idx", "\n", "self", ".", "ind_l2_w_freq", "[", "i", "]", "[", "-", "1", "]", "=", "current_new_idx", "\n", "current_new_idx", "+=", "1", "\n", "\n", "", "", "self", ".", "ind_l2_w_freq", "[", "self", ".", "UNK_IND", "]", "[", "1", "]", "=", "total_freq_filtering", "#update <unk> frequency", "\n", "\n", "\n", "print", "(", "\"{}/{} word types are filtered\"", ".", "format", "(", "vocab_size", "-", "current_new_idx", ",", "vocab_size", ")", ")", "\n", "\n", "return", "compact_mapping", ",", "total_freq_filtering", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Dictionary.store_dict": [[79, 86], ["len", "range", "str", "str", "f_out.write"], "methods", ["None"], ["", "def", "store_dict", "(", "self", ",", "f_out", ")", ":", "\n", "        ", "vocab_size", "=", "len", "(", "self", ".", "ind_l2_w_freq", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", ")", ":", "\n", "#print(ind_l2_w_freq[i])", "\n", "            ", "self", ".", "ind_l2_w_freq", "[", "i", "]", "[", "1", "]", "=", "str", "(", "self", ".", "ind_l2_w_freq", "[", "i", "]", "[", "1", "]", ")", "\n", "self", ".", "ind_l2_w_freq", "[", "i", "]", "[", "2", "]", "=", "str", "(", "self", ".", "ind_l2_w_freq", "[", "i", "]", "[", "2", "]", ")", "\n", "f_out", ".", "write", "(", "'\\t'", ".", "join", "(", "self", ".", "ind_l2_w_freq", "[", "i", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Condition_Seq2PairDataset.__init__": [[91, 100], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", ",", "bptt", ",", "n_further", ",", "dilated_head_span_no_use", ",", "random_start_no_use", ",", "device", ")", ":", "\n", "        ", "self", ".", "w_ind_gpt2", "=", "w_ind_gpt2_tensor", "\n", "self", ".", "w_ind_spacy", "=", "w_ind_spacy_tensor", "\n", "self", ".", "idx_gpt2_to_spacy", "=", "idx_gpt2_to_spacy_tensor", "\n", "self", ".", "n_further", "=", "n_further", "\n", "self", ".", "seq_len", "=", "bptt", "\n", "#self.dilated_head_span = dilated_head_span", "\n", "#self.head_num = int(bptt / dilated_head_span) + 2", "\n", "self", ".", "output_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Condition_Seq2PairDataset.__len__": [[101, 103], ["int", "utils.Condition_Seq2PairDataset.w_ind_gpt2.size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "int", "(", "(", "self", ".", "w_ind_gpt2", ".", "size", "(", "0", ")", "-", "self", ".", "seq_len", ")", "/", "self", ".", "seq_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Condition_Seq2PairDataset.__getitem__": [[104, 149], ["utils.Condition_Seq2PairDataset.w_ind_gpt2[].to", "utils.Condition_Seq2PairDataset.idx_gpt2_to_spacy[].to", "min", "target_small.to.to.size", "len", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "target_small.to.to.to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "feature", "=", "self", ".", "w_ind_gpt2", "[", "idx", "*", "self", ".", "seq_len", ":", "(", "idx", "+", "1", ")", "*", "self", ".", "seq_len", "]", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "idx_gpt2_to_spacy_small", "=", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", ":", "(", "idx", "+", "1", ")", "*", "self", ".", "seq_len", "]", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "idx_gpt2_to_spacy_small", "=", "idx_gpt2_to_spacy_small", "-", "idx_gpt2_to_spacy_small", "[", "0", "]", "\n", "#init_head_posi = random.randint(0, self.dilated_head_span - 1)", "\n", "#inner_idx_tensor = torch.empty(self.head_num, dtype = torch.long, device = self.output_device)", "\n", "#future_mask = torch.zeros( (self.head_num, self.seq_len), dtype = torch.float, device = self.output_device)", "\n", "#for i in range(self.head_num):", "\n", "#    inner_idx = min(self.seq_len-1, i*self.dilated_head_span+init_head_posi)", "\n", "#    inner_idx_tensor[i] = inner_idx", "\n", "#    future_mask[i,:inner_idx+1] = 1", "\n", "\n", "if", "self", ".", "idx_gpt2_to_spacy", "is", "None", "or", "self", ".", "n_further", "<", "0", ":", "\n", "            ", "target_unfold", "=", "[", "]", "\n", "", "else", ":", "\n", "#spacy_idx = torch.empty( (self.head_num, self.n_further), dtype = torch.long, device = self.output_device)", "\n", "#for i in range(self.head_num):", "\n", "#    inner_idx = inner_idx_tensor[i]", "\n", "#    spacy_idx[i,:] = torch.tensor(list(range( self.idx_gpt2_to_spacy[idx*self.seq_len+inner_idx] - self.idx_gpt2_to_spacy[idx*self.seq_len], self.idx_gpt2_to_spacy[idx*self.seq_len+inner_idx] - self.idx_gpt2_to_spacy[idx*self.seq_len] + self.n_further )), dtype = torch.long, device = self.output_device)", "\n", "\n", "            ", "start", "=", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", "]", "+", "2", "\n", "end", "=", "min", "(", "len", "(", "self", ".", "w_ind_spacy", ")", ",", "self", ".", "idx_gpt2_to_spacy", "[", "(", "idx", "+", "1", ")", "*", "self", ".", "seq_len", "]", "+", "2", "+", "self", ".", "n_further", ")", "\n", "target_small", "=", "self", ".", "w_ind_spacy", "[", "start", ":", "end", "]", "\n", "#handle the out of boundary case", "\n", "#oob_num = end - self.w_ind_spacy.size(0)", "\n", "#if oob_num >= 0:", "\n", "#    target_small = torch.cat( (self.w_ind_spacy[start:],torch.zeros(oob_num+1, dtype = torch.int) ), dim=0 ).to( dtype = torch.long, device = self.output_device)", "\n", "#else:", "\n", "#    target_small = self.w_ind_spacy[start:end].to( dtype = torch.long, device = self.output_device)", "\n", "target_size", "=", "target_small", ".", "size", "(", "0", ")", "\n", "add_0_num", "=", "self", ".", "seq_len", "+", "self", ".", "n_further", "-", "target_size", "+", "1000", "\n", "if", "add_0_num", ">", "0", ":", "\n", "                ", "target_small", "=", "torch", ".", "cat", "(", "(", "target_small", ",", "torch", ".", "zeros", "(", "add_0_num", ",", "dtype", "=", "torch", ".", "int", ")", ")", ",", "dim", "=", "0", ")", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "", "else", ":", "\n", "                ", "target_small", "=", "target_small", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "\n", "\n", "#target_unfold = target_small[spacy_idx]", "\n", "#try:", "\n", "#    target_unfold = target_small[spacy_idx]", "\n", "#except:", "\n", "#    print(target_small, spacy_idx, oob_num, start, end, idx, self.idx_gpt2_to_spacy[idx*self.seq_len])", "\n", "\n", "#return [feature, target_unfold, inner_idx_tensor, future_mask]", "\n", "", "", "return", "[", "feature", ",", "target_small", ",", "idx_gpt2_to_spacy_small", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.raw_sent_dataset.__init__": [[151, 157], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sent_list", ",", "type_idx_list", ",", "tokenizer_GPT2", ",", "bptt", ",", "device", ")", ":", "\n", "        ", "self", ".", "sent_list", "=", "sent_list", "\n", "self", ".", "tokenizer_GPT2", "=", "tokenizer_GPT2", "\n", "self", ".", "type_idx_list", "=", "type_idx_list", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.raw_sent_dataset.__len__": [[158, 160], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "sent_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.raw_sent_dataset.__getitem__": [[161, 173], ["utils.raw_sent_dataset.tokenizer_GPT2.encode", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "sent", "=", "self", ".", "sent_list", "[", "idx", "]", "\n", "sent_idx_arr", "=", "self", ".", "tokenizer_GPT2", ".", "encode", "(", "sent", ")", "\n", "sent_len", "=", "len", "(", "sent_idx_arr", ")", "\n", "start_idx", "=", "sent_len", "-", "self", ".", "bptt", "\n", "if", "start_idx", ">", "0", ":", "\n", "            ", "sent_idx_arr", "=", "sent_idx_arr", "[", "start_idx", ":", "]", "\n", "sent_len", "=", "self", ".", "bptt", "\n", "", "feature", "=", "torch", ".", "zeros", "(", "self", ".", "bptt", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "feature", "[", ":", "sent_len", "]", "=", "torch", ".", "tensor", "(", "sent_idx_arr", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "inner_idx_tensor", "=", "torch", ".", "tensor", "(", "sent_len", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "return", "feature", ",", "inner_idx_tensor", ",", "self", ".", "type_idx_list", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Seq2PairDataset.__init__": [[176, 186], ["int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "random_start", ",", "device", ")", ":", "\n", "        ", "self", ".", "w_ind_gpt2", "=", "w_ind_gpt2_tensor", "\n", "self", ".", "w_ind_spacy", "=", "w_ind_spacy_tensor", "\n", "self", ".", "idx_gpt2_to_spacy", "=", "idx_gpt2_to_spacy_tensor", "\n", "self", ".", "n_further", "=", "n_further", "\n", "self", ".", "seq_len", "=", "bptt", "\n", "self", ".", "dilated_head_span", "=", "dilated_head_span", "\n", "self", ".", "head_num", "=", "int", "(", "bptt", "/", "dilated_head_span", ")", "+", "2", "\n", "self", ".", "random_start", "=", "random_start", "\n", "self", ".", "output_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Seq2PairDataset.__len__": [[187, 189], ["int", "utils.Seq2PairDataset.w_ind_gpt2.size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "int", "(", "(", "self", ".", "w_ind_gpt2", ".", "size", "(", "0", ")", "-", "self", ".", "seq_len", ")", "/", "self", ".", "seq_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.Seq2PairDataset.__getitem__": [[190, 237], ["utils.Seq2PairDataset.w_ind_gpt2[].to", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "random.randint", "int", "min", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.Seq2PairDataset.w_ind_spacy.size", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "utils.Seq2PairDataset.w_ind_spacy[].to", "list", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "feature", "=", "self", ".", "w_ind_gpt2", "[", "idx", "*", "self", ".", "seq_len", ":", "(", "idx", "+", "1", ")", "*", "self", ".", "seq_len", "]", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "if", "self", ".", "random_start", ":", "\n", "            ", "init_head_posi", "=", "random", ".", "randint", "(", "1", ",", "self", ".", "dilated_head_span", "-", "1", ")", "\n", "", "else", ":", "\n", "#init_head_posi = 20", "\n", "            ", "init_head_posi", "=", "int", "(", "self", ".", "dilated_head_span", "/", "2", ")", "\n", "", "inner_idx_tensor", "=", "torch", ".", "empty", "(", "self", ".", "head_num", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "future_mask", "=", "torch", ".", "zeros", "(", "(", "self", ".", "head_num", ",", "self", ".", "seq_len", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "self", ".", "output_device", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "head_num", ")", ":", "\n", "            ", "inner_idx", "=", "min", "(", "self", ".", "seq_len", "-", "1", ",", "i", "*", "self", ".", "dilated_head_span", "+", "init_head_posi", ")", "\n", "inner_idx_tensor", "[", "i", "]", "=", "inner_idx", "\n", "future_mask", "[", "i", ",", ":", "inner_idx", "+", "1", "]", "=", "1", "\n", "\n", "", "if", "self", ".", "idx_gpt2_to_spacy", "is", "None", "or", "self", ".", "n_further", "<", "0", ":", "\n", "            ", "target_unfold", "=", "[", "]", "\n", "#idx_feature_to_target = []", "\n", "#inner_idx_tensor = []", "\n", "#future_mask = []", "\n", "", "else", ":", "\n", "            ", "spacy_idx", "=", "torch", ".", "empty", "(", "(", "self", ".", "head_num", ",", "self", ".", "n_further", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "#spacy_idx = torch.empty( (self.head_num, self.n_further), dtype = torch.long)", "\n", "#inner_idx_arr = []", "\n", "#future_mask = torch.ones( (self.head_num, self.seq_len), dtype = torch.bool, device = self.output_device)", "\n", "for", "i", "in", "range", "(", "self", ".", "head_num", ")", ":", "\n", "                ", "inner_idx", "=", "inner_idx_tensor", "[", "i", "]", "\n", "spacy_idx", "[", "i", ",", ":", "]", "=", "torch", ".", "tensor", "(", "list", "(", "range", "(", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", "+", "inner_idx", "]", "-", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", "]", ",", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", "+", "inner_idx", "]", "-", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", "]", "+", "self", ".", "n_further", ")", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "\n", "", "start", "=", "self", ".", "idx_gpt2_to_spacy", "[", "idx", "*", "self", ".", "seq_len", "]", "+", "1", "\n", "end", "=", "self", ".", "idx_gpt2_to_spacy", "[", "(", "idx", "+", "1", ")", "*", "self", ".", "seq_len", "]", "+", "1", "+", "self", ".", "n_further", "\n", "#handle the out of boundary case", "\n", "oob_num", "=", "end", "-", "self", ".", "w_ind_spacy", ".", "size", "(", "0", ")", "\n", "if", "oob_num", ">=", "0", ":", "\n", "                ", "target_small", "=", "torch", ".", "cat", "(", "(", "self", ".", "w_ind_spacy", "[", "start", ":", "]", ",", "torch", ".", "zeros", "(", "oob_num", "+", "1", ",", "dtype", "=", "torch", ".", "int", ")", ")", ",", "dim", "=", "0", ")", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "", "else", ":", "\n", "                ", "target_small", "=", "self", ".", "w_ind_spacy", "[", "start", ":", "end", "]", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "output_device", ")", "\n", "#print(target_small, spacy_idx)", "\n", "", "target_unfold", "=", "target_small", "[", "spacy_idx", "]", "\n", "#try:", "\n", "#    target_unfold = target_small[spacy_idx]", "\n", "#except:", "\n", "#    print(target_small, spacy_idx, oob_num, start, end, idx, self.idx_gpt2_to_spacy[idx*self.seq_len])", "\n", "#idx_feature_to_target = self.idx_gpt2_to_spacy[idx:idx+self.seq_len].to(dtype = torch.long, device = self.output_device)", "\n", "#idx_feature_to_target -= idx_feature_to_target[0]", "\n", "\n", "#debug target[-1] = idx", "\n", "", "return", "[", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_idx2word_freq": [[247, 256], ["enumerate", "line.rstrip().split", "len", "idx2word_freq.append", "line.rstrip", "len", "int", "int"], "function", ["None"], ["", "", "def", "load_idx2word_freq", "(", "f_in", ")", ":", "\n", "    ", "idx2word_freq", "=", "[", "]", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "f_in", ")", ":", "\n", "        ", "fields", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "fields", ")", "==", "3", ":", "\n", "            ", "assert", "len", "(", "idx2word_freq", ")", "==", "int", "(", "fields", "[", "2", "]", ")", "\n", "idx2word_freq", ".", "append", "(", "[", "fields", "[", "0", "]", ",", "int", "(", "fields", "[", "1", "]", ")", "]", ")", "\n", "\n", "", "", "return", "idx2word_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.create_data_loader_split": [[257, 281], ["torch.load", "torch.load", "torch.load", "w_ind_gpt2_tensor.size", "int", "range", "dataset_arr.append", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataset_class", "range"], "function", ["None"], ["", "def", "create_data_loader_split", "(", "f_in", ",", "bsz", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "device", ",", "split_num", ",", "dataset_class", ")", ":", "\n", "    ", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", "=", "torch", ".", "load", "(", "f_in", ",", "map_location", "=", "'cpu'", ")", "\n", "#print(w_ind_gpt2_tensor.size(), w_ind_spacy_tensor.size(), idx_gpt2_to_spacy_tensor.size())", "\n", "training_size", "=", "w_ind_gpt2_tensor", ".", "size", "(", "0", ")", "\n", "#idx_arr = np.random.permutation(training_size)", "\n", "split_size", "=", "int", "(", "training_size", "/", "split_num", ")", "\n", "dataset_arr", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "split_num", ")", ":", "\n", "        ", "start", "=", "i", "*", "split_size", "\n", "if", "i", "==", "split_num", "-", "1", ":", "\n", "            ", "end", "=", "training_size", "\n", "", "else", ":", "\n", "            ", "end", "=", "(", "i", "+", "1", ")", "*", "split_size", "\n", "", "start_idx_spacy", "=", "idx_gpt2_to_spacy_tensor", "[", "start", "]", "\n", "end_idx_spacy", "=", "idx_gpt2_to_spacy_tensor", "[", "end", "-", "1", "]", "\n", "random_start", "=", "True", "\n", "dataset_arr", ".", "append", "(", "dataset_class", "(", "w_ind_gpt2_tensor", "[", "start", ":", "end", "]", ",", "w_ind_spacy_tensor", "[", "start_idx_spacy", ":", "end_idx_spacy", "]", ",", "idx_gpt2_to_spacy_tensor", "[", "start", ":", "end", "]", "-", "start_idx_spacy", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "random_start", ",", "device", ")", ")", "#assume that the dataset are randomly shuffled beforehand", "\n", "\n", "", "use_cuda", "=", "False", "\n", "if", "device", ".", "type", "==", "'cuda'", ":", "\n", "        ", "use_cuda", "=", "True", "\n", "#dataloader_arr = [torch.utils.data.DataLoader(dataset_arr[i], batch_size = bsz, shuffle = True, pin_memory=use_cuda, drop_last=False) for i in range(split_num)]", "\n", "", "dataloader_arr", "=", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset_arr", "[", "i", "]", ",", "batch_size", "=", "bsz", ",", "shuffle", "=", "True", ",", "pin_memory", "=", "not", "use_cuda", ",", "drop_last", "=", "True", ")", "for", "i", "in", "range", "(", "split_num", ")", "]", "\n", "return", "dataloader_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.create_data_loader": [[282, 290], ["torch.load", "torch.load", "torch.load", "dataset_class", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "function", ["None"], ["", "def", "create_data_loader", "(", "f_in", ",", "bsz", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "device", ",", "dataset_class", ",", "want_to_shuffle", "=", "True", ",", "random_start", "=", "True", ")", ":", "\n", "    ", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", "=", "torch", ".", "load", "(", "f_in", ",", "map_location", "=", "'cpu'", ")", "\n", "dataset", "=", "dataset_class", "(", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "random_start", ",", "device", ")", "\n", "use_cuda", "=", "False", "\n", "if", "device", ".", "type", "==", "'cuda'", ":", "\n", "        ", "use_cuda", "=", "True", "\n", "#return torch.utils.data.DataLoader(dataset, batch_size = bsz, shuffle = want_to_shuffle, pin_memory=use_cuda, drop_last=False)", "\n", "", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "batch_size", "=", "bsz", ",", "shuffle", "=", "want_to_shuffle", ",", "pin_memory", "=", "not", "use_cuda", ",", "drop_last", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_corpus": [[292, 323], ["open", "utils.load_idx2word_freq", "open", "utils.create_data_loader", "open", "utils.create_data_loader", "open", "utils.create_data_loader_split"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_idx2word_freq", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.create_data_loader", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.create_data_loader", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.create_data_loader_split"], ["", "def", "load_corpus", "(", "data_path", ",", "train_bsz", ",", "eval_bsz", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "device", ",", "tensor_folder", "=", "\"tensors\"", ",", "split_num", "=", "1", ",", "skip_training", "=", "False", ",", "want_to_shuffle_val", "=", "False", ",", "condition", "=", "False", ",", "load_testing", "=", "False", ",", "random_start", "=", "True", ")", ":", "\n", "    ", "train_corpus_name", "=", "data_path", "+", "\"/\"", "+", "tensor_folder", "+", "\"/train.pt\"", "\n", "val_org_corpus_name", "=", "data_path", "+", "\"/\"", "+", "tensor_folder", "+", "\"/val_org.pt\"", "\n", "dictionary_input_name", "=", "data_path", "+", "\"/\"", "+", "tensor_folder", "+", "\"/dict_idx_compact\"", "\n", "\n", "if", "condition", ":", "\n", "        ", "dataset_class", "=", "Condition_Seq2PairDataset", "\n", "", "else", ":", "\n", "        ", "dataset_class", "=", "Seq2PairDataset", "\n", "\n", "", "with", "open", "(", "dictionary_input_name", ")", "as", "f_in", ":", "\n", "        ", "idx2word_freq", "=", "load_idx2word_freq", "(", "f_in", ")", "\n", "\n", "", "with", "open", "(", "val_org_corpus_name", ",", "'rb'", ")", "as", "f_in", ":", "\n", "        ", "dataloader_val", "=", "create_data_loader", "(", "f_in", ",", "eval_bsz", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "device", ",", "dataset_class", ",", "want_to_shuffle", "=", "want_to_shuffle_val", ",", "random_start", "=", "random_start", ")", "\n", "\n", "", "if", "load_testing", ":", "\n", "        ", "test_org_corpus_name", "=", "data_path", "+", "\"/\"", "+", "tensor_folder", "+", "\"/test_org.pt\"", "\n", "with", "open", "(", "test_org_corpus_name", ",", "'rb'", ")", "as", "f_in", ":", "\n", "            ", "dataloader_test", "=", "create_data_loader", "(", "f_in", ",", "eval_bsz", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "device", ",", "dataset_class", ",", "want_to_shuffle", "=", "want_to_shuffle_val", ",", "random_start", "=", "random_start", ")", "\n", "\n", "", "", "if", "skip_training", ":", "\n", "        ", "dataloader_train_arr", "=", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "train_corpus_name", ",", "'rb'", ")", "as", "f_in", ":", "\n", "            ", "dataloader_train_arr", "=", "create_data_loader_split", "(", "f_in", ",", "train_bsz", ",", "bptt", ",", "n_further", ",", "dilated_head_span", ",", "device", ",", "split_num", ",", "dataset_class", ")", "\n", "\n", "", "", "if", "load_testing", ":", "\n", "        ", "return", "idx2word_freq", ",", "dataloader_train_arr", ",", "dataloader_val", ",", "dataloader_test", "\n", "", "else", ":", "\n", "        ", "return", "idx2word_freq", ",", "dataloader_train_arr", ",", "dataloader_val", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_emb_from_path": [[324, 331], ["torch.load", "torch.load", "torch.load", "torch.load.size", "load_emb_file_tensor"], "function", ["None"], ["", "", "def", "load_emb_from_path", "(", "emb_file_path", ",", "device", ",", "idx2word_freq", ")", ":", "\n", "    ", "if", "emb_file_path", "[", "-", "3", ":", "]", "==", "'.pt'", ":", "\n", "        ", "word_emb", "=", "torch", ".", "load", "(", "emb_file_path", ",", "map_location", "=", "device", ")", "\n", "output_emb_size", "=", "word_emb", ".", "size", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "word_emb", ",", "output_emb_size", ",", "oov_list", "=", "load_emb_file_tensor", "(", "emb_file_path", ",", "device", ",", "idx2word_freq", ")", "\n", "", "return", "word_emb", ",", "output_emb_size", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.loading_all_models": [[332, 359], ["utils.load_emb_from_path", "torch.load", "torch.load", "torch.load", "gpt2_model.modeling_gpt2.GPT2Model.from_pretrained", "gpt2_model.configuration_gpt2.GPT2Config.from_pretrained", "model.EMB2SEQ", "model_code.EMB2SEQ.load_state_dict", "utils.output_parallel_models", "os.path.join", "args.de_model.split", "torch.load", "torch.load", "torch.load", "os.path.join", "word_emb.norm"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_emb_from_path", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.output_parallel_models"], ["", "def", "loading_all_models", "(", "args", ",", "idx2word_freq", ",", "device", ")", ":", "\n", "\n", "    ", "word_emb", ",", "output_emb_size", "=", "load_emb_from_path", "(", "args", ".", "emb_file", ",", "device", ",", "idx2word_freq", ")", "\n", "\n", "model_name", "=", "'distilgpt2'", "\n", "\n", "encoder_state_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint_topics", ",", "'encoder.pt'", ")", ",", "map_location", "=", "device", ")", "\n", "encoder", "=", "GPT2Model", ".", "from_pretrained", "(", "model_name", ",", "state_dict", "=", "encoder_state_dict", ")", "\n", "gpt2_config", "=", "GPT2Config", ".", "from_pretrained", "(", "model_name", ")", "\n", "#encoder = model_code.SEQ2EMB(args.en_model.split('+'), ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouti, args.dropoute, max_sent_len,  external_emb, [], trans_layers = args.encode_trans_layers, trans_nhid = args.trans_nhid) ", "\n", "\n", "if", "args", ".", "nhidlast2", "<", "0", ":", "\n", "        ", "args", ".", "nhidlast2", "=", "encoder", ".", "output_dim", "\n", "", "decoder", "=", "model_code", ".", "EMB2SEQ", "(", "args", ".", "de_model", ".", "split", "(", "'+'", ")", ",", "gpt2_config", ".", "n_embd", ",", "args", ".", "nhidlast2", ",", "output_emb_size", ",", "1", ",", "args", ".", "n_basis", ",", "positional_option", "=", "args", ".", "positional_option", ",", "dropoutp", "=", "args", ".", "dropoutp", ",", "trans_layers", "=", "args", ".", "trans_layers", ",", "using_memory", "=", "args", ".", "de_en_connection", ",", "dropout_prob_trans", "=", "args", ".", "dropout_prob_trans", ")", "\n", "\n", "#encoder.load_state_dict(torch.load(os.path.join(args.checkpoint, 'encoder.pt'), map_location=device))", "\n", "decoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint_topics", ",", "'decoder.pt'", ")", ",", "map_location", "=", "device", ")", ")", "\n", "\n", "#if len(args.emb_file) == 0:", "\n", "#    word_emb = encoder.encoder.weight.detach()", "\n", "\n", "word_norm_emb", "=", "word_emb", "/", "(", "0.000000000001", "+", "word_emb", ".", "norm", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "word_norm_emb", "[", "0", ",", ":", "]", "=", "0", "\n", "\n", "parallel_encoder", ",", "parallel_decoder", "=", "output_parallel_models", "(", "args", ".", "cuda_topics", ",", "args", ".", "single_gpu", ",", "encoder", ",", "decoder", ")", "\n", "\n", "return", "parallel_encoder", ",", "parallel_decoder", ",", "encoder", ",", "decoder", ",", "word_norm_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.seed_all_randomness": [[360, 369], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "print", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "seed_all_randomness", "(", "seed", ",", "use_cuda", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "if", "not", "use_cuda", ":", "\n", "            ", "print", "(", "\"WARNING: You have a CUDA device, so you should probably run with --cuda\"", ")", "\n", "", "else", ":", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.output_parallel_models": [[370, 383], ["encoder.cuda", "decoder.cuda", "torch.DataParallel().cuda", "torch.DataParallel().cuda", "torch.DataParallel", "torch.DataParallel"], "function", ["None"], ["", "", "", "def", "output_parallel_models", "(", "use_cuda", ",", "single_gpu", ",", "encoder", ",", "decoder", ")", ":", "\n", "    ", "if", "use_cuda", ":", "\n", "        ", "if", "single_gpu", ":", "\n", "            ", "parallel_encoder", "=", "encoder", ".", "cuda", "(", ")", "\n", "parallel_decoder", "=", "decoder", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "            ", "parallel_encoder", "=", "nn", ".", "DataParallel", "(", "encoder", ",", "dim", "=", "0", ")", ".", "cuda", "(", ")", "\n", "parallel_decoder", "=", "nn", ".", "DataParallel", "(", "decoder", ",", "dim", "=", "0", ")", ".", "cuda", "(", ")", "\n", "#parallel_decoder = decoder.cuda()", "\n", "", "", "else", ":", "\n", "        ", "parallel_encoder", "=", "encoder", "\n", "parallel_decoder", "=", "decoder", "\n", "", "return", "parallel_encoder", ",", "parallel_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_emb_file_to_dict": [[384, 407], ["open", "line.rstrip().split", "len", "len", "float", "numpy.array", "word.lower", "line.rstrip"], "function", ["None"], ["", "def", "load_emb_file_to_dict", "(", "emb_file", ",", "lowercase_emb", "=", "False", ",", "convert_np", "=", "True", ")", ":", "\n", "    ", "word2emb", "=", "{", "}", "\n", "with", "open", "(", "emb_file", ")", "as", "f_in", ":", "\n", "        ", "for", "line", "in", "f_in", ":", "\n", "            ", "word_val", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "word_val", ")", "<", "3", ":", "\n", "                ", "continue", "\n", "", "word", "=", "word_val", "[", "0", "]", "\n", "#val = np.array([float(x) for x in  word_val[1:]])", "\n", "val", "=", "[", "float", "(", "x", ")", "for", "x", "in", "word_val", "[", "1", ":", "]", "]", "\n", "if", "convert_np", ":", "\n", "                ", "val", "=", "np", ".", "array", "(", "val", ")", "\n", "", "if", "lowercase_emb", ":", "\n", "                ", "word_lower", "=", "word", ".", "lower", "(", ")", "\n", "if", "word_lower", "not", "in", "word2emb", ":", "\n", "                    ", "word2emb", "[", "word_lower", "]", "=", "val", "\n", "", "else", ":", "\n", "                    ", "if", "word", "==", "word_lower", ":", "\n", "                        ", "word2emb", "[", "word_lower", "]", "=", "val", "\n", "", "", "", "else", ":", "\n", "                ", "word2emb", "[", "word", "]", "=", "val", "\n", "", "emb_size", "=", "len", "(", "val", ")", "\n", "", "", "return", "word2emb", ",", "emb_size", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_emb_file_to_tensor": [[408, 434], ["utils.load_emb_file_to_dict", "len", "torch.empty", "torch.empty", "torch.empty", "range", "print", "print", "torch.tensor", "torch.tensor", "torch.tensor", "oov_list.append", "len", "float", "float"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_emb_file_to_dict", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "load_emb_file_to_tensor", "(", "emb_file", ",", "device", ",", "idx2word_freq", ")", ":", "\n", "    ", "word2emb", ",", "emb_size", "=", "load_emb_file_to_dict", "(", "emb_file", ",", "convert_np", "=", "False", ")", "\n", "num_w", "=", "len", "(", "idx2word_freq", ")", "\n", "#emb_size = len(word2emb.values()[0])", "\n", "#external_emb = torch.empty(num_w, emb_size, device = device, requires_grad = update_target_emb)", "\n", "external_emb", "=", "torch", ".", "empty", "(", "num_w", ",", "emb_size", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", "\n", "#OOV_num = 0", "\n", "OOV_freq", "=", "0", "\n", "total_freq", "=", "0", "\n", "oov_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_special_token", ",", "num_w", ")", ":", "\n", "        ", "w", "=", "idx2word_freq", "[", "i", "]", "[", "0", "]", "\n", "total_freq", "+=", "idx2word_freq", "[", "i", "]", "[", "1", "]", "\n", "if", "w", "in", "word2emb", ":", "\n", "            ", "val", "=", "torch", ".", "tensor", "(", "word2emb", "[", "w", "]", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", "\n", "#val = np.array(word2emb[w])", "\n", "external_emb", "[", "i", ",", ":", "]", "=", "val", "\n", "", "else", ":", "\n", "            ", "oov_list", ".", "append", "(", "i", ")", "\n", "external_emb", "[", "i", ",", ":", "]", "=", "0", "\n", "#OOV_num += 1", "\n", "OOV_freq", "+=", "idx2word_freq", "[", "i", "]", "[", "1", "]", "\n", "\n", "", "", "print", "(", "\"OOV word type percentage: {}%\"", ".", "format", "(", "len", "(", "oov_list", ")", "/", "float", "(", "num_w", ")", "*", "100", ")", ")", "\n", "print", "(", "\"OOV token percentage: {}%\"", ".", "format", "(", "OOV_freq", "/", "float", "(", "total_freq", ")", "*", "100", ")", ")", "\n", "return", "external_emb", ",", "emb_size", ",", "oov_list", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.create_exp_dir": [[435, 445], ["print", "os.path.exists", "os.mkdir", "os.mkdir", "os.path.join", "os.path.join", "shutil.copyfile", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "create_exp_dir", "(", "path", ",", "scripts_to_save", "=", "None", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "path", ")", "\n", "\n", "", "print", "(", "'Experiment dir : {}'", ".", "format", "(", "path", ")", ")", "\n", "if", "scripts_to_save", "is", "not", "None", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'scripts'", ")", ")", "\n", "for", "script", "in", "scripts_to_save", ":", "\n", "            ", "dst_file", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'scripts'", ",", "os", ".", "path", ".", "basename", "(", "script", ")", ")", "\n", "shutil", ".", "copyfile", "(", "script", ",", "dst_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.str2bool": [[446, 453], ["v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "", "", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'True'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'False'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.save_checkpoint": [[454, 464], ["torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "encoder.state_dict", "os.path.join", "torch.save", "torch.save", "torch.save", "external_emb.size", "torch.save", "torch.save", "torch.save", "optimizer_e.state_dict", "os.path.join", "optimizer_d.state_dict", "os.path.join", "decoder.state_dict", "os.path.join", "os.path.join"], "function", ["None"], ["", "", "def", "save_checkpoint", "(", "encoder", ",", "decoder", ",", "optimizer_e", ",", "optimizer_d", ",", "external_emb", ",", "path", ")", ":", "\n", "    ", "torch", ".", "save", "(", "encoder", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'encoder.pt'", ")", ")", "\n", "try", ":", "\n", "        ", "torch", ".", "save", "(", "decoder", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'decoder.pt'", ")", ")", "\n", "", "except", ":", "\n", "        ", "pass", "\n", "", "if", "external_emb", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "        ", "torch", ".", "save", "(", "external_emb", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'target_emb.pt'", ")", ")", "\n", "", "torch", ".", "save", "(", "optimizer_e", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'optimizer_e.pt'", ")", ")", "\n", "torch", ".", "save", "(", "optimizer_d", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'optimizer_d.pt'", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_topics.logging": [[134, 141], ["print", "sys.stdout.flush", "open", "f_log.write", "os.path.join"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["def", "logging", "(", "s", ",", "print_", "=", "True", ",", "log_", "=", "True", ")", ":", "\n", "    ", "if", "print_", ":", "\n", "        ", "print", "(", "s", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "if", "log_", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "save", ",", "'log.txt'", ")", ",", "'a+'", ")", "as", "f_log", ":", "\n", "            ", "f_log", ".", "write", "(", "s", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_topics.counter_to_tensor": [[163, 172], ["len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range"], "function", ["None"], ["def", "counter_to_tensor", "(", "idx2word_freq", ",", "device", ")", ":", "\n", "    ", "total", "=", "len", "(", "idx2word_freq", ")", "\n", "w_freq", "=", "torch", ".", "zeros", "(", "total", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "total", ")", ":", "\n", "        ", "w_freq", "[", "i", "]", "=", "1", "\n", "#w_freq[i] = math.sqrt(idx2word_freq[x][1])", "\n", "#w_freq[i] = idx2word_freq[x][1]", "\n", "", "w_freq", "[", "0", "]", "=", "-", "1", "\n", "return", "w_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_topics.evaluate": [[222, 276], ["encoder.eval", "decoder.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "target_unfold.view", "parallel_encoder", "future_mask.view.size", "output_emb.size", "output_emb.gather", "output_emb.gather.view", "nsd_loss.compute_loss_set", "feature.size", "total_loss.item", "len", "total_loss_set.item", "len", "total_loss_set_neg.item", "len", "total_loss_set_reg.item", "len", "total_loss_set_div.item", "len", "target_unfold.size", "output_emb.gather.size", "output_emb.unsqueeze().expand", "output_emb_masked.reshape.reshape", "future_mask.view.view", "parallel_decoder", "parallel_decoder", "inner_idx_tensor.unsqueeze().expand", "output_emb.unsqueeze", "inner_idx_tensor.unsqueeze"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.compute_loss_set"], ["def", "evaluate", "(", "dataloader", ",", "external_emb", ",", "current_coeff_opt", ")", ":", "\n", "# Turn on evaluation mode which disables dropout.", "\n", "    ", "encoder", ".", "eval", "(", ")", "\n", "decoder", ".", "eval", "(", ")", "\n", "total_loss", "=", "0", "\n", "total_loss_set", "=", "0", "\n", "total_loss_set_reg", "=", "0", "\n", "total_loss_set_div", "=", "0", "\n", "total_loss_set_neg", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "            ", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "=", "sample_batched", "\n", "target", "=", "target_unfold", ".", "view", "(", "-", "1", ",", "target_unfold", ".", "size", "(", "2", ")", ")", "\n", "#print(target)", "\n", "output_emb", ",", "past", "=", "parallel_encoder", "(", "feature", ")", "\n", "batch_size", ",", "num_head", ",", "seq_len", "=", "future_mask", ".", "size", "(", ")", "\n", "hidden_size", "=", "output_emb", ".", "size", "(", "2", ")", "\n", "output_emb_head", "=", "output_emb", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "inner_idx_tensor", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "num_head", ",", "hidden_size", ")", ")", "\n", "output_emb_last", "=", "output_emb_head", ".", "view", "(", "-", "1", ",", "output_emb_head", ".", "size", "(", "2", ")", ")", "\n", "\n", "#output_emb_masked = output_emb.unsqueeze(dim=1).expand(batch_size,num_head,seq_len,hidden_size)", "\n", "#output_emb_masked[future_mask.unsqueeze(dim=-1).expand( batch_size,num_head,seq_len,hidden_size)] = 0", "\n", "#output_emb_masked = output_emb_masked.view(-1, seq_len, hidden_size)", "\n", "if", "args", ".", "de_en_connection", ":", "\n", "                ", "output_emb_masked", "=", "output_emb", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "batch_size", ",", "num_head", ",", "seq_len", ",", "hidden_size", ")", "\n", "#output_emb_masked = output_emb.unsqueeze(dim=0).expand(num_head,batch_size,seq_len,hidden_size)", "\n", "#future_mask_expanded = future_mask.unsqueeze(dim=-1).expand( batch_size,num_head,seq_len,hidden_size)", "\n", "#output_emb_masked[future_mask_expanded] = 0", "\n", "#output_emb_masked = output_emb_masked.view(-1, seq_len, hidden_size)", "\n", "output_emb_masked", "=", "output_emb_masked", ".", "reshape", "(", "-", "1", ",", "seq_len", ",", "hidden_size", ")", "\n", "future_mask", "=", "future_mask", ".", "view", "(", "-", "1", ",", "seq_len", ")", "\n", "\n", "basis_pred", "=", "parallel_decoder", "(", "output_emb_last", ",", "output_emb_masked", ",", "memory_attention_mask", "=", "future_mask", ")", "\n", "", "else", ":", "\n", "                ", "basis_pred", "=", "parallel_decoder", "(", "output_emb_last", ")", "\n", "\n", "#output_emb, hidden, output_emb_last = parallel_encoder(feature.t())", "\n", "#output_emb_last, output_emb = parallel_encoder(feature)", "\n", "#basis_pred=  parallel_decoder(output_emb_last, output_emb)", "\n", "\n", "", "compute_target_grad", "=", "False", "\n", "loss_set", ",", "loss_set_reg", ",", "loss_set_div", ",", "loss_set_neg", "=", "nsd_loss", ".", "compute_loss_set", "(", "basis_pred", ",", "external_emb", ",", "target", ",", "args", ".", "L1_losss_B", ",", "device", ",", "w_freq", ",", "current_coeff_opt", ",", "compute_target_grad", ",", "args", ".", "coeff_opt_algo", ")", "\n", "#print(loss_set, loss_set_reg, loss_set_div, loss_set_neg)", "\n", "loss", "=", "loss_set", "+", "loss_set_neg", "\n", "batch_size", "=", "feature", ".", "size", "(", "0", ")", "\n", "total_loss", "+=", "loss", "*", "batch_size", "\n", "total_loss_set", "+=", "loss_set", "*", "batch_size", "\n", "total_loss_set_reg", "+=", "loss_set_reg", "*", "batch_size", "\n", "total_loss_set_div", "+=", "loss_set_div", "*", "batch_size", "\n", "total_loss_set_neg", "+=", "loss_set_neg", "*", "batch_size", "\n", "\n", "", "", "return", "total_loss", ".", "item", "(", ")", "/", "len", "(", "dataloader", ".", "dataset", ")", ",", "total_loss_set", ".", "item", "(", ")", "/", "len", "(", "dataloader", ".", "dataset", ")", ",", "total_loss_set_neg", ".", "item", "(", ")", "/", "len", "(", "dataloader", ".", "dataset", ")", ",", "total_loss_set_reg", ".", "item", "(", ")", "/", "len", "(", "dataloader", ".", "dataset", ")", ",", "total_loss_set_div", ".", "item", "(", ")", "/", "len", "(", "dataloader", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_topics.train_one_epoch": [[278, 405], ["time.time", "encoder.train", "decoder.train", "enumerate", "target_unfold.view", "optimizer_e.zero_grad", "optimizer_d.zero_grad", "parallel_encoder", "output_emb.size", "future_mask.view.size", "output_emb.gather", "output_emb.gather.view", "nsd_loss.compute_loss_set", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "loss.item", "loss.backward", "gc.collect", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer_e.step", "optimizer_d.step", "target_unfold.size", "output_emb.gather.size", "output_emb.unsqueeze().expand", "output_emb_masked.reshape.reshape", "future_mask.view.view", "parallel_decoder", "parallel_decoder", "sys.stdout.write", "encoder.parameters", "decoder.parameters", "external_emb.grad.data.zero_", "main_train_topics.logging", "time.time", "inner_idx_tensor.unsqueeze().expand", "loss_set.item", "loss_set_reg.item", "loss_set_div.item", "loss_set_neg.item", "time.time", "print", "output_emb.unsqueeze", "external_emb.data.norm", "inner_idx_tensor.unsqueeze", "len"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.compute_loss_set", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.step", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.step", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.main_train_topics.logging", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "train_one_epoch", "(", "dataloader_train", ",", "external_emb", ",", "lr", ",", "current_coeff_opt", ",", "split_i", ")", ":", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "total_loss", "=", "0.", "\n", "total_loss_set", "=", "0.", "\n", "total_loss_set_reg", "=", "0.", "\n", "total_loss_set_div", "=", "0.", "\n", "total_loss_set_neg", "=", "0.", "\n", "\n", "encoder", ".", "train", "(", ")", "\n", "decoder", ".", "train", "(", ")", "\n", "for", "i_batch", ",", "sample_batched", "in", "enumerate", "(", "dataloader_train", ")", ":", "\n", "#init_head_posi = random.randint(0, args.dilated_head_span - 1)", "\n", "#dilated_heads_posi = list(range(init_head_posi, len(feature), args.dilated_head_span))", "\n", "#if dilated_heads_posi[-1] != len(feature)-1:", "\n", "#    dilated_heads_posi.append(len(feature)-1)", "\n", "        ", "feature", ",", "target_unfold", ",", "inner_idx_tensor", ",", "future_mask", "=", "sample_batched", "\n", "#feature.size(0) % args.dilated_head_span", "\n", "#target_idx = idx_feature_to_target[:,init_head_posi:].unfold(dim=1, args.n_further, args.dilated_head_span)", "\n", "#target_idx = torch.cat( (target_idx, idx_feature_to_target[:,-1]), dim=1)", "\n", "\n", "#print(feature.size())", "\n", "#print(target_unfold.size()) ", "\n", "#print(future_mask.size())", "\n", "#target should have size (batch, num_head, n_further)", "\n", "target", "=", "target_unfold", ".", "view", "(", "-", "1", ",", "target_unfold", ".", "size", "(", "2", ")", ")", "\n", "\n", "optimizer_e", ".", "zero_grad", "(", ")", "\n", "optimizer_d", ".", "zero_grad", "(", ")", "\n", "#output_emb, hidden, output_emb_last = parallel_encoder(feature.t())", "\n", "#output_emb, hidden, output_emb_last = parallel_encoder(feature)", "\n", "#output_emb_last, output_emb = parallel_encoder(feature)", "\n", "output_emb", ",", "past", "=", "parallel_encoder", "(", "feature", ")", "\n", "#output_emb should have size (batch, seq_len, hidden_size)", "\n", "hidden_size", "=", "output_emb", ".", "size", "(", "2", ")", "\n", "batch_size", ",", "num_head", ",", "seq_len", "=", "future_mask", ".", "size", "(", ")", "\n", "\n", "#print(inner_idx_tensor)", "\n", "output_emb_head", "=", "output_emb", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "inner_idx_tensor", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "num_head", ",", "hidden_size", ")", ")", "\n", "#output_emb_head should have size (batch, num_head, hidden_size)", "\n", "output_emb_last", "=", "output_emb_head", ".", "view", "(", "-", "1", ",", "output_emb_head", ".", "size", "(", "2", ")", ")", "\n", "\n", "if", "args", ".", "de_en_connection", ":", "\n", "            ", "output_emb_masked", "=", "output_emb", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "batch_size", ",", "num_head", ",", "seq_len", ",", "hidden_size", ")", "\n", "#output_emb_masked = output_emb.unsqueeze(dim=0).expand(num_head,batch_size,seq_len,hidden_size)", "\n", "#future_mask_expanded = future_mask.unsqueeze(dim=-1).expand( batch_size,num_head,seq_len,hidden_size)", "\n", "#output_emb_masked[future_mask_expanded] = 0", "\n", "#output_emb_masked = output_emb_masked.view(-1, seq_len, hidden_size)", "\n", "output_emb_masked", "=", "output_emb_masked", ".", "reshape", "(", "-", "1", ",", "seq_len", ",", "hidden_size", ")", "\n", "future_mask", "=", "future_mask", ".", "view", "(", "-", "1", ",", "seq_len", ")", "\n", "\n", "basis_pred", "=", "parallel_decoder", "(", "output_emb_last", ",", "output_emb_masked", ",", "memory_attention_mask", "=", "future_mask", ")", "\n", "", "else", ":", "\n", "            ", "basis_pred", "=", "parallel_decoder", "(", "output_emb_last", ")", "\n", "", "compute_target_grad", "=", "args", ".", "update_target_emb", "\n", "loss_set", ",", "loss_set_reg", ",", "loss_set_div", ",", "loss_set_neg", "=", "nsd_loss", ".", "compute_loss_set", "(", "basis_pred", ",", "external_emb", ",", "target", ",", "args", ".", "L1_losss_B", ",", "device", ",", "w_freq", ",", "current_coeff_opt", ",", "compute_target_grad", ",", "args", ".", "coeff_opt_algo", ")", "\n", "if", "torch", ".", "isnan", "(", "loss_set", ")", ":", "\n", "            ", "sys", ".", "stdout", ".", "write", "(", "'nan, '", ")", "\n", "continue", "\n", "", "total_loss_set", "+=", "loss_set", ".", "item", "(", ")", "*", "args", ".", "small_batch_size", "/", "args", ".", "batch_size", "\n", "total_loss_set_reg", "+=", "loss_set_reg", ".", "item", "(", ")", "*", "args", ".", "small_batch_size", "/", "args", ".", "batch_size", "\n", "total_loss_set_div", "+=", "loss_set_div", ".", "item", "(", ")", "*", "args", ".", "small_batch_size", "/", "args", ".", "batch_size", "\n", "total_loss_set_neg", "+=", "loss_set_neg", ".", "item", "(", ")", "*", "args", ".", "small_batch_size", "/", "args", ".", "batch_size", "\n", "\n", "#BT_nonneg = torch.max( torch.tensor([0.0], device=device), BT )", "\n", "#loss = loss_set + loss_set_neg + args.w_loss_coeff* loss_coeff_pred", "\n", "#loss = 9 * torch.max( torch.tensor([0.7], device=device), loss_set) +  loss_set + loss_set_neg + args.w_loss_coeff* loss_coeff_pred + 0.01 * loss_set_div", "\n", "#loss = 4 * torch.max( torch.tensor([0.7], device=device), loss_set) + 4 * torch.max( torch.tensor([0.7], device=device), -loss_set_neg) +  loss_set + loss_set_neg + args.w_loss_coeff* loss_coeff_pred", "\n", "#loss = loss_set + 0.9 * loss_set_neg + args.w_loss_coeff* loss_coeff_pred", "\n", "#loss = loss_set + args.w_loss_coeff* loss_coeff_pred", "\n", "#loss = loss_set + args.w_loss_coeff* loss_coeff_pred", "\n", "loss", "=", "loss_set", "\n", "if", "-", "loss_set_neg", ">", "1", ":", "\n", "            ", "loss", "-=", "loss_set_neg", "\n", "", "else", ":", "\n", "            ", "loss", "+=", "loss_set_neg", "\n", "\n", "", "loss", "*=", "args", ".", "small_batch_size", "/", "args", ".", "batch_size", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "gc", ".", "collect", "(", ")", "\n", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "encoder", ".", "parameters", "(", ")", ",", "args", ".", "clip", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "decoder", ".", "parameters", "(", ")", ",", "args", ".", "clip", ")", "\n", "optimizer_e", ".", "step", "(", ")", "\n", "optimizer_d", ".", "step", "(", ")", "\n", "\n", "#del sample_batched", "\n", "#break", "\n", "\n", "if", "args", ".", "update_target_emb", ":", "\n", "#print(external_emb.requires_grad)", "\n", "#print(external_emb.grad)", "\n", "            ", "if", "args", ".", "optimizer", "==", "'SGD'", ":", "\n", "                ", "external_emb", ".", "data", "-=", "lr", "/", "10.0", "*", "external_emb", ".", "grad", ".", "data", "\n", "", "else", ":", "\n", "#external_emb.data -= 0.1/10.0 * external_emb.grad.data", "\n", "                ", "external_emb", ".", "data", "-=", "10", "/", "10.0", "*", "external_emb", ".", "grad", ".", "data", "\n", "", "external_emb", ".", "data", "[", "0", ",", ":", "]", "=", "0", "\n", "external_emb", ".", "grad", ".", "data", ".", "zero_", "(", ")", "\n", "#with torch.no_grad():", "\n", "external_emb", ".", "data", "=", "external_emb", ".", "data", "/", "(", "0.000000000001", "+", "external_emb", ".", "data", ".", "norm", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "", "if", "i_batch", "%", "args", ".", "log_interval", "==", "0", "and", "i_batch", ">", "0", ":", "\n", "            ", "cur_loss", "=", "total_loss", "/", "args", ".", "log_interval", "\n", "cur_loss_set", "=", "total_loss_set", "/", "args", ".", "log_interval", "\n", "cur_loss_set_reg", "=", "total_loss_set_reg", "/", "args", ".", "log_interval", "\n", "cur_loss_set_div", "=", "total_loss_set_div", "/", "args", ".", "log_interval", "\n", "cur_loss_set_neg", "=", "total_loss_set_neg", "/", "args", ".", "log_interval", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "logging", "(", "'| e {:3d} {:3d} | {:5d}/{:5d} b | lr {:02.2f} | ms/batch {:5.2f} | '", "\n", "'l {:5.2f} | l_f {:5.4f} + {:5.4f} = {:5.4f} | reg {:5.2f} | div {:5.2f} '", ".", "format", "(", "\n", "epoch", ",", "split_i", ",", "i_batch", ",", "len", "(", "dataloader_train", ".", "dataset", ")", "//", "args", ".", "batch_size", ",", "optimizer_e", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "\n", "elapsed", "*", "1000", "/", "args", ".", "log_interval", ",", "cur_loss", ",", "cur_loss_set", ",", "cur_loss_set_neg", ",", "cur_loss_set", "+", "cur_loss_set_neg", ",", "cur_loss_set_reg", ",", "cur_loss_set_div", ")", ")", "\n", "\n", "#if args.coeff_opt == 'maxlc' and current_coeff_opt == 'max' and cur_loss_set + cur_loss_set_neg < -0.02:", "\n", "if", "args", ".", "coeff_opt", "==", "'maxlc'", "and", "current_coeff_opt", "==", "'max'", "and", "cur_loss_set", "+", "cur_loss_set_neg", "<", "-", "0.02", ":", "\n", "                ", "current_coeff_opt", "=", "'lc'", "\n", "print", "(", "\"switch to lc\"", ")", "\n", "", "total_loss", "=", "0.", "\n", "total_loss_set", "=", "0.", "\n", "total_loss_set_reg", "=", "0.", "\n", "total_loss_set_div", "=", "0.", "\n", "total_loss_set_neg", "=", "0.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "", "", "return", "current_coeff_opt", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.predict_basis": [[5, 22], ["model_set", "model_set"], "function", ["None"], ["def", "predict_basis", "(", "model_set", ",", "n_basis", ",", "output_emb", ",", "predict_coeff_sum", "=", "False", ")", ":", "\n", "#print( output_emb.size() )", "\n", "#output_emb should have dimension ( n_batch, n_emb_size)", "\n", "\n", "    ", "if", "predict_coeff_sum", ":", "\n", "        ", "basis_pred", ",", "coeff_pred", "=", "model_set", "(", "output_emb", ",", "predict_coeff_sum", "=", "True", ")", "\n", "#basis_pred should have dimension ( n_basis, n_batch, n_emb_size)", "\n", "#coeff_pred should have dimension ( n_basis, n_batch, 2)", "\n", "\n", "#basis_pred = basis_pred.permute(1,0,2)", "\n", "#coeff_pred = coeff_pred.permute(1,0,2)", "\n", "#basis_pred should have dimension ( n_batch, n_basis, n_emb_size)", "\n", "return", "basis_pred", ",", "coeff_pred", "\n", "", "else", ":", "\n", "        ", "basis_pred", "=", "model_set", "(", "output_emb", ",", "predict_coeff_sum", "=", "False", ")", "\n", "#basis_pred = basis_pred.permute(1,0,2)", "\n", "return", "basis_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_max_iter": [[23, 52], ["target_embeddings.size", "target_embeddings.permute", "basis_pred.norm", "torch.zeros", "range", "torch.zeros.permute", "basis_pred.size", "target_embeddings.size", "torch.bmm", "torch.max", "torch.zeros", "torch.zeros.scatter_", "torch.zeros.scatter_add_", "torch.bmm", "basis_pred.size", "target_embeddings.size", "torch.zeros.permute"], "function", ["None"], ["", "", "def", "estimate_coeff_mat_batch_max_iter", "(", "target_embeddings", ",", "basis_pred", ",", "device", ")", ":", "\n", "    ", "batch_size", "=", "target_embeddings", ".", "size", "(", "0", ")", "\n", "#A = basis_pred.permute(0,2,1)", "\n", "C", "=", "target_embeddings", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "#basis_pred_norm = basis_pred / (0.000000000001 + basis_pred.norm(dim = 2, keepdim=True) )", "\n", "\n", "basis_pred_norm", "=", "basis_pred", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "#basis_pred_norm_sq = basis_pred_norm * basis_pred_norm", "\n", "XX", "=", "basis_pred_norm", "*", "basis_pred_norm", "\n", "n_not_sparse", "=", "2", "\n", "coeff_mat_trans", "=", "torch", ".", "zeros", "(", "batch_size", ",", "basis_pred", ".", "size", "(", "1", ")", ",", "target_embeddings", ".", "size", "(", "1", ")", ",", "requires_grad", "=", "False", ",", "device", "=", "device", ")", "\n", "for", "i", "in", "range", "(", "n_not_sparse", ")", ":", "\n", "        ", "XY", "=", "torch", ".", "bmm", "(", "basis_pred", ",", "C", ")", "\n", "coeff", "=", "XY", "/", "XX", "\n", "#coeff should have dimension ( n_batch, n_basis, n_set)", "\n", "max_v", ",", "max_i", "=", "torch", ".", "max", "(", "coeff", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "max_v", "[", "max_v", "<", "0", "]", "=", "0", "\n", "\n", "coeff_mat_trans_temp", "=", "torch", ".", "zeros", "(", "batch_size", ",", "basis_pred", ".", "size", "(", "1", ")", ",", "target_embeddings", ".", "size", "(", "1", ")", ",", "requires_grad", "=", "False", ",", "device", "=", "device", ")", "\n", "coeff_mat_trans_temp", ".", "scatter_", "(", "dim", "=", "1", ",", "index", "=", "max_i", ",", "src", "=", "max_v", ")", "\n", "coeff_mat_trans", ".", "scatter_add_", "(", "dim", "=", "1", ",", "index", "=", "max_i", ",", "src", "=", "max_v", ")", "\n", "#pred_emb = torch.bmm(coeff_mat_trans_temp.permute(0,2,1),basis_pred)", "\n", "#C = C - pred_emb", "\n", "pred_emb", "=", "torch", ".", "bmm", "(", "coeff_mat_trans", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "basis_pred", ")", "\n", "C", "=", "(", "target_embeddings", "-", "pred_emb", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "#pred_emb = max_v * torch.gather(basis_pred,  max_i", "\n", "\n", "", "return", "coeff_mat_trans", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "#torch.gather(coeff_mat_trans , dim=1, index = max_i)", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_max": [[55, 70], ["target_embeddings.size", "target_embeddings.permute", "basis_pred.norm", "torch.bmm", "torch.max", "torch.zeros", "torch.zeros.scatter_", "torch.zeros.permute", "basis_pred.size", "target_embeddings.size"], "function", ["None"], ["", "def", "estimate_coeff_mat_batch_max", "(", "target_embeddings", ",", "basis_pred", ",", "device", ")", ":", "\n", "    ", "batch_size", "=", "target_embeddings", ".", "size", "(", "0", ")", "\n", "C", "=", "target_embeddings", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "basis_pred_norm", "=", "basis_pred", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "XX", "=", "basis_pred_norm", "*", "basis_pred_norm", "\n", "XY", "=", "torch", ".", "bmm", "(", "basis_pred", ",", "C", ")", "\n", "coeff", "=", "XY", "/", "XX", "\n", "#coeff should have dimension ( n_batch, n_basis, n_set)", "\n", "max_v", ",", "max_i", "=", "torch", ".", "max", "(", "coeff", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "max_v", "[", "max_v", "<", "0", "]", "=", "0", "\n", "\n", "coeff_mat_trans", "=", "torch", ".", "zeros", "(", "batch_size", ",", "basis_pred", ".", "size", "(", "1", ")", ",", "target_embeddings", ".", "size", "(", "1", ")", ",", "requires_grad", "=", "False", ",", "device", "=", "device", ")", "\n", "coeff_mat_trans", ".", "scatter_", "(", "dim", "=", "1", ",", "index", "=", "max_i", ",", "src", "=", "max_v", ")", "\n", "return", "coeff_mat_trans", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_opt": [[71, 120], ["target_embeddings.size", "model.MatrixReconstruction", "torch.nn.MSELoss", "range", "model.MatrixReconstruction.coeff.detach", "target_embeddings.size", "basis_pred.size", "torch.optim.SGD", "torch.optim.Adam.zero_grad", "model.MatrixReconstruction", "loss.backward", "torch.optim.Adam.step", "model.MatrixReconstruction.compute_coeff_pos", "model.MatrixReconstruction.parameters", "torch.optim.ASGD", "torch.nn.MSELoss.", "model.MatrixReconstruction.coeff.abs().sum", "model.MatrixReconstruction.parameters", "torch.optim.Adagrad", "model.MatrixReconstruction.parameters", "torch.optim.RMSprop", "model.MatrixReconstruction.coeff.abs", "model.MatrixReconstruction.parameters", "torch.optim.Adam", "RuntimeError", "model.MatrixReconstruction.parameters"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.step", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.model.MatrixReconstruction.compute_coeff_pos"], ["", "def", "estimate_coeff_mat_batch_opt", "(", "target_embeddings", ",", "basis_pred", ",", "L1_losss_B", ",", "device", ",", "coeff_opt", ",", "lr", ",", "max_iter", ")", ":", "\n", "    ", "batch_size", "=", "target_embeddings", ".", "size", "(", "0", ")", "\n", "mr", "=", "MR", "(", "batch_size", ",", "target_embeddings", ".", "size", "(", "1", ")", ",", "basis_pred", ".", "size", "(", "1", ")", ",", "device", "=", "device", ")", "\n", "loss_func", "=", "torch", ".", "nn", ".", "MSELoss", "(", "reduction", "=", "'sum'", ")", "\n", "\n", "# opt = torch.optim.LBFGS(mr.parameters(), lr=lr, max_iter=max_iter, max_eval=None, tolerance_grad=1e-05,", "\n", "#                         tolerance_change=1e-09, history_size=100, line_search_fn=None)", "\n", "#", "\n", "# def closure():", "\n", "#     opt.zero_grad()", "\n", "#     mr.compute_coeff_pos()", "\n", "#     pred = mr(basis_pred)", "\n", "#     loss = loss_func(pred, target_embeddings) / 2", "\n", "#     # loss += L1_losss_B * mr.coeff.abs().sum()", "\n", "#     loss += L1_losss_B * (mr.coeff.abs().sum() + mr.coeff.diagonal(dim1=1, dim2=2).abs().sum())", "\n", "#     # print('loss:', loss.item())", "\n", "#     loss.backward()", "\n", "#", "\n", "#     return loss", "\n", "#", "\n", "# opt.step(closure)", "\n", "\n", "if", "coeff_opt", "==", "'sgd'", ":", "\n", "        ", "opt", "=", "torch", ".", "optim", ".", "SGD", "(", "mr", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ")", "\n", "", "elif", "coeff_opt", "==", "'asgd'", ":", "\n", "        ", "opt", "=", "torch", ".", "optim", ".", "ASGD", "(", "mr", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "lambd", "=", "0.0001", ",", "alpha", "=", "0.75", ",", "t0", "=", "1000000.0", ",", "weight_decay", "=", "0", ")", "\n", "", "elif", "coeff_opt", "==", "'adagrad'", ":", "\n", "        ", "opt", "=", "torch", ".", "optim", ".", "Adagrad", "(", "mr", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "lr_decay", "=", "0", ",", "weight_decay", "=", "0", ",", "initial_accumulator_value", "=", "0", ")", "\n", "", "elif", "coeff_opt", "==", "'rmsprop'", ":", "\n", "        ", "opt", "=", "torch", ".", "optim", ".", "RMSprop", "(", "mr", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "alpha", "=", "0.99", ",", "eps", "=", "1e-08", ",", "weight_decay", "=", "0", ",", "momentum", "=", "0", ",", "\n", "centered", "=", "False", ")", "\n", "", "elif", "coeff_opt", "==", "'adam'", ":", "\n", "        ", "opt", "=", "torch", ".", "optim", ".", "Adam", "(", "mr", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-08", ",", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "'%s not implemented for coefficient estimation. Please check args.'", "%", "coeff_opt", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "max_iter", ")", ":", "\n", "        ", "opt", ".", "zero_grad", "(", ")", "\n", "pred", "=", "mr", "(", "basis_pred", ")", "\n", "loss", "=", "loss_func", "(", "pred", ",", "target_embeddings", ")", "/", "2", "\n", "# loss += L1_losss_B * mr.coeff.abs().sum()", "\n", "#loss += L1_losss_B * (mr.coeff.abs().sum() + mr.coeff.diagonal(dim1=1, dim2=2).abs().sum())", "\n", "loss", "+=", "L1_losss_B", "*", "mr", ".", "coeff", ".", "abs", "(", ")", ".", "sum", "(", ")", "\n", "# print('loss:', loss.item())", "\n", "loss", ".", "backward", "(", ")", "\n", "opt", ".", "step", "(", ")", "\n", "mr", ".", "compute_coeff_pos", "(", ")", "\n", "\n", "", "return", "mr", ".", "coeff", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.target_emb_preparation": [[121, 134], ["torch.cat", "target_embeddings.norm"], "function", ["None"], ["", "def", "target_emb_preparation", "(", "target_index", ",", "w_embeddings", ",", "n_batch", ",", "n_set", ",", "rotate_shift", ")", ":", "\n", "    ", "target_embeddings", "=", "w_embeddings", "[", "target_index", ",", ":", "]", "\n", "#print( target_embeddings.size() )", "\n", "#target_embeddings should have dimension (n_batch, n_set, n_emb_size)", "\n", "#should be the same as w_embeddings.select(0,target_set) and select should not copy the data", "\n", "target_embeddings", "=", "target_embeddings", "/", "(", "0.000000000001", "+", "target_embeddings", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ")", "# If this step is really slow, consider to do normalization before doing unfold", "\n", "\n", "#target_embeddings_4d = target_embeddings.view(-1,n_batch, n_set, target_embeddings.size(2))", "\n", "target_embeddings_rotate", "=", "torch", ".", "cat", "(", "(", "target_embeddings", "[", "rotate_shift", ":", ",", ":", ",", ":", "]", ",", "target_embeddings", "[", ":", "rotate_shift", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "#target_emb_neg = target_embeddings_rotate.view(-1,n_set, target_embeddings.size(2))", "\n", "\n", "#return target_embeddings, target_emb_neg", "\n", "return", "target_embeddings", ",", "target_embeddings_rotate", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.compute_loss_set": [[136, 214], ["target_set.size", "target_set.size", "random.randint", "torch.bmm", "torch.bmm", "torch.mean", "torch.isnan", "basis_pred_norm.mean", "nsd_loss.target_emb_preparation", "torch.no_grad", "torch.cat", "torch.mean", "print", "print", "print", "basis_pred.norm", "torch.no_grad", "basis_pred_norm.mean", "torch.mean", "torch.no_grad", "nsd_loss.target_emb_preparation", "torch.sum", "torch.sum().float", "nsd_loss.estimate_coeff_mat_batch_max", "nsd_loss.estimate_coeff_mat_batch_max", "torch.pow", "basis_pred.norm", "torch.bmm.norm", "target_embeddings.norm", "torch.mean", "torch.enable_grad", "nsd_loss.estimate_coeff_mat_batch_opt", "nsd_loss.estimate_coeff_mat_batch_opt", "target_embeddings.detach", "basis_pred.detach", "target_emb_neg.detach", "basis_pred.detach", "torch.norm", "torch.pow", "torch.sum", "target_embeddings.detach", "basis_pred.detach", "target_emb_neg.detach", "basis_pred.detach", "torch.norm"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.target_emb_preparation", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.target_emb_preparation", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_max", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_max", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_opt", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.nsd_loss.estimate_coeff_mat_batch_opt"], ["", "def", "compute_loss_set", "(", "basis_pred", ",", "w_embeddings", ",", "target_set", ",", "L1_losss_B", ",", "device", ",", "w_freq", ",", "coeff_opt", ",", "compute_target_grad", ",", "coeff_opt_algo", ")", ":", "\n", "\n", "#basis_pred, coeff_pred = predict_basis(model_set, n_basis, output_emb, predict_coeff_sum = True)", "\n", "#basis_pred should have dimension ( n_batch, n_basis, n_emb_size)", "\n", "#print( basis_pred.size() )", "\n", "#print( target_set.size() )", "\n", "#target_set should have dimension (n_batch, n_set)", "\n", "\n", "    ", "n_set", "=", "target_set", ".", "size", "(", "1", ")", "\n", "n_batch", "=", "target_set", ".", "size", "(", "0", ")", "\n", "rotate_shift", "=", "random", ".", "randint", "(", "1", ",", "n_batch", "-", "1", ")", "\n", "if", "compute_target_grad", ":", "\n", "        ", "target_embeddings", ",", "target_emb_neg", "=", "target_emb_preparation", "(", "target_set", ",", "w_embeddings", ",", "n_batch", ",", "n_set", ",", "rotate_shift", ")", "\n", "", "else", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "target_embeddings", ",", "target_emb_neg", "=", "target_emb_preparation", "(", "target_set", ",", "w_embeddings", ",", "n_batch", ",", "n_set", ",", "rotate_shift", ")", "\n", "#print( target_embeddings.size() )", "\n", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "target_freq", "=", "w_freq", "[", "target_set", "]", "\n", "#target_freq = torch.masked_select( target_freq, target_freq.gt(0))", "\n", "target_freq_inv", "=", "1", "/", "target_freq", "\n", "target_freq_inv", "[", "target_freq_inv", "<", "0", "]", "=", "0", "#handle null case", "\n", "inv_mean", "=", "torch", ".", "sum", "(", "target_freq_inv", ")", "/", "torch", ".", "sum", "(", "target_freq_inv", ">", "0", ")", ".", "float", "(", ")", "\n", "if", "inv_mean", ">", "0", ":", "\n", "            ", "target_freq_inv_norm", "=", "target_freq_inv", "/", "inv_mean", "\n", "", "else", ":", "\n", "            ", "target_freq_inv_norm", "=", "target_freq_inv", "\n", "\n", "", "target_freq_inv_norm_neg", "=", "torch", ".", "cat", "(", "(", "target_freq_inv_norm", "[", "rotate_shift", ":", ",", ":", "]", ",", "target_freq_inv_norm", "[", ":", "rotate_shift", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "\n", "#coeff_mat = estimate_coeff_mat_batch(target_embeddings.cpu(), basis_pred.detach(), L1_losss_B)", "\n", "if", "coeff_opt", "==", "'lc'", ":", "\n", "            ", "lr_coeff", "=", "0.05", "\n", "iter_coeff", "=", "60", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "coeff_mat", "=", "estimate_coeff_mat_batch_opt", "(", "target_embeddings", ".", "detach", "(", ")", ",", "basis_pred", ".", "detach", "(", ")", ",", "L1_losss_B", ",", "device", ",", "coeff_opt_algo", ",", "lr_coeff", ",", "iter_coeff", ")", "\n", "coeff_mat_neg", "=", "estimate_coeff_mat_batch_opt", "(", "target_emb_neg", ".", "detach", "(", ")", ",", "basis_pred", ".", "detach", "(", ")", ",", "L1_losss_B", ",", "device", ",", "coeff_opt_algo", ",", "lr_coeff", ",", "iter_coeff", ")", "\n", "", "", "else", ":", "\n", "            ", "coeff_mat", "=", "estimate_coeff_mat_batch_max", "(", "target_embeddings", ".", "detach", "(", ")", ",", "basis_pred", ".", "detach", "(", ")", ",", "device", ")", "\n", "#coeff_mat = estimate_coeff_mat_batch_max_iter(target_embeddings, basis_pred.detach(), device)", "\n", "coeff_mat_neg", "=", "estimate_coeff_mat_batch_max", "(", "target_emb_neg", ".", "detach", "(", ")", ",", "basis_pred", ".", "detach", "(", ")", ",", "device", ")", "\n", "#if coeff_opt == 'lc' and  coeff_opt_algo != 'sgd_bmm':", "\n", "#    lr_coeff = 0.05", "\n", "#    iter_coeff = 60", "\n", "#    coeff_mat = estimate_coeff_mat_batch_opt(target_embeddings.detach(), basis_pred.detach(), L1_losss_B, device, coeff_opt_algo, lr_coeff, iter_coeff)", "\n", "#    coeff_mat_neg = estimate_coeff_mat_batch_opt(target_emb_neg.detach(), basis_pred.detach(), L1_losss_B, device, coeff_opt_algo, lr_coeff, iter_coeff)", "\n", "\n", "", "", "pred_embeddings", "=", "torch", ".", "bmm", "(", "coeff_mat", ",", "basis_pred", ")", "\n", "pred_embeddings_neg", "=", "torch", ".", "bmm", "(", "coeff_mat_neg", ",", "basis_pred", ")", "\n", "#pred_embeddings should have dimension (n_batch, n_set, n_emb_size)", "\n", "#loss_set = torch.mean( target_freq_inv_norm * torch.norm( pred_embeddings.cuda() - target_embeddings, dim = 2 ) )", "\n", "loss_set", "=", "torch", ".", "mean", "(", "target_freq_inv_norm", "*", "torch", ".", "pow", "(", "torch", ".", "norm", "(", "pred_embeddings", "-", "target_embeddings", ",", "dim", "=", "2", ")", ",", "2", ")", ")", "\n", "loss_set_neg", "=", "-", "torch", ".", "mean", "(", "target_freq_inv_norm_neg", "*", "torch", ".", "pow", "(", "torch", ".", "norm", "(", "pred_embeddings_neg", "-", "target_emb_neg", ",", "dim", "=", "2", ")", ",", "2", ")", ")", "\n", "\n", "#if random.randint(0,n_batch) == 1:", "\n", "#    print(\"coeff_sum_basis/coeff_mean\", coeff_sum_basis/coeff_mean )", "\n", "#    print(\"coeff_sum_basis\", coeff_sum_basis[0,:] )", "\n", "#    #print(\"target_freq_inv_norm\", target_freq_inv_norm )", "\n", "#    print(\"pred_embeddings\", pred_embeddings[0,:,:] )", "\n", "#    print(\"target_embeddings\", target_embeddings[0,:,:] )", "\n", "#    print(\"target_set\", target_set[0,:])", "\n", "\n", "if", "torch", ".", "isnan", "(", "loss_set", ")", ":", "\n", "#print(\"output_embeddings\", output_emb.norm(dim = 1))", "\n", "        ", "print", "(", "\"basis_pred\"", ",", "basis_pred", ".", "norm", "(", "dim", "=", "2", ")", ")", "\n", "print", "(", "\"pred_embeddings\"", ",", "pred_embeddings", ".", "norm", "(", "dim", "=", "2", ")", ")", "\n", "print", "(", "\"target_embeddings\"", ",", "target_embeddings", ".", "norm", "(", "dim", "=", "2", ")", ")", "\n", "\n", "", "basis_pred_norm", "=", "basis_pred", "/", "basis_pred", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "pred_mean", "=", "basis_pred_norm", ".", "mean", "(", "dim", "=", "0", ",", "keepdim", "=", "True", ")", "\n", "loss_set_reg", "=", "-", "torch", ".", "mean", "(", "(", "basis_pred_norm", "-", "pred_mean", ")", ".", "norm", "(", "dim", "=", "2", ")", ")", "\n", "\n", "", "pred_mean", "=", "basis_pred_norm", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "loss_set_div", "=", "-", "torch", ".", "mean", "(", "(", "basis_pred_norm", "-", "pred_mean", ")", ".", "norm", "(", "dim", "=", "2", ")", ")", "\n", "\n", "return", "loss_set", ",", "loss_set_reg", ",", "loss_set_div", ",", "loss_set_neg", "\n", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2_memory_saving.convert_stop_to_ind_lower": [[73, 83], ["set", "set", "enumerate", "line.rstrip", "set.add", "line.rstrip.lower", "set.add"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add"], ["", "def", "convert_stop_to_ind_lower", "(", "f_in", ",", "idx2word_freq", ")", ":", "\n", "    ", "stop_word_org_set", "=", "set", "(", ")", "\n", "for", "line", "in", "f_in", ":", "\n", "        ", "w", "=", "line", ".", "rstrip", "(", ")", "\n", "stop_word_org_set", ".", "add", "(", "w", ")", "\n", "", "stop_word_set", "=", "set", "(", ")", "\n", "for", "idx", ",", "(", "w", ",", "freq", ")", "in", "enumerate", "(", "idx2word_freq", ")", ":", "\n", "        ", "if", "w", ".", "lower", "(", ")", "in", "stop_word_org_set", ":", "\n", "            ", "stop_word_set", ".", "add", "(", "idx", ")", "\n", "", "", "return", "stop_word_set", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2_memory_saving.densify_index": [[84, 98], ["w_ind_spacy_corpus_new.append", "range", "len", "idx2word_freq_new.append"], "function", ["None"], ["", "def", "densify_index", "(", "w_ind_spacy_corpus", ",", "idx2word_freq", ")", ":", "\n", "    ", "w_ind_spacy_corpus_new", "=", "[", "]", "\n", "num_special_tok", "=", "3", "\n", "idx2word_freq_new", "=", "[", "[", "w", ",", "freq", "]", "for", "w", ",", "freq", "in", "idx2word_freq", "[", ":", "num_special_tok", "]", "]", "\n", "existing_idx_dict", "=", "{", "x", ":", "x", "for", "x", "in", "range", "(", "num_special_tok", ")", "}", "\n", "for", "w_idx", "in", "w_ind_spacy_corpus", ":", "\n", "        ", "if", "w_idx", "in", "existing_idx_dict", ":", "\n", "            ", "w_idx_compact", "=", "existing_idx_dict", "[", "w_idx", "]", "\n", "", "else", ":", "\n", "            ", "w_idx_compact", "=", "len", "(", "existing_idx_dict", ")", "\n", "existing_idx_dict", "[", "w_idx", "]", "=", "w_idx_compact", "\n", "idx2word_freq_new", ".", "append", "(", "idx2word_freq", "[", "w_idx", "]", ")", "\n", "", "w_ind_spacy_corpus_new", ".", "append", "(", "w_idx_compact", ")", "\n", "", "return", "w_ind_spacy_corpus_new", ",", "idx2word_freq_new", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2_memory_saving.load_w_ind": [[100, 162], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "map_indices_to_tensors_gpt2_memory_saving.densify_index", "print", "line.rstrip().split", "sent_spacy.split", "gpt2_to_spacy.split", "len", "int", "int", "org_idx_l2_compact_idx.append", "int", "gpt2_to_spacy_inner.append", "print", "sys.stdout.flush", "line.rstrip", "sent_gpt2.split", "len", "len"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2.densify_index", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "load_w_ind", "(", "f_in", ",", "max_sent_num", ",", "idx2word_freq", ",", "min_freq", ",", "stop_ind_set", ",", "total_num_words", ",", "total_num_words_spacy", ")", ":", "\n", "    ", "w_ind_gpt2_corpus", "=", "np", ".", "zeros", "(", "total_num_words", ",", "dtype", "=", "'int32'", ")", "\n", "w_ind_spacy_corpus", "=", "np", ".", "zeros", "(", "total_num_words_spacy", ",", "dtype", "=", "'int32'", ")", "\n", "idx_gpt2_to_spacy", "=", "np", ".", "zeros", "(", "total_num_words", ",", "dtype", "=", "'int32'", ")", "\n", "#last_sent = ''", "\n", "#num_duplicated_sent = 0", "\n", "#num_too_long_sent = 0", "\n", "num_stop_words", "=", "0", "\n", "num_rare_words", "=", "0", "\n", "\n", "current_num_word_gpt2", "=", "0", "\n", "current_num_word_spacy", "=", "0", "\n", "for", "line_idx", ",", "line", "in", "enumerate", "(", "f_in", ")", ":", "\n", "        ", "sent_spacy", ",", "sent_gpt2", ",", "gpt2_to_spacy", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "gpt2_idx", "=", "[", "int", "(", "x", ")", "for", "x", "in", "sent_gpt2", ".", "split", "(", ")", "]", "\n", "w_ind_gpt2_corpus", "[", "current_num_word_gpt2", ":", "current_num_word_gpt2", "+", "len", "(", "gpt2_idx", ")", "]", "=", "gpt2_idx", "\n", "org_idx_l2_compact_idx", "=", "[", "]", "\n", "current_num_word_spacy_old", "=", "current_num_word_spacy", "\n", "#valid_w_idx = []", "\n", "for", "w_idx_str", "in", "sent_spacy", ".", "split", "(", ")", ":", "\n", "            ", "w_idx", "=", "int", "(", "w_idx_str", ")", "\n", "w", ",", "freq", "=", "idx2word_freq", "[", "w_idx", "]", "\n", "org_idx_l2_compact_idx", ".", "append", "(", "current_num_word_spacy", "-", "current_num_word_spacy_old", ")", "\n", "if", "w_idx", "in", "stop_ind_set", ":", "\n", "                ", "num_stop_words", "+=", "1", "\n", "continue", "\n", "", "if", "freq", "<", "min_freq", ":", "\n", "                ", "num_rare_words", "+=", "1", "\n", "continue", "\n", "", "if", "w_idx", "==", "EOS_IDX", ":", "\n", "                ", "continue", "\n", "#w_ind_spacy_corpus.append(w_idx)", "\n", "", "w_ind_spacy_corpus", "[", "current_num_word_spacy", "]", "=", "w_idx", "\n", "current_num_word_spacy", "+=", "1", "\n", "#valid_w_idx.append(w_idx)", "\n", "", "gpt2_to_spacy_inner", "=", "[", "]", "\n", "for", "mapping_idx_str", "in", "gpt2_to_spacy", ".", "split", "(", ")", ":", "\n", "            ", "mapping_idx", "=", "int", "(", "mapping_idx_str", ")", "\n", "gpt2_to_spacy_inner", ".", "append", "(", "org_idx_l2_compact_idx", "[", "mapping_idx", "]", "+", "current_num_word_spacy_old", ")", "\n", "", "idx_gpt2_to_spacy", "[", "current_num_word_gpt2", ":", "current_num_word_gpt2", "+", "len", "(", "gpt2_idx", ")", "]", "=", "gpt2_to_spacy_inner", "\n", "current_num_word_gpt2", "+=", "len", "(", "gpt2_idx", ")", "\n", "#if current_sent == last_sent:", "\n", "#    num_duplicated_sent += 1", "\n", "#    continue", "\n", "#last_sent = current_sent", "\n", "#if len(fields) > max_sent_len:", "\n", "#    num_too_long_sent += 1", "\n", "#    continue", "\n", "if", "line_idx", "%", "1000000", "==", "0", ":", "\n", "            ", "print", "(", "line_idx", ",", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "if", "line_idx", ">=", "max_sent_num", ":", "\n", "            ", "break", "\n", "\n", "", "", "w_ind_spacy_corpus_new", ",", "idx2word_freq_new", "=", "densify_index", "(", "w_ind_spacy_corpus", ",", "idx2word_freq", ")", "\n", "#print( \"Finish loading {} sentences. While removing {} duplicated and {} long sentences\".format(len(w_ind_corpus),num_duplicated_sent, num_too_long_sent) )", "\n", "#print(len(w_ind_gpt2_corpus))", "\n", "#print(len(idx_gpt2_to_spacy))", "\n", "#assert len(w_ind_gpt2_corpus) == len(idx_gpt2_to_spacy)", "\n", "print", "(", "\"Remove {} stop words and {} rare words\"", ".", "format", "(", "num_stop_words", ",", "num_rare_words", ")", ")", "\n", "#print( \"Finish loading {} sentences. Average gpt2 tokens {}. Average Spacy tokens {}.\".format(line_idx, len(w_ind_gpt2_corpus)/float(line_idx), len(w_ind_spacy_corpus)/float(line_idx) ) )", "\n", "return", "w_ind_gpt2_corpus", ",", "w_ind_spacy_corpus_new", ",", "idx_gpt2_to_spacy", ",", "idx2word_freq_new", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2_memory_saving.store_tensors": [[238, 249], ["torch.tensor", "torch.tensor", "int", "int", "torch.tensor", "torch.save"], "function", ["None"], ["def", "store_tensors", "(", "f_out", ",", "w_ind_gpt2_corpus", ",", "w_ind_spacy_corpus_new", ",", "idx_gpt2_to_spacy", ",", "start_idx", ",", "end_idx", ")", ":", "\n", "    ", "w_ind_gpt2_tensor", "=", "torch", ".", "tensor", "(", "w_ind_gpt2_corpus", "[", "start_idx", ":", "end_idx", "]", ",", "dtype", "=", "store_type", ")", "\n", "idx_gpt2_to_spacy_tensor", "=", "torch", ".", "tensor", "(", "idx_gpt2_to_spacy", "[", "start_idx", ":", "end_idx", "]", ",", "dtype", "=", "store_type", ")", "\n", "#print(idx_gpt2_to_spacy[start_idx:end_idx].dtype)", "\n", "#print(idx_gpt2_to_spacy_tensor.dtype)", "\n", "start_idx_spacy", "=", "int", "(", "idx_gpt2_to_spacy", "[", "start_idx", "]", ")", "\n", "#print((idx_gpt2_to_spacy_tensor-start_idx_spacy).dtype)", "\n", "end_idx_spacy", "=", "int", "(", "idx_gpt2_to_spacy", "[", "end_idx", "-", "1", "]", ")", "\n", "#print(start_idx_spacy, end_idx_spacy)", "\n", "w_ind_spacy_tensor", "=", "torch", ".", "tensor", "(", "w_ind_spacy_corpus_new", "[", "start_idx_spacy", ":", "end_idx_spacy", "]", ",", "dtype", "=", "store_type", ")", "\n", "torch", ".", "save", "(", "[", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", "-", "start_idx_spacy", "]", ",", "f_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2.convert_stop_to_ind_lower": [[71, 81], ["set", "set", "enumerate", "line.rstrip", "set.add", "line.rstrip.lower", "set.add"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add"], ["", "def", "convert_stop_to_ind_lower", "(", "f_in", ",", "idx2word_freq", ")", ":", "\n", "    ", "stop_word_org_set", "=", "set", "(", ")", "\n", "for", "line", "in", "f_in", ":", "\n", "        ", "w", "=", "line", ".", "rstrip", "(", ")", "\n", "stop_word_org_set", ".", "add", "(", "w", ")", "\n", "", "stop_word_set", "=", "set", "(", ")", "\n", "for", "idx", ",", "(", "w", ",", "freq", ")", "in", "enumerate", "(", "idx2word_freq", ")", ":", "\n", "        ", "if", "w", ".", "lower", "(", ")", "in", "stop_word_org_set", ":", "\n", "            ", "stop_word_set", ".", "add", "(", "idx", ")", "\n", "", "", "return", "stop_word_set", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2.densify_index": [[82, 96], ["w_ind_spacy_corpus_new.append", "range", "len", "idx2word_freq_new.append"], "function", ["None"], ["", "def", "densify_index", "(", "w_ind_spacy_corpus", ",", "idx2word_freq", ")", ":", "\n", "    ", "w_ind_spacy_corpus_new", "=", "[", "]", "\n", "num_special_tok", "=", "3", "\n", "idx2word_freq_new", "=", "[", "[", "w", ",", "freq", "]", "for", "w", ",", "freq", "in", "idx2word_freq", "[", ":", "num_special_tok", "]", "]", "\n", "existing_idx_dict", "=", "{", "x", ":", "x", "for", "x", "in", "range", "(", "num_special_tok", ")", "}", "\n", "for", "w_idx", "in", "w_ind_spacy_corpus", ":", "\n", "        ", "if", "w_idx", "in", "existing_idx_dict", ":", "\n", "            ", "w_idx_compact", "=", "existing_idx_dict", "[", "w_idx", "]", "\n", "", "else", ":", "\n", "            ", "w_idx_compact", "=", "len", "(", "existing_idx_dict", ")", "\n", "existing_idx_dict", "[", "w_idx", "]", "=", "w_idx_compact", "\n", "idx2word_freq_new", ".", "append", "(", "idx2word_freq", "[", "w_idx", "]", ")", "\n", "", "w_ind_spacy_corpus_new", ".", "append", "(", "w_idx_compact", ")", "\n", "", "return", "w_ind_spacy_corpus_new", ",", "idx2word_freq_new", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2.load_w_ind": [[98, 153], ["enumerate", "map_indices_to_tensors_gpt2.densify_index", "print", "print", "line.rstrip().split", "len", "sent_spacy.split", "gpt2_to_spacy.split", "len", "len", "int", "int", "org_idx_l2_compact_idx.append", "w_ind_spacy_corpus.append", "int", "idx_gpt2_to_spacy.append", "print", "sys.stdout.flush", "line.rstrip", "sent_gpt2.split", "len", "float", "len", "float", "len"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2.densify_index", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "load_w_ind", "(", "f_in", ",", "max_sent_num", ",", "idx2word_freq", ",", "min_freq", ",", "stop_ind_set", ")", ":", "\n", "    ", "w_ind_gpt2_corpus", "=", "[", "]", "\n", "w_ind_spacy_corpus", "=", "[", "]", "\n", "idx_gpt2_to_spacy", "=", "[", "]", "\n", "#last_sent = ''", "\n", "#num_duplicated_sent = 0", "\n", "#num_too_long_sent = 0", "\n", "num_stop_words", "=", "0", "\n", "num_rare_words", "=", "0", "\n", "\n", "for", "line_idx", ",", "line", "in", "enumerate", "(", "f_in", ")", ":", "\n", "        ", "sent_spacy", ",", "sent_gpt2", ",", "gpt2_to_spacy", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "w_ind_gpt2_corpus", "+=", "[", "int", "(", "x", ")", "for", "x", "in", "sent_gpt2", ".", "split", "(", ")", "]", "\n", "org_idx_l2_compact_idx", "=", "[", "]", "\n", "current_num_word_spacy", "=", "len", "(", "w_ind_spacy_corpus", ")", "\n", "#valid_w_idx = []", "\n", "for", "w_idx_str", "in", "sent_spacy", ".", "split", "(", ")", ":", "\n", "            ", "w_idx", "=", "int", "(", "w_idx_str", ")", "\n", "w", ",", "freq", "=", "idx2word_freq", "[", "w_idx", "]", "\n", "org_idx_l2_compact_idx", ".", "append", "(", "len", "(", "w_ind_spacy_corpus", ")", "-", "current_num_word_spacy", ")", "\n", "if", "w_idx", "in", "stop_ind_set", ":", "\n", "                ", "num_stop_words", "+=", "1", "\n", "continue", "\n", "", "if", "freq", "<", "min_freq", ":", "\n", "                ", "num_rare_words", "+=", "1", "\n", "continue", "\n", "", "if", "w_idx", "==", "EOS_IDX", ":", "\n", "                ", "continue", "\n", "", "w_ind_spacy_corpus", ".", "append", "(", "w_idx", ")", "\n", "#valid_w_idx.append(w_idx)", "\n", "", "for", "mapping_idx_str", "in", "gpt2_to_spacy", ".", "split", "(", ")", ":", "\n", "            ", "mapping_idx", "=", "int", "(", "mapping_idx_str", ")", "\n", "idx_gpt2_to_spacy", ".", "append", "(", "org_idx_l2_compact_idx", "[", "mapping_idx", "]", "+", "current_num_word_spacy", ")", "\n", "\n", "#if current_sent == last_sent:", "\n", "#    num_duplicated_sent += 1", "\n", "#    continue", "\n", "#last_sent = current_sent", "\n", "#if len(fields) > max_sent_len:", "\n", "#    num_too_long_sent += 1", "\n", "#    continue", "\n", "", "if", "line_idx", "%", "1000000", "==", "0", ":", "\n", "            ", "print", "(", "line_idx", ",", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "if", "line_idx", ">=", "max_sent_num", ":", "\n", "            ", "break", "\n", "\n", "", "", "w_ind_spacy_corpus_new", ",", "idx2word_freq_new", "=", "densify_index", "(", "w_ind_spacy_corpus", ",", "idx2word_freq", ")", "\n", "#print( \"Finish loading {} sentences. While removing {} duplicated and {} long sentences\".format(len(w_ind_corpus),num_duplicated_sent, num_too_long_sent) )", "\n", "#print(len(w_ind_gpt2_corpus))", "\n", "#print(len(idx_gpt2_to_spacy))", "\n", "assert", "len", "(", "w_ind_gpt2_corpus", ")", "==", "len", "(", "idx_gpt2_to_spacy", ")", "\n", "print", "(", "\"Remove {} stop words and {} rare words\"", ".", "format", "(", "num_stop_words", ",", "num_rare_words", ")", ")", "\n", "print", "(", "\"Finish loading {} sentences. Average gpt2 tokens {}. Average Spacy tokens {}.\"", ".", "format", "(", "line_idx", ",", "len", "(", "w_ind_gpt2_corpus", ")", "/", "float", "(", "line_idx", ")", ",", "len", "(", "w_ind_spacy_corpus", ")", "/", "float", "(", "line_idx", ")", ")", ")", "\n", "return", "w_ind_gpt2_corpus", ",", "w_ind_spacy_corpus_new", ",", "idx_gpt2_to_spacy", ",", "idx2word_freq_new", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.preprocessing.map_indices_to_tensors_gpt2.store_tensors": [[202, 210], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.save"], "function", ["None"], ["def", "store_tensors", "(", "f_out", ",", "w_ind_gpt2_corpus", ",", "w_ind_spacy_corpus_new", ",", "idx_gpt2_to_spacy", ",", "start_idx", ",", "end_idx", ")", ":", "\n", "    ", "w_ind_gpt2_tensor", "=", "torch", ".", "tensor", "(", "w_ind_gpt2_corpus", "[", "start_idx", ":", "end_idx", "]", ",", "dtype", "=", "store_type", ")", "\n", "idx_gpt2_to_spacy_tensor", "=", "torch", ".", "tensor", "(", "idx_gpt2_to_spacy", "[", "start_idx", ":", "end_idx", "]", ",", "dtype", "=", "store_type", ")", "\n", "start_idx_spacy", "=", "idx_gpt2_to_spacy", "[", "start_idx", "]", "\n", "end_idx_spacy", "=", "idx_gpt2_to_spacy", "[", "end_idx", "-", "1", "]", "\n", "#print(start_idx_spacy, end_idx_spacy)", "\n", "w_ind_spacy_tensor", "=", "torch", ".", "tensor", "(", "w_ind_spacy_corpus_new", "[", "start_idx_spacy", ":", "end_idx_spacy", "]", ",", "dtype", "=", "store_type", ")", "\n", "torch", ".", "save", "(", "[", "w_ind_gpt2_tensor", ",", "w_ind_spacy_tensor", ",", "idx_gpt2_to_spacy_tensor", "-", "start_idx_spacy", "]", ",", "f_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.tools.word_emb_global_clustering.load_emb_file_to_np": [[19, 42], ["utils.load_emb_file_to_dict", "len", "range", "numpy.array", "print", "print", "external_emb_list.append", "oov_list.append", "len", "float", "float"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_emb_file_to_dict", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["def", "load_emb_file_to_np", "(", "emb_file", ",", "idx2word_freq", ")", ":", "\n", "    ", "word2emb", ",", "emb_size", "=", "utils", ".", "load_emb_file_to_dict", "(", "emb_file", ",", "convert_np", "=", "False", ")", "\n", "num_w", "=", "len", "(", "idx2word_freq", ")", "\n", "#external_emb = np.empty( (num_w, emb_size) )", "\n", "external_emb_list", "=", "[", "]", "\n", "OOV_freq", "=", "0", "\n", "total_freq", "=", "0", "\n", "oov_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_special_token", ",", "num_w", ")", ":", "\n", "        ", "w", "=", "idx2word_freq", "[", "i", "]", "[", "0", "]", "\n", "total_freq", "+=", "idx2word_freq", "[", "i", "]", "[", "1", "]", "\n", "if", "w", "in", "word2emb", ":", "\n", "            ", "val", "=", "word2emb", "[", "w", "]", "\n", "external_emb_list", ".", "append", "(", "val", ")", "\n", "#external_emb[i,:] = val", "\n", "", "else", ":", "\n", "            ", "oov_list", ".", "append", "(", "i", ")", "\n", "#external_emb[i,:] = 0", "\n", "OOV_freq", "+=", "idx2word_freq", "[", "i", "]", "[", "1", "]", "\n", "", "", "external_emb", "=", "np", ".", "array", "(", "external_emb_list", ")", "\n", "print", "(", "\"OOV word type percentage: {}%\"", ".", "format", "(", "len", "(", "oov_list", ")", "/", "float", "(", "num_w", ")", "*", "100", ")", ")", "\n", "print", "(", "\"OOV token percentage: {}%\"", ".", "format", "(", "OOV_freq", "/", "float", "(", "total_freq", ")", "*", "100", ")", ")", "\n", "return", "external_emb", ",", "emb_size", ",", "oov_list", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_gpt2.GPT2Config.__init__": [[58, 114], ["configuration_utils.PretrainedConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "50257", ",", "\n", "n_positions", "=", "1024", ",", "\n", "n_ctx", "=", "1024", ",", "\n", "n_embd", "=", "768", ",", "\n", "n_layer", "=", "12", ",", "\n", "n_head", "=", "12", ",", "\n", "resid_pdrop", "=", "0.1", ",", "\n", "embd_pdrop", "=", "0.1", ",", "\n", "attn_pdrop", "=", "0.1", ",", "\n", "layer_norm_epsilon", "=", "1e-5", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "summary_type", "=", "\"cls_index\"", ",", "\n", "summary_use_proj", "=", "True", ",", "\n", "summary_activation", "=", "None", ",", "\n", "summary_proj_to_labels", "=", "True", ",", "\n", "summary_first_dropout", "=", "0.1", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"Constructs GPT2Config.\n\n        Args:\n            vocab_size: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            resid_pdrop: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attn_pdrop: The dropout ratio for the attention\n                probabilities.\n            embd_pdrop: The dropout ratio for the embeddings.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "super", "(", "GPT2Config", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "n_ctx", "=", "n_ctx", "\n", "self", ".", "n_positions", "=", "n_positions", "\n", "self", ".", "n_embd", "=", "n_embd", "\n", "self", ".", "n_layer", "=", "n_layer", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "resid_pdrop", "=", "resid_pdrop", "\n", "self", ".", "embd_pdrop", "=", "embd_pdrop", "\n", "self", ".", "attn_pdrop", "=", "attn_pdrop", "\n", "self", ".", "layer_norm_epsilon", "=", "layer_norm_epsilon", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "summary_type", "=", "summary_type", "\n", "self", ".", "summary_use_proj", "=", "summary_use_proj", "\n", "self", ".", "summary_activation", "=", "summary_activation", "\n", "self", ".", "summary_first_dropout", "=", "summary_first_dropout", "\n", "self", ".", "summary_proj_to_labels", "=", "summary_proj_to_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_gpt2.GPT2Config.max_position_embeddings": [[115, 118], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "max_position_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_gpt2.GPT2Config.hidden_size": [[119, 122], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "hidden_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_embd", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_gpt2.GPT2Config.num_attention_heads": [[123, 126], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_attention_heads", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_head", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_gpt2.GPT2Config.num_hidden_layers": [[127, 130], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_hidden_layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_layer", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Attention.__init__": [[103, 120], ["torch.Module.__init__", "modeling_gpt2.Attention.register_buffer", "modeling_utils.Conv1D", "modeling_utils.Conv1D", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "set", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nx", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "\n", "n_state", "=", "nx", "# in Attention: n_state=768 (nx=n_embd)", "\n", "# [switch nx => n_state from Block to Attention to keep identical to TF implem]", "\n", "assert", "n_state", "%", "config", ".", "n_head", "==", "0", "\n", "self", ".", "register_buffer", "(", "\"bias\"", ",", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "n_ctx", ",", "n_ctx", ")", ")", ".", "view", "(", "1", ",", "1", ",", "n_ctx", ",", "n_ctx", ")", ")", "\n", "self", ".", "n_head", "=", "config", ".", "n_head", "\n", "self", ".", "split_size", "=", "n_state", "\n", "self", ".", "scale", "=", "scale", "\n", "\n", "self", ".", "c_attn", "=", "Conv1D", "(", "n_state", "*", "3", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "attn_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attn_pdrop", ")", "\n", "self", ".", "resid_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "resid_pdrop", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Attention.prune_heads": [[121, 142], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "mask.view().contiguous().eq.view().contiguous().eq.view().contiguous().eq", "[].long", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_utils.prune_conv1d_layer", "modeling_utils.prune_conv1d_layer", "modeling_gpt2.Attention.pruned_heads.union", "len", "set", "len", "sum", "mask.view().contiguous().eq.view().contiguous().eq.view().contiguous", "len", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "mask.view().contiguous().eq.view().contiguous().eq.view", "len"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_conv1d_layer", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_conv1d_layer"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "mask", "=", "torch", ".", "ones", "(", "self", ".", "n_head", ",", "self", ".", "split_size", "//", "self", ".", "n_head", ")", "\n", "heads", "=", "set", "(", "heads", ")", "-", "self", ".", "pruned_heads", "# Convert to set and emove already pruned heads", "\n", "for", "head", "in", "heads", ":", "\n", "# Compute how many pruned heads are before the head and move the index accordingly", "\n", "            ", "head", "=", "head", "-", "sum", "(", "1", "if", "h", "<", "head", "else", "0", "for", "h", "in", "self", ".", "pruned_heads", ")", "\n", "mask", "[", "head", "]", "=", "0", "\n", "", "mask", "=", "mask", ".", "view", "(", "-", "1", ")", ".", "contiguous", "(", ")", ".", "eq", "(", "1", ")", "\n", "index", "=", "torch", ".", "arange", "(", "len", "(", "mask", ")", ")", "[", "mask", "]", ".", "long", "(", ")", "\n", "index_attn", "=", "torch", ".", "cat", "(", "[", "index", ",", "index", "+", "self", ".", "split_size", ",", "index", "+", "(", "2", "*", "self", ".", "split_size", ")", "]", ")", "\n", "\n", "# Prune conv1d layers", "\n", "self", ".", "c_attn", "=", "prune_conv1d_layer", "(", "self", ".", "c_attn", ",", "index_attn", ",", "dim", "=", "1", ")", "\n", "self", ".", "c_proj", "=", "prune_conv1d_layer", "(", "self", ".", "c_proj", ",", "index", ",", "dim", "=", "0", ")", "\n", "\n", "# Update hyper params", "\n", "self", ".", "split_size", "=", "(", "self", ".", "split_size", "//", "self", ".", "n_head", ")", "*", "(", "self", ".", "n_head", "-", "len", "(", "heads", ")", ")", "\n", "self", ".", "n_head", "=", "self", ".", "n_head", "-", "len", "(", "heads", ")", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Attention._attn": [[143, 166], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling_gpt2.Attention.attn_dropout", "modeling_gpt2.Attention.size", "modeling_gpt2.Attention.size", "torch.Softmax", "torch.Softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "outputs.append", "math.sqrt", "v.size"], "methods", ["None"], ["", "def", "_attn", "(", "self", ",", "q", ",", "k", ",", "v", ",", "attention_mask", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "w", "=", "torch", ".", "matmul", "(", "q", ",", "k", ")", "\n", "if", "self", ".", "scale", ":", "\n", "            ", "w", "=", "w", "/", "math", ".", "sqrt", "(", "v", ".", "size", "(", "-", "1", ")", ")", "\n", "", "nd", ",", "ns", "=", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", "\n", "b", "=", "self", ".", "bias", "[", ":", ",", ":", ",", "ns", "-", "nd", ":", "ns", ",", ":", "ns", "]", "\n", "w", "=", "w", "*", "b", "-", "1e4", "*", "(", "1", "-", "b", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask", "\n", "            ", "w", "=", "w", "+", "attention_mask", "\n", "\n", "", "w", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "w", ")", "\n", "w", "=", "self", ".", "attn_dropout", "(", "w", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "w", "=", "w", "*", "head_mask", "\n", "\n", "", "outputs", "=", "[", "torch", ".", "matmul", "(", "w", ",", "v", ")", "]", "\n", "if", "self", ".", "output_attentions", ":", "\n", "            ", "outputs", ".", "append", "(", "w", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Attention.merge_heads": [[167, 171], ["x.permute().contiguous.permute().contiguous.permute().contiguous", "x.permute().contiguous.permute().contiguous.view", "x.permute().contiguous.permute().contiguous.permute", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size"], "methods", ["None"], ["", "def", "merge_heads", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "x", ".", "size", "(", "-", "2", ")", "*", "x", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct merge_states", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Attention.split_heads": [[172, 179], ["x.view.view.view", "x.view.view.permute", "x.view.view.permute", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "split_heads", "(", "self", ",", "x", ",", "k", "=", "False", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "n_head", ",", "x", ".", "size", "(", "-", "1", ")", "//", "self", ".", "n_head", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct split_states", "\n", "if", "k", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (batch, head, head_features, seq_length)", "\n", "", "else", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# (batch, head, seq_length, head_features)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Attention.forward": [[180, 201], ["modeling_gpt2.Attention.c_attn", "modeling_gpt2.Attention.split", "modeling_gpt2.Attention.split_heads", "modeling_gpt2.Attention.split_heads", "modeling_gpt2.Attention.split_heads", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "modeling_gpt2.Attention._attn", "modeling_gpt2.Attention.merge_heads", "modeling_gpt2.Attention.c_proj", "modeling_gpt2.Attention.resid_dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layer_past[].transpose", "torch.cat.transpose", "torch.cat.transpose"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention._attn", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.merge_heads"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ",", "attention_mask", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "x", "=", "self", ".", "c_attn", "(", "x", ")", "\n", "query", ",", "key", ",", "value", "=", "x", ".", "split", "(", "self", ".", "split_size", ",", "dim", "=", "2", ")", "\n", "query", "=", "self", ".", "split_heads", "(", "query", ")", "\n", "key", "=", "self", ".", "split_heads", "(", "key", ",", "k", "=", "True", ")", "\n", "value", "=", "self", ".", "split_heads", "(", "value", ")", "\n", "if", "layer_past", "is", "not", "None", ":", "\n", "            ", "past_key", ",", "past_value", "=", "layer_past", "[", "0", "]", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "layer_past", "[", "1", "]", "# transpose back cf below", "\n", "key", "=", "torch", ".", "cat", "(", "(", "past_key", ",", "key", ")", ",", "dim", "=", "-", "1", ")", "\n", "value", "=", "torch", ".", "cat", "(", "(", "past_value", ",", "value", ")", ",", "dim", "=", "-", "2", ")", "\n", "", "present", "=", "torch", ".", "stack", "(", "(", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "value", ")", ")", "# transpose to have same shapes for stacking", "\n", "\n", "attn_outputs", "=", "self", ".", "_attn", "(", "query", ",", "key", ",", "value", ",", "attention_mask", ",", "head_mask", ")", "\n", "a", "=", "attn_outputs", "[", "0", "]", "\n", "\n", "a", "=", "self", ".", "merge_heads", "(", "a", ")", "\n", "a", "=", "self", ".", "c_proj", "(", "a", ")", "\n", "a", "=", "self", ".", "resid_dropout", "(", "a", ")", "\n", "\n", "outputs", "=", "[", "a", ",", "present", "]", "+", "attn_outputs", "[", "1", ":", "]", "\n", "return", "outputs", "# a, present, (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.MLP.__init__": [[204, 211], ["torch.Module.__init__", "modeling_utils.Conv1D", "modeling_utils.Conv1D", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_state", ",", "config", ")", ":", "# in MLP: n_state=3072 (4 * n_embd)", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "c_fc", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "nx", ",", "n_state", ")", "\n", "self", ".", "act", "=", "gelu", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "resid_pdrop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.MLP.forward": [[212, 216], ["modeling_gpt2.MLP.act", "modeling_gpt2.MLP.c_proj", "modeling_gpt2.MLP.dropout", "modeling_gpt2.MLP.c_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "act", "(", "self", ".", "c_fc", "(", "x", ")", ")", "\n", "h2", "=", "self", ".", "c_proj", "(", "h", ")", "\n", "return", "self", ".", "dropout", "(", "h2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Block.__init__": [[219, 226], ["torch.Module.__init__", "torch.LayerNorm", "torch.LayerNorm", "modeling_gpt2.Attention", "torch.LayerNorm", "torch.LayerNorm", "modeling_gpt2.MLP"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Block", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "ln_1", "=", "nn", ".", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "nx", ",", "n_ctx", ",", "config", ",", "scale", ")", "\n", "self", ".", "ln_2", "=", "nn", ".", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "nx", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.Block.forward": [[227, 239], ["modeling_gpt2.Block.attn", "modeling_gpt2.Block.mlp", "modeling_gpt2.Block.ln_1", "modeling_gpt2.Block.ln_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ",", "attention_mask", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "output_attn", "=", "self", ".", "attn", "(", "\n", "self", ".", "ln_1", "(", "x", ")", ",", "layer_past", "=", "layer_past", ",", "attention_mask", "=", "attention_mask", ",", "head_mask", "=", "head_mask", "\n", ")", "\n", "a", "=", "output_attn", "[", "0", "]", "# output_attn: a, present, (attentions)", "\n", "\n", "x", "=", "x", "+", "a", "\n", "m", "=", "self", ".", "mlp", "(", "self", ".", "ln_2", "(", "x", ")", ")", "\n", "x", "=", "x", "+", "m", "\n", "\n", "outputs", "=", "[", "x", "]", "+", "output_attn", "[", "1", ":", "]", "\n", "return", "outputs", "# x, present, (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2PreTrainedModel.__init__": [[251, 253], ["modeling_utils.PreTrainedModel.__init__"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "GPT2PreTrainedModel", ",", "self", ")", ".", "__init__", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2PreTrainedModel._init_weights": [[254, 266], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ",", "Conv1D", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "Conv1D", ")", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2Model.__init__": [[357, 370], ["modeling_gpt2.GPT2PreTrainedModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.LayerNorm", "torch.LayerNorm", "modeling_gpt2.GPT2Model.init_weights", "modeling_gpt2.Block", "range"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2Model", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "output_hidden_states", "=", "config", ".", "output_hidden_states", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "self", ".", "output_past", "=", "config", ".", "output_past", "\n", "\n", "self", ".", "wte", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "wpe", "=", "nn", ".", "Embedding", "(", "config", ".", "n_positions", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "config", ".", "embd_pdrop", ")", "\n", "self", ".", "h", "=", "nn", ".", "ModuleList", "(", "[", "Block", "(", "config", ".", "n_ctx", ",", "config", ",", "scale", "=", "True", ")", "for", "_", "in", "range", "(", "config", ".", "n_layer", ")", "]", ")", "\n", "self", ".", "ln_f", "=", "nn", ".", "LayerNorm", "(", "config", ".", "n_embd", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2Model.get_input_embeddings": [[371, 373], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wte", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2Model.set_input_embeddings": [[374, 376], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "wte", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2Model._prune_heads": [[377, 383], ["heads_to_prune.items", "modeling_gpt2.GPT2Model.h[].attn.prune_heads"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "h", "[", "layer", "]", ".", "attn", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2Model.forward": [[384, 503], ["modeling_gpt2.GPT2Model.wpe", "modeling_gpt2.GPT2Model.drop", "enumerate", "modeling_gpt2.GPT2Model.ln_f", "hidden_states.view.view.view", "ValueError", "token_type_ids.view.view.view", "position_ids.unsqueeze().view.unsqueeze().view.view", "[].size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().view.unsqueeze().view.unsqueeze().view", "attention_mask.to.to.view", "attention_mask.to.to.unsqueeze().unsqueeze", "attention_mask.to.to.to", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "modeling_gpt2.GPT2Model.wte", "modeling_gpt2.GPT2Model.wte", "zip", "block", "tuple", "input_ids.view.view.size", "input_ids.view.view.view", "len", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "hidden_states.view.view.size", "tuple.append", "ValueError", "position_ids.unsqueeze().view.unsqueeze().view.unsqueeze", "attention_mask.to.to.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "t.view", "modeling_gpt2.GPT2Model.size", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "hidden_states.view.view.view", "modeling_gpt2.GPT2Model.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "modeling_gpt2.GPT2Model.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "if", "token_type_ids", "is", "not", "None", ":", "\n", "            ", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "if", "position_ids", "is", "not", "None", ":", "\n", "            ", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "\n", "", "if", "past", "is", "None", ":", "\n", "            ", "past_length", "=", "0", "\n", "past", "=", "[", "None", "]", "*", "len", "(", "self", ".", "h", ")", "\n", "", "else", ":", "\n", "            ", "past_length", "=", "past", "[", "0", "]", "[", "0", "]", ".", "size", "(", "-", "2", ")", "\n", "", "if", "position_ids", "is", "None", ":", "\n", "            ", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "position_ids", "=", "torch", ".", "arange", "(", "past_length", ",", "input_shape", "[", "-", "1", "]", "+", "past_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "\n", "# Attention mask.", "\n", "", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_mask", "=", "attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "attention_mask", "=", "(", "1.0", "-", "attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# head_mask has shape n_layer x batch x n_heads x N x N", "\n", "", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "self", ".", "config", ".", "n_layer", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "head_mask", "=", "(", "\n", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "# We can specify head_mask for each layer", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "\n", ")", "# switch to fload if need + fp16 compatibility", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "n_layer", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "wte", "(", "input_ids", ")", "\n", "", "position_embeds", "=", "self", ".", "wpe", "(", "position_ids", ")", "\n", "if", "token_type_ids", "is", "not", "None", ":", "\n", "            ", "token_type_embeds", "=", "self", ".", "wte", "(", "token_type_ids", ")", "\n", "", "else", ":", "\n", "            ", "token_type_embeds", "=", "0", "\n", "", "hidden_states", "=", "inputs_embeds", "+", "position_embeds", "+", "token_type_embeds", "\n", "hidden_states", "=", "self", ".", "drop", "(", "hidden_states", ")", "\n", "\n", "output_shape", "=", "input_shape", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "\n", "presents", "=", "(", ")", "\n", "all_attentions", "=", "[", "]", "\n", "all_hidden_states", "=", "(", ")", "\n", "for", "i", ",", "(", "block", ",", "layer_past", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "h", ",", "past", ")", ")", ":", "\n", "            ", "if", "self", ".", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ".", "view", "(", "*", "output_shape", ")", ",", ")", "\n", "\n", "", "outputs", "=", "block", "(", "\n", "hidden_states", ",", "layer_past", "=", "layer_past", ",", "attention_mask", "=", "attention_mask", ",", "head_mask", "=", "head_mask", "[", "i", "]", "\n", ")", "\n", "\n", "hidden_states", ",", "present", "=", "outputs", "[", ":", "2", "]", "\n", "if", "self", ".", "output_past", ":", "\n", "                ", "presents", "=", "presents", "+", "(", "present", ",", ")", "\n", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "                ", "all_attentions", ".", "append", "(", "outputs", "[", "2", "]", ")", "\n", "\n", "", "", "hidden_states", "=", "self", ".", "ln_f", "(", "hidden_states", ")", "\n", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "*", "output_shape", ")", "\n", "# Add last hidden state", "\n", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "if", "self", ".", "output_past", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "presents", ",", ")", "\n", "", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_hidden_states", ",", ")", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "# let the number of heads free (-1) so we can extract attention even after head pruning", "\n", "            ", "attention_output_shape", "=", "input_shape", "[", ":", "-", "1", "]", "+", "(", "-", "1", ",", ")", "+", "all_attentions", "[", "0", "]", ".", "shape", "[", "-", "2", ":", "]", "\n", "all_attentions", "=", "tuple", "(", "t", ".", "view", "(", "*", "attention_output_shape", ")", "for", "t", "in", "all_attentions", ")", "\n", "outputs", "=", "outputs", "+", "(", "all_attentions", ",", ")", "\n", "", "return", "outputs", "# last hidden state, (presents), (all hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2LMHeadModel.__init__": [[552, 558], ["modeling_gpt2.GPT2PreTrainedModel.__init__", "modeling_gpt2.GPT2Model", "torch.Linear", "torch.Linear", "modeling_gpt2.GPT2LMHeadModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2LMHeadModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "GPT2Model", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "n_embd", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2LMHeadModel.get_output_embeddings": [[559, 561], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2LMHeadModel.prepare_inputs_for_generation": [[562, 570], ["inputs.update", "input_ids[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "**", "kwargs", ")", ":", "\n", "# only last token for inputs_ids if past is defined in kwargs", "\n", "        ", "if", "\"past\"", "in", "kwargs", "and", "kwargs", "[", "\"past\"", "]", ":", "\n", "            ", "input_ids", "=", "input_ids", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "inputs", "=", "{", "\"input_ids\"", ":", "input_ids", "}", "\n", "inputs", ".", "update", "(", "kwargs", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2LMHeadModel.forward": [[571, 606], ["modeling_gpt2.GPT2LMHeadModel.transformer", "modeling_gpt2.GPT2LMHeadModel.lm_head", "lm_logits[].contiguous", "labels[].contiguous", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "lm_logits[].contiguous.view", "labels[].contiguous.view", "lm_logits[].contiguous.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", ")", ":", "\n", "        ", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "past", "=", "past", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "hidden_states", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "\n", "outputs", "=", "(", "lm_logits", ",", ")", "+", "transformer_outputs", "[", "1", ":", "]", "\n", "if", "labels", "is", "not", "None", ":", "\n", "# Shift so that tokens < n predict n", "\n", "            ", "shift_logits", "=", "lm_logits", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "labels", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "# Flatten the tokens", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "100", ")", "\n", "loss", "=", "loss_fct", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (loss), lm_logits, presents, (all hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2DoubleHeadsModel.__init__": [[680, 688], ["modeling_gpt2.GPT2PreTrainedModel.__init__", "modeling_gpt2.GPT2Model", "torch.Linear", "torch.Linear", "modeling_utils.SequenceSummary", "modeling_gpt2.GPT2DoubleHeadsModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2DoubleHeadsModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "config", ".", "num_labels", "=", "1", "\n", "self", ".", "transformer", "=", "GPT2Model", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "n_embd", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "multiple_choice_head", "=", "SequenceSummary", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2DoubleHeadsModel.get_output_embeddings": [[689, 691], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.GPT2DoubleHeadsModel.forward": [[692, 733], ["modeling_gpt2.GPT2DoubleHeadsModel.transformer", "modeling_gpt2.GPT2DoubleHeadsModel.lm_head", "modeling_gpt2.GPT2DoubleHeadsModel.multiple_choice_head().squeeze", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "lm_logits[].contiguous", "lm_labels[].contiguous", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_gpt2.GPT2DoubleHeadsModel.multiple_choice_head", "modeling_gpt2.GPT2DoubleHeadsModel.view", "mc_labels.view", "lm_logits[].contiguous.view", "lm_labels[].contiguous.view", "modeling_gpt2.GPT2DoubleHeadsModel.size", "lm_logits[].contiguous.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "mc_token_ids", "=", "None", ",", "\n", "lm_labels", "=", "None", ",", "\n", "mc_labels", "=", "None", ",", "\n", ")", ":", "\n", "        ", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "past", "=", "past", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "mc_logits", "=", "self", ".", "multiple_choice_head", "(", "hidden_states", ",", "mc_token_ids", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "outputs", "=", "(", "lm_logits", ",", "mc_logits", ")", "+", "transformer_outputs", "[", "1", ":", "]", "\n", "if", "mc_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "mc_logits", ".", "view", "(", "-", "1", ",", "mc_logits", ".", "size", "(", "-", "1", ")", ")", ",", "mc_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "", "if", "lm_labels", "is", "not", "None", ":", "\n", "            ", "shift_logits", "=", "lm_logits", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "lm_labels", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.load_tf_weights_in_gpt2": [[43, 96], ["os.path.abspath", "logger.info", "tf.train.list_variables", "zip", "logger.info", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "logger.info", "torch.from_numpy", "torch.from_numpy", "logger.error", "tf.train.load_variable.squeeze", "re.fullmatch", "re.split", "getattr", "len", "int", "getattr", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_gpt2", "(", "model", ",", "config", ",", "gpt2_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\n", "\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", "\n", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "gpt2_checkpoint_path", ")", "\n", "logger", ".", "info", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ".", "squeeze", "(", ")", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", "[", "6", ":", "]", "# skip \"model/\"", "\n", "name", "=", "name", ".", "split", "(", "\"/\"", ")", "\n", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r\"[A-Za-z]+\\d+\"", ",", "m_name", ")", ":", "\n", "                ", "scope_names", "=", "re", ".", "split", "(", "r\"(\\d+)\"", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "scope_names", "=", "[", "m_name", "]", "\n", "", "if", "scope_names", "[", "0", "]", "==", "\"w\"", "or", "scope_names", "[", "0", "]", "==", "\"g\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"b\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"bias\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"wpe\"", "or", "scope_names", "[", "0", "]", "==", "\"wte\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "scope_names", "[", "0", "]", ")", "\n", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "scope_names", "[", "0", "]", ")", "\n", "", "if", "len", "(", "scope_names", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "scope_names", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2.gelu": [[98, 100], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.__init__": [[107, 118], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-6", ",", "weight_decay", "=", "0.0", ",", "correct_bias", "=", "True", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "eps", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "correct_bias", "=", "correct_bias", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.AdamW.step": [[119, 179], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "exp_avg_sq.sqrt().add_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "p.data.add_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Adam does not support sparse gradients, please consider SparseAdam instead\"", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1.0", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1.0", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "\n", "step_size", "=", "group", "[", "\"lr\"", "]", "\n", "if", "group", "[", "\"correct_bias\"", "]", ":", "# No bias correction for Bert", "\n", "                    ", "bias_correction1", "=", "1.0", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1.0", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "step_size", "=", "step_size", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "# Add weight decay at the end (fixed version)", "\n", "if", "group", "[", "\"weight_decay\"", "]", ">", "0.0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "\"lr\"", "]", "*", "group", "[", "\"weight_decay\"", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.get_constant_schedule": [[28, 32], ["torch.optim.lr_scheduler.LambdaLR"], "function", ["None"], ["def", "get_constant_schedule", "(", "optimizer", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a constant learning rate.\n    \"\"\"", "\n", "return", "LambdaLR", "(", "optimizer", ",", "lambda", "_", ":", "1", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.get_constant_schedule_with_warmup": [[34, 45], ["torch.optim.lr_scheduler.LambdaLR", "float", "float", "max"], "function", ["None"], ["", "def", "get_constant_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a constant learning rate preceded by a warmup\n    period during which the learning rate increases linearly between 0 and 1.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1.0", ",", "num_warmup_steps", ")", ")", "\n", "", "return", "1.0", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.get_linear_schedule_with_warmup": [[47, 60], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max"], "function", ["None"], ["", "def", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "return", "max", "(", "\n", "0.0", ",", "float", "(", "num_training_steps", "-", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.get_cosine_schedule_with_warmup": [[62, 75], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max", "math.cos", "float"], "function", ["None"], ["", "def", "get_cosine_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "num_cycles", "=", "0.5", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "progress", "=", "float", "(", "current_step", "-", "num_warmup_steps", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", "return", "max", "(", "0.0", ",", "0.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "float", "(", "num_cycles", ")", "*", "2.0", "*", "progress", ")", ")", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.optimization.get_cosine_with_hard_restarts_schedule_with_warmup": [[77, 94], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max", "math.cos", "float"], "function", ["None"], ["", "def", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "num_cycles", "=", "1.0", ",", "last_epoch", "=", "-", "1", "\n", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function with several hard restarts, after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "progress", "=", "float", "(", "current_step", "-", "num_warmup_steps", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", "if", "progress", ">=", "1.0", ":", "\n", "            ", "return", "0.0", "\n", "", "return", "max", "(", "0.0", ",", "0.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "(", "(", "float", "(", "num_cycles", ")", "*", "progress", ")", "%", "1.0", ")", ")", ")", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer.__init__": [[115, 147], ["tokenization_utils.PreTrainedTokenizer.__init__", "tokenization_gpt2.bytes_to_unicode", "dict", "regex.compile", "open", "json.load", "open", "tuple", "zip", "tokenization_gpt2.GPT2Tokenizer.encoder.items", "tokenization_gpt2.GPT2Tokenizer.byte_encoder.items", "merges_handle.read().split", "merge.split", "range", "len", "merges_handle.read"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.bytes_to_unicode"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_file", ",", "\n", "merges_file", ",", "\n", "errors", "=", "\"replace\"", ",", "\n", "unk_token", "=", "\"<|endoftext|>\"", ",", "\n", "bos_token", "=", "\"<|endoftext|>\"", ",", "\n", "eos_token", "=", "\"<|endoftext|>\"", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", "GPT2Tokenizer", ",", "self", ")", ".", "__init__", "(", "bos_token", "=", "bos_token", ",", "eos_token", "=", "eos_token", ",", "unk_token", "=", "unk_token", ",", "**", "kwargs", ")", "\n", "self", ".", "max_len_single_sentence", "=", "(", "\n", "self", ".", "max_len", "\n", ")", "# no default special tokens - you can update this value if you add special tokens", "\n", "self", ".", "max_len_sentences_pair", "=", "(", "\n", "self", ".", "max_len", "\n", ")", "# no default special tokens - you can update this value if you add special tokens", "\n", "\n", "with", "open", "(", "vocab_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "vocab_handle", ":", "\n", "            ", "self", ".", "encoder", "=", "json", ".", "load", "(", "vocab_handle", ")", "\n", "", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "errors", "=", "errors", "# how to handle errors in decoding", "\n", "self", ".", "byte_encoder", "=", "bytes_to_unicode", "(", ")", "\n", "self", ".", "byte_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "byte_encoder", ".", "items", "(", ")", "}", "\n", "with", "open", "(", "merges_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "merges_handle", ":", "\n", "            ", "bpe_merges", "=", "merges_handle", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "[", "1", ":", "-", "1", "]", "\n", "", "bpe_merges", "=", "[", "tuple", "(", "merge", ".", "split", "(", ")", ")", "for", "merge", "in", "bpe_merges", "]", "\n", "self", ".", "bpe_ranks", "=", "dict", "(", "zip", "(", "bpe_merges", ",", "range", "(", "len", "(", "bpe_merges", ")", ")", ")", ")", "\n", "self", ".", "cache", "=", "{", "}", "\n", "\n", "# Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions", "\n", "self", ".", "pat", "=", "re", ".", "compile", "(", "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer.vocab_size": [[148, 151], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer.bpe": [[152, 193], ["tuple", "tokenization_gpt2.get_pairs", "min", "tuple", "len", "len", "tokenization_gpt2.get_pairs", "tuple.index", "tuple.extend", "tuple.append", "tuple.append", "tokenization_gpt2.GPT2Tokenizer.bpe_ranks.get", "tuple.extend", "float", "len"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.get_pairs", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.get_pairs"], ["", "def", "bpe", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "in", "self", ".", "cache", ":", "\n", "            ", "return", "self", ".", "cache", "[", "token", "]", "\n", "", "word", "=", "tuple", "(", "token", ")", "\n", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "            ", "return", "token", "\n", "\n", "", "while", "True", ":", "\n", "            ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "self", ".", "bpe_ranks", ".", "get", "(", "pair", ",", "float", "(", "\"inf\"", ")", ")", ")", "\n", "if", "bigram", "not", "in", "self", ".", "bpe_ranks", ":", "\n", "                ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "", "except", "ValueError", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                    ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "", "", "word", "=", "\" \"", ".", "join", "(", "word", ")", "\n", "self", ".", "cache", "[", "token", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer._tokenize": [[194, 210], ["regex.findall", "bpe_tokens.extend", "token.encode", "tokenization_gpt2.GPT2Tokenizer.bpe().split", "tokenization_gpt2.GPT2Tokenizer.bpe"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer.bpe"], ["", "def", "_tokenize", "(", "self", ",", "text", ",", "add_prefix_space", "=", "False", ")", ":", "\n", "        ", "\"\"\" Tokenize a string.\n            Args:\n                - add_prefix_space (boolean, default False):\n                    Begin the sentence with at least one space to get invariance to word order in GPT-2 (and RoBERTa) tokenizers.\n        \"\"\"", "\n", "if", "add_prefix_space", ":", "\n", "            ", "text", "=", "\" \"", "+", "text", "\n", "\n", "", "bpe_tokens", "=", "[", "]", "\n", "for", "token", "in", "re", ".", "findall", "(", "self", ".", "pat", ",", "text", ")", ":", "\n", "            ", "token", "=", "\"\"", ".", "join", "(", "\n", "self", ".", "byte_encoder", "[", "b", "]", "for", "b", "in", "token", ".", "encode", "(", "\"utf-8\"", ")", "\n", ")", "# Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)", "\n", "bpe_tokens", ".", "extend", "(", "bpe_token", "for", "bpe_token", "in", "self", ".", "bpe", "(", "token", ")", ".", "split", "(", "\" \"", ")", ")", "\n", "", "return", "bpe_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id": [[211, 214], ["tokenization_gpt2.GPT2Tokenizer.encoder.get", "tokenization_gpt2.GPT2Tokenizer.encoder.get"], "methods", ["None"], ["", "def", "_convert_token_to_id", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\" Converts a token (str) in an id using the vocab. \"\"\"", "\n", "return", "self", ".", "encoder", ".", "get", "(", "token", ",", "self", ".", "encoder", ".", "get", "(", "self", ".", "unk_token", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token": [[215, 218], ["tokenization_gpt2.GPT2Tokenizer.decoder.get"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"", "\n", "return", "self", ".", "decoder", ".", "get", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string": [[219, 225], ["bytearray().decode", "bytearray"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"", "\n", "text", "=", "\"\"", ".", "join", "(", "tokens", ")", "\n", "text", "=", "bytearray", "(", "[", "self", ".", "byte_decoder", "[", "c", "]", "for", "c", "in", "text", "]", ")", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "self", ".", "errors", ")", "\n", "#print(text)", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.GPT2Tokenizer.save_vocabulary": [[226, 251], ["os.path.join", "os.path.join", "os.path.isdir", "logger.error", "open", "f.write", "open", "writer.write", "sorted", "json.dumps", "tokenization_gpt2.GPT2Tokenizer.bpe_ranks.items", "writer.write", "logger.warning"], "methods", ["None"], ["", "def", "save_vocabulary", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\"Save the tokenizer vocabulary and merge files to a directory.\"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"Vocabulary path ({}) should be a directory\"", ".", "format", "(", "save_directory", ")", ")", "\n", "return", "\n", "", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "VOCAB_FILES_NAMES", "[", "\"vocab_file\"", "]", ")", "\n", "merge_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "VOCAB_FILES_NAMES", "[", "\"merges_file\"", "]", ")", "\n", "\n", "with", "open", "(", "vocab_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "self", ".", "encoder", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "index", "=", "0", "\n", "with", "open", "(", "merge_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "\"#version: 0.2\\n\"", ")", "\n", "for", "bpe_tokens", ",", "token_index", "in", "sorted", "(", "self", ".", "bpe_ranks", ".", "items", "(", ")", ",", "key", "=", "lambda", "kv", ":", "kv", "[", "1", "]", ")", ":", "\n", "                ", "if", "index", "!=", "token_index", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"Saving vocabulary to {}: BPE merge indices are not consecutive.\"", "\n", "\" Please check that the tokenizer is not corrupted!\"", ".", "format", "(", "merge_file", ")", "\n", ")", "\n", "index", "=", "token_index", "\n", "", "writer", ".", "write", "(", "\" \"", ".", "join", "(", "bpe_tokens", ")", "+", "\"\\n\"", ")", "\n", "index", "+=", "1", "\n", "\n", "", "", "return", "vocab_file", ",", "merge_file", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.bytes_to_unicode": [[62, 86], ["functools.lru_cache", "range", "dict", "list", "chr", "zip", "list", "list", "range", "bs.append", "cs.append", "range", "range", "ord", "ord", "ord", "ord", "ord", "ord"], "function", ["None"], ["@", "lru_cache", "(", ")", "\n", "def", "bytes_to_unicode", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns list of utf-8 byte and a mapping to unicode strings.\n    We specifically avoids mapping to whitespace/control characters the bpe code barfs on.\n\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    \"\"\"", "\n", "bs", "=", "(", "\n", "list", "(", "range", "(", "ord", "(", "\"!\"", ")", ",", "ord", "(", "\"~\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00a1\"", ")", ",", "ord", "(", "\"\u00ac\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00ae\"", ")", ",", "ord", "(", "\"\u00ff\"", ")", "+", "1", ")", ")", "\n", ")", "\n", "cs", "=", "bs", "[", ":", "]", "\n", "n", "=", "0", "\n", "for", "b", "in", "range", "(", "2", "**", "8", ")", ":", "\n", "        ", "if", "b", "not", "in", "bs", ":", "\n", "            ", "bs", ".", "append", "(", "b", ")", "\n", "cs", ".", "append", "(", "2", "**", "8", "+", "n", ")", "\n", "n", "+=", "1", "\n", "", "", "cs", "=", "[", "chr", "(", "n", ")", "for", "n", "in", "cs", "]", "\n", "return", "dict", "(", "zip", "(", "bs", ",", "cs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_gpt2.get_pairs": [[88, 99], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add"], ["", "def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_torch_available": [[96, 98], ["None"], "function", ["None"], ["def", "is_torch_available", "(", ")", ":", "\n", "    ", "return", "_torch_available", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_tf_available": [[100, 102], ["None"], "function", ["None"], ["", "def", "is_tf_available", "(", ")", ":", "\n", "    ", "return", "_tf_available", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.add_start_docstrings": [[104, 110], ["None"], "function", ["None"], ["", "def", "add_start_docstrings", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "fn", ".", "__doc__", "=", "\"\"", ".", "join", "(", "docstr", ")", "+", "fn", ".", "__doc__", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.add_end_docstrings": [[112, 118], ["None"], "function", ["None"], ["", "def", "add_end_docstrings", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "fn", ".", "__doc__", "=", "fn", ".", "__doc__", "+", "\"\"", ".", "join", "(", "docstr", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_remote_url": [[120, 123], ["urllib.parse.urlparse"], "function", ["None"], ["", "def", "is_remote_url", "(", "url_or_filename", ")", ":", "\n", "    ", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "return", "parsed", ".", "scheme", "in", "(", "\"http\"", ",", "\"https\"", ",", "\"s3\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.hf_bucket_url": [[125, 131], ["None"], "function", ["None"], ["", "def", "hf_bucket_url", "(", "identifier", ",", "postfix", "=", "None", ",", "cdn", "=", "False", ")", ":", "\n", "    ", "endpoint", "=", "CLOUDFRONT_DISTRIB_PREFIX", "if", "cdn", "else", "S3_BUCKET_PREFIX", "\n", "if", "postfix", "is", "None", ":", "\n", "        ", "return", "\"/\"", ".", "join", "(", "(", "endpoint", ",", "identifier", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "\"/\"", ".", "join", "(", "(", "endpoint", ",", "identifier", ",", "postfix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.url_to_filename": [[133, 155], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "url.endswith", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode"], ["", "", "def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name\n    so that TF 2.0 can identify it as a HDF5 file\n    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "\"utf-8\"", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "\"utf-8\"", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "\".\"", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "if", "url", ".", "endswith", "(", "\".h5\"", ")", ":", "\n", "        ", "filename", "+=", "\".h5\"", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.filename_to_url": [[157, 181], ["isinstance", "os.path.join", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "\"url\"", "]", "\n", "etag", "=", "metadata", "[", "\"etag\"", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.cached_path": [[183, 223], ["isinstance", "isinstance", "file_utils.is_remote_url", "str", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError", "urllib.parse.urlparse"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_remote_url", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "\n", "url_or_filename", ",", "cache_dir", "=", "None", ",", "force_download", "=", "False", ",", "proxies", "=", "None", ",", "resume_download", "=", "False", ",", "user_agent", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    Args:\n        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n        force_download: if True, re-dowload the file even if it's already cached in the cache dir.\n        resume_download: if True, resume the download if incompletly recieved file is found.\n        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "is_remote_url", "(", "url_or_filename", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "\n", "url_or_filename", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "user_agent", "=", "user_agent", ",", "\n", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "urlparse", "(", "url_or_filename", ")", ".", "scheme", "==", "\"\"", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.split_s3_path": [[225, 236], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.s3_request": [[238, 255], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.s3_etag": [[257, 264], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object", "botocore.config.Config"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ",", "proxies", "=", "None", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ",", "config", "=", "Config", "(", "proxies", "=", "proxies", ")", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.s3_get": [[266, 272], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "botocore.config.Config", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "None", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ",", "config", "=", "Config", "(", "proxies", "=", "proxies", ")", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.http_get": [[274, 305], ["file_utils.is_torch_available", "file_utils.is_tf_available", "isinstance", "requests.get", "requests.get.headers.get", "tqdm.auto.tqdm", "requests.get.iter_content", "tqdm.auto.tqdm.close", "isinstance", "sys.version.split", "int", "bool", "tqdm.auto.tqdm.update", "temp_file.write", "len", "logger.getEffectiveLevel", "user_agent.items"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_torch_available", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_tf_available", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update"], ["", "def", "http_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "None", ",", "resume_size", "=", "0", ",", "user_agent", "=", "None", ")", ":", "\n", "    ", "ua", "=", "\"transformers/{}; python/{}\"", ".", "format", "(", "__version__", ",", "sys", ".", "version", ".", "split", "(", ")", "[", "0", "]", ")", "\n", "if", "is_torch_available", "(", ")", ":", "\n", "        ", "ua", "+=", "\"; torch/{}\"", ".", "format", "(", "torch", ".", "__version__", ")", "\n", "", "if", "is_tf_available", "(", ")", ":", "\n", "        ", "ua", "+=", "\"; tensorflow/{}\"", ".", "format", "(", "tf", ".", "__version__", ")", "\n", "", "if", "isinstance", "(", "user_agent", ",", "dict", ")", ":", "\n", "        ", "ua", "+=", "\"; \"", "+", "\"; \"", ".", "join", "(", "\"{}/{}\"", ".", "format", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "user_agent", ".", "items", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "user_agent", ",", "str", ")", ":", "\n", "        ", "ua", "+=", "\"; \"", "+", "user_agent", "\n", "", "headers", "=", "{", "\"user-agent\"", ":", "ua", "}", "\n", "if", "resume_size", ">", "0", ":", "\n", "        ", "headers", "[", "\"Range\"", "]", "=", "\"bytes=%d-\"", "%", "(", "resume_size", ",", ")", "\n", "", "response", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ",", "proxies", "=", "proxies", ",", "headers", "=", "headers", ")", "\n", "if", "response", ".", "status_code", "==", "416", ":", "# Range not satisfiable", "\n", "        ", "return", "\n", "", "content_length", "=", "response", ".", "headers", ".", "get", "(", "\"Content-Length\"", ")", "\n", "total", "=", "resume_size", "+", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "\n", "unit", "=", "\"B\"", ",", "\n", "unit_scale", "=", "True", ",", "\n", "total", "=", "total", ",", "\n", "initial", "=", "resume_size", ",", "\n", "desc", "=", "\"Downloading\"", ",", "\n", "disable", "=", "bool", "(", "logger", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "NOTSET", ")", ",", "\n", ")", "\n", "for", "chunk", "in", "response", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.get_from_cache": [[307, 401], ["isinstance", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "filelock.FileLock", "requests.head", "os.path.exists", "os.path.join", "os.path.exists", "functools.partial", "requests.head.headers.get", "fnmatch.filter", "functools.partial.", "logger.info", "url.startswith", "temp_file.flush", "logger.info", "os.rename", "logger.info", "os.listdir", "open", "os.stat", "os.path.exists", "file_utils.s3_get", "file_utils.http_get", "open", "json.dump", "file.endswith", "file.endswith", "logger.warn"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.url_to_filename", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.s3_etag", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.s3_get", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.http_get"], ["", "def", "get_from_cache", "(", "\n", "url", ",", "cache_dir", "=", "None", ",", "force_download", "=", "False", ",", "proxies", "=", "None", ",", "etag_timeout", "=", "10", ",", "resume_download", "=", "False", ",", "user_agent", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ",", "proxies", "=", "proxies", ")", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "            ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ",", "proxies", "=", "proxies", ",", "timeout", "=", "etag_timeout", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "                ", "etag", "=", "None", "\n", "", "else", ":", "\n", "                ", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "", "", "except", "(", "EnvironmentError", ",", "requests", ".", "exceptions", ".", "Timeout", ")", ":", "\n", "            ", "etag", "=", "None", "\n", "\n", "", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "# If we don't have a connection (etag is None) and can't identify the file", "\n", "# try to get the last downloaded one", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", "and", "etag", "is", "None", ":", "\n", "        ", "matching_files", "=", "[", "\n", "file", "\n", "for", "file", "in", "fnmatch", ".", "filter", "(", "os", ".", "listdir", "(", "cache_dir", ")", ",", "filename", "+", "\".*\"", ")", "\n", "if", "not", "file", ".", "endswith", "(", "\".json\"", ")", "and", "not", "file", ".", "endswith", "(", "\".lock\"", ")", "\n", "]", "\n", "if", "matching_files", ":", "\n", "            ", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "matching_files", "[", "-", "1", "]", ")", "\n", "\n", "# Prevent parallel downloads of the same file with a lock.", "\n", "", "", "lock_path", "=", "cache_path", "+", "\".lock\"", "\n", "with", "FileLock", "(", "lock_path", ")", ":", "\n", "\n", "        ", "if", "resume_download", ":", "\n", "            ", "incomplete_path", "=", "cache_path", "+", "\".incomplete\"", "\n", "\n", "@", "contextmanager", "\n", "def", "_resumable_file_manager", "(", ")", ":", "\n", "                ", "with", "open", "(", "incomplete_path", ",", "\"a+b\"", ")", "as", "f", ":", "\n", "                    ", "yield", "f", "\n", "\n", "", "", "temp_file_manager", "=", "_resumable_file_manager", "\n", "if", "os", ".", "path", ".", "exists", "(", "incomplete_path", ")", ":", "\n", "                ", "resume_size", "=", "os", ".", "stat", "(", "incomplete_path", ")", ".", "st_size", "\n", "", "else", ":", "\n", "                ", "resume_size", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "temp_file_manager", "=", "partial", "(", "tempfile", ".", "NamedTemporaryFile", ",", "dir", "=", "cache_dir", ",", "delete", "=", "False", ")", "\n", "resume_size", "=", "0", "\n", "\n", "", "if", "etag", "is", "not", "None", "and", "(", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", "or", "force_download", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "            ", "with", "temp_file_manager", "(", ")", "as", "temp_file", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"%s not found in cache or force_download set to True, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", "\n", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                    ", "if", "resume_download", ":", "\n", "                        ", "logger", ".", "warn", "(", "'Warning: resumable downloads are not implemented for \"s3://\" urls'", ")", "\n", "", "s3_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "proxies", ")", "\n", "", "else", ":", "\n", "                    ", "http_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "proxies", ",", "resume_size", "=", "resume_size", ",", "user_agent", "=", "user_agent", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"storing %s in cache at %s\"", ",", "url", ",", "cache_path", ")", "\n", "os", ".", "rename", "(", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "\n", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "\"url\"", ":", "url", ",", "\"etag\"", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "with", "open", "(", "meta_path", ",", "\"w\"", ")", "as", "meta_file", ":", "\n", "                    ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "", "", "", "return", "cache_path", "\n", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.convert_gpt2_original_tf_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch": [[29, 48], ["transformers.GPT2Model", "transformers.load_tf_weights_in_gpt2", "print", "torch.save", "print", "transformers.GPT2Config", "transformers.GPT2Config.from_json_file", "transformers.GPT2Model.state_dict", "open", "f.write", "GPT2Config.from_json_file.to_json_string"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.load_tf_weights_in_gpt2", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_json_file", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_json_string"], ["def", "convert_gpt2_checkpoint_to_pytorch", "(", "gpt2_checkpoint_path", ",", "gpt2_config_file", ",", "pytorch_dump_folder_path", ")", ":", "\n", "# Construct model", "\n", "    ", "if", "gpt2_config_file", "==", "\"\"", ":", "\n", "        ", "config", "=", "GPT2Config", "(", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "GPT2Config", ".", "from_json_file", "(", "gpt2_config_file", ")", "\n", "", "model", "=", "GPT2Model", "(", "config", ")", "\n", "\n", "# Load weights from numpy", "\n", "load_tf_weights_in_gpt2", "(", "model", ",", "config", ",", "gpt2_checkpoint_path", ")", "\n", "\n", "# Save pytorch-model", "\n", "pytorch_weights_dump_path", "=", "pytorch_dump_folder_path", "+", "\"/\"", "+", "WEIGHTS_NAME", "\n", "pytorch_config_dump_path", "=", "pytorch_dump_folder_path", "+", "\"/\"", "+", "CONFIG_NAME", "\n", "print", "(", "\"Save PyTorch model to {}\"", ".", "format", "(", "pytorch_weights_dump_path", ")", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "pytorch_weights_dump_path", ")", "\n", "print", "(", "\"Save configuration file to {}\"", ".", "format", "(", "pytorch_config_dump_path", ")", ")", "\n", "with", "open", "(", "pytorch_config_dump_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.bos_token": [[146, 149], ["None"], "methods", ["None"], ["", "@", "bos_token", ".", "setter", "\n", "def", "bos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.eos_token": [[150, 153], ["None"], "methods", ["None"], ["", "@", "eos_token", ".", "setter", "\n", "def", "eos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_eos_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.unk_token": [[154, 157], ["None"], "methods", ["None"], ["", "@", "unk_token", ".", "setter", "\n", "def", "unk_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_unk_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.sep_token": [[158, 161], ["None"], "methods", ["None"], ["", "@", "sep_token", ".", "setter", "\n", "def", "sep_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_sep_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.pad_token": [[162, 165], ["None"], "methods", ["None"], ["", "@", "pad_token", ".", "setter", "\n", "def", "pad_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_pad_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.cls_token": [[166, 169], ["None"], "methods", ["None"], ["", "@", "cls_token", ".", "setter", "\n", "def", "cls_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_cls_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.mask_token": [[170, 173], ["None"], "methods", ["None"], ["", "@", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_mask_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.additional_special_tokens": [[174, 177], ["None"], "methods", ["None"], ["", "@", "additional_special_tokens", ".", "setter", "\n", "def", "additional_special_tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_additional_special_tokens", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.bos_token_id": [[178, 182], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "bos_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "bos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.eos_token_id": [[183, 187], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "eos_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "eos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.unk_token_id": [[188, 192], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "unk_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the unknown token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "unk_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.sep_token_id": [[193, 197], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "sep_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "sep_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.pad_token_id": [[198, 202], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "pad_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the padding token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "pad_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.pad_token_type_id": [[203, 207], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "pad_token_type_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the padding token type in the vocabulary.\"\"\"", "\n", "return", "self", ".", "_pad_token_type_id", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.cls_token_id": [[208, 212], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "cls_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "cls_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.mask_token_id": [[213, 217], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "mask_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "mask_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.additional_special_tokens_ids": [[218, 222], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "additional_special_tokens_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\" Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "additional_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.__init__": [[223, 255], ["kwargs.pop", "set", "kwargs.items", "int", "setattr", "isinstance", "isinstance", "all", "isinstance"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "max_len", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "None", "\n", "self", ".", "_eos_token", "=", "None", "\n", "self", ".", "_unk_token", "=", "None", "\n", "self", ".", "_sep_token", "=", "None", "\n", "self", ".", "_pad_token", "=", "None", "\n", "self", ".", "_cls_token", "=", "None", "\n", "self", ".", "_mask_token", "=", "None", "\n", "self", ".", "_pad_token_type_id", "=", "0", "\n", "self", ".", "_additional_special_tokens", "=", "[", "]", "\n", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n", "# Padding side is right by default and over-riden in subclasses. If specified in the kwargs, it is changed.", "\n", "self", ".", "padding_side", "=", "kwargs", ".", "pop", "(", "\"padding_side\"", ",", "self", ".", "padding_side", ")", "\n", "\n", "# Added tokens", "\n", "self", ".", "added_tokens_encoder", "=", "{", "}", "\n", "self", ".", "unique_added_tokens_encoder", "=", "set", "(", ")", "\n", "self", ".", "added_tokens_decoder", "=", "{", "}", "\n", "\n", "# inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)", "\n", "self", ".", "init_inputs", "=", "(", ")", "\n", "self", ".", "init_kwargs", "=", "{", "}", "\n", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", ":", "\n", "                ", "if", "key", "==", "\"additional_special_tokens\"", ":", "\n", "                    ", "assert", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "isinstance", "(", "t", ",", "str", ")", "for", "t", "in", "value", ")", "\n", "", "else", ":", "\n", "                    ", "assert", "isinstance", "(", "value", ",", "str", ")", "\n", "", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.from_pretrained": [[256, 310], ["cls._from_pretrained"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._from_pretrained"], ["", "", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"\n        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n\n        Args:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n\n            resume_download: (`optional`) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n\n            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n\n        Examples::\n\n            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n\n            # Download vocabulary from S3 and cache.\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n            # Download vocabulary from S3 (user-uploaded) and cache.\n            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n\n            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n\n            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n\n            # You can link tokens to special vocabulary when instantiating\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n            # You should be sure '<unk>' is in the vocabulary when doing that.\n            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n            assert tokenizer.unk_token == '<unk>'\n\n        \"\"\"", "\n", "return", "cls", ".", "_from_pretrained", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._from_pretrained": [[311, 485], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "list", "vocab_files.items", "resolved_vocab_files.pop", "json.load.update", "resolved_vocab_files.pop", "resolved_vocab_files.pop", "resolved_vocab_files.items", "cls.unique_added_tokens_encoder.update", "cls.max_model_input_sizes.keys", "cls.pretrained_vocab_files_map.items", "logger.info", "cls.vocab_files_names.items", "additional_files_names.items", "all", "vocab_files.items", "json.load.pop", "json.load.items", "cls", "set", "cls.added_tokens_encoder.update", "cls.added_tokens_decoder.update", "cls.unique_added_tokens_encoder.update", "os.path.isdir", "os.path.exists", "os.path.dirname", "os.path.join", "EnvironmentError", "EnvironmentError", "logger.info", "logger.info", "open", "json.load", "isinstance", "min", "open", "json.load", "OSError", "open", "json.load", "set", "os.path.join", "os.path.isdir", "os.path.exists", "logger.info", "file_utils.cached_path", "json.load.get", "json.load.items", "cls.added_tokens_encoder.keys", "os.path.exists", "logger.info", "os.path.isfile", "file_utils.is_remote_url", "file_utils.hf_bucket_url", "vocab_files.values", "list", "list", "int", "cls.vocab_files_names.values", "cls.vocab_files_names.values"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.cached_path", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_remote_url", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.hf_bucket_url"], ["", "@", "classmethod", "\n", "def", "_from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "*", "init_inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "\n", "s3_models", "=", "list", "(", "cls", ".", "max_model_input_sizes", ".", "keys", "(", ")", ")", "\n", "vocab_files", "=", "{", "}", "\n", "init_configuration", "=", "{", "}", "\n", "if", "pretrained_model_name_or_path", "in", "s3_models", ":", "\n", "# Get the vocabulary from AWS S3 bucket", "\n", "            ", "for", "file_id", ",", "map_list", "in", "cls", ".", "pretrained_vocab_files_map", ".", "items", "(", ")", ":", "\n", "                ", "vocab_files", "[", "file_id", "]", "=", "map_list", "[", "pretrained_model_name_or_path", "]", "\n", "", "if", "(", "\n", "cls", ".", "pretrained_init_configuration", "\n", "and", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_init_configuration", "\n", ")", ":", "\n", "                ", "init_configuration", "=", "cls", ".", "pretrained_init_configuration", "[", "pretrained_model_name_or_path", "]", "\n", "", "", "else", ":", "\n", "# Get the vocabulary from local files", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Model name '{}' not found in model shortcut name list ({}). \"", "\n", "\"Assuming '{}' is a path or url to a directory containing tokenizer files.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\", \"", ".", "join", "(", "s3_models", ")", ",", "pretrained_model_name_or_path", "\n", ")", "\n", ")", "\n", "\n", "# Look for the tokenizer main vocabulary files", "\n", "for", "file_id", ",", "file_name", "in", "cls", ".", "vocab_files_names", ".", "items", "(", ")", ":", "\n", "                ", "if", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "# If a directory is provided we look for the standard filenames", "\n", "                    ", "full_file_name", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "file_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "full_file_name", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Didn't find file {}. We won't load it.\"", ".", "format", "(", "full_file_name", ")", ")", "\n", "full_file_name", "=", "None", "\n", "", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "pretrained_model_name_or_path", ")", ":", "\n", "# If a path to a file is provided we use it (will only work for non-BPE tokenizer using a single vocabulary file)", "\n", "                    ", "full_file_name", "=", "pretrained_model_name_or_path", "\n", "", "else", ":", "\n", "                    ", "full_file_name", "=", "hf_bucket_url", "(", "pretrained_model_name_or_path", ",", "postfix", "=", "file_name", ")", "\n", "\n", "", "vocab_files", "[", "file_id", "]", "=", "full_file_name", "\n", "\n", "# Look for the additional tokens files", "\n", "", "additional_files_names", "=", "{", "\n", "\"added_tokens_file\"", ":", "ADDED_TOKENS_FILE", ",", "\n", "\"special_tokens_map_file\"", ":", "SPECIAL_TOKENS_MAP_FILE", ",", "\n", "\"tokenizer_config_file\"", ":", "TOKENIZER_CONFIG_FILE", ",", "\n", "}", "\n", "\n", "# If a path to a file was provided, get the parent directory", "\n", "saved_directory", "=", "pretrained_model_name_or_path", "\n", "if", "os", ".", "path", ".", "exists", "(", "saved_directory", ")", "and", "not", "os", ".", "path", ".", "isdir", "(", "saved_directory", ")", ":", "\n", "                ", "saved_directory", "=", "os", ".", "path", ".", "dirname", "(", "saved_directory", ")", "\n", "\n", "", "for", "file_id", ",", "file_name", "in", "additional_files_names", ".", "items", "(", ")", ":", "\n", "                ", "full_file_name", "=", "os", ".", "path", ".", "join", "(", "saved_directory", ",", "file_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "full_file_name", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Didn't find file {}. We won't load it.\"", ".", "format", "(", "full_file_name", ")", ")", "\n", "full_file_name", "=", "None", "\n", "", "vocab_files", "[", "file_id", "]", "=", "full_file_name", "\n", "\n", "", "if", "all", "(", "full_file_name", "is", "None", "for", "full_file_name", "in", "vocab_files", ".", "values", "(", ")", ")", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\n", "\"Model name '{}' was not found in tokenizers model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to a directory containing vocabulary files \"", "\n", "\"named {} but couldn't find such vocabulary files at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "list", "(", "cls", ".", "vocab_files_names", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "# Get files from url, cache, or disk depending on the case", "\n", "", "", "try", ":", "\n", "            ", "resolved_vocab_files", "=", "{", "}", "\n", "for", "file_id", ",", "file_path", "in", "vocab_files", ".", "items", "(", ")", ":", "\n", "                ", "if", "file_path", "is", "None", ":", "\n", "                    ", "resolved_vocab_files", "[", "file_id", "]", "=", "None", "\n", "", "else", ":", "\n", "                    ", "resolved_vocab_files", "[", "file_id", "]", "=", "cached_path", "(", "\n", "file_path", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", ")", "\n", "", "", "", "except", "EnvironmentError", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "s3_models", ":", "\n", "                ", "msg", "=", "\"Couldn't reach server at '{}' to download vocabulary files.\"", "\n", "", "else", ":", "\n", "                ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in tokenizers model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to a directory containing vocabulary files \"", "\n", "\"named {}, but couldn't find such vocabulary files at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "list", "(", "cls", ".", "vocab_files_names", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "for", "file_id", ",", "file_path", "in", "vocab_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "file_path", "==", "resolved_vocab_files", "[", "file_id", "]", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading file {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading file {} from cache at {}\"", ".", "format", "(", "file_path", ",", "resolved_vocab_files", "[", "file_id", "]", ")", ")", "\n", "\n", "# Prepare tokenizer initialization kwargs", "\n", "# Did we saved some inputs and kwargs to reload ?", "\n", "", "", "tokenizer_config_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"tokenizer_config_file\"", ",", "None", ")", "\n", "if", "tokenizer_config_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "tokenizer_config_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "tokenizer_config_handle", ":", "\n", "                ", "init_kwargs", "=", "json", ".", "load", "(", "tokenizer_config_handle", ")", "\n", "", "saved_init_inputs", "=", "init_kwargs", ".", "pop", "(", "\"init_inputs\"", ",", "(", ")", ")", "\n", "if", "not", "init_inputs", ":", "\n", "                ", "init_inputs", "=", "saved_init_inputs", "\n", "", "", "else", ":", "\n", "            ", "init_kwargs", "=", "init_configuration", "\n", "\n", "# Update with newly provided kwargs", "\n", "", "init_kwargs", ".", "update", "(", "kwargs", ")", "\n", "\n", "# Set max length if needed", "\n", "if", "pretrained_model_name_or_path", "in", "cls", ".", "max_model_input_sizes", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer", "\n", "# wont index sequences longer than the number of positional embeddings", "\n", "            ", "max_len", "=", "cls", ".", "max_model_input_sizes", "[", "pretrained_model_name_or_path", "]", "\n", "if", "max_len", "is", "not", "None", "and", "isinstance", "(", "max_len", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "init_kwargs", "[", "\"max_len\"", "]", "=", "min", "(", "init_kwargs", ".", "get", "(", "\"max_len\"", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "\n", "# Merge resolved_vocab_files arguments in init_kwargs.", "\n", "", "", "added_tokens_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"added_tokens_file\"", ",", "None", ")", "\n", "special_tokens_map_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"special_tokens_map_file\"", ",", "None", ")", "\n", "for", "args_name", ",", "file_path", "in", "resolved_vocab_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "args_name", "not", "in", "init_kwargs", ":", "\n", "                ", "init_kwargs", "[", "args_name", "]", "=", "file_path", "\n", "", "", "if", "special_tokens_map_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "special_tokens_map_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "special_tokens_map_handle", ":", "\n", "                ", "special_tokens_map", "=", "json", ".", "load", "(", "special_tokens_map_handle", ")", "\n", "", "for", "key", ",", "value", "in", "special_tokens_map", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "not", "in", "init_kwargs", ":", "\n", "                    ", "init_kwargs", "[", "key", "]", "=", "value", "\n", "\n", "# Instantiate tokenizer.", "\n", "", "", "", "try", ":", "\n", "            ", "tokenizer", "=", "cls", "(", "*", "init_inputs", ",", "**", "init_kwargs", ")", "\n", "", "except", "OSError", ":", "\n", "            ", "OSError", "(", "\n", "\"Unable to load vocabulary from file. \"", "\n", "\"Please check that the provided vocabulary is accessible and not corrupted.\"", "\n", ")", "\n", "\n", "# Save inputs and kwargs for saving and re-loading with ``save_pretrained``", "\n", "", "tokenizer", ".", "init_inputs", "=", "init_inputs", "\n", "tokenizer", ".", "init_kwargs", "=", "init_kwargs", "\n", "\n", "# update unique_added_tokens_encoder with special tokens for correct tokenization", "\n", "tokenizer", ".", "unique_added_tokens_encoder", ".", "update", "(", "set", "(", "tokenizer", ".", "all_special_tokens", ")", ")", "\n", "\n", "# Add supplementary tokens.", "\n", "if", "added_tokens_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "added_tokens_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "added_tokens_handle", ":", "\n", "                ", "added_tok_encoder", "=", "json", ".", "load", "(", "added_tokens_handle", ")", "\n", "", "added_tok_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "added_tok_encoder", ".", "items", "(", ")", "}", "\n", "tokenizer", ".", "added_tokens_encoder", ".", "update", "(", "added_tok_encoder", ")", "\n", "tokenizer", ".", "added_tokens_decoder", ".", "update", "(", "added_tok_decoder", ")", "\n", "tokenizer", ".", "unique_added_tokens_encoder", ".", "update", "(", "set", "(", "tokenizer", ".", "added_tokens_encoder", ".", "keys", "(", ")", ")", ")", "\n", "\n", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.save_pretrained": [[486, 526], ["os.path.join", "os.path.join", "os.path.join", "copy.deepcopy", "copy.deepcopy", "tokenization_utils.PreTrainedTokenizer.vocab_files_names.keys", "tokenization_utils.PreTrainedTokenizer.save_vocabulary", "os.path.isdir", "logger.error", "copy.deepcopy.pop", "open", "f.write", "open", "f.write", "open", "f.write", "json.dumps", "json.dumps", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.save_vocabulary"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save the tokenizer vocabulary files together with:\n                - added tokens,\n                - special-tokens-to-class-attributes-mapping,\n                - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n\n            This won't save modifications other than (added tokens and special token mapping) you may have\n            applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n\n            This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"Saving directory ({}) should be a directory\"", ".", "format", "(", "save_directory", ")", ")", "\n", "return", "\n", "\n", "", "special_tokens_map_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "SPECIAL_TOKENS_MAP_FILE", ")", "\n", "added_tokens_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "ADDED_TOKENS_FILE", ")", "\n", "tokenizer_config_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "TOKENIZER_CONFIG_FILE", ")", "\n", "\n", "tokenizer_config", "=", "copy", ".", "deepcopy", "(", "self", ".", "init_kwargs", ")", "\n", "tokenizer_config", "[", "\"init_inputs\"", "]", "=", "copy", ".", "deepcopy", "(", "self", ".", "init_inputs", ")", "\n", "for", "file_id", "in", "self", ".", "vocab_files_names", ".", "keys", "(", ")", ":", "\n", "            ", "tokenizer_config", ".", "pop", "(", "file_id", ",", "None", ")", "\n", "\n", "", "with", "open", "(", "tokenizer_config_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "tokenizer_config", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "with", "open", "(", "special_tokens_map_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "self", ".", "special_tokens_map", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "with", "open", "(", "added_tokens_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "if", "self", ".", "added_tokens_encoder", ":", "\n", "                ", "out_str", "=", "json", ".", "dumps", "(", "self", ".", "added_tokens_encoder", ",", "ensure_ascii", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "out_str", "=", "\"{}\"", "\n", "", "f", ".", "write", "(", "out_str", ")", "\n", "\n", "", "vocab_files", "=", "self", ".", "save_vocabulary", "(", "save_directory", ")", "\n", "\n", "return", "vocab_files", "+", "(", "special_tokens_map_file", ",", "added_tokens_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.save_vocabulary": [[527, 534], ["None"], "methods", ["None"], ["", "def", "save_vocabulary", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n            and special token mappings.\n\n            Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.vocab_size": [[535, 538], ["None"], "methods", ["None"], ["", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "\"\"\" Size of the base vocabulary (without the added tokens) \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.__len__": [[539, 542], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\" Size of the full vocabulary with the added tokens \"\"\"", "\n", "return", "self", ".", "vocab_size", "+", "len", "(", "self", ".", "added_tokens_encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.add_tokens": [[543, 587], ["dict", "tokenization_utils.PreTrainedTokenizer.added_tokens_encoder.update", "set().union", "tokenization_utils.PreTrainedTokenizer.added_tokens_decoder.update", "len", "isinstance", "set", "tokenization_utils.PreTrainedTokenizer.init_kwargs.get", "token.lower.lower.lower", "to_add_tokens.append", "logger.info", "dict.items", "set", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "enumerate", "tokenization_utils.PreTrainedTokenizer.added_tokens_encoder.keys", "len"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "def", "add_tokens", "(", "self", ",", "new_tokens", ")", ":", "\n", "        ", "\"\"\"\n        Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n\n        Args:\n            new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let's see how to increase the vocabulary of Bert model and tokenizer\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            model = BertModel.from_pretrained('bert-base-uncased')\n\n            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n            print('We have added', num_added_toks, 'tokens')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n        \"\"\"", "\n", "if", "not", "new_tokens", ":", "\n", "            ", "return", "0", "\n", "\n", "", "to_add_tokens", "=", "[", "]", "\n", "for", "token", "in", "new_tokens", ":", "\n", "            ", "assert", "isinstance", "(", "token", ",", "str", ")", "\n", "if", "self", ".", "init_kwargs", ".", "get", "(", "\"do_lower_case\"", ",", "False", ")", "and", "token", "not", "in", "self", ".", "all_special_tokens", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "if", "(", "\n", "token", "!=", "self", ".", "unk_token", "\n", "and", "self", ".", "convert_tokens_to_ids", "(", "token", ")", "==", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "unk_token", ")", "\n", "and", "token", "not", "in", "to_add_tokens", "\n", ")", ":", "\n", "                ", "to_add_tokens", ".", "append", "(", "token", ")", "\n", "logger", ".", "info", "(", "\"Adding %s to the vocabulary\"", ",", "token", ")", "\n", "\n", "", "", "added_tok_encoder", "=", "dict", "(", "(", "tok", ",", "len", "(", "self", ")", "+", "i", ")", "for", "i", ",", "tok", "in", "enumerate", "(", "to_add_tokens", ")", ")", "\n", "added_tok_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "added_tok_encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "added_tokens_encoder", ".", "update", "(", "added_tok_encoder", ")", "\n", "self", ".", "unique_added_tokens_encoder", "=", "set", "(", "self", ".", "added_tokens_encoder", ".", "keys", "(", ")", ")", ".", "union", "(", "set", "(", "self", ".", "all_special_tokens", ")", ")", "\n", "self", ".", "added_tokens_decoder", ".", "update", "(", "added_tok_decoder", ")", "\n", "\n", "return", "len", "(", "to_add_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.num_added_tokens": [[588, 606], ["len", "tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens"], ["", "def", "num_added_tokens", "(", "self", ",", "pair", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns the number of added tokens when encoding a sequence with special tokens.\n\n        Note:\n            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n            inside your training loop.\n\n        Args:\n            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n                number of added tokens in the case of a single sequence if set to False.\n\n        Returns:\n            Number of tokens added to sequences\n        \"\"\"", "\n", "token_ids_0", "=", "[", "]", "\n", "token_ids_1", "=", "[", "]", "\n", "return", "len", "(", "self", ".", "build_inputs_with_special_tokens", "(", "token_ids_0", ",", "token_ids_1", "if", "pair", "else", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.add_special_tokens": [[607, 660], ["special_tokens_dict.items", "logger.info", "setattr", "tokenization_utils.PreTrainedTokenizer.add_tokens", "isinstance", "tokenization_utils.PreTrainedTokenizer.add_tokens", "isinstance", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_tokens", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_tokens"], ["", "def", "add_special_tokens", "(", "self", ",", "special_tokens_dict", ")", ":", "\n", "        ", "\"\"\"\n        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n        to class attributes. If special tokens are NOT in the vocabulary, they are added\n        to it (indexed starting from the last index of the current vocabulary).\n\n        Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n\n        - special tokens are carefully handled by the tokenizer (they are never split)\n        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n\n        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be '[CLS]' and XLM's one is also registered to be '</s>')\n\n        Args:\n            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n                ``additional_special_tokens``].\n\n                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let's see how to add a new classification token to GPT-2\n            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n            model = GPT2Model.from_pretrained('gpt2')\n\n            special_tokens_dict = {'cls_token': '<CLS>'}\n\n            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n            print('We have added', num_added_toks, 'tokens')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n\n            assert tokenizer.cls_token == '<CLS>'\n        \"\"\"", "\n", "if", "not", "special_tokens_dict", ":", "\n", "            ", "return", "0", "\n", "\n", "", "added_tokens", "=", "0", "\n", "for", "key", ",", "value", "in", "special_tokens_dict", ".", "items", "(", ")", ":", "\n", "            ", "assert", "key", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", "\n", "if", "key", "==", "\"additional_special_tokens\"", ":", "\n", "                ", "assert", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "isinstance", "(", "t", ",", "str", ")", "for", "t", "in", "value", ")", "\n", "added_tokens", "+=", "self", ".", "add_tokens", "(", "value", ")", "\n", "", "else", ":", "\n", "                ", "assert", "isinstance", "(", "value", ",", "str", ")", "\n", "added_tokens", "+=", "self", ".", "add_tokens", "(", "[", "value", "]", ")", "\n", "", "logger", ".", "info", "(", "\"Assigning %s to the %s key of the tokenizer\"", ",", "value", ",", "key", ")", "\n", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n", "", "return", "added_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.tokenize": [[661, 729], ["tokenization_utils.PreTrainedTokenizer.init_kwargs.get", "tokenization_utils.PreTrainedTokenizer.tokenize.split_on_tokens"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Take care of added tokens.\n\n            text: The sequence to be encoded.\n            **kwargs: passed to the child `self.tokenize()` method\n        \"\"\"", "\n", "all_special_tokens", "=", "self", ".", "all_special_tokens", "\n", "\n", "def", "lowercase_text", "(", "t", ")", ":", "\n", "# convert non-special tokens to lowercase", "\n", "            ", "escaped_special_toks", "=", "[", "re", ".", "escape", "(", "s_tok", ")", "for", "s_tok", "in", "all_special_tokens", "]", "\n", "pattern", "=", "r\"(\"", "+", "r\"|\"", ".", "join", "(", "escaped_special_toks", ")", "+", "r\")|\"", "+", "r\"(.+?)\"", "\n", "return", "re", ".", "sub", "(", "pattern", ",", "lambda", "m", ":", "m", ".", "groups", "(", ")", "[", "0", "]", "or", "m", ".", "groups", "(", ")", "[", "1", "]", ".", "lower", "(", ")", ",", "t", ")", "\n", "\n", "", "if", "self", ".", "init_kwargs", ".", "get", "(", "\"do_lower_case\"", ",", "False", ")", ":", "\n", "            ", "text", "=", "lowercase_text", "(", "text", ")", "\n", "\n", "", "def", "split_on_token", "(", "tok", ",", "text", ")", ":", "\n", "            ", "result", "=", "[", "]", "\n", "split_text", "=", "text", ".", "split", "(", "tok", ")", "\n", "for", "i", ",", "sub_text", "in", "enumerate", "(", "split_text", ")", ":", "\n", "                ", "sub_text", "=", "sub_text", ".", "strip", "(", ")", "\n", "if", "i", "==", "0", "and", "not", "sub_text", ":", "\n", "                    ", "result", "+=", "[", "tok", "]", "\n", "", "elif", "i", "==", "len", "(", "split_text", ")", "-", "1", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", "+=", "[", "sub_text", "]", "\n", "", "else", ":", "\n", "                        ", "pass", "\n", "", "", "else", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", "+=", "[", "sub_text", "]", "\n", "", "result", "+=", "[", "tok", "]", "\n", "", "", "return", "result", "\n", "\n", "", "def", "split_on_tokens", "(", "tok_list", ",", "text", ")", ":", "\n", "            ", "if", "not", "text", ".", "strip", "(", ")", ":", "\n", "                ", "return", "[", "]", "\n", "", "if", "not", "tok_list", ":", "\n", "                ", "return", "self", ".", "_tokenize", "(", "text", ",", "**", "kwargs", ")", "\n", "\n", "", "tokenized_text", "=", "[", "]", "\n", "text_list", "=", "[", "text", "]", "\n", "for", "tok", "in", "tok_list", ":", "\n", "                ", "tokenized_text", "=", "[", "]", "\n", "for", "sub_text", "in", "text_list", ":", "\n", "                    ", "if", "sub_text", "not", "in", "self", ".", "unique_added_tokens_encoder", ":", "\n", "                        ", "tokenized_text", "+=", "split_on_token", "(", "tok", ",", "sub_text", ")", "\n", "", "else", ":", "\n", "                        ", "tokenized_text", "+=", "[", "sub_text", "]", "\n", "", "", "text_list", "=", "tokenized_text", "\n", "\n", "", "return", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "(", "\n", "self", ".", "_tokenize", "(", "token", ",", "**", "kwargs", ")", "if", "token", "not", "in", "self", ".", "unique_added_tokens_encoder", "else", "[", "token", "]", "\n", "for", "token", "in", "tokenized_text", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "added_tokens", "=", "self", ".", "unique_added_tokens_encoder", "\n", "tokenized_text", "=", "split_on_tokens", "(", "added_tokens", ",", "text", ")", "\n", "return", "tokenized_text", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._tokenize": [[730, 738], ["None"], "methods", ["None"], ["", "def", "_tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Do NOT take care of added tokens.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids": [[739, 753], ["isinstance", "tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc", "ids.append", "tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a single token, or a sequence of tokens, (str) in a single integer id\n            (resp. a sequence of ids), using the vocabulary.\n        \"\"\"", "\n", "if", "tokens", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "isinstance", "(", "tokens", ",", "str", ")", ":", "\n", "            ", "return", "self", ".", "_convert_token_to_id_with_added_voc", "(", "tokens", ")", "\n", "\n", "", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "_convert_token_to_id_with_added_voc", "(", "token", ")", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc": [[754, 761], ["tokenization_utils.PreTrainedTokenizer._convert_token_to_id"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._convert_token_to_id"], ["", "def", "_convert_token_to_id_with_added_voc", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "token", "in", "self", ".", "added_tokens_encoder", ":", "\n", "            ", "return", "self", ".", "added_tokens_encoder", "[", "token", "]", "\n", "", "return", "self", ".", "_convert_token_to_id", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._convert_token_to_id": [[762, 764], ["None"], "methods", ["None"], ["", "def", "_convert_token_to_id", "(", "self", ",", "token", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode": [[765, 824], ["tokenization_utils.PreTrainedTokenizer.encode_plus"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.encode_plus"], ["", "def", "encode", "(", "\n", "self", ",", "\n", "text", ",", "\n", "text_pair", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n\n        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        \"\"\"", "\n", "encoded_inputs", "=", "self", ".", "encode_plus", "(", "\n", "text", ",", "\n", "text_pair", "=", "text_pair", ",", "\n", "max_length", "=", "max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "**", "kwargs", "\n", ")", "\n", "\n", "return", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode_plus": [[825, 928], ["tokenization_utils.PreTrainedTokenizer.encode_plus.get_input_ids"], "methods", ["None"], ["", "def", "encode_plus", "(", "\n", "self", ",", "\n", "text", ",", "\n", "text_pair", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n            **kwargs: passed to the `self.tokenize()` method\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    token_type_ids: list[int] if return_token_type_ids is True (default)\n                    attention_mask: list[int] if return_attention_mask is True (default)\n                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n                ``input_ids``: list of token ids to be fed to a model\n                ``token_type_ids``: list of token type ids to be fed to a model\n                ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "\n", "def", "get_input_ids", "(", "text", ")", ":", "\n", "            ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "                ", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenize", "(", "text", ",", "**", "kwargs", ")", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "text", ")", ">", "0", "and", "isinstance", "(", "text", "[", "0", "]", ",", "str", ")", ":", "\n", "                ", "return", "self", ".", "convert_tokens_to_ids", "(", "text", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "text", ")", ">", "0", "and", "isinstance", "(", "text", "[", "0", "]", ",", "int", ")", ":", "\n", "                ", "return", "text", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"", "\n", ")", "\n", "\n", "", "", "first_ids", "=", "get_input_ids", "(", "text", ")", "\n", "second_ids", "=", "get_input_ids", "(", "text_pair", ")", "if", "text_pair", "is", "not", "None", "else", "None", "\n", "\n", "return", "self", ".", "prepare_for_model", "(", "\n", "first_ids", ",", "\n", "pair_ids", "=", "second_ids", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.batch_encode_plus": [[930, 1031], ["max", "isinstance", "tokenization_utils.PreTrainedTokenizer.encode_plus", "tokenization_utils.PreTrainedTokenizer.items", "map", "batch_outputs.items", "file_utils.is_tf_available", "len", "batch_outputs[].append", "tf.abs", "torch.abs", "len", "len", "file_utils.is_tf_available", "tf.constant", "file_utils.is_torch_available", "torch.tensor", "logger.warning", "len"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.encode_plus", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_tf_available", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_tf_available", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_torch_available"], ["", "def", "batch_encode_plus", "(", "\n", "self", ",", "\n", "batch_text_or_text_pairs", "=", "None", ",", "\n", "add_special_tokens", "=", "False", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_input_lengths", "=", "False", ",", "\n", "return_attention_masks", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            batch_text_or_text_pairs: Batch of sequences or pair of sequences to be encoded.\n                This can be a list of string/string-sequences/int-sequences or a list of pair of\n                string/string-sequences/int-sequence (see details in encode_plus)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary`\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        \"\"\"", "\n", "batch_outputs", "=", "{", "}", "\n", "for", "ids_or_pair_ids", "in", "batch_text_or_text_pairs", ":", "\n", "            ", "if", "isinstance", "(", "ids_or_pair_ids", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "assert", "len", "(", "ids_or_pair_ids", ")", "==", "2", "\n", "ids", ",", "pair_ids", "=", "ids_or_pair_ids", "\n", "", "else", ":", "\n", "                ", "ids", ",", "pair_ids", "=", "ids_or_pair_ids", ",", "None", "\n", "", "outputs", "=", "self", ".", "encode_plus", "(", "\n", "ids", ",", "\n", "pair_ids", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "max_length", "=", "max_length", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_tensors", "=", "None", ",", "\n", ")", "\n", "\n", "# Append the non-padded length to the output", "\n", "if", "return_input_lengths", ":", "\n", "                ", "outputs", "[", "\"input_len\"", "]", "=", "len", "(", "outputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "outputs", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "not", "in", "batch_outputs", ":", "\n", "                    ", "batch_outputs", "[", "key", "]", "=", "[", "]", "\n", "", "batch_outputs", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n", "# Compute longest sequence size", "\n", "", "", "max_seq_len", "=", "max", "(", "map", "(", "len", ",", "batch_outputs", "[", "\"input_ids\"", "]", ")", ")", "\n", "\n", "if", "return_attention_masks", ":", "\n", "# Allow the model to not give any special attention to padded input", "\n", "            ", "batch_outputs", "[", "\"attention_mask\"", "]", "=", "[", "[", "0", "]", "*", "len", "(", "v", ")", "for", "v", "in", "batch_outputs", "[", "\"input_ids\"", "]", "]", "\n", "\n", "", "if", "return_tensors", "is", "not", "None", ":", "\n", "\n", "# Do the tensor conversion in batch", "\n", "            ", "for", "key", ",", "value", "in", "batch_outputs", ".", "items", "(", ")", ":", "\n", "\n", "                ", "padded_value", "=", "value", "\n", "if", "key", "!=", "\"input_len\"", ":", "\n", "# Padding handle", "\n", "                    ", "padded_value", "=", "[", "\n", "v", "+", "[", "self", ".", "pad_token_id", "if", "key", "==", "\"input_ids\"", "else", "1", "]", "*", "(", "max_seq_len", "-", "len", "(", "v", ")", ")", "\n", "for", "v", "in", "padded_value", "\n", "]", "\n", "\n", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "                    ", "batch_outputs", "[", "key", "]", "=", "tf", ".", "constant", "(", "padded_value", ")", "\n", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "                    ", "batch_outputs", "[", "key", "]", "=", "torch", ".", "tensor", "(", "padded_value", ")", "\n", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "# encoder_attention_mask requires 1 for real token, 0 for padding, just invert value", "\n", "", "", "", "if", "return_attention_masks", ":", "\n", "            ", "if", "is_tf_available", "(", ")", ":", "\n", "                ", "batch_outputs", "[", "\"attention_mask\"", "]", "=", "tf", ".", "abs", "(", "batch_outputs", "[", "\"attention_mask\"", "]", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "batch_outputs", "[", "\"attention_mask\"", "]", "=", "torch", ".", "abs", "(", "batch_outputs", "[", "\"attention_mask\"", "]", "-", "1", ")", "\n", "\n", "", "", "return", "batch_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.prepare_for_model": [[1032, 1216], ["bool", "len", "len", "tokenization_utils.PreTrainedTokenizer.truncate_sequences", "tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens", "tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences", "tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask", "logger.warning", "logger.warning", "file_utils.is_tf_available", "tf.constant", "tf.constant", "tokenization_utils.PreTrainedTokenizer.num_added_tokens", "len", "len", "len", "tf.constant", "file_utils.is_torch_available", "torch.tensor", "torch.tensor", "len", "len", "ValueError", "len", "torch.tensor", "logger.warning", "len", "len", "len", "len", "str", "len"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.truncate_sequences", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_tf_available", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.num_added_tokens", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_torch_available"], ["", "def", "prepare_for_model", "(", "\n", "self", ",", "\n", "ids", ",", "\n", "pair_ids", "=", "None", ",", "\n", "max_length", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n        It adds special tokens, truncates\n        sequences if overflowing while taking into account the special tokens and manages a window stride for\n        overflowing tokens\n\n        Args:\n            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n                list of inputs.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    token_type_ids: list[int] if return_token_type_ids is True (default)\n                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n                ``input_ids``: list of token ids to be fed to a model\n                ``token_type_ids``: list of token type ids to be fed to a model\n\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "pair", "=", "bool", "(", "pair_ids", "is", "not", "None", ")", "\n", "len_ids", "=", "len", "(", "ids", ")", "\n", "len_pair_ids", "=", "len", "(", "pair_ids", ")", "if", "pair", "else", "0", "\n", "\n", "encoded_inputs", "=", "{", "}", "\n", "\n", "# Handle max sequence length", "\n", "total_len", "=", "len_ids", "+", "len_pair_ids", "+", "(", "self", ".", "num_added_tokens", "(", "pair", "=", "pair", ")", "if", "add_special_tokens", "else", "0", ")", "\n", "if", "max_length", "and", "total_len", ">", "max_length", ":", "\n", "            ", "ids", ",", "pair_ids", ",", "overflowing_tokens", "=", "self", ".", "truncate_sequences", "(", "\n", "ids", ",", "\n", "pair_ids", "=", "pair_ids", ",", "\n", "num_tokens_to_remove", "=", "total_len", "-", "max_length", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "stride", "=", "stride", ",", "\n", ")", "\n", "if", "return_overflowing_tokens", ":", "\n", "                ", "encoded_inputs", "[", "\"overflowing_tokens\"", "]", "=", "overflowing_tokens", "\n", "encoded_inputs", "[", "\"num_truncated_tokens\"", "]", "=", "total_len", "-", "max_length", "\n", "\n", "# Handle special_tokens", "\n", "", "", "if", "add_special_tokens", ":", "\n", "            ", "sequence", "=", "self", ".", "build_inputs_with_special_tokens", "(", "ids", ",", "pair_ids", ")", "\n", "token_type_ids", "=", "self", ".", "create_token_type_ids_from_sequences", "(", "ids", ",", "pair_ids", ")", "\n", "", "else", ":", "\n", "            ", "sequence", "=", "ids", "+", "pair_ids", "if", "pair", "else", "ids", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "ids", ")", "+", "(", "[", "1", "]", "*", "len", "(", "pair_ids", ")", "if", "pair", "else", "[", "]", ")", "\n", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "            ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "self", ".", "get_special_tokens_mask", "(", "ids", ",", "pair_ids", ")", "\n", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "sequence", "\n", "if", "return_token_type_ids", ":", "\n", "            ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "token_type_ids", "\n", "\n", "", "if", "max_length", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", ">", "max_length", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "encoded_inputs", "[", "\"input_ids\"", "]", "[", ":", "max_length", "]", "\n", "if", "return_token_type_ids", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "encoded_inputs", "[", "\"token_type_ids\"", "]", "[", ":", "max_length", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "[", ":", "max_length", "]", "\n", "\n", "", "", "if", "max_length", "is", "None", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum sequence length \"", "\n", "\"for this model ({} > {}). Running this sequence through the model will result in \"", "\n", "\"indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "\n", "", "needs_to_be_padded", "=", "pad_to_max_length", "and", "(", "\n", "max_length", "\n", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "<", "max_length", "\n", "or", "max_length", "is", "None", "\n", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "<", "self", ".", "max_len", "\n", "and", "self", ".", "max_len", "<=", "10000", "\n", ")", "\n", "\n", "if", "pad_to_max_length", "and", "max_length", "is", "None", "and", "self", ".", "max_len", ">", "10000", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Sequence can't be padded as no maximum length is specified and the model maximum length is too high.\"", "\n", ")", "\n", "\n", "", "if", "needs_to_be_padded", ":", "\n", "            ", "difference", "=", "(", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "max_len", ")", "-", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "if", "self", ".", "padding_side", "==", "\"right\"", ":", "\n", "                ", "if", "return_attention_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "+", "[", "0", "]", "*", "difference", "\n", "", "if", "return_token_type_ids", ":", "\n", "                    ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "(", "\n", "encoded_inputs", "[", "\"token_type_ids\"", "]", "+", "[", "self", ".", "pad_token_type_id", "]", "*", "difference", "\n", ")", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "+", "[", "1", "]", "*", "difference", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "encoded_inputs", "[", "\"input_ids\"", "]", "+", "[", "self", ".", "pad_token_id", "]", "*", "difference", "\n", "", "elif", "self", ".", "padding_side", "==", "\"left\"", ":", "\n", "                ", "if", "return_attention_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "0", "]", "*", "difference", "+", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "", "if", "return_token_type_ids", ":", "\n", "                    ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "[", "self", ".", "pad_token_type_id", "]", "*", "difference", "+", "encoded_inputs", "[", "\n", "\"token_type_ids\"", "\n", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "[", "1", "]", "*", "difference", "+", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "[", "self", ".", "pad_token_id", "]", "*", "difference", "+", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid padding strategy:\"", "+", "str", "(", "self", ".", "padding_side", ")", ")", "\n", "\n", "", "", "elif", "return_attention_mask", ":", "\n", "            ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "# Prepare inputs as tensors if asked", "\n", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"input_ids\"", "]", "]", ")", "\n", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "\n", "if", "\"attention_mask\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"attention_mask\"", "]", "]", ")", "\n", "\n", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"input_ids\"", "]", "]", ")", "\n", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "\n", "if", "\"attention_mask\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"attention_mask\"", "]", "]", ")", "\n", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "return", "encoded_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.truncate_sequences": [[1217, 1260], ["range", "min", "len", "min", "len", "len", "min", "len", "len", "len", "ValueError", "ValueError", "len"], "methods", ["None"], ["", "def", "truncate_sequences", "(", "\n", "self", ",", "ids", ",", "pair_ids", "=", "None", ",", "num_tokens_to_remove", "=", "0", ",", "truncation_strategy", "=", "\"longest_first\"", ",", "stride", "=", "0", "\n", ")", ":", "\n", "        ", "\"\"\"Truncates a sequence pair in place to the maximum length.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences).\n                    Overflowing tokens only contains overflow from the first sequence.\n                - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n        \"\"\"", "\n", "if", "num_tokens_to_remove", "<=", "0", ":", "\n", "            ", "return", "ids", ",", "pair_ids", ",", "[", "]", "\n", "\n", "", "if", "truncation_strategy", "==", "\"longest_first\"", ":", "\n", "            ", "overflowing_tokens", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_tokens_to_remove", ")", ":", "\n", "                ", "if", "pair_ids", "is", "None", "or", "len", "(", "ids", ")", ">", "len", "(", "pair_ids", ")", ":", "\n", "                    ", "overflowing_tokens", "=", "[", "ids", "[", "-", "1", "]", "]", "+", "overflowing_tokens", "\n", "ids", "=", "ids", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "pair_ids", "=", "pair_ids", "[", ":", "-", "1", "]", "\n", "", "", "window_len", "=", "min", "(", "len", "(", "ids", ")", ",", "stride", ")", "\n", "if", "window_len", ">", "0", ":", "\n", "                ", "overflowing_tokens", "=", "ids", "[", "-", "window_len", ":", "]", "+", "overflowing_tokens", "\n", "", "", "elif", "truncation_strategy", "==", "\"only_first\"", ":", "\n", "            ", "assert", "len", "(", "ids", ")", ">", "num_tokens_to_remove", "\n", "window_len", "=", "min", "(", "len", "(", "ids", ")", ",", "stride", "+", "num_tokens_to_remove", ")", "\n", "overflowing_tokens", "=", "ids", "[", "-", "window_len", ":", "]", "\n", "ids", "=", "ids", "[", ":", "-", "num_tokens_to_remove", "]", "\n", "", "elif", "truncation_strategy", "==", "\"only_second\"", ":", "\n", "            ", "assert", "pair_ids", "is", "not", "None", "and", "len", "(", "pair_ids", ")", ">", "num_tokens_to_remove", "\n", "window_len", "=", "min", "(", "len", "(", "pair_ids", ")", ",", "stride", "+", "num_tokens_to_remove", ")", "\n", "overflowing_tokens", "=", "pair_ids", "[", "-", "window_len", ":", "]", "\n", "pair_ids", "=", "pair_ids", "[", ":", "-", "num_tokens_to_remove", "]", "\n", "", "elif", "truncation_strategy", "==", "\"do_not_truncate\"", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input sequence are too long for max_length. Please select a truncation strategy.\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Truncation_strategy should be selected in ['longest_first', 'only_first', 'only_second', 'do_not_truncate']\"", "\n", ")", "\n", "", "return", "(", "ids", ",", "pair_ids", ",", "overflowing_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences": [[1261, 1265], ["len", "len", "len"], "methods", ["None"], ["", "def", "create_token_type_ids_from_sequences", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "len", "(", "token_ids_0", ")", "*", "[", "0", "]", "\n", "", "return", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", "+", "[", "1", "]", "*", "len", "(", "token_ids_1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens": [[1266, 1277], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        \"\"\"", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "token_ids_0", "\n", "", "return", "token_ids_0", "+", "token_ids_1", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask": [[1278, 1294], ["len", "len"], "methods", ["None"], ["", "def", "get_special_tokens_mask", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ",", "already_has_special_tokens", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n        \"\"\"", "\n", "return", "[", "0", "]", "*", "(", "(", "len", "(", "token_ids_1", ")", "if", "token_ids_1", "else", "0", ")", "+", "len", "(", "token_ids_0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens": [[1295, 1317], ["isinstance", "int", "tokenization_utils.PreTrainedTokenizer._convert_id_to_token", "tokens.append", "tokens.append", "tokenization_utils.PreTrainedTokenizer._convert_id_to_token"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ",", "skip_special_tokens", "=", "False", ")", ":", "\n", "        ", "\"\"\" Converts a single index or a sequence of indices (integers) in a token \"\n            (resp.) a sequence of tokens (str), using the vocabulary and added tokens.\n\n            Args:\n                skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n        \"\"\"", "\n", "if", "isinstance", "(", "ids", ",", "int", ")", ":", "\n", "            ", "if", "ids", "in", "self", ".", "added_tokens_decoder", ":", "\n", "                ", "return", "self", ".", "added_tokens_decoder", "[", "ids", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "_convert_id_to_token", "(", "ids", ")", "\n", "", "", "tokens", "=", "[", "]", "\n", "for", "index", "in", "ids", ":", "\n", "            ", "index", "=", "int", "(", "index", ")", "\n", "if", "skip_special_tokens", "and", "index", "in", "self", ".", "all_special_ids", ":", "\n", "                ", "continue", "\n", "", "if", "index", "in", "self", ".", "added_tokens_decoder", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "added_tokens_decoder", "[", "index", "]", ")", "\n", "", "else", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "_convert_id_to_token", "(", "index", ")", ")", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer._convert_id_to_token": [[1318, 1320], ["None"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string": [[1321, 1327], ["tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a sequence of tokens (string) in a single string.\n            The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n            but we often want to remove sub-word tokenization artifacts at the same time.\n        \"\"\"", "\n", "return", "\" \"", ".", "join", "(", "self", ".", "convert_ids_to_tokens", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.decode": [[1328, 1365], ["tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "sub_texts.append", "tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "sub_texts.append", "current_sub_text.append", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string", "sub_texts.append", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string"], ["", "def", "decode", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n        with options to remove special tokens and clean up tokenization spaces.\n        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n\n        Args:\n            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n            skip_special_tokens: if set to True, will replace special tokens.\n            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n        \"\"\"", "\n", "filtered_tokens", "=", "self", ".", "convert_ids_to_tokens", "(", "token_ids", ",", "skip_special_tokens", "=", "skip_special_tokens", ")", "\n", "\n", "# To avoid mixing byte-level and unicode for byte-level BPT", "\n", "# we need to build string separatly for added tokens and byte-level tokens", "\n", "# cf. https://github.com/huggingface/transformers/issues/1133", "\n", "sub_texts", "=", "[", "]", "\n", "current_sub_text", "=", "[", "]", "\n", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "if", "skip_special_tokens", "and", "token", "in", "self", ".", "all_special_ids", ":", "\n", "                ", "continue", "\n", "", "if", "token", "in", "self", ".", "added_tokens_encoder", ":", "\n", "                ", "if", "current_sub_text", ":", "\n", "                    ", "sub_texts", ".", "append", "(", "self", ".", "convert_tokens_to_string", "(", "current_sub_text", ")", ")", "\n", "current_sub_text", "=", "[", "]", "\n", "", "sub_texts", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "current_sub_text", ".", "append", "(", "token", ")", "\n", "", "", "if", "current_sub_text", ":", "\n", "            ", "sub_texts", ".", "append", "(", "self", ".", "convert_tokens_to_string", "(", "current_sub_text", ")", ")", "\n", "", "text", "=", "\" \"", ".", "join", "(", "sub_texts", ")", "\n", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "clean_text", "=", "self", ".", "clean_up_tokenization", "(", "text", ")", "\n", "return", "clean_text", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.special_tokens_map": [[1366, 1377], ["getattr"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "special_tokens_map", "(", "self", ")", ":", "\n", "        ", "\"\"\" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n            values ('<unk>', '<cls>'...)\n        \"\"\"", "\n", "set_attr", "=", "{", "}", "\n", "for", "attr", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", ":", "\n", "            ", "attr_value", "=", "getattr", "(", "self", ",", "\"_\"", "+", "attr", ")", "\n", "if", "attr_value", ":", "\n", "                ", "set_attr", "[", "attr", "]", "=", "attr_value", "\n", "", "", "return", "set_attr", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.all_special_tokens": [[1378, 1389], ["set_attr.values", "list", "set", "isinstance", "list"], "methods", ["None"], ["", "@", "property", "\n", "def", "all_special_tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\" List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n            (cls_token, unk_token...).\n        \"\"\"", "\n", "all_toks", "=", "[", "]", "\n", "set_attr", "=", "self", ".", "special_tokens_map", "\n", "for", "attr_value", "in", "set_attr", ".", "values", "(", ")", ":", "\n", "            ", "all_toks", "=", "all_toks", "+", "(", "list", "(", "attr_value", ")", "if", "isinstance", "(", "attr_value", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "attr_value", "]", ")", "\n", "", "all_toks", "=", "list", "(", "set", "(", "all_toks", ")", ")", "\n", "return", "all_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.all_special_ids": [[1390, 1398], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "all_special_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\" List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n            class attributes (cls_token, unk_token...).\n        \"\"\"", "\n", "all_toks", "=", "self", ".", "all_special_tokens", "\n", "all_ids", "=", "self", ".", "convert_tokens_to_ids", "(", "all_toks", ")", "\n", "return", "all_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization": [[1399, 1417], ["out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "clean_up_tokenization", "(", "out_string", ")", ":", "\n", "        ", "\"\"\" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n        \"\"\"", "\n", "out_string", "=", "(", "\n", "out_string", ".", "replace", "(", "\" .\"", ",", "\".\"", ")", "\n", ".", "replace", "(", "\" ?\"", ",", "\"?\"", ")", "\n", ".", "replace", "(", "\" !\"", ",", "\"!\"", ")", "\n", ".", "replace", "(", "\" ,\"", ",", "\",\"", ")", "\n", ".", "replace", "(", "\" ' \"", ",", "\"'\"", ")", "\n", ".", "replace", "(", "\" n't\"", ",", "\"n't\"", ")", "\n", ".", "replace", "(", "\" 'm\"", ",", "\"'m\"", ")", "\n", ".", "replace", "(", "\" do not\"", ",", "\" don't\"", ")", "\n", ".", "replace", "(", "\" 's\"", ",", "\"'s\"", ")", "\n", ".", "replace", "(", "\" 've\"", ",", "\"'ve\"", ")", "\n", ".", "replace", "(", "\" 're\"", ",", "\"'re\"", ")", "\n", ")", "\n", "return", "out_string", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.__init__": [[1423, 1425], ["tokenization_utils.PreTrainedTokenizer.__init__"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedTokenizerFast", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.tokenizer": [[1426, 1431], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokenizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_tokenizer", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "self", ".", "_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decoder": [[1432, 1437], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_decoder", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "self", ".", "_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.vocab_size": [[1438, 1441], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.get_vocab_size"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "get_vocab_size", "(", "with_added_tokens", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.__len__": [[1442, 1444], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.get_vocab_size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "get_vocab_size", "(", "with_added_tokens", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.bos_token": [[1445, 1449], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "bos_token", ".", "setter", "\n", "def", "bos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.eos_token": [[1450, 1454], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "eos_token", ".", "setter", "\n", "def", "eos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_eos_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.unk_token": [[1455, 1459], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "unk_token", ".", "setter", "\n", "def", "unk_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_unk_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.sep_token": [[1460, 1464], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "sep_token", ".", "setter", "\n", "def", "sep_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_sep_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.pad_token": [[1465, 1469], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "pad_token", ".", "setter", "\n", "def", "pad_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_pad_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.cls_token": [[1470, 1474], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "cls_token", ".", "setter", "\n", "def", "cls_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_cls_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.mask_token": [[1475, 1479], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_mask_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.additional_special_tokens": [[1480, 1484], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "additional_special_tokens", ".", "setter", "\n", "def", "additional_special_tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_additional_special_tokens", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens": [[1485, 1488], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.add_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens"], ["", "def", "_update_special_tokens", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_tokenizer", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tokenizer", ".", "add_special_tokens", "(", "self", ".", "all_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_encoding": [[1489, 1533], ["file_utils.is_tf_available", "tf.constant", "tf.constant", "tf.constant", "file_utils.is_torch_available", "torch.tensor", "torch.tensor", "torch.tensor", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_tf_available", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_torch_available"], ["", "", "@", "staticmethod", "\n", "def", "_convert_encoding", "(", "\n", "encoding", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", ")", ":", "\n", "        ", "encoding_dict", "=", "{", "\n", "\"input_ids\"", ":", "encoding", ".", "ids", ",", "\n", "}", "\n", "if", "return_token_type_ids", ":", "\n", "            ", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "encoding", ".", "type_ids", "\n", "", "if", "return_attention_mask", ":", "\n", "            ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "encoding", ".", "attention_mask", "\n", "", "if", "return_overflowing_tokens", ":", "\n", "            ", "overflowing", "=", "encoding", ".", "overflowing", "\n", "encoding_dict", "[", "\"overflowing_tokens\"", "]", "=", "overflowing", ".", "ids", "if", "overflowing", "is", "not", "None", "else", "[", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "            ", "encoding_dict", "[", "\"special_tokens_mask\"", "]", "=", "encoding", ".", "special_tokens_mask", "\n", "\n", "# Prepare inputs as tensors if asked", "\n", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoding_dict", "[", "\"input_ids\"", "]", "]", ")", "\n", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoding_dict", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "\n", "if", "\"attention_mask\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "tf", ".", "constant", "(", "[", "encoding_dict", "[", "\"attention_mask\"", "]", "]", ")", "\n", "\n", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoding_dict", "[", "\"input_ids\"", "]", "]", ")", "\n", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoding_dict", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "\n", "if", "\"attention_mask\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoding_dict", "[", "\"attention_mask\"", "]", "]", ")", "\n", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "return", "encoding_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.encode_plus": [[1534, 1553], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.encode", "tokenization_utils.PreTrainedTokenizerFast._convert_encoding"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_encoding"], ["", "def", "encode_plus", "(", "\n", "self", ",", "\n", "text", ",", "\n", "text_pair", "=", "None", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "encoding", "=", "self", ".", "tokenizer", ".", "encode", "(", "text", ",", "text_pair", ")", "\n", "return", "self", ".", "_convert_encoding", "(", "\n", "encoding", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.tokenize": [[1555, 1557], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.encode"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", ".", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc": [[1558, 1563], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.token_to_id"], "methods", ["None"], ["", "def", "_convert_token_to_id_with_added_voc", "(", "self", ",", "token", ")", ":", "\n", "        ", "id", "=", "self", ".", "tokenizer", ".", "token_to_id", "(", "token", ")", "\n", "if", "id", "is", "None", ":", "\n", "            ", "return", "self", ".", "unk_token_id", "\n", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token": [[1564, 1566], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.id_to_token", "int"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "id_to_token", "(", "int", "(", "index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string": [[1567, 1569], ["tokenization_utils.PreTrainedTokenizerFast.decoder.decode"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "self", ".", "decoder", ".", "decode", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_tokens": [[1570, 1572], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.add_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_tokens"], ["", "def", "add_tokens", "(", "self", ",", "new_tokens", ")", ":", "\n", "        ", "self", ".", "tokenizer", ".", "add_tokens", "(", "new_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens": [[1573, 1577], ["tokenization_utils.PreTrainedTokenizer.add_special_tokens", "tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "def", "add_special_tokens", "(", "self", ",", "special_tokens_dict", ")", ":", "\n", "        ", "added", "=", "super", "(", ")", ".", "add_special_tokens", "(", "special_tokens_dict", ")", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "return", "added", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.encode_batch": [[1578, 1597], ["tokenization_utils.PreTrainedTokenizerFast._convert_encoding", "tokenization_utils.PreTrainedTokenizerFast.tokenizer.encode_batch"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_encoding", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.encode_batch"], ["", "def", "encode_batch", "(", "\n", "self", ",", "\n", "texts", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", ")", ":", "\n", "        ", "return", "[", "\n", "self", ".", "_convert_encoding", "(", "\n", "encoding", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", ")", "\n", "for", "encoding", "in", "self", ".", "tokenizer", ".", "encode_batch", "(", "texts", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode": [[1599, 1607], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.decode", "tokenization_utils.PreTrainedTokenizerFast.clean_up_tokenization"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization"], ["", "def", "decode", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "True", ")", ":", "\n", "        ", "text", "=", "self", ".", "tokenizer", ".", "decode", "(", "token_ids", ",", "skip_special_tokens", ")", "\n", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "clean_text", "=", "self", ".", "clean_up_tokenization", "(", "text", ")", "\n", "return", "clean_text", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode_batch": [[1608, 1612], ["tokenization_utils.PreTrainedTokenizerFast.clean_up_tokenization", "tokenization_utils.PreTrainedTokenizerFast.tokenizer.decode_batch"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode_batch"], ["", "", "def", "decode_batch", "(", "self", ",", "ids_batch", ",", "skip_special_tokens", "=", "False", ",", "clear_up_tokenization_spaces", "=", "True", ")", ":", "\n", "        ", "return", "[", "\n", "self", ".", "clean_up_tokenization", "(", "text", ")", "if", "clear_up_tokenization_spaces", "else", "text", "\n", "for", "text", "in", "self", ".", "tokenizer", ".", "decode_batch", "(", "ids_batch", ",", "skip_special_tokens", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.dummy_inputs": [[77, 85], ["torch.tensor"], "methods", ["None"], ["@", "property", "\n", "def", "dummy_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\" Dummy inputs to do a forward pass in the network.\n\n        Returns:\n            torch.Tensor with dummy inputs\n        \"\"\"", "\n", "return", "{", "\"input_ids\"", ":", "torch", ".", "tensor", "(", "DUMMY_INPUTS", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.__init__": [[86, 98], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["", "def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "PretrainedConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"", "\n", "\"To create a model from a pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", "\n", ")", "\n", "# Save config in model", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.base_model": [[99, 102], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "base_model", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.get_input_embeddings": [[103, 111], ["getattr", "getattr.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.get_input_embeddings"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\" Get model's input embeddings\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "\n", "if", "base_model", "is", "not", "self", ":", "\n", "            ", "return", "base_model", ".", "get_input_embeddings", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.set_input_embeddings": [[112, 120], ["getattr", "getattr.set_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.set_input_embeddings"], ["", "", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set model's input embeddings\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "\n", "if", "base_model", "is", "not", "self", ":", "\n", "            ", "base_model", ".", "set_input_embeddings", "(", "value", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.get_output_embeddings": [[121, 126], ["None"], "methods", ["None"], ["", "", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\" Get model's output embeddings\n            Return None if the model doesn't have output embeddings\n        \"\"\"", "\n", "return", "None", "# Overwrite for models with output embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.tie_weights": [[127, 134], ["modeling_utils.PreTrainedModel.get_output_embeddings", "modeling_utils.PreTrainedModel._tie_or_clone_weights", "modeling_utils.PreTrainedModel.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2DoubleHeadsModel.get_output_embeddings", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._tie_or_clone_weights", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.get_input_embeddings"], ["", "def", "tie_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n        \"\"\"", "\n", "output_embeddings", "=", "self", ".", "get_output_embeddings", "(", ")", "\n", "if", "output_embeddings", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tie_or_clone_weights", "(", "output_embeddings", ",", "self", ".", "get_input_embeddings", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._tie_or_clone_weights": [[135, 152], ["torch.nn.Parameter", "hasattr", "torch.nn.functional.pad", "hasattr", "hasattr", "input_embeddings.weight.clone"], "methods", ["None"], ["", "", "def", "_tie_or_clone_weights", "(", "self", ",", "output_embeddings", ",", "input_embeddings", ")", ":", "\n", "        ", "\"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n        \"\"\"", "\n", "if", "self", ".", "config", ".", "torchscript", ":", "\n", "            ", "output_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "input_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "output_embeddings", ".", "weight", "=", "input_embeddings", ".", "weight", "\n", "\n", "", "if", "hasattr", "(", "output_embeddings", ",", "\"bias\"", ")", "and", "output_embeddings", ".", "bias", "is", "not", "None", ":", "\n", "            ", "output_embeddings", ".", "bias", ".", "data", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "\n", "output_embeddings", ".", "bias", ".", "data", ",", "\n", "(", "0", ",", "output_embeddings", ".", "weight", ".", "shape", "[", "0", "]", "-", "output_embeddings", ".", "bias", ".", "shape", "[", "0", "]", ")", ",", "\n", "\"constant\"", ",", "\n", "0", ",", "\n", ")", "\n", "", "if", "hasattr", "(", "output_embeddings", ",", "\"out_features\"", ")", "and", "hasattr", "(", "input_embeddings", ",", "\"num_embeddings\"", ")", ":", "\n", "            ", "output_embeddings", ".", "out_features", "=", "input_embeddings", ".", "num_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.resize_token_embeddings": [[153, 179], ["getattr", "getattr._resize_token_embeddings", "modeling_utils.PreTrainedModel.tie_weights"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._resize_token_embeddings", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.tie_weights"], ["", "", "def", "resize_token_embeddings", "(", "self", ",", "new_num_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Arguments:\n\n            new_num_tokens: (`optional`) int:\n                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.\n                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n\n        Return: ``torch.nn.Embeddings``\n            Pointer to the input tokens Embeddings Module of the model\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "# get the base model if needed", "\n", "model_embeds", "=", "base_model", ".", "_resize_token_embeddings", "(", "new_num_tokens", ")", "\n", "if", "new_num_tokens", "is", "None", ":", "\n", "            ", "return", "model_embeds", "\n", "\n", "# Update base model and current model config", "\n", "", "self", ".", "config", ".", "vocab_size", "=", "new_num_tokens", "\n", "base_model", ".", "vocab_size", "=", "new_num_tokens", "\n", "\n", "# Tie weights again if needed", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n", "return", "model_embeds", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._resize_token_embeddings": [[180, 185], ["modeling_utils.PreTrainedModel.get_input_embeddings", "modeling_utils.PreTrainedModel._get_resized_embeddings", "modeling_utils.PreTrainedModel.set_input_embeddings", "modeling_utils.PreTrainedModel.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.get_input_embeddings", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._get_resized_embeddings", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.set_input_embeddings", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.get_input_embeddings"], ["", "def", "_resize_token_embeddings", "(", "self", ",", "new_num_tokens", ")", ":", "\n", "        ", "old_embeddings", "=", "self", ".", "get_input_embeddings", "(", ")", "\n", "new_embeddings", "=", "self", ".", "_get_resized_embeddings", "(", "old_embeddings", ",", "new_num_tokens", ")", "\n", "self", ".", "set_input_embeddings", "(", "new_embeddings", ")", "\n", "return", "self", ".", "get_input_embeddings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._get_resized_embeddings": [[186, 219], ["old_embeddings.weight.size", "torch.nn.Embedding", "torch.nn.Embedding.to", "modeling_utils.PreTrainedModel._init_weights", "min"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2PreTrainedModel._init_weights"], ["", "def", "_get_resized_embeddings", "(", "self", ",", "old_embeddings", ",", "new_num_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\" Build a resized Embedding Module from a provided token Embedding Module.\n            Increasing the size will add newly initialized vectors at the end\n            Reducing the size will remove vectors from the end\n\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: return the provided token Embedding Module.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n        \"\"\"", "\n", "if", "new_num_tokens", "is", "None", ":", "\n", "            ", "return", "old_embeddings", "\n", "\n", "", "old_num_tokens", ",", "old_embedding_dim", "=", "old_embeddings", ".", "weight", ".", "size", "(", ")", "\n", "if", "old_num_tokens", "==", "new_num_tokens", ":", "\n", "            ", "return", "old_embeddings", "\n", "\n", "# Build new embeddings", "\n", "", "new_embeddings", "=", "nn", ".", "Embedding", "(", "new_num_tokens", ",", "old_embedding_dim", ")", "\n", "new_embeddings", ".", "to", "(", "old_embeddings", ".", "weight", ".", "device", ")", "\n", "\n", "# initialize all new embeddings (in particular added tokens)", "\n", "self", ".", "_init_weights", "(", "new_embeddings", ")", "\n", "\n", "# Copy word embeddings from the previous weights", "\n", "num_tokens_to_copy", "=", "min", "(", "old_num_tokens", ",", "new_num_tokens", ")", "\n", "new_embeddings", ".", "weight", ".", "data", "[", ":", "num_tokens_to_copy", ",", ":", "]", "=", "old_embeddings", ".", "weight", ".", "data", "[", ":", "num_tokens_to_copy", ",", ":", "]", "\n", "\n", "return", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights": [[220, 231], ["modeling_utils.PreTrainedModel.apply", "modeling_utils.PreTrainedModel.tie_weights", "modeling_utils.PreTrainedModel.prune_heads"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.tie_weights", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.prune_heads"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\" Initialize and prunes weights if needed. \"\"\"", "\n", "# Initialize weights", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n", "# Prune heads if needed", "\n", "if", "self", ".", "config", ".", "pruned_heads", ":", "\n", "            ", "self", ".", "prune_heads", "(", "self", ".", "config", ".", "pruned_heads", ")", "\n", "\n", "# Tie weights if needed", "\n", "", "self", ".", "tie_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.prune_heads": [[232, 246], ["heads_to_prune.items", "modeling_utils.PreTrainedModel.base_model._prune_heads", "list", "set", "set", "modeling_utils.PreTrainedModel.config.pruned_heads.get"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model._prune_heads"], ["", "def", "prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the base model.\n\n            Arguments:\n\n                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n                E.g. {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n        \"\"\"", "\n", "# save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "union_heads", "=", "set", "(", "self", ".", "config", ".", "pruned_heads", ".", "get", "(", "layer", ",", "[", "]", ")", ")", "|", "set", "(", "heads", ")", "\n", "self", ".", "config", ".", "pruned_heads", "[", "layer", "]", "=", "list", "(", "union_heads", ")", "# Unfortunately we have to store it as list for JSON", "\n", "\n", "", "self", ".", "base_model", ".", "_prune_heads", "(", "heads_to_prune", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.save_pretrained": [[247, 265], ["os.path.isdir", "model_to_save.config.save_pretrained", "os.path.join", "torch.save", "logger.info", "hasattr", "model_to_save.state_dict"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.save_pretrained"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save a model and its configuration file to a directory, so that it\n            can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n        \"\"\"", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "\n", "save_directory", "\n", ")", ",", "\"Saving path should be a directory where the model and configuration can be saved\"", "\n", "\n", "# Only save the model itself if we are using distributed training", "\n", "model_to_save", "=", "self", ".", "module", "if", "hasattr", "(", "self", ",", "\"module\"", ")", "else", "self", "\n", "\n", "# Save configuration file", "\n", "model_to_save", ".", "config", ".", "save_pretrained", "(", "save_directory", ")", "\n", "\n", "# If we save using the predefined names, we can load using `from_pretrained`", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "WEIGHTS_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "logger", ".", "info", "(", "\"Model weights saved in {}\"", ".", "format", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.from_pretrained": [[266, 538], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "cls", "load_tf2_checkpoint_in_pytorch_model.tie_weights", "load_tf2_checkpoint_in_pytorch_model.eval", "isinstance", "cls.config_class.from_pretrained", "file_utils.cached_path.endswith", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling_utils.PreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.tie_weights", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "*", "model_args", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with ``model.train()``\n\n        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n        It is up to you to train those weights with a downstream fine-tuning task.\n\n        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n                - None if you are both providing the configuration and state dictionary (resp. with keyword arguments ``config`` and ``state_dict``)\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n\n            config: (`optional`) one of:\n                    - an instance of a class derived from :class:`~transformers.PretrainedConfig`, or\n                    - a string valid as input to :func:`~transformers.PretrainedConfig.from_pretrained()`\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            resume_download: (`optional`) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n\n        Examples::\n\n            model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n            model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n            model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"", "\n", "config", "=", "kwargs", ".", "pop", "(", "\"config\"", ",", "None", ")", "\n", "state_dict", "=", "kwargs", ".", "pop", "(", "\"state_dict\"", ",", "None", ")", "\n", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "from_tf", "=", "kwargs", ".", "pop", "(", "\"from_tf\"", ",", "False", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "output_loading_info", "=", "kwargs", ".", "pop", "(", "\"output_loading_info\"", ",", "False", ")", "\n", "\n", "# Load config if we don't provide a configuration", "\n", "if", "not", "isinstance", "(", "config", ",", "PretrainedConfig", ")", ":", "\n", "            ", "config_path", "=", "config", "if", "config", "is", "not", "None", "else", "pretrained_model_name_or_path", "\n", "config", ",", "model_kwargs", "=", "cls", ".", "config_class", ".", "from_pretrained", "(", "\n", "config_path", ",", "\n", "*", "model_args", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "return_unused_kwargs", "=", "True", ",", "\n", "force_download", "=", "force_download", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "model_kwargs", "=", "kwargs", "\n", "\n", "# Load model", "\n", "", "if", "pretrained_model_name_or_path", "is", "not", "None", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_model_archive_map", ":", "\n", "                ", "archive_file", "=", "cls", ".", "pretrained_model_archive_map", "[", "pretrained_model_name_or_path", "]", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                ", "if", "from_tf", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", ")", ")", ":", "\n", "# Load from a TF 1.0 checkpoint", "\n", "                    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", ")", "\n", "", "elif", "from_tf", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF2_WEIGHTS_NAME", ")", ")", ":", "\n", "# Load from a TF 2.0 checkpoint", "\n", "                    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF2_WEIGHTS_NAME", ")", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", ")", ":", "\n", "# Load from a PyTorch checkpoint", "\n", "                    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "EnvironmentError", "(", "\n", "\"Error no file named {} found in directory {} or `from_tf` set to False\"", ".", "format", "(", "\n", "[", "WEIGHTS_NAME", ",", "TF2_WEIGHTS_NAME", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", "]", ",", "pretrained_model_name_or_path", "\n", ")", "\n", ")", "\n", "", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", "+", "\".index\"", ")", ":", "\n", "                ", "assert", "(", "\n", "from_tf", "\n", ")", ",", "\"We found a TensorFlow checkpoint at {}, please set from_tf to True to load from this checkpoint\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", "+", "\".index\"", "\n", ")", "\n", "archive_file", "=", "pretrained_model_name_or_path", "+", "\".index\"", "\n", "", "else", ":", "\n", "                ", "archive_file", "=", "hf_bucket_url", "(", "pretrained_model_name_or_path", ",", "postfix", "=", "WEIGHTS_NAME", ")", "\n", "if", "from_tf", ":", "\n", "                    ", "raise", "EnvironmentError", "(", "\n", "\"Loading a PyTorch model from a TF checkpoint is not supported when using a model identifier name.\"", "\n", ")", "\n", "\n", "# redirect to the cache, if necessary", "\n", "", "", "try", ":", "\n", "                ", "resolved_archive_file", "=", "cached_path", "(", "\n", "archive_file", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "                ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_model_archive_map", ":", "\n", "                    ", "msg", "=", "\"Couldn't reach server at '{}' to download pretrained weights.\"", ".", "format", "(", "archive_file", ")", "\n", "", "else", ":", "\n", "                    ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to model weight files named one of {} but \"", "\n", "\"couldn't find any such file at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "cls", ".", "pretrained_model_archive_map", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ",", "\n", "[", "WEIGHTS_NAME", ",", "TF2_WEIGHTS_NAME", ",", "TF_WEIGHTS_NAME", "]", ",", "\n", ")", "\n", ")", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading weights file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading weights file {} from cache at {}\"", ".", "format", "(", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "resolved_archive_file", "=", "None", "\n", "\n", "# Instantiate model.", "\n", "", "model", "=", "cls", "(", "config", ",", "*", "model_args", ",", "**", "model_kwargs", ")", "\n", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "try", ":", "\n", "                ", "state_dict", "=", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "\"cpu\"", ")", "\n", "", "except", "Exception", ":", "\n", "                ", "raise", "OSError", "(", "\n", "\"Unable to load weights from pytorch checkpoint file. \"", "\n", "\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"", "\n", ")", "\n", "\n", "", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "\n", "if", "from_tf", ":", "\n", "            ", "if", "resolved_archive_file", ".", "endswith", "(", "\".index\"", ")", ":", "\n", "# Load from a TensorFlow 1.X checkpoint - provided by original authors", "\n", "                ", "model", "=", "cls", ".", "load_tf_weights", "(", "model", ",", "config", ",", "resolved_archive_file", "[", ":", "-", "6", "]", ")", "# Remove the '.index'", "\n", "", "else", ":", "\n", "# Load from our TensorFlow 2.0 checkpoints", "\n", "                ", "try", ":", "\n", "                    ", "from", "transformers", "import", "load_tf2_checkpoint_in_pytorch_model", "\n", "\n", "model", "=", "load_tf2_checkpoint_in_pytorch_model", "(", "model", ",", "resolved_archive_file", ",", "allow_missing_keys", "=", "True", ")", "\n", "", "except", "ImportError", ":", "\n", "                    ", "logger", ".", "error", "(", "\n", "\"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"", "\n", "\"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"", "\n", ")", "\n", "raise", "\n", "", "", "", "else", ":", "\n", "# Convert old format to new format if needed from a PyTorch state_dict", "\n", "            ", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "                ", "new_key", "=", "None", "\n", "if", "\"gamma\"", "in", "key", ":", "\n", "                    ", "new_key", "=", "key", ".", "replace", "(", "\"gamma\"", ",", "\"weight\"", ")", "\n", "", "if", "\"beta\"", "in", "key", ":", "\n", "                    ", "new_key", "=", "key", ".", "replace", "(", "\"beta\"", ",", "\"bias\"", ")", "\n", "", "if", "new_key", ":", "\n", "                    ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "                ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "", "metadata", "=", "getattr", "(", "state_dict", ",", "\"_metadata\"", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "                ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "# PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants", "\n", "# so we need to apply the function recursively.", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "\"\"", ")", ":", "\n", "                ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", "\n", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                    ", "if", "child", "is", "not", "None", ":", "\n", "                        ", "load", "(", "child", ",", "prefix", "+", "name", "+", "\".\"", ")", "\n", "\n", "# Make sure we are able to load base models as well as derived models (with heads)", "\n", "", "", "", "start_prefix", "=", "\"\"", "\n", "model_to_load", "=", "model", "\n", "if", "not", "hasattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "and", "any", "(", "\n", "s", ".", "startswith", "(", "cls", ".", "base_model_prefix", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", "\n", ")", ":", "\n", "                ", "start_prefix", "=", "cls", ".", "base_model_prefix", "+", "\".\"", "\n", "", "if", "hasattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "and", "not", "any", "(", "\n", "s", ".", "startswith", "(", "cls", ".", "base_model_prefix", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", "\n", ")", ":", "\n", "                ", "model_to_load", "=", "getattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "\n", "\n", "", "load", "(", "model_to_load", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", "\n", ")", "\n", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", "\n", ")", "\n", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Error(s) in loading state_dict for {}:\\n\\t{}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", "\n", ")", "\n", ")", "\n", "\n", "", "", "model", ".", "tie_weights", "(", ")", "# make sure word embedding weights are still tied if needed", "\n", "\n", "# Set model in evaluation mode to desactivate DropOut modules by default", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "if", "output_loading_info", ":", "\n", "            ", "loading_info", "=", "{", "\"missing_keys\"", ":", "missing_keys", ",", "\"unexpected_keys\"", ":", "unexpected_keys", ",", "\"error_msgs\"", ":", "error_msgs", "}", "\n", "return", "model", ",", "loading_info", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.prepare_inputs_for_generation": [[539, 541], ["None"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "{", "\"input_ids\"", ":", "input_ids", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._do_output_past": [[542, 552], ["hasattr", "hasattr", "len", "len"], "methods", ["None"], ["", "def", "_do_output_past", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "has_output_past", "=", "hasattr", "(", "self", ".", "config", ",", "\"output_past\"", ")", "and", "self", ".", "config", ".", "output_past", "\n", "has_mem_len", "=", "hasattr", "(", "self", ".", "config", ",", "\"mem_len\"", ")", "and", "self", ".", "config", ".", "mem_len", "\n", "\n", "if", "has_output_past", "and", "not", "has_mem_len", "and", "len", "(", "outputs", ")", ">", "1", ":", "\n", "            ", "return", "True", "\n", "", "elif", "has_mem_len", "and", "self", ".", "config", ".", "mem_len", ">", "0", "and", "len", "(", "outputs", ")", ">", "1", ":", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.generate": [[553, 750], ["torch.no_grad", "isinstance", "isinstance", "modeling_utils.PreTrainedModel.get_output_embeddings", "AttributeError", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "torch.full", "input_ids.contiguous().view.contiguous().view.unsqueeze().expand", "input_ids.contiguous().view.contiguous().view.contiguous().view", "modeling_utils.PreTrainedModel._generate_beam_search", "modeling_utils.PreTrainedModel._generate_no_beam_search", "output.view.view.view", "input_ids.contiguous().view.contiguous().view.dim", "input_ids.contiguous().view.contiguous().view.unsqueeze", "input_ids.contiguous().view.contiguous().view.contiguous", "next", "modeling_utils.PreTrainedModel.parameters"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2DoubleHeadsModel.get_output_embeddings", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._generate_beam_search", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._generate_no_beam_search"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "max_length", "=", "None", ",", "\n", "do_sample", "=", "None", ",", "\n", "num_beams", "=", "None", ",", "\n", "temperature", "=", "None", ",", "\n", "top_k", "=", "None", ",", "\n", "top_p", "=", "None", ",", "\n", "repetition_penalty", "=", "None", ",", "\n", "bos_token_id", "=", "None", ",", "\n", "pad_token_id", "=", "None", ",", "\n", "eos_token_ids", "=", "None", ",", "\n", "length_penalty", "=", "None", ",", "\n", "num_return_sequences", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy or penalized greedy decoding, sampling with top-k or nucleus sampling\n        and beam-search.\n\n        Adapted in part from `Facebook's XLM beam search code`_.\n\n        .. _`Facebook's XLM beam search code`:\n           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n\n\n        Parameters:\n\n            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n                The sequence used as a prompt for the generation. If `None` the method initializes\n                it as an empty `torch.LongTensor` of shape `(1,)`.\n\n            max_length: (`optional`) int\n                The max length of the sequence to be generated.  Between 1 and infinity. Default to 20.\n\n            do_sample: (`optional`) bool\n                If set to `False` greedy decoding is used. Otherwise sampling is used. Default to greedy sampling.\n\n            num_beams: (`optional`) int\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n            temperature: (`optional`) float\n                The value used to module the next token probabilities. Must be strictely positive. Default to 1.0.\n\n            top_k: (`optional`) int\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n\n            top_p: (`optional`) float\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n\n            repetition_penalty: (`optional`) float\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n            bos_token_id: (`optional`) int\n                Beginning of sentence token if no prompt is provided. Default to 0.\n\n            eos_token_ids: (`optional`) int or list of int\n                End of sequence token or list of tokens to stop the generation. Default to 0.\n            length_penalty: (`optional`) float\n                Exponential penalty to the length. Default to 1.\n\n            num_return_sequences: (`optional`) int\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n        Examples::\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            outputs = model.generate(max_length=40, bos_token_id=tokenizer.bos_token_id, eos_token_ids=tokenizer.eos_token_id)  # do greedy decoding without beam search\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, do_sample=True, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, bos_token_id=tokenizer.bos_token_id, eos_token_ids=tokenizer.eos_token_id, num_beams=3)  # generate sequences using greedy beam search decoding (3 beams)\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences using using greedy search\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n        \"\"\"", "\n", "\n", "# We cannot generate if the model does not have a LM head", "\n", "if", "self", ".", "get_output_embeddings", "(", ")", "is", "None", ":", "\n", "            ", "raise", "AttributeError", "(", "\n", "\"You tried to generate sequences with a model that does not have a LM Head.\"", "\n", "\"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`)\"", "\n", ")", "\n", "\n", "", "max_length", "=", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "config", ".", "max_length", "\n", "do_sample", "=", "do_sample", "if", "do_sample", "is", "not", "None", "else", "self", ".", "config", ".", "do_sample", "\n", "num_beams", "=", "num_beams", "if", "num_beams", "is", "not", "None", "else", "self", ".", "config", ".", "num_beams", "\n", "temperature", "=", "temperature", "if", "temperature", "is", "not", "None", "else", "self", ".", "config", ".", "temperature", "\n", "top_k", "=", "top_k", "if", "top_k", "is", "not", "None", "else", "self", ".", "config", ".", "top_k", "\n", "top_p", "=", "top_p", "if", "top_p", "is", "not", "None", "else", "self", ".", "config", ".", "top_p", "\n", "repetition_penalty", "=", "repetition_penalty", "if", "repetition_penalty", "is", "not", "None", "else", "self", ".", "config", ".", "repetition_penalty", "\n", "bos_token_id", "=", "bos_token_id", "if", "bos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "bos_token_id", "\n", "pad_token_id", "=", "pad_token_id", "if", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "pad_token_id", "\n", "eos_token_ids", "=", "eos_token_ids", "if", "eos_token_ids", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_ids", "\n", "length_penalty", "=", "length_penalty", "if", "length_penalty", "is", "not", "None", "else", "self", ".", "config", ".", "length_penalty", "\n", "num_return_sequences", "=", "(", "\n", "num_return_sequences", "if", "num_return_sequences", "is", "not", "None", "else", "self", ".", "config", ".", "num_return_sequences", "\n", ")", "\n", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "# overriden by the input batch_size", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "1", "\n", "", "if", "isinstance", "(", "eos_token_ids", ",", "int", ")", ":", "\n", "            ", "eos_token_ids", "=", "[", "eos_token_ids", "]", "\n", "\n", "", "assert", "isinstance", "(", "max_length", ",", "int", ")", "and", "max_length", ">", "0", ",", "\"`max_length` should be a strictely positive integer.\"", "\n", "assert", "isinstance", "(", "do_sample", ",", "bool", ")", ",", "\"`do_sample` should be a boolean.\"", "\n", "assert", "isinstance", "(", "num_beams", ",", "int", ")", "and", "num_beams", ">", "0", ",", "\"`num_beams` should be a strictely positive integer.\"", "\n", "assert", "temperature", ">", "0", ",", "\"`temperature` should be strictely positive.\"", "\n", "assert", "isinstance", "(", "top_k", ",", "int", ")", "and", "top_k", ">=", "0", ",", "\"`top_k` should be a positive integer.\"", "\n", "assert", "0", "<=", "top_p", "<=", "1", ",", "\"`top_p` should be between 0 and 1.\"", "\n", "assert", "repetition_penalty", ">=", "1.0", ",", "\"`repetition_penalty` should be >= 1.\"", "\n", "assert", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", ",", "\"`bos_token_id` should be a positive integer.\"", "\n", "assert", "isinstance", "(", "pad_token_id", ",", "int", ")", "and", "pad_token_id", ">=", "0", ",", "\"`pad_token_id` should be a positive integer.\"", "\n", "assert", "isinstance", "(", "eos_token_ids", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "\n", "e", ">=", "0", "for", "e", "in", "eos_token_ids", "\n", ")", ",", "\"`eos_token_ids` should be a positive integer or a list/tuple of positive integers.\"", "\n", "assert", "length_penalty", ">", "0", ",", "\"`length_penalty` should be strictely positive.\"", "\n", "assert", "(", "\n", "isinstance", "(", "num_return_sequences", ",", "int", ")", "and", "num_return_sequences", ">", "0", "\n", ")", ",", "\"`num_return_sequences` should be a strictely positive integer.\"", "\n", "\n", "if", "input_ids", "is", "None", ":", "\n", "            ", "input_ids", "=", "torch", ".", "full", "(", "\n", "(", "batch_size", ",", "1", ")", ",", "bos_token_id", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", ")", "\n", "", "else", ":", "\n", "            ", "assert", "input_ids", ".", "dim", "(", ")", "==", "2", ",", "\"Input prompt should be of shape (batch_size, sequence length).\"", "\n", "\n", "# current position and vocab size", "\n", "", "cur_len", "=", "input_ids", ".", "shape", "[", "1", "]", "\n", "vocab_size", "=", "self", ".", "config", ".", "vocab_size", "\n", "\n", "if", "num_return_sequences", "!=", "1", ":", "\n", "# Expand input to num return sequences", "\n", "            ", "input_ids", "=", "input_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "num_return_sequences", ",", "cur_len", ")", "\n", "input_ids", "=", "input_ids", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "batch_size", "*", "num_return_sequences", ",", "cur_len", "\n", ")", "# (batch_size * num_return_sequences, cur_len)", "\n", "effective_batch_size", "=", "batch_size", "*", "num_return_sequences", "\n", "", "else", ":", "\n", "            ", "effective_batch_size", "=", "batch_size", "\n", "\n", "", "if", "num_beams", ">", "1", ":", "\n", "            ", "output", "=", "self", ".", "_generate_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "effective_batch_size", ",", "\n", "length_penalty", ",", "\n", "num_beams", ",", "\n", "vocab_size", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_generate_no_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "effective_batch_size", ",", "\n", ")", "\n", "\n", "", "if", "num_return_sequences", "!=", "1", ":", "\n", "            ", "output", "=", "output", ".", "view", "(", "batch_size", ",", "num_return_sequences", ",", "-", "1", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._generate_no_beam_search": [[751, 820], ["torch.cat.new().fill_", "modeling_utils.PreTrainedModel.prepare_inputs_for_generation", "modeling_utils.PreTrainedModel.", "modeling_utils.PreTrainedModel._do_output_past", "torch.cat", "input_ids[].masked_fill_", "torch.cat.new", "range", "modeling_utils.top_k_top_p_filtering", "torch.multinomial().squeeze", "torch.argmax", "torch.cat.new().fill_.mul_", "torch.cat.new().fill_.max", "torch.cat.new().fill_.to", "set", "tokens_to_add.unsqueeze", "tokens_to_add.ne().long", "input_ids[].tolist", "torch.multinomial", "torch.nn.functional.softmax", "tokens_to_add.ne"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._do_output_past", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.top_k_top_p_filtering"], ["", "def", "_generate_no_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "batch_size", ",", "\n", ")", ":", "\n", "        ", "\"\"\" Generate sequences for each example without beam search (num_beams == 1).\n            All returned sequence are generated independantly.\n        \"\"\"", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "unfinished_sents", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "past", "=", "None", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "past", "=", "past", ")", "\n", "outputs", "=", "self", "(", "**", "model_inputs", ")", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "# if model has past, then set the past variable to speed up decoding", "\n", "if", "self", ".", "_do_output_past", "(", "outputs", ")", ":", "\n", "                ", "past", "=", "outputs", "[", "1", "]", "\n", "\n", "# repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)", "\n", "", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "for", "previous_token", "in", "set", "(", "input_ids", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "# if score < 0 then repetition penalty has to multiplied to reduce the previous token probability", "\n", "                        ", "if", "next_token_logits", "[", "i", ",", "previous_token", "]", "<", "0", ":", "\n", "                            ", "next_token_logits", "[", "i", ",", "previous_token", "]", "*=", "repetition_penalty", "\n", "", "else", ":", "\n", "                            ", "next_token_logits", "[", "i", ",", "previous_token", "]", "/=", "repetition_penalty", "\n", "\n", "", "", "", "", "if", "do_sample", ":", "\n", "# Temperature (higher temperature => more likely to sample low probability tokens)", "\n", "                ", "if", "temperature", "!=", "1.0", ":", "\n", "                    ", "next_token_logits", "=", "next_token_logits", "/", "temperature", "\n", "# Top-p/top-k filtering", "\n", "", "next_token_logits", "=", "top_k_top_p_filtering", "(", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ")", "\n", "# Sample", "\n", "next_token", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "# Greedy decoding", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# update generations and finished sentences", "\n", "", "tokens_to_add", "=", "next_token", "*", "unfinished_sents", "+", "pad_token_id", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "tokens_to_add", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "for", "eos_token_id", "in", "eos_token_ids", ":", "\n", "                ", "unfinished_sents", ".", "mul_", "(", "tokens_to_add", ".", "ne", "(", "eos_token_id", ")", ".", "long", "(", ")", ")", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add eos_token_ids to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_length", ":", "\n", "            ", "input_ids", "[", ":", ",", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", ",", "eos_token_ids", "[", "0", "]", ")", "\n", "\n", "", "return", "input_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._generate_beam_search": [[821, 1005], ["torch.cat.unsqueeze().expand", "torch.cat.contiguous().view", "torch.zeros", "beam_scores.new.new.view", "torch.cat.new", "enumerate", "torch.cat.new().fill_", "enumerate", "modeling_utils.BeamHypotheses", "modeling_utils.PreTrainedModel.prepare_inputs_for_generation", "modeling_utils.PreTrainedModel.", "modeling_utils.PreTrainedModel._do_output_past", "range", "beam_scores.new.new.new", "torch.cat.new", "torch.cat.new", "torch.cat", "all", "best.append", "torch.cat.unsqueeze", "torch.cat.contiguous", "range", "range", "range", "modeling_utils.top_k_top_p_filtering", "torch.multinomial", "torch.nn.functional.log_softmax", "torch.gather", "next_words.view.view.view", "next_scores.view.view.view", "torch.nn.functional.log_softmax", "_scores.view.view.view", "torch.topk", "next_scores.view.view.size", "next_words.view.view.size", "zip", "next_batch_beam.extend", "len", "tuple", "max", "len", "torch.cat.new", "set", "torch.nn.functional.softmax", "beam_scores[].expand_as", "torch.nn.functional.log_softmax.size", "beam_scores[].expand_as", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "torch.cat.new.unsqueeze", "torch.cat", "reordered_past.append", "torch.cat.new.max().item", "input_ids[].tolist", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "layer_past[].unsqueeze().clone().detach", "word_id.item", "input_ids[].clone", "score.item", "torch.cat.new.max", "next_scores[].max", "layer_past[].unsqueeze().clone", "layer_past[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel._do_output_past", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.top_k_top_p_filtering", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add"], ["", "def", "_generate_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "batch_size", ",", "\n", "length_penalty", ",", "\n", "num_beams", ",", "\n", "vocab_size", ",", "\n", ")", ":", "\n", "        ", "\"\"\" Generate sequences for each example with beam search.\n        \"\"\"", "\n", "# Expand input to num beams", "\n", "input_ids", "=", "input_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "num_beams", ",", "cur_len", ")", "\n", "input_ids", "=", "input_ids", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "num_beams", ",", "cur_len", ")", "# (batch_size * num_beams, cur_len)", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "\n", "BeamHypotheses", "(", "num_beams", ",", "max_length", ",", "length_penalty", ",", "early_stopping", "=", "False", ")", "for", "_", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "num_beams", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "-", "1e9", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "# shape (batch_size * num_beams,)", "\n", "\n", "# cache compute states", "\n", "past", "=", "None", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "past", "=", "past", ")", "\n", "outputs", "=", "self", "(", "**", "model_inputs", ")", "# (batch_size * num_beams, cur_len, vocab_size)", "\n", "scores", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "# if model has past, then set the past variable to speed up decoding", "\n", "if", "self", ".", "_do_output_past", "(", "outputs", ")", ":", "\n", "                ", "past", "=", "outputs", "[", "1", "]", "\n", "\n", "# repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)", "\n", "", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", "*", "num_beams", ")", ":", "\n", "                    ", "for", "previous_token", "in", "set", "(", "input_ids", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "# if score < 0 then repetition penalty has to multiplied to reduce the previous token probability", "\n", "                        ", "if", "scores", "[", "i", ",", "previous_token", "]", "<", "0", ":", "\n", "                            ", "scores", "[", "i", ",", "previous_token", "]", "*=", "repetition_penalty", "\n", "", "else", ":", "\n", "                            ", "scores", "[", "i", ",", "previous_token", "]", "/=", "repetition_penalty", "\n", "\n", "", "", "", "", "if", "do_sample", ":", "\n", "# Temperature (higher temperature => more likely to sample low probability tokens)", "\n", "                ", "if", "temperature", "!=", "1.0", ":", "\n", "                    ", "scores", "=", "scores", "/", "temperature", "\n", "# Top-p/top-k filtering", "\n", "", "scores", "=", "top_k_top_p_filtering", "(", "\n", "scores", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ",", "min_tokens_to_keep", "=", "2", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# Sample 2 next words for each beam (so we have some spare tokens and match output of greedy beam search)", "\n", "next_words", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "2", ")", "# (batch_size * num_beams, 2)", "\n", "# Compute next scores", "\n", "_scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (batch_size * num_beams, vocab_size)", "\n", "_scores", "=", "torch", ".", "gather", "(", "_scores", ",", "-", "1", ",", "next_words", ")", "# (batch_size * num_beams, 2)", "\n", "next_scores", "=", "_scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "_scores", ")", "# (batch_size * num_beams, 2)", "\n", "# Match shape of greedy beam search", "\n", "next_words", "=", "next_words", ".", "view", "(", "batch_size", ",", "2", "*", "num_beams", ")", "# (batch_size, 2 * num_beams)", "\n", "next_scores", "=", "next_scores", ".", "view", "(", "batch_size", ",", "2", "*", "num_beams", ")", "# (batch_size, 2 * num_beams)", "\n", "", "else", ":", "\n", "# do greedy beam search", "\n", "                ", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (batch_size * num_beams, vocab_size)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "batch_size", "*", "num_beams", ",", "vocab_size", ")", "\n", "# Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# re-organize to group the beam together (we are keeping top hypothesis accross beams)", "\n", "_scores", "=", "_scores", ".", "view", "(", "batch_size", ",", "num_beams", "*", "vocab_size", ")", "# (batch_size, num_beams * vocab_size)", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "num_beams", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "batch_size", ",", "2", "*", "num_beams", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (batch_size * num_beams) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "batch_ex", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "batch_ex", "]", "=", "done", "[", "batch_ex", "]", "or", "generated_hyps", "[", "batch_ex", "]", ".", "is_done", "(", "next_scores", "[", "batch_ex", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "batch_ex", "]", ":", "\n", "                    ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "pad_token_id", ",", "0", ")", "]", "*", "num_beams", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "score", "in", "zip", "(", "next_words", "[", "batch_ex", "]", ",", "next_scores", "[", "batch_ex", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "vocab_size", "\n", "word_id", "=", "idx", "%", "vocab_size", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", ".", "item", "(", ")", "in", "eos_token_ids", "or", "cur_len", "+", "1", "==", "max_length", ":", "\n", "                        ", "generated_hyps", "[", "batch_ex", "]", ".", "add", "(", "\n", "input_ids", "[", "batch_ex", "*", "num_beams", "+", "beam_id", ",", ":", "cur_len", "]", ".", "clone", "(", ")", ",", "score", ".", "item", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "next_sent_beam", ".", "append", "(", "(", "score", ",", "word_id", ",", "batch_ex", "*", "num_beams", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "num_beams", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_length", "else", "num_beams", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                    ", "next_sent_beam", "=", "[", "(", "0", ",", "pad_token_id", ",", "0", ")", "]", "*", "num_beams", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "num_beams", "*", "(", "batch_ex", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "batch_size", "*", "num_beams", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "input_ids", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "input_ids", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch", "\n", "input_ids", "=", "input_ids", "[", "beam_idx", ",", ":", "]", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "beam_words", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# re-order internal states", "\n", "if", "past", ":", "\n", "                ", "reordered_past", "=", "[", "]", "\n", "for", "layer_past", "in", "past", ":", "\n", "# get the correct batch idx from layer past batch dim", "\n", "# batch dim of `past` and `mems` is at 2nd position", "\n", "                    ", "reordered_layer_past", "=", "[", "layer_past", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "i", "in", "beam_idx", "]", "\n", "reordered_layer_past", "=", "torch", ".", "cat", "(", "reordered_layer_past", ",", "dim", "=", "1", ")", "\n", "# check that shape matches", "\n", "assert", "reordered_layer_past", ".", "shape", "==", "layer_past", ".", "shape", "\n", "reordered_past", ".", "append", "(", "reordered_layer_past", ")", "\n", "", "past", "=", "tuple", "(", "reordered_past", ")", "\n", "\n", "# update current length", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "# visualize hypotheses", "\n", "# print([len(x) for x in generated_hyps], cur_len)", "\n", "# globals().update( locals() );", "\n", "# !import code; code.interact(local=vars())", "\n", "# for ii in range(batch_size):", "\n", "#     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):", "\n", "#         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))", "\n", "#     print(\"\")", "\n", "\n", "# select the best hypotheses", "\n", "", "", "tgt_len", "=", "input_ids", ".", "new", "(", "batch_size", ")", "\n", "best", "=", "[", "]", "\n", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "best_hyp", "=", "max", "(", "hypotheses", ".", "hyp", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "[", "1", "]", "\n", "tgt_len", "[", "i", "]", "=", "len", "(", "best_hyp", ")", "+", "1", "# +1 for the <EOS> symbol", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# generate target batch", "\n", "", "decoded", "=", "input_ids", ".", "new", "(", "batch_size", ",", "tgt_len", ".", "max", "(", ")", ".", "item", "(", ")", ")", ".", "fill_", "(", "pad_token_id", ")", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "            ", "decoded", "[", "i", ",", ":", "tgt_len", "[", "i", "]", "-", "1", "]", "=", "hypo", "\n", "decoded", "[", "i", ",", "tgt_len", "[", "i", "]", "-", "1", "]", "=", "eos_token_ids", "[", "0", "]", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.BeamHypotheses.__init__": [[1043, 1053], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n_hyp", ",", "max_length", ",", "length_penalty", ",", "early_stopping", ")", ":", "\n", "        ", "\"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"", "\n", "self", ".", "max_length", "=", "max_length", "-", "1", "# ignoring bos_token", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "early_stopping", "=", "early_stopping", "\n", "self", ".", "n_hyp", "=", "n_hyp", "\n", "self", ".", "hyp", "=", "[", "]", "\n", "self", ".", "worst_score", "=", "1e9", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.BeamHypotheses.__len__": [[1054, 1059], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of hypotheses in the list.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "hyp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.BeamHypotheses.add": [[1060, 1073], ["modeling_utils.BeamHypotheses.hyp.append", "len", "len", "len", "sorted", "min", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "hyp", ",", "sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"", "\n", "score", "=", "sum_logprobs", "/", "len", "(", "hyp", ")", "**", "self", ".", "length_penalty", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "n_hyp", "or", "score", ">", "self", ".", "worst_score", ":", "\n", "            ", "self", ".", "hyp", ".", "append", "(", "(", "score", ",", "hyp", ")", ")", "\n", "if", "len", "(", "self", ")", ">", "self", ".", "n_hyp", ":", "\n", "                ", "sorted_scores", "=", "sorted", "(", "[", "(", "s", ",", "idx", ")", "for", "idx", ",", "(", "s", ",", "_", ")", "in", "enumerate", "(", "self", ".", "hyp", ")", "]", ")", "\n", "del", "self", ".", "hyp", "[", "sorted_scores", "[", "0", "]", "[", "1", "]", "]", "\n", "self", ".", "worst_score", "=", "sorted_scores", "[", "1", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "worst_score", "=", "min", "(", "score", ",", "self", ".", "worst_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.BeamHypotheses.is_done": [[1074, 1085], ["len"], "methods", ["None"], ["", "", "", "def", "is_done", "(", "self", ",", "best_sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "n_hyp", ":", "\n", "            ", "return", "False", "\n", "", "elif", "self", ".", "early_stopping", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "worst_score", ">=", "best_sum_logprobs", "/", "self", ".", "max_length", "**", "self", ".", "length_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.Conv1D.__init__": [[1088, 1098], ["torch.nn.Module.__init__", "torch.empty", "torch.nn.init.normal_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf", ",", "nx", ")", ":", "\n", "        ", "\"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n            Basically works like a Linear layer but the weights are transposed\n        \"\"\"", "\n", "super", "(", "Conv1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nf", "=", "nf", "\n", "w", "=", "torch", ".", "empty", "(", "nx", ",", "nf", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "w", ",", "std", "=", "0.02", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "w", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "nf", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.Conv1D.forward": [[1099, 1104], ["torch.addmm", "x.view.view.view", "x.view.view.view", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size_out", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "nf", ",", ")", "\n", "x", "=", "torch", ".", "addmm", "(", "self", ".", "bias", ",", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", ",", "self", ".", "weight", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "size_out", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PoolerStartLogits.__init__": [[1109, 1112], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "PoolerStartLogits", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PoolerStartLogits.forward": [[1113, 1128], ["modeling_utils.PoolerStartLogits.dense().squeeze", "modeling_utils.PoolerStartLogits.dense", "next", "modeling_utils.PoolerStartLogits.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Args:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"", "\n", "x", "=", "self", ".", "dense", "(", "hidden_states", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "p_mask", "is", "not", "None", ":", "\n", "            ", "if", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "==", "torch", ".", "float16", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "65500", "*", "p_mask", "\n", "", "else", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "1e30", "*", "p_mask", "\n", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PoolerEndLogits.__init__": [[1134, 1140], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.LayerNorm", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "PoolerEndLogits", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense_0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense_1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PoolerEndLogits.forward": [[1141, 1175], ["modeling_utils.PoolerEndLogits.dense_0", "modeling_utils.PoolerEndLogits.activation", "modeling_utils.PoolerEndLogits.LayerNorm", "modeling_utils.PoolerEndLogits.dense_1().squeeze", "start_positions[].expand", "hidden_states.gather", "start_states.expand.expand.expand", "torch.cat", "modeling_utils.PoolerEndLogits.dense_1", "next", "modeling_utils.PoolerEndLogits.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_states", "=", "None", ",", "start_positions", "=", "None", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"", "\n", "assert", "(", "\n", "start_states", "is", "not", "None", "or", "start_positions", "is", "not", "None", "\n", ")", ",", "\"One of start_states, start_positions should be not None\"", "\n", "if", "start_positions", "is", "not", "None", ":", "\n", "            ", "slen", ",", "hsz", "=", "hidden_states", ".", "shape", "[", "-", "2", ":", "]", "\n", "start_positions", "=", "start_positions", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "start_positions", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "start_states", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ")", "# shape (bsz, slen, hsz)", "\n", "\n", "", "x", "=", "self", ".", "dense_0", "(", "torch", ".", "cat", "(", "[", "hidden_states", ",", "start_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "x", "=", "self", ".", "LayerNorm", "(", "x", ")", "\n", "x", "=", "self", ".", "dense_1", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "p_mask", "is", "not", "None", ":", "\n", "            ", "if", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "==", "torch", ".", "float16", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "65500", "*", "p_mask", "\n", "", "else", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "1e30", "*", "p_mask", "\n", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PoolerAnswerClass.__init__": [[1180, 1185], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "PoolerAnswerClass", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense_0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "dense_1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PoolerAnswerClass.forward": [[1186, 1222], ["modeling_utils.PoolerAnswerClass.dense_0", "modeling_utils.PoolerAnswerClass.activation", "modeling_utils.PoolerAnswerClass.dense_1().squeeze", "start_positions[].expand", "hidden_states.gather().squeeze", "cls_index[].expand", "hidden_states.gather().squeeze", "torch.cat", "modeling_utils.PoolerAnswerClass.dense_1", "hidden_states.gather", "hidden_states.gather"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_states", "=", "None", ",", "start_positions", "=", "None", ",", "cls_index", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span.\n            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n                position of the CLS token. If None, take the last token.\n\n            note(Original repo):\n                no dependency on end_feature so that we can obtain one single `cls_logits`\n                for each sample\n        \"\"\"", "\n", "hsz", "=", "hidden_states", ".", "shape", "[", "-", "1", "]", "\n", "assert", "(", "\n", "start_states", "is", "not", "None", "or", "start_positions", "is", "not", "None", "\n", ")", ",", "\"One of start_states, start_positions should be not None\"", "\n", "if", "start_positions", "is", "not", "None", ":", "\n", "            ", "start_positions", "=", "start_positions", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "start_positions", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, hsz)", "\n", "\n", "", "if", "cls_index", "is", "not", "None", ":", "\n", "            ", "cls_index", "=", "cls_index", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "cls_token_state", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "cls_index", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, hsz)", "\n", "", "else", ":", "\n", "            ", "cls_token_state", "=", "hidden_states", "[", ":", ",", "-", "1", ",", ":", "]", "# shape (bsz, hsz)", "\n", "\n", "", "x", "=", "self", ".", "dense_0", "(", "torch", ".", "cat", "(", "[", "start_states", ",", "cls_token_state", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "x", "=", "self", ".", "dense_1", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.SQuADHead.__init__": [[1265, 1273], ["torch.nn.Module.__init__", "modeling_utils.PoolerStartLogits", "modeling_utils.PoolerEndLogits", "modeling_utils.PoolerAnswerClass"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "SQuADHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "start_n_top", "=", "config", ".", "start_n_top", "\n", "self", ".", "end_n_top", "=", "config", ".", "end_n_top", "\n", "\n", "self", ".", "start_logits", "=", "PoolerStartLogits", "(", "config", ")", "\n", "self", ".", "end_logits", "=", "PoolerEndLogits", "(", "config", ")", "\n", "self", ".", "answer_class", "=", "PoolerAnswerClass", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.SQuADHead.forward": [[1274, 1339], ["modeling_utils.SQuADHead.start_logits", "modeling_utils.SQuADHead.end_logits", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "hidden_states.size", "torch.nn.functional.softmax", "torch.topk", "start_top_index.unsqueeze().expand", "torch.gather", "torch.einsum.unsqueeze().expand", "hidden_states.unsqueeze().expand_as", "modeling_utils.SQuADHead.end_logits", "torch.nn.functional.softmax", "torch.topk", "end_top_log_probs.view.view.view", "end_top_index.view.view.view", "torch.einsum", "modeling_utils.SQuADHead.answer_class", "modeling_utils.SQuADHead.answer_class", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "p_mask.unsqueeze", "x.squeeze_", "start_top_index.unsqueeze", "torch.einsum.unsqueeze", "hidden_states.unsqueeze", "x.dim"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "hidden_states", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "cls_index", "=", "None", ",", "is_impossible", "=", "None", ",", "p_mask", "=", "None", "\n", ")", ":", "\n", "        ", "outputs", "=", "(", ")", "\n", "\n", "start_logits", "=", "self", ".", "start_logits", "(", "hidden_states", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, let's remove the dimension added by batch splitting", "\n", "            ", "for", "x", "in", "(", "start_positions", ",", "end_positions", ",", "cls_index", ",", "is_impossible", ")", ":", "\n", "                ", "if", "x", "is", "not", "None", "and", "x", ".", "dim", "(", ")", ">", "1", ":", "\n", "                    ", "x", ".", "squeeze_", "(", "-", "1", ")", "\n", "\n", "# during training, compute the end logits based on the ground truth of the start position", "\n", "", "", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "\n", "if", "cls_index", "is", "not", "None", "and", "is_impossible", "is", "not", "None", ":", "\n", "# Predict answerability from the representation of CLS and START", "\n", "                ", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "cls_index", "=", "cls_index", ")", "\n", "loss_fct_cls", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "cls_loss", "=", "loss_fct_cls", "(", "cls_logits", ",", "is_impossible", ")", "\n", "\n", "# note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss", "\n", "total_loss", "+=", "cls_loss", "*", "0.5", "\n", "\n", "", "outputs", "=", "(", "total_loss", ",", ")", "+", "outputs", "\n", "\n", "", "else", ":", "\n", "# during inference, compute the end logits based on beam search", "\n", "            ", "bsz", ",", "slen", ",", "hsz", "=", "hidden_states", ".", "size", "(", ")", "\n", "start_log_probs", "=", "F", ".", "softmax", "(", "start_logits", ",", "dim", "=", "-", "1", ")", "# shape (bsz, slen)", "\n", "\n", "start_top_log_probs", ",", "start_top_index", "=", "torch", ".", "topk", "(", "\n", "start_log_probs", ",", "self", ".", "start_n_top", ",", "dim", "=", "-", "1", "\n", ")", "# shape (bsz, start_n_top)", "\n", "start_top_index_exp", "=", "start_top_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "torch", ".", "gather", "(", "hidden_states", ",", "-", "2", ",", "start_top_index_exp", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "start_states", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ",", "-", "1", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "\n", "hidden_states_expanded", "=", "hidden_states", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "start_states", "\n", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "p_mask", "=", "p_mask", ".", "unsqueeze", "(", "-", "1", ")", "if", "p_mask", "is", "not", "None", "else", "None", "\n", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states_expanded", ",", "start_states", "=", "start_states", ",", "p_mask", "=", "p_mask", ")", "\n", "end_log_probs", "=", "F", ".", "softmax", "(", "end_logits", ",", "dim", "=", "1", ")", "# shape (bsz, slen, start_n_top)", "\n", "\n", "end_top_log_probs", ",", "end_top_index", "=", "torch", ".", "topk", "(", "\n", "end_log_probs", ",", "self", ".", "end_n_top", ",", "dim", "=", "1", "\n", ")", "# shape (bsz, end_n_top, start_n_top)", "\n", "end_top_log_probs", "=", "end_top_log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "end_top_index", "=", "end_top_index", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "\n", "start_states", "=", "torch", ".", "einsum", "(", "\"blh,bl->bh\"", ",", "hidden_states", ",", "start_log_probs", ")", "\n", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_states", "=", "start_states", ",", "cls_index", "=", "cls_index", ")", "\n", "\n", "outputs", "=", "(", "start_top_log_probs", ",", "start_top_index", ",", "end_top_log_probs", ",", "end_top_index", ",", "cls_logits", ")", "+", "outputs", "\n", "\n", "# return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits", "\n", "# or (if labels are provided) (total_loss,)", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.SequenceSummary.__init__": [[1357, 1386], ["torch.nn.Module.__init__", "Identity", "Identity", "Identity", "Identity", "hasattr", "hasattr", "torch.nn.Linear", "hasattr", "torch.nn.Tanh", "hasattr", "torch.nn.Dropout", "hasattr", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "SequenceSummary", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "summary_type", "=", "config", ".", "summary_type", "if", "hasattr", "(", "config", ",", "\"summary_type\"", ")", "else", "\"last\"", "\n", "if", "self", ".", "summary_type", "==", "\"attn\"", ":", "\n", "# We should use a standard multi-head attention module with absolute positional embedding for that.", "\n", "# Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276", "\n", "# We can probably just use the multi-head attention module of PyTorch >=1.1.0", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "self", ".", "summary", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_use_proj\"", ")", "and", "config", ".", "summary_use_proj", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "\"summary_proj_to_labels\"", ")", "and", "config", ".", "summary_proj_to_labels", "and", "config", ".", "num_labels", ">", "0", ":", "\n", "                ", "num_classes", "=", "config", ".", "num_labels", "\n", "", "else", ":", "\n", "                ", "num_classes", "=", "config", ".", "hidden_size", "\n", "", "self", ".", "summary", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_classes", ")", "\n", "\n", "", "self", ".", "activation", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_activation\"", ")", "and", "config", ".", "summary_activation", "==", "\"tanh\"", ":", "\n", "            ", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "", "self", ".", "first_dropout", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_first_dropout\"", ")", "and", "config", ".", "summary_first_dropout", ">", "0", ":", "\n", "            ", "self", ".", "first_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "summary_first_dropout", ")", "\n", "\n", "", "self", ".", "last_dropout", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_last_dropout\"", ")", "and", "config", ".", "summary_last_dropout", ">", "0", ":", "\n", "            ", "self", ".", "last_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "summary_last_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.SequenceSummary.forward": [[1387, 1417], ["modeling_utils.SequenceSummary.first_dropout", "modeling_utils.SequenceSummary.summary", "modeling_utils.SequenceSummary.activation", "modeling_utils.SequenceSummary.last_dropout", "hidden_states.mean", "hidden_states.gather().squeeze", "torch.full_like", "cls_index.expand.expand.unsqueeze().unsqueeze", "cls_index.expand.expand.expand", "hidden_states.gather", "cls_index.expand.expand.unsqueeze", "hidden_states.size", "cls_index.expand.expand.dim"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "cls_index", "=", "None", ")", ":", "\n", "        ", "\"\"\" hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.\n            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n                if summary_type == 'cls_index' and cls_index is None:\n                    we take the last token of the sequence as classification token\n        \"\"\"", "\n", "if", "self", ".", "summary_type", "==", "\"last\"", ":", "\n", "            ", "output", "=", "hidden_states", "[", ":", ",", "-", "1", "]", "\n", "", "elif", "self", ".", "summary_type", "==", "\"first\"", ":", "\n", "            ", "output", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "", "elif", "self", ".", "summary_type", "==", "\"mean\"", ":", "\n", "            ", "output", "=", "hidden_states", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "summary_type", "==", "\"cls_index\"", ":", "\n", "            ", "if", "cls_index", "is", "None", ":", "\n", "                ", "cls_index", "=", "torch", ".", "full_like", "(", "hidden_states", "[", "...", ",", ":", "1", ",", ":", "]", ",", "hidden_states", ".", "shape", "[", "-", "2", "]", "-", "1", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "else", ":", "\n", "                ", "cls_index", "=", "cls_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "cls_index", "=", "cls_index", ".", "expand", "(", "(", "-", "1", ",", ")", "*", "(", "cls_index", ".", "dim", "(", ")", "-", "1", ")", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "# shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states", "\n", "", "output", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "cls_index", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, XX, hidden_size)", "\n", "", "elif", "self", ".", "summary_type", "==", "\"attn\"", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "output", "=", "self", ".", "first_dropout", "(", "output", ")", "\n", "output", "=", "self", ".", "summary", "(", "output", ")", "\n", "output", "=", "self", ".", "activation", "(", "output", ")", "\n", "output", "=", "self", ".", "last_dropout", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.top_k_top_p_filtering": [[1007, 1040], ["float", "min", "torch.sort", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "max", "logits.size", "torch.nn.functional.softmax", "torch.topk"], "function", ["None"], ["", "", "def", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "1.0", ",", "filter_value", "=", "-", "float", "(", "\"Inf\"", ")", ",", "min_tokens_to_keep", "=", "1", ")", ":", "\n", "    ", "\"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size, vocabulary size)\n            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n            Make sure we keep at least min_tokens_to_keep per batch example in the output\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "if", "top_k", ">", "0", ":", "\n", "        ", "top_k", "=", "min", "(", "max", "(", "top_k", ",", "min_tokens_to_keep", ")", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", "<", "1.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold (token with 0 are kept)", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "if", "min_tokens_to_keep", ">", "1", ":", "\n", "# Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)", "\n", "            ", "sorted_indices_to_remove", "[", "...", ",", ":", "min_tokens_to_keep", "]", "=", "0", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "1", ",", "sorted_indices", ",", "sorted_indices_to_remove", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_linear_layer": [[1419, 1442], ["index.to.to", "layer.weight.index_select().clone().detach", "list", "len", "torch.nn.Linear().to", "nn.Linear().to.weight.copy_", "layer.weight.size", "layer.weight.index_select().clone().detach.contiguous", "nn.Linear().to.bias.copy_", "layer.weight.index_select().clone", "layer.bias.clone().detach", "layer.bias[].clone().detach", "torch.nn.Linear", "layer.bias[].clone().detach.contiguous", "layer.weight.index_select", "layer.bias.clone", "layer.bias[].clone"], "function", ["None"], ["", "", "def", "prune_linear_layer", "(", "layer", ",", "index", ",", "dim", "=", "0", ")", ":", "\n", "    ", "\"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "index", "=", "index", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "W", "=", "layer", ".", "weight", ".", "index_select", "(", "dim", ",", "index", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "layer", ".", "bias", "is", "not", "None", ":", "\n", "        ", "if", "dim", "==", "1", ":", "\n", "            ", "b", "=", "layer", ".", "bias", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "b", "=", "layer", ".", "bias", "[", "index", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "", "new_size", "=", "list", "(", "layer", ".", "weight", ".", "size", "(", ")", ")", "\n", "new_size", "[", "dim", "]", "=", "len", "(", "index", ")", "\n", "new_layer", "=", "nn", ".", "Linear", "(", "new_size", "[", "1", "]", ",", "new_size", "[", "0", "]", ",", "bias", "=", "layer", ".", "bias", "is", "not", "None", ")", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "weight", ".", "copy_", "(", "W", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "True", "\n", "if", "layer", ".", "bias", "is", "not", "None", ":", "\n", "        ", "new_layer", ".", "bias", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "bias", ".", "copy_", "(", "b", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "True", "\n", "", "return", "new_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_conv1d_layer": [[1444, 1466], ["index.to.to", "layer.weight.index_select().clone().detach", "list", "len", "Conv1D().to", "Conv1D().to.weight.copy_", "Conv1D().to.bias.copy_", "layer.bias.clone().detach", "layer.bias[].clone().detach", "layer.weight.size", "layer.weight.index_select().clone().detach.contiguous", "layer.bias[].clone().detach.contiguous", "layer.weight.index_select().clone", "modeling_utils.Conv1D", "layer.bias.clone", "layer.bias[].clone", "layer.weight.index_select"], "function", ["None"], ["", "def", "prune_conv1d_layer", "(", "layer", ",", "index", ",", "dim", "=", "1", ")", ":", "\n", "    ", "\"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "index", "=", "index", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "W", "=", "layer", ".", "weight", ".", "index_select", "(", "dim", ",", "index", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "dim", "==", "0", ":", "\n", "        ", "b", "=", "layer", ".", "bias", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "        ", "b", "=", "layer", ".", "bias", "[", "index", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "new_size", "=", "list", "(", "layer", ".", "weight", ".", "size", "(", ")", ")", "\n", "new_size", "[", "dim", "]", "=", "len", "(", "index", ")", "\n", "new_layer", "=", "Conv1D", "(", "new_size", "[", "1", "]", ",", "new_size", "[", "0", "]", ")", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "weight", ".", "copy_", "(", "W", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "True", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "bias", ".", "copy_", "(", "b", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "True", "\n", "return", "new_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_layer": [[1468, 1479], ["isinstance", "modeling_utils.prune_linear_layer", "isinstance", "modeling_utils.prune_conv1d_layer", "ValueError"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_linear_layer", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_conv1d_layer"], ["", "def", "prune_layer", "(", "layer", ",", "index", ",", "dim", "=", "None", ")", ":", "\n", "    ", "\"\"\" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "if", "isinstance", "(", "layer", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "return", "prune_linear_layer", "(", "layer", ",", "index", ",", "dim", "=", "0", "if", "dim", "is", "None", "else", "dim", ")", "\n", "", "elif", "isinstance", "(", "layer", ",", "Conv1D", ")", ":", "\n", "        ", "return", "prune_conv1d_layer", "(", "layer", ",", "index", ",", "dim", "=", "1", "if", "dim", "is", "None", "else", "dim", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Can't prune layer of class {}\"", ".", "format", "(", "layer", ".", "__class__", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.__init__": [[50, 91], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "dict", "kwargs.pop", "dict", "kwargs.items", "dict", "zip", "setattr", "range", "int", "configuration_utils.PretrainedConfig.id2label.items", "configuration_utils.PretrainedConfig.id2label.values", "configuration_utils.PretrainedConfig.id2label.keys", "int", "configuration_utils.PretrainedConfig.label2id.items", "logger.error"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "# Attributes with defaults", "\n", "        ", "self", ".", "output_attentions", "=", "kwargs", ".", "pop", "(", "\"output_attentions\"", ",", "False", ")", "\n", "self", ".", "output_hidden_states", "=", "kwargs", ".", "pop", "(", "\"output_hidden_states\"", ",", "False", ")", "\n", "self", ".", "output_past", "=", "kwargs", ".", "pop", "(", "\"output_past\"", ",", "True", ")", "# Not used by all models", "\n", "self", ".", "torchscript", "=", "kwargs", ".", "pop", "(", "\"torchscript\"", ",", "False", ")", "# Only used by PyTorch models", "\n", "self", ".", "use_bfloat16", "=", "kwargs", ".", "pop", "(", "\"use_bfloat16\"", ",", "False", ")", "\n", "self", ".", "pruned_heads", "=", "kwargs", ".", "pop", "(", "\"pruned_heads\"", ",", "{", "}", ")", "\n", "\n", "# Is decoder is used in encoder-decoder models to differentiate encoder from decoder", "\n", "self", ".", "is_decoder", "=", "kwargs", ".", "pop", "(", "\"is_decoder\"", ",", "False", ")", "\n", "\n", "# Parameters for sequence generation", "\n", "self", ".", "max_length", "=", "kwargs", ".", "pop", "(", "\"max_length\"", ",", "20", ")", "\n", "self", ".", "do_sample", "=", "kwargs", ".", "pop", "(", "\"do_sample\"", ",", "False", ")", "\n", "self", ".", "num_beams", "=", "kwargs", ".", "pop", "(", "\"num_beams\"", ",", "1", ")", "\n", "self", ".", "temperature", "=", "kwargs", ".", "pop", "(", "\"temperature\"", ",", "1.0", ")", "\n", "self", ".", "top_k", "=", "kwargs", ".", "pop", "(", "\"top_k\"", ",", "50", ")", "\n", "self", ".", "top_p", "=", "kwargs", ".", "pop", "(", "\"top_p\"", ",", "1.0", ")", "\n", "self", ".", "repetition_penalty", "=", "kwargs", ".", "pop", "(", "\"repetition_penalty\"", ",", "1.0", ")", "\n", "self", ".", "bos_token_id", "=", "kwargs", ".", "pop", "(", "\"bos_token_id\"", ",", "0", ")", "\n", "self", ".", "pad_token_id", "=", "kwargs", ".", "pop", "(", "\"pad_token_id\"", ",", "0", ")", "\n", "self", ".", "eos_token_ids", "=", "kwargs", ".", "pop", "(", "\"eos_token_ids\"", ",", "0", ")", "\n", "self", ".", "length_penalty", "=", "kwargs", ".", "pop", "(", "\"length_penalty\"", ",", "1.0", ")", "\n", "self", ".", "num_return_sequences", "=", "kwargs", ".", "pop", "(", "\"num_return_sequences\"", ",", "1", ")", "\n", "\n", "# Fine-tuning task arguments", "\n", "self", ".", "finetuning_task", "=", "kwargs", ".", "pop", "(", "\"finetuning_task\"", ",", "None", ")", "\n", "self", ".", "num_labels", "=", "kwargs", ".", "pop", "(", "\"num_labels\"", ",", "2", ")", "\n", "self", ".", "id2label", "=", "kwargs", ".", "pop", "(", "\"id2label\"", ",", "{", "i", ":", "\"LABEL_{}\"", ".", "format", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_labels", ")", "}", ")", "\n", "self", ".", "id2label", "=", "dict", "(", "(", "int", "(", "key", ")", ",", "value", ")", "for", "key", ",", "value", "in", "self", ".", "id2label", ".", "items", "(", ")", ")", "\n", "self", ".", "label2id", "=", "kwargs", ".", "pop", "(", "\"label2id\"", ",", "dict", "(", "zip", "(", "self", ".", "id2label", ".", "values", "(", ")", ",", "self", ".", "id2label", ".", "keys", "(", ")", ")", ")", ")", "\n", "self", ".", "label2id", "=", "dict", "(", "(", "key", ",", "int", "(", "value", ")", ")", "for", "key", ",", "value", "in", "self", ".", "label2id", ".", "items", "(", ")", ")", "\n", "\n", "# Additional attributes without default values", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "", "except", "AttributeError", "as", "err", ":", "\n", "                ", "logger", ".", "error", "(", "\"Can't set {} with value {} for {}\"", ".", "format", "(", "key", ",", "value", ",", "self", ")", ")", "\n", "raise", "err", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.save_pretrained": [[92, 105], ["os.path.isdir", "os.path.join", "configuration_utils.PretrainedConfig.to_json_file", "logger.info"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_json_file"], ["", "", "", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save a configuration object to the directory `save_directory`, so that it\n            can be re-loaded using the :func:`~transformers.PretrainedConfig.from_pretrained` class method.\n        \"\"\"", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "\n", "save_directory", "\n", ")", ",", "\"Saving path should be a directory where the model and configuration can be saved\"", "\n", "\n", "# If we save using the predefined names, we can load using `from_pretrained`", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "CONFIG_NAME", ")", "\n", "\n", "self", ".", "to_json_file", "(", "output_config_file", ")", "\n", "logger", ".", "info", "(", "\"Configuration saved in {}\"", ".", "format", "(", "output_config_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained": [[106, 232], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "hasattr", "kwargs.items", "logger.info", "os.path.isdir", "file_utils.cached_path", "cls.from_json_file", "logger.info", "logger.info", "dict", "hasattr", "kwargs.pop", "str", "os.path.join", "EnvironmentError", "EnvironmentError", "setattr", "to_remove.append", "os.path.isfile", "file_utils.is_remote_url", "file_utils.hf_bucket_url", "int", "cls.from_json_file.pruned_heads.items", "cls.pretrained_config_archive_map.keys"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.cached_path", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_json_file", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.is_remote_url", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.hf_bucket_url"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\" Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a pre-trained model configuration that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing a configuration file saved using the :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n\n                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            resume_download: (`optional`) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            return_unused_kwargs: (`optional`) bool:\n\n                - If False, then this function returns just the final configuration object.\n                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n\n        Examples::\n\n            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n            # derived class: BertConfig\n            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n            config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n            assert config.output_attention == True\n            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n                                                               foo=False, return_unused_kwargs=True)\n            assert config.output_attention == True\n            assert unused_kwargs == {'foo': False}\n\n        \"\"\"", "\n", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "return_unused_kwargs", "=", "kwargs", ".", "pop", "(", "\"return_unused_kwargs\"", ",", "False", ")", "\n", "\n", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_config_archive_map", ":", "\n", "            ", "config_file", "=", "cls", ".", "pretrained_config_archive_map", "[", "pretrained_model_name_or_path", "]", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CONFIG_NAME", ")", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "pretrained_model_name_or_path", ")", ":", "\n", "            ", "config_file", "=", "pretrained_model_name_or_path", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "hf_bucket_url", "(", "pretrained_model_name_or_path", ",", "postfix", "=", "CONFIG_NAME", ")", "\n", "\n", "", "try", ":", "\n", "# Load from URL or cache if already cached", "\n", "            ", "resolved_config_file", "=", "cached_path", "(", "\n", "config_file", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", ")", "\n", "# Load config", "\n", "config", "=", "cls", ".", "from_json_file", "(", "resolved_config_file", ")", "\n", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_config_archive_map", ":", "\n", "                ", "msg", "=", "\"Couldn't reach server at '{}' to download pretrained model configuration file.\"", ".", "format", "(", "\n", "config_file", "\n", ")", "\n", "", "else", ":", "\n", "                ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to a configuration file named {} or \"", "\n", "\"a directory containing such a file but couldn't find any such file at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "cls", ".", "pretrained_config_archive_map", ".", "keys", "(", ")", ")", ",", "\n", "config_file", ",", "\n", "CONFIG_NAME", ",", "\n", ")", "\n", ")", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "except", "json", ".", "JSONDecodeError", ":", "\n", "            ", "msg", "=", "(", "\n", "\"Couldn't reach server at '{}' to download configuration file or \"", "\n", "\"configuration file is not a valid JSON file. \"", "\n", "\"Please check network or file content here: {}.\"", ".", "format", "(", "config_file", ",", "resolved_config_file", ")", "\n", ")", "\n", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "if", "resolved_config_file", "==", "config_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading configuration file {}\"", ".", "format", "(", "config_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading configuration file {} from cache at {}\"", ".", "format", "(", "config_file", ",", "resolved_config_file", ")", ")", "\n", "\n", "", "if", "hasattr", "(", "config", ",", "\"pruned_heads\"", ")", ":", "\n", "            ", "config", ".", "pruned_heads", "=", "dict", "(", "(", "int", "(", "key", ")", ",", "value", ")", "for", "key", ",", "value", "in", "config", ".", "pruned_heads", ".", "items", "(", ")", ")", "\n", "\n", "# Update config with kwargs if needed", "\n", "", "to_remove", "=", "[", "]", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "key", ")", ":", "\n", "                ", "setattr", "(", "config", ",", "key", ",", "value", ")", "\n", "to_remove", ".", "append", "(", "key", ")", "\n", "", "", "for", "key", "in", "to_remove", ":", "\n", "            ", "kwargs", ".", "pop", "(", "key", ",", "None", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config %s\"", ",", "str", "(", "config", ")", ")", "\n", "if", "return_unused_kwargs", ":", "\n", "            ", "return", "config", ",", "kwargs", "\n", "", "else", ":", "\n", "            ", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_dict": [[233, 237], ["cls"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `Config` from a Python dictionary of parameters.\"\"\"", "\n", "return", "cls", "(", "**", "json_object", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_json_file": [[238, 245], ["json.loads", "cls", "open", "reader.read"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `Config` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "dict_obj", "=", "json", ".", "loads", "(", "text", ")", "\n", "return", "cls", "(", "**", "dict_obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.__eq__": [[246, 248], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.__repr__": [[249, 251], ["str", "configuration_utils.PretrainedConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_dict": [[252, 256], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_json_string": [[257, 260], ["json.dumps", "configuration_utils.PretrainedConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_json_file": [[261, 265], ["open", "writer.write", "configuration_utils.PretrainedConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.to_json_string"], ["", "def", "to_json_file", "(", "self", ",", "json_file_path", ")", ":", "\n", "        ", "\"\"\" Save this instance to a json file.\"\"\"", "\n", "with", "open", "(", "json_file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.__init__": [[104, 121], ["torch.Module.__init__", "modeling_gpt2_condition.Attention.register_buffer", "modeling_utils.Conv1D", "modeling_utils.Conv1D", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "set", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nx", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "\n", "n_state", "=", "nx", "# in Attention: n_state=768 (nx=n_embd)", "\n", "# [switch nx => n_state from Block to Attention to keep identical to TF implem]", "\n", "assert", "n_state", "%", "config", ".", "n_head", "==", "0", "\n", "self", ".", "register_buffer", "(", "\"bias\"", ",", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "n_ctx", ",", "n_ctx", ")", ")", ".", "view", "(", "1", ",", "1", ",", "n_ctx", ",", "n_ctx", ")", ")", "\n", "self", ".", "n_head", "=", "config", ".", "n_head", "\n", "self", ".", "split_size", "=", "n_state", "\n", "self", ".", "scale", "=", "scale", "\n", "\n", "self", ".", "c_attn", "=", "Conv1D", "(", "n_state", "*", "3", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "attn_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attn_pdrop", ")", "\n", "self", ".", "resid_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "resid_pdrop", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.prune_heads": [[122, 143], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "mask.view().contiguous().eq.view().contiguous().eq.view().contiguous().eq", "[].long", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_utils.prune_conv1d_layer", "modeling_utils.prune_conv1d_layer", "modeling_gpt2_condition.Attention.pruned_heads.union", "len", "set", "len", "sum", "mask.view().contiguous().eq.view().contiguous().eq.view().contiguous", "len", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "mask.view().contiguous().eq.view().contiguous().eq.view", "len"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_conv1d_layer", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.prune_conv1d_layer"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "mask", "=", "torch", ".", "ones", "(", "self", ".", "n_head", ",", "self", ".", "split_size", "//", "self", ".", "n_head", ")", "\n", "heads", "=", "set", "(", "heads", ")", "-", "self", ".", "pruned_heads", "# Convert to set and emove already pruned heads", "\n", "for", "head", "in", "heads", ":", "\n", "# Compute how many pruned heads are before the head and move the index accordingly", "\n", "            ", "head", "=", "head", "-", "sum", "(", "1", "if", "h", "<", "head", "else", "0", "for", "h", "in", "self", ".", "pruned_heads", ")", "\n", "mask", "[", "head", "]", "=", "0", "\n", "", "mask", "=", "mask", ".", "view", "(", "-", "1", ")", ".", "contiguous", "(", ")", ".", "eq", "(", "1", ")", "\n", "index", "=", "torch", ".", "arange", "(", "len", "(", "mask", ")", ")", "[", "mask", "]", ".", "long", "(", ")", "\n", "index_attn", "=", "torch", ".", "cat", "(", "[", "index", ",", "index", "+", "self", ".", "split_size", ",", "index", "+", "(", "2", "*", "self", ".", "split_size", ")", "]", ")", "\n", "\n", "# Prune conv1d layers", "\n", "self", ".", "c_attn", "=", "prune_conv1d_layer", "(", "self", ".", "c_attn", ",", "index_attn", ",", "dim", "=", "1", ")", "\n", "self", ".", "c_proj", "=", "prune_conv1d_layer", "(", "self", ".", "c_proj", ",", "index", ",", "dim", "=", "0", ")", "\n", "\n", "# Update hyper params", "\n", "self", ".", "split_size", "=", "(", "self", ".", "split_size", "//", "self", ".", "n_head", ")", "*", "(", "self", ".", "n_head", "-", "len", "(", "heads", ")", ")", "\n", "self", ".", "n_head", "=", "self", ".", "n_head", "-", "len", "(", "heads", ")", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention._attn": [[144, 167], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling_gpt2_condition.Attention.attn_dropout", "modeling_gpt2_condition.Attention.size", "modeling_gpt2_condition.Attention.size", "torch.Softmax", "torch.Softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "outputs.append", "math.sqrt", "v.size"], "methods", ["None"], ["", "def", "_attn", "(", "self", ",", "q", ",", "k", ",", "v", ",", "attention_mask", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "w", "=", "torch", ".", "matmul", "(", "q", ",", "k", ")", "\n", "if", "self", ".", "scale", ":", "\n", "            ", "w", "=", "w", "/", "math", ".", "sqrt", "(", "v", ".", "size", "(", "-", "1", ")", ")", "\n", "", "nd", ",", "ns", "=", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", "\n", "b", "=", "self", ".", "bias", "[", ":", ",", ":", ",", "ns", "-", "nd", ":", "ns", ",", ":", "ns", "]", "\n", "w", "=", "w", "*", "b", "-", "1e4", "*", "(", "1", "-", "b", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask", "\n", "            ", "w", "=", "w", "+", "attention_mask", "\n", "\n", "", "w", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "w", ")", "\n", "w", "=", "self", ".", "attn_dropout", "(", "w", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "w", "=", "w", "*", "head_mask", "\n", "\n", "", "outputs", "=", "[", "torch", ".", "matmul", "(", "w", ",", "v", ")", "]", "\n", "if", "self", ".", "output_attentions", ":", "\n", "            ", "outputs", ".", "append", "(", "w", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.merge_heads": [[168, 172], ["x.permute().contiguous.permute().contiguous.permute().contiguous", "x.permute().contiguous.permute().contiguous.view", "x.permute().contiguous.permute().contiguous.permute", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size"], "methods", ["None"], ["", "def", "merge_heads", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "x", ".", "size", "(", "-", "2", ")", "*", "x", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct merge_states", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads": [[173, 180], ["x.view.view.view", "x.view.view.permute", "x.view.view.permute", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "split_heads", "(", "self", ",", "x", ",", "k", "=", "False", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "n_head", ",", "x", ".", "size", "(", "-", "1", ")", "//", "self", ".", "n_head", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct split_states", "\n", "if", "k", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (batch, head, head_features, seq_length)", "\n", "", "else", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# (batch, head, seq_length, head_features)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.forward": [[181, 202], ["modeling_gpt2_condition.Attention.c_attn", "modeling_gpt2_condition.Attention.split", "modeling_gpt2_condition.Attention.split_heads", "modeling_gpt2_condition.Attention.split_heads", "modeling_gpt2_condition.Attention.split_heads", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "modeling_gpt2_condition.Attention._attn", "modeling_gpt2_condition.Attention.merge_heads", "modeling_gpt2_condition.Attention.c_proj", "modeling_gpt2_condition.Attention.resid_dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layer_past[].transpose", "torch.cat.transpose", "torch.cat.transpose"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.split_heads", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention._attn", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.merge_heads"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ",", "attention_mask", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "x", "=", "self", ".", "c_attn", "(", "x", ")", "\n", "query", ",", "key", ",", "value", "=", "x", ".", "split", "(", "self", ".", "split_size", ",", "dim", "=", "2", ")", "\n", "query", "=", "self", ".", "split_heads", "(", "query", ")", "\n", "key", "=", "self", ".", "split_heads", "(", "key", ",", "k", "=", "True", ")", "\n", "value", "=", "self", ".", "split_heads", "(", "value", ")", "\n", "if", "layer_past", "is", "not", "None", ":", "\n", "            ", "past_key", ",", "past_value", "=", "layer_past", "[", "0", "]", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "layer_past", "[", "1", "]", "# transpose back cf below", "\n", "key", "=", "torch", ".", "cat", "(", "(", "past_key", ",", "key", ")", ",", "dim", "=", "-", "1", ")", "\n", "value", "=", "torch", ".", "cat", "(", "(", "past_value", ",", "value", ")", ",", "dim", "=", "-", "2", ")", "\n", "", "present", "=", "torch", ".", "stack", "(", "(", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "value", ")", ")", "# transpose to have same shapes for stacking", "\n", "\n", "attn_outputs", "=", "self", ".", "_attn", "(", "query", ",", "key", ",", "value", ",", "attention_mask", ",", "head_mask", ")", "\n", "a", "=", "attn_outputs", "[", "0", "]", "\n", "\n", "a", "=", "self", ".", "merge_heads", "(", "a", ")", "\n", "a", "=", "self", ".", "c_proj", "(", "a", ")", "\n", "a", "=", "self", ".", "resid_dropout", "(", "a", ")", "\n", "\n", "outputs", "=", "[", "a", ",", "present", "]", "+", "attn_outputs", "[", "1", ":", "]", "\n", "return", "outputs", "# a, present, (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.MLP.__init__": [[205, 212], ["torch.Module.__init__", "modeling_utils.Conv1D", "modeling_utils.Conv1D", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_state", ",", "config", ")", ":", "# in MLP: n_state=3072 (4 * n_embd)", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "c_fc", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "nx", ",", "n_state", ")", "\n", "self", ".", "act", "=", "gelu", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "resid_pdrop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.MLP.forward": [[213, 217], ["modeling_gpt2_condition.MLP.act", "modeling_gpt2_condition.MLP.c_proj", "modeling_gpt2_condition.MLP.dropout", "modeling_gpt2_condition.MLP.c_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "act", "(", "self", ".", "c_fc", "(", "x", ")", ")", "\n", "h2", "=", "self", ".", "c_proj", "(", "h", ")", "\n", "return", "self", ".", "dropout", "(", "h2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Block.__init__": [[220, 227], ["torch.Module.__init__", "torch.LayerNorm", "torch.LayerNorm", "modeling_gpt2_condition.Attention", "torch.LayerNorm", "torch.LayerNorm", "modeling_gpt2_condition.MLP"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Block", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "ln_1", "=", "nn", ".", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "nx", ",", "n_ctx", ",", "config", ",", "scale", ")", "\n", "self", ".", "ln_2", "=", "nn", ".", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "nx", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Block.forward": [[228, 240], ["modeling_gpt2_condition.Block.attn", "modeling_gpt2_condition.Block.mlp", "modeling_gpt2_condition.Block.ln_1", "modeling_gpt2_condition.Block.ln_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ",", "attention_mask", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "output_attn", "=", "self", ".", "attn", "(", "\n", "self", ".", "ln_1", "(", "x", ")", ",", "layer_past", "=", "layer_past", ",", "attention_mask", "=", "attention_mask", ",", "head_mask", "=", "head_mask", "\n", ")", "\n", "a", "=", "output_attn", "[", "0", "]", "# output_attn: a, present, (attentions)", "\n", "\n", "x", "=", "x", "+", "a", "\n", "m", "=", "self", ".", "mlp", "(", "self", ".", "ln_2", "(", "x", ")", ")", "\n", "x", "=", "x", "+", "m", "\n", "\n", "outputs", "=", "[", "x", "]", "+", "output_attn", "[", "1", ":", "]", "\n", "return", "outputs", "# x, present, (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2PreTrainedModel.__init__": [[270, 272], ["modeling_utils.PreTrainedModel.__init__"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "GPT2PreTrainedModel", ",", "self", ")", ".", "__init__", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2PreTrainedModel._init_weights": [[273, 285], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ",", "Conv1D", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "Conv1D", ")", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.__init__": [[376, 391], ["modeling_gpt2_condition.GPT2PreTrainedModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "modeling_gpt2_condition.GPT2Model.init_weights", "modeling_gpt2_condition.Block", "range"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2Model", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "output_hidden_states", "=", "config", ".", "output_hidden_states", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "self", ".", "output_past", "=", "config", ".", "output_past", "\n", "\n", "self", ".", "wte", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "wpe", "=", "nn", ".", "Embedding", "(", "config", ".", "n_positions", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "config", ".", "embd_pdrop", ")", "\n", "self", ".", "h", "=", "nn", ".", "ModuleList", "(", "[", "Block", "(", "config", ".", "n_ctx", ",", "config", ",", "scale", "=", "True", ")", "for", "_", "in", "range", "(", "config", ".", "n_layer", ")", "]", ")", "\n", "self", ".", "ln_f", "=", "nn", ".", "LayerNorm", "(", "config", ".", "n_embd", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "linear_future", "=", "nn", ".", "Linear", "(", "config", ".", "word_emb_dim", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "wpe_future", "=", "nn", ".", "Embedding", "(", "config", ".", "n_positions", ",", "config", ".", "n_embd", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.get_input_embeddings": [[392, 394], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wte", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.set_input_embeddings": [[395, 397], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "wte", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model._prune_heads": [[398, 404], ["heads_to_prune.items", "modeling_gpt2_condition.GPT2Model.h[].attn.prune_heads"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.Attention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "h", "[", "layer", "]", ".", "attn", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2Model.forward": [[405, 537], ["modeling_gpt2_condition.GPT2Model.wpe", "modeling_gpt2_condition.GPT2Model.drop", "enumerate", "modeling_gpt2_condition.GPT2Model.ln_f", "stitch_tensor.view", "ValueError", "token_type_ids.view.view.view", "position_ids.unsqueeze().view.unsqueeze().view.view", "[].size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().view.unsqueeze().view.unsqueeze().view", "attention_mask.to.to.view", "attention_mask.to.to.unsqueeze().unsqueeze", "attention_mask.to.to.to", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "modeling_gpt2_condition.GPT2Model.wte", "modeling_gpt2_condition.GPT2Model.wte", "modeling_gpt2_condition.GPT2Model.wpe_future", "modeling_gpt2_condition.diff_conversion", "torch.split", "torch.split", "torch.split", "torch.split", "modeling_gpt2_condition.stitch_tensor", "zip", "block", "tuple", "input_ids.view.view.size", "input_ids.view.view.view", "len", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "stitch_tensor.size", "stitch_tensor.size", "stitch_tensor.size", "stitch_tensor.size", "tuple.append", "ValueError", "position_ids.unsqueeze().view.unsqueeze().view.unsqueeze", "attention_mask.to.to.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "modeling_gpt2_condition.GPT2Model.linear_future", "insert_pos_emb[].unsqueeze().unsqueeze", "enumerate", "t.view", "modeling_gpt2_condition.GPT2Model.size", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "stitch_tensor.view", "modeling_gpt2_condition.GPT2Model.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "modeling_gpt2_condition.GPT2Model.parameters", "next", "insert_pos_emb[].unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "modeling_gpt2_condition.GPT2Model.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.diff_conversion", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.stitch_tensor"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "insert_loc", "=", "None", ",", "\n", "future_emb_chosen_arr", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "if", "token_type_ids", "is", "not", "None", ":", "\n", "            ", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "if", "position_ids", "is", "not", "None", ":", "\n", "            ", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "\n", "", "if", "past", "is", "None", ":", "\n", "            ", "past_length", "=", "0", "\n", "past", "=", "[", "None", "]", "*", "len", "(", "self", ".", "h", ")", "\n", "", "else", ":", "\n", "            ", "past_length", "=", "past", "[", "0", "]", "[", "0", "]", ".", "size", "(", "-", "2", ")", "\n", "", "if", "position_ids", "is", "None", ":", "\n", "            ", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "position_ids", "=", "torch", ".", "arange", "(", "past_length", ",", "input_shape", "[", "-", "1", "]", "+", "past_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "\n", "# Attention mask.", "\n", "", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_mask", "=", "attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "attention_mask", "=", "(", "1.0", "-", "attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# head_mask has shape n_layer x batch x n_heads x N x N", "\n", "", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "self", ".", "config", ".", "n_layer", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "head_mask", "=", "(", "\n", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "# We can specify head_mask for each layer", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "\n", ")", "# switch to fload if need + fp16 compatibility", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "n_layer", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "wte", "(", "input_ids", ")", "\n", "", "position_embeds", "=", "self", ".", "wpe", "(", "position_ids", ")", "\n", "if", "token_type_ids", "is", "not", "None", ":", "\n", "            ", "token_type_embeds", "=", "self", ".", "wte", "(", "token_type_ids", ")", "\n", "", "else", ":", "\n", "            ", "token_type_embeds", "=", "0", "\n", "\n", "", "hidden_states", "=", "inputs_embeds", "+", "position_embeds", "+", "token_type_embeds", "\n", "\n", "if", "insert_loc", "is", "not", "None", ":", "\n", "#future_emb_chosen_arr should have dimension [(batch_size, chosen_topic_num, word embedding size) * num_insert ]", "\n", "            ", "insert_pos_emb", "=", "self", ".", "wpe_future", "(", "torch", ".", "tensor", "(", "insert_loc", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", ")", "\n", "future_emb_extended", "=", "[", "self", ".", "linear_future", "(", "x", ")", "+", "insert_pos_emb", "[", "i", ",", ":", "]", ".", "unsqueeze", "(", "dim", "=", "0", ")", ".", "unsqueeze", "(", "dim", "=", "0", ")", "for", "i", ",", "x", "in", "enumerate", "(", "future_emb_chosen_arr", ")", "]", "\n", "insert_loc_diff", "=", "diff_conversion", "(", "insert_loc", ",", "hidden_states", ".", "size", "(", "1", ")", ")", "\n", "hidden_states_pieces", "=", "torch", ".", "split", "(", "hidden_states", ",", "insert_loc_diff", ",", "dim", "=", "1", ")", "\n", "hidden_states", "=", "stitch_tensor", "(", "hidden_states_pieces", ",", "future_emb_extended", ",", "dim", "=", "1", ")", "\n", "input_shape", "=", "(", "hidden_states", ".", "size", "(", "0", ")", ",", "hidden_states", ".", "size", "(", "1", ")", ")", "\n", "\n", "", "hidden_states", "=", "self", ".", "drop", "(", "hidden_states", ")", "\n", "\n", "output_shape", "=", "input_shape", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "\n", "presents", "=", "(", ")", "\n", "all_attentions", "=", "[", "]", "\n", "all_hidden_states", "=", "(", ")", "\n", "for", "i", ",", "(", "block", ",", "layer_past", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "h", ",", "past", ")", ")", ":", "\n", "            ", "if", "self", ".", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ".", "view", "(", "*", "output_shape", ")", ",", ")", "\n", "\n", "", "outputs", "=", "block", "(", "\n", "hidden_states", ",", "layer_past", "=", "layer_past", ",", "attention_mask", "=", "attention_mask", ",", "head_mask", "=", "head_mask", "[", "i", "]", "\n", ")", "\n", "\n", "hidden_states", ",", "present", "=", "outputs", "[", ":", "2", "]", "\n", "if", "self", ".", "output_past", ":", "\n", "                ", "presents", "=", "presents", "+", "(", "present", ",", ")", "\n", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "                ", "all_attentions", ".", "append", "(", "outputs", "[", "2", "]", ")", "\n", "\n", "", "", "hidden_states", "=", "self", ".", "ln_f", "(", "hidden_states", ")", "\n", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "*", "output_shape", ")", "\n", "# Add last hidden state", "\n", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "if", "self", ".", "output_past", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "presents", ",", ")", "\n", "", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_hidden_states", ",", ")", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "# let the number of heads free (-1) so we can extract attention even after head pruning", "\n", "            ", "attention_output_shape", "=", "input_shape", "[", ":", "-", "1", "]", "+", "(", "-", "1", ",", ")", "+", "all_attentions", "[", "0", "]", ".", "shape", "[", "-", "2", ":", "]", "\n", "all_attentions", "=", "tuple", "(", "t", ".", "view", "(", "*", "attention_output_shape", ")", "for", "t", "in", "all_attentions", ")", "\n", "outputs", "=", "outputs", "+", "(", "all_attentions", ",", ")", "\n", "", "return", "outputs", "# last hidden state, (presents), (all hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.__init__": [[586, 592], ["modeling_gpt2_condition.GPT2PreTrainedModel.__init__", "modeling_gpt2_condition.GPT2Model", "torch.Linear", "torch.Linear", "modeling_gpt2_condition.GPT2LMHeadModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2LMHeadModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "GPT2Model", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "n_embd", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.get_output_embeddings": [[593, 595], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.prepare_inputs_for_generation": [[596, 604], ["inputs.update", "input_ids[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "**", "kwargs", ")", ":", "\n", "# only last token for inputs_ids if past is defined in kwargs", "\n", "        ", "if", "\"past\"", "in", "kwargs", "and", "kwargs", "[", "\"past\"", "]", ":", "\n", "            ", "input_ids", "=", "input_ids", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "inputs", "=", "{", "\"input_ids\"", ":", "input_ids", "}", "\n", "inputs", ".", "update", "(", "kwargs", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.forward": [[605, 655], ["modeling_gpt2_condition.GPT2LMHeadModel.transformer", "modeling_gpt2_condition.GPT2LMHeadModel.lm_head", "lm_logits[].contiguous", "labels[].contiguous", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_gpt2_condition.diff_conversion", "torch.split", "torch.split", "torch.split", "torch.split", "modeling_gpt2_condition.stitch_tensor", "lm_logits[].contiguous.view", "labels[].contiguous.view", "stitch_tensor.size", "future_emb_chosen.size", "ignore_labels.append", "lm_logits[].contiguous.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "next", "modeling_gpt2_condition.GPT2LMHeadModel.parameters"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.diff_conversion", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.stitch_tensor"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "insert_loc", "=", "None", ",", "\n", "future_emb_chosen_arr", "=", "None", ",", "\n", ")", ":", "\n", "        ", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "past", "=", "past", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "insert_loc", "=", "insert_loc", ",", "\n", "future_emb_chosen_arr", "=", "future_emb_chosen_arr", ",", "\n", ")", "\n", "hidden_states", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "\n", "outputs", "=", "(", "lm_logits", ",", ")", "+", "transformer_outputs", "[", "1", ":", "]", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "insert_loc", "is", "not", "None", ":", "\n", "#print(labels.size())", "\n", "                ", "insert_loc_plus", "=", "insert_loc", "+", "1", "\n", "insert_loc_label", "=", "diff_conversion", "(", "insert_loc_plus", ",", "labels", ".", "size", "(", "1", ")", ")", "\n", "labels_pieces", "=", "torch", ".", "split", "(", "labels", ",", "insert_loc_label", ",", "dim", "=", "1", ")", "\n", "ignore_labels", "=", "[", "]", "\n", "for", "future_emb_chosen", "in", "future_emb_chosen_arr", ":", "\n", "                    ", "batch_size", ",", "chosen_topic_num", ",", "emb_size", "=", "future_emb_chosen", ".", "size", "(", ")", "\n", "ignore_labels", ".", "append", "(", "-", "100", "*", "torch", ".", "ones", "(", "(", "batch_size", ",", "chosen_topic_num", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", ")", "\n", "", "labels", "=", "stitch_tensor", "(", "labels_pieces", ",", "ignore_labels", ",", "dim", "=", "1", ")", "\n", "#print(labels.size())", "\n", "# Shift so that tokens < n predict n", "\n", "", "shift_logits", "=", "lm_logits", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "labels", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "# Flatten the tokens", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "100", ")", "\n", "loss", "=", "loss_fct", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (loss), lm_logits, presents, (all hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2DoubleHeadsModel.__init__": [[729, 737], ["modeling_gpt2_condition.GPT2PreTrainedModel.__init__", "modeling_gpt2_condition.GPT2Model", "torch.Linear", "torch.Linear", "modeling_utils.SequenceSummary", "modeling_gpt2_condition.GPT2DoubleHeadsModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2DoubleHeadsModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "config", ".", "num_labels", "=", "1", "\n", "self", ".", "transformer", "=", "GPT2Model", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "n_embd", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "multiple_choice_head", "=", "SequenceSummary", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2DoubleHeadsModel.get_output_embeddings": [[738, 740], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.GPT2DoubleHeadsModel.forward": [[741, 782], ["modeling_gpt2_condition.GPT2DoubleHeadsModel.transformer", "modeling_gpt2_condition.GPT2DoubleHeadsModel.lm_head", "modeling_gpt2_condition.GPT2DoubleHeadsModel.multiple_choice_head().squeeze", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "lm_logits[].contiguous", "lm_labels[].contiguous", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_gpt2_condition.GPT2DoubleHeadsModel.multiple_choice_head", "modeling_gpt2_condition.GPT2DoubleHeadsModel.view", "mc_labels.view", "lm_logits[].contiguous.view", "lm_labels[].contiguous.view", "modeling_gpt2_condition.GPT2DoubleHeadsModel.size", "lm_logits[].contiguous.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "mc_token_ids", "=", "None", ",", "\n", "lm_labels", "=", "None", ",", "\n", "mc_labels", "=", "None", ",", "\n", ")", ":", "\n", "        ", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "past", "=", "past", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "mc_logits", "=", "self", ".", "multiple_choice_head", "(", "hidden_states", ",", "mc_token_ids", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "outputs", "=", "(", "lm_logits", ",", "mc_logits", ")", "+", "transformer_outputs", "[", "1", ":", "]", "\n", "if", "mc_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "mc_logits", ".", "view", "(", "-", "1", ",", "mc_logits", ".", "size", "(", "-", "1", ")", ")", ",", "mc_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "", "if", "lm_labels", "is", "not", "None", ":", "\n", "            ", "shift_logits", "=", "lm_logits", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "lm_labels", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.load_tf_weights_in_gpt2": [[44, 97], ["os.path.abspath", "logger.info", "tf.train.list_variables", "zip", "logger.info", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "logger.info", "torch.from_numpy", "torch.from_numpy", "logger.error", "tf.train.load_variable.squeeze", "re.fullmatch", "re.split", "getattr", "len", "int", "getattr", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_gpt2", "(", "model", ",", "config", ",", "gpt2_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\n", "\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", "\n", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "gpt2_checkpoint_path", ")", "\n", "logger", ".", "info", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ".", "squeeze", "(", ")", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", "[", "6", ":", "]", "# skip \"model/\"", "\n", "name", "=", "name", ".", "split", "(", "\"/\"", ")", "\n", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r\"[A-Za-z]+\\d+\"", ",", "m_name", ")", ":", "\n", "                ", "scope_names", "=", "re", ".", "split", "(", "r\"(\\d+)\"", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "scope_names", "=", "[", "m_name", "]", "\n", "", "if", "scope_names", "[", "0", "]", "==", "\"w\"", "or", "scope_names", "[", "0", "]", "==", "\"g\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"b\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"bias\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"wpe\"", "or", "scope_names", "[", "0", "]", "==", "\"wte\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "scope_names", "[", "0", "]", ")", "\n", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "scope_names", "[", "0", "]", ")", "\n", "", "if", "len", "(", "scope_names", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "scope_names", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.gelu": [[99, 101], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.stitch_tensor": [[242, 249], ["torch.cat", "torch.cat", "range", "int", "int", "len", "len"], "function", ["None"], ["", "", "def", "stitch_tensor", "(", "labels_pieces", ",", "ignore_labels", ",", "dim", ")", ":", "\n", "#print(len(labels_pieces))", "\n", "#print(len(ignore_labels))", "\n", "    ", "merged_list", "=", "[", "labels_pieces", "[", "int", "(", "i", "/", "2", ")", "]", "if", "i", "%", "2", "==", "0", "else", "ignore_labels", "[", "int", "(", "(", "i", "-", "1", ")", "/", "2", ")", "]", "for", "i", "in", "range", "(", "len", "(", "labels_pieces", ")", "+", "len", "(", "ignore_labels", ")", ")", "]", "\n", "#print(labels_pieces, ignore_labels, merged_list)", "\n", "labels", "=", "torch", ".", "cat", "(", "merged_list", ",", "dim", "=", "dim", ")", "\n", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_gpt2_condition.diff_conversion": [[250, 259], ["numpy.diff().tolist", "np.diff().tolist.insert", "np.diff().tolist.append", "numpy.diff"], "function", ["None"], ["", "def", "diff_conversion", "(", "insert_loc_plus", ",", "seq_len", ")", ":", "\n", "#print(insert_loc_plus)", "\n", "    ", "first_loc", "=", "insert_loc_plus", "[", "0", "]", "\n", "last_loc", "=", "insert_loc_plus", "[", "-", "1", "]", "\n", "insert_loc_label", "=", "np", ".", "diff", "(", "insert_loc_plus", ")", ".", "tolist", "(", ")", "\n", "insert_loc_label", ".", "insert", "(", "0", ",", "first_loc", ")", "\n", "insert_loc_label", ".", "append", "(", "seq_len", "-", "last_loc", ")", "\n", "#print(insert_loc_label)", "\n", "return", "insert_loc_label", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.__init__": [[7, 10], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "unigram_list", "=", "[", "]", "\n", "self", ".", "bigram_list", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.ngram_list": [[12, 21], ["range", "range", "res.append", "len"], "methods", ["None"], ["", "def", "ngram_list", "(", "self", ",", "n", ",", "doc", ")", ":", "\n", "        ", "res", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "doc", ")", "-", "n", "+", "1", ")", ":", "\n", "            ", "string", "=", "\"\"", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                ", "string", "+=", "doc", "[", "i", "+", "j", "]", ".", "text", "+", "\" \"", "\n", "", "string", "+=", "doc", "[", "i", "+", "n", "-", "1", "]", ".", "text", "\n", "res", ".", "append", "(", "string", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add": [[23, 28], ["spacy.lang.en.English", "spacy.lang.en.English.", "ngram.ngram.unigram_list.extend", "ngram.ngram.bigram_list.extend", "ngram.ngram.ngram_list", "ngram.ngram.ngram_list"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.ngram_list", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.ngram_list"], ["", "def", "add", "(", "self", ",", "string", ")", ":", "\n", "        ", "nlp", "=", "English", "(", ")", "\n", "doc", "=", "nlp", "(", "string", ")", "\n", "self", ".", "unigram_list", ".", "extend", "(", "self", ".", "ngram_list", "(", "1", ",", "doc", ")", ")", "\n", "self", ".", "bigram_list", ".", "extend", "(", "self", ".", "ngram_list", "(", "2", ",", "doc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.diversity_n": [[29, 34], ["len", "len", "len", "set", "len", "set", "len"], "methods", ["None"], ["", "def", "diversity_n", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "unigram_list", ")", "==", "0", ":", "\n", "            ", "return", "0", ",", "0", "\n", "#print(self.bigram_list)", "\n", "", "return", "len", "(", "set", "(", "self", ".", "unigram_list", ")", ")", "/", "(", "1e-15", "+", "len", "(", "self", ".", "unigram_list", ")", ")", ",", "len", "(", "set", "(", "self", ".", "bigram_list", ")", ")", "/", "(", "1e-15", "+", "len", "(", "self", ".", "bigram_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.main": [[35, 41], ["ngram.ngram", "ngram.add", "ngram.add", "ngram.add", "print", "ngram.diversity_n"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.diversity_n"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "n", "=", "ngram", "(", ")", "\n", "n", ".", "add", "(", "\"I am who I am, am I the who? Oh, I am!\"", ")", "\n", "n", ".", "add", "(", "\"I love it I love it I love it I love it\"", ")", "\n", "n", ".", "add", "(", "\"In October 2012, Steyer stepped down from his position at Farallon in order to focus on advocating for alternative energy.[18][19] Steyer decided to dispose of his carbon-polluting investments in 2012, although critics say he did not dispose of them quickly enough and noted that the lifespan of the facilities he funded would extend through 2030\"", ")", "\n", "print", "(", "n", ".", "diversity_n", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.interactive_testing_functions.create_args_parser": [[23, 59], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "utils_testing.add_model_arguments"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.add_model_arguments"], ["def", "create_args_parser", "(", ")", ":", "\n", "\t", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch Interactive LM'", ")", "\n", "#path", "\n", "parser", ".", "add_argument", "(", "'--checkpoint_topics'", ",", "type", "=", "str", ",", "default", "=", "'../../models/'", ",", "\n", "help", "=", "'topical model checkpoint to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint_conditional'", ",", "type", "=", "str", ",", "default", "=", "'../../models/'", ",", "\n", "help", "=", "'conditional LM model checkpoint to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--emb_file'", ",", "type", "=", "str", ",", "default", "=", "'target_emb.pt'", ",", "\n", "help", "=", "'path to a word embedding file'", ")", "\n", "parser", ".", "add_argument", "(", "'--word_dict'", ",", "type", "=", "str", ",", "default", "=", "'../../data/processed/wiki2016_gpt2/tensors_all_min100/dict_idx_compact'", ",", "\n", "help", "=", "'path to a dictionary file'", ")", "\n", "#parser.add_argument('--outf', type=str, default='../gen_log/generated.txt',", "\n", "#                    help='output file for generated text')", "\n", "\n", "#parser.add_argument('--batch_size', type=int, default=3, metavar='N',", "\n", "#                    help='batch size')", "\n", "parser", ".", "add_argument", "(", "'--num_sent_gen'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'In each prompt, generate how many sentences'", ")", "\n", "parser", ".", "add_argument", "(", "'--gen_sent_len'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'In each prompt, generate sentences with length gen_sent_len'", ")", "\n", "parser", ".", "add_argument", "(", "'--bptt'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "'sequence length'", ")", "\n", "parser", ".", "add_argument", "(", "'--bptt_conditional'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "'sequence length'", ")", "\n", "parser", ".", "add_argument", "(", "'--top_k_nn'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Representing each topic using how many words'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--cuda_topics'", ",", "type", "=", "str2bool", ",", "nargs", "=", "'?'", ",", "default", "=", "True", ",", "\n", "help", "=", "'use CUDA for topical model'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda_conditional'", ",", "type", "=", "str2bool", ",", "nargs", "=", "'?'", ",", "default", "=", "True", ",", "\n", "help", "=", "'use CUDA for conditional LM'", ")", "\n", "parser", ".", "add_argument", "(", "'--single_gpu'", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use single GPU'", ")", "\n", "\n", "utils_testing", ".", "add_model_arguments", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.interactive_testing_functions.load_all_components": [[61, 93], ["torch.device", "torch.device", "range", "utils.loading_all_models", "word_norm_emb.size", "torch.load", "gpt2_model.configuration_gpt2.GPT2Config.from_pretrained", "gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.from_pretrained().cuda", "gpt2_model.tokenization_gpt2.GPT2Tokenizer.from_pretrained", "encoder.eval", "decoder.eval", "GPT2LMHeadModel.from_pretrained().cuda.eval", "os.path.join", "open", "utils.load_idx2word_freq", "len", "os.path.join", "gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.from_pretrained"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.loading_all_models", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils.load_idx2word_freq", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained"], ["", "def", "load_all_components", "(", "args", ")", ":", "\n", "\t", "if", "args", ".", "emb_file", "==", "\"target_emb.pt\"", ":", "\n", "\t\t", "args", ".", "emb_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint_topics", ",", "\"target_emb.pt\"", ")", "\n", "", "device_topics", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "args", ".", "cuda_topics", "else", "\"cpu\"", ")", "\n", "device_conditional", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "args", ".", "cuda_conditional", "else", "\"cpu\"", ")", "\n", "#device_topics = torch.device(\"cuda:0\" if args.cuda_topics else \"cpu\")", "\n", "#device_conditional = torch.device(\"cuda:1\" if args.cuda_conditional else \"cpu\")", "\n", "with", "open", "(", "args", ".", "word_dict", ")", "as", "f_in", ":", "\n", "\t\t", "idx2word_freq", "=", "utils", ".", "load_idx2word_freq", "(", "f_in", ")", "\n", "", "word_d2_idx", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "idx2word_freq", ")", ")", ":", "\n", "\t\t", "w", ",", "freq", "=", "idx2word_freq", "[", "i", "]", "\n", "word_d2_idx", "[", "w", "]", "=", "i", "\n", "\n", "", "parallel_encoder", ",", "parallel_decoder", ",", "encoder", ",", "decoder", ",", "word_norm_emb", "=", "utils", ".", "loading_all_models", "(", "args", ",", "idx2word_freq", ",", "device_topics", ")", "\n", "output_emb_size", "=", "word_norm_emb", ".", "size", "(", "1", ")", "\n", "#print(next(encoder.parameters()).device)", "\n", "\n", "model_name", "=", "'gpt2'", "\n", "\n", "encoder_state_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint_conditional", ",", "'encoder.pt'", ")", ",", "map_location", "=", "device_conditional", ")", "\n", "gpt2_config", "=", "GPT2Config", ".", "from_pretrained", "(", "model_name", ")", "\n", "gpt2_config", ".", "word_emb_dim", "=", "output_emb_size", "\n", "model_condition", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "model_name", ",", "state_dict", "=", "encoder_state_dict", ",", "config", "=", "gpt2_config", ")", ".", "cuda", "(", "device_conditional", ")", "\n", "#print(next(model_condition.parameters()).device)", "\n", "\n", "tokenizer_GPT2", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "'distilgpt2'", ")", "\n", "\n", "encoder", ".", "eval", "(", ")", "\n", "decoder", ".", "eval", "(", ")", "\n", "model_condition", ".", "eval", "(", ")", "\n", "return", "idx2word_freq", ",", "word_d2_idx", ",", "parallel_encoder", ",", "parallel_decoder", ",", "word_norm_emb", ",", "model_condition", ",", "tokenizer_GPT2", ",", "device_topics", ",", "device_conditional", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.interactive_testing_functions.show_future_topics": [[94, 123], ["tokenizer_GPT2.tokenize", "tokenizer_GPT2.convert_tokens_to_ids", "torch.tensor().unsqueeze", "encoder", "decoder", "basis_norm_pred.permute.permute", "torch.matmul", "torch.topk", "range", "print", "len", "word_norm_emb.unsqueeze", "range", "print", "torch.tensor", "decoder.norm", "top_value.sum", "str", "top_index[].item"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decoder", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "show_future_topics", "(", "prompt", ",", "encoder", ",", "decoder", ",", "word_norm_emb", ",", "n_basis", ",", "top_k", ",", "bptt", ",", "idx2word_freq", ",", "tokenizer_GPT2", ",", "device_topics", ")", ":", "\n", "    ", "tokenized_text", "=", "tokenizer_GPT2", ".", "tokenize", "(", "prompt", ",", "add_prefix_space", "=", "True", ")", "\n", "#print(tokenized_text)", "\n", "indexed_tokens", "=", "tokenizer_GPT2", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "start_idx", "=", "len", "(", "indexed_tokens", ")", "-", "bptt", "\n", "if", "start_idx", ">", "0", ":", "\n", "        ", "indexed_tokens", "=", "indexed_tokens", "[", "start_idx", ":", "]", "\n", "", "feature", "=", "torch", ".", "tensor", "(", "indexed_tokens", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_topics", ")", ".", "unsqueeze", "(", "0", ")", "\n", "output_emb", ",", "past", "=", "encoder", "(", "feature", ")", "\n", "output_emb_last", "=", "output_emb", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "basis_pred", "=", "decoder", "(", "output_emb_last", ")", "\n", "basis_norm_pred", "=", "basis_pred", "/", "(", "0.000000000001", "+", "basis_pred", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "basis_norm_pred", "=", "basis_norm_pred", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "sim_pairwise", "=", "torch", ".", "matmul", "(", "word_norm_emb", ".", "unsqueeze", "(", "dim", "=", "0", ")", ",", "basis_norm_pred", ")", "\n", "top_value", ",", "top_index", "=", "torch", ".", "topk", "(", "sim_pairwise", ",", "top_k", ",", "dim", "=", "1", ",", "sorted", "=", "True", ")", "\n", "top_value", "=", "top_value", "/", "(", "0.000000000001", "+", "top_value", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "#out_str = ''", "\n", "for", "j", "in", "range", "(", "n_basis", ")", ":", "\n", "        ", "out_str", "=", "str", "(", "j", ")", "+", "', '", "\n", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "#for k in range(3):", "\n", "            ", "word_nn", "=", "idx2word_freq", "[", "top_index", "[", "0", ",", "k", ",", "j", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "#out_str += word_nn+' {:5.3f} '.format(top_value[0,k,j].item())", "\n", "out_str", "+=", "word_nn", "+", "', '", "\n", "", "print", "(", "out_str", ")", "\n", "", "print", "(", ")", "\n", "\n", "return", "top_value", ",", "top_index", ",", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.interactive_testing_functions.conditional_generation": [[124, 173], ["word_w_sum_norm.to.to", "torch.tensor", "torch.tensor", "feature.size", "insert_loc_list.append", "feature[].unsqueeze().expand().to", "torch.cat().unsqueeze().expand", "future_emb_chosen_arr.append", "utils_testing.sample_seq", "utils_testing.sample_seq", "print", "range", "print", "range", "torch.sum", "top_value.unsqueeze().sum", "isinstance", "numpy.sort", "numpy.array", "future_emb_chosen_topics.size", "future_emb_chosen_words.size", "word_norm_emb.size", "tokenizer_GPT2.convert_tokens_to_string", "generated_sent.replace().replace().replace.replace().replace().replace", "utils_testing.print_sampled_sent", "tokenizer_GPT2.convert_tokens_to_string", "generated_sent_org.replace().replace().replace.replace().replace().replace", "utils_testing.print_sampled_sent", "word_norm_emb_w_sum.norm", "torch.tensor.append", "torch.tensor.append", "feature[].unsqueeze().expand", "torch.cat().unsqueeze", "torch.tensor.tolist", "torch.tensor.tolist", "torch.tensor.tolist", "torch.tensor.tolist", "top_value.unsqueeze", "top_value.unsqueeze", "print", "tokenizer_GPT2._convert_id_to_token", "generated_sent.replace().replace().replace.replace().replace", "str", "tokenizer_GPT2._convert_id_to_token", "generated_sent_org.replace().replace().replace.replace().replace", "str", "feature[].unsqueeze", "torch.cat", "tokenizer_GPT2.decode", "output[].tolist", "tokenizer_GPT2.decode", "output_org[].tolist", "generated_sent.replace().replace().replace.replace", "generated_sent_org.replace().replace().replace.replace"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.sample_seq", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_sampled_sent", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.src.utils_testing.print_sampled_sent", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "conditional_generation", "(", "selected_conditions", ",", "gen_sent_len", ",", "num_sent_gen", ",", "word_d2_idx", ",", "idx2word_freq", ",", "model_condition", ",", "word_norm_emb", ",", "top_index", ",", "top_value", ",", "feature", ",", "bptt_conditional", ",", "tokenizer_GPT2", ",", "device_conditional", ")", ":", "\n", "    ", "word_norm_emb_top", "=", "word_norm_emb", "[", "top_index", ",", ":", "]", "\n", "word_norm_emb_w_sum", "=", "torch", ".", "sum", "(", "word_norm_emb_top", "*", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "/", "top_value", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "word_w_sum_norm", "=", "word_norm_emb_w_sum", "/", "(", "0.000000000001", "+", "word_norm_emb_w_sum", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "word_w_sum_norm", "=", "word_w_sum_norm", ".", "to", "(", "device", "=", "device_conditional", ")", "\n", "selected_topic_idx", "=", "[", "]", "\n", "selected_word_idx", "=", "[", "]", "\n", "for", "x", "in", "selected_conditions", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "int", ")", ":", "\n", "            ", "selected_topic_idx", ".", "append", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "if", "x", "not", "in", "word_d2_idx", ":", "\n", "                ", "print", "(", "'Warning: Ignore the word '", "+", "x", "+", "' because it is too rare'", ")", "\n", "continue", "\n", "", "selected_word_idx", ".", "append", "(", "word_d2_idx", "[", "x", "]", ")", "\n", "", "", "selected_topic_idx", "=", "torch", ".", "tensor", "(", "np", ".", "sort", "(", "selected_topic_idx", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "selected_word_idx", "=", "torch", ".", "tensor", "(", "selected_word_idx", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device_conditional", ")", "\n", "\n", "end_int", "=", "feature", ".", "size", "(", "1", ")", "\n", "max_prompt_len", "=", "bptt_conditional", "-", "gen_sent_len", "\n", "start_int", "=", "0", "\n", "if", "end_int", ">", "max_prompt_len", ":", "\n", "        ", "start_int", "=", "end_int", "-", "max_prompt_len", "\n", "", "insert_loc_list", "=", "[", "]", "\n", "insert_loc_list", ".", "append", "(", "end_int", "-", "1", ")", "\n", "insert_loc_truncated", "=", "np", ".", "array", "(", "insert_loc_list", ")", "-", "start_int", "\n", "\n", "feature_expanded", "=", "feature", "[", "0", ",", "start_int", ":", "end_int", "]", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_sent_gen", ",", "end_int", "-", "start_int", ")", ".", "to", "(", "device", "=", "device_conditional", ")", "\n", "future_emb_chosen_topics", "=", "word_w_sum_norm", "[", "0", ",", "selected_topic_idx", ",", ":", "]", "\n", "future_emb_chosen_words", "=", "word_norm_emb", "[", "selected_word_idx", ",", ":", "]", "\n", "num_selection", "=", "future_emb_chosen_topics", ".", "size", "(", "0", ")", "+", "future_emb_chosen_words", ".", "size", "(", "0", ")", "\n", "future_emb_chosen", "=", "torch", ".", "cat", "(", "[", "future_emb_chosen_topics", ",", "future_emb_chosen_words", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_sent_gen", ",", "num_selection", ",", "word_norm_emb", ".", "size", "(", "-", "1", ")", ")", "\n", "future_emb_chosen_arr", "=", "[", "]", "\n", "future_emb_chosen_arr", ".", "append", "(", "future_emb_chosen", ")", "\n", "truncate_idx", "=", "0", "\n", "output", "=", "utils_testing", ".", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "insert_loc_truncated", "[", "truncate_idx", ":", "]", ",", "future_emb_chosen_arr", "[", "truncate_idx", ":", "]", ",", "gen_sent_len", ",", "device_conditional", ")", "\n", "output_org", "=", "utils_testing", ".", "sample_seq", "(", "model_condition", ",", "feature_expanded", ",", "None", ",", "None", ",", "gen_sent_len", ",", "device_conditional", ")", "\n", "\n", "print", "(", "colorama", ".", "Fore", ".", "BLUE", "+", "\"Prompt: \"", "+", "tokenizer_GPT2", ".", "decode", "(", "feature", "[", "0", ",", "start_int", ":", "end_int", "]", ")", "+", "'\\n'", "+", "colorama", ".", "Style", ".", "RESET_ALL", ")", "\n", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "        ", "generated_sent", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "[", "tokenizer_GPT2", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "output", "[", "j", ",", ":", "]", ".", "tolist", "(", ")", "]", ")", "\n", "generated_sent", "=", "generated_sent", ".", "replace", "(", "'\u00e2\u0080\u0099'", ",", "\"'\"", ")", ".", "replace", "(", "'\u00e2\u0080\u0093'", ",", "'-'", ")", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", "\n", "utils_testing", ".", "print_sampled_sent", "(", "selected_topic_idx", ".", "tolist", "(", ")", ",", "generated_sent", ",", "top_index", "[", "0", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "sys", ".", "stdout", ",", "'conditional '", "+", "str", "(", "j", ")", ",", "selected_word_idx", ".", "tolist", "(", ")", ")", "\n", "\n", "", "print", "(", "\"\\n\"", "+", "colorama", ".", "Fore", ".", "BLUE", "+", "\"Prompt: \"", "+", "tokenizer_GPT2", ".", "decode", "(", "feature", "[", "0", ",", "start_int", ":", "end_int", "]", ")", "+", "'\\n'", "+", "colorama", ".", "Style", ".", "RESET_ALL", ")", "\n", "for", "j", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "        ", "generated_sent_org", "=", "tokenizer_GPT2", ".", "convert_tokens_to_string", "(", "[", "tokenizer_GPT2", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "output_org", "[", "j", ",", ":", "]", ".", "tolist", "(", ")", "]", ")", "\n", "generated_sent_org", "=", "generated_sent_org", ".", "replace", "(", "'\u00e2\u0080\u0099'", ",", "\"'\"", ")", ".", "replace", "(", "'\u00e2\u0080\u0093'", ",", "'-'", ")", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", "\n", "utils_testing", ".", "print_sampled_sent", "(", "selected_topic_idx", ".", "tolist", "(", ")", ",", "generated_sent_org", ",", "top_index", "[", "0", ",", ":", ",", ":", "]", ",", "idx2word_freq", ",", "sys", ".", "stdout", ",", "'original '", "+", "str", "(", "j", ")", ",", "selected_word_idx", ".", "tolist", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.condition_LM_test.load_STS_filtered": [[121, 141], ["type_d2_idx.items", "open", "len", "line.rstrip().split", "sent_list.append", "type_idx_list.append", "len", "line.rstrip"], "function", ["None"], ["def", "load_STS_filtered", "(", "data_path", ")", ":", "\n", "    ", "sent_list", "=", "[", "]", "\n", "type_idx_list", "=", "[", "]", "\n", "type_d2_idx", "=", "{", "}", "\n", "\n", "with", "open", "(", "data_path", ")", "as", "f_in", ":", "\n", "        ", "for", "line", "in", "f_in", ":", "\n", "            ", "fields", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "sent_list", ".", "append", "(", "fields", "[", "0", "]", ")", "\n", "sent_type", "=", "fields", "[", "-", "1", "]", "\n", "if", "sent_type", "in", "type_d2_idx", ":", "\n", "                ", "type_idx", "=", "type_d2_idx", "[", "sent_type", "]", "\n", "", "else", ":", "\n", "                ", "type_idx", "=", "len", "(", "type_d2_idx", ")", "\n", "type_d2_idx", "[", "sent_type", "]", "=", "type_idx", "\n", "", "type_idx_list", ".", "append", "(", "type_idx", ")", "\n", "", "", "idx_l2_type", "=", "[", "''", "]", "*", "len", "(", "type_d2_idx", ")", "\n", "for", "type_name", ",", "type_idx", "in", "type_d2_idx", ".", "items", "(", ")", ":", "\n", "        ", "idx_l2_type", "[", "type_idx", "]", "=", "type_name", "\n", "", "return", "sent_list", ",", "type_idx_list", ",", "idx_l2_type", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.condition_LM_test.load_STS_org": [[142, 164], ["type_d2_idx.items", "open", "len", "line.rstrip().split", "sent_list.append", "sent_list.append", "type_idx_list.append", "type_idx_list.append", "len", "line.rstrip"], "function", ["None"], ["", "def", "load_STS_org", "(", "data_path", ")", ":", "\n", "    ", "sent_list", "=", "[", "]", "\n", "type_idx_list", "=", "[", "]", "\n", "type_d2_idx", "=", "{", "}", "\n", "\n", "with", "open", "(", "data_path", ")", "as", "f_in", ":", "\n", "        ", "for", "line", "in", "f_in", ":", "\n", "            ", "fields", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "sent_list", ".", "append", "(", "fields", "[", "-", "2", "]", ")", "\n", "sent_list", ".", "append", "(", "fields", "[", "-", "1", "]", ")", "\n", "sent_type", "=", "fields", "[", "0", "]", "\n", "if", "sent_type", "in", "type_d2_idx", ":", "\n", "                ", "type_idx", "=", "type_d2_idx", "[", "sent_type", "]", "\n", "", "else", ":", "\n", "                ", "type_idx", "=", "len", "(", "type_d2_idx", ")", "\n", "type_d2_idx", "[", "sent_type", "]", "=", "type_idx", "\n", "", "type_idx_list", ".", "append", "(", "type_idx", ")", "\n", "type_idx_list", ".", "append", "(", "type_idx", ")", "\n", "", "", "idx_l2_type", "=", "[", "''", "]", "*", "len", "(", "type_d2_idx", ")", "\n", "for", "type_name", ",", "type_idx", "in", "type_d2_idx", ".", "items", "(", ")", ":", "\n", "        ", "idx_l2_type", "[", "type_idx", "]", "=", "type_name", "\n", "", "return", "sent_list", ",", "type_idx_list", ",", "idx_l2_type", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.condition_LM_test.convert_stop_to_ind_lower": [[188, 198], ["set", "set", "enumerate", "line.rstrip", "set.add", "line.rstrip.lower", "set.add"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add"], ["", "def", "convert_stop_to_ind_lower", "(", "f_in", ",", "idx2word_freq", ")", ":", "\n", "    ", "stop_word_org_set", "=", "set", "(", ")", "\n", "for", "line", "in", "f_in", ":", "\n", "        ", "w", "=", "line", ".", "rstrip", "(", ")", "\n", "stop_word_org_set", ".", "add", "(", "w", ")", "\n", "", "stop_word_set", "=", "set", "(", ")", "\n", "for", "idx", ",", "(", "w", ",", "freq", ")", "in", "enumerate", "(", "idx2word_freq", ")", ":", "\n", "        ", "if", "w", ".", "lower", "(", ")", "in", "stop_word_org_set", ":", "\n", "            ", "stop_word_set", ".", "add", "(", "idx", ")", "\n", "", "", "return", "stop_word_set", "\n", "", "with", "open", "(", "args", ".", "stop_word_file", ")", "as", "f_in", ":", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.__init__": [[97, 102], ["spacy.lang.en.English"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "model_results", "=", "{", "}", "\n", "#self.model_results[\"time_count\"] = 0", "\n", "self", ".", "gpt2_model", "=", "model", "\n", "self", ".", "nlp", "=", "English", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.add_model": [[103, 128], ["ngram.ngram.ngram"], "methods", ["None"], ["", "def", "add_model", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "self", ".", "model_results", "[", "model_name", "]", "=", "{", "}", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"batch count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"BLEU_count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"BLEU_count_arr\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"BLEU\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"BLEU_arr\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"context_len_arr\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"context BLEU\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"gen_list_temp\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"self BLEU count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"self BLEU\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"token hit\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"word type hit\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"topic hit\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"exact token hit\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"exact word type hit\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"exact topic hit\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"perplexity\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"ngram\"", "]", "=", "ngram", "(", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"unigram\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"bigram\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"time_sum\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"time_count\"", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update_self_BLEU": [[129, 140], ["len", "nltk.translate.bleu_score.SmoothingFunction", "range", "range", "nltk.translate.bleu_score.sentence_bleu"], "methods", ["None"], ["", "def", "update_self_BLEU", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "count", "=", "0", "\n", "gen_list_temp", "=", "self", ".", "model_results", "[", "model_name", "]", "[", "\"gen_list_temp\"", "]", "\n", "num_sent_gen", "=", "len", "(", "gen_list_temp", ")", "\n", "cc", "=", "nltk", ".", "translate", ".", "bleu_score", ".", "SmoothingFunction", "(", ")", "\n", "for", "i", "in", "range", "(", "num_sent_gen", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "i", "+", "1", ",", "num_sent_gen", ")", ":", "\n", "                ", "self_BLEU", "=", "nltk", ".", "translate", ".", "bleu_score", ".", "sentence_bleu", "(", "[", "gen_list_temp", "[", "i", "]", "]", ",", "gen_list_temp", "[", "j", "]", ",", "weights", "=", "(", "0.5", ",", "0.5", ")", ",", "smoothing_function", "=", "cc", ".", "method3", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"self BLEU count\"", "]", "+=", "1", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"self BLEU\"", "]", "+=", "self_BLEU", "\n", "", "", "self", ".", "model_results", "[", "model_name", "]", "[", "\"gen_list_temp\"", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.update": [[142, 170], ["tokenizer.convert_tokens_to_string", "result_statistics.sample_statistics", "result_statistics.perplexity", "[].add", "context.unsqueeze", "sentence.unsqueeze", "result_statistics.eval_using_BLEU", "tokenizer._convert_id_to_token", "len", "[].append", "[].append", "[].append", "[].append", "len", "sentence.tolist"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.sample_statistics", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.perplexity", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.add", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.eval_using_BLEU", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token"], ["", "def", "update", "(", "self", ",", "model_name", ",", "sentence", ",", "context", ",", "selected_topic_idx", ",", "top_index", ",", "idx2word_freq", ",", "tokenizer", ",", "word_raw_list_i_j", "=", "None", ",", "word_raw_rest_list_i_j", "=", "None", ",", "j", "=", "None", ")", ":", "\n", "        ", "generated_sent", "=", "tokenizer", ".", "convert_tokens_to_string", "(", "[", "tokenizer", ".", "_convert_id_to_token", "(", "x", ")", "for", "x", "in", "sentence", ".", "tolist", "(", ")", "]", ")", "\n", "num_token_hit", ",", "num_word_type_hit", ",", "num_topic_hit", "=", "sample_statistics", "(", "selected_topic_idx", ",", "generated_sent", ",", "top_index", ",", "idx2word_freq", ")", "\n", "log_perplexity", "=", "perplexity", "(", "self", ".", "gpt2_model", ",", "context", ".", "unsqueeze", "(", "0", ")", ",", "sentence", ".", "unsqueeze", "(", "0", ")", ")", "\n", "if", "word_raw_list_i_j", "is", "not", "None", ":", "\n", "#context_sents = tokenizer.convert_tokens_to_string( [tokenizer._convert_id_to_token(x) for x in context.tolist()] )", "\n", "            ", "while", "j", ">=", "len", "(", "self", ".", "model_results", "[", "model_name", "]", "[", "'BLEU_count_arr'", "]", ")", ":", "\n", "                ", "self", ".", "model_results", "[", "model_name", "]", "[", "'BLEU_count_arr'", "]", ".", "append", "(", "1e-15", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'BLEU_arr'", "]", ".", "append", "(", "0", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'context_len_arr'", "]", ".", "append", "(", "0", ")", "\n", "", "BLEU", ",", "context_BLEU", ",", "gen_list", "=", "eval_using_BLEU", "(", "generated_sent", ",", "word_raw_list_i_j", ",", "word_raw_rest_list_i_j", ",", "self", ".", "nlp", ")", "\n", "if", "BLEU", "!=", "-", "1", ":", "\n", "                ", "self", ".", "model_results", "[", "model_name", "]", "[", "\"gen_list_temp\"", "]", ".", "append", "(", "gen_list", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"BLEU\"", "]", "+=", "BLEU", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"context BLEU\"", "]", "+=", "context_BLEU", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"BLEU_count\"", "]", "+=", "1", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'BLEU_count_arr'", "]", "[", "j", "]", "+=", "1", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'BLEU_arr'", "]", "[", "j", "]", "+=", "BLEU", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'context_len_arr'", "]", "[", "j", "]", "+=", "len", "(", "word_raw_list_i_j", "[", ":", "-", "1", "]", ")", "\n", "", "", "self", ".", "model_results", "[", "model_name", "]", "[", "\"count\"", "]", "+=", "1", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"token hit\"", "]", "+=", "num_token_hit", "[", "0", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"word type hit\"", "]", "+=", "num_word_type_hit", "[", "0", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"topic hit\"", "]", "+=", "num_topic_hit", "[", "0", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"exact token hit\"", "]", "+=", "num_token_hit", "[", "1", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"exact word type hit\"", "]", "+=", "num_word_type_hit", "[", "1", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"exact topic hit\"", "]", "+=", "num_topic_hit", "[", "1", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"perplexity\"", "]", "+=", "log_perplexity", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"ngram\"", "]", ".", "add", "(", "generated_sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.renew_ngram": [[172, 180], ["result_statistics.result_statistics.model_results.items", "model[].diversity_n", "ngram.ngram.ngram"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.ngram.ngram.diversity_n"], ["", "def", "renew_ngram", "(", "self", ")", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "self", ".", "model_results", ".", "items", "(", ")", ":", "\n", "            ", "model", "[", "\"batch count\"", "]", "+=", "1", "\n", "unigram", ",", "bigram", "=", "model", "[", "\"ngram\"", "]", ".", "diversity_n", "(", ")", "\n", "#print(model[\"ngram\"].diversity_n())", "\n", "model", "[", "\"unigram\"", "]", "+=", "unigram", "\n", "model", "[", "\"bigram\"", "]", "+=", "bigram", "\n", "model", "[", "\"ngram\"", "]", "=", "ngram", "(", ")", "\n", "#print(model[\"ngram\"].diversity_n())", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print": [[183, 206], ["result_statistics.result_statistics.model_results.items", "result_statistics.result_statistics.print"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "", "def", "print", "(", "self", ")", ":", "\n", "        ", "for", "model_name", ",", "model", "in", "self", ".", "model_results", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "model_name", ",", "\"count: \"", ",", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"BLEU count: \"", ",", "model", "[", "\"BLEU_count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"batch count: \"", ",", "model", "[", "\"batch count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"BLEU: \"", ",", "model", "[", "\"BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", ")", "\n", "print", "(", "model_name", "+", "\" \"", "+", "\"BLEU_arr: \"", "+", "str", "(", "[", "model", "[", "\"BLEU_arr\"", "]", "[", "x", "]", "/", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"BLEU_count_arr\"", "]", ")", ")", "]", ")", ")", "\n", "print", "(", "model_name", "+", "\" \"", "+", "\"context_len_arr: \"", "+", "str", "(", "[", "model", "[", "\"context_len_arr\"", "]", "[", "x", "]", "/", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"BLEU_count_arr\"", "]", ")", ")", "]", ")", ")", "\n", "print", "(", "model_name", ",", "\"self BLEU: \"", ",", "model", "[", "\"self BLEU\"", "]", "/", "(", "1e-13", "+", "model", "[", "\"self BLEU count\"", "]", ")", ")", "\n", "print", "(", "model_name", ",", "\"context BLEU: \"", ",", "model", "[", "\"context BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", ")", "\n", "print", "(", "model_name", "+", "\" \"", "+", "\"BLEU Diff: \"", "+", "str", "(", "model", "[", "\"BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", "-", "model", "[", "\"context BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", ")", ")", "\n", "print", "(", "model_name", ",", "\"token hit: \"", ",", "model", "[", "\"token hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"exact token hit: \"", ",", "model", "[", "\"exact token hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"word type hit: \"", ",", "model", "[", "\"word type hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"exact word type hit: \"", ",", "model", "[", "\"exact word type hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"topic hit: \"", ",", "model", "[", "\"topic hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"exact topic hit: \"", ",", "model", "[", "\"exact topic hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"perplexity: \"", ",", "math", ".", "exp", "(", "model", "[", "\"perplexity\"", "]", "/", "model", "[", "\"count\"", "]", ")", ")", "\n", "print", "(", "model_name", ",", "\"unigram: \"", ",", "model", "[", "\"unigram\"", "]", "/", "model", "[", "\"batch count\"", "]", ")", "\n", "print", "(", "model_name", ",", "\"bigram: \"", ",", "model", "[", "\"bigram\"", "]", "/", "model", "[", "\"batch count\"", "]", ")", "\n", "if", "model", "[", "'time_count'", "]", ">", "0", ":", "\n", "                ", "print", "(", "model_name", ",", "\"running time: \"", ",", "model", "[", "\"time_sum\"", "]", "/", "model", "[", "'time_count'", "]", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.generate_report": [[209, 233], ["outf.write", "result_statistics.result_statistics.model_results.items", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "math.exp", "str", "range", "range", "len", "len"], "methods", ["None"], ["", "", "def", "generate_report", "(", "self", ",", "outf", ")", ":", "\n", "        ", "outf", ".", "write", "(", "'Reports: \\n'", ")", "\n", "for", "model_name", ",", "model", "in", "self", ".", "model_results", ".", "items", "(", ")", ":", "\n", "            ", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"count: \"", "+", "str", "(", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"BLEU count: \"", "+", "str", "(", "model", "[", "\"BLEU_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"batch count: \"", "+", "str", "(", "model", "[", "\"batch count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"BLEU: \"", "+", "str", "(", "model", "[", "\"BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"BLEU_arr: \"", "+", "str", "(", "[", "model", "[", "\"BLEU_arr\"", "]", "[", "x", "]", "/", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"BLEU_count_arr\"", "]", ")", ")", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"context_len_arr: \"", "+", "str", "(", "[", "model", "[", "\"context_len_arr\"", "]", "[", "x", "]", "/", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"BLEU_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"BLEU_count_arr\"", "]", ")", ")", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"self BLEU: \"", "+", "str", "(", "model", "[", "\"self BLEU\"", "]", "/", "(", "1e-13", "+", "model", "[", "\"self BLEU count\"", "]", ")", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"context BLEU: \"", "+", "str", "(", "model", "[", "\"context BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"BLEU Diff: \"", "+", "str", "(", "model", "[", "\"BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", "-", "model", "[", "\"context BLEU\"", "]", "/", "model", "[", "\"BLEU_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"token hit: \"", "+", "str", "(", "model", "[", "\"token hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"exact token hit: \"", "+", "str", "(", "model", "[", "\"exact token hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"word type hit: \"", "+", "str", "(", "model", "[", "\"word type hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"exact word type hit: \"", "+", "str", "(", "model", "[", "\"exact word type hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"topic hit: \"", "+", "str", "(", "model", "[", "\"topic hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"exact topic hit: \"", "+", "str", "(", "model", "[", "\"exact topic hit\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"perplexity: \"", "+", "str", "(", "math", ".", "exp", "(", "model", "[", "\"perplexity\"", "]", "/", "model", "[", "\"count\"", "]", ")", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"unigram: \"", "+", "str", "(", "model", "[", "\"unigram\"", "]", "/", "model", "[", "\"batch count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"bigram: \"", "+", "str", "(", "model", "[", "\"bigram\"", "]", "/", "model", "[", "\"batch count\"", "]", ")", "+", "'\\n'", ")", "\n", "if", "model", "[", "'time_count'", "]", ">", "0", ":", "\n", "                ", "outf", ".", "write", "(", "model_name", "+", "\" \"", "+", "\"running time: \"", "+", "str", "(", "model", "[", "\"time_sum\"", "]", "/", "model", "[", "'time_count'", "]", ")", "+", "'\\n'", ")", "\n", "", "outf", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.sample_statistics": [[9, 64], ["len", "top_index_im.size", "range", "range", "range", "find_all"], "function", ["None"], ["def", "sample_statistics", "(", "selected_topic_idx", ",", "generated_sent", ",", "top_index_im", ",", "idx2word_freq", ",", "selected_word_idx", "=", "None", ")", ":", "\n", "    ", "def", "highlight_words", "(", "word_nn", ",", "generated_sent", ",", "topic_l2_word_d2_count_t", ",", "exact_count", ")", ":", "\n", "        ", "def", "find_all", "(", "a_str", ",", "sub", ")", ":", "\n", "            ", "start", "=", "0", "\n", "while", "True", ":", "\n", "                ", "start", "=", "a_str", ".", "find", "(", "sub", ",", "start", ")", "\n", "if", "start", "==", "-", "1", ":", "return", "\n", "yield", "start", "\n", "start", "+=", "len", "(", "sub", ")", "\n", "\n", "", "", "for", "m_start", "in", "find_all", "(", "generated_sent", ",", "word_nn", ")", ":", "\n", "            ", "start", "=", "m_start", "\n", "end", "=", "m_start", "+", "len", "(", "word_nn", ")", "\n", "if", "not", "(", "end", "<", "len", "(", "generated_sent", ")", "-", "1", "and", "generated_sent", "[", "end", "+", "1", "]", ".", "isalpha", "(", ")", ")", "or", "(", "start", "!=", "0", "and", "generated_sent", "[", "start", "-", "1", "]", ".", "isalpha", "(", ")", ")", ":", "\n", "                ", "if", "word_nn", "not", "in", "exact_count", ":", "\n", "                    ", "exact_count", "[", "word_nn", "]", "=", "0", "\n", "", "exact_count", "[", "word_nn", "]", "+=", "1", "\n", "", "if", "not", "(", "end", "<", "len", "(", "generated_sent", ")", "-", "1", "and", "generated_sent", "[", "end", "+", "1", "]", "!=", "' '", "and", "start", "!=", "0", "and", "generated_sent", "[", "start", "-", "1", "]", "!=", "' '", ")", ":", "\n", "                ", "if", "word_nn", "not", "in", "topic_l2_word_d2_count_t", ":", "\n", "                    ", "topic_l2_word_d2_count_t", "[", "word_nn", "]", "=", "0", "\n", "", "topic_l2_word_d2_count_t", "[", "word_nn", "]", "+=", "1", "\n", "", "", "return", "generated_sent", "\n", "\n", "", "num_selected", "=", "len", "(", "selected_topic_idx", ")", "\n", "top_k", "=", "top_index_im", ".", "size", "(", "0", ")", "\n", "topic_l2_word_d2_count", "=", "[", "{", "}", "for", "t", "in", "range", "(", "num_selected", ")", "]", "\n", "exact_count", "=", "[", "{", "}", "for", "t", "in", "range", "(", "num_selected", ")", "]", "\n", "for", "t", "in", "range", "(", "num_selected", ")", ":", "\n", "        ", "topic_idx", "=", "selected_topic_idx", "[", "t", "]", "\n", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "            ", "word_nn", "=", "idx2word_freq", "[", "top_index_im", "[", "k", ",", "topic_idx", "]", ".", "item", "(", ")", "]", "[", "0", "]", "\n", "generated_sent", "=", "highlight_words", "(", "word_nn", ",", "generated_sent", ",", "topic_l2_word_d2_count", "[", "t", "]", ",", "exact_count", "[", "t", "]", ")", "\n", "\n", "", "", "num_token_hit", "=", "[", "0", ",", "0", "]", "\n", "num_word_type_hit", "=", "[", "0", ",", "0", "]", "\n", "num_topic_hit", "=", "[", "0", ",", "0", "]", "\n", "for", "t", "in", "range", "(", "num_selected", ")", ":", "\n", "        ", "if", "len", "(", "topic_l2_word_d2_count", "[", "t", "]", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "topic_idx", "=", "selected_topic_idx", "[", "t", "]", "\n", "num_topic_hit", "[", "0", "]", "+=", "1", "\n", "for", "each", "in", "topic_l2_word_d2_count", "[", "t", "]", ":", "\n", "            ", "num_word_type_hit", "[", "0", "]", "+=", "1", "\n", "num_token_hit", "[", "0", "]", "+=", "topic_l2_word_d2_count", "[", "t", "]", "[", "each", "]", "\n", "\n", "", "", "for", "t", "in", "range", "(", "num_selected", ")", ":", "\n", "        ", "if", "len", "(", "exact_count", "[", "t", "]", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "topic_idx", "=", "selected_topic_idx", "[", "t", "]", "\n", "num_topic_hit", "[", "1", "]", "+=", "1", "\n", "for", "each", "in", "exact_count", "[", "t", "]", ":", "\n", "            ", "num_word_type_hit", "[", "1", "]", "+=", "1", "\n", "num_token_hit", "[", "1", "]", "+=", "exact_count", "[", "t", "]", "[", "each", "]", "\n", "\n", "", "", "return", "num_token_hit", ",", "num_word_type_hit", ",", "num_topic_hit", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.perplexity": [[65, 74], ["torch.tensor().new_full", "torch.cat", "torch.cat", "model", "next", "model.parameters", "torch.tensor"], "function", ["None"], ["", "def", "perplexity", "(", "model", ",", "context", ",", "generated_sent", ")", ":", "\n", "    ", "feature", "=", "context", "\n", "feature_generated", "=", "generated_sent", "\n", "device", "=", "next", "(", "model", ".", "parameters", "(", ")", ")", ".", "device", "\n", "feature_empty", "=", "torch", ".", "tensor", "(", "(", ")", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", ".", "new_full", "(", "(", "feature", ".", "shape", "[", "0", "]", ",", "feature", ".", "shape", "[", "1", "]", ")", ",", "-", "100", ")", "\n", "feature", "=", "torch", ".", "cat", "(", "(", "feature", ",", "feature_generated", ")", ",", "dim", "=", "1", ")", "\n", "feature_generated", "=", "torch", ".", "cat", "(", "(", "feature_empty", ",", "feature_generated", ")", ",", "dim", "=", "1", ")", "\n", "outputs_GPT2LMHeadModel", "=", "model", "(", "feature", ",", "labels", "=", "feature_generated", ")", "\n", "return", "outputs_GPT2LMHeadModel", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.eval_using_BLEU": [[75, 94], ["spacy_nlp", "sys.stdout.flush", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.sentence_bleu", "nltk.translate.bleu_score.sentence_bleu", "len", "len", "gen_list.append"], "function", ["None"], ["", "def", "eval_using_BLEU", "(", "generated_sentence", ",", "word_raw_list_i_j", ",", "word_raw_rest_list_i_j", ",", "spacy_nlp", ")", ":", "\n", "    ", "if", "len", "(", "word_raw_list_i_j", ")", "<=", "1", ":", "\n", "        ", "return", "-", "1", ",", "-", "1", ",", "[", "]", "\n", "", "future_window_size", "=", "25", "\n", "if", "len", "(", "word_raw_rest_list_i_j", ")", "<=", "future_window_size", "+", "1", ":", "\n", "        ", "return", "-", "1", ",", "-", "1", ",", "[", "]", "\n", "\n", "", "doc", "=", "spacy_nlp", "(", "generated_sentence", ")", "\n", "gen_list", "=", "[", "]", "\n", "for", "tok", "in", "doc", ":", "\n", "        ", "gen_list", ".", "append", "(", "tok", ".", "text", ")", "\n", "#print(word_raw_list_i_j[:-1])", "\n", "#print(word_raw_rest_list_i_j[1:future_window_size + 1])", "\n", "#print(gen_list)", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "cc", "=", "nltk", ".", "translate", ".", "bleu_score", ".", "SmoothingFunction", "(", ")", "\n", "context_BLEU", "=", "nltk", ".", "translate", ".", "bleu_score", ".", "sentence_bleu", "(", "[", "word_raw_list_i_j", "[", ":", "-", "1", "]", "]", ",", "gen_list", ",", "weights", "=", "(", "0.5", ",", "0.5", ")", ",", "smoothing_function", "=", "cc", ".", "method3", ")", "\n", "BLEU", "=", "nltk", ".", "translate", ".", "bleu_score", ".", "sentence_bleu", "(", "[", "word_raw_rest_list_i_j", "[", "1", ":", "future_window_size", "+", "1", "]", "]", ",", "gen_list", ",", "weights", "=", "(", "0.5", ",", "0.5", ")", ",", "smoothing_function", "=", "cc", ".", "method3", ")", "\n", "return", "BLEU", ",", "context_BLEU", ",", "gen_list", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.show_samples_on_paper.top_words": [[21, 25], ["[].split", "out_str[].replace", "input_text.tolist", "out_str.index"], "function", ["None"], ["def", "top_words", "(", "input_text", ")", ":", "\n", "    ", "input_list", "=", "input_text", ".", "tolist", "(", ")", "[", "0", "]", ".", "split", "(", "','", ")", "\n", "out_str", "=", "','", ".", "join", "(", "input_list", "[", ":", "M", "]", ")", "\n", "return", "out_str", "[", "out_str", ".", "index", "(", "': '", ")", "+", "2", ":", "]", ".", "replace", "(", "'%'", ",", "'\\%'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.pplm.__init__": [[649, 693], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "numpy.random.seed", "torch.load", "torch.load", "torch.load", "torch.load", "gpt2_model.configuration_gpt2.GPT2Config.from_pretrained", "gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.from_pretrained", "run_pplm_fine_tuned.pplm.model.to", "run_pplm_fine_tuned.pplm.model.eval", "gpt2_model.tokenization_gpt2.GPT2Tokenizer.from_pretrained", "run_pplm_fine_tuned.pplm.model.parameters", "print", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "seed", ",", "\n", "pretrained_model", ",", "\n", "checkpoint", ",", "\n", "device", ",", "\n", "discrim", "=", "None", ",", "\n", "discrim_weights", "=", "None", ",", "\n", "discrim_meta", "=", "None", ",", "\n", "class_label", "=", "-", "1", ",", "\n", ")", ":", "\n", "\n", "\n", "# load data", "\n", "        ", "self", ".", "device", "=", "device", "\n", "self", ".", "class_label", "=", "class_label", "\n", "self", ".", "discrim", "=", "discrim", "\n", "self", ".", "discrim_weights", "=", "discrim_weights", "\n", "self", ".", "discrim_meta", "=", "discrim_meta", "\n", "\n", "\n", "# set Random seed", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "\n", "# self.device = device", "\n", "# # load model", "\n", "encoder_state_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "checkpoint", ",", "'encoder.pt'", ")", ",", "map_location", "=", "device", ")", "\n", "gpt2_config", "=", "GPT2Config", ".", "from_pretrained", "(", "pretrained_model", ",", "output_hidden_states", "=", "True", ")", "\n", "gpt2_config", ".", "word_emb_dim", "=", "300", "\n", "self", ".", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "pretrained_model", ",", "state_dict", "=", "encoder_state_dict", ",", "config", "=", "gpt2_config", ")", "\n", "\n", "#self.model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)", "\n", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "# # load tokenizer", "\n", "self", ".", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "pretrained_model", ")", "\n", "\n", "# # Freeze GPT-2 weights", "\n", "for", "param", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "", "print", "(", "\"pplm model: \"", ",", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.pplm.run_pplm_example": [[697, 791], ["run_pplm_fine_tuned.full_text_generation", "enumerate", "run_pplm_fine_tuned.pplm.tokenizer.encode", "run_pplm_fine_tuned.pplm.tokenizer.encode", "generated_texts.tolist", "run_pplm_fine_tuned.pplm.tokenizer.decode", "texts.append", "print", "input"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.full_text_generation", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "run_pplm_example", "(", "self", ",", "\n", "cond_text", "=", "\"\"", ",", "\n", "uncond", "=", "False", ",", "\n", "num_samples", "=", "1", ",", "\n", "bag_of_words", "=", "None", ",", "\n", "length", "=", "100", ",", "\n", "stepsize", "=", "0.02", ",", "\n", "temperature", "=", "1.0", ",", "\n", "top_k", "=", "10", ",", "\n", "sample", "=", "False", ",", "\n", "num_iterations", "=", "1", ",", "\n", "grad_length", "=", "10000", ",", "\n", "horizon_length", "=", "1", ",", "\n", "window_length", "=", "0", ",", "\n", "decay", "=", "False", ",", "\n", "gamma", "=", "1.5", ",", "\n", "gm_scale", "=", "0.9", ",", "\n", "kl_scale", "=", "0.01", ",", "\n", "colorama", "=", "False", ",", "\n", ")", ":", "\n", "# figure out conditioning text", "\n", "        ", "if", "uncond", ":", "\n", "            ", "tokenized_cond_text", "=", "self", ".", "tokenizer", ".", "encode", "(", "[", "self", ".", "tokenizer", ".", "bos_token", "]", ")", "\n", "", "else", ":", "\n", "            ", "raw_text", "=", "cond_text", "\n", "while", "not", "raw_text", ":", "\n", "                ", "print", "(", "\"Did you forget to add `--cond_text`? \"", ")", "\n", "raw_text", "=", "input", "(", "\"Model prompt >>> \"", ")", "\n", "", "tokenized_cond_text", "=", "self", ".", "tokenizer", ".", "encode", "(", "self", ".", "tokenizer", ".", "bos_token", "+", "raw_text", ")", "\n", "\n", "\n", "\n", "# full_text_generation returns:", "\n", "# pert_gen_tok_texts, discrim_losses, losses_in_time, generated_text", "\n", "", "pert_gen_tok_texts", ",", "_", ",", "_", ",", "generated_texts", ",", "perplexity", "=", "full_text_generation", "(", "\n", "model", "=", "self", ".", "model", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "context", "=", "tokenized_cond_text", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", "num_samples", "=", "num_samples", ",", "\n", "bag_of_words", "=", "bag_of_words", ",", "\n", "discrim", "=", "self", ".", "discrim", ",", "\n", "class_label", "=", "self", ".", "class_label", ",", "\n", "length", "=", "length", ",", "\n", "stepsize", "=", "stepsize", ",", "\n", "temperature", "=", "temperature", ",", "\n", "top_k", "=", "top_k", ",", "\n", "sample", "=", "sample", ",", "\n", "num_iterations", "=", "num_iterations", ",", "\n", "grad_length", "=", "grad_length", ",", "\n", "horizon_length", "=", "horizon_length", ",", "\n", "window_length", "=", "window_length", ",", "\n", "decay", "=", "decay", ",", "\n", "gamma", "=", "gamma", ",", "\n", "gm_scale", "=", "gm_scale", ",", "\n", "kl_scale", "=", "kl_scale", ",", "\n", ")", "\n", "\n", "# # untokenize unperturbed text", "\n", "# pert_gen_text = self.tokenizer.decode(generated_texts[0].tolist()[0])", "\n", "\n", "# bow_word_ids = set()", "\n", "# if bag_of_words and colorama:", "\n", "#     bow_indices = get_bag_of_words_indices(bag_of_words, self.tokenizer)", "\n", "#     for single_bow_list in bow_indices:", "\n", "#         # filtering all words in the list composed of more than 1 token", "\n", "#         filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))", "\n", "#         # w[0] because we are sure w has only 1 item because previous fitler", "\n", "#         bow_word_ids.update(w[0] for w in filtered)", "\n", "\n", "# # # iterate through the perturbed texts", "\n", "texts", "=", "[", "]", "\n", "for", "i", ",", "generated_text", "in", "enumerate", "(", "generated_texts", ".", "tolist", "(", ")", ")", ":", "\n", "            ", "gen_text", "=", "self", ".", "tokenizer", ".", "decode", "(", "generated_text", ")", "\n", "texts", ".", "append", "(", "gen_text", ")", "\n", "#     try:", "\n", "#         # untokenize unperturbed text", "\n", "#         if colorama:", "\n", "#             import colorama", "\n", "\n", "#             pert_gen_text = \"\"", "\n", "#             for word_id in generated_text.tolist()[0]:", "\n", "#                 if word_id in bow_word_ids:", "\n", "#                     pert_gen_text += \"{}{}{}\".format(", "\n", "#                         colorama.Fore.RED, self.tokenizer.decode([word_id]), colorama.Style.RESET_ALL", "\n", "#                     )", "\n", "#                 else:", "\n", "#                     pert_gen_text += self.tokenizer.decode([word_id])", "\n", "#         else:", "\n", "#             pert_gen_text = self.tokenizer.decode(generated_text.tolist()[0])", "\n", "#     except Exception as exc:", "\n", "#         print(\"Ignoring error while generating perturbed text:\", exc)", "\n", "\n", "", "return", "texts", ",", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.to_var": [[86, 92], ["torch.autograd.Variable", "torch.cuda.is_available", "torch.cuda.is_available", "x.to.cuda", "x.to.to"], "function", ["None"], ["def", "to_var", "(", "x", ",", "requires_grad", "=", "False", ",", "volatile", "=", "False", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "device", "==", "\"cuda\"", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "elif", "device", "!=", "\"cuda\"", ":", "\n", "        ", "x", "=", "x", ".", "to", "(", "device", ")", "\n", "", "return", "Variable", "(", "x", ",", "requires_grad", "=", "requires_grad", ",", "volatile", "=", "volatile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.top_k_filter": [[94, 108], ["values[].view().expand_as", "torch.where", "torch.where", "torch.topk", "torch.topk", "torch.where", "torch.where", "values[].view", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "function", ["None"], ["", "def", "top_k_filter", "(", "logits", ",", "k", ",", "probs", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Masks everything but the k top entries as -infinity (1e10).\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n    sum of the denominator.\n    \"\"\"", "\n", "if", "k", "==", "0", ":", "\n", "        ", "return", "logits", "\n", "", "else", ":", "\n", "        ", "values", "=", "torch", ".", "topk", "(", "logits", ",", "k", ")", "[", "0", "]", "\n", "batch_mins", "=", "values", "[", ":", ",", "-", "1", "]", ".", "view", "(", "-", "1", ",", "1", ")", ".", "expand_as", "(", "logits", ")", "\n", "if", "probs", ":", "\n", "            ", "return", "torch", ".", "where", "(", "logits", "<", "batch_mins", ",", "torch", ".", "ones_like", "(", "logits", ")", "*", "0.0", ",", "logits", ")", "\n", "", "return", "torch", ".", "where", "(", "logits", "<", "batch_mins", ",", "torch", ".", "ones_like", "(", "logits", ")", "*", "-", "BIG_CONST", ",", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.perturb_past": [[110, 263], ["torch.enable_grad", "torch.enable_grad", "range", "list", "numpy.zeros().astype", "torch.ones", "torch.ones", "ones_mask.permute.permute", "torch.cat().to", "torch.cat().to", "torch.ones_like().to", "torch.ones_like().to", "list", "model", "torch.softmax", "loss_per_iter.append", "loss.backward", "list", "run_pplm_fine_tuned.to_var", "map", "torch.arange", "torch.arange", "tuple", "tuple", "ones_mask.permute.permute", "run_pplm_fine_tuned.to_var", "map", "torch.sum().detach", "torch.sum().detach", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.unsqueeze", "torch.unsqueeze", "model.resize_token_embeddings", "range", "classifier", "torch.tensor", "torch.tensor", "torch.nn.CrossEntropyLoss.", "loss_list.append", "torch.softmax", "loss.data.cpu().numpy", "map", "p_.grad.data.zero_", "new_past.append", "torch.from_numpy", "torch.from_numpy", "numpy.zeros", "tuple", "tuple", "tuple", "tuple", "torch.cat", "torch.cat", "torch.ones_like", "torch.ones_like", "torch.from_numpy", "torch.from_numpy", "torch.mm", "torch.mm", "loss_list.append", "torch.matmul", "torch.matmul", "model", "correction.detach", "torch.max", "torch.max", "enumerate", "p_.detach", "torch.sum", "torch.sum", "torch.t", "torch.t", "torch.log", "torch.log", "torch.sum", "torch.sum", "loss.data.cpu", "torch.norm", "torch.norm", "enumerate", "torch.norm", "torch.norm", "enumerate", "torch.zeros", "torch.zeros", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.to_var", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.to_var", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.modeling_utils.PreTrainedModel.resize_token_embeddings"], ["", "", "def", "perturb_past", "(", "\n", "past", ",", "\n", "model", ",", "\n", "last", ",", "\n", "unpert_past", "=", "None", ",", "\n", "unpert_logits", "=", "None", ",", "\n", "accumulated_hidden", "=", "None", ",", "\n", "grad_norms", "=", "None", ",", "\n", "stepsize", "=", "0.01", ",", "\n", "one_hot_bows_vectors", "=", "None", ",", "\n", "classifier", "=", "None", ",", "\n", "class_label", "=", "None", ",", "\n", "loss_type", "=", "0", ",", "\n", "num_iterations", "=", "3", ",", "\n", "horizon_length", "=", "1", ",", "\n", "window_length", "=", "0", ",", "\n", "decay", "=", "False", ",", "\n", "gamma", "=", "1.5", ",", "\n", "kl_scale", "=", "0.01", ",", "\n", "device", "=", "\"cuda\"", ",", "\n", ")", ":", "\n", "    ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "# Generate inital perturbed past", "\n", "        ", "grad_accumulator", "=", "[", "(", "np", ".", "zeros", "(", "p", ".", "shape", ")", ".", "astype", "(", "\"float32\"", ")", ")", "for", "p", "in", "past", "]", "\n", "\n", "\n", "if", "accumulated_hidden", "is", "None", ":", "\n", "            ", "accumulated_hidden", "=", "0", "\n", "\n", "", "if", "decay", ":", "\n", "            ", "decay_mask", "=", "torch", ".", "arange", "(", "0.0", ",", "1.0", "+", "SMALL_CONST", ",", "1.0", "/", "(", "window_length", ")", ")", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "decay_mask", "=", "1.0", "\n", "\n", "# TODO fix this comment (SUMANTH)", "\n", "# Generate a mask is gradient perturbated is based on a past window", "\n", "", "_", ",", "_", ",", "_", ",", "curr_length", ",", "_", "=", "past", "[", "0", "]", ".", "shape", "\n", "\n", "if", "curr_length", ">", "window_length", "and", "window_length", ">", "0", ":", "\n", "            ", "ones_key_val_shape", "=", "tuple", "(", "past", "[", "0", "]", ".", "shape", "[", ":", "-", "2", "]", ")", "+", "tuple", "(", "[", "window_length", "]", ")", "+", "tuple", "(", "past", "[", "0", "]", ".", "shape", "[", "-", "1", ":", "]", ")", "\n", "\n", "zeros_key_val_shape", "=", "(", "\n", "tuple", "(", "past", "[", "0", "]", ".", "shape", "[", ":", "-", "2", "]", ")", "+", "tuple", "(", "[", "curr_length", "-", "window_length", "]", ")", "+", "tuple", "(", "past", "[", "0", "]", ".", "shape", "[", "-", "1", ":", "]", ")", "\n", ")", "\n", "\n", "ones_mask", "=", "torch", ".", "ones", "(", "ones_key_val_shape", ")", "\n", "ones_mask", "=", "decay_mask", "*", "ones_mask", ".", "permute", "(", "0", ",", "1", ",", "2", ",", "4", ",", "3", ")", "\n", "ones_mask", "=", "ones_mask", ".", "permute", "(", "0", ",", "1", ",", "2", ",", "4", ",", "3", ")", "\n", "\n", "window_mask", "=", "torch", ".", "cat", "(", "(", "ones_mask", ",", "torch", ".", "zeros", "(", "zeros_key_val_shape", ")", ")", ",", "dim", "=", "-", "2", ")", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "            ", "window_mask", "=", "torch", ".", "ones_like", "(", "past", "[", "0", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "# accumulate perturbations for num_iterations", "\n", "", "loss_per_iter", "=", "[", "]", "\n", "new_accumulated_hidden", "=", "None", "\n", "for", "i", "in", "range", "(", "num_iterations", ")", ":", "\n", "# print(\"Iteration \", i + 1)", "\n", "            ", "curr_perturbation", "=", "[", "\n", "to_var", "(", "torch", ".", "from_numpy", "(", "p_", ")", ",", "requires_grad", "=", "True", ",", "device", "=", "device", ")", "for", "p_", "in", "grad_accumulator", "\n", "]", "\n", "\n", "# Compute hidden using perturbed past", "\n", "perturbed_past", "=", "list", "(", "map", "(", "add", ",", "past", ",", "curr_perturbation", ")", ")", "\n", "_", ",", "_", ",", "_", ",", "curr_length", ",", "_", "=", "curr_perturbation", "[", "0", "]", ".", "shape", "\n", "all_logits", ",", "_", ",", "all_hidden", "=", "model", "(", "last", ",", "past", "=", "perturbed_past", ")", "\n", "hidden", "=", "all_hidden", "[", "-", "1", "]", "\n", "new_accumulated_hidden", "=", "accumulated_hidden", "+", "torch", ".", "sum", "(", "hidden", ",", "dim", "=", "1", ")", ".", "detach", "(", ")", "\n", "# TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)", "\n", "logits", "=", "all_logits", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss_list", "=", "[", "]", "\n", "if", "loss_type", "==", "PPLM_BOW", "or", "loss_type", "==", "PPLM_BOW_DISCRIM", ":", "\n", "                ", "for", "one_hot_bow", "in", "one_hot_bows_vectors", ":", "\n", "                    ", "bow_logits", "=", "torch", ".", "mm", "(", "probs", ",", "torch", ".", "t", "(", "one_hot_bow", ")", ")", "\n", "bow_loss", "=", "-", "torch", ".", "log", "(", "torch", ".", "sum", "(", "bow_logits", ")", ")", "\n", "loss", "+=", "bow_loss", "\n", "loss_list", ".", "append", "(", "bow_loss", ")", "\n", "# print(\" pplm_bow_loss:\", loss.data.cpu().numpy())", "\n", "\n", "", "", "if", "loss_type", "==", "2", "or", "loss_type", "==", "3", ":", "\n", "                ", "ce_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "# TODO why we need to do this assignment and not just using unpert_past? (Sumanth)", "\n", "curr_unpert_past", "=", "unpert_past", "\n", "curr_probs", "=", "torch", ".", "unsqueeze", "(", "probs", ",", "dim", "=", "1", ")", "\n", "wte", "=", "model", ".", "resize_token_embeddings", "(", ")", "\n", "for", "_", "in", "range", "(", "horizon_length", ")", ":", "\n", "                    ", "inputs_embeds", "=", "torch", ".", "matmul", "(", "curr_probs", ",", "wte", ".", "weight", ".", "data", ")", "\n", "_", ",", "curr_unpert_past", ",", "curr_all_hidden", "=", "model", "(", "past", "=", "curr_unpert_past", ",", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "curr_hidden", "=", "curr_all_hidden", "[", "-", "1", "]", "\n", "new_accumulated_hidden", "=", "new_accumulated_hidden", "+", "torch", ".", "sum", "(", "curr_hidden", ",", "dim", "=", "1", ")", "\n", "\n", "", "prediction", "=", "classifier", "(", "new_accumulated_hidden", "/", "(", "curr_length", "+", "1", "+", "horizon_length", ")", ")", "\n", "\n", "label", "=", "torch", ".", "tensor", "(", "prediction", ".", "shape", "[", "0", "]", "*", "[", "class_label", "]", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "discrim_loss", "=", "ce_loss", "(", "prediction", ",", "label", ")", "\n", "# print(\" pplm_discrim_loss:\", discrim_loss.data.cpu().numpy())", "\n", "loss", "+=", "discrim_loss", "\n", "loss_list", ".", "append", "(", "discrim_loss", ")", "\n", "\n", "", "kl_loss", "=", "0.0", "\n", "if", "kl_scale", ">", "0.0", ":", "\n", "                ", "unpert_probs", "=", "F", ".", "softmax", "(", "unpert_logits", "[", ":", ",", "-", "1", ",", ":", "]", ",", "dim", "=", "-", "1", ")", "\n", "unpert_probs", "=", "unpert_probs", "+", "SMALL_CONST", "*", "(", "unpert_probs", "<=", "SMALL_CONST", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ".", "detach", "(", ")", "\n", "correction", "=", "SMALL_CONST", "*", "(", "probs", "<=", "SMALL_CONST", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ".", "detach", "(", ")", "\n", "corrected_probs", "=", "probs", "+", "correction", ".", "detach", "(", ")", "\n", "kl_loss", "=", "kl_scale", "*", "(", "(", "corrected_probs", "*", "(", "corrected_probs", "/", "unpert_probs", ")", ".", "log", "(", ")", ")", ".", "sum", "(", ")", ")", "\n", "# print(\" kl_loss\", kl_loss.data.cpu().numpy())", "\n", "loss", "+=", "kl_loss", "\n", "\n", "", "loss_per_iter", ".", "append", "(", "loss", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "# print(\" pplm_loss\", (loss - kl_loss).data.cpu().numpy())", "\n", "\n", "# compute gradients", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# calculate gradient norms", "\n", "if", "grad_norms", "is", "not", "None", "and", "loss_type", "==", "PPLM_BOW", ":", "\n", "                ", "grad_norms", "=", "[", "\n", "torch", ".", "max", "(", "grad_norms", "[", "index", "]", ",", "torch", ".", "norm", "(", "p_", ".", "grad", "*", "window_mask", ")", ")", "\n", "for", "index", ",", "p_", "in", "enumerate", "(", "curr_perturbation", ")", "\n", "]", "\n", "", "else", ":", "\n", "                ", "grad_norms", "=", "[", "\n", "(", "torch", ".", "norm", "(", "p_", ".", "grad", "*", "window_mask", ")", "+", "SMALL_CONST", ")", "for", "index", ",", "p_", "in", "enumerate", "(", "curr_perturbation", ")", "\n", "]", "\n", "\n", "# normalize gradients", "\n", "", "grad", "=", "[", "\n", "-", "stepsize", "*", "(", "p_", ".", "grad", "*", "window_mask", "/", "grad_norms", "[", "index", "]", "**", "gamma", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "index", ",", "p_", "in", "enumerate", "(", "curr_perturbation", ")", "\n", "]", "\n", "\n", "# accumulate gradient", "\n", "grad_accumulator", "=", "list", "(", "map", "(", "add", ",", "grad", ",", "grad_accumulator", ")", ")", "\n", "\n", "# reset gradients, just to make sure", "\n", "for", "p_", "in", "curr_perturbation", ":", "\n", "                ", "p_", ".", "grad", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "# removing past from the graph", "\n", "", "new_past", "=", "[", "]", "\n", "for", "p_", "in", "past", ":", "\n", "                ", "new_past", ".", "append", "(", "p_", ".", "detach", "(", ")", ")", "\n", "", "past", "=", "new_past", "\n", "\n", "# apply the accumulated perturbations to the past", "\n", "", "grad_accumulator", "=", "[", "to_var", "(", "torch", ".", "from_numpy", "(", "p_", ")", ",", "requires_grad", "=", "True", ",", "device", "=", "device", ")", "for", "p_", "in", "grad_accumulator", "]", "\n", "pert_past", "=", "list", "(", "map", "(", "add", ",", "past", ",", "grad_accumulator", ")", ")", "\n", "\n", "return", "pert_past", ",", "new_accumulated_hidden", ",", "grad_norms", ",", "loss_per_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.get_classifier": [[265, 304], ["pplm_classification_head.ClassificationHead().to", "ClassificationHead().to.load_state_dict", "ClassificationHead().to.eval", "isinstance", "gpt2_model.file_utils.cached_path", "torch.load", "torch.load", "isinstance", "pplm_classification_head.ClassificationHead", "ValueError", "print", "print", "print", "set", "print", "print", "print", "params[].values"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.file_utils.cached_path", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "", "def", "get_classifier", "(", "\n", "name", ":", "Optional", "[", "str", "]", ",", "class_label", ":", "Union", "[", "str", ",", "int", "]", ",", "device", ":", "str", "\n", ")", "->", "Tuple", "[", "Optional", "[", "ClassificationHead", "]", ",", "Optional", "[", "int", "]", "]", ":", "\n", "    ", "if", "name", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "", "params", "=", "DISCRIMINATOR_MODELS_PARAMS", "[", "name", "]", "\n", "classifier", "=", "ClassificationHead", "(", "class_size", "=", "params", "[", "\"class_size\"", "]", ",", "embed_size", "=", "params", "[", "\"embed_size\"", "]", ")", ".", "to", "(", "device", ")", "\n", "if", "\"url\"", "in", "params", ":", "\n", "        ", "resolved_archive_file", "=", "cached_path", "(", "params", "[", "\"url\"", "]", ")", "\n", "", "elif", "\"path\"", "in", "params", ":", "\n", "        ", "resolved_archive_file", "=", "params", "[", "\"path\"", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Either url or path have to be specified \"", "\"in the discriminator model parameters\"", ")", "\n", "", "classifier", ".", "load_state_dict", "(", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "device", ")", ")", "\n", "classifier", ".", "eval", "(", ")", "\n", "\n", "if", "isinstance", "(", "class_label", ",", "str", ")", ":", "\n", "        ", "if", "class_label", "in", "params", "[", "\"class_vocab\"", "]", ":", "\n", "            ", "label_id", "=", "params", "[", "\"class_vocab\"", "]", "[", "class_label", "]", "\n", "", "else", ":", "\n", "            ", "label_id", "=", "params", "[", "\"default_class\"", "]", "\n", "print", "(", "\"class_label {} not in class_vocab\"", ".", "format", "(", "class_label", ")", ")", "\n", "print", "(", "\"available values are: {}\"", ".", "format", "(", "params", "[", "\"class_vocab\"", "]", ")", ")", "\n", "print", "(", "\"using default class {}\"", ".", "format", "(", "label_id", ")", ")", "\n", "\n", "", "", "elif", "isinstance", "(", "class_label", ",", "int", ")", ":", "\n", "        ", "if", "class_label", "in", "set", "(", "params", "[", "\"class_vocab\"", "]", ".", "values", "(", ")", ")", ":", "\n", "            ", "label_id", "=", "class_label", "\n", "", "else", ":", "\n", "            ", "label_id", "=", "params", "[", "\"default_class\"", "]", "\n", "print", "(", "\"class_label {} not in class_vocab\"", ".", "format", "(", "class_label", ")", ")", "\n", "print", "(", "\"available values are: {}\"", ".", "format", "(", "params", "[", "\"class_vocab\"", "]", ")", ")", "\n", "print", "(", "\"using default class {}\"", ".", "format", "(", "label_id", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "label_id", "=", "params", "[", "\"default_class\"", "]", "\n", "\n", "", "return", "classifier", ",", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.get_bag_of_words_indices": [[306, 312], ["bow_indices.append", "tokenizer.encode", "word.strip"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "get_bag_of_words_indices", "(", "bag_of_words", ":", "List", "[", "str", "]", ",", "tokenizer", ")", "->", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", ":", "\n", "    ", "bow_indices", "=", "[", "]", "\n", "\n", "bow_indices", ".", "append", "(", "[", "tokenizer", ".", "encode", "(", "word", ".", "strip", "(", ")", ",", "add_prefix_space", "=", "True", ",", "add_special_tokens", "=", "False", ")", "for", "word", "in", "bag_of_words", "]", ")", "\n", "\n", "return", "bow_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.build_bows_one_hot_vectors": [[314, 332], ["list", "torch.tensor().to", "torch.tensor().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to.scatter_", "one_hot_bows_vectors.append", "filter", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "len"], "function", ["None"], ["", "def", "build_bows_one_hot_vectors", "(", "bow_indices", ",", "tokenizer", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "    ", "if", "bow_indices", "is", "None", ":", "\n", "        ", "return", "None", "\n", "\n", "", "one_hot_bows_vectors", "=", "[", "]", "\n", "\n", "for", "single_bow", "in", "bow_indices", ":", "\n", "        ", "single_bow", "=", "list", "(", "filter", "(", "lambda", "x", ":", "len", "(", "x", ")", "<=", "1", ",", "single_bow", ")", ")", "\n", "single_bow", "=", "torch", ".", "tensor", "(", "single_bow", ")", ".", "to", "(", "device", ")", "\n", "num_words", "=", "single_bow", ".", "shape", "[", "0", "]", "\n", "#if num_words == 0:", "\n", "#    single_bow = [[318]]", "\n", "#    single_bow = torch.tensor(single_bow).to(device)", "\n", "#    num_words = 1", "\n", "one_hot_bow", "=", "torch", ".", "zeros", "(", "num_words", ",", "tokenizer", ".", "vocab_size", ")", ".", "to", "(", "device", ")", "\n", "one_hot_bow", ".", "scatter_", "(", "1", ",", "single_bow", ",", "1", ")", "\n", "one_hot_bows_vectors", ".", "append", "(", "one_hot_bow", ")", "\n", "", "return", "one_hot_bows_vectors", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.full_text_generation": [[334, 417], ["run_pplm_fine_tuned.get_classifier", "torch.tensor", "torch.tensor", "run_pplm_fine_tuned.generate_text_pplm", "run_pplm_fine_tuned.get_bag_of_words_indices", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.tensor.unsqueeze().expand", "Exception", "torch.tensor.unsqueeze"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.get_classifier", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.generate_text_pplm", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.get_bag_of_words_indices"], ["", "def", "full_text_generation", "(", "\n", "model", ",", "\n", "tokenizer", ",", "\n", "context", "=", "None", ",", "\n", "num_samples", "=", "1", ",", "\n", "device", "=", "\"cuda\"", ",", "\n", "bag_of_words", "=", "None", ",", "\n", "discrim", "=", "None", ",", "\n", "class_label", "=", "None", ",", "\n", "length", "=", "100", ",", "\n", "stepsize", "=", "0.02", ",", "\n", "temperature", "=", "1.0", ",", "\n", "top_k", "=", "10", ",", "\n", "sample", "=", "False", ",", "\n", "num_iterations", "=", "3", ",", "\n", "grad_length", "=", "10000", ",", "\n", "horizon_length", "=", "1", ",", "\n", "window_length", "=", "0", ",", "\n", "decay", "=", "False", ",", "\n", "gamma", "=", "1.5", ",", "\n", "gm_scale", "=", "0.9", ",", "\n", "kl_scale", "=", "0.01", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "classifier", ",", "class_id", "=", "get_classifier", "(", "discrim", ",", "class_label", ",", "device", ")", "\n", "\n", "bow_indices", "=", "[", "]", "\n", "if", "bag_of_words", ":", "\n", "        ", "bow_indices", "=", "get_bag_of_words_indices", "(", "bag_of_words", ",", "tokenizer", ")", "\n", "\n", "\n", "", "if", "bag_of_words", "and", "classifier", ":", "\n", "# print(\"Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.\")", "\n", "        ", "loss_type", "=", "PPLM_BOW_DISCRIM", "\n", "\n", "", "elif", "bag_of_words", ":", "\n", "        ", "loss_type", "=", "PPLM_BOW", "\n", "# print(\"Using PPLM-BoW\")", "\n", "\n", "", "elif", "classifier", "is", "not", "None", ":", "\n", "        ", "loss_type", "=", "PPLM_DISCRIM", "\n", "# print(\"Using PPLM-Discrim\")", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Specify either a bag of words or a discriminator\"", ")", "\n", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "pert_gen_tok_texts", "=", "[", "]", "\n", "discrim_losses", "=", "[", "]", "\n", "losses_in_time", "=", "[", "]", "\n", "# generated_texts = []", "\n", "\n", "\n", "context_t", "=", "torch", ".", "tensor", "(", "context", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "\n", "pert_gen_tok_texts", ",", "discrim_losses", ",", "losses_in_time", ",", "generated_texts", ",", "perplexity", "=", "generate_text_pplm", "(", "\n", "model", "=", "model", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "context", "=", "context_t", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_samples", ",", "context_t", ".", "shape", "[", "0", "]", ")", ",", "\n", "device", "=", "device", ",", "\n", "perturb", "=", "True", ",", "\n", "bow_indices", "=", "bow_indices", ",", "\n", "classifier", "=", "classifier", ",", "\n", "class_label", "=", "class_id", ",", "\n", "loss_type", "=", "loss_type", ",", "\n", "length", "=", "length", ",", "\n", "stepsize", "=", "stepsize", ",", "\n", "temperature", "=", "temperature", ",", "\n", "top_k", "=", "top_k", ",", "\n", "sample", "=", "sample", ",", "\n", "num_iterations", "=", "num_iterations", ",", "\n", "grad_length", "=", "grad_length", ",", "\n", "horizon_length", "=", "horizon_length", ",", "\n", "window_length", "=", "window_length", ",", "\n", "decay", "=", "decay", ",", "\n", "gamma", "=", "gamma", ",", "\n", "gm_scale", "=", "gm_scale", ",", "\n", "kl_scale", "=", "kl_scale", ",", "\n", ")", "\n", "return", "pert_gen_tok_texts", ",", "discrim_losses", ",", "losses_in_time", ",", "generated_texts", ",", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.generate_text_pplm": [[419, 574], ["run_pplm_fine_tuned.build_bows_one_hot_vectors", "torch.ones", "torch.ones", "tqdm.trange", "perplexity.pow.pow", "model", "model", "torch.softmax", "range", "torch.sum", "torch.sum", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "classifier", "torch.tensor", "torch.tensor", "torch.nn.CrossEntropyLoss.", "torch.softmax", "run_pplm_fine_tuned.top_k_filter", "run_pplm_fine_tuned.top_k_filter", "torch.softmax", "torch.multinomial", "torch.multinomial", "torch.topk", "torch.topk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model", "run_pplm_fine_tuned.perturb_past", "loss_in_time.append", "torch.mean", "torch.mean", "torch.sum", "torch.sum", "[].tolist", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.build_bows_one_hot_vectors", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.top_k_filter", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.top_k_filter", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.perturb_past"], ["", "def", "generate_text_pplm", "(", "\n", "model", ",", "\n", "tokenizer", ",", "\n", "context", "=", "None", ",", "\n", "past", "=", "None", ",", "\n", "device", "=", "\"cuda\"", ",", "\n", "perturb", "=", "True", ",", "\n", "bow_indices", "=", "None", ",", "\n", "classifier", "=", "None", ",", "\n", "class_label", "=", "None", ",", "\n", "loss_type", "=", "0", ",", "\n", "length", "=", "100", ",", "\n", "stepsize", "=", "0.02", ",", "\n", "temperature", "=", "1.0", ",", "\n", "top_k", "=", "10", ",", "\n", "sample", "=", "False", ",", "\n", "num_iterations", "=", "3", ",", "\n", "grad_length", "=", "10000", ",", "\n", "horizon_length", "=", "1", ",", "\n", "window_length", "=", "0", ",", "\n", "decay", "=", "False", ",", "\n", "gamma", "=", "1.5", ",", "\n", "gm_scale", "=", "0.9", ",", "\n", "kl_scale", "=", "0.01", ",", "\n", ")", ":", "\n", "    ", "output_so_far", "=", "context", "\n", "\n", "sample_num", "=", "output_so_far", ".", "shape", "[", "0", "]", "\n", "# if context:", "\n", "#     context_t = torch.tensor(context, device=device, dtype=torch.long)", "\n", "#     while len(context_t.shape) < 2:", "\n", "#         context_t = context_t.unsqueeze(0)", "\n", "#     output_so_far = context_t", "\n", "\n", "# collect one hot vectors for bags of words", "\n", "one_hot_bows_vectors", "=", "build_bows_one_hot_vectors", "(", "bow_indices", ",", "tokenizer", ",", "device", ")", "\n", "\n", "grad_norms", "=", "None", "\n", "last", "=", "None", "\n", "unpert_discrim_loss", "=", "0", "\n", "loss_in_time", "=", "[", "]", "\n", "generated", "=", "None", "\n", "generated_prob", "=", "torch", ".", "ones", "(", "sample_num", ")", "\n", "\n", "\n", "for", "i", "in", "trange", "(", "length", ",", "ascii", "=", "True", ")", ":", "\n", "\n", "# Get past/probs for current output, except for last word", "\n", "# Note that GPT takes 2 inputs: past + current_token", "\n", "\n", "# run model forward to obtain unperturbed", "\n", "        ", "if", "past", "is", "None", "and", "output_so_far", "is", "not", "None", ":", "\n", "            ", "last", "=", "output_so_far", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "output_so_far", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "                ", "_", ",", "past", ",", "_", "=", "model", "(", "output_so_far", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "\n", "", "", "unpert_logits", ",", "unpert_past", ",", "unpert_all_hidden", "=", "model", "(", "output_so_far", ")", "\n", "unpert_last_hidden", "=", "unpert_all_hidden", "[", "-", "1", "]", "\n", "\n", "# check if we are abowe grad max length", "\n", "if", "i", ">=", "grad_length", ":", "\n", "            ", "current_stepsize", "=", "stepsize", "*", "0", "\n", "", "else", ":", "\n", "            ", "current_stepsize", "=", "stepsize", "\n", "\n", "# modify the past if necessary", "\n", "", "if", "not", "perturb", "or", "num_iterations", "==", "0", ":", "\n", "            ", "pert_past", "=", "past", "\n", "\n", "", "else", ":", "\n", "            ", "accumulated_hidden", "=", "unpert_last_hidden", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "accumulated_hidden", "=", "torch", ".", "sum", "(", "accumulated_hidden", ",", "dim", "=", "1", ")", "\n", "\n", "if", "past", "is", "not", "None", ":", "\n", "                ", "pert_past", ",", "_", ",", "grad_norms", ",", "loss_this_iter", "=", "perturb_past", "(", "\n", "past", ",", "\n", "model", ",", "\n", "last", ",", "\n", "unpert_past", "=", "unpert_past", ",", "\n", "unpert_logits", "=", "unpert_logits", ",", "\n", "accumulated_hidden", "=", "accumulated_hidden", ",", "\n", "grad_norms", "=", "grad_norms", ",", "\n", "stepsize", "=", "current_stepsize", ",", "\n", "one_hot_bows_vectors", "=", "one_hot_bows_vectors", ",", "\n", "classifier", "=", "classifier", ",", "\n", "class_label", "=", "class_label", ",", "\n", "loss_type", "=", "loss_type", ",", "\n", "num_iterations", "=", "num_iterations", ",", "\n", "horizon_length", "=", "horizon_length", ",", "\n", "window_length", "=", "window_length", ",", "\n", "decay", "=", "decay", ",", "\n", "gamma", "=", "gamma", ",", "\n", "kl_scale", "=", "kl_scale", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "loss_in_time", ".", "append", "(", "loss_this_iter", ")", "\n", "", "else", ":", "\n", "                ", "pert_past", "=", "past", "\n", "\n", "", "", "pert_logits", ",", "past", ",", "pert_all_hidden", "=", "model", "(", "last", ",", "past", "=", "pert_past", ")", "\n", "pert_logits", "=", "pert_logits", "[", ":", ",", "-", "1", ",", ":", "]", "/", "temperature", "# + SMALL_CONST", "\n", "pert_probs", "=", "F", ".", "softmax", "(", "pert_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "classifier", "is", "not", "None", ":", "\n", "            ", "ce_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "prediction", "=", "classifier", "(", "torch", ".", "mean", "(", "unpert_last_hidden", ",", "dim", "=", "1", ")", ")", "\n", "label", "=", "torch", ".", "tensor", "(", "[", "class_label", "]", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "unpert_discrim_loss", "=", "ce_loss", "(", "prediction", ",", "label", ")", "\n", "# print(\"unperturbed discrim loss\", unpert_discrim_loss.data.cpu().numpy())", "\n", "", "else", ":", "\n", "            ", "unpert_discrim_loss", "=", "0", "\n", "\n", "# Fuse the modified model and original model", "\n", "", "if", "perturb", ":", "\n", "\n", "            ", "unpert_probs", "=", "F", ".", "softmax", "(", "unpert_logits", "[", ":", ",", "-", "1", ",", ":", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "pert_probs", "=", "(", "pert_probs", "**", "gm_scale", ")", "*", "(", "unpert_probs", "**", "(", "1", "-", "gm_scale", ")", ")", "# + SMALL_CONST", "\n", "\n", "pert_probs", "=", "top_k_filter", "(", "pert_probs", ",", "k", "=", "top_k", ",", "probs", "=", "True", ")", "# + SMALL_CONST", "\n", "\n", "# rescale", "\n", "if", "torch", ".", "sum", "(", "pert_probs", ")", "<=", "1", ":", "\n", "                ", "pert_probs", "=", "pert_probs", "/", "torch", ".", "sum", "(", "pert_probs", ")", "\n", "\n", "\n", "\n", "\n", "", "", "else", ":", "\n", "            ", "pert_logits", "=", "top_k_filter", "(", "pert_logits", ",", "k", "=", "top_k", ")", "# + SMALL_CONST", "\n", "pert_probs", "=", "F", ".", "softmax", "(", "pert_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# sample or greedy", "\n", "", "if", "sample", ":", "\n", "            ", "last", "=", "torch", ".", "multinomial", "(", "pert_probs", ",", "num_samples", "=", "1", ")", "\n", "\n", "", "else", ":", "\n", "            ", "_", ",", "last", "=", "torch", ".", "topk", "(", "pert_probs", ",", "k", "=", "1", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "for", "j", "in", "range", "(", "sample_num", ")", ":", "\n", "            ", "last_prob", "=", "pert_probs", "[", "j", "]", "[", "last", "[", "j", "]", "]", ".", "tolist", "(", ")", "[", "0", "]", "\n", "generated_prob", "[", "j", "]", "*=", "last_prob", "\n", "\n", "\n", "# update context/output_so_far appending the new token", "\n", "", "output_so_far", "=", "last", "if", "output_so_far", "is", "None", "else", "torch", ".", "cat", "(", "(", "output_so_far", ",", "last", ")", ",", "dim", "=", "1", ")", "\n", "\n", "generated", "=", "last", "if", "generated", "is", "None", "else", "torch", ".", "cat", "(", "(", "generated", ",", "last", ")", ",", "dim", "=", "1", ")", "\n", "\n", "\n", "", "perplexity", "=", "1", "/", "generated_prob", "\n", "power", "=", "1", "/", "length", "\n", "perplexity", "=", "perplexity", ".", "pow", "(", "power", ")", "\n", "\n", "return", "output_so_far", ",", "unpert_discrim_loss", ",", "loss_in_time", ",", "generated", ",", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.set_generic_model_params": [[576, 586], ["ValueError", "ValueError", "open", "json.load"], "function", ["None"], ["", "def", "set_generic_model_params", "(", "discrim_weights", ",", "discrim_meta", ")", ":", "\n", "    ", "if", "discrim_weights", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"When using a generic discriminator, \"", "\"discrim_weights need to be specified\"", ")", "\n", "", "if", "discrim_meta", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"When using a generic discriminator, \"", "\"discrim_meta need to be specified\"", ")", "\n", "\n", "", "with", "open", "(", "discrim_meta", ",", "\"r\"", ")", "as", "discrim_meta_file", ":", "\n", "        ", "meta", "=", "json", ".", "load", "(", "discrim_meta_file", ")", "\n", "", "meta", "[", "\"path\"", "]", "=", "discrim_weights", "\n", "DISCRIMINATOR_MODELS_PARAMS", "[", "\"generic\"", "]", "=", "meta", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.run_pplm_example": [[589, 644], ["torch.manual_seed", "torch.manual_seed", "numpy.random.seed", "print", "gpt2_model.modeling_gpt2_condition.GPT2LMHeadModel.from_pretrained", "GPT2LMHeadModel.from_pretrained.to", "GPT2LMHeadModel.from_pretrained.eval", "gpt2_model.tokenization_gpt2.GPT2Tokenizer.from_pretrained", "GPT2LMHeadModel.from_pretrained.parameters", "run_pplm_fine_tuned.set_generic_model_params", "print", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.gpt2_model.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.set_generic_model_params", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "def", "run_pplm_example", "(", "\n", "pretrained_model", "=", "\"gpt2-large\"", ",", "\n", "cond_text", "=", "\"\"", ",", "\n", "uncond", "=", "False", ",", "\n", "num_samples", "=", "1", ",", "\n", "bag_of_words", "=", "None", ",", "\n", "discrim", "=", "None", ",", "\n", "discrim_weights", "=", "None", ",", "\n", "discrim_meta", "=", "None", ",", "\n", "class_label", "=", "-", "1", ",", "\n", "length", "=", "100", ",", "\n", "stepsize", "=", "0.02", ",", "\n", "temperature", "=", "1.0", ",", "\n", "top_k", "=", "10", ",", "\n", "sample", "=", "False", ",", "\n", "num_iterations", "=", "3", ",", "\n", "grad_length", "=", "10000", ",", "\n", "horizon_length", "=", "1", ",", "\n", "window_length", "=", "0", ",", "\n", "decay", "=", "False", ",", "\n", "gamma", "=", "1.5", ",", "\n", "gm_scale", "=", "0.9", ",", "\n", "kl_scale", "=", "0.01", ",", "\n", "seed", "=", "0", ",", "\n", "no_cuda", "=", "False", ",", "\n", "colorama", "=", "False", ",", "\n", ")", ":", "\n", "# set Random seed", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "\n", "# setup the device", "\n", "device", "=", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "no_cuda", "else", "\"cpu\"", "\n", "\n", "if", "discrim", "==", "\"generic\"", ":", "\n", "        ", "set_generic_model_params", "(", "discrim_weights", ",", "discrim_meta", ")", "\n", "\n", "", "print", "(", "\"pretrained_model set to discriminator's = {}\"", ".", "format", "(", "pretrained_model", ")", ")", "\n", "\n", "if", "discrim", "is", "not", "None", ":", "\n", "        ", "pretrained_model", "=", "DISCRIMINATOR_MODELS_PARAMS", "[", "discrim", "]", "[", "\"pretrained_model\"", "]", "\n", "print", "(", "\"discrim = {}, pretrained_model set \"", "\"to discriminator's = {}\"", ".", "format", "(", "discrim", ",", "pretrained_model", ")", ")", "\n", "\n", "# load pretrained model", "\n", "", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "pretrained_model", ",", "output_hidden_states", "=", "True", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# load tokenizer", "\n", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "pretrained_model", ")", "\n", "\n", "# Freeze GPT-2 weights", "\n", "for", "param", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.main": [[794, 803], ["torch.device", "torch.device", "run_pplm_fine_tuned.pplm", "run_pplm_fine_tuned.pplm.run_pplm_example", "print"], "function", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.run_pplm_fine_tuned.run_pplm_example", "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.result_statistics.result_statistics.print"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "device_pplm", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "pplm_model", "=", "pplm", "(", "seed", "=", "0", ",", "pretrained_model", "=", "'gpt2-medium'", ",", "device", "=", "device_pplm", ")", "\n", "context", "=", "\"Hello, my name is \"", "\n", "bag_of_words1", "=", "[", "'Barrichello'", ",", "'Rosberg'", ",", "'Kovalainen'", ",", "'Coulthard'", ",", "'Fisichella'", "]", "\n", "bag_of_words2", "=", "[", "'these'", ",", "'are'", ",", "'easy'", ",", "'words'", ",", "'ohmygodthatisnoteasy'", "]", "\n", "bag_of_words3", "=", "[", "\"is\"", "]", "\n", "gen_text", ",", "_", "=", "pplm_model", ".", "run_pplm_example", "(", "context", ",", "False", ",", "3", ",", "bag_of_words1", ",", "5", ",", "0.05", ",", "1.0", ",", "5", ",", "True", ",", "1", ",", "10000", ",", "1", ",", "0", ",", "False", ",", "1.5", ",", "0.9", ",", "0.01", ",", "True", ")", "\n", "print", "(", "gen_text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.__init__": [[12, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "model_results", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.add_model": [[15, 36], ["None"], "methods", ["None"], ["", "def", "add_model", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "self", ".", "model_results", "[", "model_name", "]", "=", "{", "}", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"batch count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"topics_diversity_dist\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"top_words_diversity_dist\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"specificity\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"novelty\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"novelty_word\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"novelty_word_w\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_50\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_50_arr\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"context_len_arr_no_stop\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"context_len_arr\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_50_w\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_topic_f_50\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_50_count\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_50_count_arr\"", "]", "=", "[", "]", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_50_count_w\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_all\"", "]", "=", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "\"relevancy_f_all_count\"", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.evaluate_topic_models": [[39, 127], ["top_value.size", "word_w_sum_norm.size", "range", "topic_embedding.size", "topic_embedding.mean", "torch.sqrt().mean().item", "len", "torch.zeros", "range", "range", "topic_result_statistics.topic_result_statistics.evaluate_topic_models._emb_var"], "methods", ["None"], ["", "def", "evaluate_topic_models", "(", "self", ",", "model_name", ",", "top_value", ",", "top_index", ",", "word_w_sum_norm", ",", "word_idx_list", ",", "word_idx_rest_list", ",", "word_full_list", ",", "idx2word_freq", ",", "word_norm_emb", ")", ":", "\n", "        ", "def", "_emb_var", "(", "topic_embedding", ")", ":", "\n", "            ", "n_basis", "=", "topic_embedding", ".", "size", "(", "0", ")", "\n", "topic_embedding_mean", "=", "topic_embedding", ".", "mean", "(", "dim", "=", "0", ",", "keepdim", "=", "True", ")", "\n", "topic_sq", "=", "(", "topic_embedding", "-", "topic_embedding_mean", ")", "*", "(", "topic_embedding", "-", "topic_embedding_mean", ")", "\n", "return", "torch", ".", "sqrt", "(", "topic_sq", ".", "sum", "(", "dim", "=", "1", ")", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "def", "compute_word_weight", "(", "word_index", ",", "idx2word_freq", ",", "device", ")", ":", "\n", "            ", "alpha", "=", "1e-4", "\n", "num_word", "=", "len", "(", "word_index", ")", "\n", "word_weight_context", "=", "torch", ".", "zeros", "(", "num_word", ",", "device", "=", "device", ")", "\n", "for", "w_i", "in", "range", "(", "num_word", ")", ":", "\n", "                ", "w_idx", "=", "word_index", "[", "w_i", "]", "\n", "prob", "=", "idx2word_freq", "[", "w_idx", "]", "[", "2", "]", "\n", "word_weight_context", "[", "w_i", "]", "=", "alpha", "/", "(", "alpha", "+", "prob", ")", "\n", "", "return", "word_weight_context", "\n", "\n", "", "batch_size", ",", "num_head", ",", "top_k", ",", "n_basis", "=", "top_value", ".", "size", "(", ")", "\n", "emb_size", "=", "word_w_sum_norm", ".", "size", "(", "-", "1", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "num_head", ")", ":", "\n", "                ", "word_index_future", "=", "word_idx_rest_list", "[", "i", "]", "[", "j", "]", "\n", "word_index_context", "=", "word_idx_list", "[", "i", "]", "[", "j", "]", "\n", "#word_raw_context = word_raw_list[i][j]", "\n", "#context_proc, bad_context = utils_testing.preprocessing_context(word_raw_context)", "\n", "#if bad_context:", "\n", "#    continue", "\n", "if", "len", "(", "word_index_context", ")", "<=", "1", ":", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "word_index_future", ")", "<=", "1", ":", "\n", "#skip all the results at the end of each paragraph, because we cannot evaluate the relevency", "\n", "#when evaluating the relevancy, we skip the first future token because it might be an incompleted word (a word piece)", "\n", "                    ", "continue", "\n", "", "self", ".", "model_results", "[", "model_name", "]", "[", "'count'", "]", "+=", "1", "\n", "topic_embedding", "=", "word_w_sum_norm", "[", "i", ",", "j", ",", ":", ",", ":", "]", "\n", "assert", "torch", ".", "nonzero", "(", "torch", ".", "sum", "(", "topic_embedding", ",", "dim", "=", "1", ")", "==", "0", ")", ".", "size", "(", "0", ")", "==", "0", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'topics_diversity_dist'", "]", "+=", "_emb_var", "(", "topic_embedding", ")", "\n", "top_index_ij_np", "=", "top_index", "[", "i", ",", "j", ",", ":", ",", ":", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "specificity", "=", "0", "\n", "#t_word_emb = np.empty( (top_k, n_basis, emb_size) )", "\n", "t_word_emb", "=", "torch", ".", "empty", "(", "(", "top_k", ",", "n_basis", ",", "emb_size", ")", ")", "\n", "for", "k", "in", "range", "(", "top_k", ")", ":", "\n", "                    ", "for", "n", "in", "range", "(", "n_basis", ")", ":", "\n", "                        ", "specificity", "+=", "1.0", "/", "idx2word_freq", "[", "top_index_ij_np", "[", "k", ",", "n", "]", "]", "[", "1", "]", "\n", "t_word_emb", "[", "k", ",", "n", ",", ":", "]", "=", "word_norm_emb", "[", "top_index_ij_np", "[", "k", ",", "n", "]", ",", ":", "]", "\n", "#t_word_emb_tensor = torch.tensor(t_word_emb.reshape(top_k * n_basis, emb_size))", "\n", "", "", "self", ".", "model_results", "[", "model_name", "]", "[", "'top_words_diversity_dist'", "]", "+=", "_emb_var", "(", "t_word_emb", ".", "view", "(", "top_k", "*", "n_basis", ",", "emb_size", ")", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'specificity'", "]", "+=", "specificity", "/", "(", "top_k", "*", "n_basis", ")", "\n", "\n", "word_emb_context", "=", "word_norm_emb", "[", "word_index_context", "[", ":", "-", "1", "]", ",", ":", "]", "\n", "word_weight_context", "=", "compute_word_weight", "(", "word_index_context", "[", ":", "-", "1", "]", ",", "idx2word_freq", ",", "word_norm_emb", ".", "device", ")", "\n", "\n", "topic_word_sim", "=", "torch", ".", "mm", "(", "topic_embedding", ",", "word_emb_context", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "topic_word_sim_max_topic", ",", "_", "=", "topic_word_sim", ".", "max", "(", "dim", "=", "1", ")", "\n", "topic_word_sim_max_word", ",", "_", "=", "topic_word_sim", ".", "max", "(", "dim", "=", "0", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'novelty'", "]", "+=", "1", "-", "topic_word_sim_max_topic", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'novelty_word'", "]", "+=", "1", "-", "topic_word_sim_max_word", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'novelty_word_w'", "]", "+=", "1", "-", "(", "(", "topic_word_sim_max_word", "*", "word_weight_context", ")", ".", "sum", "(", ")", "/", "word_weight_context", ".", "sum", "(", ")", ")", ".", "item", "(", ")", "\n", "\n", "word_emb_future", "=", "word_norm_emb", "[", "word_index_future", "[", "1", ":", "]", ",", ":", "]", "\n", "word_weight_future", "=", "compute_word_weight", "(", "word_index_future", "[", "1", ":", "]", ",", "idx2word_freq", ",", "word_norm_emb", ".", "device", ")", "\n", "\n", "topic_future_word_sim", "=", "torch", ".", "mm", "(", "topic_embedding", ",", "word_emb_future", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "topic_future_word_sim_max_word", ",", "_", "=", "topic_future_word_sim", ".", "max", "(", "dim", "=", "0", ")", "\n", "#print(topic_future_word_sim_max.size())", "\n", "#print(min(50,topic_future_word_sim_max.size(0)))", "\n", "future_window_size", "=", "50", "\n", "if", "topic_future_word_sim_max_word", ".", "size", "(", "0", ")", ">", "future_window_size", ":", "\n", "                    ", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_count'", "]", "+=", "future_window_size", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50'", "]", "+=", "topic_future_word_sim_max_word", "[", ":", "future_window_size", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "while", "j", ">=", "len", "(", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_count_arr'", "]", ")", ":", "\n", "                        ", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_count_arr'", "]", ".", "append", "(", "0", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_arr'", "]", ".", "append", "(", "0", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'context_len_arr_no_stop'", "]", ".", "append", "(", "0", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'context_len_arr'", "]", ".", "append", "(", "0", ")", "\n", "", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_arr'", "]", "[", "j", "]", "+=", "topic_future_word_sim_max_word", "[", ":", "future_window_size", "]", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_count_arr'", "]", "[", "j", "]", "+=", "1", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'context_len_arr_no_stop'", "]", "[", "j", "]", "+=", "len", "(", "word_index_context", "[", ":", "-", "1", "]", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'context_len_arr'", "]", "[", "j", "]", "+=", "len", "(", "word_full_list", "[", "i", "]", "[", "j", "]", "[", ":", "-", "1", "]", ")", "\n", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_count_w'", "]", "+=", "1", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_50_w'", "]", "+=", "(", "(", "topic_future_word_sim_max_word", "[", ":", "future_window_size", "]", "*", "word_weight_future", "[", ":", "future_window_size", "]", ")", ".", "sum", "(", ")", "/", "word_weight_future", "[", ":", "future_window_size", "]", ".", "sum", "(", ")", ")", ".", "item", "(", ")", "\n", "\n", "topic_future_word_sim_max_topic", ",", "_", "=", "topic_future_word_sim", "[", ":", ",", ":", "future_window_size", "]", ".", "max", "(", "dim", "=", "1", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_topic_f_50'", "]", "+=", "topic_future_word_sim_max_topic", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_all_count'", "]", "+=", "topic_future_word_sim_max_word", ".", "size", "(", "0", ")", "\n", "self", ".", "model_results", "[", "model_name", "]", "[", "'relevancy_f_all'", "]", "+=", "topic_future_word_sim_max_word", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.topic_result_statistics.topic_result_statistics.generate_report": [[129, 151], ["outf.write", "topic_result_statistics.topic_result_statistics.model_results.items", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "outf.write", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "sum", "float", "sum", "float", "range", "range", "sum", "range", "sum", "len", "len", "len"], "methods", ["None"], ["", "", "", "def", "generate_report", "(", "self", ",", "outf", ")", ":", "\n", "        ", "outf", ".", "write", "(", "'Reports: \\n'", ")", "\n", "for", "model_name", ",", "model", "in", "self", ".", "model_results", ".", "items", "(", ")", ":", "\n", "            ", "outf", ".", "write", "(", "model_name", "+", "\" count: \"", "+", "str", "(", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" topics_diversity_dist: \"", "+", "str", "(", "model", "[", "\"topics_diversity_dist\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" top_words_diversity_dist: \"", "+", "str", "(", "model", "[", "\"top_words_diversity_dist\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" specificity: \"", "+", "str", "(", "model", "[", "\"specificity\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" novelty: \"", "+", "str", "(", "model", "[", "\"novelty\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" novelty_word: \"", "+", "str", "(", "model", "[", "\"novelty_word\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" novelty_word_w: \"", "+", "str", "(", "model", "[", "\"novelty_word_w\"", "]", "/", "model", "[", "\"count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_f_50: \"", "+", "str", "(", "model", "[", "\"relevancy_f_50\"", "]", "/", "model", "[", "\"relevancy_f_50_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_f_50_arr: \"", "+", "str", "(", "[", "model", "[", "\"relevancy_f_50_arr\"", "]", "[", "x", "]", "/", "model", "[", "\"relevancy_f_50_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"relevancy_f_50_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"relevancy_f_50_count_arr\"", "]", ")", ")", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" context_len_arr: \"", "+", "str", "(", "[", "model", "[", "\"context_len_arr\"", "]", "[", "x", "]", "/", "model", "[", "\"relevancy_f_50_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"relevancy_f_50_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"relevancy_f_50_count_arr\"", "]", ")", ")", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" context_len_all: \"", "+", "str", "(", "sum", "(", "model", "[", "\"context_len_arr\"", "]", ")", "/", "float", "(", "sum", "(", "model", "[", "\"relevancy_f_50_count_arr\"", "]", ")", ")", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" context_len_arr_no_stop: \"", "+", "str", "(", "[", "model", "[", "\"context_len_arr_no_stop\"", "]", "[", "x", "]", "/", "model", "[", "\"relevancy_f_50_count_arr\"", "]", "[", "x", "]", "if", "model", "[", "\"relevancy_f_50_count_arr\"", "]", "[", "x", "]", ">", "0", "else", "0", "for", "x", "in", "range", "(", "len", "(", "model", "[", "\"relevancy_f_50_count_arr\"", "]", ")", ")", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" context_len_all_no_stop: \"", "+", "str", "(", "sum", "(", "model", "[", "\"context_len_arr_no_stop\"", "]", ")", "/", "float", "(", "sum", "(", "model", "[", "\"relevancy_f_50_count_arr\"", "]", ")", ")", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_f_50_w: \"", "+", "str", "(", "model", "[", "\"relevancy_f_50_w\"", "]", "/", "model", "[", "\"relevancy_f_50_count_w\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_topic_f_50: \"", "+", "str", "(", "model", "[", "\"relevancy_topic_f_50\"", "]", "/", "model", "[", "\"relevancy_f_50_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_f_all: \"", "+", "str", "(", "model", "[", "\"relevancy_f_all\"", "]", "/", "model", "[", "\"relevancy_f_all_count\"", "]", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_f_50 - (1 - novelty_word): \"", "+", "str", "(", "(", "model", "[", "\"relevancy_f_50\"", "]", "/", "model", "[", "\"relevancy_f_50_count\"", "]", "+", "model", "[", "\"novelty_word\"", "]", "/", "model", "[", "\"count\"", "]", ")", "-", "1", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "model_name", "+", "\" relevancy_f_50_w - (1 - novelty_word_w): \"", "+", "str", "(", "(", "model", "[", "\"relevancy_f_50_w\"", "]", "/", "model", "[", "\"relevancy_f_50_count_w\"", "]", "+", "model", "[", "\"novelty_word_w\"", "]", "/", "model", "[", "\"count\"", "]", ")", "-", "1", ")", "+", "'\\n'", ")", "\n", "outf", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__": [[7, 14], ["super().__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.__init__"], ["def", "__init__", "(", "self", ",", "class_size", ",", "embed_size", ")", ":", "\n", "        ", "super", "(", "ClassificationHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "class_size", "=", "class_size", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "# self.mlp1 = torch.nn.Linear(embed_size, embed_size)", "\n", "# self.mlp2 = (torch.nn.Linear(embed_size, class_size))", "\n", "self", ".", "mlp", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_size", ",", "class_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.evaluation.pplm_classification_head.ClassificationHead.forward": [[15, 20], ["pplm_classification_head.ClassificationHead.mlp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_state", ")", ":", "\n", "# hidden_state = F.relu(self.mlp1(hidden_state))", "\n", "# hidden_state = self.mlp2(hidden_state)", "\n", "        ", "logits", "=", "self", ".", "mlp", "(", "hidden_state", ")", "\n", "return", "logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.iesl_interactive_LM.Results.analyze_crowdsourcing_text_generator.map_value_f": [[17, 19], ["None"], "function", ["None"], ["def", "map_value_f", "(", "x", ",", "mapping_dict", ")", ":", "\n", "\t", "return", "mapping_dict", "[", "x", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.Results.analyze_crowdsourcing_text_generator.get_topic_index": [[28, 30], ["[].strip", "x.split", "topic.split"], "function", ["None"], ["def", "get_topic_index", "(", "x", ")", ":", "\n", "\t", "return", "'|'", ".", "join", "(", "[", "topic", ".", "split", "(", "':'", ")", "[", "0", "]", ".", "strip", "(", ")", "for", "topic", "in", "x", ".", "split", "(", "'|'", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.Results.analyze_crowdsourcing_text_generator.compute_accuracy": [[33, 47], ["x[].split", "numpy.zeros", "numpy.zeros", "str().split", "numpy.sum", "float", "str", "int", "int", "numpy.abs"], "function", ["None"], ["def", "compute_accuracy", "(", "x", ")", ":", "\n", "\t", "gt", "=", "x", "[", "0", "]", ".", "split", "(", "'|'", ")", "\n", "if", "x", "[", "1", "]", "is", "np", ".", "nan", ":", "\n", "\t\t", "pred", "=", "[", "]", "\n", "", "else", ":", "\n", "\t\t", "pred", "=", "str", "(", "x", "[", "1", "]", ")", ".", "split", "(", "'|'", ")", "\n", "\n", "", "gt_array", "=", "np", ".", "zeros", "(", "num_topic", ")", "\n", "for", "gt_i", "in", "gt", ":", "\n", "\t\t", "gt_array", "[", "int", "(", "gt_i", ")", "]", "=", "1", "\n", "", "pred_array", "=", "np", ".", "zeros", "(", "num_topic", ")", "\n", "for", "pred_i", "in", "pred", ":", "\n", "\t\t", "pred_array", "[", "int", "(", "pred_i", ")", "]", "=", "1", "\n", "", "return", "1", "-", "np", ".", "sum", "(", "np", ".", "abs", "(", "gt_array", "-", "pred_array", ")", ")", "/", "float", "(", "num_topic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.Results.analyze_crowdsourcing_text_generator.compute_recall": [[53, 67], ["x[].split", "numpy.zeros", "numpy.zeros", "str().split", "numpy.sum", "numpy.sum", "numpy.logical_and", "str", "int", "int"], "function", ["None"], ["def", "compute_recall", "(", "x", ")", ":", "\n", "\t", "gt", "=", "x", "[", "0", "]", ".", "split", "(", "'|'", ")", "\n", "if", "x", "[", "1", "]", "is", "np", ".", "nan", ":", "\n", "\t\t", "pred", "=", "[", "]", "\n", "", "else", ":", "\n", "\t\t", "pred", "=", "str", "(", "x", "[", "1", "]", ")", ".", "split", "(", "'|'", ")", "\n", "\n", "", "gt_array", "=", "np", ".", "zeros", "(", "num_topic", ")", "\n", "for", "gt_i", "in", "gt", ":", "\n", "\t\t", "gt_array", "[", "int", "(", "gt_i", ")", "]", "=", "1", "\n", "", "pred_array", "=", "np", ".", "zeros", "(", "num_topic", ")", "\n", "for", "pred_i", "in", "pred", ":", "\n", "\t\t", "pred_array", "[", "int", "(", "pred_i", ")", "]", "=", "1", "\n", "", "return", "np", ".", "sum", "(", "np", ".", "logical_and", "(", "gt_array", ",", "pred_array", ")", ")", "/", "np", ".", "sum", "(", "gt_array", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.Results.analyze_crowdsourcing_text_generator.compute_precision": [[72, 89], ["x[].split", "numpy.zeros", "numpy.zeros", "str().split", "numpy.sum", "numpy.sum", "numpy.sum", "str", "int", "int", "numpy.logical_and"], "function", ["None"], ["def", "compute_precision", "(", "x", ")", ":", "\n", "\t", "gt", "=", "x", "[", "0", "]", ".", "split", "(", "'|'", ")", "\n", "if", "x", "[", "1", "]", "is", "np", ".", "nan", ":", "\n", "\t\t", "pred", "=", "[", "]", "\n", "", "else", ":", "\n", "\t\t", "pred", "=", "str", "(", "x", "[", "1", "]", ")", ".", "split", "(", "'|'", ")", "\n", "\n", "", "gt_array", "=", "np", ".", "zeros", "(", "num_topic", ")", "\n", "for", "gt_i", "in", "gt", ":", "\n", "\t\t", "gt_array", "[", "int", "(", "gt_i", ")", "]", "=", "1", "\n", "", "pred_array", "=", "np", ".", "zeros", "(", "num_topic", ")", "\n", "for", "pred_i", "in", "pred", ":", "\n", "\t\t", "pred_array", "[", "int", "(", "pred_i", ")", "]", "=", "1", "\n", "", "if", "np", ".", "sum", "(", "pred_array", ")", ">", "0", ":", "\n", "\t\t", "return", "np", ".", "sum", "(", "np", ".", "logical_and", "(", "gt_array", ",", "pred_array", ")", ")", "/", "np", ".", "sum", "(", "pred_array", ")", "\n", "", "else", ":", "\n", "\t\t", "return", "np", ".", "nan", "\n", "\n"]], "home.repos.pwc.inspect_result.iesl_interactive_LM.Results.analyze_crowdsourcing_text_generator.count_num_f": [[95, 103], ["str", "len", "str().replace().replace().split", "str().replace().replace", "str().replace", "str"], "function", ["None"], ["def", "count_num_f", "(", "x", ")", ":", "\n", "\t", "if", "'None'", "==", "str", "(", "x", ")", ":", "\n", "\t\t", "return", "0", "\n", "", "else", ":", "\n", "\t\t", "out", "=", "len", "(", "str", "(", "x", ")", ".", "replace", "(", "'None|'", ",", "''", ")", ".", "replace", "(", "'|None'", ",", "''", ")", ".", "split", "(", "'|'", ")", ")", "\n", "#out = len(x.replace('None','').strip().split('|'))", "\n", "#print(x, x.replace('None','').strip(), out)", "\n", "return", "out", "\n", "\n"]]}