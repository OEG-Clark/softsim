{"home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.BertLayerNorm.__init__": [[52, 59], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BertLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.BertLayerNorm.forward": [[60, 65], ["x.mean", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "weight", "*", "x", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.XLNetLayerNorm.__init__": [[68, 75], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "eps", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "XLNetLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "d_model", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "d_model", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.XLNetLayerNorm.forward": [[76, 81], ["x.mean", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "weight", "*", "x", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.BertPreTrainedModel.__init__": [[92, 94], ["transformers.PreTrainedModel.__init__"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["def", "__init__", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertPreTrainedModel", ",", "self", ")", ".", "__init__", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.BertPreTrainedModel.init_weights": [[95, 107], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.XLNetPreTrainedModel.__init__": [[115, 117], ["transformers.PreTrainedModel.__init__"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["def", "__init__", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "XLNetPreTrainedModel", ",", "self", ")", ".", "__init__", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert.XLNetPreTrainedModel.init_weights": [[118, 135], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_", "isinstance", "module.mask_emb.data.normal_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the weights.\n        :param module:\n        :return:\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "XLNetLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "XLNetModel", ")", ":", "\n", "            ", "module", ".", "mask_emb", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.set_seed": [[52, 58], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.init_args": [[60, 152], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "MODEL_CLASSES.keys", "glue_utils.processors.keys"], "function", ["None"], ["", "", "def", "init_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Model type selected in the list: \"", "+", "\", \"", ".", "join", "(", "MODEL_CLASSES", ".", "keys", "(", ")", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--absa_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Downstream absa layer type selected in the list: [linear, gru, san, tfm, crf]\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tfm_mode\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"mode of the pre-trained transformer, selected from: [finetune]\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fix_tfm\"", ",", "default", "=", "None", ",", "type", "=", "int", ",", "required", "=", "True", ",", "\n", "help", "=", "\"whether fix the transformer params or not\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name selected in the list: \"", "+", "\", \"", ".", "join", "(", "\n", "ALL_MODELS", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train selected in the list: \"", "+", "\", \"", ".", "join", "(", "processors", ".", "keys", "(", ")", ")", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--config_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained config name or path if not the same as model_name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained tokenizer name or path if not the same as model_name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after tokenization. Sequences longer \"", "\n", "\"than this will be truncated, sequences shorter will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--evaluate_during_training\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Rul evaluation during training at each logging step.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "8", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Batch size per GPU/CPU for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "8", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Weight deay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "default", "=", "3.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_steps\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"If > 0: set total number of training steps to perform. Override num_train_epochs.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--logging_steps'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Log every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--save_steps'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"Save checkpoint every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_all_checkpoints\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "'--overwrite_output_dir'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Overwrite the content of the output directory\"", ")", "\n", "parser", ".", "add_argument", "(", "'--overwrite_cache'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Overwrite the cached training and evaluation sets\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tagging_schema'", ",", "type", "=", "str", ",", "default", "=", "'BIEOS'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--overfit\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"if evaluate overfit or not\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"For distributed training: local_rank\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"For distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"For distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--MASTER_ADDR'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--MASTER_PORT'", ",", "type", "=", "str", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "output_dir", "=", "'%s-%s-%s-%s'", "%", "(", "args", ".", "model_type", ",", "args", ".", "absa_type", ",", "args", ".", "task_name", ",", "args", ".", "tfm_mode", ")", "\n", "\n", "if", "args", ".", "fix_tfm", ":", "\n", "        ", "output_dir", "=", "'%s-fix'", "%", "output_dir", "\n", "", "if", "args", ".", "overfit", ":", "\n", "        ", "output_dir", "=", "'%s-overfit'", "%", "output_dir", "\n", "args", ".", "max_steps", "=", "3000", "\n", "", "args", ".", "output_dir", "=", "output_dir", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.train": [[154, 256], ["torch.utils.data.DataLoader", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "model.zero_grad", "tqdm.trange", "main.set_seed", "tensorboardX.SummaryWriter", "max", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "len", "int", "tqdm.tqdm", "enumerate", "tensorboardX.SummaryWriter.close", "model.train", "tuple", "model", "loss.mean.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "loss.mean.item", "tqdm.trange.close", "len", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "loss.mean.mean", "model.parameters", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "model.zero_grad", "tqdm.tqdm.close", "len", "model.named_parameters", "model.named_parameters", "any", "t.to", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "os.path.join", "model_to_save.save_pretrained", "torch.save", "torch.save", "logger.info", "any", "main.evaluate", "evaluate.items", "os.path.exists", "os.makedirs", "hasattr", "os.path.join", "tensorboardX.SummaryWriter.add_scalar", "transformers.get_linear_schedule_with_warmup.get_lr"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.set_seed", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.train", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.evaluate"], ["", "def", "train", "(", "args", ",", "train_dataset", ",", "model", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\" Train the model \"\"\"", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "tb_writer", "=", "SummaryWriter", "(", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "per_gpu_train_batch_size", "*", "max", "(", "1", ",", "args", ".", "n_gpu", ")", "\n", "# draw training samples from shuffled dataset", "\n", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "if", "args", ".", "local_rank", "==", "-", "1", "else", "DistributedSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "if", "args", ".", "max_steps", ">", "0", ":", "\n", "        ", "t_total", "=", "args", ".", "max_steps", "\n", "args", ".", "num_train_epochs", "=", "args", ".", "max_steps", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "+", "1", "\n", "", "else", ":", "\n", "        ", "t_total", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "# Prepare optimizer and schedule (linear warmup and decay)", "\n", "", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "args", ".", "weight_decay", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", ")", "\n", "\n", "# Train!", "\n", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "args", ".", "num_train_epochs", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per GPU = %d\"", ",", "args", ".", "per_gpu_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"", ",", "\n", "args", ".", "train_batch_size", "*", "args", ".", "gradient_accumulation_steps", "*", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "if", "args", ".", "local_rank", "!=", "-", "1", "else", "1", ")", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "args", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "t_total", ")", "\n", "\n", "global_step", "=", "0", "\n", "tr_loss", ",", "logging_loss", "=", "0.0", ",", "0.0", "\n", "model", ".", "zero_grad", "(", ")", "\n", "train_iterator", "=", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", "\n", "# set the seed number", "\n", "set_seed", "(", "args", ")", "# Added here for reproductibility (even between python 2 and 3)", "\n", "for", "_", "in", "train_iterator", ":", "\n", "        ", "epoch_iterator", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "epoch_iterator", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "inputs", "=", "{", "'input_ids'", ":", "batch", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "batch", "[", "1", "]", ",", "\n", "'token_type_ids'", ":", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "'bert'", ",", "'xlnet'", "]", "else", "None", ",", "# XLM don't use segment_ids", "\n", "'labels'", ":", "batch", "[", "3", "]", "}", "\n", "ouputs", "=", "model", "(", "**", "inputs", ")", "\n", "# loss with attention mask", "\n", "loss", "=", "ouputs", "[", "0", "]", "# model outputs are always tuple in pytorch-transformers (see doc)", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu parallel training", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "logging_steps", ">", "0", "and", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "# Log metrics", "\n", "                    ", "if", "args", ".", "local_rank", "==", "-", "1", "and", "args", ".", "evaluate_during_training", ":", "# Only evaluate when single GPU otherwise metrics may not average well", "\n", "                        ", "results", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ")", "\n", "for", "key", ",", "value", "in", "results", ".", "items", "(", ")", ":", "\n", "                            ", "tb_writer", ".", "add_scalar", "(", "'eval_{}'", ".", "format", "(", "key", ")", ",", "value", ",", "global_step", ")", "\n", "", "", "tb_writer", ".", "add_scalar", "(", "'lr'", ",", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "global_step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'loss'", ",", "(", "tr_loss", "-", "logging_loss", ")", "/", "args", ".", "logging_steps", ",", "global_step", ")", "\n", "logging_loss", "=", "tr_loss", "\n", "\n", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "save_steps", ">", "0", "and", "global_step", "%", "args", ".", "save_steps", "==", "0", ":", "\n", "# Save model checkpoint per each N steps", "\n", "                    ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'checkpoint-{}'", ".", "format", "(", "global_step", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "                        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "output_dir", ")", "\n", "torch", ".", "save", "(", "args", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'training_args.bin'", ")", ")", "\n", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "output_dir", ")", "\n", "\n", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "                ", "epoch_iterator", ".", "close", "(", ")", "\n", "break", "\n", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "            ", "train_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "tb_writer", ".", "close", "(", ")", "\n", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.evaluate": [[258, 330], ["zip", "main.load_and_cache_examples", "torch.utils.data.DataLoader", "tqdm.tqdm", "glue_utils.compute_metrics_absa", "results.update", "os.path.join", "os.makedirs", "max", "torch.utils.data.SequentialSampler", "torch.utils.data.distributed.DistributedSampler", "model.eval", "tuple", "numpy.argmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.tagger.viterbi_tags", "open", "sorted", "os.path.exists", "torch.no_grad", "torch.no_grad", "model", "tmp_eval_loss.mean().item", "torch.cat.append", "torch.cat.append", "logits.detach().cpu().numpy", "inputs[].detach().cpu().numpy", "numpy.append", "numpy.append", "glue_utils.compute_metrics_absa.keys", "writer.write", "t.to", "logits.detach().cpu().numpy", "inputs[].detach().cpu().numpy", "logger.info", "tmp_eval_loss.mean", "logits.detach().cpu", "inputs[].detach().cpu", "str", "logits.detach().cpu", "inputs[].detach().cpu", "str", "logits.detach", "inputs[].detach", "logits.detach", "inputs[].detach"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.load_and_cache_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.compute_metrics_absa", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.viterbi_tags"], ["", "def", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "mode", ",", "prefix", "=", "\"\"", ")", ":", "\n", "# Loop to handle MNLI double evaluation (matched, mis-matched)", "\n", "    ", "eval_task_names", "=", "(", "args", ".", "task_name", ",", ")", "\n", "eval_outputs_dirs", "=", "(", "args", ".", "output_dir", ",", ")", "\n", "\n", "results", "=", "{", "}", "\n", "for", "eval_task", ",", "eval_output_dir", "in", "zip", "(", "eval_task_names", ",", "eval_outputs_dirs", ")", ":", "\n", "        ", "eval_dataset", ",", "eval_evaluate_label_ids", "=", "load_and_cache_examples", "(", "args", ",", "eval_task", ",", "tokenizer", ",", "mode", "=", "mode", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "eval_output_dir", ")", "and", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "os", ".", "makedirs", "(", "eval_output_dir", ")", "\n", "\n", "", "args", ".", "eval_batch_size", "=", "args", ".", "per_gpu_eval_batch_size", "*", "max", "(", "1", ",", "args", ".", "n_gpu", ")", "\n", "# Note that DistributedSampler samples randomly", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "if", "args", ".", "local_rank", "==", "-", "1", "else", "DistributedSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "# Eval!", "\n", "#logger.info(\"***** Running evaluation on %s.txt *****\" % mode)", "\n", "eval_loss", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "preds", "=", "None", "\n", "out_label_ids", "=", "None", "\n", "crf_logits", ",", "crf_mask", "=", "[", "]", ",", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "inputs", "=", "{", "'input_ids'", ":", "batch", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "batch", "[", "1", "]", ",", "\n", "'token_type_ids'", ":", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "'bert'", ",", "'xlnet'", "]", "else", "None", ",", "# XLM don't use segment_ids", "\n", "'labels'", ":", "batch", "[", "3", "]", "}", "\n", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "# logits: (bsz, seq_len, label_size)", "\n", "# here the loss is the masked loss", "\n", "tmp_eval_loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "crf_logits", ".", "append", "(", "logits", ")", "\n", "crf_mask", ".", "append", "(", "batch", "[", "1", "]", ")", "\n", "", "nb_eval_steps", "+=", "1", "\n", "if", "preds", "is", "None", ":", "\n", "                ", "preds", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "out_label_ids", "=", "inputs", "[", "'labels'", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "preds", "=", "np", ".", "append", "(", "preds", ",", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "out_label_ids", "=", "np", ".", "append", "(", "out_label_ids", ",", "inputs", "[", "'labels'", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "# argmax operation over the last dimension", "\n", "if", "model", ".", "tagger_config", ".", "absa_type", "!=", "'crf'", ":", "\n", "# greedy decoding", "\n", "            ", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "# viterbi decoding for CRF-based model", "\n", "            ", "crf_logits", "=", "torch", ".", "cat", "(", "crf_logits", ",", "dim", "=", "0", ")", "\n", "crf_mask", "=", "torch", ".", "cat", "(", "crf_mask", ",", "dim", "=", "0", ")", "\n", "preds", "=", "model", ".", "tagger", ".", "viterbi_tags", "(", "logits", "=", "crf_logits", ",", "mask", "=", "crf_mask", ")", "\n", "", "result", "=", "compute_metrics_absa", "(", "preds", ",", "out_label_ids", ",", "eval_evaluate_label_ids", ",", "args", ".", "tagging_schema", ")", "\n", "result", "[", "'eval_loss'", "]", "=", "eval_loss", "\n", "results", ".", "update", "(", "result", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "eval_output_dir", ",", "\"%s_results.txt\"", "%", "mode", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "#logger.info(\"***** %s results *****\" % mode)", "\n", "            ", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "if", "'eval_loss'", "in", "key", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "#logger.info(\"***** %s results *****\" % mode)", "\n", "\n", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.load_and_cache_examples": [[332, 375], ["os.path.join", "os.path.exists", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "print", "torch.load", "torch.load", "processor.get_labels", "glue_utils.convert_examples_to_seq_features", "list().pop", "str", "str", "processor.get_train_examples", "torch.save", "torch.save", "processor.get_dev_examples", "bool", "bool", "list", "processor.get_test_examples", "Exception", "filter", "args.model_name_or_path.split"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_labels", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.convert_examples_to_seq_features", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_train_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_dev_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_test_examples"], ["", "def", "load_and_cache_examples", "(", "args", ",", "task", ",", "tokenizer", ",", "mode", "=", "'train'", ")", ":", "\n", "    ", "processor", "=", "processors", "[", "task", "]", "(", ")", "\n", "# Load data features from cache or dataset file", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'cached_{}_{}_{}_{}'", ".", "format", "(", "\n", "mode", ",", "\n", "list", "(", "filter", "(", "None", ",", "args", ".", "model_name_or_path", ".", "split", "(", "'/'", ")", ")", ")", ".", "pop", "(", ")", ",", "\n", "str", "(", "args", ".", "max_seq_length", ")", ",", "\n", "str", "(", "task", ")", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", ":", "\n", "        ", "print", "(", "\"cached_features_file:\"", ",", "cached_features_file", ")", "\n", "features", "=", "torch", ".", "load", "(", "cached_features_file", ")", "\n", "", "else", ":", "\n", "#logger.info(\"Creating features from dataset file at %s\", args.data_dir)", "\n", "        ", "label_list", "=", "processor", ".", "get_labels", "(", "args", ".", "tagging_schema", ")", "\n", "if", "mode", "==", "'train'", ":", "\n", "            ", "examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "tagging_schema", ")", "\n", "", "elif", "mode", "==", "'dev'", ":", "\n", "            ", "examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ",", "args", ".", "tagging_schema", ")", "\n", "", "elif", "mode", "==", "'test'", ":", "\n", "            ", "examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "tagging_schema", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Invalid data mode %s...\"", "%", "mode", ")", "\n", "", "features", "=", "convert_examples_to_seq_features", "(", "examples", "=", "examples", ",", "label_list", "=", "label_list", ",", "tokenizer", "=", "tokenizer", ",", "\n", "cls_token_at_end", "=", "bool", "(", "args", ".", "model_type", "in", "[", "'xlnet'", "]", ")", ",", "\n", "cls_token", "=", "tokenizer", ".", "cls_token", ",", "\n", "sep_token", "=", "tokenizer", ".", "sep_token", ",", "\n", "cls_token_segment_id", "=", "2", "if", "args", ".", "model_type", "in", "[", "'xlnet'", "]", "else", "0", ",", "\n", "pad_on_left", "=", "bool", "(", "args", ".", "model_type", "in", "[", "'xlnet'", "]", ")", ",", "\n", "pad_token_segment_id", "=", "4", "if", "args", ".", "model_type", "in", "[", "'xlnet'", "]", "else", "0", ")", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "#logger.info(\"Saving features into cached file %s\", cached_features_file)", "\n", "            ", "torch", ".", "save", "(", "features", ",", "cached_features_file", ")", "\n", "\n", "# Convert to Tensors and build dataset", "\n", "", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "# used in evaluation", "\n", "all_evaluate_label_ids", "=", "[", "f", ".", "evaluate_label_ids", "for", "f", "in", "features", "]", "\n", "dataset", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "return", "dataset", ",", "all_evaluate_label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.main": [[377, 539], ["main.init_args", "logging.basicConfig", "logger.warning", "main.set_seed", "init_args.task_name.lower", "processor.get_labels", "len", "init_args.model_type.lower", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "torch.nn.DataParallel.to", "logger.info", "logger.info", "open", "open.write", "zip", "range", "open.write", "open.write", "open.close", "os.path.exists", "os.listdir", "ValueError", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "bool", "ValueError", "torch.distributed.barrier", "torch.distributed.barrier", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "main.load_and_cache_examples", "main.train", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "torch.save", "torch.save", "model_class.from_pretrained", "tokenizer_class.from_pretrained", "torch.nn.DataParallel.to", "list", "logging.getLogger().setLevel", "model_class.from_pretrained", "torch.nn.DataParallel.to", "main.evaluate", "dict", "results.update", "main.evaluate", "dict", "test_results.update", "int", "print", "open.write", "open.write", "open.write", "bool", "torch.nn.DataParallel", "torch.nn.DataParallel", "os.mkdir", "hasattr", "os.path.join", "dev_f1_values.append", "dev_loss_values.append", "test_f1_values.append", "test_loss_values.append", "torch.get_rank", "os.path.exists", "os.path.dirname", "logging.getLogger", "len", "checkpoint.split", "int", "test_f1_k.split", "torch.cuda.is_available", "torch.cuda.is_available", "sorted", "dict.items", "dict.items", "glob.glob"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.init_args", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.set_seed", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_labels", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.load_and_cache_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.train", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.evaluate", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.main.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "\n", "    ", "args", "=", "init_args", "(", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", "and", "not", "args", ".", "overwrite_output_dir", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "\n", "# Setup CUDA, GPU & distributed training", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "os", ".", "environ", "[", "'MASTER_ADDR'", "]", "=", "args", ".", "MASTER_ADDR", "\n", "os", ".", "environ", "[", "'MASTER_PORT'", "]", "=", "args", ".", "MASTER_PORT", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "rank", "=", "args", ".", "local_rank", ",", "world_size", "=", "1", ")", "\n", "args", ".", "n_gpu", "=", "1", "\n", "\n", "", "args", ".", "device", "=", "device", "\n", "\n", "# Setup logging", "\n", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s - %(levelname)s - %(name)s -   %(message)s'", ",", "\n", "datefmt", "=", "'%m/%d/%Y %H:%M:%S'", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ")", "\n", "# not using 16-bits training", "\n", "logger", ".", "warning", "(", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: False\"", ",", "\n", "args", ".", "local_rank", ",", "device", ",", "args", ".", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "args", ")", "\n", "\n", "# Prepare task", "\n", "args", ".", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "if", "args", ".", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "args", ".", "task_name", ")", "\n", "", "processor", "=", "processors", "[", "args", ".", "task_name", "]", "(", ")", "\n", "args", ".", "output_mode", "=", "output_modes", "[", "args", ".", "task_name", "]", "\n", "label_list", "=", "processor", ".", "get_labels", "(", "args", ".", "tagging_schema", ")", "\n", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "if", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "\n", "# initialize the pre-trained model", "\n", "", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "config_class", ",", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "config", "=", "config_class", ".", "from_pretrained", "(", "args", ".", "config_name", "if", "args", ".", "config_name", "else", "args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "finetuning_task", "=", "args", ".", "task_name", ",", "cache_dir", "=", "\"./cache\"", ")", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "tokenizer_name", "if", "args", ".", "tokenizer_name", "else", "args", ".", "model_name_or_path", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "cache_dir", "=", "'./cache'", ")", "\n", "\n", "config", ".", "absa_type", "=", "args", ".", "absa_type", "\n", "config", ".", "tfm_mode", "=", "args", ".", "tfm_mode", "\n", "config", ".", "fix_tfm", "=", "args", ".", "fix_tfm", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ",", "from_tf", "=", "bool", "(", "'.ckpt'", "in", "args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "cache_dir", "=", "'./cache'", ")", "\n", "# Distributed and parallel training", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "args", ".", "local_rank", ",", "\n", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "args", ".", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Training", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_dataset", ",", "train_evaluate_label_ids", "=", "load_and_cache_examples", "(", "args", ",", "args", ".", "task_name", ",", "tokenizer", ",", "mode", "=", "'train'", ")", "\n", "global_step", ",", "tr_loss", "=", "train", "(", "args", ",", "train_dataset", ",", "model", ",", "tokenizer", ")", "\n", "\n", "", "if", "args", ".", "do_train", "and", "(", "args", ".", "local_rank", "==", "-", "1", "or", "dist", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "# Create output directory if needed", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "os", ".", "mkdir", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "model_to_save", ".", "save_pretrained", "(", "args", ".", "output_dir", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "args", ".", "output_dir", ")", "\n", "\n", "# Good practice: save your training arguments together with the trained model", "\n", "# save the model configuration", "\n", "torch", ".", "save", "(", "args", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'training_args.bin'", ")", ")", "\n", "\n", "# Load a trained model and vocabulary that you have fine-tuned", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "output_dir", ")", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "output_dir", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "# Validation", "\n", "", "results", "=", "{", "}", "\n", "best_f1", "=", "-", "999999.0", "\n", "best_checkpoint", "=", "None", "\n", "checkpoints", "=", "[", "args", ".", "output_dir", "]", "\n", "if", "args", ".", "eval_all_checkpoints", ":", "\n", "        ", "checkpoints", "=", "list", "(", "os", ".", "path", ".", "dirname", "(", "c", ")", "for", "c", "in", "sorted", "(", "glob", ".", "glob", "(", "args", ".", "output_dir", "+", "'/**/'", "+", "WEIGHTS_NAME", ",", "recursive", "=", "True", ")", ")", ")", "\n", "logging", ".", "getLogger", "(", "\"pytorch_transformers.modeling_utils\"", ")", ".", "setLevel", "(", "logging", ".", "WARN", ")", "# Reduce logging", "\n", "", "logger", ".", "info", "(", "\"Perform validation on the following checkpoints: %s\"", ",", "checkpoints", ")", "\n", "test_results", "=", "{", "}", "\n", "for", "checkpoint", "in", "checkpoints", ":", "\n", "        ", "global_step", "=", "checkpoint", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "if", "len", "(", "checkpoints", ")", ">", "1", "else", "\"\"", "\n", "if", "global_step", "==", "'finetune'", "or", "global_step", "==", "'train'", "or", "global_step", "==", "'fix'", "or", "global_step", "==", "'overfit'", ":", "\n", "            ", "continue", "\n", "# validation set", "\n", "", "model", "=", "model_class", ".", "from_pretrained", "(", "checkpoint", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "dev_result", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "mode", "=", "'dev'", ",", "prefix", "=", "global_step", ")", "\n", "\n", "# regard the micro-f1 as the criteria of model selection", "\n", "if", "int", "(", "global_step", ")", ">", "1000", "and", "dev_result", "[", "'micro-f1'", "]", ">", "best_f1", ":", "\n", "            ", "best_f1", "=", "dev_result", "[", "'micro-f1'", "]", "\n", "best_checkpoint", "=", "checkpoint", "\n", "", "dev_result", "=", "dict", "(", "(", "k", "+", "'_{}'", ".", "format", "(", "global_step", ")", ",", "v", ")", "for", "k", ",", "v", "in", "dev_result", ".", "items", "(", ")", ")", "\n", "results", ".", "update", "(", "dev_result", ")", "\n", "\n", "test_result", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "mode", "=", "'test'", ",", "prefix", "=", "global_step", ")", "\n", "test_result", "=", "dict", "(", "(", "k", "+", "'_{}'", ".", "format", "(", "global_step", ")", ",", "v", ")", "for", "k", ",", "v", "in", "test_result", ".", "items", "(", ")", ")", "\n", "test_results", ".", "update", "(", "test_result", ")", "\n", "\n", "", "best_ckpt_string", "=", "\"\\nThe best checkpoint is %s\"", "%", "best_checkpoint", "\n", "logger", ".", "info", "(", "best_ckpt_string", ")", "\n", "dev_f1_values", ",", "dev_loss_values", "=", "[", "]", ",", "[", "]", "\n", "for", "k", "in", "results", ":", "\n", "        ", "v", "=", "results", "[", "k", "]", "\n", "if", "'micro-f1'", "in", "k", ":", "\n", "            ", "dev_f1_values", ".", "append", "(", "(", "k", ",", "v", ")", ")", "\n", "", "if", "'eval_loss'", "in", "k", ":", "\n", "            ", "dev_loss_values", ".", "append", "(", "(", "k", ",", "v", ")", ")", "\n", "", "", "test_f1_values", ",", "test_loss_values", "=", "[", "]", ",", "[", "]", "\n", "for", "k", "in", "test_results", ":", "\n", "        ", "v", "=", "test_results", "[", "k", "]", "\n", "if", "'micro-f1'", "in", "k", ":", "\n", "            ", "test_f1_values", ".", "append", "(", "(", "k", ",", "v", ")", ")", "\n", "", "if", "'eval_loss'", "in", "k", ":", "\n", "            ", "test_loss_values", ".", "append", "(", "(", "k", ",", "v", ")", ")", "\n", "", "", "log_file_path", "=", "'%s/log.txt'", "%", "args", ".", "output_dir", "\n", "log_file", "=", "open", "(", "log_file_path", ",", "'a'", ")", "\n", "log_file", ".", "write", "(", "\"\\tValidation:\\n\"", ")", "\n", "for", "(", "test_f1_k", ",", "test_f1_v", ")", ",", "(", "test_loss_k", ",", "test_loss_v", ")", ",", "(", "dev_f1_k", ",", "dev_f1_v", ")", ",", "(", "dev_loss_k", ",", "dev_loss_v", ")", "in", "zip", "(", "\n", "test_f1_values", ",", "test_loss_values", ",", "dev_f1_values", ",", "dev_loss_values", ")", ":", "\n", "        ", "global_step", "=", "int", "(", "test_f1_k", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", "\n", "if", "not", "args", ".", "overfit", "and", "global_step", "<=", "1000", ":", "\n", "            ", "continue", "\n", "", "print", "(", "'test-%s: %.5lf, test-%s: %.5lf, dev-%s: %.5lf, dev-%s: %.5lf'", "%", "(", "test_f1_k", ",", "\n", "test_f1_v", ",", "test_loss_k", ",", "test_loss_v", ",", "\n", "dev_f1_k", ",", "dev_f1_v", ",", "dev_loss_k", ",", "\n", "dev_loss_v", ")", ")", "\n", "validation_string", "=", "'\\t\\tdev-%s: %.5lf, dev-%s: %.5lf'", "%", "(", "dev_f1_k", ",", "dev_f1_v", ",", "dev_loss_k", ",", "dev_loss_v", ")", "\n", "log_file", ".", "write", "(", "validation_string", "+", "'\\n'", ")", "\n", "\n", "", "n_times", "=", "args", ".", "max_steps", "//", "args", ".", "save_steps", "+", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "n_times", ")", ":", "\n", "        ", "step", "=", "i", "*", "100", "\n", "log_file", ".", "write", "(", "'\\tStep %s:\\n'", "%", "step", ")", "\n", "precision", "=", "test_results", "[", "'precision_%s'", "%", "step", "]", "\n", "recall", "=", "test_results", "[", "'recall_%s'", "%", "step", "]", "\n", "micro_f1", "=", "test_results", "[", "'micro-f1_%s'", "%", "step", "]", "\n", "macro_f1", "=", "test_results", "[", "'macro-f1_%s'", "%", "step", "]", "\n", "log_file", ".", "write", "(", "'\\t\\tprecision: %.4lf, recall: %.4lf, micro-f1: %.4lf, macro-f1: %.4lf\\n'", "\n", "%", "(", "precision", ",", "recall", ",", "micro_f1", ",", "macro_f1", ")", ")", "\n", "", "log_file", ".", "write", "(", "\"\\tBest checkpoint: %s\\n\"", "%", "best_checkpoint", ")", "\n", "log_file", ".", "write", "(", "'******************************************\\n'", ")", "\n", "log_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.load_and_cache_examples": [[40, 80], ["glue_utils.ABSAProcessor", "os.path.join", "os.path.exists", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "print", "torch.load", "glue_utils.ABSAProcessor.get_test_examples", "glue_utils.ABSAProcessor.get_labels", "glue_utils.ABSAProcessor.get_test_examples", "glue_utils.convert_examples_to_seq_features", "torch.save", "total_words.append", "list().pop", "str", "str", "text.split", "bool", "bool", "list", "filter", "args.model_name_or_path.split"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_test_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_labels", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_test_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.convert_examples_to_seq_features"], ["def", "load_and_cache_examples", "(", "args", ",", "task", ",", "tokenizer", ")", ":", "\n", "# similar to that in main.py", "\n", "    ", "processor", "=", "ABSAProcessor", "(", ")", "\n", "# Load data features from cache or dataset file", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'cached_{}_{}_{}_{}'", ".", "format", "(", "\n", "'test'", ",", "\n", "list", "(", "filter", "(", "None", ",", "args", ".", "model_name_or_path", ".", "split", "(", "'/'", ")", ")", ")", ".", "pop", "(", ")", ",", "\n", "str", "(", "args", ".", "max_seq_length", ")", ",", "\n", "str", "(", "task", ")", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", ":", "\n", "        ", "print", "(", "\"cached_features_file:\"", ",", "cached_features_file", ")", "\n", "features", "=", "torch", ".", "load", "(", "cached_features_file", ")", "\n", "examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "tagging_schema", ")", "\n", "", "else", ":", "\n", "#logger.info(\"Creating features from dataset file at %s\", args.data_dir)", "\n", "        ", "label_list", "=", "processor", ".", "get_labels", "(", "args", ".", "tagging_schema", ")", "\n", "examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "tagging_schema", ")", "\n", "features", "=", "convert_examples_to_seq_features", "(", "examples", "=", "examples", ",", "label_list", "=", "label_list", ",", "tokenizer", "=", "tokenizer", ",", "\n", "cls_token_at_end", "=", "bool", "(", "args", ".", "model_type", "in", "[", "'xlnet'", "]", ")", ",", "\n", "cls_token", "=", "tokenizer", ".", "cls_token", ",", "\n", "sep_token", "=", "tokenizer", ".", "sep_token", ",", "\n", "cls_token_segment_id", "=", "2", "if", "args", ".", "model_type", "in", "[", "'xlnet'", "]", "else", "0", ",", "\n", "pad_on_left", "=", "bool", "(", "args", ".", "model_type", "in", "[", "'xlnet'", "]", ")", ",", "\n", "pad_token_segment_id", "=", "4", "if", "args", ".", "model_type", "in", "[", "'xlnet'", "]", "else", "0", ")", "\n", "torch", ".", "save", "(", "features", ",", "cached_features_file", ")", "\n", "", "total_words", "=", "[", "]", "\n", "for", "input_example", "in", "examples", ":", "\n", "        ", "text", "=", "input_example", ".", "text_a", "\n", "total_words", ".", "append", "(", "text", ".", "split", "(", "' '", ")", ")", "\n", "\n", "# Convert to Tensors and build dataset", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "# used in evaluation", "\n", "all_evaluate_label_ids", "=", "[", "f", ".", "evaluate_label_ids", "for", "f", "in", "features", "]", "\n", "dataset", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "return", "dataset", ",", "all_evaluate_label_ids", ",", "total_words", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.init_args": [[82, 104], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "MODEL_CLASSES.keys"], "function", ["None"], ["", "def", "init_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--absa_home\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"Home directory of the trained ABSA model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ckpt\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"Directory of model checkpoint for evaluation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"The incoming data dir. Should contain the files of test/unseen data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"task name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Model type selected in the list: \"", "+", "\", \"", ".", "join", "(", "MODEL_CLASSES", ".", "keys", "(", ")", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name selected in the list: \"", "+", "\", \"", ".", "join", "(", "ALL_MODELS", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after tokenization. Sequences longer \"", "\n", "\"than this will be truncated, sequences shorter will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tagging_schema'", ",", "type", "=", "str", ",", "default", "=", "'BIEOS'", ",", "help", "=", "\"Tagging schema, should be kept same with \"", "\n", "\"that of ckpt\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.main": [[106, 125], ["work.init_args", "torch.device", "torch.cuda.is_available", "init_args.model_type.lower", "print", "model_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained.to", "model_class.from_pretrained.eval", "work.predict", "torch.cuda.device_count", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.init_args", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.predict"], ["", "def", "main", "(", ")", ":", "\n", "# perform evaluation on single GPU", "\n", "    ", "args", "=", "init_args", "(", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "args", ".", "device", "=", "device", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "args", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "_", ",", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "\n", "# load the trained model (including the fine-tuned GPT/BERT/XLNET)", "\n", "print", "(", "\"Load checkpoint %s/%s...\"", "%", "(", "args", ".", "ckpt", ",", "WEIGHTS_NAME", ")", ")", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "ckpt", ")", "\n", "# follow the property of tokenizer in the loaded model, e.g., do_lower_case=True", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "absa_home", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "predict", "(", "args", ",", "model", ",", "tokenizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.predict": [[127, 196], ["work.load_and_cache_examples", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "print", "tqdm.tqdm", "tuple", "torch.no_grad", "model", "seq_utils.tag2ts", "print", "Exception", "t.to", "numpy.argmax", "model.tagger.viterbi_tags", "len", "len", "seq_utils.ot2bieos_ts", "output_ts.append", "logits.detach().cpu().numpy", "seq_utils.ot2bieos_ts", "inputs[].detach().cpu().numpy", "numpy.append", "seq_utils.bio2ot_ts", "inputs[].detach().cpu().numpy", "logits.detach().cpu", "inputs[].detach().cpu", "inputs[].detach().cpu", "logits.detach", "inputs[].detach", "inputs[].detach"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.work.load_and_cache_examples", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.tag2ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.viterbi_tags", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.bio2ot_ts"], ["", "def", "predict", "(", "args", ",", "model", ",", "tokenizer", ")", ":", "\n", "    ", "dataset", ",", "evaluate_label_ids", ",", "total_words", "=", "load_and_cache_examples", "(", "args", ",", "args", ".", "task_name", ",", "tokenizer", ")", "\n", "sampler", "=", "SequentialSampler", "(", "dataset", ")", "\n", "# process the incoming data one by one", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "1", ")", "\n", "print", "(", "\"***** Running prediction *****\"", ")", "\n", "\n", "total_preds", ",", "gold_labels", "=", "None", ",", "None", "\n", "idx", "=", "0", "\n", "if", "args", ".", "tagging_schema", "==", "'BIEOS'", ":", "\n", "        ", "absa_label_vocab", "=", "{", "'O'", ":", "0", ",", "'EQ'", ":", "1", ",", "'B-POS'", ":", "2", ",", "'I-POS'", ":", "3", ",", "'E-POS'", ":", "4", ",", "'S-POS'", ":", "5", ",", "\n", "'B-NEG'", ":", "6", ",", "'I-NEG'", ":", "7", ",", "'E-NEG'", ":", "8", ",", "'S-NEG'", ":", "9", ",", "\n", "'B-NEU'", ":", "10", ",", "'I-NEU'", ":", "11", ",", "'E-NEU'", ":", "12", ",", "'S-NEU'", ":", "13", "}", "\n", "", "elif", "args", ".", "tagging_schema", "==", "'BIO'", ":", "\n", "        ", "absa_label_vocab", "=", "{", "'O'", ":", "0", ",", "'EQ'", ":", "1", ",", "'B-POS'", ":", "2", ",", "'I-POS'", ":", "3", ",", "\n", "'B-NEG'", ":", "4", ",", "'I-NEG'", ":", "5", ",", "'B-NEU'", ":", "6", ",", "'I-NEU'", ":", "7", "}", "\n", "", "elif", "args", ".", "tagging_schema", "==", "'OT'", ":", "\n", "        ", "absa_label_vocab", "=", "{", "'O'", ":", "0", ",", "'EQ'", ":", "1", ",", "'T-POS'", ":", "2", ",", "'T-NEG'", ":", "3", ",", "'T-NEU'", ":", "4", "}", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Invalid tagging schema %s...\"", "%", "args", ".", "tagging_schema", ")", "\n", "", "absa_id2tag", "=", "{", "}", "\n", "for", "k", "in", "absa_label_vocab", ":", "\n", "        ", "v", "=", "absa_label_vocab", "[", "k", "]", "\n", "absa_id2tag", "[", "v", "]", "=", "k", "\n", "\n", "", "for", "batch", "in", "tqdm", "(", "dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "        ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "inputs", "=", "{", "'input_ids'", ":", "batch", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "batch", "[", "1", "]", ",", "\n", "'token_type_ids'", ":", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "'bert'", ",", "'xlnet'", "]", "else", "None", ",", "\n", "# XLM don't use segment_ids", "\n", "'labels'", ":", "batch", "[", "3", "]", "}", "\n", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "# logits: (1, seq_len, label_size)", "\n", "logits", "=", "outputs", "[", "1", "]", "\n", "# preds: (1, seq_len)", "\n", "if", "model", ".", "tagger_config", ".", "absa_type", "!=", "'crf'", ":", "\n", "                ", "preds", "=", "np", ".", "argmax", "(", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "mask", "=", "batch", "[", "1", "]", "\n", "preds", "=", "model", ".", "tagger", ".", "viterbi_tags", "(", "logits", "=", "logits", ",", "mask", "=", "mask", ")", "\n", "", "label_indices", "=", "evaluate_label_ids", "[", "idx", "]", "\n", "words", "=", "total_words", "[", "idx", "]", "\n", "pred_labels", "=", "preds", "[", "0", "]", "[", "label_indices", "]", "\n", "assert", "len", "(", "words", ")", "==", "len", "(", "pred_labels", ")", "\n", "pred_tags", "=", "[", "absa_id2tag", "[", "label", "]", "for", "label", "in", "pred_labels", "]", "\n", "\n", "if", "args", ".", "tagging_schema", "==", "'OT'", ":", "\n", "                ", "pred_tags", "=", "ot2bieos_ts", "(", "pred_tags", ")", "\n", "", "elif", "args", ".", "tagging_schema", "==", "'BIO'", ":", "\n", "                ", "pred_tags", "=", "ot2bieos_ts", "(", "bio2ot_ts", "(", "pred_tags", ")", ")", "\n", "", "else", ":", "\n", "# current tagging schema is BIEOS, do nothing", "\n", "                ", "pass", "\n", "", "p_ts_sequence", "=", "tag2ts", "(", "ts_tag_sequence", "=", "pred_tags", ")", "\n", "output_ts", "=", "[", "]", "\n", "for", "t", "in", "p_ts_sequence", ":", "\n", "                ", "beg", ",", "end", ",", "sentiment", "=", "t", "\n", "aspect", "=", "words", "[", "beg", ":", "end", "+", "1", "]", "\n", "output_ts", ".", "append", "(", "'%s: %s'", "%", "(", "aspect", ",", "sentiment", ")", ")", "\n", "", "print", "(", "\"Input: %s, output: %s\"", "%", "(", "' '", ".", "join", "(", "words", ")", ",", "'\\t'", ".", "join", "(", "output_ts", ")", ")", ")", "\n", "if", "inputs", "[", "'labels'", "]", "is", "not", "None", ":", "\n", "# for the unseen data, there is no ``labels''", "\n", "                ", "if", "gold_labels", "is", "None", ":", "\n", "                    ", "gold_labels", "=", "inputs", "[", "'labels'", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                    ", "gold_labels", "=", "np", ".", "append", "(", "gold_labels", ",", "inputs", "[", "'labels'", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "", "", "", "idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts": [[6, 46], ["len", "range", "new_ts_sequence.append", "cur_ts_tag.split", "new_ts_sequence.append", "new_ts_sequence.append", "new_ts_sequence.append", "new_ts_sequence.append", "new_ts_sequence.append", "new_ts_sequence.append"], "function", ["None"], ["def", "ot2bieos_ts", "(", "ts_tag_sequence", ")", ":", "\n", "    ", "\"\"\"\n    ot2bieos function for targeted-sentiment task, ts refers to targeted -sentiment / aspect-based sentiment\n    :param ts_tag_sequence: tag sequence for targeted sentiment\n    :return:\n    \"\"\"", "\n", "n_tags", "=", "len", "(", "ts_tag_sequence", ")", "\n", "new_ts_sequence", "=", "[", "]", "\n", "prev_pos", "=", "'$$$'", "\n", "for", "i", "in", "range", "(", "n_tags", ")", ":", "\n", "        ", "cur_ts_tag", "=", "ts_tag_sequence", "[", "i", "]", "\n", "if", "cur_ts_tag", "==", "'O'", "or", "cur_ts_tag", "==", "'EQ'", ":", "\n", "# when meet the EQ label, regard it as O label", "\n", "            ", "new_ts_sequence", ".", "append", "(", "'O'", ")", "\n", "cur_pos", "=", "'O'", "\n", "", "else", ":", "\n", "            ", "cur_pos", ",", "cur_sentiment", "=", "cur_ts_tag", ".", "split", "(", "'-'", ")", "\n", "# cur_pos is T", "\n", "if", "cur_pos", "!=", "prev_pos", ":", "\n", "# prev_pos is O and new_cur_pos can only be B or S", "\n", "                ", "if", "i", "==", "n_tags", "-", "1", ":", "\n", "                    ", "new_ts_sequence", ".", "append", "(", "'S-%s'", "%", "cur_sentiment", ")", "\n", "", "else", ":", "\n", "                    ", "next_ts_tag", "=", "ts_tag_sequence", "[", "i", "+", "1", "]", "\n", "if", "next_ts_tag", "==", "'O'", ":", "\n", "                        ", "new_ts_sequence", ".", "append", "(", "'S-%s'", "%", "cur_sentiment", ")", "\n", "", "else", ":", "\n", "                        ", "new_ts_sequence", ".", "append", "(", "'B-%s'", "%", "cur_sentiment", ")", "\n", "", "", "", "else", ":", "\n", "# prev_pos is T and new_cur_pos can only be I or E", "\n", "                ", "if", "i", "==", "n_tags", "-", "1", ":", "\n", "                    ", "new_ts_sequence", ".", "append", "(", "'E-%s'", "%", "cur_sentiment", ")", "\n", "", "else", ":", "\n", "                    ", "next_ts_tag", "=", "ts_tag_sequence", "[", "i", "+", "1", "]", "\n", "if", "next_ts_tag", "==", "'O'", ":", "\n", "                        ", "new_ts_sequence", ".", "append", "(", "'E-%s'", "%", "cur_sentiment", ")", "\n", "", "else", ":", "\n", "                        ", "new_ts_sequence", ".", "append", "(", "'I-%s'", "%", "cur_sentiment", ")", "\n", "", "", "", "", "prev_pos", "=", "cur_pos", "\n", "", "return", "new_ts_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts_batch": [[48, 60], ["len", "range", "seq_utils.ot2bieos_ts", "new_ts_tag_seqs.append"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts"], ["", "def", "ot2bieos_ts_batch", "(", "ts_tag_seqs", ")", ":", "\n", "    ", "\"\"\"\n    batch version of function ot2bieos_ts\n    :param ts_tag_seqs:\n    :return:\n    \"\"\"", "\n", "new_ts_tag_seqs", "=", "[", "]", "\n", "n_seqs", "=", "len", "(", "ts_tag_seqs", ")", "\n", "for", "i", "in", "range", "(", "n_seqs", ")", ":", "\n", "        ", "new_ts_seq", "=", "ot2bieos_ts", "(", "ts_tag_sequence", "=", "ts_tag_seqs", "[", "i", "]", ")", "\n", "new_ts_tag_seqs", ".", "append", "(", "new_ts_seq", ")", "\n", "", "return", "new_ts_tag_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bio_ts": [[62, 88], ["len", "range", "new_ts_sequence.append", "cur_ts_tag.split", "new_ts_sequence.append", "new_ts_sequence.append"], "function", ["None"], ["", "def", "ot2bio_ts", "(", "ts_tag_sequence", ")", ":", "\n", "    ", "\"\"\"\n    ot2bio function for ts tag sequence\n    :param ts_tag_sequence:\n    :return:\n    \"\"\"", "\n", "new_ts_sequence", "=", "[", "]", "\n", "n_tag", "=", "len", "(", "ts_tag_sequence", ")", "\n", "prev_pos", "=", "'$$$'", "\n", "for", "i", "in", "range", "(", "n_tag", ")", ":", "\n", "        ", "cur_ts_tag", "=", "ts_tag_sequence", "[", "i", "]", "\n", "if", "cur_ts_tag", "==", "'O'", ":", "\n", "            ", "new_ts_sequence", ".", "append", "(", "'O'", ")", "\n", "cur_pos", "=", "'O'", "\n", "", "else", ":", "\n", "# current tag is subjective tag, i.e., cur_pos is T", "\n", "# print(cur_ts_tag)", "\n", "            ", "cur_pos", ",", "cur_sentiment", "=", "cur_ts_tag", ".", "split", "(", "'-'", ")", "\n", "if", "cur_pos", "==", "prev_pos", ":", "\n", "# prev_pos is T", "\n", "                ", "new_ts_sequence", ".", "append", "(", "'I-%s'", "%", "cur_sentiment", ")", "\n", "", "else", ":", "\n", "# prev_pos is O", "\n", "                ", "new_ts_sequence", ".", "append", "(", "'B-%s'", "%", "cur_sentiment", ")", "\n", "", "", "prev_pos", "=", "cur_pos", "\n", "", "return", "new_ts_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bio_ts_batch": [[90, 102], ["len", "range", "seq_utils.ot2bio_ts", "new_ts_tag_seqs.append"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bio_ts"], ["", "def", "ot2bio_ts_batch", "(", "ts_tag_seqs", ")", ":", "\n", "    ", "\"\"\"\n    batch version of function ot2bio_ts\n    :param ts_tag_seqs:\n    :return:\n    \"\"\"", "\n", "new_ts_tag_seqs", "=", "[", "]", "\n", "n_seqs", "=", "len", "(", "ts_tag_seqs", ")", "\n", "for", "i", "in", "range", "(", "n_seqs", ")", ":", "\n", "        ", "new_ts_seq", "=", "ot2bio_ts", "(", "ts_tag_sequence", "=", "ts_tag_seqs", "[", "i", "]", ")", "\n", "new_ts_tag_seqs", ".", "append", "(", "new_ts_seq", ")", "\n", "", "return", "new_ts_tag_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.bio2ot_ts": [[104, 120], ["len", "range", "new_ts_sequence.append", "ts_tag.split", "new_ts_sequence.append"], "function", ["None"], ["", "def", "bio2ot_ts", "(", "ts_tag_sequence", ")", ":", "\n", "    ", "\"\"\"\n    perform bio-->ot for ts tag sequence\n    :param ts_tag_sequence:\n    :return:\n    \"\"\"", "\n", "new_ts_sequence", "=", "[", "]", "\n", "n_tags", "=", "len", "(", "ts_tag_sequence", ")", "\n", "for", "i", "in", "range", "(", "n_tags", ")", ":", "\n", "        ", "ts_tag", "=", "ts_tag_sequence", "[", "i", "]", "\n", "if", "ts_tag", "==", "'O'", "or", "ts_tag", "==", "'EQ'", ":", "\n", "            ", "new_ts_sequence", ".", "append", "(", "'O'", ")", "\n", "", "else", ":", "\n", "            ", "pos", ",", "sentiment", "=", "ts_tag", ".", "split", "(", "'-'", ")", "\n", "new_ts_sequence", ".", "append", "(", "'T-%s'", "%", "sentiment", ")", "\n", "", "", "return", "new_ts_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.bio2ot_ts_batch": [[122, 134], ["len", "range", "seq_utils.bio2ot_ts", "new_ts_tag_seqs.append"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.bio2ot_ts"], ["", "def", "bio2ot_ts_batch", "(", "ts_tag_seqs", ")", ":", "\n", "    ", "\"\"\"\n    batch version of function bio2ot_ts\n    :param ts_tag_seqs:\n    :return:\n    \"\"\"", "\n", "new_ts_tag_seqs", "=", "[", "]", "\n", "n_seqs", "=", "len", "(", "ts_tag_seqs", ")", "\n", "for", "i", "in", "range", "(", "n_seqs", ")", ":", "\n", "        ", "new_ts_seq", "=", "bio2ot_ts", "(", "ts_tag_sequence", "=", "ts_tag_seqs", "[", "i", "]", ")", "\n", "new_ts_tag_seqs", ".", "append", "(", "new_ts_seq", ")", "\n", "", "return", "new_ts_tag_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.tag2ts": [[136, 175], ["len", "range", "ts_tag.split", "len", "sentiments.append", "ts_sequence.append", "len", "ts_sequence.append", "len", "set"], "function", ["None"], ["", "def", "tag2ts", "(", "ts_tag_sequence", ")", ":", "\n", "    ", "\"\"\"\n    transform ts tag sequence to targeted sentiment\n    :param ts_tag_sequence: tag sequence for ts task\n    :return:\n    \"\"\"", "\n", "n_tags", "=", "len", "(", "ts_tag_sequence", ")", "\n", "ts_sequence", ",", "sentiments", "=", "[", "]", ",", "[", "]", "\n", "beg", ",", "end", "=", "-", "1", ",", "-", "1", "\n", "for", "i", "in", "range", "(", "n_tags", ")", ":", "\n", "        ", "ts_tag", "=", "ts_tag_sequence", "[", "i", "]", "\n", "# current position and sentiment", "\n", "# tag O and tag EQ will not be counted", "\n", "eles", "=", "ts_tag", ".", "split", "(", "'-'", ")", "\n", "if", "len", "(", "eles", ")", "==", "2", ":", "\n", "            ", "pos", ",", "sentiment", "=", "eles", "\n", "", "else", ":", "\n", "            ", "pos", ",", "sentiment", "=", "'O'", ",", "'O'", "\n", "", "if", "sentiment", "!=", "'O'", ":", "\n", "# current word is a subjective word", "\n", "            ", "sentiments", ".", "append", "(", "sentiment", ")", "\n", "", "if", "pos", "==", "'S'", ":", "\n", "# singleton", "\n", "            ", "ts_sequence", ".", "append", "(", "(", "i", ",", "i", ",", "sentiment", ")", ")", "\n", "sentiments", "=", "[", "]", "\n", "", "elif", "pos", "==", "'B'", ":", "\n", "            ", "beg", "=", "i", "\n", "if", "len", "(", "sentiments", ")", ">", "1", ":", "\n", "# remove the effect of the noisy I-{POS,NEG,NEU}", "\n", "                ", "sentiments", "=", "[", "sentiments", "[", "-", "1", "]", "]", "\n", "", "", "elif", "pos", "==", "'E'", ":", "\n", "            ", "end", "=", "i", "\n", "# schema1: only the consistent sentiment tags are accepted", "\n", "# that is, all of the sentiment tags are the same", "\n", "if", "end", ">", "beg", ">", "-", "1", "and", "len", "(", "set", "(", "sentiments", ")", ")", "==", "1", ":", "\n", "                ", "ts_sequence", ".", "append", "(", "(", "beg", ",", "end", ",", "sentiment", ")", ")", "\n", "sentiments", "=", "[", "]", "\n", "beg", ",", "end", "=", "-", "1", ",", "-", "1", "\n", "", "", "", "return", "ts_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.logsumexp": [[177, 191], ["tensor.max", "stable_vec.exp().sum().log", "max_score.unsqueeze", "stable_vec.exp().sum", "stable_vec.exp"], "function", ["None"], ["", "def", "logsumexp", "(", "tensor", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n\n    :param tensor:\n    :param dim:\n    :param keepdim:\n    :return:\n    \"\"\"", "\n", "max_score", ",", "_", "=", "tensor", ".", "max", "(", "dim", ",", "keepdim", "=", "keepdim", ")", "\n", "if", "keepdim", ":", "\n", "        ", "stable_vec", "=", "tensor", "-", "max_score", "\n", "", "else", ":", "\n", "        ", "stable_vec", "=", "tensor", "-", "max_score", ".", "unsqueeze", "(", "dim", ")", "\n", "", "return", "max_score", "+", "(", "stable_vec", ".", "exp", "(", ")", ".", "sum", "(", "dim", ",", "keepdim", "=", "keepdim", ")", ")", ".", "log", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.viterbi_decode": [[193, 327], ["list", "range", "torch.max", "reversed", "viterbi_path.reverse", "numpy.array", "torch.cat.size", "torch.zeros", "torch.cat", "torch.cat", "torch.zeros", "torch.cat", "torch.cat", "torch.cat.size", "torch.zeros", "path_scores.append", "path_scores.append", "torch.max", "path_indices.append", "int", "viterbi_path.append", "torch.zeros", "torch.zeros", "len", "Exception", "torch.ones", "path_scores[].unsqueeze", "torch.zeros", "path_scores.append", "path_scores.append", "paths.squeeze", "best_path.numpy", "int", "torch.tensor", "torch.tensor", "range", "logger.warning", "scores.squeeze"], "function", ["None"], ["", "def", "viterbi_decode", "(", "tag_sequence", ",", "transition_matrix", ",", "\n", "tag_observations", "=", "None", ",", "allowed_start_transitions", "=", "None", ",", "\n", "allowed_end_transitions", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Perform Viterbi decoding in log space over a sequence given a transition matrix\n    specifying pairwise (transition) potentials between tags and a matrix of shape\n    (sequence_length, num_tags) specifying unary potentials for possible tags per\n    timestep.\n    Parameters\n    ----------\n    tag_sequence : torch.Tensor, required.\n        A tensor of shape (sequence_length, num_tags) representing scores for\n        a set of tags over a given sequence.\n    transition_matrix : torch.Tensor, required.\n        A tensor of shape (num_tags, num_tags) representing the binary potentials\n        for transitioning between a given pair of tags.\n    tag_observations : Optional[List[int]], optional, (default = None)\n        A list of length ``sequence_length`` containing the class ids of observed\n        elements in the sequence, with unobserved elements being set to -1. Note that\n        it is possible to provide evidence which results in degenerate labelings if\n        the sequences of tags you provide as evidence cannot transition between each\n        other, or those transitions are extremely unlikely. In this situation we log a\n        warning, but the responsibility for providing self-consistent evidence ultimately\n        lies with the user.\n    allowed_start_transitions : torch.Tensor, optional, (default = None)\n        An optional tensor of shape (num_tags,) describing which tags the START token\n        may transition *to*. If provided, additional transition constraints will be used for\n        determining the start element of the sequence.\n    allowed_end_transitions : torch.Tensor, optional, (default = None)\n        An optional tensor of shape (num_tags,) describing which tags may transition *to* the\n        end tag. If provided, additional transition constraints will be used for determining\n        the end element of the sequence.\n    Returns\n    -------\n    viterbi_path : List[int]\n        The tag indices of the maximum likelihood tag sequence.\n    viterbi_score : torch.Tensor\n        The score of the viterbi path.\n    \"\"\"", "\n", "sequence_length", ",", "num_tags", "=", "list", "(", "tag_sequence", ".", "size", "(", ")", ")", "\n", "\n", "has_start_end_restrictions", "=", "allowed_end_transitions", "is", "not", "None", "or", "allowed_start_transitions", "is", "not", "None", "\n", "\n", "if", "has_start_end_restrictions", ":", "\n", "\n", "        ", "if", "allowed_end_transitions", "is", "None", ":", "\n", "            ", "allowed_end_transitions", "=", "torch", ".", "zeros", "(", "num_tags", ")", "\n", "", "if", "allowed_start_transitions", "is", "None", ":", "\n", "            ", "allowed_start_transitions", "=", "torch", ".", "zeros", "(", "num_tags", ")", "\n", "\n", "", "num_tags", "=", "num_tags", "+", "2", "\n", "new_transition_matrix", "=", "torch", ".", "zeros", "(", "num_tags", ",", "num_tags", ")", "\n", "new_transition_matrix", "[", ":", "-", "2", ",", ":", "-", "2", "]", "=", "transition_matrix", "\n", "\n", "# Start and end transitions are fully defined, but cannot transition between each other.", "\n", "# pylint: disable=not-callable", "\n", "allowed_start_transitions", "=", "torch", ".", "cat", "(", "[", "allowed_start_transitions", ",", "torch", ".", "tensor", "(", "[", "-", "math", ".", "inf", ",", "-", "math", ".", "inf", "]", ")", "]", ")", "\n", "allowed_end_transitions", "=", "torch", ".", "cat", "(", "[", "allowed_end_transitions", ",", "torch", ".", "tensor", "(", "[", "-", "math", ".", "inf", ",", "-", "math", ".", "inf", "]", ")", "]", ")", "\n", "# pylint: enable=not-callable", "\n", "\n", "# First define how we may transition FROM the start and end tags.", "\n", "new_transition_matrix", "[", "-", "2", ",", ":", "]", "=", "allowed_start_transitions", "\n", "# We cannot transition from the end tag to any tag.", "\n", "new_transition_matrix", "[", "-", "1", ",", ":", "]", "=", "-", "math", ".", "inf", "\n", "\n", "new_transition_matrix", "[", ":", ",", "-", "1", "]", "=", "allowed_end_transitions", "\n", "# We cannot transition to the start tag from any tag.", "\n", "new_transition_matrix", "[", ":", ",", "-", "2", "]", "=", "-", "math", ".", "inf", "\n", "\n", "transition_matrix", "=", "new_transition_matrix", "\n", "\n", "", "if", "tag_observations", ":", "\n", "        ", "if", "len", "(", "tag_observations", ")", "!=", "sequence_length", ":", "\n", "            ", "raise", "Exception", "(", "\"Observations were provided, but they were not the same length \"", "\n", "\"as the sequence. Found sequence of length: {} and evidence: {}\"", "\n", ".", "format", "(", "sequence_length", ",", "tag_observations", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "tag_observations", "=", "[", "-", "1", "for", "_", "in", "range", "(", "sequence_length", ")", "]", "\n", "\n", "\n", "", "if", "has_start_end_restrictions", ":", "\n", "        ", "tag_observations", "=", "[", "num_tags", "-", "2", "]", "+", "tag_observations", "+", "[", "num_tags", "-", "1", "]", "\n", "zero_sentinel", "=", "torch", ".", "zeros", "(", "1", ",", "num_tags", ")", "\n", "extra_tags_sentinel", "=", "torch", ".", "ones", "(", "sequence_length", ",", "2", ")", "*", "-", "math", ".", "inf", "\n", "tag_sequence", "=", "torch", ".", "cat", "(", "[", "tag_sequence", ",", "extra_tags_sentinel", "]", ",", "-", "1", ")", "\n", "tag_sequence", "=", "torch", ".", "cat", "(", "[", "zero_sentinel", ",", "tag_sequence", ",", "zero_sentinel", "]", ",", "0", ")", "\n", "sequence_length", "=", "tag_sequence", ".", "size", "(", "0", ")", "\n", "\n", "", "path_scores", "=", "[", "]", "\n", "path_indices", "=", "[", "]", "\n", "\n", "if", "tag_observations", "[", "0", "]", "!=", "-", "1", ":", "\n", "        ", "one_hot", "=", "torch", ".", "zeros", "(", "num_tags", ")", "\n", "one_hot", "[", "tag_observations", "[", "0", "]", "]", "=", "100000.", "\n", "path_scores", ".", "append", "(", "one_hot", ")", "\n", "", "else", ":", "\n", "        ", "path_scores", ".", "append", "(", "tag_sequence", "[", "0", ",", ":", "]", ")", "\n", "\n", "# Evaluate the scores for all possible paths.", "\n", "", "for", "timestep", "in", "range", "(", "1", ",", "sequence_length", ")", ":", "\n", "# Add pairwise potentials to current scores.", "\n", "        ", "summed_potentials", "=", "path_scores", "[", "timestep", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "+", "transition_matrix", "\n", "scores", ",", "paths", "=", "torch", ".", "max", "(", "summed_potentials", ",", "0", ")", "\n", "\n", "# If we have an observation for this timestep, use it", "\n", "# instead of the distribution over tags.", "\n", "observation", "=", "tag_observations", "[", "timestep", "]", "\n", "# Warn the user if they have passed", "\n", "# invalid/extremely unlikely evidence.", "\n", "if", "tag_observations", "[", "timestep", "-", "1", "]", "!=", "-", "1", "and", "observation", "!=", "-", "1", ":", "\n", "            ", "if", "transition_matrix", "[", "tag_observations", "[", "timestep", "-", "1", "]", ",", "observation", "]", "<", "-", "10000", ":", "\n", "                ", "logger", ".", "warning", "(", "\"The pairwise potential between tags you have passed as \"", "\n", "\"observations is extremely unlikely. Double check your evidence \"", "\n", "\"or transition potentials!\"", ")", "\n", "", "", "if", "observation", "!=", "-", "1", ":", "\n", "            ", "one_hot", "=", "torch", ".", "zeros", "(", "num_tags", ")", "\n", "one_hot", "[", "observation", "]", "=", "100000.", "\n", "path_scores", ".", "append", "(", "one_hot", ")", "\n", "", "else", ":", "\n", "            ", "path_scores", ".", "append", "(", "tag_sequence", "[", "timestep", ",", ":", "]", "+", "scores", ".", "squeeze", "(", ")", ")", "\n", "", "path_indices", ".", "append", "(", "paths", ".", "squeeze", "(", ")", ")", "\n", "\n", "# Construct the most likely sequence backwards.", "\n", "", "viterbi_score", ",", "best_path", "=", "torch", ".", "max", "(", "path_scores", "[", "-", "1", "]", ",", "0", ")", "\n", "viterbi_path", "=", "[", "int", "(", "best_path", ".", "numpy", "(", ")", ")", "]", "\n", "for", "backward_timestep", "in", "reversed", "(", "path_indices", ")", ":", "\n", "        ", "viterbi_path", ".", "append", "(", "int", "(", "backward_timestep", "[", "viterbi_path", "[", "-", "1", "]", "]", ")", ")", "\n", "# Reverse the backward path.", "\n", "", "viterbi_path", ".", "reverse", "(", ")", "\n", "\n", "if", "has_start_end_restrictions", ":", "\n", "        ", "viterbi_path", "=", "viterbi_path", "[", "1", ":", "-", "1", "]", "\n", "#return viterbi_path, viterbi_score", "\n", "", "return", "np", ".", "array", "(", "viterbi_path", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert_utils.build_tf_xlnet_to_pytorch_map": [[23, 92], ["hasattr", "tf_to_pt_map.update", "enumerate", "tf_to_pt_map.update", "hasattr", "tf_to_pt_map.update", "hasattr", "hasattr", "r_r_list.append", "r_w_list.append", "r_s_list.append", "seg_embed_list.append"], "function", ["None"], ["def", "build_tf_xlnet_to_pytorch_map", "(", "model", ",", "config", ",", "tf_weights", "=", "None", ")", ":", "\n", "    ", "\"\"\" A map of modules from TF to PyTorch.\n        I use a map to keep the PyTorch model as\n        identical to the original PyTorch model as possible.\n    \"\"\"", "\n", "tf_to_pt_map", "=", "{", "}", "\n", "\n", "if", "hasattr", "(", "model", ",", "'transformer'", ")", ":", "\n", "        ", "if", "hasattr", "(", "model", ",", "'lm_loss'", ")", ":", "\n", "# We will load also the output bias", "\n", "            ", "tf_to_pt_map", "[", "'model/lm_loss/bias'", "]", "=", "model", ".", "lm_loss", ".", "bias", "\n", "", "if", "hasattr", "(", "model", ",", "'sequence_summary'", ")", "and", "'model/sequnece_summary/summary/kernel'", "in", "tf_weights", ":", "\n", "# We will load also the sequence summary", "\n", "            ", "tf_to_pt_map", "[", "'model/sequnece_summary/summary/kernel'", "]", "=", "model", ".", "sequence_summary", ".", "summary", ".", "weight", "\n", "tf_to_pt_map", "[", "'model/sequnece_summary/summary/bias'", "]", "=", "model", ".", "sequence_summary", ".", "summary", ".", "bias", "\n", "", "if", "hasattr", "(", "model", ",", "'logits_proj'", ")", "and", "config", ".", "finetuning_task", "is", "not", "None", "and", "'model/regression_{}/logit/kernel'", ".", "format", "(", "config", ".", "finetuning_task", ")", "in", "tf_weights", ":", "\n", "            ", "tf_to_pt_map", "[", "'model/regression_{}/logit/kernel'", ".", "format", "(", "config", ".", "finetuning_task", ")", "]", "=", "model", ".", "logits_proj", ".", "weight", "\n", "tf_to_pt_map", "[", "'model/regression_{}/logit/bias'", ".", "format", "(", "config", ".", "finetuning_task", ")", "]", "=", "model", ".", "logits_proj", ".", "bias", "\n", "\n", "# Now load the rest of the transformer", "\n", "", "model", "=", "model", ".", "transformer", "\n", "\n", "# Embeddings and output", "\n", "", "tf_to_pt_map", ".", "update", "(", "{", "'model/transformer/word_embedding/lookup_table'", ":", "model", ".", "word_embedding", ".", "weight", ",", "\n", "'model/transformer/mask_emb/mask_emb'", ":", "model", ".", "mask_emb", "}", ")", "\n", "\n", "# Transformer blocks", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "model", ".", "layer", ")", ":", "\n", "        ", "layer_str", "=", "\"model/transformer/layer_%d/\"", "%", "i", "\n", "tf_to_pt_map", ".", "update", "(", "{", "\n", "layer_str", "+", "\"rel_attn/LayerNorm/gamma\"", ":", "b", ".", "rel_attn", ".", "layer_norm", ".", "weight", ",", "\n", "layer_str", "+", "\"rel_attn/LayerNorm/beta\"", ":", "b", ".", "rel_attn", ".", "layer_norm", ".", "bias", ",", "\n", "layer_str", "+", "\"rel_attn/o/kernel\"", ":", "b", ".", "rel_attn", ".", "o", ",", "\n", "layer_str", "+", "\"rel_attn/q/kernel\"", ":", "b", ".", "rel_attn", ".", "q", ",", "\n", "layer_str", "+", "\"rel_attn/k/kernel\"", ":", "b", ".", "rel_attn", ".", "k", ",", "\n", "layer_str", "+", "\"rel_attn/r/kernel\"", ":", "b", ".", "rel_attn", ".", "r", ",", "\n", "layer_str", "+", "\"rel_attn/v/kernel\"", ":", "b", ".", "rel_attn", ".", "v", ",", "\n", "layer_str", "+", "\"ff/LayerNorm/gamma\"", ":", "b", ".", "ff", ".", "layer_norm", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/LayerNorm/beta\"", ":", "b", ".", "ff", ".", "layer_norm", ".", "bias", ",", "\n", "layer_str", "+", "\"ff/layer_1/kernel\"", ":", "b", ".", "ff", ".", "layer_1", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/layer_1/bias\"", ":", "b", ".", "ff", ".", "layer_1", ".", "bias", ",", "\n", "layer_str", "+", "\"ff/layer_2/kernel\"", ":", "b", ".", "ff", ".", "layer_2", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/layer_2/bias\"", ":", "b", ".", "ff", ".", "layer_2", ".", "bias", ",", "\n", "}", ")", "\n", "\n", "# Relative positioning biases", "\n", "", "if", "config", ".", "untie_r", ":", "\n", "        ", "r_r_list", "=", "[", "]", "\n", "r_w_list", "=", "[", "]", "\n", "r_s_list", "=", "[", "]", "\n", "seg_embed_list", "=", "[", "]", "\n", "for", "b", "in", "model", ".", "layer", ":", "\n", "            ", "r_r_list", ".", "append", "(", "b", ".", "rel_attn", ".", "r_r_bias", ")", "\n", "r_w_list", ".", "append", "(", "b", ".", "rel_attn", ".", "r_w_bias", ")", "\n", "r_s_list", ".", "append", "(", "b", ".", "rel_attn", ".", "r_s_bias", ")", "\n", "seg_embed_list", ".", "append", "(", "b", ".", "rel_attn", ".", "seg_embed", ")", "\n", "", "", "else", ":", "\n", "        ", "r_r_list", "=", "[", "model", ".", "r_r_bias", "]", "\n", "r_w_list", "=", "[", "model", ".", "r_w_bias", "]", "\n", "r_s_list", "=", "[", "model", ".", "r_s_bias", "]", "\n", "seg_embed_list", "=", "[", "model", ".", "seg_embed", "]", "\n", "\n", "", "tf_to_pt_map", ".", "update", "(", "{", "\n", "'model/transformer/r_r_bias'", ":", "r_r_list", ",", "\n", "'model/transformer/r_w_bias'", ":", "r_w_list", ",", "\n", "'model/transformer/r_s_bias'", ":", "r_s_list", ",", "\n", "'model/transformer/seg_embed'", ":", "seg_embed_list", "}", ")", "\n", "return", "tf_to_pt_map", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert_utils.load_tf_weights_in_bert": [[94, 159], ["os.path.abspath", "logger.info", "tf.train.list_variables", "zip", "logger.info", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "logger.info", "torch.from_numpy", "logger.error", "logger.info", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr", "getattr", "logger.info"], "function", ["None"], ["", "def", "load_tf_weights_in_bert", "(", "model", ",", "config", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "logger", ".", "info", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", ",", "\"global_step\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'squad'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'classifier'", ")", "\n", "", "else", ":", "\n", "                ", "try", ":", "\n", "                    ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "except", "AttributeError", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert_utils.load_tf_weights_in_xlnet": [[161, 220], ["tf.train.list_variables", "bert_utils.build_tf_xlnet_to_pytorch_map", "build_tf_xlnet_to_pytorch_map.items", "logger.info", "logger.info", "tf.train.load_variable", "logger.info", "isinstance", "tf_weights.pop", "tf_weights.pop", "tf_weights.pop", "logger.error", "logger.info", "logger.info", "np.transpose", "enumerate", "logger.info", "torch.from_numpy", "len", "logger.info", "torch.from_numpy", "tf_weights.keys"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.bert_utils.build_tf_xlnet_to_pytorch_map"], ["", "def", "load_tf_weights_in_xlnet", "(", "model", ",", "config", ",", "tf_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. \"", "\n", "\"Please see https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "\n", "# load weights from TF model", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "tf_weights", "=", "{", "}", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "tf_weights", "[", "name", "]", "=", "array", "\n", "\n", "# Build TF to PyTorch weights loading map", "\n", "", "tf_to_pt_map", "=", "build_tf_xlnet_to_pytorch_map", "(", "model", ",", "config", ",", "tf_weights", ")", "\n", "\n", "for", "name", ",", "pointer", "in", "tf_to_pt_map", ".", "items", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Importing {}\"", ".", "format", "(", "name", ")", ")", "\n", "if", "name", "not", "in", "tf_weights", ":", "\n", "            ", "logger", ".", "info", "(", "\"{} not in tf pre-trained weights, skipping\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "array", "=", "tf_weights", "[", "name", "]", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "'kernel'", "in", "name", "and", "(", "'ff'", "in", "name", "or", "'summary'", "in", "name", "or", "'logit'", "in", "name", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Transposing\"", ")", "\n", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "\n", "", "if", "isinstance", "(", "pointer", ",", "list", ")", ":", "\n", "# Here we will split the TF weigths", "\n", "            ", "assert", "len", "(", "pointer", ")", "==", "array", ".", "shape", "[", "0", "]", "\n", "for", "i", ",", "p_i", "in", "enumerate", "(", "pointer", ")", ":", "\n", "                ", "arr_i", "=", "array", "[", "i", ",", "...", "]", "\n", "try", ":", "\n", "                    ", "assert", "p_i", ".", "shape", "==", "arr_i", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "                    ", "e", ".", "args", "+=", "(", "p_i", ".", "shape", ",", "arr_i", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {} for layer {}\"", ".", "format", "(", "name", ",", "i", ")", ")", "\n", "p_i", ".", "data", "=", "torch", ".", "from_numpy", "(", "arr_i", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "                ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "tf_weights", ".", "pop", "(", "name", ",", "None", ")", "\n", "tf_weights", ".", "pop", "(", "name", "+", "'/Adam'", ",", "None", ")", "\n", "tf_weights", ".", "pop", "(", "name", "+", "'/Adam_1'", ",", "None", ")", "\n", "", "logger", ".", "info", "(", "\"Weights not copied to PyTorch model: {}\"", ".", "format", "(", "', '", ".", "join", "(", "tf_weights", ".", "keys", "(", ")", ")", ")", ")", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.TaggerConfig.__init__": [[10, 15], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "hidden_dropout_prob", "=", "0.1", "\n", "self", ".", "hidden_size", "=", "768", "\n", "self", ".", "n_rnn_layers", "=", "1", "# not used if tagger is non-RNN model", "\n", "self", ".", "bidirectional", "=", "True", "# not used if tagger is non-RNN model", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.SAN.__init__": [[18, 25], ["torch.Module.__init__", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.Dropout", "torch.Dropout", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "SAN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "nhead", "=", "nhead", "\n", "self", ".", "self_attn", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "d_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.SAN.forward": [[26, 39], ["absa_layer.SAN.self_attn", "absa_layer.SAN.norm", "absa_layer.SAN.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param src:\n        :param src_mask:\n        :param src_key_padding_mask:\n        :return:\n        \"\"\"", "\n", "src2", ",", "_", "=", "self", ".", "self_attn", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "src_mask", ",", "key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout", "(", "src2", ")", "\n", "# apply layer normalization", "\n", "src", "=", "self", ".", "norm", "(", "src", ")", "\n", "return", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.GRU.__init__": [[43, 65], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "bidirectional", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n\n        :param input_size:\n        :param hidden_size:\n        :param bidirectional:\n        \"\"\"", "\n", "super", "(", "GRU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "if", "bidirectional", ":", "\n", "            ", "self", ".", "hidden_size", "=", "hidden_size", "//", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "Wxrz", "=", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "input_size", ",", "out_features", "=", "2", "*", "self", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "Whrz", "=", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "hidden_size", ",", "out_features", "=", "2", "*", "self", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "Wxn", "=", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "input_size", ",", "out_features", "=", "self", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "Whn", "=", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "hidden_size", ",", "out_features", "=", "self", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "LNx1", "=", "nn", ".", "LayerNorm", "(", "2", "*", "self", ".", "hidden_size", ")", "\n", "self", ".", "LNh1", "=", "nn", ".", "LayerNorm", "(", "2", "*", "self", ".", "hidden_size", ")", "\n", "self", ".", "LNx2", "=", "nn", ".", "LayerNorm", "(", "self", ".", "hidden_size", ")", "\n", "self", ".", "LNh2", "=", "nn", ".", "LayerNorm", "(", "self", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.GRU.forward": [[66, 107], ["range", "x.size", "absa_layer.GRU.init_hidden", "x.transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.chunk", "torch.sigmoid.chunk", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "x.size", "absa_layer.GRU.forward.recurrence"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.LSTM.init_hidden"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n\n        :param x: input tensor, shape: (batch_size, seq_len, input_size)\n        :return:\n        \"\"\"", "\n", "def", "recurrence", "(", "xt", ",", "htm1", ")", ":", "\n", "            ", "\"\"\"\n\n            :param xt: current input\n            :param htm1: previous hidden state\n            :return:\n            \"\"\"", "\n", "gates_rz", "=", "torch", ".", "sigmoid", "(", "self", ".", "LNx1", "(", "self", ".", "Wxrz", "(", "xt", ")", ")", "+", "self", ".", "LNh1", "(", "self", ".", "Whrz", "(", "htm1", ")", ")", ")", "\n", "rt", ",", "zt", "=", "gates_rz", ".", "chunk", "(", "2", ",", "1", ")", "\n", "nt", "=", "torch", ".", "tanh", "(", "self", ".", "LNx2", "(", "self", ".", "Wxn", "(", "xt", ")", ")", "+", "rt", "*", "self", ".", "LNh2", "(", "self", ".", "Whn", "(", "htm1", ")", ")", ")", "\n", "ht", "=", "(", "1.0", "-", "zt", ")", "*", "nt", "+", "zt", "*", "htm1", "\n", "return", "ht", "\n", "\n", "", "steps", "=", "range", "(", "x", ".", "size", "(", "1", ")", ")", "\n", "bs", "=", "x", ".", "size", "(", "0", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "bs", ")", "\n", "# shape: (seq_len, bsz, input_size)", "\n", "input", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "output", "=", "[", "]", "\n", "for", "t", "in", "steps", ":", "\n", "            ", "hidden", "=", "recurrence", "(", "input", "[", "t", "]", ",", "hidden", ")", "\n", "output", ".", "append", "(", "hidden", ")", "\n", "# shape: (bsz, seq_len, input_size)", "\n", "", "output", "=", "torch", ".", "stack", "(", "output", ",", "0", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "output_b", "=", "[", "]", "\n", "hidden_b", "=", "self", ".", "init_hidden", "(", "bs", ")", "\n", "for", "t", "in", "steps", "[", ":", ":", "-", "1", "]", ":", "\n", "                ", "hidden_b", "=", "recurrence", "(", "input", "[", "t", "]", ",", "hidden_b", ")", "\n", "output_b", ".", "append", "(", "hidden_b", ")", "\n", "", "output_b", "=", "output_b", "[", ":", ":", "-", "1", "]", "\n", "output_b", "=", "torch", ".", "stack", "(", "output_b", ",", "0", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "output", "=", "torch", ".", "cat", "(", "[", "output", ",", "output_b", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.GRU.init_hidden": [[108, 111], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "bs", ")", ":", "\n", "        ", "h_0", "=", "torch", ".", "zeros", "(", "bs", ",", "self", ".", "hidden_size", ")", ".", "cuda", "(", ")", "\n", "return", "h_0", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.__init__": [[116, 134], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Parameter", "torch.Parameter", "absa_layer.CRF.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_tags", ",", "constraints", "=", "None", ",", "include_start_end_transitions", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param num_tags:\n        :param constraints:\n        :param include_start_end_transitions:\n        \"\"\"", "\n", "super", "(", "CRF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_tags", "=", "num_tags", "\n", "self", ".", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", "self", ".", "transitions", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_tags", ",", "self", ".", "num_tags", ")", ")", "\n", "constraint_mask", "=", "torch", ".", "Tensor", "(", "self", ".", "num_tags", "+", "2", ",", "self", ".", "num_tags", "+", "2", ")", ".", "fill_", "(", "1.", ")", "\n", "if", "include_start_end_transitions", ":", "\n", "            ", "self", ".", "start_transitions", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_tags", ")", ")", "\n", "self", ".", "end_transitions", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_tags", ")", ")", "\n", "# register the constraint_mask", "\n", "", "self", ".", "constraint_mask", "=", "nn", ".", "Parameter", "(", "constraint_mask", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.forward": [[135, 148], ["absa_layer.CRF._input_likelihood", "absa_layer.CRF._joint_likelihood", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "tags.size"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF._input_likelihood", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF._joint_likelihood"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "tags", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param inputs: (bsz, seq_len, num_tags), logits calculated from a linear layer\n        :param tags: (bsz, seq_len)\n        :param mask: (bsz, seq_len), mask for the padding token\n        :return:\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "*", "tags", ".", "size", "(", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "log_denominator", "=", "self", ".", "_input_likelihood", "(", "inputs", ",", "mask", ")", "\n", "log_numerator", "=", "self", ".", "_joint_likelihood", "(", "inputs", ",", "tags", ",", "mask", ")", "\n", "return", "torch", ".", "sum", "(", "log_numerator", "-", "log_denominator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.reset_parameters": [[149, 158], ["torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        initialize the parameters in CRF\n        :return:\n        \"\"\"", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "transitions", ")", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "start_transitions", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "end_transitions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF._input_likelihood": [[159, 197], ["logits.transpose().contiguous.transpose().contiguous.size", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose().contiguous", "logits.transpose().contiguous.transpose().contiguous.transpose().contiguous", "range", "seq_utils.logsumexp", "logits[].view", "absa_layer.CRF.transitions.view", "alpha.view", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose", "logits.transpose().contiguous.transpose().contiguous.transpose", "absa_layer.CRF.start_transitions.view", "absa_layer.CRF.end_transitions.view", "seq_utils.logsumexp", "mask[].view", "mask.float().transpose().contiguous.float().transpose().contiguous.float"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.logsumexp", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.logsumexp"], ["", "", "def", "_input_likelihood", "(", "self", ",", "logits", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n\n        :param logits: emission score calculated by a linear layer, shape: (batch_size, seq_len, num_tags)\n        :param mask:\n        :return:\n        \"\"\"", "\n", "bsz", ",", "seq_len", ",", "num_tags", "=", "logits", ".", "size", "(", ")", "\n", "# Transpose batch size and sequence dimensions", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "logits", "=", "logits", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Initial alpha is the (batch_size, num_tags) tensor of likelihoods combining the", "\n", "# transitions to the initial states and the logits for the first timestep.", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "alpha", "=", "self", ".", "start_transitions", ".", "view", "(", "1", ",", "num_tags", ")", "+", "logits", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "logits", "[", "0", "]", "\n", "\n", "", "for", "t", "in", "range", "(", "1", ",", "seq_len", ")", ":", "\n", "# iteration starts from 1", "\n", "            ", "emit_scores", "=", "logits", "[", "t", "]", ".", "view", "(", "bsz", ",", "1", ",", "num_tags", ")", "\n", "transition_scores", "=", "self", ".", "transitions", ".", "view", "(", "1", ",", "num_tags", ",", "num_tags", ")", "\n", "broadcast_alpha", "=", "alpha", ".", "view", "(", "bsz", ",", "num_tags", ",", "1", ")", "\n", "\n", "# calculate the likelihood", "\n", "inner", "=", "broadcast_alpha", "+", "emit_scores", "+", "transition_scores", "\n", "\n", "# mask the padded token when met the padded token, retain the previous alpha", "\n", "alpha", "=", "(", "logsumexp", "(", "inner", ",", "1", ")", "*", "mask", "[", "t", "]", ".", "view", "(", "bsz", ",", "1", ")", "+", "alpha", "*", "(", "1", "-", "mask", "[", "t", "]", ")", ".", "view", "(", "bsz", ",", "1", ")", ")", "\n", "# Every sequence needs to end with a transition to the stop_tag.", "\n", "", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "stops", "=", "alpha", "+", "self", ".", "end_transitions", ".", "view", "(", "1", ",", "num_tags", ")", "\n", "", "else", ":", "\n", "            ", "stops", "=", "alpha", "\n", "\n", "# Finally we log_sum_exp along the num_tags dim, result is (batch_size,)", "\n", "", "return", "logsumexp", "(", "stops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF._joint_likelihood": [[198, 245], ["logits.transpose().contiguous.transpose().contiguous.size", "logits.transpose().contiguous.transpose().contiguous.transpose().contiguous", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose().contiguous", "tags.transpose().contiguous.transpose().contiguous.transpose().contiguous", "range", "tags.transpose().contiguous.transpose().contiguous.gather().squeeze", "last_inputs.gather", "last_input_score.squeeze.squeeze.squeeze", "absa_layer.CRF.start_transitions.index_select", "logits[].gather().squeeze", "mask.float().transpose().contiguous.float().transpose().contiguous.sum().long", "absa_layer.CRF.end_transitions.index_select", "tags.transpose().contiguous.gather().squeeze.view", "logits.transpose().contiguous.transpose().contiguous.transpose", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose", "tags.transpose().contiguous.transpose().contiguous.transpose", "tags.transpose().contiguous.transpose().contiguous.gather", "logits[].gather", "mask.float().transpose().contiguous.float().transpose().contiguous.sum", "last_tag_index.view", "mask.float().transpose().contiguous.float().transpose().contiguous.float", "current_tag.view", "next_tag.view", "current_tag.view"], "methods", ["None"], ["", "def", "_joint_likelihood", "(", "self", ",", "logits", ",", "tags", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        calculate the likelihood for the input tag sequence\n        :param logits:\n        :param tags: shape: (bsz, seq_len)\n        :param mask: shape: (bsz, seq_len)\n        :return:\n        \"\"\"", "\n", "bsz", ",", "seq_len", ",", "_", "=", "logits", ".", "size", "(", ")", "\n", "\n", "# Transpose batch size and sequence dimensions:", "\n", "logits", "=", "logits", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "tags", "=", "tags", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Start with the transition scores from start_tag to the first tag in each input", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "score", "=", "self", ".", "start_transitions", ".", "index_select", "(", "0", ",", "tags", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "score", "=", "0.0", "\n", "\n", "", "for", "t", "in", "range", "(", "seq_len", "-", "1", ")", ":", "\n", "            ", "current_tag", ",", "next_tag", "=", "tags", "[", "t", "]", ",", "tags", "[", "t", "+", "1", "]", "\n", "# The scores for transitioning from current_tag to next_tag", "\n", "transition_score", "=", "self", ".", "transitions", "[", "current_tag", ".", "view", "(", "-", "1", ")", ",", "next_tag", ".", "view", "(", "-", "1", ")", "]", "\n", "\n", "# The score for using current_tag", "\n", "emit_score", "=", "logits", "[", "t", "]", ".", "gather", "(", "1", ",", "current_tag", ".", "view", "(", "bsz", ",", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "score", "=", "score", "+", "transition_score", "*", "mask", "[", "t", "+", "1", "]", "+", "emit_score", "*", "mask", "[", "t", "]", "\n", "\n", "", "last_tag_index", "=", "mask", ".", "sum", "(", "0", ")", ".", "long", "(", ")", "-", "1", "\n", "last_tags", "=", "tags", ".", "gather", "(", "0", ",", "last_tag_index", ".", "view", "(", "1", ",", "bsz", ")", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "# Compute score of transitioning to `stop_tag` from each \"last tag\".", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "last_transition_score", "=", "self", ".", "end_transitions", ".", "index_select", "(", "0", ",", "last_tags", ")", "\n", "", "else", ":", "\n", "            ", "last_transition_score", "=", "0.0", "\n", "\n", "", "last_inputs", "=", "logits", "[", "-", "1", "]", "# (batch_size, num_tags)", "\n", "last_input_score", "=", "last_inputs", ".", "gather", "(", "1", ",", "last_tags", ".", "view", "(", "-", "1", ",", "1", ")", ")", "# (batch_size, 1)", "\n", "last_input_score", "=", "last_input_score", ".", "squeeze", "(", ")", "# (batch_size,)", "\n", "\n", "score", "=", "score", "+", "last_transition_score", "+", "last_input_score", "*", "mask", "[", "-", "1", "]", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.CRF.viterbi_tags": [[246, 304], ["logits.size", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "zip", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.Tensor.fill_", "torch.Tensor.fill_", "seq_utils.viterbi_decode", "best_paths.append", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "absa_layer.CRF.start_transitions.detach", "absa_layer.CRF.end_transitions.detach", "absa_layer.CRF.constraint_mask[].detach", "absa_layer.CRF.constraint_mask[].detach", "absa_layer.CRF.constraint_mask[].detach", "absa_layer.CRF.constraint_mask[].detach"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.viterbi_decode"], ["", "def", "viterbi_tags", "(", "self", ",", "logits", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n\n        :param logits: (bsz, seq_len, num_tags), emission scores\n        :param mask:\n        :return:\n        \"\"\"", "\n", "_", ",", "max_seq_len", ",", "num_tags", "=", "logits", ".", "size", "(", ")", "\n", "\n", "# Get the tensors out of the variables", "\n", "logits", ",", "mask", "=", "logits", ".", "data", ",", "mask", ".", "data", "\n", "\n", "# Augment transitions matrix with start and end transitions", "\n", "start_tag", "=", "num_tags", "\n", "end_tag", "=", "num_tags", "+", "1", "\n", "transitions", "=", "torch", ".", "Tensor", "(", "num_tags", "+", "2", ",", "num_tags", "+", "2", ")", ".", "fill_", "(", "-", "10000.", ")", "\n", "\n", "# Apply transition constraints", "\n", "constrained_transitions", "=", "(", "\n", "self", ".", "transitions", "*", "self", ".", "constraint_mask", "[", ":", "num_tags", ",", ":", "num_tags", "]", "+", "\n", "-", "10000.0", "*", "(", "1", "-", "self", ".", "constraint_mask", "[", ":", "num_tags", ",", ":", "num_tags", "]", ")", "\n", ")", "\n", "\n", "transitions", "[", ":", "num_tags", ",", ":", "num_tags", "]", "=", "constrained_transitions", ".", "data", "\n", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "transitions", "[", "start_tag", ",", ":", "num_tags", "]", "=", "(", "\n", "self", ".", "start_transitions", ".", "detach", "(", ")", "*", "self", ".", "constraint_mask", "[", "start_tag", ",", ":", "num_tags", "]", ".", "data", "+", "\n", "-", "10000.0", "*", "(", "1", "-", "self", ".", "constraint_mask", "[", "start_tag", ",", ":", "num_tags", "]", ".", "detach", "(", ")", ")", "\n", ")", "\n", "transitions", "[", ":", "num_tags", ",", "end_tag", "]", "=", "(", "\n", "self", ".", "end_transitions", ".", "detach", "(", ")", "*", "self", ".", "constraint_mask", "[", ":", "num_tags", ",", "end_tag", "]", ".", "data", "+", "\n", "-", "10000.0", "*", "(", "1", "-", "self", ".", "constraint_mask", "[", ":", "num_tags", ",", "end_tag", "]", ".", "detach", "(", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "transitions", "[", "start_tag", ",", ":", "num_tags", "]", "=", "(", "-", "10000.0", "*", "\n", "(", "1", "-", "self", ".", "constraint_mask", "[", "start_tag", ",", ":", "num_tags", "]", ".", "detach", "(", ")", ")", ")", "\n", "transitions", "[", ":", "num_tags", ",", "end_tag", "]", "=", "-", "10000.0", "*", "(", "1", "-", "self", ".", "constraint_mask", "[", ":", "num_tags", ",", "end_tag", "]", ".", "detach", "(", ")", ")", "\n", "\n", "", "best_paths", "=", "[", "]", "\n", "# Pad the max sequence length by 2 to account for start_tag + end_tag.", "\n", "tag_sequence", "=", "torch", ".", "Tensor", "(", "max_seq_len", "+", "2", ",", "num_tags", "+", "2", ")", "\n", "\n", "for", "prediction", ",", "prediction_mask", "in", "zip", "(", "logits", ",", "mask", ")", ":", "\n", "# perform viterbi decoding sample by sample", "\n", "            ", "seq_len", "=", "torch", ".", "sum", "(", "prediction_mask", ")", "\n", "# Start with everything totally unlikely", "\n", "tag_sequence", ".", "fill_", "(", "-", "10000.", ")", "\n", "# At timestep 0 we must have the START_TAG", "\n", "tag_sequence", "[", "0", ",", "start_tag", "]", "=", "0.", "\n", "# At steps 1, ..., sequence_length we just use the incoming prediction", "\n", "tag_sequence", "[", "1", ":", "(", "seq_len", "+", "1", ")", ",", ":", "num_tags", "]", "=", "prediction", "[", ":", "seq_len", "]", "\n", "# And at the last timestep we must have the END_TAG", "\n", "tag_sequence", "[", "seq_len", "+", "1", ",", "end_tag", "]", "=", "0.", "\n", "viterbi_path", "=", "viterbi_decode", "(", "tag_sequence", "[", ":", "(", "seq_len", "+", "2", ")", "]", ",", "transitions", ")", "\n", "viterbi_path", "=", "viterbi_path", "[", "1", ":", "-", "1", "]", "\n", "best_paths", ".", "append", "(", "viterbi_path", ")", "\n", "", "return", "best_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.LSTM.__init__": [[308, 327], ["torch.Module.__init__", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "bidirectional", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n\n        :param input_size:\n        :param hidden_size:\n        :param bidirectional:\n        \"\"\"", "\n", "super", "(", "LSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "if", "bidirectional", ":", "\n", "            ", "self", ".", "hidden_size", "=", "hidden_size", "//", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "LNx", "=", "nn", ".", "LayerNorm", "(", "4", "*", "self", ".", "hidden_size", ")", "\n", "self", ".", "LNh", "=", "nn", ".", "LayerNorm", "(", "4", "*", "self", ".", "hidden_size", ")", "\n", "self", ".", "LNc", "=", "nn", ".", "LayerNorm", "(", "self", ".", "hidden_size", ")", "\n", "self", ".", "Wx", "=", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "input_size", ",", "out_features", "=", "4", "*", "self", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "Wh", "=", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "hidden_size", ",", "out_features", "=", "4", "*", "self", ".", "hidden_size", ",", "bias", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.LSTM.forward": [[328, 374], ["range", "absa_layer.LSTM.init_hidden", "x.transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "gates.chunk", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "x.size", "x.size", "absa_layer.LSTM.forward.recurrence"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.LSTM.init_hidden"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n\n        :param x: input, shape: (batch_size, seq_len, input_size)\n        :return:\n        \"\"\"", "\n", "def", "recurrence", "(", "xt", ",", "hidden", ")", ":", "\n", "            ", "\"\"\"\n            recurrence function enhanced with layer norm\n            :param input: input to the current cell\n            :param hidden:\n            :return:\n            \"\"\"", "\n", "htm1", ",", "ctm1", "=", "hidden", "\n", "gates", "=", "self", ".", "LNx", "(", "self", ".", "Wx", "(", "xt", ")", ")", "+", "self", ".", "LNh", "(", "self", ".", "Wh", "(", "htm1", ")", ")", "\n", "it", ",", "ft", ",", "gt", ",", "ot", "=", "gates", ".", "chunk", "(", "4", ",", "1", ")", "\n", "it", "=", "torch", ".", "sigmoid", "(", "it", ")", "\n", "ft", "=", "torch", ".", "sigmoid", "(", "ft", ")", "\n", "gt", "=", "torch", ".", "tanh", "(", "gt", ")", "\n", "ot", "=", "torch", ".", "sigmoid", "(", "ot", ")", "\n", "ct", "=", "(", "ft", "*", "ctm1", ")", "+", "(", "it", "*", "gt", ")", "\n", "ht", "=", "ot", "*", "torch", ".", "tanh", "(", "self", ".", "LNc", "(", "ct", ")", ")", "# n_b x hidden_dim", "\n", "\n", "return", "ht", ",", "ct", "\n", "", "output", "=", "[", "]", "\n", "# sequence_length", "\n", "steps", "=", "range", "(", "x", ".", "size", "(", "1", ")", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "x", ".", "size", "(", "0", ")", ")", "\n", "# change to: (seq_len, bs, hidden_size)", "\n", "input", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "for", "t", "in", "steps", ":", "\n", "            ", "hidden", "=", "recurrence", "(", "input", "[", "t", "]", ",", "hidden", ")", "\n", "output", ".", "append", "(", "hidden", "[", "0", "]", ")", "\n", "# (bs, seq_len, hidden_size)", "\n", "", "output", "=", "torch", ".", "stack", "(", "output", ",", "0", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "hidden_b", "=", "self", ".", "init_hidden", "(", "x", ".", "size", "(", "0", ")", ")", "\n", "output_b", "=", "[", "]", "\n", "for", "t", "in", "steps", "[", ":", ":", "-", "1", "]", ":", "\n", "                ", "hidden_b", "=", "recurrence", "(", "input", "[", "t", "]", ",", "hidden_b", ")", "\n", "output_b", ".", "append", "(", "hidden_b", "[", "0", "]", ")", "\n", "", "output_b", "=", "output_b", "[", ":", ":", "-", "1", "]", "\n", "output_b", "=", "torch", ".", "stack", "(", "output_b", ",", "0", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "output", "=", "torch", ".", "cat", "(", "[", "output", ",", "output_b", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.LSTM.init_hidden": [[375, 379], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "bs", ")", ":", "\n", "        ", "h_0", "=", "torch", ".", "zeros", "(", "bs", ",", "self", ".", "hidden_size", ")", ".", "cuda", "(", ")", "\n", "c_0", "=", "torch", ".", "zeros", "(", "bs", ",", "self", ".", "hidden_size", ")", ".", "cuda", "(", ")", "\n", "return", "h_0", ",", "c_0", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.BertABSATagger.__init__": [[382, 433], ["bert.BertPreTrainedModel.__init__", "absa_layer.TaggerConfig", "bert_config.absa_type.lower", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "transformers.BertModel", "Exception", "absa_layer.BertABSATagger.bert.parameters", "torch.Dropout", "torch.Dropout", "absa_layer.LSTM", "absa_layer.GRU", "torch.TransformerEncoderLayer", "torch.TransformerEncoderLayer", "absa_layer.SAN", "absa_layer.CRF", "Exception"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_config", ")", ":", "\n", "        ", "\"\"\"\n\n        :param bert_config: configuration for bert model\n        \"\"\"", "\n", "super", "(", "BertABSATagger", ",", "self", ")", ".", "__init__", "(", "bert_config", ")", "\n", "self", ".", "num_labels", "=", "bert_config", ".", "num_labels", "\n", "self", ".", "tagger_config", "=", "TaggerConfig", "(", ")", "\n", "self", ".", "tagger_config", ".", "absa_type", "=", "bert_config", ".", "absa_type", ".", "lower", "(", ")", "\n", "if", "bert_config", ".", "tfm_mode", "==", "'finetune'", ":", "\n", "# initialized with pre-trained BERT and perform finetuning", "\n", "# print(\"Fine-tuning the pre-trained BERT...\")", "\n", "            ", "self", ".", "bert", "=", "BertModel", "(", "bert_config", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Invalid transformer mode %s!!!\"", "%", "bert_config", ".", "tfm_mode", ")", "\n", "", "self", ".", "bert_dropout", "=", "nn", ".", "Dropout", "(", "bert_config", ".", "hidden_dropout_prob", ")", "\n", "# fix the parameters in BERT and regard it as feature extractor", "\n", "if", "bert_config", ".", "fix_tfm", ":", "\n", "# fix the parameters of the (pre-trained or randomly initialized) transformers during fine-tuning", "\n", "            ", "for", "p", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "self", ".", "tagger", "=", "None", "\n", "if", "self", ".", "tagger_config", ".", "absa_type", "==", "'linear'", ":", "\n", "# hidden size at the penultimate layer", "\n", "            ", "penultimate_hidden_size", "=", "bert_config", ".", "hidden_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "tagger_dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "tagger_config", ".", "hidden_dropout_prob", ")", "\n", "if", "self", ".", "tagger_config", ".", "absa_type", "==", "'lstm'", ":", "\n", "                ", "self", ".", "tagger", "=", "LSTM", "(", "input_size", "=", "bert_config", ".", "hidden_size", ",", "\n", "hidden_size", "=", "self", ".", "tagger_config", ".", "hidden_size", ",", "\n", "bidirectional", "=", "self", ".", "tagger_config", ".", "bidirectional", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "absa_type", "==", "'gru'", ":", "\n", "                ", "self", ".", "tagger", "=", "GRU", "(", "input_size", "=", "bert_config", ".", "hidden_size", ",", "\n", "hidden_size", "=", "self", ".", "tagger_config", ".", "hidden_size", ",", "\n", "bidirectional", "=", "self", ".", "tagger_config", ".", "bidirectional", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "absa_type", "==", "'tfm'", ":", "\n", "# transformer encoder layer", "\n", "                ", "self", ".", "tagger", "=", "nn", ".", "TransformerEncoderLayer", "(", "d_model", "=", "bert_config", ".", "hidden_size", ",", "\n", "nhead", "=", "12", ",", "\n", "dim_feedforward", "=", "4", "*", "bert_config", ".", "hidden_size", ",", "\n", "dropout", "=", "0.1", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "absa_type", "==", "'san'", ":", "\n", "# vanilla self attention networks", "\n", "                ", "self", ".", "tagger", "=", "SAN", "(", "d_model", "=", "bert_config", ".", "hidden_size", ",", "nhead", "=", "12", ",", "dropout", "=", "0.1", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "absa_type", "==", "'crf'", ":", "\n", "                ", "self", ".", "tagger", "=", "CRF", "(", "num_tags", "=", "self", ".", "num_labels", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Unimplemented downstream tagger %s...'", "%", "self", ".", "tagger_config", ".", "absa_type", ")", "\n", "", "penultimate_hidden_size", "=", "self", ".", "tagger_config", ".", "hidden_size", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "penultimate_hidden_size", ",", "bert_config", ".", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.BertABSATagger.forward": [[434, 480], ["absa_layer.BertABSATagger.bert", "absa_layer.BertABSATagger.bert_dropout", "absa_layer.BertABSATagger.classifier", "absa_layer.BertABSATagger.tagger_dropout", "absa_layer.BertABSATagger.classifier", "absa_layer.BertABSATagger.tagger", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "absa_layer.BertABSATagger.tagger", "absa_layer.BertABSATagger.tagger", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "tagger_input.transpose.transpose.transpose", "absa_layer.BertABSATagger.tagger", "classifier_input.transpose.transpose.transpose", "Exception", "attention_mask.view", "absa_layer.BertABSATagger.view", "labels.view", "absa_layer.BertABSATagger.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "head_mask", "=", "head_mask", ")", "\n", "# the hidden states of the last Bert Layer, shape: (bsz, seq_len, hsz)", "\n", "tagger_input", "=", "outputs", "[", "0", "]", "\n", "tagger_input", "=", "self", ".", "bert_dropout", "(", "tagger_input", ")", "\n", "#print(\"tagger_input.shape:\", tagger_input.shape)", "\n", "if", "self", ".", "tagger", "is", "None", "or", "self", ".", "tagger_config", ".", "absa_type", "==", "'crf'", ":", "\n", "# regard classifier as the tagger", "\n", "            ", "logits", "=", "self", ".", "classifier", "(", "tagger_input", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "tagger_config", ".", "absa_type", "==", "'lstm'", ":", "\n", "# customized LSTM", "\n", "                ", "classifier_input", ",", "_", "=", "self", ".", "tagger", "(", "tagger_input", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "absa_type", "==", "'gru'", ":", "\n", "# customized GRU", "\n", "                ", "classifier_input", ",", "_", "=", "self", ".", "tagger", "(", "tagger_input", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "absa_type", "==", "'san'", "or", "self", ".", "tagger_config", ".", "absa_type", "==", "'tfm'", ":", "\n", "# vanilla self-attention networks or transformer", "\n", "# adapt the input format for the transformer or self attention networks", "\n", "                ", "tagger_input", "=", "tagger_input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "classifier_input", "=", "self", ".", "tagger", "(", "tagger_input", ")", "\n", "classifier_input", "=", "classifier_input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"Unimplemented downstream tagger %s...\"", "%", "self", ".", "tagger_config", ".", "absa_type", ")", "\n", "", "classifier_input", "=", "self", ".", "tagger_dropout", "(", "classifier_input", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "classifier_input", ")", "\n", "", "outputs", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "tagger_config", ".", "absa_type", "!=", "'crf'", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                    ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "", "else", ":", "\n", "                ", "log_likelihood", "=", "self", ".", "tagger", "(", "inputs", "=", "logits", ",", "tags", "=", "labels", ",", "mask", "=", "attention_mask", ")", "\n", "loss", "=", "-", "log_likelihood", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.XLNetABSATagger.__init__": [[484, 509], ["bert.XLNetPreTrainedModel.__init__", "transformers.XLNetModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "absa_layer.XLNetABSATagger.apply", "torch.Dropout", "torch.Dropout", "getattr", "Exception", "Exception"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__"], ["    ", "def", "__init__", "(", "self", ",", "xlnet_config", ")", ":", "\n", "        ", "super", "(", "XLNetABSATagger", ",", "self", ")", ".", "__init__", "(", "xlnet_config", ")", "\n", "self", ".", "num_labels", "=", "xlnet_config", ".", "num_labels", "\n", "self", ".", "xlnet", "=", "XLNetModel", "(", "xlnet_config", ")", "\n", "self", ".", "tagger_config", "=", "xlnet_config", ".", "absa_tagger_config", "\n", "self", ".", "tagger", "=", "None", "\n", "if", "self", ".", "tagger_config", ".", "tagger", "==", "''", ":", "\n", "# hidden size at the penultimate layer", "\n", "            ", "penultimate_hidden_size", "=", "xlnet_config", ".", "d_model", "\n", "", "else", ":", "\n", "            ", "self", ".", "tagger_dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "tagger_config", ".", "hidden_dropout_prob", ")", "\n", "if", "self", ".", "tagger_config", ".", "tagger", "in", "[", "'RNN'", ",", "'LSTM'", ",", "'GRU'", "]", ":", "\n", "# 2-layer bi-directional rnn decoder", "\n", "                ", "self", ".", "tagger", "=", "getattr", "(", "nn", ",", "self", ".", "tagger_config", ".", "tagger", ")", "(", "\n", "input_size", "=", "xlnet_config", ".", "d_model", ",", "hidden_size", "=", "self", ".", "tagger_config", ".", "hidden_size", "//", "2", ",", "\n", "num_layers", "=", "self", ".", "tagger_config", ".", "n_rnn_layers", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "True", ")", "\n", "", "elif", "self", ".", "tagger_config", ".", "tagger", "in", "[", "'CRF'", "]", ":", "\n", "# crf tagger", "\n", "                ", "raise", "Exception", "(", "\"Unimplemented now!!\"", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Unimplemented tagger %s...'", "%", "self", ".", "tagger_config", ".", "tagger", ")", "\n", "", "penultimate_hidden_size", "=", "self", ".", "tagger_config", ".", "hidden_size", "\n", "", "self", ".", "tagger_dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "tagger_config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "penultimate_hidden_size", ",", "xlnet_config", ".", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.absa_layer.XLNetABSATagger.forward": [[510, 559], ["absa_layer.XLNetABSATagger.xlnet", "absa_layer.XLNetABSATagger.classifier", "absa_layer.XLNetABSATagger.tagger_dropout", "absa_layer.XLNetABSATagger.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "absa_layer.XLNetABSATagger.tagger", "Exception", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "absa_layer.XLNetABSATagger.view", "labels.view", "absa_layer.XLNetABSATagger.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "input_mask", "=", "None", ",", "attention_mask", "=", "None", ",", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "target_mapping", "=", "None", ",", "labels", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param input_ids: Indices of input sequence tokens in the vocabulary\n        :param token_type_ids: A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n        The embeddings from these tokens will be summed with the respective token embeddings\n        :param input_mask: Mask to avoid performing attention on padding token indices.\n        :param attention_mask: Mask to avoid performing attention on padding token indices.\n        :param mems: list of torch.FloatTensor (one for each layer):\n        that contains pre-computed hidden-states (key and values in the attention blocks)\n        :param perm_mask:\n        :param target_mapping:\n        :param labels:\n        :param head_mask:\n        :return:\n        \"\"\"", "\n", "transformer_outputs", "=", "self", ".", "xlnet", "(", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "attention_mask", "=", "attention_mask", ",", "\n", "mems", "=", "mems", ",", "perm_mask", "=", "perm_mask", ",", "target_mapping", "=", "target_mapping", ",", "\n", "head_mask", "=", "head_mask", ")", "\n", "# hidden states from the last transformer layer, xlnet has done the dropout,", "\n", "# no need to do the additional dropout", "\n", "tagger_input", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "tagger", "is", "None", ":", "\n", "# regard classifier as the tagger", "\n", "            ", "logits", "=", "self", ".", "classifier", "(", "tagger_input", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "tagger_config", ".", "tagger", "in", "[", "'RNN'", ",", "'LSTM'", ",", "'GRU'", "]", ":", "\n", "                ", "classifier_input", ",", "_", "=", "self", ".", "tagger", "(", "tagger_input", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"Unimplemented tagger %s...\"", "%", "self", ".", "tagger_config", ".", "tagger", ")", "\n", "", "classifier_input", "=", "self", ".", "tagger_dropout", "(", "classifier_input", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "classifier_input", ")", "\n", "# transformer outputs: (last_hidden_state, mems, hidden_states, attentions)", "\n", "", "outputs", "=", "(", "logits", ",", ")", "+", "transformer_outputs", "[", "1", ":", "]", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.InputExample.__init__": [[35, 51], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.InputFeatures.__init__": [[56, 61], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.SeqInputFeatures.__init__": [[65, 72], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "evaluate_label_ids", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_ids", "=", "label_ids", "\n", "# mapping between word index and head token index", "\n", "self", ".", "evaluate_label_ids", "=", "evaluate_label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.DataProcessor.get_train_examples": [[77, 80], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.DataProcessor.get_dev_examples": [[81, 84], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.DataProcessor.get_test_examples": [[85, 88], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.DataProcessor.get_labels": [[89, 92], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.DataProcessor._read_tsv": [[93, 104], ["io.open", "csv.reader", "lines.append", "list"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "        ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8-sig\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "                ", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", ":", "\n", "                    ", "line", "=", "list", "(", "cell", "for", "cell", "in", "line", ")", "\n", "", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_train_examples": [[108, 110], ["glue_utils.ABSAProcessor._create_examples"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor._create_examples"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "tagging_schema", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "data_dir", "=", "data_dir", ",", "set_type", "=", "'train'", ",", "tagging_schema", "=", "tagging_schema", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_dev_examples": [[111, 113], ["glue_utils.ABSAProcessor._create_examples"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor._create_examples"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "tagging_schema", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "data_dir", "=", "data_dir", ",", "set_type", "=", "'dev'", ",", "tagging_schema", "=", "tagging_schema", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_test_examples": [[114, 116], ["glue_utils.ABSAProcessor._create_examples"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor._create_examples"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "tagging_schema", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "data_dir", "=", "data_dir", ",", "set_type", "=", "'test'", ",", "tagging_schema", "=", "tagging_schema", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor.get_labels": [[117, 128], ["Exception"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ",", "tagging_schema", ")", ":", "\n", "        ", "if", "tagging_schema", "==", "'OT'", ":", "\n", "            ", "return", "[", "]", "\n", "", "elif", "tagging_schema", "==", "'BIO'", ":", "\n", "            ", "return", "[", "'O'", ",", "'EQ'", ",", "'B-POS'", ",", "'I-POS'", ",", "'B-NEG'", ",", "'I-NEG'", ",", "'B-NEU'", ",", "'I-NEU'", "]", "\n", "", "elif", "tagging_schema", "==", "'BIEOS'", ":", "\n", "            ", "return", "[", "'O'", ",", "'EQ'", ",", "'B-POS'", ",", "'I-POS'", ",", "'E-POS'", ",", "'S-POS'", ",", "\n", "'B-NEG'", ",", "'I-NEG'", ",", "'E-NEG'", ",", "'S-NEG'", ",", "\n", "'B-NEU'", ",", "'I-NEU'", ",", "'E-NEU'", ",", "'S-NEU'", "]", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Invalid tagging schema %s...\"", "%", "tagging_schema", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.ABSAProcessor._create_examples": [[129, 173], ["os.path.join", "np.zeros", "print", "io.open", "line.strip().split", "tag_string.split", "seq_utils.tag2ts", "examples.append", "tag_item.split", "words.append", "seq_utils.ot2bio_ts.append", "seq_utils.ot2bieos_ts", "glue_utils.InputExample", "line.strip", "len", "Exception", "seq_utils.ot2bio_ts", "len", "len"], "methods", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.tag2ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bio_ts"], ["", "", "def", "_create_examples", "(", "self", ",", "data_dir", ",", "set_type", ",", "tagging_schema", ")", ":", "\n", "        ", "examples", "=", "[", "]", "\n", "file", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"%s.txt\"", "%", "set_type", ")", "\n", "class_count", "=", "np", ".", "zeros", "(", "3", ")", "\n", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'UTF-8'", ")", "as", "fp", ":", "\n", "            ", "sample_id", "=", "0", "\n", "for", "line", "in", "fp", ":", "\n", "                ", "sent_string", ",", "tag_string", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'####'", ")", "\n", "words", "=", "[", "]", "\n", "tags", "=", "[", "]", "\n", "for", "tag_item", "in", "tag_string", ".", "split", "(", "' '", ")", ":", "\n", "                    ", "eles", "=", "tag_item", ".", "split", "(", "'='", ")", "\n", "if", "len", "(", "eles", ")", "==", "1", ":", "\n", "                        ", "raise", "Exception", "(", "\"Invalid samples %s...\"", "%", "tag_string", ")", "\n", "", "elif", "len", "(", "eles", ")", "==", "2", ":", "\n", "                        ", "word", ",", "tag", "=", "eles", "\n", "", "else", ":", "\n", "                        ", "word", "=", "''", ".", "join", "(", "(", "len", "(", "eles", ")", "-", "2", ")", "*", "[", "'='", "]", ")", "\n", "tag", "=", "eles", "[", "-", "1", "]", "\n", "", "words", ".", "append", "(", "word", ")", "\n", "tags", ".", "append", "(", "tag", ")", "\n", "# convert from ot to bieos", "\n", "", "if", "tagging_schema", "==", "'BIEOS'", ":", "\n", "                    ", "tags", "=", "ot2bieos_ts", "(", "tags", ")", "\n", "", "elif", "tagging_schema", "==", "'BIO'", ":", "\n", "                    ", "tags", "=", "ot2bio_ts", "(", "tags", ")", "\n", "", "else", ":", "\n", "# original tags follow the OT tagging schema, do nothing", "\n", "                    ", "pass", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "sample_id", ")", "\n", "text_a", "=", "' '", ".", "join", "(", "words", ")", "\n", "#label = [absa_label_vocab[tag] for tag in tags]", "\n", "gold_ts", "=", "tag2ts", "(", "ts_tag_sequence", "=", "tags", ")", "\n", "for", "(", "b", ",", "e", ",", "s", ")", "in", "gold_ts", ":", "\n", "                    ", "if", "s", "==", "'POS'", ":", "\n", "                        ", "class_count", "[", "0", "]", "+=", "1", "\n", "", "if", "s", "==", "'NEG'", ":", "\n", "                        ", "class_count", "[", "1", "]", "+=", "1", "\n", "", "if", "s", "==", "'NEU'", ":", "\n", "                        ", "class_count", "[", "2", "]", "+=", "1", "\n", "", "", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "tags", ")", ")", "\n", "sample_id", "+=", "1", "\n", "", "", "print", "(", "\"%s class count: %s\"", "%", "(", "set_type", ",", "class_count", ")", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils._truncate_seq_pair": [[175, 190], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.convert_examples_to_seq_features": [[192, 297], ["enumerate", "enumerate", "print", "example.text_a.split", "zip", "np.array", "examples_tokenized.append", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "tokenizer.tokenize", "tokens_a.extend", "np.array.append", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "glue_utils.SeqInputFeatures", "labels_a.extend", "labels_a.extend", "len", "str", "str", "str", "str", "str", "len"], "function", ["None"], ["", "", "", "def", "convert_examples_to_seq_features", "(", "examples", ",", "label_list", ",", "tokenizer", ",", "\n", "cls_token_at_end", "=", "False", ",", "pad_on_left", "=", "False", ",", "cls_token", "=", "'[CLS]'", ",", "\n", "sep_token", "=", "'[SEP]'", ",", "pad_token", "=", "0", ",", "sequence_a_segment_id", "=", "0", ",", "\n", "sequence_b_segment_id", "=", "1", ",", "cls_token_segment_id", "=", "1", ",", "pad_token_segment_id", "=", "0", ",", "\n", "mask_padding_with_zero", "=", "True", ")", ":", "\n", "# feature extraction for sequence labeling", "\n", "    ", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "features", "=", "[", "]", "\n", "max_seq_length", "=", "-", "1", "\n", "examples_tokenized", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "[", "]", "\n", "labels_a", "=", "[", "]", "\n", "evaluate_label_ids", "=", "[", "]", "\n", "words", "=", "example", ".", "text_a", ".", "split", "(", "' '", ")", "\n", "wid", ",", "tid", "=", "0", ",", "0", "\n", "for", "word", ",", "label", "in", "zip", "(", "words", ",", "example", ".", "label", ")", ":", "\n", "            ", "subwords", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "tokens_a", ".", "extend", "(", "subwords", ")", "\n", "if", "label", "!=", "'O'", ":", "\n", "                ", "labels_a", ".", "extend", "(", "[", "label", "]", "+", "[", "'EQ'", "]", "*", "(", "len", "(", "subwords", ")", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "labels_a", ".", "extend", "(", "[", "'O'", "]", "*", "len", "(", "subwords", ")", ")", "\n", "", "evaluate_label_ids", ".", "append", "(", "tid", ")", "\n", "wid", "+=", "1", "\n", "# move the token pointer", "\n", "tid", "+=", "len", "(", "subwords", ")", "\n", "#print(evaluate_label_ids)", "\n", "", "assert", "tid", "==", "len", "(", "tokens_a", ")", "\n", "evaluate_label_ids", "=", "np", ".", "array", "(", "evaluate_label_ids", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "examples_tokenized", ".", "append", "(", "(", "tokens_a", ",", "labels_a", ",", "evaluate_label_ids", ")", ")", "\n", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", ":", "\n", "            ", "max_seq_length", "=", "len", "(", "tokens_a", ")", "\n", "# count on the [CLS] and [SEP]", "\n", "", "", "max_seq_length", "+=", "2", "\n", "#max_seq_length = 128", "\n", "for", "ex_index", ",", "(", "tokens_a", ",", "labels_a", ",", "evaluate_label_ids", ")", "in", "enumerate", "(", "examples_tokenized", ")", ":", "\n", "#tokens_a = tokenizer.tokenize(example.text_a)", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "# for sequence labeling, better not truncate the sequence", "\n", "#if len(tokens_a) > max_seq_length - 2:", "\n", "#    tokens_a = tokens_a[:(max_seq_length - 2)]", "\n", "#    labels_a = labels_a", "\n", "        ", "tokens", "=", "tokens_a", "+", "[", "sep_token", "]", "\n", "segment_ids", "=", "[", "sequence_a_segment_id", "]", "*", "len", "(", "tokens", ")", "\n", "labels", "=", "labels_a", "+", "[", "'O'", "]", "\n", "if", "cls_token_at_end", ":", "\n", "# evaluate label ids not change", "\n", "            ", "tokens", "=", "tokens", "+", "[", "cls_token", "]", "\n", "segment_ids", "=", "segment_ids", "+", "[", "cls_token_segment_id", "]", "\n", "labels", "=", "labels", "+", "[", "'O'", "]", "\n", "", "else", ":", "\n", "# right shift 1 for evaluate label ids", "\n", "            ", "tokens", "=", "[", "cls_token", "]", "+", "tokens", "\n", "segment_ids", "=", "[", "cls_token_segment_id", "]", "+", "segment_ids", "\n", "labels", "=", "[", "'O'", "]", "+", "labels", "\n", "evaluate_label_ids", "+=", "1", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "if", "mask_padding_with_zero", "else", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "# Zero-pad up to the sequence length.", "\n", "padding_length", "=", "max_seq_length", "-", "len", "(", "input_ids", ")", "\n", "#print(\"Current labels:\", labels)", "\n", "label_ids", "=", "[", "label_map", "[", "label", "]", "for", "label", "in", "labels", "]", "\n", "\n", "# pad the input sequence and the mask sequence", "\n", "if", "pad_on_left", ":", "\n", "            ", "input_ids", "=", "(", "[", "pad_token", "]", "*", "padding_length", ")", "+", "input_ids", "\n", "input_mask", "=", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "+", "input_mask", "\n", "segment_ids", "=", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "+", "segment_ids", "\n", "# pad sequence tag 'O'", "\n", "label_ids", "=", "(", "[", "0", "]", "*", "padding_length", ")", "+", "label_ids", "\n", "# right shift padding_length for evaluate_label_ids", "\n", "evaluate_label_ids", "+=", "padding_length", "\n", "", "else", ":", "\n", "# evaluate ids not change", "\n", "            ", "input_ids", "=", "input_ids", "+", "(", "[", "pad_token", "]", "*", "padding_length", ")", "\n", "input_mask", "=", "input_mask", "+", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "\n", "segment_ids", "=", "segment_ids", "+", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "\n", "# pad sequence tag 'O'", "\n", "label_ids", "=", "label_ids", "+", "(", "[", "0", "]", "*", "padding_length", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "label_ids", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"labels: %s \"", "%", "' '", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "label_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"evaluate label ids: %s\"", "%", "evaluate_label_ids", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "SeqInputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_ids", "=", "label_ids", ",", "\n", "evaluate_label_ids", "=", "evaluate_label_ids", ")", ")", "\n", "", "print", "(", "\"maximal sequence length is\"", ",", "max_seq_length", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.convert_examples_to_features": [[299, 410], ["enumerate", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "logger.info", "tokenizer.tokenize", "glue_utils._truncate_seq_pair", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "glue_utils.InputFeatures", "len", "float", "KeyError", "len", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils._truncate_seq_pair"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "\n", "tokenizer", ",", "output_mode", ",", "\n", "cls_token_at_end", "=", "False", ",", "pad_on_left", "=", "False", ",", "\n", "cls_token", "=", "'[CLS]'", ",", "sep_token", "=", "'[SEP]'", ",", "pad_token", "=", "0", ",", "\n", "sequence_a_segment_id", "=", "0", ",", "sequence_b_segment_id", "=", "1", ",", "\n", "cls_token_segment_id", "=", "1", ",", "pad_token_segment_id", "=", "0", ",", "\n", "mask_padding_with_zero", "=", "True", ")", ":", "\n", "    ", "\"\"\" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    \"\"\"", "\n", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Writing example %d of %d\"", "%", "(", "ex_index", ",", "len", "(", "examples", ")", ")", ")", "\n", "\n", "", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids:   0   0   0   0  0     0   0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambiguously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "tokens_a", "+", "[", "sep_token", "]", "\n", "segment_ids", "=", "[", "sequence_a_segment_id", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "sep_token", "]", "\n", "segment_ids", "+=", "[", "sequence_b_segment_id", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "cls_token_at_end", ":", "\n", "            ", "tokens", "=", "tokens", "+", "[", "cls_token", "]", "\n", "segment_ids", "=", "segment_ids", "+", "[", "cls_token_segment_id", "]", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "cls_token", "]", "+", "tokens", "\n", "segment_ids", "=", "[", "cls_token_segment_id", "]", "+", "segment_ids", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "if", "mask_padding_with_zero", "else", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding_length", "=", "max_seq_length", "-", "len", "(", "input_ids", ")", "\n", "if", "pad_on_left", ":", "\n", "            ", "input_ids", "=", "(", "[", "pad_token", "]", "*", "padding_length", ")", "+", "input_ids", "\n", "input_mask", "=", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "+", "input_mask", "\n", "segment_ids", "=", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "+", "segment_ids", "\n", "", "else", ":", "\n", "            ", "input_ids", "=", "input_ids", "+", "(", "[", "pad_token", "]", "*", "padding_length", ")", "\n", "input_mask", "=", "input_mask", "+", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "\n", "segment_ids", "=", "segment_ids", "+", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "if", "output_mode", "==", "\"classification\"", ":", "\n", "            ", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "", "elif", "output_mode", "==", "\"regression\"", ":", "\n", "            ", "label_id", "=", "float", "(", "example", ".", "label", ")", "\n", "", "else", ":", "\n", "            ", "raise", "KeyError", "(", "output_mode", ")", "\n", "\n", "", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.match_ts": [[412, 434], ["np.zeros", "np.zeros", "np.zeros"], "function", ["None"], ["", "def", "match_ts", "(", "gold_ts_sequence", ",", "pred_ts_sequence", ")", ":", "\n", "    ", "\"\"\"\n    calculate the number of correctly predicted targeted sentiment\n    :param gold_ts_sequence: gold standard targeted sentiment sequence\n    :param pred_ts_sequence: predicted targeted sentiment sequence\n    :return:\n    \"\"\"", "\n", "# positive, negative and neutral", "\n", "tag2tagid", "=", "{", "'POS'", ":", "0", ",", "'NEG'", ":", "1", ",", "'NEU'", ":", "2", "}", "\n", "hit_count", ",", "gold_count", ",", "pred_count", "=", "np", ".", "zeros", "(", "3", ")", ",", "np", ".", "zeros", "(", "3", ")", ",", "np", ".", "zeros", "(", "3", ")", "\n", "for", "t", "in", "gold_ts_sequence", ":", "\n", "#print(t)", "\n", "        ", "ts_tag", "=", "t", "[", "2", "]", "\n", "tid", "=", "tag2tagid", "[", "ts_tag", "]", "\n", "gold_count", "[", "tid", "]", "+=", "1", "\n", "", "for", "t", "in", "pred_ts_sequence", ":", "\n", "        ", "ts_tag", "=", "t", "[", "2", "]", "\n", "tid", "=", "tag2tagid", "[", "ts_tag", "]", "\n", "if", "t", "in", "gold_ts_sequence", ":", "\n", "            ", "hit_count", "[", "tid", "]", "+=", "1", "\n", "", "pred_count", "[", "tid", "]", "+=", "1", "\n", "", "return", "hit_count", ",", "gold_count", ",", "pred_count", "\n", "\n"]], "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.compute_metrics_absa": [[436, 515], ["len", "np.zeros", "range", "range", "ts_f1.mean", "sum", "sum", "print", "sum", "np.zeros", "np.zeros", "np.zeros", "np.zeros", "np.zeros", "np.zeros", "glue_utils.match_ts", "float", "float", "len", "len", "seq_utils.ot2bieos_ts", "seq_utils.ot2bieos_ts", "seq_utils.tag2ts", "seq_utils.tag2ts", "float", "float", "float", "float", "Exception", "seq_utils.ot2bieos_ts", "seq_utils.ot2bieos_ts", "seq_utils.bio2ot_ts", "seq_utils.bio2ot_ts"], "function", ["home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.glue_utils.match_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.tag2ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.tag2ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.ot2bieos_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.bio2ot_ts", "home.repos.pwc.inspect_result.lixin4ever_BERT-E2E-ABSA.None.seq_utils.bio2ot_ts"], ["", "def", "compute_metrics_absa", "(", "preds", ",", "labels", ",", "all_evaluate_label_ids", ",", "tagging_schema", ")", ":", "\n", "    ", "if", "tagging_schema", "==", "'BIEOS'", ":", "\n", "        ", "absa_label_vocab", "=", "{", "'O'", ":", "0", ",", "'EQ'", ":", "1", ",", "'B-POS'", ":", "2", ",", "'I-POS'", ":", "3", ",", "'E-POS'", ":", "4", ",", "'S-POS'", ":", "5", ",", "\n", "'B-NEG'", ":", "6", ",", "'I-NEG'", ":", "7", ",", "'E-NEG'", ":", "8", ",", "'S-NEG'", ":", "9", ",", "\n", "'B-NEU'", ":", "10", ",", "'I-NEU'", ":", "11", ",", "'E-NEU'", ":", "12", ",", "'S-NEU'", ":", "13", "}", "\n", "", "elif", "tagging_schema", "==", "'BIO'", ":", "\n", "        ", "absa_label_vocab", "=", "{", "'O'", ":", "0", ",", "'EQ'", ":", "1", ",", "'B-POS'", ":", "2", ",", "'I-POS'", ":", "3", ",", "\n", "'B-NEG'", ":", "4", ",", "'I-NEG'", ":", "5", ",", "'B-NEU'", ":", "6", ",", "'I-NEU'", ":", "7", "}", "\n", "", "elif", "tagging_schema", "==", "'OT'", ":", "\n", "        ", "absa_label_vocab", "=", "{", "'O'", ":", "0", ",", "'EQ'", ":", "1", ",", "'T-POS'", ":", "2", ",", "'T-NEG'", ":", "3", ",", "'T-NEU'", ":", "4", "}", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Invalid tagging schema %s...\"", "%", "tagging_schema", ")", "\n", "", "absa_id2tag", "=", "{", "}", "\n", "for", "k", "in", "absa_label_vocab", ":", "\n", "        ", "v", "=", "absa_label_vocab", "[", "k", "]", "\n", "absa_id2tag", "[", "v", "]", "=", "k", "\n", "# number of true postive, gold standard, predicted targeted sentiment", "\n", "", "n_tp_ts", ",", "n_gold_ts", ",", "n_pred_ts", "=", "np", ".", "zeros", "(", "3", ")", ",", "np", ".", "zeros", "(", "3", ")", ",", "np", ".", "zeros", "(", "3", ")", "\n", "# precision, recall and f1 for aspect-based sentiment analysis", "\n", "ts_precision", ",", "ts_recall", ",", "ts_f1", "=", "np", ".", "zeros", "(", "3", ")", ",", "np", ".", "zeros", "(", "3", ")", ",", "np", ".", "zeros", "(", "3", ")", "\n", "n_samples", "=", "len", "(", "all_evaluate_label_ids", ")", "\n", "pred_y", ",", "gold_y", "=", "[", "]", ",", "[", "]", "\n", "class_count", "=", "np", ".", "zeros", "(", "3", ")", "\n", "for", "i", "in", "range", "(", "n_samples", ")", ":", "\n", "        ", "evaluate_label_ids", "=", "all_evaluate_label_ids", "[", "i", "]", "\n", "pred_labels", "=", "preds", "[", "i", "]", "[", "evaluate_label_ids", "]", "\n", "gold_labels", "=", "labels", "[", "i", "]", "[", "evaluate_label_ids", "]", "\n", "assert", "len", "(", "pred_labels", ")", "==", "len", "(", "gold_labels", ")", "\n", "# here, no EQ tag will be induced", "\n", "pred_tags", "=", "[", "absa_id2tag", "[", "label", "]", "for", "label", "in", "pred_labels", "]", "\n", "gold_tags", "=", "[", "absa_id2tag", "[", "label", "]", "for", "label", "in", "gold_labels", "]", "\n", "\n", "if", "tagging_schema", "==", "'OT'", ":", "\n", "            ", "gold_tags", "=", "ot2bieos_ts", "(", "gold_tags", ")", "\n", "pred_tags", "=", "ot2bieos_ts", "(", "pred_tags", ")", "\n", "", "elif", "tagging_schema", "==", "'BIO'", ":", "\n", "            ", "gold_tags", "=", "ot2bieos_ts", "(", "bio2ot_ts", "(", "gold_tags", ")", ")", "\n", "pred_tags", "=", "ot2bieos_ts", "(", "bio2ot_ts", "(", "pred_tags", ")", ")", "\n", "", "else", ":", "\n", "# current tagging schema is BIEOS, do nothing", "\n", "            ", "pass", "\n", "", "g_ts_sequence", ",", "p_ts_sequence", "=", "tag2ts", "(", "ts_tag_sequence", "=", "gold_tags", ")", ",", "tag2ts", "(", "ts_tag_sequence", "=", "pred_tags", ")", "\n", "\n", "hit_ts_count", ",", "gold_ts_count", ",", "pred_ts_count", "=", "match_ts", "(", "gold_ts_sequence", "=", "g_ts_sequence", ",", "\n", "pred_ts_sequence", "=", "p_ts_sequence", ")", "\n", "n_tp_ts", "+=", "hit_ts_count", "\n", "n_gold_ts", "+=", "gold_ts_count", "\n", "n_pred_ts", "+=", "pred_ts_count", "\n", "for", "(", "b", ",", "e", ",", "s", ")", "in", "g_ts_sequence", ":", "\n", "            ", "if", "s", "==", "'POS'", ":", "\n", "                ", "class_count", "[", "0", "]", "+=", "1", "\n", "", "if", "s", "==", "'NEG'", ":", "\n", "                ", "class_count", "[", "1", "]", "+=", "1", "\n", "", "if", "s", "==", "'NEU'", ":", "\n", "                ", "class_count", "[", "2", "]", "+=", "1", "\n", "", "", "", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "n_ts", "=", "n_tp_ts", "[", "i", "]", "\n", "n_g_ts", "=", "n_gold_ts", "[", "i", "]", "\n", "n_p_ts", "=", "n_pred_ts", "[", "i", "]", "\n", "ts_precision", "[", "i", "]", "=", "float", "(", "n_ts", ")", "/", "float", "(", "n_p_ts", "+", "SMALL_POSITIVE_CONST", ")", "\n", "ts_recall", "[", "i", "]", "=", "float", "(", "n_ts", ")", "/", "float", "(", "n_g_ts", "+", "SMALL_POSITIVE_CONST", ")", "\n", "ts_f1", "[", "i", "]", "=", "2", "*", "ts_precision", "[", "i", "]", "*", "ts_recall", "[", "i", "]", "/", "(", "ts_precision", "[", "i", "]", "+", "ts_recall", "[", "i", "]", "+", "SMALL_POSITIVE_CONST", ")", "\n", "\n", "", "macro_f1", "=", "ts_f1", ".", "mean", "(", ")", "\n", "\n", "# calculate micro-average scores for ts task", "\n", "# TP", "\n", "n_tp_total", "=", "sum", "(", "n_tp_ts", ")", "\n", "# TP + FN", "\n", "n_g_total", "=", "sum", "(", "n_gold_ts", ")", "\n", "print", "(", "\"class_count:\"", ",", "class_count", ")", "\n", "\n", "# TP + FP", "\n", "n_p_total", "=", "sum", "(", "n_pred_ts", ")", "\n", "micro_p", "=", "float", "(", "n_tp_total", ")", "/", "(", "n_p_total", "+", "SMALL_POSITIVE_CONST", ")", "\n", "micro_r", "=", "float", "(", "n_tp_total", ")", "/", "(", "n_g_total", "+", "SMALL_POSITIVE_CONST", ")", "\n", "micro_f1", "=", "2", "*", "micro_p", "*", "micro_r", "/", "(", "micro_p", "+", "micro_r", "+", "SMALL_POSITIVE_CONST", ")", "\n", "scores", "=", "{", "'macro-f1'", ":", "macro_f1", ",", "'precision'", ":", "micro_p", ",", "\"recall\"", ":", "micro_r", ",", "\"micro-f1\"", ":", "micro_f1", "}", "\n", "return", "scores", "\n", "\n"]]}