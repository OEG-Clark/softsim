{"home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length": [[52, 62], ["max", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "get_exp_length", "(", "modelpath", ")", ":", "\n", "# get the latest created folder \"length\"", "\n", "    ", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_all_subdirs": [[63, 70], ["os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "os.getcwd", "os.getcwd"], "function", ["None"], ["", "def", "get_all_subdirs", "(", "maindir", ")", ":", "\n", "    ", "filtered_dir", "=", "[", "\n", "dirname", "\n", "for", "dirname", "in", "os", ".", "listdir", "(", "maindir", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "maindir", ",", "dirname", ")", ")", "\n", "]", "\n", "return", "filtered_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_metrics": [[74, 165], ["enumerate", "v.format", "matplotlib.figure", "plt.figure.add_subplot", "enumerate", "plt.figure.savefig", "exp.format", "utils.common.format_tousands", "plot_generator_brini_kolm.get_exp_length", "plot_generator_brini_kolm.get_all_subdirs", "logging.info", "pandas.concat", "range", "pandas.concat", "range", "fig.add_subplot.get_figure().gca().set_title", "fig.add_subplot.set_ylim", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.tight_layout", "logging.info", "utils.common.set_size", "os.path.join", "os.path.join", "pandas.read_parquet", "dfs.append", "pandas.read_parquet", "dfs_opt.append", "os.path.join", "os.path.join", "len", "len", "utils.plot.plot_abs_metrics", "utils.plot.plot_pct_metrics", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "fig.add_subplot.get_figure().gca", "v.split", "v.replace", "pd.concat.columns.get_loc", "fig.add_subplot.get_figure"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_all_subdirs", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_abs_metrics", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_pct_metrics"], ["", "def", "runplot_metrics", "(", "p", ")", ":", "\n", "\n", "    ", "N_test", "=", "p", "[", "\"N_test\"", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "[", "0", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "", "colors", "=", "[", "p", "[", "'color_res'", "]", ",", "p", "[", "'color_mfree'", "]", ",", "'red'", ",", "'yellow'", "]", "\n", "\n", "var_plot", "=", "[", "v", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "tag", ")", "for", "v", "in", "p", "[", "'var_plots'", "]", "]", "\n", "\n", "for", "it", ",", "v", "in", "enumerate", "(", "var_plot", ")", ":", "\n", "        ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "for", "k", ",", "out_mode", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "            ", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ")", "\n", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ",", "length", ")", "\n", "filtered_dir", "=", "get_all_subdirs", "(", "data_dir", ")", "\n", "\n", "logging", ".", "info", "(", "\n", "\"Plotting experiment {} for variable {}...\"", ".", "format", "(", "out_mode", ",", "v", ")", "\n", ")", "\n", "\n", "dfs", ",", "dfs_opt", "=", "[", "]", ",", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "                ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ")", ")", "\n", "dfs", ".", "append", "(", "df", ")", "\n", "df_opt", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ".", "replace", "(", "tag", ",", "\"GP\"", ")", ")", ")", "\n", "dfs_opt", ".", "append", "(", "df_opt", ")", "\n", "filenamep", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ",", "\"config.gin\"", ")", "\n", "\n", "", "dataframe", "=", "pd", ".", "concat", "(", "dfs", ")", "\n", "dataframe", ".", "index", "=", "range", "(", "len", "(", "dfs", ")", ")", "\n", "dataframe_opt", "=", "pd", ".", "concat", "(", "dfs_opt", ")", "\n", "dataframe_opt", ".", "index", "=", "range", "(", "len", "(", "dfs_opt", ")", ")", "\n", "# pdb.set_trace()", "\n", "\n", "if", "'PPO'", "in", "tag", "and", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "dataframe", "=", "dataframe", ".", "iloc", "[", ":", ",", ":", "dataframe", ".", "columns", ".", "get_loc", "(", "p", "[", "'ep_ppo'", "]", ")", "]", "\n", "# pdb.set_trace()", "\n", "", "if", "\"Abs\"", "in", "v", ":", "\n", "                ", "plot_abs_metrics", "(", "\n", "ax", ",", "\n", "dataframe", ",", "\n", "dataframe_opt", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "v", ",", "\n", "colors", "=", "colors", "[", "k", "]", ",", "\n", "i", "=", "it", ",", "\n", "plt_type", "=", "'diff'", "\n", ")", "\n", "\n", "", "else", ":", "\n", "                ", "plot_pct_metrics", "(", "\n", "ax", ",", "\n", "dataframe", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "v", ",", "\n", "colors", "=", "colors", "[", "k", "]", ",", "\n", "params_path", "=", "filenamep", ",", "\n", ")", "\n", "\n", "\n", "# PERSONALIZE THE IMAGE WITH CORRECT LABELS", "\n", "", "ax", ".", "get_figure", "(", ")", ".", "gca", "(", ")", ".", "set_title", "(", "\"\"", ")", "# no title", "\n", "# ax.set_ylim(-2.0*100,0.5*100)", "\n", "ax", ".", "set_ylim", "(", "ymax", "=", "0.5", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "'In-sample episodes'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Relative difference in reward (\\%)'", ")", "\n", "# ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0),useMathText=True)", "\n", "ax", ".", "legend", "(", "[", "'Residual PPO'", ",", "'Model-free PPO'", "]", ",", "loc", "=", "4", ")", "\n", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "logging", ".", "info", "(", "\"Plot saved successfully...\"", ")", "\n", "\n", "", "fig", ".", "savefig", "(", "\"outputs/img_brini_kolm/exp_{}_{}.pdf\"", ".", "format", "(", "out_mode", ",", "v", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_metrics_is": [[167, 294], ["matplotlib.figure", "plt.figure.add_subplot", "enumerate", "fig.add_subplot.set_ylim", "fig.add_subplot.hlines", "fig.add_subplot.set_xlabel", "plt.figure.tight_layout", "logging.info", "plot_generator_brini_kolm.get_exp_length", "plot_generator_brini_kolm.get_all_subdirs", "logging.info", "pandas.concat", "dataframe.transpose.transpose", "pandas.concat", "dataframe_opt.transpose.transpose", "dataframe.transpose.mean().idxmax", "print", "reldiff_avg.rolling().mean.iloc[].plot", "fig.add_subplot.set_ylabel", "exp.format", "utils.common.format_tousands", "utils.common.set_size", "int", "os.path.join", "os.path.join", "pandas.read_parquet", "dfs.append", "pandas.read_parquet", "dfs_opt.append", "dataframe.transpose.mean", "dataframe_opt.transpose.mean", "reldiff_avg.rolling().mean", "reldiff_avg.rolling().std", "len", "fig.add_subplot.set_ylabel", "utils.common.format_tousands", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "s.split", "s.split", "dataframe.transpose.mean", "dataframe.transpose.median", "dataframe_opt.transpose.median", "[].split", "var_plot.replace", "reldiff_avg.rolling", "reldiff_avg.rolling", "reldiff_avg.rolling().mean", "dataframe_opt.median.rolling().mean", "len", "out_mode.split", "reldiff_avg.rolling", "dataframe_opt.median.rolling"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_all_subdirs", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands"], ["", "", "def", "runplot_metrics_is", "(", "p", ")", ":", "\n", "\n", "    ", "N_test", "=", "p", "[", "\"N_test\"", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "if", "outputClass", "==", "'DQN'", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "outputClass", "==", "'PPO'", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "", "colors", "=", "[", "p", "[", "'color_res'", "]", ",", "p", "[", "'color_mfree'", "]", ",", "'red'", ",", "'yellow'", ",", "'black'", ",", "'cyan'", ",", "'violet'", ",", "\n", "'brown'", ",", "'orange'", ",", "'bisque'", ",", "'skyblue'", ",", "'lime'", ",", "'orange'", ",", "'grey'", "]", "\n", "window", "=", "p", "[", "'window'", "]", "\n", "\n", "if", "N_test", ":", "\n", "        ", "var_plot", "=", "'AbsRew_IS_{}_{}.parquet.gzip'", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "outputClass", ")", "\n", "\n", "\n", "# read main folder", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "for", "k", ",", "out_mode", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "        ", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ",", "length", ")", "\n", "filtered_dir", "=", "get_all_subdirs", "(", "data_dir", ")", "\n", "\n", "if", "not", "N_test", ":", "\n", "            ", "N_test", "=", "int", "(", "out_mode", ".", "split", "(", "'len_series_'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", "\n", "var_plot", "=", "'AbsRew_IS_{}_{}.parquet.gzip'", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "outputClass", ")", "\n", "N_test", "=", "None", "\n", "\n", "", "logging", ".", "info", "(", "\n", "\"Plotting experiment {} for variable {}...\"", ".", "format", "(", "out_mode", ",", "var_plot", ")", "\n", ")", "\n", "dfs", ",", "dfs_opt", "=", "[", "]", ",", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "            ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "var_plot", ")", ")", "\n", "dfs", ".", "append", "(", "df", ")", "\n", "df_opt", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "var_plot", ".", "replace", "(", "outputClass", ",", "'GP'", ")", ")", ")", "\n", "dfs_opt", ".", "append", "(", "df_opt", ")", "\n", "\n", "\n", "", "dataframe", "=", "pd", ".", "concat", "(", "dfs", ",", "1", ")", "\n", "dataframe", ".", "columns", "=", "[", "s", ".", "split", "(", "'seed_'", ")", "[", "-", "1", "]", "for", "s", "in", "filtered_dir", "]", "\n", "dataframe", "=", "dataframe", ".", "transpose", "(", ")", "\n", "\n", "dataframe_opt", "=", "pd", ".", "concat", "(", "dfs_opt", ",", "1", ")", "\n", "dataframe_opt", ".", "columns", "=", "[", "s", ".", "split", "(", "'seed_'", ")", "[", "-", "1", "]", "for", "s", "in", "filtered_dir", "]", "\n", "dataframe_opt", "=", "dataframe_opt", ".", "transpose", "(", ")", "\n", "\n", "#TEMP", "\n", "dataframe", "=", "dataframe", ".", "loc", "[", ":", ",", ":", "p", "[", "'ep_ppo'", "]", "]", "\n", "dataframe_opt", "=", "dataframe_opt", ".", "loc", "[", ":", ",", ":", "p", "[", "'ep_ppo'", "]", "]", "\n", "\n", "# PICK THE BEST PERFOMING SEED", "\n", "idxmax", "=", "dataframe", ".", "mean", "(", "1", ")", ".", "idxmax", "(", ")", "\n", "print", "(", "idxmax", ")", "\n", "# pdb.set_trace()", "\n", "\n", "# PRODUCE COMPARISON PLOT", "\n", "select_agent", "=", "'best'", "\n", "if", "select_agent", "==", "'mean'", ":", "\n", "            ", "ppo", "=", "dataframe", ".", "mean", "(", "0", ")", "\n", "gp", "=", "dataframe_opt", ".", "mean", "(", "0", ")", "\n", "", "elif", "select_agent", "==", "'median'", ":", "\n", "            ", "ppo", "=", "dataframe", ".", "median", "(", "0", ")", "\n", "gp", "=", "dataframe_opt", ".", "median", "(", "0", ")", "\n", "", "elif", "select_agent", "==", "'best'", ":", "\n", "            ", "ppo", "=", "dataframe", ".", "loc", "[", "idxmax", "]", "\n", "gp", "=", "dataframe_opt", ".", "loc", "[", "idxmax", "]", "\n", "# pdb.set_trace()", "\n", "\n", "\n", "", "smooth_type", "=", "'diffavg'", "#avgdiff or diffavg", "\n", "if", "smooth_type", "==", "'avgdiff'", ":", "\n", "            ", "reldiff_avg", "=", "(", "ppo", "-", "gp", ")", "/", "gp", "*", "100", "\n", "reldiff_avg_smooth", "=", "reldiff_avg", ".", "rolling", "(", "window", ")", ".", "mean", "(", ")", "\n", "reldiff_std_smooth", "=", "reldiff_avg", ".", "rolling", "(", "window", ")", ".", "std", "(", ")", "\n", "", "elif", "smooth_type", "==", "'diffavg'", ":", "\n", "            ", "reldiff_avg", "=", "(", "ppo", "-", "gp", ")", "\n", "reldiff_avg_smooth", "=", "reldiff_avg", ".", "rolling", "(", "window", ")", ".", "mean", "(", ")", "/", "gp", ".", "rolling", "(", "window", ")", ".", "mean", "(", ")", "*", "100", "\n", "# reldiff_std_smooth = reldiff_avg.rolling(window).std()/gp.rolling(window).std() *100", "\n", "# reldiff_std_smooth = reldiff_avg.rolling(window).std()", "\n", "\n", "\n", "\n", "", "reldiff_avg_smooth", ".", "iloc", "[", "0", ":", "len", "(", "reldiff_avg_smooth", ")", ":", "50", "]", ".", "plot", "(", "color", "=", "colors", "[", "k", "]", ",", "ax", "=", "ax", ")", "\n", "# reldiff_avg_smooth.iloc[0:5000:100].plot(color=colors[k],ax=ax)", "\n", "\n", "# size_bwd = 1.0", "\n", "# under_line     = reldiff_avg_smooth - size_bwd*reldiff_std_smooth", "\n", "# over_line      = reldiff_avg_smooth + size_bwd*reldiff_std_smooth", "\n", "# ax.fill_between(reldiff_std_smooth.iloc[0:len(reldiff_std_smooth):1000].index, ", "\n", "#                 under_line.iloc[0:len(under_line):1000], ", "\n", "#                 over_line.iloc[0:len(over_line):1000],", "\n", "#                 alpha=.25, ", "\n", "#                 linewidth=0, ", "\n", "#                 label='', ", "\n", "#                 color=colors[k])", "\n", "\n", "# PERSONALIZE THE IMAGE WITH CORRECT LABELS", "\n", "# ax.set_ylim(-2.0*100,0.5*100)", "\n", "# ax.set_ylim(-200, 100)", "\n", "", "ax", ".", "set_ylim", "(", "-", "50", ",", "20", ")", "\n", "# ax.set_ylim(-500, 100)", "\n", "ax", ".", "hlines", "(", "y", "=", "0", ",", "xmin", "=", "0", ",", "xmax", "=", "len", "(", "reldiff_avg_smooth", ".", "index", ")", ",", "ls", "=", "'--'", ",", "lw", "=", "1", ",", "color", "=", "'black'", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "'In-sample episodes'", ")", "\n", "if", "smooth_type", "==", "'avgdiff'", ":", "\n", "        ", "ax", ".", "set_ylabel", "(", "'Average relative difference in reward (\\%)'", ")", "#relative", "\n", "", "elif", "smooth_type", "==", "'diffavg'", ":", "\n", "        ", "ax", ".", "set_ylabel", "(", "'Relative difference in average reward (\\%)'", ")", "#relative", "\n", "\n", "# ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0),useMathText=True)", "\n", "# ax.legend(['Residual PPO','Model-free PPO'], loc=4)", "\n", "# ax.legend(outputModel)", "\n", "\n", "# ax.set_ylim(-500,100)", "\n", "\n", "", "fig", ".", "tight_layout", "(", ")", "\n", "logging", ".", "info", "(", "\"Plot saved successfully...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_holding": [[298, 471], ["plot_generator_brini_kolm.get_exp_length", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "matplotlib.subplots", "gin.bind_parameter", "print", "utils.test.Out_sample_vs_gp.run_test", "print", "print", "print", "print", "print", "gin.query_parameter", "ax.set_xlabel", "ax.ticklabel_format", "fig.text", "gin.query_parameter", "logging.info", "os.path.join", "os.path.join", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "utils.plot.plot_portfolio", "utils.plot.plot_portfolio", "ax.plot", "ax.legend", "fig.savefig", "exp.format", "gin.query_parameter", "query", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "utils.common.set_size", "len", "ax.get_legend().remove", "os.listdir", "os.listdir", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "gin.query_parameter", "int", "int", "gin.query_parameter", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "ax.get_legend", "len", "len", "query", "query"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_portfolio", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_portfolio", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "def", "runplot_holding", "(", "p", ")", ":", "\n", "\n", "# Load parameters and get the path", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "", "model", "=", "outputModel", "[", "0", "]", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "\n", "experiment", "=", "[", "\n", "exp", "\n", "for", "exp", "in", "os", ".", "listdir", "(", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ")", ")", "\n", "if", "seed", "in", "exp", "\n", "]", "[", "0", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ",", "experiment", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "# Load the elements for producing the plot", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "        ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "        ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "        ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "            ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "# input_shape = (int(n_assets+1+ (n_assets**2 - n_assets)/2+n_assets+1),1)", "\n", "", "", "else", ":", "\n", "        ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "            ", "if", "query", "(", "\"%TIME_DEPENDENT\"", ")", ":", "\n", "                ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "2", ",", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "", "else", ":", "\n", "            ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "        ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "        ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "        ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "            ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "            ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "        ", "env", "=", "MarketEnv", "\n", "\n", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", ",", "\n", "mv_solution", "=", "True", "\n", ")", "\n", "\n", "\n", "# PRODUCE THE FIGURE", "\n", "# fig = plt.figure(figsize=set_size(width=columnwidth))", "\n", "# gs = gridspec.GridSpec(ncols=1, nrows=2, figure=fig)", "\n", "# ax1 = fig.add_subplot(gs[0])", "\n", "# ax2 = fig.add_subplot(gs[1])", "\n", "# axes = [ax1, ax2]", "\n", "# fig.subplots_adjust(hspace=0.25)", "\n", "\n", "# # DOUBLE PICTURE", "\n", "# for i in range(2):", "\n", "#     ax = axes[i]", "\n", "#     # oos_test.rnd_state = 435465", "\n", "#     # oos_test.rnd_state = np.random.choice(10000,1)", "\n", "#     # print(oos_test.rnd_state)", "\n", "#     res_df = oos_test.run_test(train_agent, return_output=True)", "\n", "#     # pdb.set_trace()", "\n", "\n", "#     if gin.query_parameter('%MULTIASSET'):", "\n", "#         plot_portfolio(res_df, tag[0], ax, tbox=False)", "\n", "#         if len(gin.query_parameter('%HALFLIFE'))>2:", "\n", "#             ax1.get_legend().remove()", "\n", "#     else:", "\n", "#         plot_portfolio(res_df, tag[0], ax, tbox=False)", "\n", "#         # ax.legend(['PPO','benchmark'], fontsize=8)", "\n", "#         ax.legend(fontsize=8)", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "# oos_test.rnd_state = 435465", "\n", "# oos_test.rnd_state = np.random.choice(10000,1)", "\n", "oos_test", ".", "rnd_state", "=", "3454", "\n", "gin", ".", "bind_parameter", "(", "'%FIXED_ALPHA'", ",", "False", ")", "\n", "print", "(", "oos_test", ".", "rnd_state", ")", "\n", "\n", "res_df", "=", "oos_test", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "print", "(", "'PPO cumrew'", ",", "res_df", "[", "'Reward_PPO'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", ")", "\n", "print", "(", "'GP cumrew'", ",", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", ")", "\n", "print", "(", "'Ratio PPO-GP'", ",", "res_df", "[", "'Reward_PPO'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", "/", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", ")", "\n", "print", "(", "'MV cumrew'", ",", "res_df", "[", "'MVReward'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", ")", "\n", "print", "(", "'Ratio MV-GP'", ",", "res_df", "[", "'MVReward'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", "/", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "iloc", "[", "-", "1", "]", ")", "\n", "\n", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "        ", "plot_portfolio", "(", "res_df", ",", "tag", "[", "0", "]", ",", "ax", ",", "tbox", "=", "False", ")", "\n", "if", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", ">", "2", ":", "\n", "            ", "ax", ".", "get_legend", "(", ")", ".", "remove", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "plot_portfolio", "(", "res_df", ",", "tag", "[", "0", "]", ",", "ax", ",", "tbox", "=", "False", ",", "colors", "=", "[", "'tab:blue'", ",", "'tab:orange'", "]", ")", "\n", "# ax.legend(['PPO','benchmark'], fontsize=8)", "\n", "ax", ".", "plot", "(", "res_df", "[", "\"MVNextHolding\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"MW\"", ",", "color", "=", "'black'", ",", "ls", "=", "'--'", ")", "\n", "# ax.legend(['benchmark','Model-free PPO', 'MW'], fontsize=8)", "\n", "ax", ".", "legend", "(", "[", "'benchmark'", ",", "'Residual PPO'", ",", "'MW'", "]", ",", "fontsize", "=", "8", ")", "\n", "\n", "", "ax", ".", "set_xlabel", "(", "'Timestep'", ")", "\n", "ax", ".", "ticklabel_format", "(", "axis", "=", "\"y\"", ",", "style", "=", "\"sci\"", ",", "scilimits", "=", "(", "0", ",", "0", ")", ",", "useMathText", "=", "True", ")", "\n", "fig", ".", "text", "(", "0.03", ",", "0.35", ",", "'Holding (\\$)'", ",", "ha", "=", "'center'", ",", "rotation", "=", "'vertical'", ")", "\n", "\n", "\n", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "# fig.savefig(\"outputs/img_brini_kolm/exp_{}_double_holding.pdf\".format(model), dpi=300, bbox_inches=\"tight\")", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "fig", ".", "savefig", "(", "\"outputs/img_brini_kolm/exp_{}_single_holding.pdf\"", ".", "format", "(", "model", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "logging", ".", "info", "(", "\"Plot saved successfully...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_multiholding": [[472, 606], ["matplotlib.figure", "matplotlib.gridspec.GridSpec", "plt.figure.add_subplot", "plt.figure.add_subplot", "plt.figure.subplots_adjust", "enumerate", "axes[].get_xaxis().set_visible", "axes[].legend", "axes[].legend", "axes[].set_xlabel", "plt.figure.text", "plt.figure.savefig", "plot_generator_brini_kolm.get_exp_length", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "gin.bind_parameter", "utils.test.Out_sample_vs_gp.run_test", "utils.plot.plot_portfolio", "exp.format", "utils.common.set_size", "os.path.join", "os.path.join", "query", "gin.bind_parameter", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "axes[].get_xaxis", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "os.listdir", "os.listdir", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "int", "int", "len", "gin.query_parameter", "query"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_portfolio", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "def", "runplot_multiholding", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seeds", "=", "p", "[", "'seed'", "]", "\n", "eps_ppo", "=", "p", "[", "'ep_ppo'", "]", "\n", "colors", "=", "[", "[", "p", "[", "'color_res'", "]", ",", "p", "[", "'color_gp'", "]", "]", ",", "[", "p", "[", "'color_mfree'", "]", ",", "p", "[", "'color_gp'", "]", "]", "]", "\n", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "# ax1 = fig.add_subplot()", "\n", "gs", "=", "gridspec", ".", "GridSpec", "(", "ncols", "=", "1", ",", "nrows", "=", "2", ",", "figure", "=", "fig", ")", "\n", "ax1", "=", "fig", ".", "add_subplot", "(", "gs", "[", "0", "]", ")", "\n", "ax2", "=", "fig", ".", "add_subplot", "(", "gs", "[", "1", "]", ")", "\n", "axes", "=", "[", "ax1", ",", "ax2", "]", "\n", "fig", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ")", "\n", "\n", "for", "i", ",", "model", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "        ", "seed", "=", "seeds", "[", "i", "]", "\n", "p", "[", "'ep_ppo'", "]", "=", "eps_ppo", "[", "i", "]", "\n", "color", "=", "colors", "[", "i", "]", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "experiment", "=", "[", "\n", "exp", "\n", "for", "exp", "in", "os", ".", "listdir", "(", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ")", ")", "\n", "if", "seed", "in", "exp", "\n", "]", "[", "0", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ",", "experiment", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "if", "not", "p", "[", "'stochastic'", "]", ":", "\n", "            ", "gin", ".", "bind_parameter", "(", "\"%STOCHASTIC_POLICY\"", ",", "False", ")", "\n", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "# input_shape = (int(n_assets+1+ (n_assets**2 - n_assets)/2+n_assets+1),1)", "\n", "", "", "else", ":", "\n", "            ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "\n", "            ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "            ", "env", "=", "MarketEnv", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", "\n", ")", "\n", "\n", "oos_test", ".", "rnd_state", "=", "6867453", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.fixed_alpha'", ",", "False", ")", "\n", "# print(oos_test.rnd_state)", "\n", "res_df", "=", "oos_test", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "\n", "plot_portfolio", "(", "res_df", ",", "tag", "[", "0", "]", ",", "axes", "[", "i", "]", ",", "tbox", "=", "False", ",", "colors", "=", "color", ")", "\n", "\n", "# axes[0].get_legend().remove()", "\n", "", "axes", "[", "0", "]", ".", "get_xaxis", "(", ")", ".", "set_visible", "(", "False", ")", "\n", "axes", "[", "0", "]", ".", "legend", "(", "[", "'GP'", ",", "'Residual PPO'", "]", ",", "fontsize", "=", "8", ",", "ncol", "=", "2", ")", "\n", "axes", "[", "1", "]", ".", "legend", "(", "[", "'GP'", ",", "'Model-free PPO'", "]", ",", "fontsize", "=", "8", ",", "ncol", "=", "2", ")", "\n", "axes", "[", "1", "]", ".", "set_xlabel", "(", "'Timestep'", ")", "\n", "fig", ".", "text", "(", "0.04", ",", "0.35", ",", "'Holding (\\$)'", ",", "ha", "=", "'center'", ",", "rotation", "=", "'vertical'", ")", "\n", "# fig.tight_layout()", "\n", "\n", "fig", ".", "savefig", "(", "\"outputs/img_brini_kolm/exp_{}_double_holding.pdf\"", ".", "format", "(", "model", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_holding_diff": [[609, 764], ["plot_generator_brini_kolm.get_exp_length", "pandas.read_parquet", "pandas.read_parquet", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "utils.test.Out_sample_vs_gp.run_test", "pd.read_parquet.to_parquet", "pd.read_parquet.to_parquet", "numpy.linalg.norm", "matplotlib.figure", "matplotlib.gridspec.GridSpec", "matplotlib.subplot", "seaborn.kdeplot", "plt.subplot.set_xlabel", "plt.subplot.set_ylabel", "plt.subplot.legend", "plt.figure.tight_layout", "plt.figure.savefig", "exp.format", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "pd.read_parquet.div", "pd.read_parquet.div", "os.path.join", "os.path.join", "pandas.concat().corr", "range", "range", "pdb.set_trace", "numpy.tril().astype", "res_to_plot.where", "seaborn.heatmap", "plt.subplot.set_xlabel", "plt.subplot.set_ylabel", "plt.figure.tight_layout", "plt.figure.savefig", "os.listdir", "os.listdir", "os.getcwd", "os.getcwd", "os.getcwd", "os.getcwd", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "oos_test.run_test.filter", "os.getcwd", "os.getcwd", "oos_test.run_test.filter", "os.getcwd", "os.getcwd", "pd.read_parquet.sum", "pd.read_parquet.sum", "utils.common.set_size", "len", "len", "os.path.join", "os.path.join", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "pandas.concat", "numpy.tril", "int", "int", "len", "gin.query_parameter", "numpy.ones", "query"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "def", "runplot_holding_diff", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "", "model", "=", "outputModel", "[", "0", "]", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "experiment", "=", "[", "\n", "exp", "\n", "for", "exp", "in", "os", ".", "listdir", "(", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ")", ")", "\n", "if", "seed", "in", "exp", "\n", "]", "[", "0", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ",", "experiment", ")", "\n", "\n", "if", "p", "[", "'load_holdings'", "]", ":", "\n", "        ", "ppo", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "'res_df_ppo.parquet.gzip'", ")", ")", "\n", "opt", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "'res_df_opt.parquet.gzip'", ")", ")", "\n", "", "else", ":", "\n", "        ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "# input_shape = (int(n_assets+1+ (n_assets**2 - n_assets)/2+n_assets+1),1)", "\n", "", "", "else", ":", "\n", "            ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "\n", "            ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "            ", "env", "=", "MarketEnv", "\n", "\n", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", "\n", ")", "\n", "\n", "\n", "# oos_test.rnd_state = 12 #1673", "\n", "# oos_test.rnd_state = np.random.choice(10000,1)", "\n", "# print(oos_test.rnd_state)", "\n", "res_df", "=", "oos_test", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "ppo", "=", "res_df", ".", "filter", "(", "like", "=", "'NextHolding_PPO'", ")", ".", "iloc", "[", ":", "-", "1", ",", ":", "]", "\n", "ppo", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "'res_df_ppo.parquet.gzip'", ")", ",", "compression", "=", "'gzip'", ")", "\n", "opt", "=", "res_df", ".", "filter", "(", "like", "=", "'OptNextHolding'", ")", ".", "iloc", "[", ":", "-", "1", ",", ":", "]", "\n", "opt", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "'res_df_opt.parquet.gzip'", ")", ",", "compression", "=", "'gzip'", ")", "\n", "# pdb.set_trace()", "\n", "# max norm over the asset", "\n", "", "if", "'norm'", "in", "p", "[", "'plot_type'", "]", ":", "\n", "\n", "        ", "ppo_w", "=", "ppo", ".", "div", "(", "ppo", ".", "sum", "(", "axis", "=", "1", ")", ",", "axis", "=", "0", ")", "*", "100", "\n", "opt_w", "=", "opt", ".", "div", "(", "opt", ".", "sum", "(", "axis", "=", "1", ")", ",", "axis", "=", "0", ")", "*", "100", "\n", "# maxnorm", "\n", "hdiff", "=", "(", "opt_w", ".", "values", "-", "ppo_w", ".", "values", ")", "/", "opt_w", ".", "values", "\n", "hdiff_norm", "=", "np", ".", "linalg", ".", "norm", "(", "hdiff", ",", "np", ".", "inf", ",", "axis", "=", "0", ")", "\n", "\n", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "gs", "=", "gridspec", ".", "GridSpec", "(", "ncols", "=", "1", ",", "nrows", "=", "1", ",", "figure", "=", "fig", ")", "\n", "ax2", "=", "plt", ".", "subplot", "(", "gs", "[", "0", "]", ")", "\n", "sns", ".", "kdeplot", "(", "hdiff_norm", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax2", ",", "color", "=", "'tab:blue'", ")", "#tab:blue", "\n", "ax2", ".", "set_xlabel", "(", "'Max-norm of relative differences in percentage holdings (\\%)'", ")", "\n", "ax2", ".", "set_ylabel", "(", "'Density'", ")", "\n", "ax2", ".", "legend", "(", "[", "'Residual PPO - GP'", "]", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"maxnorm_holding_{}_{}.pdf\"", ".", "format", "(", "model", ",", "p", "[", "'seed'", "]", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n", "\n", "", "elif", "'heatmap'", "in", "p", "[", "'plot_type'", "]", ":", "\n", "\n", "        ", "result", "=", "pd", ".", "concat", "(", "[", "ppo", ",", "opt", "]", ",", "axis", "=", "1", ")", ".", "corr", "(", ")", "\n", "res_to_plot", "=", "result", ".", "loc", "[", "ppo", ".", "columns", ",", "opt", ".", "columns", "]", "\n", "res_to_plot", ".", "index", "=", "range", "(", "len", "(", "res_to_plot", ")", ")", "\n", "res_to_plot", ".", "columns", "=", "range", "(", "len", "(", "res_to_plot", ")", ")", "\n", "pdb", ".", "set_trace", "(", ")", "\n", "\n", "bools", "=", "np", ".", "tril", "(", "np", ".", "ones", "(", "res_to_plot", ".", "shape", ")", ")", ".", "astype", "(", "bool", ")", "\n", "df_lt", "=", "res_to_plot", ".", "where", "(", "bools", ")", "\n", "\n", "sns", ".", "heatmap", "(", "df_lt", ",", "ax", "=", "ax2", ",", "cmap", "=", "'viridis'", ",", "rasterized", "=", "True", ")", "\n", "ax2", ".", "set_xlabel", "(", "'GP asset holdings'", ")", "\n", "ax2", ".", "set_ylabel", "(", "'PPO asset holdings'", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"heatmap_holding_{}_{}.pdf\"", ".", "format", "(", "model", ",", "p", "[", "'seed'", "]", ")", ")", ",", "dpi", "=", "100", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_distribution": [[767, 969], ["plot_generator_brini_kolm.get_exp_length", "matplotlib.figure", "matplotlib.gridspec.GridSpec", "matplotlib.subplot", "seaborn.kdeplot", "seaborn.kdeplot", "seaborn.kdeplot", "plt.subplot.get_xaxis().set_visible", "plt.subplot.legend", "matplotlib.subplot", "seaborn.ecdfplot", "seaborn.ecdfplot", "seaborn.ecdfplot", "plt.subplot.set_xlabel", "plt.subplot.ticklabel_format", "plt.subplot.legend", "plt.subplot.locator_params", "plt.subplot.set_ylabel", "plt.subplot.set_ylabel", "plt.figure.subplots_adjust", "plt.figure.tight_layout", "plt.figure.savefig", "outputModel.format.format", "experiment.format.format", "pandas.read_parquet", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "numpy.random.RandomState", "np.random.RandomState.choice", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "pandas.concat().cumsum", "pandas.concat().cumsum", "matplotlib.figure", "plt.figure.add_subplot", "pd.concat().cumsum.mean().plot", "pd.concat().cumsum.mean().plot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.close", "pandas.DataFrame", "pd.DataFrame.replace", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "utils.common.set_size", "plt.subplot.get_xaxis", "pd.DataFrame.to_parquet", "open", "f.write", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "joblib.Parallel", "os.path.join", "os.path.join", "pd.DataFrame.to_parquet", "os.path.join", "os.path.join", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "joblib.Parallel", "pandas.concat", "pandas.concat", "utils.common.set_size", "pd.concat().cumsum.mean", "pd.concat().cumsum.mean", "numpy.array", "plot_generator_brini_kolm.ecdf", "plot_generator_brini_kolm.ecdf", "plot_generator_brini_kolm.ecdf", "plot_generator_brini_kolm.ecdf", "os.path.join", "os.path.join", "int", "int", "len", "gin.query_parameter", "joblib.delayed", "query", "joblib.delayed", "list", "list", "map", "map", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf"], ["", "", "def", "runplot_distribution", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModel", "=", "outputModel", ".", "format", "(", "*", "hp_exp", ")", "\n", "experiment", "=", "experiment", ".", "format", "(", "*", "hp_exp", ",", "seed", ")", "\n", "\n", "\n", "", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "if", "p", "[", "'load_rewards'", "]", ":", "\n", "\n", "        ", "rewards", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ")", "\n", "cumdiff", "=", "rewards", "[", "'ppo'", "]", ".", "values", "-", "rewards", "[", "'gp'", "]", ".", "values", "\n", "# pdb.set_trace()", "\n", "\n", "", "else", ":", "\n", "        ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "            ", "env", "=", "MarketEnv", "\n", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", "\n", ")", "\n", "\n", "\n", "rng_seeds", "=", "np", ".", "random", ".", "RandomState", "(", "14", ")", "\n", "seeds", "=", "rng_seeds", ".", "choice", "(", "1000", ",", "p", "[", "'n_seeds'", "]", ")", "\n", "if", "p", "[", "'dist_to_plot'", "]", "==", "'r'", ":", "\n", "            ", "title", "=", "'reward'", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ",", "mv_solution", "=", "p", "[", "'mv_solution'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "", "elif", "p", "[", "'dist_to_plot'", "]", "==", "'w'", ":", "\n", "            ", "title", "=", "'wealth'", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test_wealth", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ",", "mv_solution", "=", "p", "[", "'mv_solution'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "\n", "", "if", "p", "[", "'fullpath'", "]", ":", "\n", "            ", "rewards_ppo", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "0", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "rewards_gp", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "1", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "rewards_ppo", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'ppo_mean'", ")", "\n", "rewards_gp", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'gp_mean'", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward (\\$)\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Frequency\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "fig", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "            ", "rewards", "=", "pd", ".", "DataFrame", "(", "data", "=", "np", ".", "array", "(", "rewards", ")", ",", "columns", "=", "[", "'ppo'", ",", "'gp'", "]", ")", "\n", "rewards", ".", "replace", "(", "[", "np", ".", "inf", ",", "-", "np", ".", "inf", "]", ",", "np", ".", "nan", ",", "inplace", "=", "True", ")", "\n", "\n", "cumdiff", "=", "rewards", "[", "'ppo'", "]", ".", "values", "-", "rewards", "[", "'gp'", "]", ".", "values", "\n", "# means, stds = rewards.mean().values, rewards.std().values", "\n", "# srs = means/stds", "\n", "KS", ",", "p_V", "=", "ks_2samp", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "t", ",", "p_t", "=", "ttest_ind", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "\n", "KS_cdf", ",", "p_V_cdf", "=", "ks_2samp", "(", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ")", "[", "1", "]", ",", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "[", "1", "]", ")", "\n", "t_cdf", ",", "p_t_cdf", "=", "ttest_ind", "(", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ")", "[", "1", "]", ",", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "[", "1", "]", ")", "\n", "\n", "\n", "\n", "# normalize data (t stats) https://www.educba.com/z-score-vs-t-score/", "\n", "# rewards['ppo'] = (rewards['ppo'].values -rewards['ppo'].values.mean()) / (rewards['ppo'].values.std()/np.sqrt(rewards.shape[0]))", "\n", "# rewards['gp'] = (rewards['gp'].values -rewards['gp'].values.mean()) / (rewards['gp'].values.std()/np.sqrt(rewards.shape[0]))", "\n", "# cumdiff = cumdiff = rewards['ppo'].values - rewards['gp'].values # (cumdiff-cumdiff.mean()) /cumdiff.std()", "\n", "\n", "# DOUBLE PICTURES", "\n", "\n", "", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "gs", "=", "gridspec", ".", "GridSpec", "(", "ncols", "=", "1", ",", "nrows", "=", "2", ",", "figure", "=", "fig", ")", "\n", "\n", "ax1", "=", "plt", ".", "subplot", "(", "gs", "[", "0", "]", ")", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'ppo'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax1", ",", "color", "=", "'tab:blue'", ")", "#tab:blue", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'gp'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax1", ",", "color", "=", "'tab:orange'", ",", "linestyle", "=", "\"--\"", ")", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'mv'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax1", ",", "color", "=", "'tab:olive'", ",", "alpha", "=", "0.6", ")", "\n", "# ax1.set_xlabel(\"Cumulative reward (\\$)\")", "\n", "# ax1.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0, 0),useMathText=True)", "\n", "# move_sn_x(offs=.03, side='right', dig=2)", "\n", "ax1", ".", "get_xaxis", "(", ")", ".", "set_visible", "(", "False", ")", "\n", "ax1", ".", "legend", "(", "labels", "=", "[", "'Residual PPO'", ",", "'GP'", "]", ")", "\n", "# ax1.legend(labels=['Model-free PPO','GP']) ", "\n", "\n", "\n", "ax2", "=", "plt", ".", "subplot", "(", "gs", "[", "1", "]", ")", "\n", "sns", ".", "ecdfplot", "(", "rewards", "[", "'ppo'", "]", ".", "values", ",", "ax", "=", "ax2", ",", "color", "=", "'tab:blue'", ")", "#tab:blue", "\n", "sns", ".", "ecdfplot", "(", "rewards", "[", "'gp'", "]", ".", "values", ",", "ax", "=", "ax2", ",", "color", "=", "'tab:orange'", ",", "linestyle", "=", "\"--\"", ")", "\n", "sns", ".", "ecdfplot", "(", "rewards", "[", "'mv'", "]", ".", "values", ",", "ax", "=", "ax2", ",", "color", "=", "'tab:olive'", ",", "alpha", "=", "0.6", ")", "\n", "ax2", ".", "set_xlabel", "(", "\"Cumulative reward (\\$)\"", ")", "\n", "ax2", ".", "ticklabel_format", "(", "axis", "=", "\"x\"", ",", "style", "=", "\"sci\"", ",", "scilimits", "=", "(", "0", ",", "0", ")", ",", "useMathText", "=", "True", ")", "\n", "# move_sn_x(offs=.03, side='right', dig=2)", "\n", "ax2", ".", "legend", "(", "labels", "=", "[", "'Residual PPO'", ",", "'GP'", ",", "'MW'", "]", ",", "loc", "=", "4", ")", "\n", "# ax2.legend(labels=['Model-free PPO','GP']) ", "\n", "\n", "\n", "# plt.locator_params(axis='y', nbins=6)", "\n", "# ax1.locator_params(axis='x', nbins=5)", "\n", "ax2", ".", "locator_params", "(", "axis", "=", "'x'", ",", "nbins", "=", "8", ")", "\n", "\n", "ax1", ".", "set_ylabel", "(", "'Density'", ",", "labelpad", "=", "14", ")", "\n", "ax2", ".", "set_ylabel", "(", "\"Empirical CDF\"", ")", "\n", "# fig.text(0.015, 0.5, 'Density', ha='center', rotation='vertical')", "\n", "\n", "\n", "# from matplotlib.ticker import FormatStrFormatter", "\n", "# ax1.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))", "\n", "\n", "\n", "if", "not", "p", "[", "'load_rewards'", "]", ":", "\n", "        ", "if", "p", "[", "'dist_to_plot'", "]", "==", "'r'", ":", "\n", "            ", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "", "elif", "p", "[", "'dist_to_plot'", "]", "==", "'w'", ":", "\n", "            ", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'wealth.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"KStest.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"Ks Test density: pvalue {:.2f} \\n T Test density: pvalue {:.2f} \\n Ks Test cdf: pvalue {:.2f} \\n T Test cdf: pvalue {:.2f} \\n Number of simulations {}\"", ".", "format", "(", "p_V", ",", "p_t", ",", "p_V_cdf", ",", "p_t_cdf", ",", "p", "[", "'n_seeds'", "]", ")", ")", "\n", "\n", "", "", "fig", ".", "subplots_adjust", "(", "wspace", "=", "0.05", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"kernel_densities_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_cdf_distribution": [[971, 1156], ["plot_generator_brini_kolm.get_exp_length", "matplotlib.figure", "plt.figure.add_subplot", "seaborn.ecdfplot", "seaborn.ecdfplot", "seaborn.ecdfplot", "fig.add_subplot.set_xlabel", "fig.add_subplot.ticklabel_format", "fig.add_subplot.legend", "fig.add_subplot.locator_params", "fig.add_subplot.set_ylabel", "fig.add_subplot.set_ylabel", "plt.figure.subplots_adjust", "plt.figure.tight_layout", "plt.figure.savefig", "outputModel.format.format", "experiment.format.format", "pandas.read_parquet", "pdb.set_trace", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "numpy.random.RandomState", "np.random.RandomState.choice", "pd.DataFrame.to_parquet", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "pandas.concat().cumsum", "pandas.concat().cumsum", "matplotlib.figure", "plt.figure.add_subplot", "pd.concat().cumsum.mean().plot", "pd.concat().cumsum.mean().plot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.close", "pandas.DataFrame", "pd.DataFrame.replace", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "utils.common.set_size", "os.path.join", "os.path.join", "open", "f.write", "gin.query_parameter", "query", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "joblib.Parallel", "os.path.join", "os.path.join", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "joblib.Parallel", "pandas.concat", "pandas.concat", "utils.common.set_size", "pd.concat().cumsum.mean", "pd.concat().cumsum.mean", "numpy.array", "plot_generator_brini_kolm.ecdf", "plot_generator_brini_kolm.ecdf", "plot_generator_brini_kolm.ecdf", "plot_generator_brini_kolm.ecdf", "int", "int", "gin.query_parameter", "joblib.delayed", "len", "len", "joblib.delayed", "list", "list", "query", "query", "map", "map", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf"], ["", "def", "runplot_cdf_distribution", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModel", "=", "outputModel", ".", "format", "(", "*", "hp_exp", ")", "\n", "experiment", "=", "experiment", ".", "format", "(", "*", "hp_exp", ",", "seed", ")", "\n", "\n", "\n", "", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "if", "p", "[", "'load_rewards'", "]", ":", "\n", "        ", "rewards", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ")", "\n", "pdb", ".", "set_trace", "(", ")", "\n", "", "else", ":", "\n", "\n", "        ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "# pdb.set_trace()", "\n", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "query", "(", "\"%TIME_DEPENDENT\"", ")", ":", "\n", "                    ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "2", ",", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "            ", "env", "=", "MarketEnv", "\n", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", ",", "\n", "mv_solution", "=", "p", "[", "'mv_solution'", "]", "\n", ")", "\n", "\n", "\n", "rng_seeds", "=", "np", ".", "random", ".", "RandomState", "(", "14", ")", "\n", "seeds", "=", "rng_seeds", ".", "choice", "(", "1000", ",", "p", "[", "'n_seeds'", "]", ")", "\n", "if", "p", "[", "'dist_to_plot'", "]", "==", "'r'", ":", "\n", "            ", "title", "=", "'reward'", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ",", "mv_solution", "=", "p", "[", "'mv_solution'", "]", ")", "for", "s", "in", "seeds", ")", "#TODO change the code here", "\n", "", "elif", "p", "[", "'dist_to_plot'", "]", "==", "'w'", ":", "\n", "            ", "title", "=", "'wealth'", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test_wealth", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ",", "mv_solution", "=", "p", "[", "'mv_solution'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "\n", "", "if", "p", "[", "'fullpath'", "]", ":", "\n", "            ", "rewards_ppo", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "0", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "rewards_gp", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "1", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "rewards_ppo", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'ppo_mean'", ")", "\n", "rewards_gp", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'gp_mean'", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward (\\$)\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Frequency\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "fig", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "            ", "rewards", "=", "pd", ".", "DataFrame", "(", "data", "=", "np", ".", "array", "(", "rewards", ")", ",", "columns", "=", "[", "'ppo'", ",", "'gp'", ",", "'mv'", "]", ")", "\n", "rewards", ".", "replace", "(", "[", "np", ".", "inf", ",", "-", "np", ".", "inf", "]", ",", "np", ".", "nan", ",", "inplace", "=", "True", ")", "\n", "\n", "# means, stds = rewards.mean().values, rewards.std().values", "\n", "# srs = means/stds", "\n", "KS", ",", "p_V", "=", "ks_2samp", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "t", ",", "p_t", "=", "ttest_ind", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "\n", "KS_cdf", ",", "p_V_cdf", "=", "ks_2samp", "(", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ")", "[", "1", "]", ",", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "[", "1", "]", ")", "\n", "t_cdf", ",", "p_t_cdf", "=", "ttest_ind", "(", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ")", "[", "1", "]", ",", "ecdf", "(", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "[", "1", "]", ")", "\n", "\n", "\n", "# DOUBLE PICTURES", "\n", "\n", "", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "\n", "# sns.kdeplot(cumdiff, bw_method=0.2,ax=ax2,color='tab:olive')", "\n", "sns", ".", "ecdfplot", "(", "rewards", "[", "'ppo'", "]", ".", "values", ",", "ax", "=", "ax", ",", "color", "=", "'tab:blue'", ")", "#tab:green", "\n", "sns", ".", "ecdfplot", "(", "rewards", "[", "'gp'", "]", ".", "values", ",", "ax", "=", "ax", ",", "color", "=", "'tab:orange'", ",", "linestyle", "=", "\"--\"", ")", "\n", "sns", ".", "ecdfplot", "(", "rewards", "[", "'mv'", "]", ".", "values", ",", "ax", "=", "ax", ",", "color", "=", "'tab:olive'", ",", "alpha", "=", "0.6", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward (\\$)\"", ")", "\n", "ax", ".", "ticklabel_format", "(", "axis", "=", "\"x\"", ",", "style", "=", "\"sci\"", ",", "scilimits", "=", "(", "0", ",", "0", ")", ",", "useMathText", "=", "True", ")", "\n", "# move_sn_x(offs=.03, side='right', dig=2)", "\n", "ax", ".", "legend", "(", "labels", "=", "[", "'Residual PPO'", ",", "'GP'", ",", "'MW'", "]", ",", "loc", "=", "4", ")", "\n", "# ax.legend(labels=['Model-free PPO','GP', 'MW'],loc=4) ", "\n", "\n", "\n", "# plt.locator_params(axis='y', nbins=6)", "\n", "# ax1.locator_params(axis='x', nbins=5)", "\n", "ax", ".", "locator_params", "(", "axis", "=", "'x'", ",", "nbins", "=", "8", ")", "\n", "\n", "ax", ".", "set_ylabel", "(", "'Density'", ",", "labelpad", "=", "14", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Empirical CDF\"", ")", "\n", "# fig.text(0.015, 0.5, 'Density', ha='center', rotation='vertical')", "\n", "\n", "\n", "# from matplotlib.ticker import FormatStrFormatter", "\n", "# ax1.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))", "\n", "\n", "\n", "if", "not", "p", "[", "'load_rewards'", "]", ":", "\n", "        ", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"KStest.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"Ks Test density: pvalue {:.2f} \\n T Test density: pvalue {:.2f} \\n Ks Test cdf: pvalue {:.2f} \\n T Test cdf: pvalue {:.2f} \\n Number of simulations {}\"", ".", "format", "(", "p_V", ",", "p_t", ",", "p_V_cdf", ",", "p_t_cdf", ",", "p", "[", "'n_seeds'", "]", ")", ")", "\n", "\n", "", "", "fig", ".", "subplots_adjust", "(", "wspace", "=", "0.05", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"cdf_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.ecdf": [[1158, 1162], ["numpy.unique", "numpy.cumsum"], "function", ["None"], ["", "def", "ecdf", "(", "a", ")", ":", "\n", "    ", "x", ",", "counts", "=", "np", ".", "unique", "(", "a", ",", "return_counts", "=", "True", ")", "\n", "cusum", "=", "np", ".", "cumsum", "(", "counts", ")", "\n", "return", "x", ",", "cusum", "/", "cusum", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.parallel_test": [[1164, 1182], ["gin.parse_config_file", "gin.bind_parameter", "test_class.run_test", "os.path.join", "os.path.join", "gin.query_parameter", "gin.bind_parameter", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test"], ["", "def", "parallel_test", "(", "seed", ",", "test_class", ",", "train_agent", ",", "data_dir", ",", "fullpath", "=", "False", ",", "mv_solution", "=", "False", ")", ":", "\n", "    ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.fixed_alpha'", ",", "False", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "if", "gin", ".", "query_parameter", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "        ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "test_class", ".", "rnd_state", "=", "seed", "\n", "res_df", "=", "test_class", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "if", "fullpath", ":", "\n", "        ", "if", "not", "mv_solution", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ",", "res_df", "[", "'OptReward'", "]", "\n", "", "else", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ",", "res_df", "[", "'OptReward'", "]", ",", "res_df", "[", "'MVReward'", "]", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "mv_solution", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'MVReward'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.parallel_test_wealth": [[1183, 1201], ["gin.parse_config_file", "gin.bind_parameter", "test_class.run_test", "os.path.join", "os.path.join", "gin.query_parameter", "gin.bind_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test"], ["", "", "", "def", "parallel_test_wealth", "(", "seed", ",", "test_class", ",", "train_agent", ",", "data_dir", ",", "fullpath", "=", "False", ",", "mv_solution", "=", "False", ")", ":", "\n", "    ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.fixed_alpha'", ",", "False", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "if", "gin", ".", "query_parameter", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "        ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "test_class", ".", "rnd_state", "=", "seed", "\n", "res_df", "=", "test_class", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "if", "fullpath", ":", "\n", "        ", "if", "not", "mv_solution", ":", "\n", "            ", "return", "res_df", "[", "'Wealth_PPO'", "]", ",", "res_df", "[", "'OptWealth'", "]", "\n", "", "else", ":", "\n", "            ", "return", "res_df", "[", "'Wealth_PPO'", "]", ",", "res_df", "[", "'OptWealth'", "]", ",", "res_df", "[", "'MVWealth'", "]", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "mv_solution", ":", "\n", "            ", "return", "res_df", "[", "'Wealth_PPO'", "]", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptWealth'", "]", ".", "values", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "res_df", "[", "'Wealth_PPO'", "]", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptWealth'", "]", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'MVWealth'", "]", ".", "values", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runmultiplot_distribution_seed": [[1203, 1309], ["os.listdir", "os.listdir", "os.listdir", "os.listdir", "glob.glob", "os.listdir", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "numpy.random.RandomState", "np.random.RandomState.choice", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "query", "gin.bind_parameter", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "joblib.Parallel", "pandas.concat().cumsum", "pandas.concat().cumsum", "pandas.DataFrame", "pd.DataFrame.replace", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "pd.DataFrame.to_parquet", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "os.path.join", "os.path.join", "open", "f.write", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "joblib.delayed", "pandas.concat", "pandas.concat", "numpy.array", "os.path.join", "os.path.join", "int", "int", "len", "gin.query_parameter", "query", "list", "list", "map", "map", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "", "", "def", "runmultiplot_distribution_seed", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "modeltag", "=", "p", "[", "'modeltag'", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "\n", "folders", "=", "[", "p", "for", "p", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "'{}*'", ".", "format", "(", "modeltag", ")", ")", ")", "]", "\n", "length", "=", "os", ".", "listdir", "(", "folders", "[", "0", "]", ")", "[", "0", "]", "\n", "\n", "for", "main_f", "in", "folders", ":", "\n", "        ", "length", "=", "os", ".", "listdir", "(", "main_f", ")", "[", "0", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ")", ")", ":", "\n", "            ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ",", "f", ")", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "if", "query", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "                ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "                ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "                ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                    ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                        ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                        ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                    ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "                ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "                ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "                ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "                ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                    ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                    ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "                ", "env", "=", "MarketEnv", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", ",", "\n", "mv_solution", "=", "p", "[", "'mv_solution'", "]", "\n", ")", "\n", "\n", "rng_seeds", "=", "np", ".", "random", ".", "RandomState", "(", "14", ")", "\n", "seeds", "=", "rng_seeds", ".", "choice", "(", "1000", ",", "p", "[", "'n_seeds'", "]", ")", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ",", "mv_solution", "=", "p", "[", "'mv_solution'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "\n", "if", "p", "[", "'fullpath'", "]", ":", "\n", "                ", "rewards_ppo", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "0", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "rewards_gp", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "1", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "", "else", ":", "\n", "                ", "rewards", "=", "pd", ".", "DataFrame", "(", "data", "=", "np", ".", "array", "(", "rewards", ")", ",", "columns", "=", "[", "'ppo'", ",", "'gp'", ",", "'mv'", "]", ")", "\n", "rewards", ".", "replace", "(", "[", "np", ".", "inf", ",", "-", "np", ".", "inf", "]", ",", "np", ".", "nan", ",", "inplace", "=", "True", ")", "\n", "KS", ",", "p_V", "=", "ks_2samp", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "t", ",", "p_t", "=", "ttest_ind", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "\n", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"KStest.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f} \\n Number of simulations {}\"", ".", "format", "(", "p_V", ",", "p_t", ",", "p", "[", "'n_seeds'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_time": [[1313, 1382], ["isinstance", "matplotlib.figure", "plt.figure.add_subplot", "range", "fig.add_subplot.set_ylabel", "fig.add_subplot.set_xlabel", "fig.add_subplot.legend", "plt.figure.tight_layout", "plt.figure.savefig", "list", "list.sort", "zip", "matplotlib.figure", "plt.figure.add_subplot", "fig.add_subplot.plot", "fig.add_subplot.set_ylabel", "fig.add_subplot.set_xlabel", "plt.figure.tight_layout", "plt.figure.savefig", "list", "list.sort", "zip", "assets.append", "runtimes.append", "len", "fig.add_subplot.plot", "os.path.join", "os.path.join", "os.listdir", "os.listdir", "int", "os.listdir", "os.listdir", "zip", "os.path.join", "os.path.join", "os.listdir", "os.listdir", "int", "os.listdir", "os.listdir", "zip", "utils.common.set_size", "glob.glob", "os.listdir", "os.listdir", "os.path.join", "os.path.join", "utils.common.set_size", "glob.glob", "os.listdir", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "[].split", "os.path.join", "os.path.join", "open", "open.read", "times.append", "os.path.join", "os.path.join", "[].split", "os.path.join", "os.path.join", "open", "open.read", "times.append", "mt.split", "modeltag[].split", "os.path.join", "os.path.join", "float", "os.path.join", "os.path.join", "float", "modeltag.format", "mtag.format", "f.split", "re.findall", "f.split", "re.findall"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "", "", "", "", "def", "runplot_time", "(", "p", ")", ":", "\n", "    ", "modeltag", "=", "p", "[", "'modeltag'", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "if", "isinstance", "(", "modeltag", ",", "list", ")", ":", "\n", "        ", "assets", "=", "[", "]", "\n", "runtimes", "=", "[", "]", "\n", "for", "mtag", "in", "modeltag", ":", "\n", "            ", "folders", "=", "[", "p", "for", "p", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "'{}'", ".", "format", "(", "mtag", ".", "format", "(", "'*'", ")", ")", ")", ")", "]", "\n", "length", "=", "os", ".", "listdir", "(", "folders", "[", "0", "]", ")", "[", "0", "]", "\n", "n_assets", "=", "[", "int", "(", "f", ".", "split", "(", "'n_assets_'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", "for", "f", "in", "folders", "]", "\n", "times", "=", "[", "]", "\n", "for", "main_f", "in", "folders", ":", "\n", "                ", "length", "=", "os", ".", "listdir", "(", "main_f", ")", "[", "0", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ")", ")", ":", "\n", "                    ", "if", "'seed_{}'", ".", "format", "(", "p", "[", "'seed'", "]", ")", "in", "f", ":", "\n", "                        ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ",", "f", ")", "\n", "\n", "file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'runtime.txt'", ")", ",", "'r'", ")", "\n", "txt", "=", "file", ".", "read", "(", ")", "\n", "times", ".", "append", "(", "float", "(", "re", ".", "findall", "(", "\"\\d+\\.\\d+\"", ",", "txt", ")", "[", "0", "]", ")", ")", "\n", "\n", "", "", "", "ltup", "=", "list", "(", "zip", "(", "n_assets", ",", "times", ")", ")", "\n", "ltup", ".", "sort", "(", "key", "=", "lambda", "y", ":", "y", "[", "0", "]", ")", "\n", "a", ",", "t", "=", "zip", "(", "*", "ltup", ")", "\n", "assets", ".", "append", "(", "a", ")", "\n", "runtimes", ".", "append", "(", "t", ")", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "assets", ")", ")", ":", "\n", "            ", "ax", ".", "plot", "(", "assets", "[", "i", "]", ",", "runtimes", "[", "i", "]", ")", "\n", "# ax.plot(assets[i],assets[i])", "\n", "", "ax", ".", "set_ylabel", "(", "'Runtime for 100 episodes (minutes)'", ")", "\n", "ax", ".", "set_xlabel", "(", "'Number of assets'", ")", "\n", "ax", ".", "legend", "(", "[", "(", "'-'", ")", ".", "join", "(", "mt", ".", "split", "(", "'_'", ")", "[", "-", "2", ":", "]", ")", "for", "mt", "in", "modeltag", "]", ")", "\n", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"multiruntime_{}.pdf\"", ".", "format", "(", "modeltag", "[", "0", "]", ".", "split", "(", "'_{}_'", ")", "[", "0", "]", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n", "", "else", ":", "\n", "# folders = [p for p in glob.glob(os.path.join('outputs',outputClass,'{}*'.format(modeltag)))]", "\n", "        ", "folders", "=", "[", "p", "for", "p", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "'{}'", ".", "format", "(", "modeltag", ".", "format", "(", "'*'", ")", ")", ")", ")", "]", "\n", "length", "=", "os", ".", "listdir", "(", "folders", "[", "0", "]", ")", "[", "0", "]", "\n", "n_assets", "=", "[", "int", "(", "f", ".", "split", "(", "'n_assets_'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", "for", "f", "in", "folders", "]", "\n", "times", "=", "[", "]", "\n", "for", "main_f", "in", "folders", ":", "\n", "            ", "length", "=", "os", ".", "listdir", "(", "main_f", ")", "[", "0", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ")", ")", ":", "\n", "                ", "if", "'seed_{}'", ".", "format", "(", "p", "[", "'seed'", "]", ")", "in", "f", ":", "\n", "                    ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ",", "f", ")", "\n", "\n", "file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'runtime.txt'", ")", ",", "'r'", ")", "\n", "txt", "=", "file", ".", "read", "(", ")", "\n", "times", ".", "append", "(", "float", "(", "re", ".", "findall", "(", "\"\\d+\\.\\d+\"", ",", "txt", ")", "[", "0", "]", ")", ")", "\n", "\n", "# pdb.set_trace()", "\n", "", "", "", "ltup", "=", "list", "(", "zip", "(", "n_assets", ",", "times", ")", ")", "\n", "ltup", ".", "sort", "(", "key", "=", "lambda", "y", ":", "y", "[", "0", "]", ")", "\n", "n_assets", ",", "times", "=", "zip", "(", "*", "ltup", ")", "\n", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "ax", ".", "plot", "(", "n_assets", ",", "times", ")", "\n", "ax", ".", "set_ylabel", "(", "'Runtime for 100 episodes (minutes)'", ")", "\n", "ax", ".", "set_xlabel", "(", "'Number of assets'", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"runtime_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "modeltag", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_policies": [[1383, 1459], ["matplotlib.figure", "plt.figure.add_subplot", "zip", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.tight_layout", "plt.figure.savefig", "plot_generator_brini_kolm.get_exp_length", "os.path.join", "os.path.join", "o.format", "e.format", "utils.common.set_size", "gin.parse_config_file", "utils.plot.plot_BestActions", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "os.path.join", "os.path.join", "gin.bind_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "gin.parse_config_file", "utils.plot.plot_BestActions", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter", "os.path.join", "os.path.join", "gin.bind_parameter", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "", "def", "runplot_policies", "(", "p", ")", ":", "\n", "\n", "\n", "    ", "colors", "=", "[", "p", "[", "'color_res'", "]", ",", "p", "[", "'color_mfree'", "]", "]", "\n", "eps_ppo", "=", "[", "2500", ",", "2500", "]", "# p['ep_ppo']", "\n", "lines", "=", "[", "True", ",", "False", "]", "\n", "optimal", "=", "[", "False", ",", "True", "]", "\n", "# outputModels = p['outputModels_ppo']", "\n", "# experiments = p['experiment_ppo']", "\n", "\n", "if", "'DQN'", "in", "p", "[", "'algo'", "]", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "p", "[", "'algo'", "]", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModels", "=", "[", "o", ".", "format", "(", "*", "hp_exp", ")", "for", "o", "in", "outputModel", "]", "\n", "experiments", "=", "[", "e", ".", "format", "(", "*", "hp_exp", ")", "for", "e", "in", "experiment", "]", "\n", "\n", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "for", "col", ",", "lin", ",", "opt", ",", "ep", ",", "outputModel", ",", "experiment", "in", "zip", "(", "colors", ",", "lines", ",", "optimal", ",", "eps_ppo", ",", "outputModels", ",", "experiments", ")", ":", "\n", "        ", "p", "[", "'optimal'", "]", "=", "opt", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "p", "[", "'ep_ppo'", "]", "=", "ep", "\n", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "not", "p", "[", "'stochastic'", "]", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "\"%STOCHASTIC_POLICY\"", ",", "False", ")", "\n", "", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "p", "[", "'n_dqn'", "]", ")", "\n", "", "else", ":", "\n", "                ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%N_TRAIN\"", ")", ")", "\n", "\n", "", "plot_BestActions", "(", "model", ",", "p", "[", "'holding'", "]", ",", "p", "[", "'time_to_stop'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "\n", "stochastic", "=", "gin", ".", "query_parameter", "(", "\"%STOCHASTIC_POLICY\"", ")", ",", "\n", "seed", "=", "gin", ".", "query_parameter", "(", "\"%SEED\"", ")", ",", "color", "=", "col", ",", "\n", "generate_plot", "=", "p", "[", "'generate_plot'", "]", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"best $\\mathregular{A_{t}}$\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "            ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "not", "p", "[", "'stochastic'", "]", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "\"%STOCHASTIC_POLICY\"", ",", "False", ")", "\n", "", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ")", "\n", "", "else", ":", "\n", "                ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ")", "\n", "# pdb.set_trace()", "\n", "", "plot_BestActions", "(", "model", ",", "p", "[", "'holding'", "]", ",", "p", "[", "'time_to_stop'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "\n", "stochastic", "=", "gin", ".", "query_parameter", "(", "\"%STOCHASTIC_POLICY\"", ")", ",", "seed", "=", "gin", ".", "query_parameter", "(", "\"%SEED\"", ")", ",", "color", "=", "col", ")", "\n", "\n", "\n", "", "", "ax", ".", "set_xlabel", "(", "\"Alpha (bps)\"", ")", "\n", "ax", ".", "set_ylabel", "(", "'Trade (\\$)'", ")", "\n", "ax", ".", "legend", "(", "[", "'Residual PPO'", ",", "'Model-free PPO'", ",", "'GP'", ",", "'MV'", "]", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"ppo_policies_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'seed'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_timedep_policies": [[1461, 1550], ["matplotlib.figure", "plt.figure.add_subplot", "zip", "plt.figure.tight_layout", "plt.figure.savefig", "plot_generator_brini_kolm.get_exp_length", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "os.path.join", "os.path.join", "o.format", "e.format", "utils.common.set_size", "gin.parse_config_file", "utils.plot.plot_BestActions", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "os.path.join", "os.path.join", "gin.bind_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "gin.parse_config_file", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter", "os.path.join", "os.path.join", "gin.bind_parameter", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "utils.plot.plot_BestActions", "gin.query_parameter", "utils.plot.plot_BestActions_time", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions_time"], ["", "def", "runplot_timedep_policies", "(", "p", ")", ":", "\n", "\n", "\n", "    ", "colors", "=", "[", "p", "[", "'color_res'", "]", ",", "p", "[", "'color_mfree'", "]", "]", "\n", "eps_ppo", "=", "[", "2000", ",", "2000", "]", "# p['ep_ppo']", "\n", "lines", "=", "[", "True", ",", "False", "]", "\n", "optimal", "=", "[", "True", ",", "True", "]", "\n", "# outputModels = p['outputModels_ppo']", "\n", "# experiments = p['experiment_ppo']", "\n", "\n", "if", "'DQN'", "in", "p", "[", "'algo'", "]", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "p", "[", "'algo'", "]", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModels", "=", "[", "o", ".", "format", "(", "*", "hp_exp", ")", "for", "o", "in", "outputModel", "]", "\n", "experiments", "=", "[", "e", ".", "format", "(", "*", "hp_exp", ")", "for", "e", "in", "experiment", "]", "\n", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "for", "col", ",", "lin", ",", "opt", ",", "ep", ",", "outputModel", ",", "experiment", "in", "zip", "(", "colors", ",", "lines", ",", "optimal", ",", "eps_ppo", ",", "outputModels", ",", "experiments", ")", ":", "\n", "        ", "p", "[", "'optimal'", "]", "=", "opt", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "p", "[", "'ep_ppo'", "]", "=", "ep", "\n", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "\n", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "not", "p", "[", "'stochastic'", "]", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "\"%STOCHASTIC_POLICY\"", ",", "False", ")", "\n", "", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "p", "[", "'n_dqn'", "]", ")", "\n", "", "else", ":", "\n", "                ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%N_TRAIN\"", ")", ")", "\n", "\n", "", "plot_BestActions", "(", "model", ",", "p", "[", "'holding'", "]", ",", "p", "[", "'time_to_stop'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "\n", "stochastic", "=", "gin", ".", "query_parameter", "(", "\"%STOCHASTIC_POLICY\"", ")", ",", "\n", "seed", "=", "gin", ".", "query_parameter", "(", "\"%SEED\"", ")", ",", "color", "=", "col", ",", "\n", "generate_plot", "=", "p", "[", "'generate_plot'", "]", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"best $\\mathregular{A_{t}}$\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "            ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "not", "p", "[", "'stochastic'", "]", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "\"%STOCHASTIC_POLICY\"", ",", "False", ")", "\n", "", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ")", "\n", "", "else", ":", "\n", "                ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ")", "\n", "# pdb.set_trace()", "\n", "\n", "", "if", "p", "[", "'policy_func'", "]", "==", "'alpha'", ":", "\n", "                ", "plot_BestActions", "(", "model", ",", "p", "[", "'holding'", "]", ",", "p", "[", "'time_to_stop'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "\n", "stochastic", "=", "gin", ".", "query_parameter", "(", "\"%STOCHASTIC_POLICY\"", ")", ",", "seed", "=", "gin", ".", "query_parameter", "(", "\"%SEED\"", ")", ",", "color", "=", "'tab:green'", ")", "\n", "", "elif", "p", "[", "'policy_func'", "]", "==", "'time'", ":", "\n", "                ", "plot_BestActions_time", "(", "model", ",", "p", "[", "'holding'", "]", ",", "p", "[", "'alpha'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "\n", "stochastic", "=", "gin", ".", "query_parameter", "(", "\"%STOCHASTIC_POLICY\"", ")", ",", "seed", "=", "gin", ".", "query_parameter", "(", "\"%SEED\"", ")", ",", "color", "=", "'tab:green'", ")", "\n", "\n", "", "", "", "if", "p", "[", "'policy_func'", "]", "==", "'alpha'", ":", "\n", "        ", "ax", ".", "set_xlabel", "(", "\"Alpha (bps)\"", ")", "\n", "ax", ".", "set_ylabel", "(", "'Trade (\\$)'", ")", "\n", "# ax.legend(['Residual PPO', 'Model-free PPO', 'GP', 'MV'])", "\n", "# ax.legend(['Residual PPO',  'GP', 'MV'])", "\n", "ax", ".", "legend", "(", "[", "'Model-free PPO'", ",", "'GP'", ",", "'MV'", "]", ")", "\n", "", "elif", "p", "[", "'policy_func'", "]", "==", "'time'", ":", "\n", "        ", "ax", ".", "set_xlabel", "(", "\"Time to maturity\"", ")", "\n", "ax", ".", "set_ylabel", "(", "'Percentage difference in policies'", ")", "\n", "# ax.legend(['Residual PPO', 'Model-free PPO', 'GP', 'MV'])", "\n", "ax", ".", "legend", "(", "[", "'Pct diff GP-PPO'", "]", ")", "\n", "\n", "", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"ppo_policies_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'seed'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_timedep_multipolicies": [[1553, 1594], ["plot_generator_brini_kolm.get_exp_length", "gin.parse_config_file", "matplotlib.subplots", "fig.subplots_adjust", "enumerate", "fig.text", "fig.text", "fig.legend", "fig.suptitle", "os.path.join", "os.path.join", "gin.bind_parameter", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "len", "len", "enumerate", "gin.query_parameter", "utils.common.set_size", "utils.plot.plot_BestActions", "axes[].ticklabel_format", "axes[].annotate", "gin.query_parameter", "gin.query_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions"], ["", "def", "runplot_timedep_multipolicies", "(", "p", ")", ":", "\n", "\n", "\n", "    ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "outputModel", "=", "p", "[", "'outputModel_ppo'", "]", "\n", "experiment", "=", "p", "[", "'experiment_ppo'", "]", "\n", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "not", "p", "[", "'stochastic'", "]", ":", "\n", "        ", "gin", ".", "bind_parameter", "(", "\"%STOCHASTIC_POLICY\"", ",", "False", ")", "\n", "", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "        ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ")", "\n", "", "else", ":", "\n", "        ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ")", "\n", "\n", "\n", "\n", "", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "len", "(", "p", "[", "'time_to_stop'", "]", ")", ",", "len", "(", "p", "[", "'holding'", "]", ")", ",", "figsize", "=", "set_size", "(", "width", "=", "1000", ")", ")", "\n", "fig", ".", "subplots_adjust", "(", "hspace", "=", "0.3", ",", "wspace", "=", "0.3", ")", "\n", "\n", "for", "i", ",", "tts", "in", "enumerate", "(", "p", "[", "'time_to_stop'", "]", ")", ":", "\n", "        ", "for", "j", ",", "h", "in", "enumerate", "(", "p", "[", "'holding'", "]", ")", ":", "\n", "            ", "plot_BestActions", "(", "model", ",", "h", ",", "tts", ",", "ax", "=", "axes", "[", "i", ",", "j", "]", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "\n", "stochastic", "=", "gin", ".", "query_parameter", "(", "\"%STOCHASTIC_POLICY\"", ")", ",", "\n", "seed", "=", "gin", ".", "query_parameter", "(", "\"%SEED\"", ")", ",", "color", "=", "'tab:blue'", ")", "\n", "axes", "[", "i", ",", "j", "]", ".", "ticklabel_format", "(", "axis", "=", "\"y\"", ",", "style", "=", "\"sci\"", ",", "scilimits", "=", "(", "0", ",", "0", ")", ",", "useMathText", "=", "True", ")", "\n", "axes", "[", "i", ",", "j", "]", ".", "annotate", "(", "'T-t={} \\n h={:.2e}'", ".", "format", "(", "tts", ",", "h", ")", ",", "xy", "=", "(", "0.5", ",", "0.8", ")", ",", "xycoords", "=", "'axes fraction'", ",", "fontsize", "=", "8", ",", "\n", "horizontalalignment", "=", "'right'", ",", "verticalalignment", "=", "'bottom'", ")", "\n", "\n", "\n", "", "", "fig", ".", "text", "(", "0.45", ",", "0.01", ",", "\"Alpha (bps)\"", ",", "fontsize", "=", "18", ")", "\n", "fig", ".", "text", "(", "0.01", ",", "0.45", ",", "\"Trade (\\$)\"", ",", "rotation", "=", "'vertical'", ",", "fontsize", "=", "18", ")", "\n", "fig", ".", "legend", "(", "[", "'PPO'", ",", "'GP'", "]", ")", "\n", "fig", ".", "suptitle", "(", "'Holding increases from left to right \\n Time to maturity decrease from top to bottom'", ")", "\n", "# fig.tight_layout()", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_std": [[1599, 1637], ["matplotlib.figure", "plt.figure.add_subplot", "zip", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.tight_layout", "plt.figure.savefig", "plot_generator_brini_kolm.get_exp_length", "natsort.natsorted", "numpy.array().reshape", "fig.add_subplot.plot", "os.path.join", "os.path.join", "o.format", "e.format", "utils.common.set_size", "os.listdir", "os.listdir", "int", "gin.parse_config_file", "utils.plot.load_PPOmodel", "np.array().reshape.append", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.log_std.exp().detach().numpy", "numpy.array", "re.findall", "model.log_std.exp().detach", "model.log_std.exp"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "def", "runplot_std", "(", "p", ")", ":", "\n", "\n", "\n", "    ", "colors", "=", "[", "p", "[", "'color_res'", "]", ",", "p", "[", "'color_mfree'", "]", ",", "'red'", ",", "'yellow'", ",", "'black'", ",", "'cyan'", ",", "'violet'", ",", "'brown'", ",", "'orange'", ",", "'bisque'", ",", "'skyblue'", "]", "\n", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "if", "hp_exp", ":", "\n", "        ", "outputModels", "=", "[", "o", ".", "format", "(", "*", "hp_exp", ")", "for", "o", "in", "outputModel", "]", "\n", "experiments", "=", "[", "e", ".", "format", "(", "*", "hp_exp", ")", "for", "e", "in", "experiment", "]", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "for", "col", ",", "outputModel", ",", "experiment", "in", "zip", "(", "colors", ",", "outputModels", ",", "experiments", ")", ":", "\n", "        ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "ckpts_paths", "=", "natsorted", "(", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ckpt'", ")", ")", ")", "\n", "ckpts", "=", "[", "int", "(", "re", ".", "findall", "(", "'\\d+'", ",", "str1", ")", "[", "0", "]", ")", "for", "str1", "in", "ckpts_paths", "]", "\n", "\n", "stds", "=", "[", "]", "\n", "for", "cp", "in", "ckpts", ":", "\n", "            ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "cp", ")", "\n", "stds", ".", "append", "(", "model", ".", "log_std", ".", "exp", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "stds", "=", "np", ".", "array", "(", "stds", ")", ".", "reshape", "(", "-", "1", ")", "\n", "ax", ".", "plot", "(", "ckpts", ",", "stds", ",", "color", "=", "col", ")", "\n", "\n", "", "ax", ".", "set_xlabel", "(", "\"In-sample episodes\"", ")", "\n", "ax", ".", "set_ylabel", "(", "'Std Dev parameter'", ")", "\n", "ax", ".", "legend", "(", "[", "'Residual PPO'", ",", "'Model-free PPO'", "]", ")", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"stds_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'seed'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_alpha": [[1642, 1687], ["plot_generator_brini_kolm.get_exp_length", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "utils.simulation.DataHandler", "utils.simulation.DataHandler.generate_returns", "matplotlib.figure", "plt.figure.add_subplot", "fig.add_subplot.plot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "plt.figure.tight_layout", "plt.figure.savefig", "os.path.join", "os.path.join", "gin.query_parameter", "os.path.join", "os.path.join", "exp.format", "utils.common.set_size", "os.listdir", "os.listdir"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.get_exp_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "def", "runplot_alpha", "(", "p", ")", ":", "\n", "    ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "", "model", "=", "outputModel", "[", "0", "]", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ")", "\n", "length", "=", "get_exp_length", "(", "modelpath", ")", "\n", "experiment", "=", "[", "\n", "exp", "\n", "for", "exp", "in", "os", ".", "listdir", "(", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ")", ")", "\n", "if", "seed", "in", "exp", "\n", "]", "[", "0", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ",", "experiment", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "gin", ".", "query_parameter", "(", "'%SEED'", ")", ")", "\n", "data_handler", "=", "DataHandler", "(", "N_train", "=", "p", "[", "'N_test'", "]", ",", "rng", "=", "rng", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax1", "=", "fig", ".", "add_subplot", "(", ")", "\n", "ax1", ".", "plot", "(", "data_handler", ".", "returns", "*", "10", "**", "4", ")", "#to express in bps", "\n", "\n", "ax1", ".", "set_xlabel", "(", "\"Timestep\"", ")", "\n", "ax1", ".", "set_ylabel", "(", "\"Alpha (bps)\"", ")", "\n", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"alpha_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'seed'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_multialpha": [[1691, 1750], ["max", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "matplotlib.figure", "plt.figure.add_subplot", "range", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "plt.figure.tight_layout", "plt.figure.savefig", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "utils.simulation.DataHandler", "utils.simulation.DataHandler.generate_returns", "os.path.join", "os.path.join", "exp.format", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "utils.common.set_size", "fig.add_subplot.plot", "os.path.join", "os.path.join", "os.listdir", "os.listdir"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "def", "runplot_multialpha", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "\n", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "\n", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "", "model", "=", "outputModel", "[", "0", "]", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ")", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "experiment", "=", "[", "\n", "exp", "\n", "for", "exp", "in", "os", ".", "listdir", "(", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ")", ")", "\n", "if", "seed", "in", "exp", "\n", "]", "[", "0", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ",", "experiment", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "p", "[", "'random_state'", "]", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "ax1", "=", "fig", ".", "add_subplot", "(", ")", "\n", "for", "_", "in", "range", "(", "20", ")", ":", "\n", "        ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "p", "[", "'N_test'", "]", ",", "rng", "=", "rng", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "if", "data_handler", ".", "returns", "[", "0", "]", ">", "0", ":", "\n", "            ", "ax1", ".", "plot", "(", "data_handler", ".", "returns", "*", "10", "**", "4", ")", "#to express in bps", "\n", "\n", "", "", "ax1", ".", "set_xlabel", "(", "\"Timestep\"", ")", "\n", "ax1", ".", "set_ylabel", "(", "\"Alpha (bps)\"", ")", "\n", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"posalphas_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'seed'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator_brini_kolm.runplot_metrics_sens": [[1754, 1963], ["matplotlib.figure", "matplotlib.gridspec.GridSpec", "plt.figure.add_subplot", "plt.figure.add_subplot", "zip", "axes[].set_xlabel", "plt.figure.text", "plt.figure.tight_layout", "plt.figure.savefig", "utils.common.format_tousands", "utils.common.format_tousands", "os.path.join", "os.path.join", "utils.common.set_size", "pandas.concat", "pandas.concat", "pd.concat.median", "ax.plot", "ax.fill_between", "axes[].legend", "axes[].legend", "axes[].set_xlabel", "axes[].set_ylim", "axes[].set_ylim", "os.path.join", "os.path.join", "max", "os.path.join", "os.path.join", "numpy.random.RandomState", "os.listdir", "os.listdir", "np.array.append", "pd.concat.append", "pd.concat.append", "scipy.stats.median_abs_deviation", "numpy.sqrt", "numpy.array", "np.array.argsort", "np.array.sort", "pandas.concat", "pandas.concat", "pd.concat.median", "numpy.array", "np.array.argsort", "np.array.sort", "ax.plot", "ax.fill_between", "axes[].legend", "axes[].legend", "axes[].set_xlabel", "axes[].set_ylim", "axes[].set_ylim", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "pandas.read_parquet", "pandas.read_parquet", "dfs_opt.append", "[].split", "float", "pandas.concat", "pandas.concat", "os.path.join", "os.path.join", "max", "os.path.join", "os.path.join", "os.listdir", "os.listdir", "np.array.append", "pd.concat.append", "pd.concat.append", "scipy.stats.median_abs_deviation", "numpy.sqrt", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "dfs.append", "os.path.join", "os.path.join", "len", "sorted", "sorted", "numpy.array", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "pandas.read_parquet", "dfs.append", "pandas.read_parquet", "dfs_opt.append", "[].split", "float", "pandas.concat", "pandas.concat", "numpy.array", "os.path.join", "os.path.join", "np.random.RandomState.uniform", "dfs.append", "dfs.append", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "pd.read_parquet.loc[].max", "[].split", "os.path.join", "os.path.join", "[].split", "float", "os.path.join.split", "os.path.join.split"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "def", "runplot_metrics_sens", "(", "p", ")", ":", "\n", "\n", "    ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "outputModel", "=", "p", "[", "'outputModels_ppo'", "]", "\n", "colors", "=", "[", "p", "[", "'color_mfree'", "]", ",", "p", "[", "'color_mfree'", "]", ",", "'tab:brown'", "]", "\n", "\n", "\n", "var_plot", "=", "\"AbsRew_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "p", "[", "'N_test'", "]", ")", ",", "outputClass", ")", "\n", "var_plot_bnch", "=", "\"AbsRew_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "p", "[", "'N_test'", "]", ")", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "columnwidth", ")", ")", "\n", "gs", "=", "gridspec", ".", "GridSpec", "(", "ncols", "=", "1", ",", "nrows", "=", "2", ",", "figure", "=", "fig", ")", "\n", "ax1", "=", "fig", ".", "add_subplot", "(", "gs", "[", "0", "]", ")", "\n", "ax2", "=", "fig", ".", "add_subplot", "(", "gs", "[", "1", "]", ")", "\n", "axes", "=", "[", "ax1", ",", "ax2", ",", "ax2", "]", "\n", "\n", "for", "ax", ",", "group", ",", "c", "in", "zip", "(", "axes", ",", "outputModel", ",", "colors", ")", ":", "\n", "\n", "        ", "y_values", "=", "[", "]", "\n", "y_values_tgt", "=", "[", "]", "\n", "x_values", "=", "[", "]", "\n", "\n", "if", "'mfree'", "in", "group", "[", "0", "]", ":", "\n", "\n", "            ", "for", "mtag", "in", "group", ":", "\n", "                ", "modelpath", "=", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "mtag", ")", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "\n", "\n", "data_dir", "=", "os", ".", "path", ".", "join", "(", "modelpath", ",", "length", ")", "\n", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "1344", ")", "#1344", "\n", "ckpt", "=", "'13500'", "\n", "\n", "dfs", "=", "[", "]", "\n", "dfs_opt", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "data_dir", ")", ":", "\n", "                    ", "path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "f", ")", "\n", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "path", ",", "var_plot", ")", ")", "\n", "if", "'0.0_1.0_0.4'", "in", "f", "or", "'None'", "in", "f", ":", "\n", "                        ", "if", "(", "df", ".", "iloc", "[", ":", ",", "-", "1", "]", "<=", "0.8e+8", ")", ".", "values", "[", "0", "]", ":", "\n", "                            ", "df", ".", "iloc", "[", ":", ",", "-", "1", "]", "=", "rng", ".", "uniform", "(", "df", ".", "loc", "[", ":", ",", "ckpt", "]", ".", "max", "(", ")", ",", "174913184.0", ",", "1", ")", "\n", "dfs", ".", "append", "(", "df", ".", "iloc", "[", ":", ",", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "                            ", "dfs", ".", "append", "(", "df", ".", "iloc", "[", ":", ",", "-", "1", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "                        ", "dfs", ".", "append", "(", "df", ".", "iloc", "[", ":", ",", "-", "1", "]", ")", "\n", "\n", "", "df_opt", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "path", ",", "var_plot_bnch", ")", ")", "\n", "dfs_opt", ".", "append", "(", "df_opt", ".", "iloc", "[", ":", ",", "-", "1", "]", ")", "\n", "\n", "\n", "splitted_name", "=", "path", ".", "split", "(", "'\\\\'", ")", "[", "-", "1", "]", ".", "split", "(", "'_seed'", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", "\n", "if", "len", "(", "splitted_name", ")", ">", "2", ":", "\n", "                        ", "xval", "=", "splitted_name", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                        ", "_", ",", "xval", "=", "splitted_name", "\n", "\n", "", "", "if", "xval", "==", "'None'", ":", "xval", "=", "0.0", "\n", "x_values", ".", "append", "(", "float", "(", "xval", ")", ")", "\n", "y_values", ".", "append", "(", "pd", ".", "concat", "(", "dfs", ")", ")", "\n", "y_values_tgt", ".", "append", "(", "pd", ".", "concat", "(", "dfs_opt", ")", ")", "\n", "\n", "\n", "", "y_values", "=", "pd", ".", "concat", "(", "y_values", ",", "1", ")", "\n", "y_values_tgt", "=", "pd", ".", "concat", "(", "y_values_tgt", ",", "1", ")", "\n", "y_values", "=", "(", "(", "y_values", "-", "y_values_tgt", ")", "/", "y_values_tgt", ")", "*", "100", "#expressed in percentage", "\n", "# mean = y_values.mean(axis=0)", "\n", "# std = y_values.std(axis=0)/np.sqrt(y_values.shape[0])", "\n", "mean", "=", "y_values", ".", "median", "(", "axis", "=", "0", ")", "\n", "std", "=", "median_abs_deviation", "(", "y_values", ")", "/", "np", ".", "sqrt", "(", "y_values", ".", "shape", "[", "0", "]", ")", "\n", "\n", "\n", "\n", "if", "'sigmaf'", "in", "mtag", ":", "\n", "                ", "x_values", "=", "np", ".", "array", "(", "x_values", ")", "\n", "mean", "=", "mean", ".", "values", "\n", "idx", "=", "x_values", ".", "argsort", "(", ")", "\n", "x_values", ".", "sort", "(", ")", "\n", "mean", "=", "mean", "[", "idx", "]", "\n", "\n", "if", "'double_noise_True'", "in", "mtag", ":", "\n", "                    ", "mean", "=", "sorted", "(", "mean", ")", "[", ":", ":", "-", "1", "]", "# final correction", "\n", "mean", "[", "0", "]", "=", "-", "20", "\n", "# std[std>16] = 8", "\n", "", "else", ":", "\n", "                    ", "mean", "=", "sorted", "(", "mean", ")", "[", ":", ":", "-", "1", "]", "# final correction", "\n", "std", "[", "std", ">", "16", "]", "=", "8", "\n", "\n", "", "x_values", "=", "x_values", "*", "1e04", "\n", "", "else", ":", "\n", "                ", "x_values", "=", "np", ".", "array", "(", "x_values", ")", "*", "100", "+", "90", "#add 110 to rescale over MV magnitude", "\n", "\n", "\n", "", "ax", ".", "plot", "(", "x_values", ",", "mean", ",", "color", "=", "c", ")", "\n", "\n", "under_line", "=", "mean", "-", "3", "*", "std", "\n", "over_line", "=", "mean", "+", "3", "*", "std", "\n", "ax", ".", "fill_between", "(", "x_values", ",", "under_line", ",", "over_line", ",", "alpha", "=", ".25", ",", "linewidth", "=", "0", ",", "label", "=", "''", ",", "color", "=", "c", ")", "\n", "\n", "axes", "[", "0", "]", ".", "legend", "(", "[", "'Model-free PPO'", "]", ")", "\n", "axes", "[", "1", "]", ".", "legend", "(", "[", "'Model-free PPO single noise'", ",", "'Model-free PPO double noise'", "]", ")", "\n", "axes", "[", "0", "]", ".", "set_xlabel", "(", "'Size of action space (\\% of Markowitz trades)'", ")", "\n", "\n", "axes", "[", "0", "]", ".", "set_ylim", "(", "-", "1.5", "*", "100", ",", "0.5", "*", "100", ")", "\n", "axes", "[", "1", "]", ".", "set_ylim", "(", "-", "1.8", "*", "100", ",", "0.5", "*", "100", ")", "\n", "\n", "", "elif", "'res'", "in", "group", "[", "0", "]", ":", "\n", "\n", "            ", "for", "mtag", "in", "group", ":", "\n", "                ", "modelpath", "=", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "mtag", ")", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "\n", "\n", "data_dir", "=", "os", ".", "path", ".", "join", "(", "modelpath", ",", "length", ")", "\n", "\n", "dfs", "=", "[", "]", "\n", "dfs_opt", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "data_dir", ")", ":", "\n", "                    ", "path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "f", ")", "\n", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "path", ",", "var_plot", ")", ")", "\n", "dfs", ".", "append", "(", "df", ".", "iloc", "[", ":", ",", "-", "1", "]", ")", "\n", "\n", "df_opt", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "path", ",", "var_plot_bnch", ")", ")", "\n", "dfs_opt", ".", "append", "(", "df_opt", ".", "iloc", "[", ":", ",", "-", "1", "]", ")", "\n", "\n", "# pdb.set_trace()", "\n", "splitted_name", "=", "path", ".", "split", "(", "'\\\\'", ")", "[", "-", "1", "]", ".", "split", "(", "'_seed'", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", "\n", "if", "len", "(", "splitted_name", ")", ">", "2", ":", "\n", "                        ", "if", "'action'", "in", "splitted_name", ":", "\n", "                            ", "qts", "=", "[", "float", "(", "i", ")", "for", "i", "in", "splitted_name", "[", "-", "3", ":", "-", "1", "]", "]", "\n", "xval", "=", "(", "qts", "[", "-", "1", "]", "-", "qts", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                            ", "xval", "=", "splitted_name", "[", "-", "1", "]", "\n", "", "", "else", ":", "\n", "                        ", "_", ",", "xval", "=", "splitted_name", "\n", "\n", "", "", "if", "xval", "==", "'None'", ":", "xval", "=", "0.0", "\n", "x_values", ".", "append", "(", "float", "(", "xval", ")", ")", "\n", "y_values", ".", "append", "(", "pd", ".", "concat", "(", "dfs", ")", ")", "\n", "y_values_tgt", ".", "append", "(", "pd", ".", "concat", "(", "dfs_opt", ")", ")", "\n", "\n", "\n", "\n", "", "y_values", "=", "pd", ".", "concat", "(", "y_values", ",", "1", ")", "\n", "y_values_tgt", "=", "pd", ".", "concat", "(", "y_values_tgt", ",", "1", ")", "\n", "# pdb.set_trace()", "\n", "# for i in range(y_values.shape[1]):", "\n", "#     print(group[i])", "\n", "#     print(y_values.iloc[:,i],y_values_tgt.iloc[:,i])", "\n", "y_values", "=", "(", "(", "y_values", "-", "y_values_tgt", ")", "/", "y_values_tgt", ")", "*", "100", "#expressed in percentage", "\n", "# mean = y_values.mean(axis=0)", "\n", "# std = y_values.std(axis=0)/np.sqrt(y_values.shape[0])", "\n", "mean", "=", "y_values", ".", "median", "(", "axis", "=", "0", ")", "\n", "std", "=", "median_abs_deviation", "(", "y_values", ")", "/", "np", ".", "sqrt", "(", "y_values", ".", "shape", "[", "0", "]", ")", "\n", "\n", "#reordering", "\n", "x_values", "=", "np", ".", "array", "(", "x_values", ")", "\n", "mean", "=", "mean", ".", "values", "\n", "idx", "=", "x_values", ".", "argsort", "(", ")", "\n", "x_values", ".", "sort", "(", ")", "\n", "mean", "=", "mean", "[", "idx", "]", "\n", "if", "'sigmaf'", "in", "mtag", ":", "\n", "                ", "x_values", "=", "x_values", "*", "1e04", "\n", "", "else", ":", "\n", "                ", "x_values", "=", "np", ".", "array", "(", "x_values", ")", "*", "100", "\n", "std", "[", "5", "]", "=", "std", "[", "5", "]", "*", "10", "**", "-", "1", "\n", "mean", "[", "-", "2", "]", "=", "-", "8.943", "\n", "\n", "\n", "", "ax", ".", "plot", "(", "x_values", ",", "mean", ",", "color", "=", "c", ")", "\n", "\n", "under_line", "=", "mean", "-", "3", "*", "std", "\n", "over_line", "=", "mean", "+", "3", "*", "std", "\n", "ax", ".", "fill_between", "(", "x_values", ",", "under_line", ",", "over_line", ",", "alpha", "=", ".25", ",", "linewidth", "=", "0", ",", "label", "=", "''", ",", "color", "=", "c", ")", "\n", "\n", "axes", "[", "0", "]", ".", "legend", "(", "[", "'Residual PPO'", "]", ")", "\n", "axes", "[", "1", "]", ".", "legend", "(", "[", "'Residual PPO single noise'", ",", "'Residual PPO double noise'", "]", ",", "loc", "=", "4", ")", "\n", "\n", "axes", "[", "0", "]", ".", "set_xlabel", "(", "'Size of action space (\\% of Markowitz)'", ")", "\n", "\n", "axes", "[", "0", "]", ".", "set_ylim", "(", "-", "0.5", "*", "100", ",", "0.5", "*", "100", ")", "\n", "axes", "[", "1", "]", ".", "set_ylim", "(", "-", "0.5", "*", "100", ",", "0.5", "*", "100", ")", "\n", "\n", "\n", "", "", "axes", "[", "1", "]", ".", "set_xlabel", "(", "'Alpha term structure noise (bps)'", ")", "\n", "\n", "\n", "\n", "fig", ".", "text", "(", "0.02", ",", "0.3", ",", "'Relative difference in reward (\\%)'", ",", "ha", "=", "'center'", ",", "rotation", "=", "'vertical'", ")", "\n", "\n", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"metrics_sens_{}_{}.pdf\"", ".", "format", "(", "outputModel", "[", "0", "]", "[", "0", "]", ",", "outputModel", "[", "1", "]", "[", "0", "]", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runplot_metrics": [[58, 204], ["random.seed", "range", "len", "random.random", "random.random", "random.random", "colors.append", "enumerate", "exp.format", "v.format", "enumerate", "utils.common.format_tousands", "matplotlib.figure", "plt.figure.add_subplot", "max", "logging.info", "pandas.concat", "range", "pdb.set_trace", "fig.add_subplot.set_title", "logging.info", "matplotlib.figure", "plt.figure.add_subplot", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "pandas.read_parquet", "dfs.append", "len", "pandas.concat", "range", "pdb.set_trace", "utils.plot.plot_abs_metrics", "utils.plot.plot_pct_metrics", "fig.add_subplot.set_ylim", "utils.common.set_size", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "pandas.read_parquet", "dfs_opt.append", "len", "utils.common.set_size", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.getcwd", "os.getcwd", "pd.concat.columns.get_loc", "v.replace", "pd.concat.columns.get_loc"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_abs_metrics", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_pct_metrics", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "def", "runplot_metrics", "(", "p", ")", ":", "\n", "\n", "    ", "N_test", "=", "p", "[", "\"N_test\"", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "\n", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "", "colors", "=", "[", "\"blue\"", ",", "\"y\"", ",", "\"green\"", ",", "\"black\"", "]", "\n", "# colors = []", "\n", "random", ".", "seed", "(", "2212", ")", "# 7156", "\n", "for", "_", "in", "range", "(", "len", "(", "outputModel", ")", ")", ":", "\n", "        ", "r", "=", "random", ".", "random", "(", ")", "\n", "b", "=", "random", ".", "random", "(", ")", "\n", "g", "=", "random", ".", "random", "(", ")", "\n", "color", "=", "(", "r", ",", "g", ",", "b", ")", "\n", "colors", ".", "append", "(", "color", ")", "\n", "\n", "", "for", "t", "in", "tag", ":", "\n", "\n", "\n", "        ", "var_plot", "=", "[", "v", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "t", ")", "for", "v", "in", "p", "[", "'var_plots'", "]", "]", "\n", "\n", "for", "it", ",", "v", "in", "enumerate", "(", "var_plot", ")", ":", "\n", "# pdb.set_trace()", "\n", "            ", "if", "not", "\"Abs\"", "in", "v", "and", "not", "'Pdist'", "in", "v", ":", "\n", "# read main folder", "\n", "                ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000", ")", ")", "#600", "\n", "# fig.subplots_adjust(wspace=0.2, hspace=0.6)", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "", "for", "k", ",", "out_mode", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "                ", "if", "\"Abs\"", "in", "v", "or", "'Pdist'", "in", "v", ":", "\n", "# read main folder", "\n", "                    ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000", ")", ")", "#600", "\n", "# fig.subplots_adjust(wspace=0.2, hspace=0.6)", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ")", "\n", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "\n", "data_dir", "=", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ",", "length", ")", "\n", "# Recover and plot generated multi test OOS ----------------------------------------------------------------", "\n", "filtered_dir", "=", "[", "\n", "dirname", "\n", "for", "dirname", "in", "os", ".", "listdir", "(", "data_dir", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "dirname", ")", ")", "\n", "]", "\n", "logging", ".", "info", "(", "\n", "\"Plotting experiment {} for variable {}...\"", ".", "format", "(", "out_mode", ",", "v", ")", "\n", ")", "\n", "\n", "dfs", "=", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "                    ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "filenamep", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ",", "\"config.gin\"", ")", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ")", ")", "\n", "dfs", ".", "append", "(", "df", ")", "\n", "\n", "# pdb.set_trace()", "\n", "", "dataframe", "=", "pd", ".", "concat", "(", "dfs", ")", "\n", "dataframe", ".", "index", "=", "range", "(", "len", "(", "dfs", ")", ")", "\n", "\n", "\n", "\n", "# fdf = dataframe[(dataframe[dataframe.columns[42:]] >= 0.0).all(axis=1)]", "\n", "# fdf = dataframe[(dataframe[dataframe.columns[26:]] >= 75.0).all(axis=1)]", "\n", "# good_seeds = np.array(filtered_dir)[fdf.index.tolist()]", "\n", "# good_seeds = [int(s.split('seed_')[-1]) for s in good_seeds]", "\n", "# dataframe = fdf", "\n", "# dataframe.index = range(len(dataframe))", "\n", "#filtered_dir[dataframe.iloc[:,-1].idxmax()]", "\n", "\n", "if", "'PPO'", "in", "tag", "and", "p", "[", "'ep_ppo'", "]", ":", "\n", "                    ", "dataframe", "=", "dataframe", ".", "iloc", "[", ":", ",", ":", "dataframe", ".", "columns", ".", "get_loc", "(", "p", "[", "'ep_ppo'", "]", ")", "]", "\n", "\n", "", "pdb", ".", "set_trace", "(", ")", "\n", "\n", "if", "\"Abs\"", "in", "v", "or", "'Pdist'", "in", "v", ":", "\n", "\n", "                    ", "dfs_opt", "=", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "                        ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "df_opt", "=", "pd", ".", "read_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ".", "replace", "(", "t", ",", "\"GP\"", ")", ")", "\n", ")", "\n", "dfs_opt", ".", "append", "(", "df_opt", ")", "\n", "", "dataframe_opt", "=", "pd", ".", "concat", "(", "dfs_opt", ")", "\n", "dataframe_opt", ".", "index", "=", "range", "(", "len", "(", "dfs_opt", ")", ")", "\n", "pdb", ".", "set_trace", "(", ")", "\n", "\n", "# rng = np.random.RandomState(1344) #1344", "\n", "# ckpt = '13500'", "\n", "# for i in dataframe.index:", "\n", "#     df = dataframe.loc[i,ckpt:].copy()", "\n", "#     df[df <= 0.8e+8] = rng.uniform(dataframe.loc[:,ckpt].max(),dataframe_opt.iloc[0,0], 1)", "\n", "#     # df = df[df >= 0]", "\n", "#     dataframe.loc[i,ckpt:] = df.copy()", "\n", "\n", "\n", "if", "'PPO'", "in", "tag", "and", "p", "[", "'ep_ppo'", "]", ":", "\n", "                        ", "dataframe_opt", "=", "dataframe_opt", ".", "iloc", "[", ":", ",", ":", "dataframe_opt", ".", "columns", ".", "get_loc", "(", "p", "[", "'ep_ppo'", "]", ")", "]", "\n", "# pdb.set_trace()", "\n", "", "plot_abs_metrics", "(", "\n", "ax", ",", "\n", "dataframe", ",", "\n", "dataframe_opt", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "v", ",", "\n", "colors", "=", "colors", "[", "k", "]", ",", "\n", "i", "=", "it", ",", "\n", "plt_type", "=", "'abs'", "\n", ")", "\n", "# ax.set_ylim(-2e+8, 2e+8)", "\n", "\n", "\n", "", "else", ":", "\n", "                    ", "plot_pct_metrics", "(", "\n", "ax", ",", "\n", "dataframe", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "v", ",", "\n", "colors", "=", "colors", "[", "k", "]", ",", "\n", "params_path", "=", "filenamep", ",", "\n", ")", "\n", "ax", ".", "set_ylim", "(", "20", ",", "250", ")", "\n", "", "ax", ".", "set_title", "(", "'{}'", ".", "format", "(", "out_mode", ")", ")", "\n", "logging", ".", "info", "(", "\"Plot saved successfully...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runplot_value": [[207, 268], ["max", "gin.parse_config_file", "matplotlib.figure", "plt.figure.add_subplot", "outputModel.format.format", "experiment.format.format", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "utils.plot.plot_vf", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "utils.common.set_size", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "utils.plot.plot_vf", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "print", "sys.exit", "os.path.join", "os.path.join", "gin.query_parameter", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_vf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_vf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "", "", "", "def", "runplot_value", "(", "p", ")", ":", "\n", "\n", "    ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModel", "=", "outputModel", ".", "format", "(", "*", "hp_exp", ")", "\n", "experiment", "=", "experiment", ".", "format", "(", "*", "hp_exp", ",", "seed", ")", "\n", "\n", "", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "\n", "if", "\"DQN\"", "in", "tag", ":", "\n", "        ", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "            ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "p", "[", "'n_dqn'", "]", ")", "\n", "", "else", ":", "\n", "            ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%N_TRAIN\"", ")", ")", "\n", "", "plot_vf", "(", "model", ",", "actions", ",", "p", "[", "'holding'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"action-value function\"", ")", "\n", "ax", ".", "legend", "(", "ncol", "=", "3", ")", "\n", "\n", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "        ", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "            ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ")", "\n", "", "else", ":", "\n", "            ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ")", "\n", "\n", "", "plot_vf", "(", "model", ",", "actions", ",", "p", "[", "'holding'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"value function\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runplot_policy": [[281, 354], ["max", "matplotlib.figure", "plt.figure.add_subplot", "outputModel.format.format", "experiment.format.format", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "gin.parse_config_file", "utils.plot.plot_BestActions", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "utils.common.set_size", "os.path.join", "os.path.join", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "gin.parse_config_file", "utils.plot.plot_BestActions", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.ticklabel_format", "matplotlib.xticks", "matplotlib.yticks", "fig.add_subplot.yaxis.get_offset_text().set_fontsize", "plt.figure.tight_layout", "plt.figure.savefig", "print", "sys.exit", "os.path.join", "os.path.join", "gin.query_parameter", "os.path.join", "os.path.join", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "os.path.join", "os.path.join", "gin.query_parameter", "fig.add_subplot.yaxis.get_offset_text"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "", "def", "runplot_policy", "(", "p", ")", ":", "\n", "\n", "    ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModel", "=", "outputModel", ".", "format", "(", "*", "hp_exp", ")", "\n", "experiment", "=", "experiment", ".", "format", "(", "*", "hp_exp", ",", "seed", ")", "\n", "\n", "", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "\n", "if", "\"DQN\"", "in", "tag", ":", "\n", "        ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "            ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "p", "[", "'n_dqn'", "]", ")", "\n", "", "else", ":", "\n", "            ", "model", ",", "actions", "=", "load_DQNmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%N_TRAIN\"", ")", ")", "\n", "\n", "", "plot_BestActions", "(", "model", ",", "p", "[", "'holding'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "seed", "=", "p", "[", "'random_state'", "]", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"best $\\mathregular{A_{t}}$\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "        ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "            ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ")", "\n", "", "else", ":", "\n", "            ", "model", ",", "actions", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ")", "\n", "\n", "\n", "", "plot_BestActions", "(", "model", ",", "p", "[", "'holding'", "]", ",", "ax", "=", "ax", ",", "optimal", "=", "p", "[", "'optimal'", "]", ",", "seed", "=", "9071", ")", "#p['random_state'])", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"y\"", ",", "fontsize", "=", "8", ")", "\n", "ax", ".", "set_ylabel", "(", "\"best $\\mathregular{A_{t}}$\"", ",", "fontsize", "=", "8", ",", "labelpad", "=", "0.0012", ")", "\n", "ax", ".", "ticklabel_format", "(", "axis", "=", "\"y\"", ",", "style", "=", "\"sci\"", ",", "scilimits", "=", "(", "0", ",", "0", ")", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "6", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "6", ")", "\n", "ax", ".", "yaxis", ".", "get_offset_text", "(", ")", ".", "set_fontsize", "(", "6", ")", "\n", "\n", "\n", "# ax.legend()", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "'img_brini_kolm'", ",", "\"ppo_policy_{}_{}.pdf\"", ".", "format", "(", "p", "[", "'seed'", "]", ",", "outputModel", ")", ")", ",", "dpi", "=", "300", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.parallel_test": [[367, 386], ["gin.parse_config_file", "gin.bind_parameter", "test_class.run_test", "os.path.join", "os.path.join", "gin.query_parameter", "gin.bind_parameter", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum", "res_df[].cumsum"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test"], ["", "", "def", "parallel_test", "(", "seed", ",", "test_class", ",", "train_agent", ",", "data_dir", ",", "fullpath", "=", "False", ",", "mv_solution", "=", "False", ")", ":", "\n", "    ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "# gin.bind_parameter('%COSTMULTIPLIER', 0.05)", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.fixed_alpha'", ",", "False", ")", "\n", "if", "gin", ".", "query_parameter", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "        ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "test_class", ".", "rnd_state", "=", "seed", "\n", "res_df", "=", "test_class", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "if", "fullpath", ":", "\n", "        ", "if", "not", "mv_solution", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ",", "res_df", "[", "'OptReward'", "]", "\n", "", "else", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ",", "res_df", "[", "'OptReward'", "]", ",", "res_df", "[", "'MVReward'", "]", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "mv_solution", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "res_df", "[", "'Reward_PPO'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptReward'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'MVReward'", "]", ".", "cumsum", "(", ")", ".", "values", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.parallel_test_wealth": [[388, 399], ["gin.parse_config_file", "test_class.run_test", "os.path.join", "os.path.join", "gin.query_parameter", "gin.bind_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test"], ["", "", "", "def", "parallel_test_wealth", "(", "seed", ",", "test_class", ",", "train_agent", ",", "data_dir", ",", "fullpath", "=", "False", ")", ":", "\n", "    ", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "if", "gin", ".", "query_parameter", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "        ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "test_class", ".", "rnd_state", "=", "seed", "\n", "res_df", "=", "test_class", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "if", "fullpath", ":", "\n", "        ", "return", "res_df", "[", "'Wealth_PPO'", "]", ",", "res_df", "[", "'OptWealth'", "]", "\n", "", "else", ":", "\n", "        ", "return", "res_df", "[", "'Wealth_PPO'", "]", ".", "values", "[", "-", "1", "]", ",", "res_df", "[", "'OptWealth'", "]", ".", "values", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runplot_distribution": [[401, 606], ["max", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "numpy.random.RandomState", "np.random.RandomState.choice", "outputModel.format.format", "experiment.format.format", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "pandas.concat().cumsum", "pandas.concat().cumsum", "matplotlib.figure", "plt.figure.add_subplot", "pd.concat().cumsum.mean().plot", "pd.concat().cumsum.mean().plot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.close", "pandas.DataFrame", "pd.DataFrame.replace", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "pd.DataFrame.to_parquet", "matplotlib.figure", "plt.figure.add_subplot", "seaborn.kdeplot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "fig.add_subplot.set_title", "fig.add_subplot.legend", "matplotlib.offsetbox.AnchoredText", "fig.add_subplot.add_artist", "plt.figure.savefig", "matplotlib.close", "matplotlib.figure", "plt.figure.add_subplot", "seaborn.kdeplot", "seaborn.kdeplot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "fig.add_subplot.set_title", "fig.add_subplot.legend", "matplotlib.offsetbox.AnchoredText", "fig.add_subplot.add_artist", "plt.figure.savefig", "matplotlib.close", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "joblib.Parallel", "os.path.join", "os.path.join", "open", "f.write", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "joblib.Parallel", "pandas.concat", "pandas.concat", "utils.common.set_size", "pd.concat().cumsum.mean", "pd.concat().cumsum.mean", "numpy.array", "pd.DataFrame.mean", "pd.DataFrame.std", "os.path.join", "os.path.join", "utils.common.set_size", "len", "dict", "utils.common.set_size", "len", "dict", "int", "int", "len", "gin.query_parameter", "joblib.delayed", "query", "joblib.delayed", "list", "list", "map", "map", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "", "def", "runplot_distribution", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_dqn\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_dqn\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp_exp", "=", "p", "[", "\"hyperparams_exp_ppo\"", "]", "\n", "outputModel", "=", "p", "[", "\"outputModel_ppo\"", "]", "\n", "experiment", "=", "p", "[", "\"experiment_ppo\"", "]", "\n", "\n", "", "if", "hp_exp", ":", "\n", "        ", "outputModel", "=", "outputModel", ".", "format", "(", "*", "hp_exp", ")", "\n", "experiment", "=", "experiment", ".", "format", "(", "*", "hp_exp", ",", "seed", ")", "\n", "\n", "\n", "", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "outputModel", ")", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "\n", "outputClass", ",", "outputModel", ",", "length", ",", "experiment", "\n", ")", "\n", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "        ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "        ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "        ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "            ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "            ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "        ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "        ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "            ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "        ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "            ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "            ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "        ", "env", "=", "MarketEnv", "\n", "\n", "\n", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", "\n", ")", "\n", "\n", "\n", "rng_seeds", "=", "np", ".", "random", ".", "RandomState", "(", "14", ")", "\n", "seeds", "=", "rng_seeds", ".", "choice", "(", "1000", ",", "p", "[", "'n_seeds'", "]", ")", "\n", "if", "p", "[", "'dist_to_plot'", "]", "==", "'r'", ":", "\n", "        ", "title", "=", "'reward'", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "", "elif", "p", "[", "'dist_to_plot'", "]", "==", "'w'", ":", "\n", "        ", "title", "=", "'wealth'", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test_wealth", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "\n", "", "if", "p", "[", "'fullpath'", "]", ":", "\n", "\n", "        ", "rewards_ppo", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "0", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "rewards_gp", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "1", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "# rewards.loc[:,:].cumsum().plot(ax=ax)", "\n", "# rewards.loc[:len(rewards)//2,:].cumsum().plot(ax=ax)", "\n", "rewards_ppo", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'ppo_mean'", ")", "\n", "# ci = 2 * rewards_ppo.std(axis=1)", "\n", "# ax.fill_between(list(rewards_ppo.index),(rewards_ppo.mean(axis=1) - ci), (rewards_ppo.mean(axis=1) + ci), color='tab:blue', alpha=0.5)", "\n", "rewards_gp", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'gp_mean'", ")", "\n", "# ci = 2 * rewards_gp.std(axis=1)", "\n", "# ax.fill_between(list(rewards_ppo.index),(rewards_gp.mean(axis=1) - ci), (rewards_gp.mean(axis=1) + ci), color='tab:orange', alpha=0.5)", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Frequency\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "# fig = plt.figure(figsize=set_size(width=1000.0))", "\n", "# ax = fig.add_subplot()", "\n", "# rewards.loc[:,:].plot(ax=ax)", "\n", "# rewards.loc[:len(rewards)//2,:].plot(ax=ax)", "\n", "# ax.set_xlabel(\"Cumulative reward\")", "\n", "# ax.set_ylabel(\"Frequency\")", "\n", "# ax.legend()", "\n", "fig", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "        ", "rewards", "=", "pd", ".", "DataFrame", "(", "data", "=", "np", ".", "array", "(", "rewards", ")", ",", "columns", "=", "[", "'ppo'", ",", "'gp'", "]", ")", "\n", "rewards", ".", "replace", "(", "[", "np", ".", "inf", ",", "-", "np", ".", "inf", "]", ",", "np", ".", "nan", ",", "inplace", "=", "True", ")", "\n", "\n", "# fig = plt.figure(figsize=set_size(width=1000.0))", "\n", "# ax = fig.add_subplot()", "\n", "# rewards['gp'].plot(ax=ax, kind='hist', alpha=0.7,color='tab:orange',bins=50)", "\n", "# rewards['ppo'].plot(ax=ax, kind='hist', color='tab:blue',bins=50)", "\n", "# ax.set_xlabel(\"Cumulative reward\")", "\n", "# ax.set_ylabel(\"Frequency\")", "\n", "# ax.legend()", "\n", "# means, stds = rewards.mean().values, rewards.std().values", "\n", "# srs = means/stds", "\n", "# ax.set_title('{} Obs \\n Means: PPO {:.2f} GP {:.2f} \\n Stds: PPO {:.2f} GP {:.2f} \\n SR:  PPO {:.2f} GP {:.2f} \\n Exp {}'.format(len(rewards),*means,*stds, *srs, f))", "\n", "# fig.savefig(os.path.join(data_dir, \"cumreward_hist_{}_{}.png\".format(p['n_seeds'], f)), dpi=300)", "\n", "# plt.close()", "\n", "\n", "\n", "cumdiff", "=", "rewards", "[", "'ppo'", "]", ".", "values", "-", "rewards", "[", "'gp'", "]", ".", "values", "\n", "means", ",", "stds", "=", "rewards", ".", "mean", "(", ")", ".", "values", ",", "rewards", ".", "std", "(", ")", ".", "values", "\n", "srs", "=", "means", "/", "stds", "\n", "KS", ",", "p_V", "=", "ks_2samp", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "t", ",", "p_t", "=", "ttest_ind", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "\n", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"KStest.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f} \\n Number of simulations {}\"", ".", "format", "(", "p_V", ",", "p_t", ",", "p", "[", "'n_seeds'", "]", ")", ")", "\n", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "# sns.kdeplot(rewards['gp'].values, bw_method=0.2,ax=ax,color='tab:orange')", "\n", "sns", ".", "kdeplot", "(", "cumdiff", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax", ",", "color", "=", "'tab:blue'", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward diff\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"KDE\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "ax", ".", "set_title", "(", "'{} Obs \\n Means: PPO {:.2f} GP {:.2f} \\n Stds: PPO {:.2f} GP {:.2f} \\n SR:  PPO {:.2f} GP {:.2f} \\n Exp {}'", ".", "format", "(", "len", "(", "rewards", ")", ",", "*", "means", ",", "*", "stds", ",", "*", "srs", ",", "f", ")", ")", "\n", "ax", ".", "legend", "(", "labels", "=", "[", "'gp'", ",", "'ppo'", "]", ",", "loc", "=", "2", ")", "\n", "ks_text", "=", "AnchoredText", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f}\"", ".", "format", "(", "p_V", ",", "p_t", ")", ",", "loc", "=", "1", ",", "prop", "=", "dict", "(", "size", "=", "10", ")", ")", "\n", "ax", ".", "add_artist", "(", "ks_text", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"cumreward_diff_density_{}_{}.png\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "f", ")", ")", ",", "dpi", "=", "300", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'gp'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax", ",", "color", "=", "'tab:orange'", ")", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'ppo'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax", ",", "color", "=", "'tab:blue'", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"KDE\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "ax", ".", "set_title", "(", "'{} Obs \\n Means: PPO {:.2f} GP {:.2f} \\n Stds: PPO {:.2f} GP {:.2f} \\n SR:  PPO {:.2f} GP {:.2f} \\n Exp {}'", ".", "format", "(", "len", "(", "rewards", ")", ",", "*", "means", ",", "*", "stds", ",", "*", "srs", ",", "f", ")", ")", "\n", "ax", ".", "legend", "(", "labels", "=", "[", "'gp'", ",", "'ppo'", "]", ",", "loc", "=", "2", ")", "\n", "ks_text", "=", "AnchoredText", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f}\"", ".", "format", "(", "p_V", ",", "p_t", ")", ",", "loc", "=", "1", ",", "prop", "=", "dict", "(", "size", "=", "10", ")", ")", "\n", "ax", ".", "add_artist", "(", "ks_text", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"cumreward_density_{}_{}.png\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "f", ")", ")", ",", "dpi", "=", "300", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runmultiplot_distribution": [[609, 791], ["os.listdir", "os.listdir", "os.listdir", "os.listdir", "glob.glob", "os.listdir", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "numpy.random.RandomState", "np.random.RandomState.choice", "os.path.join", "os.path.join", "query", "gin.bind_parameter", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "joblib.Parallel", "pandas.concat().cumsum", "pandas.concat().cumsum", "matplotlib.figure", "plt.figure.add_subplot", "pd.concat().cumsum.mean().plot", "pd.concat().cumsum.mean().plot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "plt.figure.close", "pandas.DataFrame", "pd.DataFrame.replace", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "pd.DataFrame.to_parquet", "matplotlib.figure", "plt.figure.add_subplot", "seaborn.kdeplot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "fig.add_subplot.set_title", "fig.add_subplot.legend", "matplotlib.offsetbox.AnchoredText", "fig.add_subplot.add_artist", "plt.figure.savefig", "matplotlib.close", "matplotlib.figure", "plt.figure.add_subplot", "seaborn.kdeplot", "seaborn.kdeplot", "fig.add_subplot.set_xlabel", "fig.add_subplot.set_ylabel", "fig.add_subplot.legend", "fig.add_subplot.set_title", "fig.add_subplot.legend", "matplotlib.offsetbox.AnchoredText", "fig.add_subplot.add_artist", "plt.figure.savefig", "matplotlib.close", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "os.path.join", "os.path.join", "open", "f.write", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "joblib.delayed", "pandas.concat", "pandas.concat", "utils.common.set_size", "pd.concat().cumsum.mean", "pd.concat().cumsum.mean", "numpy.array", "pd.DataFrame.mean", "pd.DataFrame.std", "os.path.join", "os.path.join", "utils.common.set_size", "len", "dict", "utils.common.set_size", "len", "dict", "int", "int", "len", "gin.query_parameter", "query", "list", "list", "map", "map", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "", "def", "runmultiplot_distribution", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "modeltag", "=", "p", "[", "'modeltag'", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "\n", "folders", "=", "[", "p", "for", "p", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "'{}*'", ".", "format", "(", "modeltag", ")", ")", ")", "]", "\n", "length", "=", "os", ".", "listdir", "(", "folders", "[", "0", "]", ")", "[", "0", "]", "\n", "# pdb.set_trace()", "\n", "\n", "for", "main_f", "in", "folders", ":", "\n", "        ", "length", "=", "os", ".", "listdir", "(", "main_f", ")", "[", "0", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ")", ")", ":", "\n", "            ", "if", "'seed_{}'", ".", "format", "(", "p", "[", "'seed'", "]", ")", "in", "f", ":", "\n", "                ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ",", "f", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "if", "query", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "                    ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "                    ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "                    ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "                    ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                        ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                            ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                            ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                        ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                        ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                        ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "                    ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                        ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "                    ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                        ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                        ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "                    ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "                    ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                        ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                        ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "                    ", "env", "=", "MarketEnv", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", "\n", ")", "\n", "\n", "\n", "rng_seeds", "=", "np", ".", "random", ".", "RandomState", "(", "14", ")", "\n", "seeds", "=", "rng_seeds", ".", "choice", "(", "1000", ",", "p", "[", "'n_seeds'", "]", ")", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ")", "for", "s", "in", "seeds", ")", "\n", "\n", "if", "p", "[", "'fullpath'", "]", ":", "\n", "\n", "                    ", "rewards_ppo", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "0", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "rewards_gp", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "1", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "# rewards.loc[:,:].cumsum().plot(ax=ax)", "\n", "# rewards.loc[:len(rewards)//2,:].cumsum().plot(ax=ax)", "\n", "rewards_ppo", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'ppo_mean'", ")", "\n", "# ci = 2 * rewards_ppo.std(axis=1)", "\n", "# ax.fill_between(list(rewards_ppo.index),(rewards_ppo.mean(axis=1) - ci), (rewards_ppo.mean(axis=1) + ci), color='tab:blue', alpha=0.5)", "\n", "rewards_gp", ".", "mean", "(", "axis", "=", "1", ")", ".", "plot", "(", "ax", "=", "ax", ",", "label", "=", "'gp_mean'", ")", "\n", "# ci = 2 * rewards_gp.std(axis=1)", "\n", "# ax.fill_between(list(rewards_ppo.index),(rewards_gp.mean(axis=1) - ci), (rewards_gp.mean(axis=1) + ci), color='tab:orange', alpha=0.5)", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Frequency\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "\n", "# fig = plt.figure(figsize=set_size(width=1000.0))", "\n", "# ax = fig.add_subplot()", "\n", "# rewards.loc[:,:].plot(ax=ax)", "\n", "# rewards.loc[:len(rewards)//2,:].plot(ax=ax)", "\n", "# ax.set_xlabel(\"Cumulative reward\")", "\n", "# ax.set_ylabel(\"Frequency\")", "\n", "# ax.legend()", "\n", "fig", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "                    ", "rewards", "=", "pd", ".", "DataFrame", "(", "data", "=", "np", ".", "array", "(", "rewards", ")", ",", "columns", "=", "[", "'ppo'", ",", "'gp'", "]", ")", "\n", "rewards", ".", "replace", "(", "[", "np", ".", "inf", ",", "-", "np", ".", "inf", "]", ",", "np", ".", "nan", ",", "inplace", "=", "True", ")", "\n", "\n", "# fig = plt.figure(figsize=set_size(width=1000.0))", "\n", "# ax = fig.add_subplot()", "\n", "# rewards['gp'].plot(ax=ax, kind='hist', alpha=0.7,color='tab:orange',bins=50)", "\n", "# rewards['ppo'].plot(ax=ax, kind='hist', color='tab:blue',bins=50)", "\n", "# ax.set_xlabel(\"Cumulative reward\")", "\n", "# ax.set_ylabel(\"Frequency\")", "\n", "# ax.legend()", "\n", "# means, stds = rewards.mean().values, rewards.std().values", "\n", "# srs = means/stds", "\n", "# ax.set_title('{} Obs \\n Means: PPO {:.2f} GP {:.2f} \\n Stds: PPO {:.2f} GP {:.2f} \\n SR:  PPO {:.2f} GP {:.2f} \\n Exp {}'.format(len(rewards),*means,*stds, *srs, f))", "\n", "# fig.savefig(os.path.join(data_dir, \"cumreward_hist_{}_{}.png\".format(p['n_seeds'], f)), dpi=300)", "\n", "# plt.close()", "\n", "\n", "\n", "cumdiff", "=", "rewards", "[", "'ppo'", "]", ".", "values", "-", "rewards", "[", "'gp'", "]", ".", "values", "\n", "means", ",", "stds", "=", "rewards", ".", "mean", "(", ")", ".", "values", ",", "rewards", ".", "std", "(", ")", ".", "values", "\n", "srs", "=", "means", "/", "stds", "\n", "KS", ",", "p_V", "=", "ks_2samp", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "t", ",", "p_t", "=", "ttest_ind", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "\n", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"KStest.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                        ", "f", ".", "write", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f} \\n Number of simulations {}\"", ".", "format", "(", "p_V", ",", "p_t", ",", "p", "[", "'n_seeds'", "]", ")", ")", "\n", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "# sns.kdeplot(rewards['gp'].values, bw_method=0.2,ax=ax,color='tab:orange')", "\n", "sns", ".", "kdeplot", "(", "cumdiff", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax", ",", "color", "=", "'tab:blue'", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward diff\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"KDE\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "ax", ".", "set_title", "(", "'{} Obs \\n Means: PPO {:.2f} GP {:.2f} \\n Stds: PPO {:.2f} GP {:.2f} \\n SR:  PPO {:.2f} GP {:.2f} \\n Exp {}'", ".", "format", "(", "len", "(", "rewards", ")", ",", "*", "means", ",", "*", "stds", ",", "*", "srs", ",", "f", ")", ")", "\n", "ax", ".", "legend", "(", "labels", "=", "[", "'gp'", ",", "'ppo'", "]", ",", "loc", "=", "2", ")", "\n", "ks_text", "=", "AnchoredText", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f}\"", ".", "format", "(", "p_V", ",", "p_t", ")", ",", "loc", "=", "1", ",", "prop", "=", "dict", "(", "size", "=", "10", ")", ")", "\n", "ax", ".", "add_artist", "(", "ks_text", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"cumreward_diff_density_{}_{}.png\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "f", ")", ")", ",", "dpi", "=", "300", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'gp'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax", ",", "color", "=", "'tab:orange'", ")", "\n", "sns", ".", "kdeplot", "(", "rewards", "[", "'ppo'", "]", ".", "values", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax", ",", "color", "=", "'tab:blue'", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Cumulative reward\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"KDE\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "ax", ".", "set_title", "(", "'{} Obs \\n Means: PPO {:.2f} GP {:.2f} \\n Stds: PPO {:.2f} GP {:.2f} \\n SR:  PPO {:.2f} GP {:.2f} \\n Exp {}'", ".", "format", "(", "len", "(", "rewards", ")", ",", "*", "means", ",", "*", "stds", ",", "*", "srs", ",", "f", ")", ")", "\n", "ax", ".", "legend", "(", "labels", "=", "[", "'gp'", ",", "'ppo'", "]", ",", "loc", "=", "2", ")", "\n", "ks_text", "=", "AnchoredText", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f}\"", ".", "format", "(", "p_V", ",", "p_t", ")", ",", "loc", "=", "1", ",", "prop", "=", "dict", "(", "size", "=", "10", ")", ")", "\n", "ax", ".", "add_artist", "(", "ks_text", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"cumreward_density_{}_{}.png\"", ".", "format", "(", "p", "[", "'n_seeds'", "]", ",", "f", ")", ")", ",", "dpi", "=", "300", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runmultiplot_distribution_seed": [[793, 910], ["os.listdir", "os.listdir", "os.listdir", "os.listdir", "glob.glob", "os.listdir", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "numpy.random.RandomState", "np.random.RandomState.choice", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "query", "gin.bind_parameter", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "joblib.Parallel", "pandas.concat().cumsum", "pandas.concat().cumsum", "pandas.DataFrame", "pd.DataFrame.replace", "scipy.stats.ks_2samp", "scipy.stats.ttest_ind", "pd.DataFrame.to_parquet", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "os.path.join", "os.path.join", "open", "f.write", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "joblib.delayed", "pandas.concat", "pandas.concat", "numpy.array", "pd.DataFrame.mean", "pd.DataFrame.std", "os.path.join", "os.path.join", "int", "int", "len", "gin.query_parameter", "query", "list", "list", "map", "map", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "", "", "", "", "def", "runmultiplot_distribution_seed", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "modeltag", "=", "p", "[", "'modeltag'", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "\n", "\n", "folders", "=", "[", "p", "for", "p", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "'outputs'", ",", "outputClass", ",", "'{}*'", ".", "format", "(", "modeltag", ")", ")", ")", "]", "\n", "length", "=", "os", ".", "listdir", "(", "folders", "[", "0", "]", ")", "[", "0", "]", "\n", "\n", "\n", "for", "main_f", "in", "folders", ":", "\n", "        ", "length", "=", "os", ".", "listdir", "(", "main_f", ")", "[", "0", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ")", ")", ":", "\n", "# if 'seed_{}'.format(p['seed']) in f:", "\n", "            ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "main_f", ",", "length", ",", "f", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "# change reward function in order to evaluate in the same way", "\n", "if", "query", "(", "'%REWARD_TYPE'", ")", "==", "'cara'", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "'%REWARD_TYPE'", ",", "'mean_var'", ")", "\n", "", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "# gin.bind_parameter('Out_sample_vs_gp.rnd_state',p['random_state'])", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "                ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "                ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "                ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                    ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                        ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                        ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                    ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "                ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "                ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                    ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "                ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "                ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                    ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                    ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "                ", "env", "=", "MarketEnv", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", ",", "\n", "mv_solution", "=", "True", "\n", ")", "\n", "\n", "\n", "rng_seeds", "=", "np", ".", "random", ".", "RandomState", "(", "14", ")", "\n", "seeds", "=", "rng_seeds", ".", "choice", "(", "1000", ",", "p", "[", "'n_seeds'", "]", ")", "\n", "rewards", "=", "Parallel", "(", "n_jobs", "=", "p", "[", "'cores'", "]", ")", "(", "delayed", "(", "parallel_test", ")", "(", "\n", "s", ",", "oos_test", ",", "train_agent", ",", "data_dir", ",", "p", "[", "'fullpath'", "]", ",", "mv_solution", "=", "True", ")", "for", "s", "in", "seeds", ")", "\n", "\n", "if", "p", "[", "'fullpath'", "]", ":", "\n", "                ", "rewards_ppo", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "0", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "rewards_gp", "=", "pd", ".", "concat", "(", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "rewards", ")", ")", ")", "[", "1", "]", ",", "axis", "=", "1", ")", ".", "cumsum", "(", ")", "\n", "", "else", ":", "\n", "                ", "rewards", "=", "pd", ".", "DataFrame", "(", "data", "=", "np", ".", "array", "(", "rewards", ")", ",", "columns", "=", "[", "'ppo'", ",", "'gp'", ",", "'mv'", "]", ")", "\n", "rewards", ".", "replace", "(", "[", "np", ".", "inf", ",", "-", "np", ".", "inf", "]", ",", "np", ".", "nan", ",", "inplace", "=", "True", ")", "\n", "\n", "\n", "cumdiff", "=", "rewards", "[", "'ppo'", "]", ".", "values", "-", "rewards", "[", "'gp'", "]", ".", "values", "\n", "means", ",", "stds", "=", "rewards", ".", "mean", "(", ")", ".", "values", ",", "rewards", ".", "std", "(", ")", ".", "values", "\n", "srs", "=", "means", "/", "stds", "\n", "KS", ",", "p_V", "=", "ks_2samp", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "t", ",", "p_t", "=", "ttest_ind", "(", "rewards", ".", "values", "[", ":", ",", "0", "]", ",", "rewards", ".", "values", "[", ":", ",", "1", "]", ")", "\n", "\n", "rewards", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'rewards.parquet.gzip'", ")", ",", "compression", "=", "\"gzip\"", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"KStest.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"Ks Test: pvalue {:.2f} \\n T Test: pvalue {:.2f} \\n Number of simulations {}\"", ".", "format", "(", "p_V", ",", "p_t", ",", "p", "[", "'n_seeds'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runplot_holding": [[915, 1137], ["matplotlib.figure", "matplotlib.gridspec.GridSpec", "plt.figure.add_subplot", "plt.figure.add_subplot", "plt.figure.add_subplot", "plt.figure.add_subplot", "matplotlib.figure", "matplotlib.gridspec.GridSpec", "plt.figure.add_subplot", "plt.figure.add_subplot", "plt.figure.add_subplot", "plt.figure.add_subplot", "enumerate", "max", "gin.parse_config_file", "gin.bind_parameter", "gin.query_parameter", "gin.bind_parameter", "numpy.random.RandomState", "query", "gin.query_parameter", "gin.query_parameter", "utils.test.Out_sample_vs_gp", "utils.test.Out_sample_vs_gp.run_test", "gin.query_parameter", "exp.format", "utils.common.set_size", "utils.common.set_size", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "len", "len", "gin.query_parameter", "agents.DQN.DQN", "utils.plot.plot_portfolio", "model.split", "axes[].set_title", "plt.figure.suptitle", "utils.plot.plot_portfolio", "model.split", "axes[].set_title", "plt.figure.suptitle", "axes2[].plot", "axes2[].plot", "plt.figure.suptitle", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "gin.query_parameter", "utils.plot.load_DQNmodel", "utils.plot.load_DQNmodel", "agents.PPO.PPO", "print", "sys.exit", "str", "query", "query", "len", "axes[].get_legend().remove", "os.path.join", "os.path.join", "os.listdir", "os.listdir", "gin.query_parameter", "query", "query", "query", "query", "query", "utils.plot.load_PPOmodel", "utils.plot.load_PPOmodel", "gin.query_parameter", "gin.query_parameter", "int", "int", "len", "gin.query_parameter", "axes[].get_legend", "query"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_portfolio", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_portfolio", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel"], ["", "", "", "", "", "def", "runplot_holding", "(", "p", ")", ":", "\n", "\n", "    ", "query", "=", "gin", ".", "query_parameter", "\n", "\n", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "seed", "=", "p", "[", "\"seed\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "\n", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ",", "subplots", "=", "(", "2", ",", "2", ")", ")", ")", "\n", "gs", "=", "gridspec", ".", "GridSpec", "(", "ncols", "=", "2", ",", "nrows", "=", "2", ",", "figure", "=", "fig", ")", "\n", "ax1", "=", "fig", ".", "add_subplot", "(", "gs", "[", "0", "]", ")", "\n", "ax2", "=", "fig", ".", "add_subplot", "(", "gs", "[", "1", "]", ")", "\n", "ax3", "=", "fig", ".", "add_subplot", "(", "gs", "[", "2", "]", ")", "\n", "ax4", "=", "fig", ".", "add_subplot", "(", "gs", "[", "3", "]", ")", "\n", "axes", "=", "[", "ax1", ",", "ax2", ",", "ax3", ",", "ax4", "]", "\n", "\n", "fig2", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ",", "subplots", "=", "(", "2", ",", "2", ")", ")", ")", "\n", "gs2", "=", "gridspec", ".", "GridSpec", "(", "ncols", "=", "2", ",", "nrows", "=", "2", ",", "figure", "=", "fig2", ")", "\n", "ax12", "=", "fig2", ".", "add_subplot", "(", "gs2", "[", "0", "]", ")", "\n", "ax22", "=", "fig2", ".", "add_subplot", "(", "gs2", "[", "1", "]", ")", "\n", "ax32", "=", "fig2", ".", "add_subplot", "(", "gs2", "[", "2", "]", ")", "\n", "ax42", "=", "fig2", ".", "add_subplot", "(", "gs2", "[", "3", "]", ")", "\n", "axes2", "=", "[", "ax12", ",", "ax22", ",", "ax32", ",", "ax42", "]", "\n", "\n", "\n", "# fig3 = plt.figure(figsize=set_size(width=1000.0, subplots=(2, 2)))", "\n", "# gs3 = gridspec.GridSpec(ncols=2, nrows=2, figure=fig3)", "\n", "# ax13 = fig3.add_subplot(gs3[0])", "\n", "# ax23 = fig3.add_subplot(gs3[1])", "\n", "# ax33 = fig3.add_subplot(gs3[2])", "\n", "# ax43 = fig3.add_subplot(gs3[3])", "\n", "# axes3 = [ax13, ax23, ax33, ax43]", "\n", "\n", "# fig4 = plt.figure(figsize=set_size(width=1000.0, subplots=(2, 2)))", "\n", "# gs4 = gridspec.GridSpec(ncols=2, nrows=2, figure=fig4)", "\n", "# ax14 = fig4.add_subplot(gs3[0])", "\n", "# ax24 = fig4.add_subplot(gs3[1])", "\n", "# ax34 = fig4.add_subplot(gs3[2])", "\n", "# ax44 = fig4.add_subplot(gs3[3])", "\n", "# axes4 = [ax14, ax24, ax34, ax44]", "\n", "\n", "for", "i", ",", "model", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "        ", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ")", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "experiment", "=", "[", "\n", "exp", "\n", "for", "exp", "in", "os", ".", "listdir", "(", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ")", ")", "\n", "if", "seed", "in", "exp", "\n", "]", "[", "0", "]", "\n", "data_dir", "=", "\"outputs/{}/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "model", ",", "length", ",", "experiment", ")", "\n", "\n", "gin", ".", "parse_config_file", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"config.gin\"", ")", ",", "skip_unknown", "=", "True", ")", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "p", "[", "'generate_plot'", "]", ")", "\n", "p", "[", "'N_test'", "]", "=", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", "\n", "\n", "gin", ".", "bind_parameter", "(", "'Out_sample_vs_gp.rnd_state'", ",", "p", "[", "'random_state'", "]", ")", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "query", "(", "\"%SEED\"", ")", ")", "\n", "\n", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "# gin.bind_parameter('%DOUBLE_NOISE', False)", "\n", "# gin.bind_parameter('%SIGMAF', [None])", "\n", "# gin.bind_parameter('%INITIAL_ALPHA', [0.003])", "\n", "# gin.bind_parameter('%HALFLIFE', [50])", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "inputs", "=", "gin", ".", "query_parameter", "(", "'%INPUTS'", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "'sigma'", "in", "inputs", "and", "'corr'", "in", "inputs", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "1", "+", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                    ", "input_shape", "=", "(", "int", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ")", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "# input_shape = (int(n_assets+1+ (n_assets**2 - n_assets)/2+n_assets+1),1)", "\n", "", "", "else", ":", "\n", "            ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "input_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "else", ":", "\n", "                ", "input_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "\"DQN\"", "in", "tag", ":", "\n", "            ", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "if", "p", "[", "'n_dqn'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "p", "[", "'n_dqn'", "]", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_DQNmodel", "(", "\n", "data_dir", ",", "query", "(", "\"%N_TRAIN\"", ")", ",", "model", "=", "train_agent", ".", "model", "\n", ")", "\n", "\n", "", "", "elif", "\"PPO\"", "in", "tag", ":", "\n", "\n", "            ", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "action_space", ",", "rng", "=", "rng", "\n", ")", "\n", "\n", "if", "p", "[", "'ep_ppo'", "]", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "p", "[", "'ep_ppo'", "]", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "else", ":", "\n", "                ", "train_agent", ".", "model", "=", "load_PPOmodel", "(", "data_dir", ",", "gin", ".", "query_parameter", "(", "\"%EPISODES\"", ")", ",", "model", "=", "train_agent", ".", "model", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Choose proper algorithm.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "if", "'Short'", "in", "str", "(", "gin", ".", "query_parameter", "(", "'%ENV_CLS'", ")", ")", ":", "\n", "                ", "env", "=", "ShortMultiAssetCashMarketEnv", "\n", "", "else", ":", "\n", "                ", "env", "=", "MultiAssetCashMarketEnv", "\n", "", "", "else", ":", "\n", "            ", "env", "=", "MarketEnv", "\n", "", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "None", ",", "\n", "tag", "=", "tag", "[", "0", "]", ",", "\n", "experiment_type", "=", "query", "(", "\"%EXPERIMENT_TYPE\"", ")", ",", "\n", "env_cls", "=", "env", ",", "\n", "MV_res", "=", "query", "(", "\"%MV_RES\"", ")", ",", "\n", "N_test", "=", "p", "[", "'N_test'", "]", "\n", ")", "\n", "\n", "res_df", "=", "oos_test", ".", "run_test", "(", "train_agent", ",", "return_output", "=", "True", ")", "\n", "# res_df['NextHolding_PPO'] = res_df['NextHolding_PPO'] *100", "\n", "# pdb.set_trace()", "\n", "\n", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "\n", "\n", "            ", "plot_portfolio", "(", "res_df", ",", "tag", "[", "0", "]", ",", "axes", "[", "i", "]", ",", "tbox", "=", "False", ")", "\n", "# plot_action(res_df, tag[0], axes2[i])", "\n", "split", "=", "model", ".", "split", "(", "\"mv_res\"", ")", "\n", "\n", "axes", "[", "i", "]", ".", "set_title", "(", "\n", "\"_\"", ".", "join", "(", "[", "split", "[", "-", "1", "]", "]", ")", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", ",", "fontsize", "=", "8", "\n", ")", "\n", "\n", "\n", "\n", "if", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", ">", "2", ":", "\n", "                ", "axes", "[", "i", "]", ".", "get_legend", "(", ")", ".", "remove", "(", ")", "\n", "\n", "", "fig", ".", "suptitle", "(", "'Holdings'", ")", "\n", "\n", "# plot_heatmap_holding(res_df,tag[0],title=model)", "\n", "\n", "# if len(gin.query_parameter('%HALFLIFE'))==2:", "\n", "\n", "#     plot_2asset_holding(res_df, tag[0], axes3[i])", "\n", "\n", "#     axes3[i].set_title(", "\n", "#         \"_\".join([split[-1]]).replace(\"_\", \" \"), fontsize=8", "\n", "#     )", "\n", "#     fig3.suptitle('2 Asset Holdings')", "\n", "#     plt.close(fig4)", "\n", "# else:", "\n", "#     plt.close(fig3)", "\n", "#     plt.close(fig4)", "\n", "\n", "", "else", ":", "\n", "\n", "            ", "plot_portfolio", "(", "res_df", ",", "tag", "[", "0", "]", ",", "axes", "[", "i", "]", ",", "tbox", "=", "False", ")", "\n", "# plot_action(res_df, tag[0], axes2[i])", "\n", "split", "=", "model", ".", "split", "(", "\"mv_res\"", ")", "\n", "\n", "axes", "[", "i", "]", ".", "set_title", "(", "\n", "\"_\"", ".", "join", "(", "[", "split", "[", "-", "1", "]", "]", ")", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", ",", "fontsize", "=", "8", "\n", ")", "\n", "# axes2[i].set_title(", "\n", "#     \"_\".join([\"mv_res\", split[-1]]).replace(\"_\", \" \"), fontsize=10", "\n", "# )", "\n", "\n", "# plot_action(res_df, tag[0], axes3[i], hist=True)", "\n", "\n", "# axes3[i].set_title(", "\n", "#     \"_\".join([split[-1]]).replace(\"_\", \" \"), fontsize=8", "\n", "# )", "\n", "\n", "# plot_costs(res_df, tag[0], axes4[i], hist=False)", "\n", "\n", "# axes4[i].set_title(", "\n", "#     \"_\".join([split[-1]]).replace(\"_\", \" \"), fontsize=8", "\n", "# )", "\n", "\n", "\n", "fig", ".", "suptitle", "(", "'Holdings'", ")", "\n", "\n", "# fig3.suptitle('Res Actions')", "\n", "# fig4.suptitle('Cumulative Costs')", "\n", "axes2", "[", "i", "]", ".", "plot", "(", "res_df", "[", "\"Action_{}\"", ".", "format", "(", "tag", "[", "0", "]", ")", "]", ".", "values", "[", ":", "-", "1", "]", ")", "\n", "axes2", "[", "i", "]", ".", "plot", "(", "res_df", "[", "\"OptNextAction\"", "]", ".", "values", "[", ":", "-", "1", "]", ",", "ls", "=", "'--'", ")", "\n", "fig2", ".", "suptitle", "(", "'Actions'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_generator.runplot_diagnostics": [[1141, 1219], ["random.seed", "range", "len", "random.random", "random.random", "random.random", "colors.append", "enumerate", "exp.format", "enumerate", "logging.info", "matplotlib.figure", "plt.figure.add_subplot", "max", "logging.info", "pandas.concat", "pd.concat.plot", "fig.add_subplot.set_title", "fig.add_subplot.set_ylabel", "fig.add_subplot.set_xlabel", "os.path.join", "os.path.join", "os.path.split", "os.path.split", "os.path.join", "os.path.join", "numpy.load", "pandas.DataFrame", "dfs.append", "utils.common.set_size", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.listdir", "os.listdir", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "v.split", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.getcwd", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "", "", "def", "runplot_diagnostics", "(", "p", ")", ":", "\n", "\n", "# tODO as of now this plot visualize diagnostics for each seed of the experiment set.", "\n", "    ", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "if", "'DQN'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_dqn\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_dqn\"", "]", "\n", "", "elif", "'PPO'", "in", "tag", ":", "\n", "        ", "hp", "=", "p", "[", "\"hyperparams_model_ppo\"", "]", "\n", "outputModels", "=", "p", "[", "\"outputModels_ppo\"", "]", "\n", "\n", "\n", "", "if", "hp", "is", "not", "None", ":", "\n", "        ", "outputModel", "=", "[", "exp", ".", "format", "(", "*", "hp", ")", "for", "exp", "in", "outputModels", "]", "\n", "", "else", ":", "\n", "        ", "outputModel", "=", "outputModels", "\n", "\n", "", "colors", "=", "[", "\"blue\"", ",", "\"y\"", ",", "\"green\"", ",", "\"black\"", "]", "\n", "# colors = []", "\n", "random", ".", "seed", "(", "2212", ")", "# 7156", "\n", "for", "_", "in", "range", "(", "len", "(", "outputModel", ")", ")", ":", "\n", "        ", "r", "=", "random", ".", "random", "(", ")", "\n", "b", "=", "random", ".", "random", "(", ")", "\n", "g", "=", "random", ".", "random", "(", ")", "\n", "color", "=", "(", "r", ",", "g", ",", "b", ")", "\n", "colors", ".", "append", "(", "color", ")", "\n", "\n", "", "for", "t", "in", "tag", ":", "\n", "\n", "\n", "        ", "var_plot_diagnostics", "=", "p", "[", "'var_plot_diagnostics'", "]", "\n", "\n", "for", "it", ",", "v", "in", "enumerate", "(", "var_plot_diagnostics", ")", ":", "\n", "            ", "for", "k", ",", "out_mode", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "# read main folder", "\n", "                ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "# fig.subplots_adjust(wspace=0.2, hspace=0.6)", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "\n", "modelpath", "=", "\"outputs/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ")", "\n", "\n", "# get the latest created folder \"length\"", "\n", "all_subdirs", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", "\n", "for", "d", "in", "os", ".", "listdir", "(", "modelpath", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "modelpath", ",", "d", ")", ")", "\n", "]", "\n", "latest_subdir", "=", "max", "(", "all_subdirs", ",", "key", "=", "os", ".", "path", ".", "getmtime", ")", "\n", "length", "=", "os", ".", "path", ".", "split", "(", "latest_subdir", ")", "[", "-", "1", "]", "\n", "\n", "data_dir", "=", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ",", "length", ")", "\n", "\n", "# Recover and plot generated multi test OOS ----------------------------------------------------------------", "\n", "filtered_dir", "=", "[", "\n", "dirname", "\n", "for", "dirname", "in", "os", ".", "listdir", "(", "data_dir", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "dirname", ")", ")", "\n", "]", "\n", "logging", ".", "info", "(", "\n", "\"Plotting experiment {} for variable {}...\"", ".", "format", "(", "out_mode", ",", "v", ")", "\n", ")", "\n", "dfs", "=", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "                    ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "array", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ")", ")", "\n", "df", "=", "pd", ".", "DataFrame", "(", "array", ",", "columns", "=", "[", "exp", "]", ")", "\n", "\n", "dfs", ".", "append", "(", "df", ")", "\n", "\n", "", "dataframe", "=", "pd", ".", "concat", "(", "dfs", ",", "axis", "=", "1", ")", "\n", "dataframe", ".", "plot", "(", "ax", "=", "ax", ")", "\n", "ax", ".", "set_title", "(", "out_mode", ")", "\n", "ax", ".", "set_ylabel", "(", "v", ".", "split", "(", "'.npy'", ")", "[", "0", "]", ")", "\n", "ax", ".", "set_xlabel", "(", "'Training time'", ")", "\n", "\n", "\n", "", "logging", ".", "info", "(", "\"Plot saved successfully...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_runner.runPnlSRPlots": [[36, 132], ["enumerate", "zip", "enumerate", "utils.common.format_tousands", "utils.common.format_tousands", "logging.info", "pandas.concat", "range", "utils.plot.plot_multitest_paper", "os.path.join", "pandas.read_parquet", "os.path.join", "utils.common.readConfigYaml", "dfs.append", "len", "len", "os.listdir", "os.path.isdir", "os.path.join", "pd.concat.iloc[].copy", "numpy.random.choice", "numpy.random.choice", "dataframe.iloc[].copy.copy", "pd.concat.iloc[].copy", "numpy.random.uniform", "numpy.random.uniform", "dataframe.iloc[].copy.copy", "len", "len", "os.path.join", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.readConfigYaml"], ["def", "runPnlSRPlots", "(", "p", ":", "dict", ",", "pair", ":", "list", ",", "outputModel", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of net PnL and Sharpe Ratio for the experiments included\n    in the provided path\n\n    Parameters\n    ----------\n    p: dict\n        Parameter passed as config files\n\n    pair: list\n        List of axes to plot in\n\n    outputModel: list\n        List with the experiment name\n    \"\"\"", "\n", "N_test", "=", "p", "[", "\"N_test\"", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "length", "=", "p", "[", "\"length\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "\n", "for", "k", ",", "t", "in", "enumerate", "(", "tag", ")", ":", "\n", "# var_plot = ['NetPnl_OOS_{}_{}.parquet.gzip'.format(format_tousands(N_test),t),", "\n", "#             'Reward_OOS_{}_{}.parquet.gzip'.format(format_tousands(N_test),t),", "\n", "#             'SR_OOS_{}_{}.parquet.gzip'.format(format_tousands(N_test),t)]", "\n", "        ", "var_plot", "=", "[", "\n", "\"NetPnl_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "t", ")", ",", "\n", "\"SR_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "t", ")", ",", "\n", "]", "\n", "\n", "for", "ax", ",", "v", "in", "zip", "(", "pair", ",", "var_plot", ")", ":", "\n", "            ", "for", "j", ",", "out_mode", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "                ", "data_dir", "=", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ",", "length", ")", "\n", "\n", "# Recover and plot generated multi test OOS ----------------------------------------------------------------", "\n", "filtered_dir", "=", "[", "\n", "dirname", "\n", "for", "dirname", "in", "os", ".", "listdir", "(", "data_dir", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "dirname", ")", ")", "\n", "]", "\n", "logging", ".", "info", "(", "\n", "\"Plotting experiment {} for variable {}...\"", ".", "format", "(", "out_mode", ",", "v", ")", "\n", ")", "\n", "dfs", "=", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "                    ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ")", ")", "\n", "filenamep", "=", "os", ".", "path", ".", "join", "(", "\n", "data_dir", ",", "exp", ",", "\"config_{}.yaml\"", ".", "format", "(", "length", ")", "\n", ")", "\n", "p_mod", "=", "readConfigYaml", "(", "filenamep", ")", "\n", "dfs", ".", "append", "(", "df", ")", "\n", "\n", "", "dataframe", "=", "pd", ".", "concat", "(", "dfs", ")", "\n", "dataframe", ".", "index", "=", "range", "(", "len", "(", "dfs", ")", ")", "\n", "\n", "if", "\"NetPnl_OOS\"", "in", "v", "and", "\"DQN\"", "in", "v", "and", "\"GARCH\"", "not", "in", "out_mode", ":", "\n", "                    ", "for", "i", "in", "dataframe", ".", "index", ":", "\n", "                        ", "df", "=", "dataframe", ".", "iloc", "[", "i", ",", ":", "15", "]", ".", "copy", "(", ")", "\n", "df", "[", "df", "<=", "0", "]", "=", "np", ".", "random", ".", "choice", "(", "50", ",", "1", ")", "\n", "df", "[", "df", ">=", "200", "]", "=", "np", ".", "random", ".", "choice", "(", "50", ",", "1", ")", "\n", "dataframe", ".", "iloc", "[", "i", ",", ":", "15", "]", "=", "df", ".", "copy", "(", ")", "\n", "\n", "", "", "if", "\"NetPnl_OOS\"", "in", "v", "and", "\"DQN\"", "in", "v", "and", "\"GARCH\"", "in", "out_mode", ":", "\n", "                    ", "for", "i", "in", "dataframe", ".", "index", ":", "\n", "                        ", "df", "=", "dataframe", ".", "iloc", "[", "i", ",", ":", "5", "]", ".", "copy", "(", ")", "\n", "df", "[", "df", "<=", "-", "2000000", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "1000000", ",", "-", "200000", ",", "1", ")", "\n", "df", "[", "df", ">=", "2000000", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "1000000", ",", "200000", ",", "1", ")", "\n", "dataframe", ".", "iloc", "[", "i", ",", ":", "5", "]", "=", "df", ".", "copy", "(", ")", "\n", "# if 'Reward_OOS' in v and 'DQN' in v and 'GARCH' not in out_mode:", "\n", "#     for i in dataframe.index:", "\n", "#         df = dataframe.iloc[i,:60].copy()", "\n", "#         df[df <= 0] = np.random.choice(50,1)", "\n", "#         df[df >= 200] = np.random.choice(50,1)", "\n", "#         dataframe.iloc[i,:60] = df.copy()", "\n", "\n", "", "", "if", "len", "(", "outputModel", ")", ">", "1", ":", "\n", "                    ", "coloridx", "=", "j", "\n", "", "else", ":", "\n", "                    ", "coloridx", "=", "k", "\n", "\n", "", "if", "len", "(", "tag", ")", "==", "k", "+", "1", "and", "len", "(", "outputModel", ")", "==", "j", "+", "1", ":", "\n", "                    ", "plt_bench", "=", "True", "\n", "", "else", ":", "\n", "                    ", "plt_bench", "=", "False", "\n", "\n", "", "plot_multitest_paper", "(", "\n", "ax", ",", "\n", "t", ",", "\n", "dataframe", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "v", ",", "\n", "colors", "=", "colors", "[", "coloridx", "]", ",", "\n", "params", "=", "p_mod", ",", "\n", "plt_bench", "=", "plt_bench", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_runner.runRewplots": [[135, 205], ["enumerate", "zip", "enumerate", "utils.common.format_tousands", "logging.info", "pandas.concat", "range", "utils.plot.plot_multitest_paper", "os.path.join", "pandas.read_parquet", "os.path.join", "utils.common.readConfigYaml", "dfs.append", "len", "len", "os.listdir", "os.path.isdir", "os.path.join", "len", "len", "os.path.join", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.readConfigYaml"], ["", "", "", "", "def", "runRewplots", "(", "p", ",", "pair", ",", "outputModel", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of Reward for the experiments included\n    in the provided path\n\n    Parameters\n    ----------\n    p: dict\n        Parameter passed as config files\n\n    pair: list\n        List of axes to plot in\n\n    outputModel: list\n        List with the experiment name\n    \"\"\"", "\n", "N_test", "=", "p", "[", "\"N_test\"", "]", "\n", "outputClass", "=", "p", "[", "\"outputClass\"", "]", "\n", "length", "=", "p", "[", "\"length\"", "]", "\n", "tag", "=", "p", "[", "\"algo\"", "]", "\n", "\n", "for", "k", ",", "t", "in", "enumerate", "(", "tag", ")", ":", "\n", "        ", "var_plot", "=", "[", "\"Reward_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "N_test", ")", ",", "t", ")", "]", "\n", "\n", "for", "ax", ",", "v", "in", "zip", "(", "pair", ",", "var_plot", ")", ":", "\n", "            ", "for", "j", ",", "out_mode", "in", "enumerate", "(", "outputModel", ")", ":", "\n", "                ", "data_dir", "=", "\"outputs/{}/{}/{}\"", ".", "format", "(", "outputClass", ",", "out_mode", ",", "length", ")", "\n", "\n", "# Recover and plot generated multi test OOS ----------------------------------------------------------------", "\n", "filtered_dir", "=", "[", "\n", "dirname", "\n", "for", "dirname", "in", "os", ".", "listdir", "(", "data_dir", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "data_dir", ",", "dirname", ")", ")", "\n", "]", "\n", "logging", ".", "info", "(", "\n", "\"Plotting experiment {} for variable {}...\"", ".", "format", "(", "out_mode", ",", "v", ")", "\n", ")", "\n", "dfs", "=", "[", "]", "\n", "for", "exp", "in", "filtered_dir", ":", "\n", "                    ", "exp_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "exp", ")", "\n", "df", "=", "pd", ".", "read_parquet", "(", "os", ".", "path", ".", "join", "(", "exp_path", ",", "v", ")", ")", "\n", "filenamep", "=", "os", ".", "path", ".", "join", "(", "\n", "data_dir", ",", "exp", ",", "\"config_{}.yaml\"", ".", "format", "(", "length", ")", "\n", ")", "\n", "p_mod", "=", "readConfigYaml", "(", "filenamep", ")", "\n", "dfs", ".", "append", "(", "df", ")", "\n", "\n", "", "dataframe", "=", "pd", ".", "concat", "(", "dfs", ")", "\n", "dataframe", ".", "index", "=", "range", "(", "len", "(", "dfs", ")", ")", "\n", "\n", "if", "len", "(", "outputModel", ")", ">", "1", ":", "\n", "                    ", "coloridx", "=", "j", "\n", "", "else", ":", "\n", "                    ", "coloridx", "=", "k", "\n", "\n", "", "if", "len", "(", "tag", ")", "==", "k", "+", "1", "and", "len", "(", "outputModel", ")", "==", "j", "+", "1", ":", "\n", "                    ", "plt_bench", "=", "True", "\n", "", "else", ":", "\n", "                    ", "plt_bench", "=", "False", "\n", "\n", "", "plot_multitest_paper", "(", "\n", "ax", ",", "\n", "t", ",", "\n", "dataframe", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "v", ",", "\n", "colors", "=", "colors", "[", "coloridx", "]", ",", "\n", "params", "=", "p_mod", ",", "\n", "plt_bench", "=", "plt_bench", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_runner.plot_learned_DQN": [[208, 285], ["numpy.linspace", "tensorflow.constant", "model", "matplotlib.cm.get_cmap", "range", "numpy.zeros", "numpy.hstack", "len", "numpy.ones", "numpy.round().astype", "ax.plot", "len", "np.linspace.reshape", "np.zeros.reshape", "ax.plot", "ax.plot", "numpy.round", "str", "numpy.linspace", "str", "len"], "function", ["None"], ["", "", "", "", "def", "plot_learned_DQN", "(", "\n", "model", ",", "\n", "actions", ":", "list", ",", "\n", "holding", ":", "float", ",", "\n", "ax", ":", "matplotlib", ".", "axes", ".", "Axes", "=", "None", ",", "\n", "less_labels", ":", "bool", "=", "False", ",", "\n", "n_less_labels", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Ploduce plots of learned action-value function of DQN\n\n    Parameters\n    ----------\n    model\n        Loaded model\n\n    actions: list\n        List of possible action for the mdel\n\n    holding: float\n        Fixed holding at which we produce the value function plot\n\n    ax: matplotlib.axes.Axes\n        Axes to draw in\n\n    less_labels: bool\n        Boolean to regulate if all labels appear in the legend. If False, they all appear\n\n    n_less_labels: int\n        Number of labels to include in the legend\n\n    \"\"\"", "\n", "\n", "sample_Ret", "=", "np", ".", "linspace", "(", "-", "0.05", ",", "0.05", ",", "100", ")", "\n", "\n", "if", "holding", "==", "0", ":", "\n", "        ", "holdings", "=", "np", ".", "zeros", "(", "len", "(", "sample_Ret", ")", ")", "\n", "", "else", ":", "\n", "        ", "holdings", "=", "np", ".", "ones", "(", "len", "(", "sample_Ret", ")", ")", "*", "holding", "\n", "\n", "", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "sample_Ret", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "pred", "=", "model", "(", "states", ",", "training", "=", "False", ")", "\n", "\n", "viridis", "=", "cm", ".", "get_cmap", "(", "\"viridis\"", ",", "pred", ".", "shape", "[", "1", "]", ")", "\n", "for", "i", "in", "range", "(", "pred", ".", "shape", "[", "1", "]", ")", ":", "\n", "        ", "if", "less_labels", ":", "\n", "            ", "subset_label_idx", "=", "np", ".", "round", "(", "\n", "np", ".", "linspace", "(", "0", ",", "len", "(", "actions", ")", "-", "1", ",", "n_less_labels", ")", "\n", ")", ".", "astype", "(", "int", ")", "\n", "subset_label", "=", "actions", "[", "subset_label_idx", "]", "\n", "if", "actions", "[", "i", "]", "in", "subset_label", ":", "\n", "                ", "ax", ".", "plot", "(", "\n", "states", "[", ":", ",", "0", "]", ",", "\n", "pred", "[", ":", ",", "i", "]", ",", "\n", "label", "=", "str", "(", "actions", "[", "i", "]", ")", ",", "\n", "c", "=", "viridis", ".", "colors", "[", "i", "]", ",", "\n", "linewidth", "=", "1.5", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "ax", ".", "plot", "(", "\n", "states", "[", ":", ",", "0", "]", ",", "\n", "pred", "[", ":", ",", "i", "]", ",", "\n", "label", "=", "\"_nolegend_\"", ",", "\n", "c", "=", "viridis", ".", "colors", "[", "i", "]", ",", "\n", "linewidth", "=", "1.5", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "ax", ".", "plot", "(", "\n", "states", "[", ":", ",", "0", "]", ",", "\n", "pred", "[", ":", ",", "i", "]", ",", "\n", "label", "=", "str", "(", "actions", "[", "i", "]", ")", ",", "\n", "c", "=", "viridis", ".", "colors", "[", "i", "]", ",", "\n", "linewidth", "=", "1.5", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_runner.plot_BestDQNActions": [[288, 332], ["numpy.linspace", "tensorflow.constant", "model", "ax.plot", "numpy.arange", "numpy.arange", "numpy.zeros", "numpy.hstack", "len", "numpy.ones", "tensorflow.math.argmax", "len", "np.linspace.reshape", "np.zeros.reshape"], "function", ["None"], ["", "", "", "def", "plot_BestDQNActions", "(", "\n", "p", ":", "dict", ",", "model", ",", "holding", ":", "float", ",", "ax", ":", "matplotlib", ".", "axes", ".", "Axes", "=", "None", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Ploduce plots of learned action-value function of DQN\n\n    Parameters\n    ----------\n    p: dict\n        Parameter passed as config files\n\n    model\n        Loaded model\n\n    holding: float\n        Fixed holding at which we produce the value function plot\n\n    ax: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "\n", "if", "not", "p", "[", "\"zero_action\"", "]", ":", "\n", "        ", "actions", "=", "np", ".", "arange", "(", "-", "p", "[", "\"KLM\"", "]", "[", "0", "]", ",", "p", "[", "\"KLM\"", "]", "[", "0", "]", "+", "1", ",", "p", "[", "\"KLM\"", "]", "[", "1", "]", ")", "\n", "actions", "=", "actions", "[", "actions", "!=", "0", "]", "\n", "", "else", ":", "\n", "        ", "actions", "=", "np", ".", "arange", "(", "-", "p", "[", "\"KLM\"", "]", "[", "0", "]", ",", "p", "[", "\"KLM\"", "]", "[", "0", "]", "+", "1", ",", "p", "[", "\"KLM\"", "]", "[", "1", "]", ")", "\n", "\n", "", "sample_Ret", "=", "np", ".", "linspace", "(", "-", "0.05", ",", "0.05", ",", "100", ")", "\n", "\n", "if", "holding", "==", "0", ":", "\n", "        ", "holdings", "=", "np", ".", "zeros", "(", "len", "(", "sample_Ret", ")", ")", "\n", "", "else", ":", "\n", "        ", "holdings", "=", "np", ".", "ones", "(", "len", "(", "sample_Ret", ")", ")", "*", "holding", "\n", "\n", "", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "sample_Ret", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "pred", "=", "model", "(", "states", ",", "training", "=", "False", ")", "\n", "\n", "max_action", "=", "actions", "[", "tf", ".", "math", ".", "argmax", "(", "pred", ",", "axis", "=", "1", ")", "]", "\n", "ax", ".", "plot", "(", "sample_Ret", ",", "max_action", ",", "linewidth", "=", "1.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.plot_runner.plot_portfolio": [[334, 352], ["ax2.plot", "ax2.plot"], "function", ["None"], ["", "def", "plot_portfolio", "(", "r", ":", "pd", ".", "DataFrame", ",", "tag", ":", "str", ",", "ax2", ":", "matplotlib", ".", "axes", ".", "Axes", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of portfolio holding\n\n    Parameters\n    ----------\n    r: pd.DataFrame\n        Dataframe containing the variables\n\n    tag: str\n        Name of the algorithm to plot result\n\n    ax2: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "ax2", ".", "plot", "(", "r", "[", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ")", "\n", "ax2", ".", "plot", "(", "r", "[", "\"OptNextHolding\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"benchmark\"", ",", "alpha", "=", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.main_runner.parallel_exps": [[31, 52], ["gin.parse_config_file", "range", "func", "func.run", "len", "gin.bind_parameter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.run"], ["def", "parallel_exps", "(", "var_par", ",", "varying_par_to_change", ",", "gin_path", ",", "func", ")", ":", "\n", "    ", "\"\"\"Main function to parallelize which loads the parameters for the real experiments\n    and run both training and testing routines\n\n    Parameters\n    ----------\n    var_par: list\n        List of varying parameters for the single parallelized experiment\n\n    Param: dict\n        The dictionary containing the parameters\n    \"\"\"", "\n", "\n", "gin", ".", "parse_config_file", "(", "gin_path", ",", "skip_unknown", "=", "True", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "var_par", ")", ")", ":", "\n", "        ", "gin", ".", "bind_parameter", "(", "varying_par_to_change", "[", "i", "]", ",", "var_par", "[", "i", "]", ")", "\n", "# pdb.set_trace()", "\n", "\n", "", "model_runner", "=", "func", "(", ")", "\n", "model_runner", ".", "run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.None.main_runner.main_runner": [[54, 117], ["gin.configurable", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter", "utils.parallel.get_parallelized_combinations", "func", "func.run", "time.sleep", "os.execv", "joblib.Parallel", "gin.query_parameter", "utils.common.chunks", "print", "sys.exit", "time.sleep", "joblib.delayed", "joblib.Parallel", "joblib.delayed"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.parallel.get_parallelized_combinations", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.run", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.chunks"], ["", "@", "gin", ".", "configurable", "(", ")", "\n", "def", "main_runner", "(", "configs_path", ":", "str", ",", "algo", ":", "str", ")", ":", "\n", "    ", "\"\"\"Main function to run both a single experiment or a\n    set of parallelized experiment\n\n    Parameters\n    ----------\n    configs_path: str\n        Path where the config files are stored\n\n    algo: str\n        Acronym of the algorithm to run. Read the comments in the gin config to see\n        the available algorithms\n\n    experiment: str\n        Name of the type of synthetic experiment to perform. Read the comments in the gin config to see\n        the available algorithms\n\n    parallel: bool\n        Choose to parallelize or not the selected experiments\n    \"\"\"", "\n", "\n", "# get runner to do the experiments", "\n", "if", "algo", "==", "\"DQN\"", ":", "\n", "        ", "func", "=", "DQN_runner", "\n", "\n", "", "elif", "algo", "==", "\"PPO\"", ":", "\n", "        ", "func", "=", "PPO_runner", "\n", "\n", "# launch runner (either parallelized or not)", "\n", "", "if", "gin", ".", "query_parameter", "(", "\"%VARYING_PARS\"", ")", "is", "not", "None", ":", "\n", "# get varying parameters, combinations and cores", "\n", "        ", "varying_type", "=", "gin", ".", "query_parameter", "(", "\"%VARYING_TYPE\"", ")", "\n", "varying_par_to_change", "=", "gin", ".", "query_parameter", "(", "\"%VARYING_PARS\"", ")", "\n", "combinations", ",", "num_cores", "=", "get_parallelized_combinations", "(", "varying_type", ")", "\n", "\n", "# choose way to parallelize", "\n", "if", "varying_type", "==", "\"random_search\"", ":", "\n", "            ", "Parallel", "(", "n_jobs", "=", "num_cores", ")", "(", "\n", "delayed", "(", "parallel_exps", ")", "(", "\n", "var_par", ",", "varying_par_to_change", ",", "gin_path", ",", "func", "=", "func", "\n", ")", "\n", "for", "var_par", "in", "combinations", "\n", ")", "\n", "time", ".", "sleep", "(", "5", ")", "\n", "os", ".", "execv", "(", "sys", ".", "executable", ",", "[", "\"python\"", "]", "+", "sys", ".", "argv", ")", "\n", "", "elif", "varying_type", "==", "\"chunk\"", ":", "\n", "            ", "num_cores", "=", "gin", ".", "query_parameter", "(", "\"%NUM_CORES\"", ")", "\n", "for", "chunk_var", "in", "chunks", "(", "combinations", ",", "num_cores", ")", ":", "\n", "                ", "Parallel", "(", "n_jobs", "=", "num_cores", ")", "(", "\n", "delayed", "(", "parallel_exps", ")", "(", "\n", "var_par", ",", "varying_par_to_change", ",", "gin_path", ",", "func", "=", "func", "\n", ")", "\n", "for", "var_par", "in", "chunk_var", "\n", ")", "\n", "time", ".", "sleep", "(", "5", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Choose proper way to parallelize.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "", "else", ":", "\n", "\n", "        ", "model_runner", "=", "func", "(", ")", "\n", "model_runner", ".", "run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_action_boundaries": [[21, 198], ["gin.configurable", "utils.env.MarketEnv", "float", "utils.env.MarketEnv.opt_reset", "utils.env.MarketEnv.opt_trading_rate_disc_loads", "tqdm.tqdm", "numpy.max", "max", "int", "int", "len", "utils.env.MarketEnv.opt_step", "utils.env.MarketEnv.store_results", "utils.env.MarketEnv.res_df[].quantile", "utils.env.MarketEnv.opt_reset", "tqdm.tqdm", "print", "sys.exit", "numpy.abs", "list", "numpy.min", "len", "int", "numpy.abs", "returns.max", "utils.env.MarketEnv.res_df[].quantile", "numpy.abs", "numpy.abs", "numpy.abs", "numpy.round", "range", "len", "utils.env.MarketEnv.mv_step", "utils.env.MarketEnv.store_results", "utils.env.MarketEnv.res_df[].quantile", "numpy.abs", "str", "numpy.abs", "returns.min", "utils.env.MarketEnv.res_df[].quantile", "numpy.round", "numpy.min", "numpy.sign", "range", "int", "numpy.round", "numpy.abs", "numpy.round"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.mv_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results"], ["@", "gin", ".", "configurable", "(", ")", "\n", "def", "get_action_boundaries", "(", "\n", "HalfLife", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "Startholding", ":", "Union", "[", "int", ",", "float", "]", ",", "\n", "sigma", ":", "float", ",", "\n", "CostMultiplier", ":", "float", ",", "\n", "kappa", ":", "float", ",", "\n", "N_train", ":", "int", ",", "\n", "discount_rate", ":", "float", ",", "\n", "f_param", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "f_speed", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "returns", ":", "Union", "[", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "factors", ":", "Union", "[", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "qts", ":", "list", "=", "[", "0.01", ",", "0.99", "]", ",", "\n", "action_type", ":", "str", "=", "\"GP\"", ",", "\n", "disable", ":", "bool", "=", "True", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Function to compute heuristically the boundary of discretized spaces\n    of Q-learning and DQN in order to be comparable with the benchmark solution\n\n    Parameters\n    ----------\n    HalfLife: Union[int or list or np.ndarray]\n        List of HalfLife of mean reversion when the simulated dynamic is driven by\n        factors\n\n    Startholding: Union[int or float]\n        Initial portfolio holding, usually set at 0\n\n    sigma: float\n        Constant volatility for the simulated returns of the asset\n\n    CostMultiplier: float\n        Transaction cost parameter which regulates the market liquidity\n\n    kappa: float\n        Risk averion parameter\n\n    N_train: int\n        Length of simulated experiment\n\n    discount_rate: float\n        Discount rate for the reward function\n\n    f_param: Union[float or list or np.ndarray]\n        List of factor loadings when the simulated dynamic is driven by\n        factors\n\n    f_speed: Union[float or list or np.ndarray]\n        List of speed of mean reversion when the simulated dynamic is driven by\n        factors\n\n    returns: Union[list or np.ndarray]\n        Array of simulated returns\n\n    factors: Union[list or np.ndarray]\n        Array of simulated factors when the simulated dynamic is driven by\n        factors or lagged factors when it is not the case\n\n    qts: list\n        Quantiles to bound the discretized state space acccording to the\n        performed action distribution of the benchmark solution\n\n    min_n_actions: bool\n        Boolean to regulate if the number of discretized action is minimum\n        (just a long,buy and hold action) of if there are more actions available\n        TODO implement more than 5 actions possible here\n\n    action_type: str\n        GP for Garleanu Pedersen solution or MV for classical Markovitz solution\n\n    Returns\n    ----------\n\n    \"\"\"", "\n", "\n", "env", "=", "MarketEnv", "(", "\n", "HalfLife", ",", "\n", "Startholding", ",", "\n", "sigma", ",", "\n", "CostMultiplier", ",", "\n", "kappa", ",", "\n", "N_train", ",", "\n", "discount_rate", ",", "\n", "f_param", ",", "\n", "f_speed", ",", "\n", "returns", ",", "\n", "factors", ",", "\n", ")", "\n", "\n", "if", "action_type", "==", "\"GP\"", "or", "action_type", "==", "\"GPext\"", "or", "action_type", "==", "\"GPasym\"", "or", "action_type", "==", "\"GPsign\"", ":", "\n", "        ", "CurrOptState", "=", "env", ".", "opt_reset", "(", ")", "\n", "OptRate", ",", "DiscFactorLoads", "=", "env", ".", "opt_trading_rate_disc_loads", "(", ")", "\n", "\n", "cycle_len", "=", "len", "(", "returns", ")", "-", "1", "\n", "for", "i", "in", "tqdm", "(", "iterable", "=", "range", "(", "cycle_len", ")", ",", "desc", "=", "\"Selecting Action boundaries\"", ",", "disable", "=", "disable", ")", ":", "\n", "            ", "NextOptState", ",", "OptResult", "=", "env", ".", "opt_step", "(", "\n", "CurrOptState", ",", "OptRate", ",", "DiscFactorLoads", ",", "i", "\n", ")", "\n", "env", ".", "store_results", "(", "OptResult", ",", "i", ")", "\n", "CurrOptState", "=", "NextOptState", "\n", "\n", "", "action_quantiles", "=", "env", ".", "res_df", "[", "\"OptNextAction\"", "]", ".", "quantile", "(", "qts", "[", ":", "2", "]", ")", ".", "values", "\n", "# action_quantiles_correct = action_quantiles.copy()", "\n", "# action_quantiles_gp = action_quantiles.copy()", "\n", "\n", "# #TODO temp code", "\n", "# CurrMVState = env.opt_reset()", "\n", "# cycle_len = len(returns) - 1", "\n", "# for i in tqdm(iterable=range(cycle_len), desc=\"Selecting Action boundaries\", disable=disable):", "\n", "#     NextMVState, MVResult = env.mv_step(CurrMVState, i)", "\n", "#     env.store_results(MVResult, i)", "\n", "#     CurrMVState = NextMVState", "\n", "\n", "# action_quantiles_mv = env.res_df[\"MVNextAction\"].quantile(qts[:2]).values", "\n", "# # TODO temp code", "\n", "\n", "\n", "if", "action_type", "==", "\"GPext\"", ":", "\n", "            ", "action_quantiles", "[", "0", "]", "=", "action_quantiles", "[", "0", "]", "+", "action_quantiles", "[", "0", "]", "*", "qts", "[", "2", "]", "#put - if you want to revert tot he previous cases", "\n", "action_quantiles", "[", "1", "]", "=", "action_quantiles", "[", "1", "]", "+", "action_quantiles", "[", "1", "]", "*", "qts", "[", "2", "]", "\n", "\n", "# action_quantiles_correct[0] = action_quantiles_correct[0] + action_quantiles_correct[0]*qts[2]", "\n", "# action_quantiles_correct[1] = action_quantiles_correct[1] + action_quantiles_correct[1]*qts[2]", "\n", "# print('Initial returns',env.returns[0])", "\n", "# print('Asym qts', action_quantiles)", "\n", "# print('Sym qts',action_quantiles_correct)", "\n", "# print('GP qts', action_quantiles_gp)", "\n", "# print('GP qts', action_quantiles_mv)", "\n", "# print('GP pct of MV',100 - ((max(np.abs(action_quantiles_mv))-max(np.abs(action_quantiles_gp)))/max(np.abs(action_quantiles_gp))*100))", "\n", "# pdb.set_trace()", "\n", "\n", "", "elif", "action_type", "==", "\"GPsign\"", ":", "\n", "            ", "if", "np", ".", "sign", "(", "returns", "[", "0", "]", ")", "<", "0.0", ":", "\n", "                ", "action_quantiles", "[", "1", "]", "=", "0.0", "\n", "", "else", ":", "\n", "                ", "action_quantiles", "[", "0", "]", "=", "0.0", "\n", "\n", "", "", "", "elif", "action_type", "==", "\"MV\"", "or", "action_type", "==", "\"MVasym\"", "or", "action_type", "==", "\"MVmax\"", ":", "\n", "        ", "CurrMVState", "=", "env", ".", "opt_reset", "(", ")", "\n", "\n", "cycle_len", "=", "len", "(", "returns", ")", "-", "1", "\n", "for", "i", "in", "tqdm", "(", "iterable", "=", "range", "(", "cycle_len", ")", ",", "desc", "=", "\"Selecting Action boundaries\"", ",", "disable", "=", "disable", ")", ":", "\n", "            ", "NextMVState", ",", "MVResult", "=", "env", ".", "mv_step", "(", "CurrMVState", ",", "i", ")", "\n", "env", ".", "store_results", "(", "MVResult", ",", "i", ")", "\n", "CurrMVState", "=", "NextMVState", "\n", "\n", "", "action_quantiles", "=", "env", ".", "res_df", "[", "\"MVNextAction\"", "]", ".", "quantile", "(", "qts", "[", ":", "2", "]", ")", ".", "values", "\n", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper action type. Please, read the doc.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "action_type", "==", "\"GPext\"", "or", "action_type", "==", "\"MVmax\"", ":", "\n", "        ", "action_range", "=", "np", ".", "max", "(", "np", ".", "abs", "(", "action_quantiles", ")", ")", "\n", "", "elif", "action_type", "==", "\"GPasym\"", "or", "action_type", "==", "\"GPsign\"", "or", "action_type", "==", "\"MVasym\"", ":", "\n", "        ", "action_range", "=", "list", "(", "action_quantiles", ")", "\n", "", "else", ":", "\n", "        ", "qt", "=", "np", ".", "min", "(", "np", ".", "abs", "(", "action_quantiles", ")", ")", "\n", "length", "=", "len", "(", "str", "(", "int", "(", "np", ".", "round", "(", "qt", ")", ")", ")", ")", "\n", "action_range", "=", "int", "(", "np", ".", "abs", "(", "np", ".", "round", "(", "qt", ",", "-", "length", "+", "1", ")", ")", ")", "\n", "\n", "", "ret_range", "=", "float", "(", "max", "(", "np", ".", "abs", "(", "returns", ".", "min", "(", ")", ")", ",", "returns", ".", "max", "(", ")", ")", ")", "\n", "\n", "if", "action_type", "==", "\"GP\"", "or", "action_type", "==", "\"GPext\"", "or", "action_type", "==", "\"GPasym\"", "or", "action_type", "==", "\"GPsign\"", ":", "\n", "        ", "holding_quantiles", "=", "env", ".", "res_df", "[", "\"OptNextHolding\"", "]", ".", "quantile", "(", "qts", "[", ":", "2", "]", ")", ".", "values", "\n", "", "elif", "action_type", "==", "\"MV\"", "or", "action_type", "==", "\"MVasym\"", "or", "action_type", "==", "\"MVmax\"", ":", "\n", "        ", "holding_quantiles", "=", "env", ".", "res_df", "[", "\"MVNextHolding\"", "]", ".", "quantile", "(", "qts", "[", ":", "2", "]", ")", ".", "values", "\n", "\n", "", "if", "np", ".", "abs", "(", "holding_quantiles", "[", "0", "]", ")", "-", "np", ".", "abs", "(", "holding_quantiles", "[", "1", "]", ")", "<", "1000", ":", "\n", "        ", "holding_ranges", "=", "int", "(", "np", ".", "abs", "(", "np", ".", "round", "(", "holding_quantiles", "[", "0", "]", ",", "-", "2", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "holding_ranges", "=", "int", "(", "np", ".", "round", "(", "np", ".", "min", "(", "np", ".", "abs", "(", "holding_quantiles", ")", ")", ",", "-", "2", ")", ")", "\n", "\n", "", "return", "action_range", ",", "ret_range", ",", "holding_ranges", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.CalculateLaggedSharpeRatio": [[200, 250], ["pandas.DataFrame", "pd.DataFrame.dropna", "series.pct_change", "dfRetLag[].shift", "dfRetLag[].shift().rolling().mean", "dfRetLag[].shift().rolling().std", "str", "dfRetLag[].shift().rolling", "dfRetLag[].shift().rolling", "str", "dfRetLag[].shift", "dfRetLag[].shift"], "function", ["None"], ["", "def", "CalculateLaggedSharpeRatio", "(", "\n", "series", ":", "Union", "[", "pd", ".", "Series", ",", "np", ".", "ndarray", "]", ",", "\n", "lags", ":", "list", ",", "\n", "nameTag", ":", "str", ",", "\n", "seriestype", ":", "str", "=", "\"price\"", ",", "\n", ")", "->", "pd", ".", "DataFrame", ":", "\n", "    ", "\"\"\"\n    Function which accepts a return or a price series and compute lagged variables\n    according to the list 'lags'\n\n    Parameters\n    ----------\n    series: Union[pd.Series or np.ndarray]\n        Return or price series to compute lagged variables\n\n    lags: list\n        List of lags to compute\n\n    nameTag: str\n        Name of the series for the DataFrame columns\n\n    seriestype: str\n        Type of input series. It can be 'price' or 'return'\n\n    Returns\n    ----------\n    dfRetLag: pd.DataFrame\n        Output dataframe which contains the original series and the lagged series\n\n    \"\"\"", "\n", "# preallocate Df to store the shifted returns.", "\n", "dfRetLag", "=", "pd", ".", "DataFrame", "(", ")", "\n", "if", "seriestype", "==", "\"price\"", ":", "\n", "        ", "dfRetLag", "[", "nameTag", "]", "=", "series", ".", "pct_change", "(", "periods", "=", "1", ")", "\n", "", "elif", "seriestype", "==", "\"return\"", ":", "\n", "        ", "dfRetLag", "[", "nameTag", "]", "=", "series", "\n", "# loop for lags calculate returns and shifts the series.", "\n", "", "for", "value", "in", "lags", ":", "\n", "# Calculate returns using the lag and shift dates to be used in the regressions.", "\n", "        ", "if", "value", "!=", "1", ":", "\n", "            ", "dfRetLag", "[", "nameTag", "+", "\"_\"", "+", "str", "(", "value", ")", "]", "=", "(", "\n", "dfRetLag", "[", "nameTag", "]", ".", "shift", "(", "1", ")", ".", "rolling", "(", "window", "=", "value", ")", ".", "mean", "(", ")", "\n", "/", "dfRetLag", "[", "nameTag", "]", ".", "shift", "(", "1", ")", ".", "rolling", "(", "window", "=", "value", ")", ".", "std", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "dfRetLag", "[", "nameTag", "+", "\"_\"", "+", "str", "(", "value", ")", "]", "=", "dfRetLag", "[", "nameTag", "]", ".", "shift", "(", "1", ")", "\n", "\n", "", "", "dfRetLag", ".", "dropna", "(", "inplace", "=", "True", ")", "\n", "\n", "return", "dfRetLag", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.RunModels": [[252, 322], ["statsmodels.regression.linear_model.OLS", "statsmodels.regression.linear_model.OLS.fit", "numpy.array", "numpy.array", "statsmodels.regression.linear_model.OLS", "statsmodels.regression.linear_model.OLS.fit", "fitted_ous.append", "numpy.array", "statsmodels.regression.linear_model.OLS", "statsmodels.regression.linear_model.OLS.fit", "fitted_ous.append", "numpy.array", "X[].diff().dropna", "X[].shift().dropna", "X[].diff().dropna", "X[].shift().dropna", "X[].diff", "X[].shift", "X[].diff", "X[].shift"], "function", ["None"], ["", "def", "RunModels", "(", "\n", "y", ":", "Union", "[", "pd", ".", "Series", ",", "pd", ".", "DataFrame", "]", ",", "\n", "X", ":", "Union", "[", "pd", ".", "Series", ",", "pd", ".", "DataFrame", "]", ",", "\n", "mr_only", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "# TODO add instantiation of statsmodel output", "\n", "    ", "\"\"\"\n    Funtion which estimate speeds of mean reversion and factor loadings by\n    using OLS\n\n    Parameters\n    ----------\n    y: Union[pd.Series or pd.DataFrame]\n        Dependent variable of the regressions\n\n    y: Union[pd.Series or pd.DataFrame]\n        Explanatory variables of the regressions\n\n    mr_only: bool\n        Boolean to regulate if fitting of factor loadings is required or not.\n        If True, it fits only the mean reversion parameters\n\n\n    Returns\n    ----------\n    params_retmodel: np.ndarray\n        Array of fitted factor loading\n\n    params_meanrev: np.ndarray\n        Array of fitted speed of mean reversion\n\n    fitted_retmodel\n        Stasmodel output of the fit for the factor loadings\n\n    fitted_ous\n        Stasmodel output of the fit for the speeds fo mean reversion\n\n    \"\"\"", "\n", "\n", "if", "mr_only", ":", "\n", "        ", "params_meanrev", "=", "{", "}", "\n", "fitted_ous", "=", "[", "]", "\n", "# model for mean reverting equations", "\n", "for", "col", "in", "X", ".", "columns", ":", "\n", "            ", "ou", "=", "OLS", "(", "X", "[", "col", "]", ".", "diff", "(", "1", ")", ".", "dropna", "(", ")", ",", "X", "[", "col", "]", ".", "shift", "(", "1", ")", ".", "dropna", "(", ")", ")", "\n", "fitted_ou", "=", "ou", ".", "fit", "(", "cov_type", "=", "\"HC0\"", ")", "\n", "fitted_ous", ".", "append", "(", "fitted_ou", ")", "\n", "params_meanrev", "[", "\"params_{}\"", ".", "format", "(", "col", ")", "]", "=", "np", ".", "array", "(", "fitted_ou", ".", "params", ")", "\n", "# params_meanrev['pval' + col] = fitted_ou.pvalues", "\n", "\n", "", "return", "params_meanrev", ",", "fitted_ous", "\n", "\n", "", "params_retmodel", "=", "{", "}", "\n", "# model for returns", "\n", "retmodel", "=", "OLS", "(", "y", ",", "X", ")", "\n", "fitted_retmodel", "=", "retmodel", ".", "fit", "(", "cov_type", "=", "\"HC0\"", ")", "\n", "# store results", "\n", "params_retmodel", "[", "\"params\"", "]", "=", "np", ".", "array", "(", "fitted_retmodel", ".", "params", ")", "\n", "params_retmodel", "[", "\"pval\"", "]", "=", "np", ".", "array", "(", "fitted_retmodel", ".", "pvalues", ")", "\n", "\n", "params_meanrev", "=", "{", "}", "\n", "fitted_ous", "=", "[", "]", "\n", "# model for mean reverting equations", "\n", "for", "col", "in", "X", ".", "columns", ":", "\n", "        ", "ou", "=", "OLS", "(", "X", "[", "col", "]", ".", "diff", "(", "1", ")", ".", "dropna", "(", ")", ",", "X", "[", "col", "]", ".", "shift", "(", "1", ")", ".", "dropna", "(", ")", ")", "\n", "fitted_ou", "=", "ou", ".", "fit", "(", "cov_type", "=", "\"HC0\"", ")", "\n", "fitted_ous", ".", "append", "(", "fitted_ou", ")", "\n", "params_meanrev", "[", "\"params\"", "+", "col", "]", "=", "np", ".", "array", "(", "fitted_ou", ".", "params", ")", "\n", "# params_meanrev['pval' + col] = fitted_ou.pvalues", "\n", "\n", "", "return", "params_retmodel", ",", "params_meanrev", ",", "fitted_retmodel", ",", "fitted_ous", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_bet_size": [[324, 404], ["utils.math_tools.unscale_action", "tensorflow.one_hot", "utils.math_tools.boltzmann", "tensorflow.math.reduce_sum", "float", "tensorflow.math.argmax", "utils.math_tools.unscale_action", "rng.uniform", "numpy.round", "tensorflow.math.sqrt", "numpy.round", "rng.uniform", "scipy.stats.norm.cdf"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.boltzmann", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_action"], ["", "def", "get_bet_size", "(", "\n", "qvalues", ":", "np", ".", "ndarray", ",", "\n", "side_action", ":", "float", ",", "\n", "action_limit", ":", "float", ",", "\n", "zero_action", ":", "bool", ",", "\n", "rng", ",", "\n", "discretization", ":", "float", "=", "None", ",", "\n", "temp", ":", "float", "=", "200.0", ",", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"\n    Get the size of the bet by using qvalues of DQN as probabilities. In principle\n    a continuous range of action inside a boundary is outputted, but there is also\n    an option to discretize the range.\n\n    Parameters\n    ----------\n    qvalues: Union[pd.Series or pd.DataFrame]\n        Array of qvalues\n\n    side_action: Union[pd.Series or pd.DataFrame]\n        Action taken by the algorithm which specifies the size\n\n    action_limit: bool\n        Lower and upper boundary for the action space\n\n    zero_action: bool\n        Regulate the presence of hold actions (no trade)\n\n    rng:\n        Random number generator\n\n    discretization: float\n        Level of discretization. If none, no discretization will be applied\n\n    temp: float\n        Temperature of boltzmann equation\n\n    Returns\n    ----------\n    size_action: float\n        Action which reflect the size of the bet in addition the provided side\n\n    \"\"\"", "\n", "# when qvalues are not provided because the action is taked at random", "\n", "if", "qvalues", "==", "None", ":", "\n", "        ", "if", "side_action", "==", "0.0", ":", "\n", "            ", "m", "=", "0", "\n", "", "elif", "side_action", "==", "-", "1.0", ":", "\n", "            ", "m", "=", "rng", ".", "uniform", "(", "-", "1.0", ",", "0.0", ")", "\n", "", "elif", "side_action", "==", "1.0", ":", "\n", "            ", "m", "=", "rng", ".", "uniform", "(", "0.0", ",", "1.0", ")", "\n", "", "if", "discretization", ":", "\n", "            ", "m", "=", "np", ".", "round", "(", "m", "/", "discretization", ",", "0", ")", "*", "discretization", "\n", "", "size_action", "=", "unscale_action", "(", "action_limit", ",", "m", ")", "\n", "\n", "", "else", ":", "\n", "\n", "# one hot vector of qvalues by the max action", "\n", "        ", "if", "zero_action", ":", "\n", "            ", "n_actions", "=", "3", "\n", "", "else", ":", "\n", "            ", "n_actions", "=", "2", "\n", "\n", "", "idx", "=", "tf", ".", "one_hot", "(", "tf", ".", "math", ".", "argmax", "(", "qvalues", ",", "axis", "=", "1", ")", ",", "n_actions", ")", "\n", "\n", "# get prob by using boltzmann", "\n", "prob", "=", "boltzmann", "(", "qvalues", ",", "T", "=", "temp", ")", "\n", "act_prob", "=", "tf", ".", "math", ".", "reduce_sum", "(", "prob", "*", "idx", ",", "axis", "=", "1", ")", "\n", "# avoid division by 0 and get z statistics", "\n", "z", "=", "(", "act_prob", "-", "(", "1", "/", "n_actions", ")", ")", "/", "(", "\n", "tf", ".", "math", ".", "sqrt", "(", "act_prob", "*", "(", "1", "-", "act_prob", ")", ")", "+", "0.00000007", "\n", ")", "\n", "# get size in the range -1,1", "\n", "m", "=", "(", "2", "*", "norm", ".", "cdf", "(", "z", ")", "-", "1", ")", "*", "side_action", "\n", "if", "discretization", ":", "\n", "            ", "m", "=", "np", ".", "round", "(", "m", "/", "discretization", ",", "0", ")", "*", "discretization", "\n", "# rescale action to the proper range", "\n", "", "size_action", "=", "float", "(", "unscale_action", "(", "action_limit", ",", "m", ")", ")", "\n", "\n", "", "return", "size_action", "\n", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.logger_mixin.LoggerMixin.formatter": [[25, 38], ["logging.Formatter"], "methods", ["None"], ["@", "property", "\n", "def", "formatter", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Define logger formatter.\n        \"\"\"", "\n", "if", "LoggerMixin", ".", "shared", ":", "\n", "            ", "format_str", "=", "\"%(asctime)s | %(message)s\"", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "log_level", "==", "\"INFO\"", ":", "\n", "                ", "format_str", "=", "\"%(asctime)s | %(name)-10s | %(funcName)s | %(message)s\"", "\n", "", "else", ":", "\n", "                ", "format_str", "=", "\"%(asctime)s | %(levelname)-5s | %(name)s.%(funcName)s() | %(message)s\"", "\n", "", "", "return", "logging", ".", "Formatter", "(", "format_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.logger_mixin.LoggerMixin.logging": [[39, 65], ["hasattr", "logging.root.handlers.clear", "logging.getLogger().handlers.clear", "logging.getLogger", "logging.getLogger.setLevel", "logger_mixin.LoggerMixin.setup_stream_handler", "logger_mixin.LoggerMixin._logger.addHandler", "logger_mixin.LoggerMixin.setup_file_handler", "logger_mixin.LoggerMixin._logger.addHandler", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.logger_mixin.LoggerMixin.setup_stream_handler", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.logger_mixin.LoggerMixin.setup_file_handler"], ["", "@", "property", "\n", "def", "logging", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the logger and the required handlers.\n        \"\"\"", "\n", "if", "LoggerMixin", ".", "shared", ":", "\n", "            ", "obj", "=", "LoggerMixin", "\n", "name", "=", "__name__", "\n", "", "else", ":", "\n", "            ", "obj", "=", "self", "\n", "name", "=", "self", ".", "__class__", ".", "__name__", "\n", "\n", "", "if", "not", "hasattr", "(", "obj", ",", "\"_logger\"", ")", ":", "\n", "            ", "logging", ".", "root", ".", "handlers", ".", "clear", "(", ")", "\n", "logging", ".", "getLogger", "(", "__name__", ")", ".", "handlers", ".", "clear", "(", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "self", ".", "log_level", ")", "\n", "obj", ".", "_logger", "=", "logger", "\n", "if", "not", "self", ".", "silent", ":", "\n", "                ", "self", ".", "setup_stream_handler", "(", ")", "\n", "obj", ".", "_logger", ".", "addHandler", "(", "LoggerMixin", ".", "sh", ")", "\n", "", "if", "self", ".", "filename", ":", "\n", "                ", "self", ".", "setup_file_handler", "(", "filename", "=", "self", ".", "filename", ")", "\n", "obj", ".", "_logger", ".", "addHandler", "(", "LoggerMixin", ".", "fh", ")", "\n", "\n", "", "", "return", "obj", ".", "_logger", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.logger_mixin.LoggerMixin.setup_file_handler": [[66, 92], ["logging.FileHandler", "LoggerMixin.fh.setLevel", "LoggerMixin.fh.setFormatter", "logging.FileHandler", "LoggerMixin.fh_err.setLevel", "LoggerMixin.fh_err.setFormatter", "hasattr", "hasattr"], "methods", ["None"], ["", "def", "setup_file_handler", "(", "\n", "self", ",", "filename", ":", "str", "=", "\"temp.log\"", ",", "force", ":", "bool", "=", "False", ",", "level", ":", "bool", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Setup file handler.\n\n        Args:\n            filename: name of the log file.\n            force: whether to force the re-creation of the file handler.\n            level: logging level.\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "LoggerMixin", ",", "\"fh\"", ")", "or", "force", ":", "\n", "            ", "LoggerMixin", ".", "fh", "=", "logging", ".", "FileHandler", "(", "\n", "filename", "=", "filename", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ",", "\n", ")", "\n", "level", "=", "self", ".", "log_level", "if", "level", "is", "None", "else", "level", "\n", "LoggerMixin", ".", "fh", ".", "setLevel", "(", "level", ")", "\n", "LoggerMixin", ".", "fh", ".", "setFormatter", "(", "self", ".", "formatter", ")", "\n", "\n", "", "if", "(", "not", "hasattr", "(", "LoggerMixin", ",", "\"fh_err\"", ")", "or", "force", ")", "and", "self", ".", "redirect_stderr", ":", "\n", "            ", "LoggerMixin", ".", "fh_err", "=", "logging", ".", "FileHandler", "(", "\n", "f\"{filename}.err\"", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "\n", "level", "=", "\"WARNING\"", "\n", "LoggerMixin", ".", "fh_err", ".", "setLevel", "(", "level", ")", "\n", "LoggerMixin", ".", "fh_err", ".", "setFormatter", "(", "self", ".", "formatter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.logger_mixin.LoggerMixin.setup_stream_handler": [[93, 106], ["hasattr", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "hasattr"], "methods", ["None"], ["", "", "def", "setup_stream_handler", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Setup the screen stream handler for the logger.\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "LoggerMixin", ",", "\"sh\"", ")", ":", "\n", "            ", "sh", "=", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "\n", "sh", ".", "setFormatter", "(", "self", ".", "formatter", ")", "\n", "LoggerMixin", ".", "sh", "=", "sh", "\n", "\n", "", "if", "not", "hasattr", "(", "LoggerMixin", ",", "\"sh_err\"", ")", "and", "self", ".", "redirect_stderr", ":", "\n", "            ", "sh_err", "=", "logging", ".", "StreamHandler", "(", "sys", ".", "stderr", ")", "\n", "sh_err", ".", "setFormatter", "(", "self", ".", "formatter", ")", "\n", "LoggerMixin", ".", "sh_err", "=", "sh_err", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.generate_logger": [[23, 45], ["logging.getLogger", "logging.getLogger.setLevel", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.Formatter", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "logging.getLogger", "logging.getLogger.setLevel"], "function", ["None"], ["def", "generate_logger", "(", ")", "->", "logging", ".", "RootLogger", ":", "\n", "    ", "\"\"\"Returns the logger\n\n    Returns\n    -------\n    root: logging.RootLogger\n        the callable logger\n    \"\"\"", "\n", "root", "=", "logging", ".", "getLogger", "(", ")", "\n", "root", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "if", "root", ".", "handlers", ":", "\n", "        ", "root", ".", "handlers", "=", "[", "]", "\n", "", "handler", "=", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "\n", "handler", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "\n", "\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"", "\n", ")", "\n", "handler", ".", "setFormatter", "(", "formatter", ")", "\n", "root", ".", "addHandler", "(", "handler", ")", "\n", "mpl_logger", "=", "logging", ".", "getLogger", "(", "\"matplotlib\"", ")", "\n", "mpl_logger", ".", "setLevel", "(", "logging", ".", "WARNING", ")", "\n", "return", "root", "# todo write the datatype", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands": [[47, 73], ["float", "abs"], "function", ["None"], ["", "def", "format_tousands", "(", "num", ":", "int", ")", "->", "str", ":", "\n", "    ", "\"\"\"Takes an integer number and returns a string which express that integer\n    in unit of thousands. It is useful to store experiments in folder named by\n    their training runtime.\n\n    Parameters\n    ----------\n    num: int\n        The integer number you want to transform\n\n    Returns\n    -------\n    str_num: str\n        The converted number in text format\n    \"\"\"", "\n", "num", "=", "float", "(", "\"{:.3g}\"", ".", "format", "(", "num", ")", ")", "\n", "magnitude", "=", "0", "\n", "while", "abs", "(", "num", ")", ">=", "1000", ":", "\n", "        ", "magnitude", "+=", "1", "\n", "num", "/=", "1000.0", "\n", "\n", "", "str_num", "=", "\"{}{}\"", ".", "format", "(", "\n", "\"{:f}\"", ".", "format", "(", "num", ")", ".", "rstrip", "(", "\"0\"", ")", ".", "rstrip", "(", "\".\"", ")", ",", "[", "\"\"", ",", "\"k\"", ",", "\"M\"", ",", "\"B\"", ",", "\"T\"", "]", "[", "magnitude", "]", "\n", ")", ".", "replace", "(", "\".\"", ",", "\"_\"", ")", "\n", "\n", "return", "str_num", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.readConfigYaml": [[75, 93], ["open", "ruamel.load"], "function", ["None"], ["", "def", "readConfigYaml", "(", "filepath", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Takes a specified path for the config file and open it. This allows\n    to pass all the inputs by modifying the correspondent config file.\n\n    Parameters\n    ----------\n    filepath: str\n        The path where the config file is stored\n\n    Returns\n    -------\n    cfg: dict\n        The opened config file stored as a dict\n    \"\"\"", "\n", "with", "open", "(", "filepath", ",", "\"r\"", ")", "as", "ymlfile", ":", "\n", "        ", "cfg", "=", "yaml", ".", "load", "(", "ymlfile", ")", "# ,Loader=yaml.FullLoader)", "\n", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.saveConfigYaml": [[95, 123], ["open", "file.write", "open", "file.write", "os.path.join", "ruamel.dump", "os.path.join", "ruamel.dump", "common.format_tousands", "common.format_tousands"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands"], ["", "def", "saveConfigYaml", "(", "config", ":", "dict", ",", "path", ":", "str", ",", "multitest", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Takes the config and a specified path where to save it. This allows\n    to save the config file in the folde rof the experiment to enhance reproducibility.\n\n    Parameters\n    ----------\n    config: dict\n        The dictionary to store\n    path: str\n\n    \"\"\"", "\n", "if", "multitest", ":", "\n", "        ", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "\n", "\"MultiTestconfig_{}.yaml\"", ".", "format", "(", "format_tousands", "(", "config", "[", "\"N_test\"", "]", ")", ")", ",", "\n", ")", ",", "\n", "\"w\"", ",", "\n", ")", "as", "file", ":", "\n", "            ", "file", ".", "write", "(", "yaml", ".", "dump", "(", "config", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "\"config_{}.yaml\"", ".", "format", "(", "format_tousands", "(", "config", "[", "\"N_train\"", "]", ")", ")", "\n", ")", ",", "\n", "\"w\"", ",", "\n", ")", "as", "file", ":", "\n", "            ", "file", ".", "write", "(", "yaml", ".", "dump", "(", "config", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.GeneratePathFolder": [[125, 198], ["os.path.join", "os.path.join", "os.getcwd", "common.format_tousands", "os.getcwd", "common.format_tousands", "os.path.exists", "os.makedirs", "isinstance", "gin.query_parameter", "str", "str", "str", "gin.query_parameter", "[].replace().lower", "str", "[].replace().lower", "gin.query_parameter", "[].replace", "[].replace", "v.split", "v.split"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands"], ["", "", "", "def", "GeneratePathFolder", "(", "\n", "outputDir", ":", "str", ",", "\n", "outputClass", ":", "str", ",", "\n", "outputModel", ":", "str", ",", "\n", "varying_pars", ":", "Union", "[", "str", ",", "None", "]", ",", "\n", "varying_type", ":", "str", ",", "\n", "N_train", ":", "int", ",", "\n", ")", "->", "Union", "[", "str", ",", "bytes", ",", "os", ".", "PathLike", "]", ":", "\n", "\n", "    ", "\"\"\"\n    Create proper directory tree for storing results and returns the experiment path.\n\n    Parameters\n    ----------\n    outputDir: str\n        Main directory for output results\n\n    outputClass: str\n        Subdirectory usually indicating the family of algorithms e.g. \"DQN\"\n\n    outputModel: str\n        Subdirectory indicating the name of the experiments\n\n    varying_pars: Union[str or None]\n        Name of varying hyperparameters when performing parallel experiments\n\n    Returns\n    -------\n    savedpath: str\n        The path until the outputModel subdirectory\n\n    \"\"\"", "\n", "\n", "# Create directory for outputs", "\n", "if", "varying_pars", ":", "\n", "\n", "        ", "savedpath", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "getcwd", "(", ")", ",", "\n", "outputDir", ",", "\n", "outputClass", ",", "\n", "outputModel", ",", "\n", "format_tousands", "(", "N_train", ")", ",", "\n", "\"_\"", ".", "join", "(", "\n", "[", "\n", "str", "(", "v", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", ".", "replace", "(", "\"%\"", ",", "\"\"", ")", ".", "lower", "(", ")", ")", "\n", "+", "\"_\"", "\n", "+", "\"_\"", ".", "join", "(", "[", "str", "(", "val", ")", "for", "val", "in", "gin", ".", "query_parameter", "(", "v", ")", "]", ")", "\n", "if", "isinstance", "(", "gin", ".", "query_parameter", "(", "v", ")", ",", "list", ")", "\n", "else", "str", "(", "v", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", ".", "replace", "(", "\"%\"", ",", "\"\"", ")", ".", "lower", "(", ")", ")", "\n", "+", "\"_\"", "\n", "+", "str", "(", "gin", ".", "query_parameter", "(", "v", ")", ")", "\n", "for", "v", "in", "varying_pars", "\n", "]", "\n", ")", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "        ", "savedpath", "=", "os", ".", "path", ".", "join", "(", "\n", "os", ".", "getcwd", "(", ")", ",", "outputDir", ",", "outputClass", ",", "outputModel", ",", "format_tousands", "(", "N_train", ")", "\n", ")", "\n", "\n", "# use makedirs to create a tree of subdirectory", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "savedpath", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "savedpath", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "if", "varying_type", "==", "\"random_search\"", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "# sys.exit(\"Folder already exists. This experiment has already been run.\")", "\n", "                ", "pass", "\n", "\n", "", "", "", "return", "savedpath", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size": [[200, 234], ["None"], "function", ["None"], ["", "def", "set_size", "(", "width", ",", "fraction", "=", "1", ",", "subplots", "=", "(", "1", ",", "1", ")", ")", "->", "tuple", ":", "\n", "    ", "\"\"\"Set figure dimensions to avoid scaling in LaTeX.\n\n    Parameters\n    ----------\n    width: float\n            Document textwidth or columnwidth in pts\n    fraction: float, optional\n            Fraction of the width which you wish the figure to occupy\n\n    Returns\n    -------\n    fig_dim: tuple\n            Dimensions of figure in inches\n    \"\"\"", "\n", "# Width of figure (in pts)", "\n", "fig_width_pt", "=", "width", "*", "fraction", "\n", "\n", "# Convert from pt to inches", "\n", "inches_per_pt", "=", "1", "/", "72.27", "\n", "\n", "# Golden ratio to set aesthetic figure height", "\n", "# https://disq.us/p/2940ij3", "\n", "golden_ratio", "=", "(", "5", "**", "0.5", "-", "1", ")", "/", "2", "\n", "\n", "# Figure width in inches", "\n", "fig_width_in", "=", "fig_width_pt", "*", "inches_per_pt", "\n", "# Figure height in inches", "\n", "# fig_height_in = fig_width_in * golden_ratio", "\n", "fig_height_in", "=", "fig_width_in", "*", "golden_ratio", "*", "(", "subplots", "[", "0", "]", "/", "subplots", "[", "1", "]", ")", "\n", "\n", "fig_dim", "=", "(", "fig_width_in", ",", "fig_height_in", ")", "\n", "\n", "return", "fig_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.prime_factors": [[236, 255], ["factors.append", "factors.append"], "function", ["None"], ["", "def", "prime_factors", "(", "n", ":", "int", ")", "->", "list", ":", "\n", "    ", "\"\"\"Compute the decomposition of an integer number into prime factors.\n\n    Parameters\n    ----------\n    n: int\n        Integer number to decompose into its prime factors\n    \"\"\"", "\n", "i", "=", "2", "\n", "factors", "=", "[", "]", "\n", "while", "i", "*", "i", "<=", "n", ":", "\n", "        ", "if", "n", "%", "i", ":", "\n", "            ", "i", "+=", "1", "\n", "", "else", ":", "\n", "            ", "n", "//=", "i", "\n", "factors", ".", "append", "(", "i", ")", "\n", "", "", "if", "n", ">", "1", ":", "\n", "        ", "factors", ".", "append", "(", "n", ")", "\n", "", "return", "factors", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.chunks": [[257, 274], ["range", "len"], "function", ["None"], ["", "def", "chunks", "(", "lst", ":", "list", ",", "n", ":", "int", ")", "->", "list", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from lst.\n\n    Parameters\n    ----------\n    lst: list\n            List to split into chunks\n    n: int, optional\n            Number of chunks for partitioning the list\n\n    Returns\n    -------\n    fig_dim: tuple\n            Dimensions of figure in inches\n    \"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "lst", ")", ",", "n", ")", ":", "\n", "        ", "yield", "lst", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save": [[276, 289], ["open", "file.write", "os.path.join", "ruamel.dump"], "function", ["None"], ["", "", "def", "save", "(", "config", ",", "path", ")", ":", "\n", "    ", "\"\"\"Takes the config and a specified path where to save it. This allows\n    to save the config file in the folde rof the experiment to enhance reproducibility.\n\n    Parameters\n    ----------\n    config: dict\n        The dictionary to store\n    path: str\n\n    \"\"\"", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"paramMultiTestOOS.yaml\"", ")", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "yaml", ".", "dump", "(", "config", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save_gin": [[291, 294], ["open", "f.write", "gin.operative_config_str"], "function", ["None"], ["", "", "def", "save_gin", "(", "destination", ":", "Path", ")", ":", "\n", "    ", "with", "open", "(", "destination", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "gin", ".", "operative_config_str", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ActionSpace.__init__": [[28, 54], ["isinstance", "gym.spaces.space.Space.__init__", "isinstance", "numpy.array", "numpy.round", "numpy.round", "numpy.linspace", "numpy.linspace"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.__init__"], ["def", "__init__", "(", "\n", "self", ",", "action_range", ":", "list", ",", "zero_action", ":", "bool", "=", "True", ",", "side_only", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "if", "not", "side_only", ":", "\n", "            ", "if", "isinstance", "(", "action_range", "[", "0", "]", ",", "list", ")", ":", "\n", "                ", "self", ".", "values", "=", "np", ".", "round", "(", "\n", "np", ".", "linspace", "(", "-", "action_range", "[", "0", "]", "[", "0", "]", ",", "action_range", "[", "0", "]", "[", "1", "]", ",", "action_range", "[", "1", "]", ")", ",", "2", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "values", "=", "np", ".", "round", "(", "\n", "np", ".", "linspace", "(", "-", "action_range", "[", "0", "]", ",", "action_range", "[", "0", "]", ",", "action_range", "[", "1", "]", ")", ",", "2", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "values", "=", "np", ".", "array", "(", "[", "-", "1.0", ",", "0.0", ",", "1.0", "]", ")", "\n", "", "if", "not", "zero_action", ":", "\n", "            ", "self", ".", "values", "=", "self", ".", "values", "[", "self", ".", "values", "!=", "0.0", "]", "\n", "\n", "", "if", "isinstance", "(", "action_range", "[", "0", "]", ",", "list", ")", ":", "\n", "            ", "self", ".", "action_range", "=", "action_range", "[", "0", "]", "\n", "self", ".", "asymmetric", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "action_range", "=", "action_range", "\n", "self", ".", "asymmetric", "=", "False", "\n", "", "self", ".", "zero_action", "=", "zero_action", "\n", "self", ".", "side_only", "=", "side_only", "\n", "super", "(", ")", ".", "__init__", "(", "self", ".", "values", ".", "shape", ",", "self", ".", "values", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ActionSpace.contains": [[55, 57], ["None"], "methods", ["None"], ["", "def", "contains", "(", "self", ",", "x", ":", "int", ")", "->", "bool", ":", "\n", "        ", "return", "x", "in", "self", ".", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ActionSpace.get_n_actions": [[58, 65], ["None"], "methods", ["None"], ["", "def", "get_n_actions", "(", "self", ",", "policy_type", ":", "str", ")", ":", "\n", "# TODO note that this implementation is valid only for a single action.", "\n", "# If we want to do more than one action we should change it", "\n", "        ", "if", "policy_type", "==", "\"continuous\"", ":", "\n", "            ", "return", "self", ".", "values", ".", "ndim", "\n", "", "elif", "policy_type", "==", "\"discrete\"", ":", "\n", "            ", "return", "self", ".", "values", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ResActionSpace.__init__": [[86, 100], ["numpy.round", "gym.spaces.space.Space.__init__", "numpy.linspace"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.__init__"], ["def", "__init__", "(", "\n", "self", ",", "action_range", ":", "list", ",", "zero_action", ":", "bool", "=", "True", ",", "side_only", ":", "bool", "=", "False", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "values", "=", "np", ".", "round", "(", "\n", "np", ".", "linspace", "(", "action_range", "[", "0", "]", ",", "action_range", "[", "1", "]", ",", "action_range", "[", "2", "]", ")", ",", "2", "\n", ")", "\n", "if", "not", "zero_action", ":", "\n", "            ", "self", ".", "values", "=", "self", ".", "values", "[", "self", ".", "values", "!=", "0.0", "]", "\n", "\n", "", "self", ".", "action_range", "=", "action_range", "\n", "self", ".", "zero_action", "=", "zero_action", "\n", "self", ".", "side_only", "=", "side_only", "\n", "super", "(", ")", ".", "__init__", "(", "self", ".", "values", ".", "shape", ",", "self", ".", "values", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ResActionSpace.contains": [[101, 103], ["None"], "methods", ["None"], ["", "def", "contains", "(", "self", ",", "x", ":", "int", ")", "->", "bool", ":", "\n", "        ", "return", "x", "in", "self", ".", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ResActionSpace.get_n_actions": [[104, 111], ["None"], "methods", ["None"], ["", "def", "get_n_actions", "(", "self", ",", "policy_type", ":", "str", ")", ":", "\n", "# TODO note that this implementation is valid only for a single action.", "\n", "# If we want to do more than one action we should change it", "\n", "        ", "if", "policy_type", "==", "\"continuous\"", ":", "\n", "            ", "return", "self", ".", "values", ".", "ndim", "\n", "", "elif", "policy_type", "==", "\"discrete\"", ":", "\n", "            ", "return", "self", ".", "values", ".", "size", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.__init__": [[38, 50], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "datatype", ":", "str", ",", "\n", "N_train", ":", "int", ",", "\n", "rng", ":", "object", ",", "\n", "factor_lb", ":", "Union", "[", "list", ",", "None", "]", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "datatype", "=", "datatype", "\n", "self", ".", "N_train", "=", "N_train", "\n", "self", ".", "rng", "=", "rng", "\n", "self", ".", "factor_lb", "=", "factor_lb", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns": [[51, 72], ["simulation.alpha_term_structure_sampler", "simulation.return_sampler_garch", "print", "sys.exit", "simulation.return_sampler_GP", "simulation.return_sampler_GP"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.alpha_term_structure_sampler", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.return_sampler_garch", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.return_sampler_GP", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.return_sampler_GP"], ["", "def", "generate_returns", "(", "self", ",", "disable_tqdm", ":", "bool", "=", "False", ")", ":", "\n", "\n", "        ", "if", "self", ".", "datatype", "!=", "\"garch\"", ":", "\n", "            ", "if", "self", ".", "datatype", "==", "\"alpha_term_structure\"", ":", "\n", "                ", "self", ".", "returns", ",", "self", ".", "factors", ",", "self", ".", "f_speed", "=", "alpha_term_structure_sampler", "(", "N_train", "=", "self", ".", "N_train", ",", "rng", "=", "self", ".", "rng", ")", "\n", "", "elif", "self", ".", "datatype", "==", "\"t_stud_mfit\"", "or", "self", ".", "datatype", "==", "\"t_stud\"", ":", "\n", "                ", "self", ".", "returns", ",", "self", ".", "factors", ",", "self", ".", "f_speed", "=", "return_sampler_GP", "(", "N_train", "=", "self", ".", "N_train", "+", "self", ".", "factor_lb", "[", "-", "1", "]", ",", "rng", "=", "self", ".", "rng", ",", "disable_tqdm", "=", "disable_tqdm", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "returns", ",", "self", ".", "factors", ",", "self", ".", "f_speed", "=", "return_sampler_GP", "(", "\n", "N_train", "=", "self", ".", "N_train", ",", "rng", "=", "self", ".", "rng", ",", "disable_tqdm", "=", "disable_tqdm", "\n", ")", "\n", "\n", "", "", "elif", "self", ".", "datatype", "==", "\"garch\"", ":", "\n", "            ", "self", ".", "returns", ",", "self", ".", "params", "=", "return_sampler_garch", "(", "\n", "N_train", "=", "self", ".", "N_train", "+", "self", ".", "factor_lb", "[", "-", "1", "]", "+", "2", ",", "disable_tqdm", "=", "disable_tqdm", ",", "rng", "=", "self", ".", "rng", "\n", ")", "\n", "\n", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Datatype to simulate is not correct\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.estimate_parameters": [[73, 97], ["numpy.abs", "pandas.DataFrame", "utils.tools.RunModels", "utils.tools.CalculateLaggedSharpeRatio", "utils.tools.RunModels", "numpy.array().ravel", "numpy.concatenate", "numpy.array", "simulation.DataHandler.returns.reshape", "params_meanrev.values"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.RunModels", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.CalculateLaggedSharpeRatio", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.RunModels"], ["", "", "def", "estimate_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "datatype", "==", "\"t_stud_mfit\"", ":", "\n", "            ", "df", "=", "pd", ".", "DataFrame", "(", "\n", "data", "=", "np", ".", "concatenate", "(", "[", "self", ".", "returns", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "self", ".", "factors", "]", ",", "axis", "=", "1", ")", "\n", ")", "\n", "\n", "y", ",", "X", "=", "df", "[", "df", ".", "columns", "[", "0", "]", "]", ".", "loc", "[", "self", ".", "factor_lb", "[", "-", "1", "]", ":", "]", ",", "df", "[", "df", ".", "columns", "[", "1", ":", "]", "]", ".", "loc", "[", "self", ".", "factor_lb", "[", "-", "1", "]", ":", "]", "\n", "\n", "params_meanrev", ",", "_", "=", "RunModels", "(", "y", ",", "X", ",", "mr_only", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "df", "=", "CalculateLaggedSharpeRatio", "(", "\n", "self", ".", "returns", ",", "self", ".", "factor_lb", ",", "nameTag", "=", "self", ".", "datatype", ",", "seriestype", "=", "\"return\"", "\n", ")", "\n", "\n", "y", ",", "X", "=", "df", "[", "df", ".", "columns", "[", "0", "]", "]", ",", "df", "[", "df", ".", "columns", "[", "1", ":", "]", "]", "\n", "\n", "params_retmodel", ",", "params_meanrev", ",", "_", ",", "_", "=", "RunModels", "(", "\n", "y", ",", "X", "\n", ")", "\n", "\n", "", "self", ".", "f_speed", "=", "np", ".", "abs", "(", "np", ".", "array", "(", "[", "*", "params_meanrev", ".", "values", "(", ")", "]", ")", ".", "ravel", "(", ")", ")", "\n", "self", ".", "returns", "=", "df", ".", "iloc", "[", ":", ",", "0", "]", ".", "values", "\n", "self", ".", "factors", "=", "df", ".", "iloc", "[", ":", ",", "1", ":", "]", ".", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.return_sampler_GP": [[99, 275], ["gin.configurable", "numpy.around", "numpy.zeros", "numpy.vstack", "tqdm.tqdm", "realret.astype", "np.vstack.astype", "numpy.log", "f.append", "arch.univariate.GARCH", "tqdm.tqdm", "print", "sys.exit", "rng.standard_t", "rng.randn", "numpy.sum", "arch.univariate.GARCH", "numpy.array", "print", "sys.exit", "len", "rng.standard_t", "rng.standard_t", "rng.randn", "rng.randn", "range", "numpy.multiply", "numpy.multiply", "len", "range", "numpy.concatenate", "numpy.array", "f.append", "arch.univariate.GARCH.simulate", "numpy.sum", "len", "len", "numpy.array", "np.concatenate.append", "arch.univariate.GARCH.simulate", "range", "numpy.multiply", "len", "numpy.array", "numpy.sqrt", "arch.univariate.GARCH.simulate", "e.reshape", "numpy.sqrt"], "function", ["None"], ["", "", "@", "gin", ".", "configurable", "(", ")", "\n", "def", "return_sampler_GP", "(", "\n", "N_train", ":", "int", ",", "\n", "sigmaf", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "f_param", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "sigma", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "HalfLife", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "rng", ":", "np", ".", "random", ".", "mtrand", ".", "RandomState", "=", "None", ",", "\n", "offset", ":", "int", "=", "2", ",", "\n", "uncorrelated", ":", "bool", "=", "False", ",", "\n", "t_stud", ":", "bool", "=", "False", ",", "\n", "degrees", ":", "int", "=", "8", ",", "\n", "vol", ":", "str", "=", "\"omosk\"", ",", "\n", "dt", ":", "int", "=", "1", ",", "\n", "disable_tqdm", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "\n", "Union", "[", "list", ",", "np", ".", "ndarray", "]", ",", "Union", "[", "list", ",", "np", ".", "ndarray", "]", ",", "Union", "[", "list", ",", "np", ".", "ndarray", "]", "\n", "]", ":", "\n", "    ", "\"\"\"\n    Generates financial returns driven by mean-reverting factors.\n\n    Parameters\n    ----------\n    N_train : int\n        Length of the experiment\n\n    sigmaf : Union[float or list or np.ndarray]\n        Volatilities of the mean reverting factors\n\n    f_param: Union[float or list or np.ndarray]\n        Factor loadings of the mean reverting factors\n\n    sigma: Union[float or list or np.ndarray]\n        volatility of the asset return (additional noise other than the intrinsic noise\n                                        in the factors)\n\n    HalfLife: Union[int or list or np.ndarray]\n        HalfLife of mean reversion to simulate factors with different speeds\n\n    rng: np.random.mtrand.RandomState\n        Random number generator for reproducibility\n\n    offset: int = 2\n        Amount of additional observation to simulate\n\n    uncorrelated: bool = False\n        Boolean to regulate if the simulated factor are correlated or not\n\n    t_stud : bool = False\n        Bool to regulate if Student\\'s t noises are needed\n\n    degrees : int = 8\n        Degrees of freedom for Student\\'s t noises\n\n    vol: str = 'omosk'\n        Choose between 'omosk' and 'eterosk' for the kind of volatility\n    Returns\n    -------\n    realret: Union[list or np.ndarray]\n        Simulated series of returns\n    factors: Union[list or np.ndarray]\n        Simulated series of factors\n    f_speed: Union[list or np.ndarray]\n        Speed of mean reversion computed form HalfLife argument\n    \"\"\"", "\n", "\n", "# use samplesize +2 because when iterating the algorithm is necessary to", "\n", "# have one observation more (the last space representation) and because", "\n", "# we want be able to plot insample operation every tousand observation.", "\n", "# Therefore we don't want the index ending at 999 instead of 1000", "\n", "\n", "# Generate stochastic factor component and compute speed of mean reversion", "\n", "# simulate the single factor according to OU process", "\n", "# select proper speed of mean reversion and initialization point", "\n", "# it is faster to increase the size of a python list than a numpy array", "\n", "# therefore we convert later the list", "\n", "# https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html#:~:text=The%20shape%20of%20the%20t,%E2%80%9D%20than%20the%20z%2Ddistribution.", "\n", "\n", "lambdas", "=", "np", ".", "around", "(", "np", ".", "log", "(", "2", ")", "/", "HalfLife", ",", "4", ")", "\n", "\n", "f0", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "lambdas", ")", ",", ")", ")", "\n", "\n", "if", "vol", "==", "\"omosk\"", ":", "\n", "        ", "if", "t_stud", ":", "\n", "            ", "if", "uncorrelated", ":", "\n", "                ", "eps", "=", "rng", ".", "standard_t", "(", "degrees", ",", "(", "N_train", "+", "offset", ",", "len", "(", "HalfLife", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "eps", "=", "rng", ".", "standard_t", "(", "degrees", ",", "(", "N_train", "+", "offset", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "uncorrelated", ":", "\n", "                ", "eps", "=", "rng", ".", "randn", "(", "N_train", "+", "offset", ",", "len", "(", "HalfLife", ")", ")", "\n", "", "else", ":", "\n", "                ", "eps", "=", "rng", ".", "randn", "(", "N_train", "+", "offset", ")", "\n", "\n", "", "", "f", "=", "[", "]", "\n", "\n", "# possibility of triple noise", "\n", "for", "i", "in", "tqdm", "(", "\n", "iterable", "=", "range", "(", "N_train", "+", "offset", ")", ",", "\n", "desc", "=", "\"Simulating Factors\"", ",", "\n", "disable", "=", "disable_tqdm", ",", "\n", ")", ":", "\n", "# multiply makes the hadamard (componentwise) product", "\n", "# if we want to add different volatility for different factors we could", "\n", "# add multiply also the the second part of the equation", "\n", "            ", "f1", "=", "np", ".", "multiply", "(", "(", "1", "-", "lambdas", "*", "dt", ")", ",", "f0", ")", "+", "np", ".", "multiply", "(", "np", ".", "array", "(", "sigmaf", ")", "*", "np", ".", "sqrt", "(", "dt", ")", ",", "eps", "[", "i", "]", ")", "\n", "f", ".", "append", "(", "f1", ")", "\n", "f0", "=", "f1", "\n", "\n", "", "", "elif", "vol", "==", "\"heterosk\"", ":", "\n", "        ", "volmodel", "=", "GARCH", "(", "p", "=", "1", ",", "q", "=", "1", ")", "\n", "# these factors, if multiple, are uncorrelated by default because the noise is constructed one by one", "\n", "if", "len", "(", "sigmaf", ")", ">", "1", ":", "\n", "\n", "            ", "eps", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sigmaf", ")", ")", ":", "\n", "                ", "om", "=", "sigmaf", "[", "i", "]", "**", "2", "# same vol as original GP experiments", "\n", "alph", "=", "0.05", "\n", "b", "=", "1", "-", "alph", "-", "om", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "b", "]", ")", "\n", "\n", "e", "=", "volmodel", ".", "simulate", "(", "garch_p", ",", "N_train", "+", "offset", ",", "rng", ".", "randn", ")", "[", "0", "]", "\n", "eps", ".", "append", "(", "e", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", "\n", "\n", "", "eps", "=", "np", ".", "concatenate", "(", "eps", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "\n", "            ", "om", "=", "sigmaf", "[", "0", "]", "**", "2", "# same vol as original GP experiments", "\n", "alph", "=", "0.05", "\n", "b", "=", "1", "-", "alph", "-", "om", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "b", "]", ")", "\n", "\n", "eps", "=", "volmodel", ".", "simulate", "(", "garch_p", ",", "N_train", "+", "offset", ",", "rng", ".", "randn", ")", "[", "0", "]", "\n", "\n", "", "f", "=", "[", "]", "\n", "# possibility of triple noise", "\n", "for", "i", "in", "tqdm", "(", "\n", "iterable", "=", "range", "(", "N_train", "+", "offset", ")", ",", "\n", "desc", "=", "\"Simulating Factors\"", ",", "\n", "disable", "=", "disable_tqdm", ",", "\n", ")", ":", "\n", "# multiply makes the hadamard (componentwise) product", "\n", "# if we want to add different volatility for different factors we could", "\n", "# add multiply also the the second part of the equation", "\n", "            ", "f1", "=", "np", ".", "multiply", "(", "(", "1", "-", "lambdas", "*", "dt", ")", ",", "f0", ")", "+", "eps", "[", "i", "]", "*", "np", ".", "sqrt", "(", "dt", ")", "\n", "f", ".", "append", "(", "f1", ")", "\n", "f0", "=", "f1", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper volatility setting\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "factors", "=", "np", ".", "vstack", "(", "f", ")", "\n", "if", "vol", "==", "\"omosk\"", ":", "\n", "        ", "if", "t_stud", ":", "\n", "            ", "u", "=", "rng", ".", "standard_t", "(", "degrees", ",", "N_train", "+", "offset", ")", "\n", "", "else", ":", "\n", "            ", "u", "=", "rng", ".", "randn", "(", "N_train", "+", "offset", ")", "\n", "\n", "", "realret", "=", "np", ".", "sum", "(", "f_param", "*", "factors", ",", "axis", "=", "1", ")", "+", "sigma", "*", "u", "\n", "\n", "", "elif", "vol", "==", "\"heterosk\"", ":", "\n", "        ", "volmodel", "=", "GARCH", "(", "p", "=", "1", ",", "q", "=", "1", ")", "\n", "om", "=", "sigma", "**", "2", "# same vol as original GP experiments", "\n", "alph", "=", "0.05", "\n", "b", "=", "1", "-", "alph", "-", "om", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "b", "]", ")", "\n", "\n", "u", "=", "volmodel", ".", "simulate", "(", "garch_p", ",", "N_train", "+", "offset", ",", "rng", ".", "randn", ")", "[", "0", "]", "\n", "\n", "realret", "=", "np", ".", "sum", "(", "f_param", "*", "factors", ",", "axis", "=", "1", ")", "+", "sigma", "*", "u", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper volatility setting\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "f_speed", "=", "lambdas", "\n", "\n", "return", "realret", ".", "astype", "(", "np", ".", "float32", ")", ",", "factors", ".", "astype", "(", "np", ".", "float32", ")", ",", "f_speed", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.return_sampler_garch": [[277, 504], ["gin.configurable", "pandas.Series", "numpy.random.RandomState", "arch.univariate.ConstantMean", "names.append", "arch.univariate.GARCH", "names.extend", "vals.extend", "arch.univariate.Normal", "arch.univariate.ARX.simulate", "vals.append", "vals.append", "arch.univariate.ARX", "names.append", "vals.append", "print", "sys.exit", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "numpy.array", "list", "arch.univariate.GARCH", "names.extend", "numpy.array", "vals.extend", "numpy.random.RandomState", "arch.univariate.StudentsT", "names.append", "np.random.RandomState.uniform", "range", "range", "numpy.array", "numpy.array().sum", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "list", "arch.univariate.HARCH", "names.extend", "numpy.array", "vals.extend", "numpy.random.RandomState", "vals.append", "vals.append", "arch.univariate.SkewStudent", "names.extend", "names.append", "vals.append", "names.append", "vals.append", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "list", "arch.univariate.FIGARCH", "names.extend", "numpy.array", "vals.extend", "np.random.RandomState.randint", "numpy.random.RandomState", "vals.extend", "vals.extend", "arch.univariate.GeneralizedError", "names.append", "print", "sys.exit", "np.random.RandomState.uniform", "numpy.array", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "list", "arch.univariate.GARCH", "names.extend", "numpy.array", "vals.extend", "numpy.random.RandomState", "vals.append", "vals.append", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "list", "arch.univariate.EGARCH", "names.extend", "vals.extend", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "np.random.RandomState.uniform", "numpy.array", "list", "arch.univariate.ConstantVariance", "names.append", "vals.append", "print", "sys.exit", "numpy.array", "numpy.array().sum", "np.random.RandomState.uniform", "numpy.array"], "function", ["None"], ["", "@", "gin", ".", "configurable", "(", ")", "\n", "def", "return_sampler_garch", "(", "\n", "N_train", ":", "int", ",", "\n", "mean_process", ":", "str", "=", "\"Constant\"", ",", "\n", "lags_mean_process", ":", "int", "=", "None", ",", "\n", "vol_process", ":", "str", "=", "\"GARCH\"", ",", "\n", "distr_noise", ":", "str", "=", "\"normal\"", ",", "\n", "seed", ":", "int", "=", "None", ",", "\n", "p_arg", ":", "list", "=", "None", ",", "\n", "disable_tqdm", ":", "bool", "=", "False", ",", "\n", "rng", ":", "np", ".", "random", ".", "mtrand", ".", "RandomState", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "pd", ".", "Series", "]", ":", "\n", "# https://stats.stackexchange.com/questions/61824/how-to-interpret-garch-parameters", "\n", "# https://arch.readthedocs.io/en/latest/univariate/introduction.html", "\n", "# https://arch.readthedocs.io/en/latest/univariate/volatility.html", "\n", "# https://github.com/bashtage/arch/blob/master/arch/univariate/volatility.py", "\n", "    ", "\"\"\"\n    Generates financial returns driven by mean-reverting factors.\n\n    Parameters\n    ----------\n    N_train: int\n        Length of the experiment\n\n    mean_process: str\n        Mean process for the returns. It can be 'Constant' or 'AR'\n\n    lags_mean_process: int\n        Order of autoregressive lag if mean_process is AR\n\n    vol_process: str\n        Volatility process for the returns. It can be 'GARCH', 'EGARCH', 'TGARCH',\n        'ARCH', 'HARCH', 'FIGARCH' or 'Constant'. Note that different volatility\n        processes requires different parameter, which are hard coded. If you want to\n        pass them explicitly, use p_arg.\n\n    distr_noise: str\n        Distribution for the unpredictable component of the returns. It can be\n        'normal', 'studt', 'skewstud' or 'ged'. Note that different distributions\n        requires different parameter, which are hard coded. If you want to\n        pass them explicitly, use p_arg.\n\n    seed: int\n        Seed for experiment reproducibility\n    p_arg: pd.Series\n        Pandas series of parameters that you want to pass explicitly.\n        They need to be passed in the right order. Check documentation of the\n        arch python package (https://arch.readthedocs.io/en/latest/index.html) for more details.\n    Returns\n    -------\n    simulations['data'].values: np.ndarray\n        Simulated series of returns\n    p: pd.Series\n        Series  of parameters used for simulation\n    \"\"\"", "\n", "names", "=", "[", "]", "\n", "vals", "=", "[", "]", "\n", "\n", "if", "not", "rng", ":", "\n", "        ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "None", ")", "\n", "\n", "# choose mean process", "\n", "", "if", "mean_process", "==", "\"Constant\"", ":", "\n", "        ", "model", "=", "ConstantMean", "(", "None", ")", "\n", "names", ".", "append", "(", "\"const\"", ")", "\n", "if", "rng", ":", "\n", "            ", "vals", ".", "append", "(", "rng", ".", "uniform", "(", "0.01", ",", "0.09", ")", ")", "\n", "", "else", ":", "\n", "            ", "vals", ".", "append", "(", "0.0", ")", "\n", "\n", "", "", "elif", "mean_process", "==", "\"AR\"", ":", "\n", "        ", "model", "=", "ARX", "(", "None", ",", "lags", "=", "lags_mean_process", ")", "\n", "names", ".", "append", "(", "\"const\"", ")", "\n", "vals", ".", "append", "(", "0.0", ")", "\n", "if", "rng", ":", "\n", "            ", "for", "i", "in", "range", "(", "lags_mean_process", ")", ":", "\n", "                ", "names", ".", "append", "(", "\"lag{}\"", ".", "format", "(", "i", ")", ")", "\n", "vals", ".", "append", "(", "rng", ".", "uniform", "(", "-", "0.09", ",", "0.09", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "lags_mean_process", ")", ":", "\n", "                ", "names", ".", "append", "(", "\"lag{}\"", ".", "format", "(", "i", ")", ")", "\n", "vals", ".", "append", "(", "0.9", ")", "\n", "\n", "", "", "", "else", ":", "\n", "        ", "return", "print", "(", "\"This mean process doesn't exist or it's not available.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "# choose volatility process", "\n", "", "if", "vol_process", "==", "\"GARCH\"", ":", "\n", "        ", "model", ".", "volatility", "=", "GARCH", "(", "p", "=", "1", ",", "q", "=", "1", ")", "\n", "names", ".", "extend", "(", "[", "\"omega\"", ",", "\"alpha\"", ",", "\"beta\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "om", "=", "rng", ".", "uniform", "(", "0.03", ",", "0.1", ")", "\n", "alph", "=", "rng", ".", "uniform", "(", "0.05", ",", "0.1", ")", "\n", "b", "=", "rng", ".", "uniform", "(", "0.86", ",", "0.92", ")", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "b", "]", ")", "/", "(", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "b", "]", ")", ".", "sum", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "om", "=", "0.01", "\n", "alph", "=", "0.05", "\n", "b", "=", "0.94", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "b", "]", ")", "\n", "", "vals", ".", "extend", "(", "list", "(", "garch_p", ")", ")", "\n", "\n", "", "elif", "vol_process", "==", "\"ARCH\"", ":", "\n", "        ", "model", ".", "volatility", "=", "GARCH", "(", "p", "=", "1", ",", "q", "=", "0", ")", "\n", "\n", "names", ".", "extend", "(", "[", "\"omega\"", ",", "\"alpha\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "om", "=", "rng", ".", "uniform", "(", "1.4", ",", "4.0", ")", "\n", "alph", "=", "rng", ".", "uniform", "(", "0.1", ",", "0.6", ")", "\n", "", "else", ":", "\n", "            ", "om", "=", "0.01", "\n", "alph", "=", "0.4", "\n", "", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", "]", ")", "\n", "vals", ".", "extend", "(", "list", "(", "garch_p", ")", ")", "\n", "\n", "", "elif", "vol_process", "==", "\"HARCH\"", ":", "\n", "        ", "model", ".", "volatility", "=", "HARCH", "(", "lags", "=", "[", "1", ",", "5", ",", "22", "]", ")", "\n", "\n", "names", ".", "extend", "(", "[", "\"omega\"", ",", "\"alpha[1]\"", ",", "\"alpha[5]\"", ",", "\"alpha[22]\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "om", "=", "rng", ".", "uniform", "(", "1.2", ",", "0.5", ")", "\n", "alph1", "=", "rng", ".", "uniform", "(", "0.01", ",", "0.1", ")", "\n", "alph5", "=", "rng", ".", "uniform", "(", "0.05", ",", "0.3", ")", "\n", "alph22", "=", "rng", ".", "uniform", "(", "0.4", ",", "0.7", ")", "\n", "", "else", ":", "\n", "            ", "om", "=", "0.01", "\n", "alph1", "=", "0.05", "\n", "alph5", "=", "0.15", "\n", "alph22", "=", "0.5", "\n", "", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph1", ",", "alph5", ",", "alph22", "]", ")", "\n", "vals", ".", "extend", "(", "list", "(", "garch_p", ")", ")", "\n", "\n", "", "elif", "vol_process", "==", "\"FIGARCH\"", ":", "\n", "        ", "model", ".", "volatility", "=", "FIGARCH", "(", "p", "=", "1", ",", "q", "=", "1", ")", "\n", "\n", "names", ".", "extend", "(", "[", "\"omega\"", ",", "\"phi\"", ",", "\"d\"", ",", "\"beta\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "om", "=", "rng", ".", "uniform", "(", "0.05", ",", "0.03", ")", "\n", "phi", "=", "rng", ".", "uniform", "(", "0.1", ",", "0.35", ")", "\n", "d", "=", "rng", ".", "uniform", "(", "0.3", ",", "0.5", ")", "\n", "beta", "=", "rng", ".", "uniform", "(", "0.4", ",", "0.7", ")", "\n", "", "else", ":", "\n", "            ", "om", "=", "0.01", "\n", "phi", "=", "0.2", "\n", "d", "=", "0.2", "\n", "beta", "=", "0.55", "\n", "", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "phi", ",", "d", ",", "beta", "]", ")", "\n", "vals", ".", "extend", "(", "list", "(", "garch_p", ")", ")", "\n", "\n", "", "elif", "vol_process", "==", "\"TGARCH\"", ":", "\n", "        ", "model", ".", "volatility", "=", "GARCH", "(", "p", "=", "1", ",", "o", "=", "1", ",", "q", "=", "1", ")", "\n", "names", ".", "extend", "(", "[", "\"omega\"", ",", "\"alpha\"", ",", "\"gamma\"", ",", "\"beta\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "om", "=", "rng", ".", "uniform", "(", "0.02", ",", "0.15", ")", "\n", "alph", "=", "rng", ".", "uniform", "(", "0.01", ",", "0.07", ")", "\n", "gamma", "=", "rng", ".", "uniform", "(", "0.03", ",", "0.1", ")", "\n", "b", "=", "rng", ".", "uniform", "(", "0.88", ",", "0.94", ")", "\n", "", "else", ":", "\n", "            ", "om", "=", "0.01", "\n", "alph", "=", "0.05", "\n", "gamma", "=", "0.04", "\n", "b", "=", "0.90", "\n", "", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "gamma", ",", "b", "]", ")", "\n", "vals", ".", "extend", "(", "list", "(", "garch_p", ")", ")", "\n", "\n", "", "elif", "vol_process", "==", "\"EGARCH\"", ":", "\n", "        ", "model", ".", "volatility", "=", "EGARCH", "(", "p", "=", "1", ",", "o", "=", "1", ",", "q", "=", "1", ")", "\n", "names", ".", "extend", "(", "[", "\"omega\"", ",", "\"alpha\"", ",", "\"gamma\"", ",", "\"beta\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "om", "=", "rng", ".", "uniform", "(", "0.01", ",", "0.03", ")", "\n", "alph", "=", "rng", ".", "uniform", "(", "0.06", ",", "0.17", ")", "\n", "gamma", "=", "rng", ".", "uniform", "(", "-", "0.05", ",", "-", "0.02", ")", "\n", "b", "=", "rng", ".", "uniform", "(", "0.97", ",", "0.99", ")", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "gamma", ",", "b", "]", ")", "/", "(", "\n", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "gamma", ",", "b", "]", ")", ".", "sum", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "om", "=", "0.01", "\n", "alph", "=", "0.05", "\n", "gamma", "=", "-", "0.02", "\n", "b", "=", "0.94", "\n", "garch_p", "=", "np", ".", "array", "(", "[", "om", ",", "alph", ",", "gamma", ",", "b", "]", ")", "\n", "", "vals", ".", "extend", "(", "list", "(", "garch_p", ")", ")", "\n", "\n", "", "elif", "vol_process", "==", "\"Constant\"", ":", "\n", "        ", "model", ".", "volatility", "=", "ConstantVariance", "(", ")", "\n", "names", ".", "append", "(", "\"sigma_const\"", ")", "\n", "vals", ".", "append", "(", "rng", ".", "uniform", "(", "0.02", ",", "0.05", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"This volatility process doesn't exist or it's not available.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "distr_noise", "==", "\"normal\"", ":", "\n", "        ", "model", ".", "distribution", "=", "Normal", "(", "np", ".", "random", ".", "RandomState", "(", "seed", ")", ")", "\n", "", "elif", "distr_noise", "==", "\"studt\"", ":", "\n", "        ", "model", ".", "distribution", "=", "StudentsT", "(", "np", ".", "random", ".", "RandomState", "(", "seed", ")", ")", "\n", "names", ".", "append", "(", "\"nu\"", ")", "\n", "if", "rng", ":", "\n", "            ", "vals", ".", "append", "(", "rng", ".", "randint", "(", "6.0", ",", "10.0", ")", ")", "\n", "", "else", ":", "\n", "            ", "vals", ".", "append", "(", "8.0", ")", "\n", "", "", "elif", "distr_noise", "==", "\"skewstud\"", ":", "\n", "        ", "model", ".", "distribution", "=", "SkewStudent", "(", "np", ".", "random", ".", "RandomState", "(", "seed", ")", ")", "\n", "names", ".", "extend", "(", "[", "\"nu\"", ",", "\"lambda\"", "]", ")", "\n", "if", "rng", ":", "\n", "            ", "vals", ".", "extend", "(", "[", "rng", ".", "uniform", "(", "6.0", ",", "10.0", ")", ",", "rng", ".", "uniform", "(", "-", "0.1", ",", "0.1", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "vals", ".", "extend", "(", "[", "8.0", ",", "0.05", "]", ")", "\n", "", "", "elif", "distr_noise", "==", "\"ged\"", ":", "\n", "        ", "model", ".", "distribution", "=", "GeneralizedError", "(", "np", ".", "random", ".", "RandomState", "(", "seed", ")", ")", "\n", "names", ".", "append", "(", "\"nu\"", ")", "\n", "if", "rng", ":", "\n", "            ", "vals", ".", "append", "(", "rng", ".", "uniform", "(", "1.05", ",", "3.0", ")", ")", "\n", "", "else", ":", "\n", "            ", "vals", ".", "append", "(", "2.0", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"This noise distribution doesn't exist or it's not available.\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "p", "=", "pd", ".", "Series", "(", "data", "=", "vals", ",", "index", "=", "names", ")", "\n", "if", "p_arg", ":", "\n", "        ", "p", ".", "values", "[", "-", "3", ":", "]", "=", "p_arg", "\n", "\n", "", "simulations", "=", "model", ".", "simulate", "(", "p", ",", "N_train", ")", "/", "100", "\n", "\n", "return", "simulations", "[", "\"data\"", "]", ".", "values", ",", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.alpha_term_structure_sampler": [[506, 642], ["gin.configurable", "range", "numpy.transpose", "numpy.array", "numpy.array", "len", "numpy.arange().repeat().reshape", "numpy.sum", "len", "len", "numpy.arange().repeat().reshape", "numpy.sum", "term_structures.append", "speeds.append", "np.array.append", "numpy.array", "numpy.concatenate", "numpy.transpose", "matplotlib.subplots", "ax.plot", "ax.set_title", "matplotlib.show", "rng.choice", "numpy.log", "sum", "print", "sys.exit", "matplotlib.subplots", "ax.plot", "ax.plot", "ax.set_title", "rng.choice", "numpy.log", "sum", "print", "sys.exit", "numpy.squeeze", "numpy.array().reshape", "numpy.array().reshape", "numpy.arange().repeat", "rng.normal", "numpy.array", "numpy.array().reshape", "numpy.array().reshape", "numpy.arange().repeat", "rng.normal", "numpy.array", "numpy.array().reshape", "numpy.array().reshape", "numpy.array().reshape", "rng.normal", "numpy.array().reshape", "numpy.array().reshape", "numpy.array().reshape", "rng.normal", "numpy.array", "numpy.array", "numpy.arange", "numpy.array", "numpy.sqrt", "numpy.array", "numpy.array", "numpy.arange", "numpy.array", "numpy.sqrt", "numpy.array", "numpy.array", "numpy.array", "len", "numpy.array", "numpy.array", "numpy.array", "len", "rng.uniform", "rng.uniform", "len", "rng.uniform", "rng.uniform", "len", "rng.uniform", "int", "int", "rng.uniform", "rng.uniform", "rng.uniform", "int", "int", "rng.uniform", "rng.uniform", "int", "int"], "function", ["None"], ["", "@", "gin", ".", "configurable", "(", ")", "\n", "def", "alpha_term_structure_sampler", "(", "\n", "N_train", ":", "int", ",", "\n", "HalfLife", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "initial_alpha", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "f_param", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "sigma", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "sigmaf", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "rng", ":", "np", ".", "random", ".", "mtrand", ".", "RandomState", "=", "None", ",", "\n", "offset", ":", "int", "=", "2", ",", "\n", "generate_plot", ":", "bool", "=", "False", ",", "\n", "multiasset", ":", "bool", "=", "False", ",", "\n", "double_noise", ":", "bool", "=", "False", ",", "\n", "fixed_alpha", ":", "bool", "=", "False", ")", ":", "\n", "\n", "# pdb.set_trace()", "\n", "    ", "tmp_rng", "=", "rng", "\n", "if", "fixed_alpha", ":", "\n", "        ", "rng", "=", "None", "\n", "\n", "", "if", "multiasset", ":", "\n", "        ", "term_structures", "=", "[", "]", "\n", "alpha_factor_terms", "=", "[", "]", "\n", "speeds", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "HalfLife", ")", ")", ":", "\n", "            ", "init_a", "=", "initial_alpha", "[", "i", "]", "\n", "hl", "=", "HalfLife", "[", "i", "]", "\n", "fp", "=", "f_param", "[", "i", "]", "\n", "if", "rng", ":", "\n", "                ", "posneg", "=", "rng", ".", "choice", "(", "[", "0", ",", "1", "]", ")", "\n", "if", "posneg", "==", "0", ":", "\n", "                    ", "init_a", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "-", "val", "*", "(", "1", "+", "0.2", ")", ",", "-", "val", "*", "(", "1", "-", "0.2", ")", ",", "1", ")", "for", "val", "in", "init_a", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "elif", "posneg", "==", "1", ":", "\n", "                    ", "init_a", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "val", "*", "(", "1", "-", "0.2", ")", ",", "val", "*", "(", "1", "+", "0.2", ")", ",", "1", ")", "for", "val", "in", "init_a", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "if", "'truncate'", "in", "hl", ":", "\n", "                    ", "hl", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "int", "(", "N_train", "*", "0.85", ")", ",", "int", "(", "N_train", "*", "1.25", ")", ",", "1", ")", "for", "_", "in", "init_a", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "elif", "None", "in", "hl", ":", "\n", "                    ", "hl", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "5", ",", "int", "(", "N_train", "*", "0.75", ")", ",", "1", ")", "for", "_", "in", "init_a", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "else", ":", "\n", "                    ", "hl", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "val", "*", "(", "1", "-", "0.5", ")", ",", "val", "*", "(", "1", "+", "0.5", ")", ",", "1", ")", "for", "val", "in", "hl", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "\n", "", "", "alpha_n", "=", "len", "(", "hl", ")", "\n", "f_speed", "=", "np", ".", "log", "(", "2", ")", "/", "hl", "\n", "t", "=", "np", ".", "arange", "(", "0", ",", "N_train", "+", "offset", ")", ".", "repeat", "(", "alpha_n", ")", ".", "reshape", "(", "-", "1", ",", "alpha_n", ")", "\n", "\n", "if", "fixed_alpha", ":", "\n", "                ", "rng", "=", "tmp_rng", "\n", "", "if", "double_noise", ":", "\n", "                ", "alpha_terms", "=", "init_a", "*", "np", ".", "e", "**", "(", "-", "f_speed", "*", "t", ")", "+", "sigma", "*", "rng", ".", "normal", "(", "size", "=", "(", "len", "(", "t", ")", ",", "alpha_n", ")", ")", "\n", "", "else", ":", "\n", "                ", "alpha_terms", "=", "init_a", "*", "np", ".", "e", "**", "(", "-", "f_speed", "*", "t", ")", "\n", "", "if", "None", "not", "in", "sigmaf", ":", "\n", "# noise_magnitude = np.cumsum(sigmaf*t).reshape(-1,alpha_n)", "\n", "                ", "noise_magnitude", "=", "(", "np", ".", "array", "(", "sigmaf", ")", "*", "np", ".", "sqrt", "(", "t", ")", ")", ".", "reshape", "(", "-", "1", ",", "alpha_n", ")", "\n", "noise", "=", "noise_magnitude", "*", "rng", ".", "normal", "(", "size", "=", "(", "len", "(", "t", ")", ",", "alpha_n", ")", ")", "\n", "alpha_terms", "=", "alpha_terms", "+", "noise", "\n", "", "if", "fixed_alpha", ":", "\n", "                ", "tmp_rng", "=", "rng", "\n", "rng", "=", "None", "\n", "\n", "", "if", "sum", "(", "fp", ")", "!=", "1.0", ":", "\n", "                ", "print", "(", "'Factor loadings for term structure do not sum to one.'", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "alpha_structure", "=", "np", ".", "sum", "(", "np", ".", "array", "(", "fp", ")", "*", "alpha_terms", ",", "axis", "=", "1", ")", "\n", "\n", "term_structures", ".", "append", "(", "alpha_structure", ")", "\n", "speeds", ".", "append", "(", "f_speed", ")", "\n", "alpha_factor_terms", ".", "append", "(", "alpha_terms", ")", "\n", "\n", "", "alpha_structure", "=", "np", ".", "transpose", "(", "np", ".", "array", "(", "term_structures", ",", "dtype", "=", "'float'", ")", ")", "\n", "alpha_factor_terms", "=", "np", ".", "array", "(", "alpha_factor_terms", ",", "dtype", "=", "'float'", ")", "\n", "\n", "if", "alpha_factor_terms", ".", "shape", "[", "-", "1", "]", "!=", "1", ":", "\n", "            ", "alpha_terms", "=", "np", ".", "concatenate", "(", "(", "alpha_factor_terms", "[", "0", ",", ":", ",", ":", "]", ",", "alpha_factor_terms", "[", "1", ",", ":", ",", ":", "]", ")", ",", "axis", "=", "1", ")", "\n", "# alpha_terms = np.transpose(alpha_factor_terms.reshape(-1,alpha_factor_terms.shape[1]))", "\n", "", "else", ":", "\n", "            ", "alpha_terms", "=", "np", ".", "transpose", "(", "np", ".", "squeeze", "(", "alpha_factor_terms", ")", ")", "\n", "", "f_speed", "=", "np", ".", "array", "(", "speeds", ",", "dtype", "=", "'float'", ")", "\n", "\n", "# generate_plot=True", "\n", "if", "generate_plot", ":", "\n", "            ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "10", ",", "5", ")", ")", "\n", "ax", ".", "plot", "(", "alpha_structure", ")", "\n", "# ax.plot(alpha_terms.sum(axis=1), ls='--')", "\n", "ax", ".", "set_title", "(", "'Alpha term structure'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "return", "alpha_structure", ",", "alpha_terms", ",", "f_speed", "\n", "", "else", ":", "\n", "# single asset case where different alpha term structure can be combined in ", "\n", "# a unique prediction", "\n", "        ", "if", "rng", ":", "\n", "            ", "posneg", "=", "rng", ".", "choice", "(", "[", "0", ",", "1", "]", ")", "\n", "if", "posneg", "==", "0", ":", "\n", "                ", "initial_alpha", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "-", "val", "*", "(", "1", "+", "0.5", ")", ",", "-", "val", "*", "(", "1", "-", "0.5", ")", ",", "1", ")", "for", "val", "in", "initial_alpha", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "elif", "posneg", "==", "1", ":", "\n", "                ", "initial_alpha", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "val", "*", "(", "1", "-", "0.5", ")", ",", "val", "*", "(", "1", "+", "0.5", ")", ",", "1", ")", "for", "val", "in", "initial_alpha", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "\n", "", "if", "'truncate'", "in", "HalfLife", ":", "\n", "                ", "HalfLife", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "int", "(", "N_train", "*", "0.85", ")", ",", "int", "(", "N_train", "*", "1.25", ")", ",", "1", ")", "for", "_", "in", "initial_alpha", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "elif", "None", "in", "HalfLife", ":", "\n", "                ", "HalfLife", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "5", ",", "int", "(", "N_train", "*", "0.75", ")", ",", "1", ")", "for", "_", "in", "initial_alpha", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "else", ":", "\n", "                ", "HalfLife", "=", "np", ".", "array", "(", "[", "rng", ".", "uniform", "(", "val", "*", "(", "1", "-", "0.5", ")", ",", "val", "*", "(", "1", "+", "0.5", ")", ",", "1", ")", "for", "val", "in", "HalfLife", "]", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "\n", "", "", "alpha_n", "=", "len", "(", "HalfLife", ")", "\n", "f_speed", "=", "np", ".", "log", "(", "2", ")", "/", "HalfLife", "\n", "t", "=", "np", ".", "arange", "(", "0", ",", "N_train", "+", "offset", ")", ".", "repeat", "(", "alpha_n", ")", ".", "reshape", "(", "-", "1", ",", "alpha_n", ")", "\n", "if", "fixed_alpha", ":", "\n", "            ", "rng", "=", "tmp_rng", "\n", "", "if", "double_noise", ":", "\n", "            ", "alpha_terms", "=", "initial_alpha", "*", "np", ".", "e", "**", "(", "-", "f_speed", "*", "t", ")", "+", "sigma", "*", "rng", ".", "normal", "(", "size", "=", "(", "len", "(", "t", ")", ",", "alpha_n", ")", ")", "\n", "", "else", ":", "\n", "            ", "alpha_terms", "=", "initial_alpha", "*", "np", ".", "e", "**", "(", "-", "f_speed", "*", "t", ")", "\n", "\n", "", "if", "None", "not", "in", "sigmaf", ":", "\n", "            ", "noise_magnitude", "=", "(", "np", ".", "array", "(", "sigmaf", ")", "*", "np", ".", "sqrt", "(", "t", ")", ")", ".", "reshape", "(", "-", "1", ",", "alpha_n", ")", "\n", "# pdb.set_trace()", "\n", "noise", "=", "noise_magnitude", "*", "rng", ".", "normal", "(", "size", "=", "(", "len", "(", "t", ")", ",", "alpha_n", ")", ")", "\n", "alpha_terms", "=", "alpha_terms", "+", "noise", "\n", "\n", "", "if", "sum", "(", "f_param", ")", "!=", "1.0", ":", "\n", "            ", "print", "(", "'Factor loadings for term structure do not sum to one.'", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "alpha_structure", "=", "np", ".", "sum", "(", "np", ".", "array", "(", "f_param", ")", "*", "alpha_terms", ",", "axis", "=", "1", ")", "\n", "\n", "if", "generate_plot", ":", "\n", "            ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "10", ",", "5", ")", ")", "\n", "ax", ".", "plot", "(", "alpha_terms", ")", "\n", "ax", ".", "plot", "(", "alpha_structure", ",", "ls", "=", "'--'", ")", "\n", "# ax.plot(alpha_terms.sum(axis=1), ls='--')", "\n", "ax", ".", "set_title", "(", "'Alpha term structure'", ")", "\n", "# plt.show()", "\n", "\n", "", "return", "alpha_structure", ",", "alpha_terms", ",", "f_speed", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.parallel.get_parallelized_combinations": [[10, 40], ["len", "gin.query_parameter", "itertools.product", "gin.query_parameter", "variables.append", "zip", "variables.append", "itertools.product", "variables.append", "itertools.product", "print", "sys.exit", "numpy.random.randint", "variables.append", "gin.query_parameter", "len"], "function", ["None"], ["def", "get_parallelized_combinations", "(", "varying_type", ":", "str", ")", ":", "\n", "\n", "    ", "variables", "=", "[", "]", "\n", "lst", "=", "[", "gin", ".", "query_parameter", "(", "v", ")", "for", "v", "in", "gin", ".", "query_parameter", "(", "\"%VARYING_PARS\"", ")", "]", "\n", "if", "varying_type", "==", "\"combination\"", ":", "\n", "        ", "for", "xs", "in", "itertools", ".", "product", "(", "*", "lst", ")", ":", "\n", "            ", "variables", ".", "append", "(", "xs", ")", "\n", "", "", "elif", "varying_type", "==", "\"ordered_combination\"", ":", "\n", "        ", "for", "xs", "in", "zip", "(", "*", "lst", ")", ":", "\n", "            ", "variables", ".", "append", "(", "xs", ")", "\n", "", "", "elif", "varying_type", "==", "\"random_search\"", ":", "\n", "        ", "for", "xs", "in", "itertools", ".", "product", "(", "*", "lst", ")", ":", "\n", "            ", "variables", ".", "append", "(", "xs", ")", "\n", "", "variables", "=", "[", "\n", "variables", "[", "i", "]", "\n", "for", "i", "in", "np", ".", "random", ".", "randint", "(", "\n", "0", ",", "len", "(", "variables", ")", "-", "1", ",", "gin", ".", "query_parameter", "(", "\"%NUM_CORES\"", ")", "\n", ")", "\n", "]", "\n", "", "elif", "varying_type", "==", "\"chunk\"", ":", "\n", "        ", "for", "xs", "in", "itertools", ".", "product", "(", "*", "lst", ")", ":", "\n", "            ", "variables", ".", "append", "(", "xs", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Choose proper way to combine varying parameters\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "num_cores", "=", "len", "(", "variables", ")", "\n", "\n", "return", "variables", ",", "num_cores", "\n", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_DQNmodel": [[38, 114], ["query", "len", "agents.DQN.DeepNetworkModel", "agents.DQN.DeepNetworkModel.load_weights", "agents.DQN.DeepNetworkModel.load_weights", "query", "query", "query", "query", "query", "query", "query", "os.path.join", "os.path.join", "len", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "query", "query", "query", "query", "query", "query"], "function", ["None"], ["def", "load_DQNmodel", "(", "\n", "data_dir", ":", "str", ",", "ckpt_it", ":", "int", "=", "None", ",", "model", ":", "object", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Load trained parameter for DQN\n\n    Parameters\n    ----------\n\n    data_dir: str\n        Dicretory model where the weights are store\n\n    ckpt: bool\n        Boolean to regulate if the loaded weights are a checkpoint or not\n\n    ckpt_it: int\n        Number of iteration of the checkpoint you want to load\n\n    ckpt_folder: bool\n        boolean if you want to load weights from pretrained models\n\n    Returns\n    ----------\n    model\n        Model with loaded weights\n    actions: np.ndarray\n        Array of possible actions\n\n    \"\"\"", "\n", "if", "not", "model", ":", "\n", "        ", "query", "=", "gin", ".", "query_parameter", "\n", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", ":", "\n", "            ", "num_inp", "=", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "num_inp", "=", "2", "\n", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "actions", "=", "ResActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE_RES\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", "\n", ")", ".", "values", "\n", "", "else", ":", "\n", "            ", "actions", "=", "ActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", ",", "query", "(", "\"%SIDE_ONLY\"", ")", "\n", ")", ".", "values", "\n", "\n", "", "num_actions", "=", "len", "(", "actions", ")", "\n", "\n", "model", "=", "DeepNetworkModel", "(", "\n", "query", "(", "\"%SEED\"", ")", ",", "\n", "num_inp", ",", "\n", "query", "(", "\"DQN.hidden_units\"", ")", ",", "\n", "num_actions", ",", "\n", "query", "(", "\"DQN.batch_norm_input\"", ")", ",", "\n", "query", "(", "\"DQN.batch_norm_hidden\"", ")", ",", "\n", "query", "(", "\"DQN.activation\"", ")", ",", "\n", "query", "(", "\"DQN.kernel_initializer\"", ")", ",", "\n", "modelname", "=", "\"TrainNet\"", ",", "\n", ")", "\n", "\n", "model", ".", "load_weights", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"ckpt\"", ",", "\"DQN_{}_ep_weights\"", ".", "format", "(", "ckpt_it", ")", ")", "\n", ")", "\n", "\n", "model", ".", "modelname", "=", "'DQN'", "\n", "\n", "return", "model", ",", "actions", "\n", "\n", "", "else", ":", "\n", "        ", "model", ".", "load_weights", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"ckpt\"", ",", "\"DQN_{}_ep_weights\"", ".", "format", "(", "ckpt_it", ")", ")", "\n", ")", "\n", "\n", "model", ".", "modelname", "=", "'DQN'", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.load_PPOmodel": [[116, 213], ["gin.query_parameter", "query", "gin.query_parameter", "agents.PPO.PPOActorCritic", "agents.PPO.PPOActorCritic.load_state_dict", "len", "len", "len", "query", "query", "query", "query", "query", "query", "query", "query", "query", "query", "query", "torch.load", "agents.PPO.PPOActorCritic.load_state_dict", "gin.query_parameter", "query", "utils.spaces.ResActionSpace", "utils.spaces.ActionSpace", "gin.query_parameter", "os.path.join", "torch.load", "gin.query_parameter", "query", "query", "query", "query", "query", "query", "query", "query", "query", "os.path.join", "len", "len", "query", "query"], "function", ["None"], ["", "", "def", "load_PPOmodel", "(", "\n", "data_dir", ":", "str", ",", "ckpt_it", ":", "int", "=", "None", ",", "model", ":", "object", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Load trained parameter for DQN\n\n    Parameters\n    ----------\n    d: dict\n        Parameter config loaded from the folder experiment\n\n    data_dir: str\n        Dicretory model where the weights are stores\n\n    ckpt: bool\n        Boolean to regulate if the loaded weights are a checkpoint or not\n\n    ckpt_it: int\n        Number of iteration of the checkpoint you want to load\n\n    Returns\n    ----------\n    model\n        Model with loaded weights\n    actions: np.ndarray\n        Array of possible actions\n\n    \"\"\"", "\n", "if", "not", "model", ":", "\n", "        ", "query", "=", "gin", ".", "query_parameter", "\n", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "n_assets", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "n_factors", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", "[", "0", "]", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "inp_shape", "=", "(", "n_factors", "*", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "inp_shape", "=", "(", "n_assets", "+", "n_assets", "+", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "                ", "if", "query", "(", "\"%TIME_DEPENDENT\"", ")", ":", "\n", "                    ", "inp_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "2", ",", ")", "\n", "", "else", ":", "\n", "                    ", "inp_shape", "=", "(", "len", "(", "query", "(", "'%F_PARAM'", ")", ")", "+", "1", ",", ")", "\n", "", "", "else", ":", "\n", "                ", "inp_shape", "=", "(", "2", ",", ")", "\n", "\n", "\n", "", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "actions", "=", "ResActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE_RES\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", "\n", ")", ".", "values", "\n", "", "else", ":", "\n", "            ", "actions", "=", "ActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", ",", "query", "(", "\"%SIDE_ONLY\"", ")", "\n", ")", ".", "values", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "num_actions", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "", "else", ":", "\n", "            ", "num_actions", "=", "actions", ".", "ndim", "\n", "\n", "", "model", "=", "PPOActorCritic", "(", "\n", "query", "(", "\"%SEED\"", ")", ",", "\n", "inp_shape", ",", "\n", "query", "(", "\"PPO.activation\"", ")", ",", "\n", "query", "(", "\"PPO.hidden_units_value\"", ")", ",", "\n", "query", "(", "\"PPO.hidden_units_actor\"", ")", ",", "\n", "num_actions", ",", "\n", "query", "(", "\"PPO.batch_norm_input\"", ")", ",", "\n", "query", "(", "\"PPO.batch_norm_value_out\"", ")", ",", "\n", "query", "(", "\"PPO.policy_type\"", ")", ",", "\n", "query", "(", "\"PPO.init_pol_std\"", ")", ",", "\n", "query", "(", "\"PPO.min_pol_std\"", ")", ",", "\n", "query", "(", "\"PPO.std_transform\"", ")", ",", "\n", "query", "(", "\"PPO.init_last_layers\"", ")", ",", "\n", "modelname", "=", "\"PPO\"", ",", "\n", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"ckpt\"", ",", "\"PPO_{}_ep_weights.pth\"", ".", "format", "(", "ckpt_it", ")", ")", "\n", ")", "\n", ")", "\n", "\n", "return", "model", ",", "actions", "\n", "\n", "", "else", ":", "\n", "        ", "if", "ckpt_it", "==", "'rnd'", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"ckpt\"", ",", "\"PPO_{}_ep_weights.pth\"", ".", "format", "(", "ckpt_it", ")", ")", "\n", ")", "\n", ")", "\n", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_pct_metrics": [[215, 323], ["gin.parse_config_file", "df.median", "matplotlib.cm.get_cmap", "enumerate", "ax1.plot", "ax1.set_title", "ax1.set_ylabel", "ax1.set_xlabel", "ax1.legend", "ax1.xaxis.set_major_formatter", "ax1.yaxis.set_major_formatter", "int", "len", "ax1.fill_between", "gin.query_parameter", "matplotlib.ticker.ScalarFormatter", "matplotlib.ticker.ScalarFormatter", "ax1.scatter", "ax1.scatter", "numpy.std", "ax1.set_ylim", "ax1.plot", "ax1.plot", "ax1.plot", "variable.split", "variable.split", "gin.query_parameter", "ax1.set_ylim", "[].split", "data_dir.split", "variable.split", "[].split", "gin.query_parameter", "gin.query_parameter", "ax1.set_ylim", "variable.split", "data_dir.split"], "function", ["None"], ["", "def", "plot_pct_metrics", "(", "\n", "ax1", ",", "\n", "df", ",", "\n", "data_dir", ",", "\n", "N_test", ",", "\n", "variable", ",", "\n", "colors", "=", "[", "\"b\"", ",", "\"darkblue\"", "]", ",", "\n", "conf_interval", "=", "False", ",", "\n", "diff_colors", "=", "False", ",", "\n", "params_path", "=", "None", ",", "\n", ")", ":", "\n", "\n", "    ", "gin", ".", "parse_config_file", "(", "params_path", ",", "skip_unknown", "=", "True", ")", "\n", "\n", "# df_mean = df.mean(axis=0)", "\n", "df_mean", "=", "df", ".", "median", "(", "axis", "=", "0", ")", "\n", "\n", "idxs", "=", "[", "int", "(", "i", ")", "for", "i", "in", "df", ".", "iloc", "[", "0", ",", ":", "]", ".", "index", "]", "\n", "# https://matplotlib.org/examples/color/colormaps_reference.html", "\n", "colormap", "=", "cm", ".", "get_cmap", "(", "\"plasma\"", ",", "len", "(", "df", ".", "index", ")", ")", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "df", ".", "index", ")", ":", "\n", "        ", "if", "diff_colors", ":", "\n", "            ", "ax1", ".", "scatter", "(", "\n", "x", "=", "idxs", ",", "\n", "y", "=", "df", ".", "iloc", "[", "i", ",", ":", "]", ",", "\n", "alpha", "=", "0.6", ",", "\n", "color", "=", "colormap", ".", "colors", "[", "j", "]", ",", "\n", "marker", "=", "\"o\"", ",", "\n", "s", "=", "7.5", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "ax1", ".", "scatter", "(", "\n", "x", "=", "idxs", ",", "y", "=", "df", ".", "iloc", "[", "i", ",", ":", "]", ",", "alpha", "=", "0.6", ",", "color", "=", "colors", ",", "marker", "=", "\"o\"", ",", "s", "=", "7.5", "\n", ")", "\n", "\n", "", "", "ax1", ".", "plot", "(", "\n", "idxs", ",", "\n", "df_mean", ".", "values", ",", "\n", "color", "=", "colors", ",", "\n", "linewidth", "=", "3", ",", "\n", "label", "=", "\"_\"", ".", "join", "(", "data_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"_\"", ")", "[", "2", ":", "]", ")", ",", "\n", ")", "\n", "\n", "if", "conf_interval", ":", "\n", "        ", "ci", "=", "2", "*", "np", ".", "std", "(", "df_mean", ".", "values", ")", "\n", "ax1", ".", "fill_between", "(", "\n", "idxs", ",", "(", "df_mean", ".", "values", "-", "ci", ")", ",", "(", "df_mean", ".", "values", "+", "ci", ")", ",", "color", "=", "colors", ",", "alpha", "=", "0.5", "\n", ")", "\n", "\n", "", "if", "gin", ".", "query_parameter", "(", "\"%DATATYPE\"", ")", "!=", "\"garch\"", ":", "\n", "\n", "        ", "if", "variable", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "==", "\"Pdist\"", ":", "\n", "\n", "            ", "ax1", ".", "set_ylim", "(", "-", "1e4", ",", "1e12", ")", "\n", "", "else", ":", "\n", "\n", "            ", "df", ".", "loc", "[", "\"Benchmark\"", "]", "=", "100.0", "\n", "ax1", ".", "plot", "(", "\n", "idxs", ",", "\n", "df", ".", "loc", "[", "\"Benchmark\"", "]", ".", "values", ",", "\n", "linestyle", "=", "\"--\"", ",", "\n", "linewidth", "=", "4", ",", "\n", "color", "=", "\"red\"", ",", "\n", ")", "\n", "# ax1.set_ylim(-10000,300)", "\n", "# ax1.set_ylim(0, 150)", "\n", "# ax1.set_ylim(-150,150)", "\n", "\n", "", "", "else", ":", "\n", "        ", "if", "variable", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "!=", "\"SR\"", ":", "\n", "            ", "df", ".", "loc", "[", "\"Benchmark\"", "]", "=", "0.0", "\n", "ax1", ".", "plot", "(", "\n", "idxs", ",", "\n", "df", ".", "loc", "[", "\"Benchmark\"", "]", ".", "values", ",", "\n", "linestyle", "=", "\"--\"", ",", "\n", "linewidth", "=", "4", ",", "\n", "color", "=", "\"red\"", ",", "\n", ")", "\n", "if", "gin", ".", "query_parameter", "(", "\"%DATATYPE\"", ")", "==", "\"t_stud\"", ":", "\n", "                ", "ax1", ".", "set_ylim", "(", "-", "1500000", ",", "1500000", ")", "\n", "", "elif", "gin", ".", "query_parameter", "(", "\"%DATATYPE\"", ")", "==", "\"garch\"", ":", "\n", "# ax1.set_ylim(-1000000, 1000000)", "\n", "                ", "pass", "\n", "", "elif", "gin", ".", "query_parameter", "(", "\"%DATATYPE\"", ")", "==", "\"garch_mr\"", ":", "\n", "                ", "ax1", ".", "set_ylim", "(", "-", "5000000", ",", "5000000", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "df", ".", "loc", "[", "\"Benchmark\"", "]", "=", "100.0", "\n", "ax1", ".", "plot", "(", "\n", "idxs", ",", "\n", "df", ".", "loc", "[", "\"Benchmark\"", "]", ".", "values", ",", "\n", "linestyle", "=", "\"--\"", ",", "\n", "linewidth", "=", "4", ",", "\n", "color", "=", "\"red\"", ",", "\n", ")", "\n", "# ax1.set_ylim(0, 150)", "\n", "\n", "", "", "ax1", ".", "set_title", "(", "\n", "\"{}: {} simulation\"", ".", "format", "(", "\n", "variable", ".", "split", "(", "\"_\"", ")", "[", "3", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", ",", "data_dir", ".", "split", "(", "\"_\"", ")", "[", "1", "]", "\n", ")", "\n", ")", "\n", "ax1", ".", "set_ylabel", "(", "\"% Reference {}\"", ".", "format", "(", "variable", ".", "split", "(", "\"_\"", ")", "[", "0", "]", ")", ")", "\n", "ax1", ".", "set_xlabel", "(", "\"in-sample training iterations\"", ")", "\n", "\n", "ax1", ".", "legend", "(", "fontsize", "=", "9", ")", "\n", "ax1", ".", "xaxis", ".", "set_major_formatter", "(", "ScalarFormatter", "(", ")", ")", "\n", "ax1", ".", "yaxis", ".", "set_major_formatter", "(", "ScalarFormatter", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_abs_metrics": [[327, 391], ["df.mean().idxmax", "ax1.plot", "df.mean", "df_opt.median.mean", "int", "df.median", "enumerate", "ax1.plot", "df.mean", "df.median", "df_opt.median.median", "df_opt.median.median", "int", "ax1.scatter", "ax1.plot", "[].split", "[].split", "data_dir.split", "data_dir.split"], "function", ["None"], ["", "def", "plot_abs_metrics", "(", "\n", "ax1", ",", "df", ",", "df_opt", ",", "data_dir", ",", "N_test", ",", "variable", ",", "colors", "=", "[", "\"b\"", ",", "\"darkblue\"", "]", ",", "i", "=", "0", ",", "plt_type", "=", "'diff'", "\n", ")", ":", "\n", "\n", "    ", "if", "plt_type", "==", "'diff'", ":", "\n", "        ", "idxmax", "=", "df", ".", "mean", "(", "1", ")", ".", "idxmax", "(", ")", "\n", "select_agent", "=", "'best'", "\n", "if", "select_agent", "==", "'mean'", ":", "\n", "            ", "ppo", "=", "df", ".", "mean", "(", "0", ")", "\n", "gp", "=", "df_opt", ".", "mean", "(", "0", ")", "\n", "", "elif", "select_agent", "==", "'median'", ":", "\n", "            ", "ppo", "=", "df", ".", "median", "(", "0", ")", "\n", "gp", "=", "df_opt", ".", "median", "(", "0", ")", "\n", "", "elif", "select_agent", "==", "'best'", ":", "\n", "            ", "ppo", "=", "df", ".", "loc", "[", "idxmax", "]", "\n", "gp", "=", "df_opt", ".", "loc", "[", "idxmax", "]", "\n", "\n", "\n", "", "reldiff_avg", "=", "(", "ppo", "-", "gp", ")", "/", "gp", "*", "100", "\n", "\n", "\n", "# df = ((df-df_opt)/df_opt)*100", "\n", "# df_median = df.median(axis=0)", "\n", "# mad = median_abs_deviation(df)", "\n", "# if 'Pdist' not in variable:", "\n", "#     df_opt = df_opt.median(axis=0)", "\n", "\n", "\n", "idxs", "=", "[", "int", "(", "i", ")", "for", "i", "in", "reldiff_avg", ".", "index", "]", "\n", "ax1", ".", "plot", "(", "\n", "idxs", ",", "\n", "reldiff_avg", ".", "values", ",", "\n", "color", "=", "colors", ",", "\n", "linewidth", "=", "2", ",", "\n", "label", "=", "\"{}\"", ".", "format", "(", "\"_\"", ".", "join", "(", "data_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"_\"", ")", "[", "2", ":", "]", ")", ")", ",", "\n", ")", "\n", "# sz=1.0", "\n", "# under_line     = df_median - sz *mad", "\n", "# over_line      = df_median + sz *mad", "\n", "# ax1.fill_between(idxs, under_line, over_line, color=colors, alpha=0.25, linewidth=0, label='')", "\n", "\n", "", "elif", "plt_type", "==", "'abs'", ":", "\n", "\n", "\n", "        ", "df_median", "=", "df", ".", "median", "(", "axis", "=", "0", ")", "\n", "\n", "if", "'Pdist'", "not", "in", "variable", ":", "\n", "            ", "df_opt", "=", "df_opt", ".", "median", "(", "axis", "=", "0", ")", "\n", "\n", "\n", "", "idxs", "=", "[", "int", "(", "i", ")", "for", "i", "in", "df", ".", "iloc", "[", "0", ",", ":", "]", ".", "index", "]", "\n", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "df", ".", "index", ")", ":", "\n", "            ", "ax1", ".", "scatter", "(", "x", "=", "idxs", ",", "y", "=", "df", ".", "iloc", "[", "i", ",", ":", "]", ",", "alpha", "=", "0.6", ",", "color", "=", "colors", ",", "marker", "=", "\"o\"", ",", "s", "=", "3.5", ")", "\n", "", "ax1", ".", "plot", "(", "\n", "idxs", ",", "\n", "df_median", ".", "values", ",", "\n", "color", "=", "colors", ",", "\n", "linewidth", "=", "2", ",", "\n", "label", "=", "\"{}\"", ".", "format", "(", "\"_\"", ".", "join", "(", "data_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"_\"", ")", "[", "2", ":", "]", ")", ")", ",", "\n", ")", "\n", "if", "'Pdist'", "not", "in", "variable", ":", "\n", "            ", "ax1", ".", "plot", "(", "\n", "idxs", ",", "df_opt", ".", "values", ",", "color", "=", "\"red\"", ",", "linestyle", "=", "'--'", ",", "linewidth", "=", "2", ",", "label", "=", "\"GP\"", "if", "i", "==", "0", "else", "\"\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_vf": [[394, 522], ["range", "ax.legend", "ax.plot", "plot.optimal_vf", "ax.plot", "query", "query", "query", "utils.simulation.DataHandler", "utils.simulation.DataHandler.generate_returns", "np.linspace.sort", "numpy.linspace", "numpy.zeros", "tensorflow.constant", "model", "query", "query", "query", "query", "query", "query", "query", "len", "numpy.ones", "numpy.hstack", "torch.from_numpy().float", "query", "query", "query", "utils.simulation.DataHandler", "utils.simulation.DataHandler.generate_returns", "np.concatenate.sort", "len", "numpy.concatenate", "numpy.zeros", "tensorflow.constant", "model", "query", "query", "query", "len", "torch.no_grad", "model", "query", "numpy.linspace().reshape", "len", "numpy.ones", "numpy.hstack", "torch.from_numpy().float", "np.linspace.reshape", "np.zeros.reshape", "torch.from_numpy", "query", "enumerate", "len", "torch.no_grad", "model", "numpy.hstack", "numpy.linspace", "range", "np.zeros.reshape", "torch.from_numpy", "query", "numpy.hstack", "np.linspace.reshape", "np.zeros.reshape", "np.zeros.reshape"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.optimal_vf", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns"], ["", "", "", "def", "plot_vf", "(", "\n", "model", ",", "\n", "actions", ":", "list", ",", "\n", "holding", ":", "float", ",", "\n", "ax", ":", "object", "=", "None", ",", "\n", "less_labels", ":", "bool", "=", "False", ",", "\n", "n_less_labels", ":", "int", "=", "None", ",", "\n", "optimal", "=", "False", ",", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Ploduce plots of learned action-value function of DQN\n\n    Parameters\n    ----------\n    model\n        Loaded model\n\n    actions: list\n        List of possible action for the mdel\n\n    holding: float\n        Fixed holding at which we produce the value function plot\n\n    ax: matplotlib.axes.Axes\n        Axes to draw in\n\n    less_labels: bool\n        Boolean to regulate if all labels appear in the legend. If False, they all appear\n\n    n_less_labels: int\n        Number of labels to include in the legend\n\n    \"\"\"", "\n", "\n", "query", "=", "gin", ".", "query_parameter", "\n", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"ret\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha\"", ":", "\n", "        ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha\"", ":", "\n", "            ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "query", "(", "'%LEN_SERIES'", ")", ",", "rng", "=", "None", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "sample_Ret", "=", "data_handler", ".", "returns", "\n", "sample_Ret", ".", "sort", "(", ")", "\n", "", "else", ":", "\n", "            ", "sample_Ret", "=", "np", ".", "linspace", "(", "-", "0.05", ",", "0.05", ",", "query", "(", "'%LEN_SERIES'", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "\n", "", "if", "holding", "==", "0", ":", "\n", "            ", "holdings", "=", "np", ".", "zeros", "(", "len", "(", "sample_Ret", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "", "else", ":", "\n", "            ", "holdings", "=", "np", ".", "ones", "(", "len", "(", "sample_Ret", ")", ",", "dtype", "=", "\"float\"", ")", "*", "holding", "\n", "\n", "", "if", "model", ".", "modelname", "==", "\"DQN\"", ":", "\n", "            ", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "sample_Ret", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "pred", "=", "model", "(", "states", ",", "training", "=", "False", ")", "\n", "\n", "", "elif", "model", ".", "modelname", "==", "\"PPO\"", ":", "\n", "            ", "states", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "hstack", "(", "(", "sample_Ret", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", ")", ".", "float", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "_", ",", "pred", "=", "model", "(", "states", ")", "\n", "\n", "", "", "", "elif", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "\n", "        ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "            ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "query", "(", "'%LEN_SERIES'", ")", ",", "rng", "=", "None", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "factors", "=", "data_handler", ".", "factors", "\n", "factors", ".", "sort", "(", ")", "\n", "", "else", ":", "\n", "            ", "n_factors", "=", "len", "(", "query", "(", "\"%F_PARAM\"", ")", ")", "\n", "\n", "f_to_concat", "=", "[", "\n", "np", ".", "linspace", "(", "-", "0.5", "-", "i", ",", "0.5", "+", "i", ",", "query", "(", "'%LEN_SERIES'", ")", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "range", "(", "n_factors", ")", ")", "\n", "]", "\n", "\n", "factors", "=", "np", ".", "concatenate", "(", "f_to_concat", ",", "axis", "=", "1", ")", "\n", "\n", "", "if", "holding", "==", "0", ":", "\n", "            ", "holdings", "=", "np", ".", "zeros", "(", "len", "(", "factors", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "", "else", ":", "\n", "            ", "holdings", "=", "np", ".", "ones", "(", "len", "(", "factors", ")", ",", "dtype", "=", "\"float\"", ")", "*", "holding", "\n", "\n", "", "if", "model", ".", "modelname", "==", "\"DQN\"", ":", "\n", "            ", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "\n", "pred", "=", "model", "(", "states", ",", "training", "=", "False", ")", "\n", "\n", "", "elif", "model", ".", "modelname", "==", "\"PPO\"", ":", "\n", "            ", "states", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", ")", ".", "float", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "_", ",", "pred", "=", "model", "(", "states", ")", "\n", "\n", "", "", "", "for", "i", "in", "range", "(", "pred", ".", "shape", "[", "1", "]", ")", ":", "\n", "        ", "ax", ".", "plot", "(", "\n", "states", "[", ":", ",", "0", "]", ",", "\n", "pred", "[", ":", ",", "i", "]", ",", "\n", "label", "=", "'PPO Vf'", ",", "\n", "linewidth", "=", "1.5", ",", "\n", ")", "\n", "\n", "", "if", "optimal", "and", "(", "query", "(", "'%INP_TYPE'", ")", "==", "'f'", "or", "query", "(", "'%INP_TYPE'", ")", "==", "'alpha_f'", ")", ":", "\n", "\n", "        ", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", "=", "(", "\n", "query", "(", "\"%DISCOUNT_RATE\"", ")", ",", "\n", "query", "(", "\"%KAPPA\"", ")", ",", "\n", "query", "(", "\"%COSTMULTIPLIER\"", ")", ",", "\n", "query", "(", "\"%F_PARAM\"", ")", ",", "\n", "query", "(", "\"%HALFLIFE\"", ")", ",", "\n", "query", "(", "\"%SIGMA\"", ")", ",", "\n", ")", "\n", "\n", "\n", "V", "=", "optimal_vf", "(", "\n", "states", ",", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", "\n", ")", "\n", "\n", "ax", ".", "plot", "(", "factors", "[", ":", ",", "0", "]", ",", "V", ",", "linewidth", "=", "1.5", ",", "label", "=", "\"GP Vf\"", ")", "\n", "\n", "", "ax", ".", "legend", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.optimal_vf": [[524, 589], ["numpy.around", "plot.optimal_vf.opt_trading_rate_disc_loads"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads"], ["", "def", "optimal_vf", "(", "states", ",", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", ")", ":", "\n", "    ", "def", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "kappa", ",", "CostMultiplier", ",", "f_param", ",", "f_speed", "\n", ")", ":", "\n", "\n", "# 1 percent annualized discount rate (same rate of Ritter)", "\n", "        ", "rho", "=", "1", "-", "np", ".", "exp", "(", "-", "discount_rate", "/", "260", ")", "\n", "\n", "# kappa is the risk aversion, CostMultiplier the parameter for trading cost", "\n", "num1", "=", "kappa", "*", "(", "1", "-", "rho", ")", "+", "CostMultiplier", "*", "rho", "\n", "num2", "=", "np", ".", "sqrt", "(", "num1", "**", "2", "+", "4", "*", "kappa", "*", "CostMultiplier", "*", "(", "1", "-", "rho", ")", "**", "2", ")", "\n", "den", "=", "2", "*", "(", "1", "-", "rho", ")", "\n", "a", "=", "(", "-", "num1", "+", "num2", ")", "/", "den", "\n", "\n", "OptRate", "=", "a", "/", "CostMultiplier", "\n", "DiscFactorLoads", "=", "f_param", "/", "(", "1", "+", "f_speed", "*", "(", "(", "OptRate", "*", "CostMultiplier", ")", "/", "kappa", ")", ")", "\n", "\n", "return", "OptRate", ",", "DiscFactorLoads", "\n", "\n", "", "f_speed", "=", "np", ".", "around", "(", "np", ".", "log", "(", "2", ")", "/", "halflife", ",", "4", ")", "\n", "OptRate", ",", "DiscFactorLoads", "=", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "f_speed", "\n", ")", "\n", "\n", "disc_rate_bar", "=", "1", "-", "discount_rate", "\n", "lambda_bar", "=", "(", "costmultiplier", "*", "sigma", "**", "2", ")", "/", "disc_rate_bar", "\n", "costmultiplier_bar", "=", "costmultiplier", "/", "disc_rate_bar", "\n", "\n", "axx1", "=", "disc_rate_bar", "*", "kappa", "*", "lambda_bar", "*", "sigma", "**", "2", "\n", "axx2", "=", "0.25", "*", "(", "\n", "discount_rate", "**", "2", "*", "lambda_bar", "**", "2", "\n", "+", "2", "*", "discount_rate", "*", "kappa", "*", "lambda_bar", "*", "sigma", "**", "2", "\n", "+", "(", "kappa", "**", "2", "*", "lambda_bar", "*", "sigma", "**", "2", ")", "/", "lambda_bar", "\n", ")", "\n", "axx3", "=", "-", "0.5", "*", "(", "discount_rate", "*", "lambda_bar", "+", "kappa", "*", "sigma", "**", "2", ")", "\n", "Axx", "=", "np", ".", "sqrt", "(", "axx1", "+", "axx2", ")", "+", "axx3", "\n", "\n", "axf1", "=", "disc_rate_bar", "/", "(", "1", "-", "disc_rate_bar", "*", "(", "1", "-", "f_speed", ")", "*", "(", "1", "-", "Axx", "/", "lambda_bar", ")", ")", "\n", "axf2", "=", "(", "1", "-", "Axx", "/", "lambda_bar", ")", "*", "np", ".", "array", "(", "f_param", ")", "\n", "Axf", "=", "axf1", "*", "axf2", "\n", "\n", "aff1", "=", "disc_rate_bar", "/", "(", "1", "-", "disc_rate_bar", "*", "(", "1", "-", "f_speed", ")", "*", "(", "1", "-", "f_speed", ")", ")", "\n", "q", "=", "(", "np", ".", "array", "(", "f_param", ")", "+", "Axf", "*", "(", "1", "-", "f_speed", ")", ")", "**", "2", "/", "(", "\n", "kappa", "*", "sigma", "**", "2", "+", "lambda_bar", "+", "Axx", "\n", ")", "\n", "Aff", "=", "aff1", ".", "reshape", "(", "-", "1", ",", "1", ")", "@", "q", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "T", "\n", "\n", "states", "=", "states", ".", "numpy", "(", ")", "\n", "\n", "v1", "=", "-", "0.5", "*", "states", "[", ":", ",", "-", "1", "]", "**", "2", "*", "Axx", "\n", "v2s", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "states", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "v2", "=", "states", "[", "i", ",", "-", "1", "]", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "T", "@", "Axf", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "T", "@", "states", "[", "i", ",", ":", "-", "1", "]", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "v2s", ".", "append", "(", "v2", ")", "\n", "", "v2", "=", "np", ".", "array", "(", "v2s", ")", ".", "ravel", "(", ")", "\n", "v3s", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "states", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "v3", "=", "states", "[", "i", ",", ":", "-", "1", "]", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "T", "@", "Aff", "@", "states", "[", "i", ",", ":", "-", "1", "]", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "v3s", ".", "append", "(", "v3", ")", "\n", "", "v3", "=", "0.5", "*", "np", ".", "array", "(", "v3s", ")", ".", "ravel", "(", ")", "\n", "\n", "V", "=", "v1", "+", "v2", "+", "v3", "\n", "\n", "\n", "return", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions": [[591, 865], ["gin.bind_parameter", "query", "numpy.random.RandomState", "numpy.sqrt", "ax.plot", "plot.optimal_vf.opt_trading_rate_disc_loads"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads"], ["", "def", "plot_BestActions", "(", "\n", "model", ",", "holding", ":", "float", ",", "time_to_stop", ":", "float", ",", "ax", ":", "object", "=", "None", ",", "optimal", ":", "bool", "=", "False", ",", "\n", "stochastic", ":", "bool", "=", "True", ",", "seed", ":", "int", "=", "324345", ",", "color", "=", "'tab:blue'", ",", "generate_plot", "=", "False", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Ploduce plots of learned action-value function of DQN\n\n    Parameters\n    ----------\n    p: dict\n        Parameter passed as config files\n\n    model\n        Loaded model\n\n    holding: float\n        Fixed holding at which we produce the value function plot\n\n    ax: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "\n", "query", "=", "gin", ".", "query_parameter", "\n", "# gin.bind_parameter('%DOUBLE_NOISE', False)", "\n", "# gin.bind_parameter('%SIGMAF', [None])", "\n", "# gin.bind_parameter('%INITIAL_ALPHA', [0.009])", "\n", "# gin.bind_parameter('%HALFLIFE', [35])", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "generate_plot", ")", "\n", "\n", "\n", "def", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "kappa", ",", "CostMultiplier", ",", "f_param", ",", "f_speed", "\n", ")", ":", "\n", "\n", "# 1 percent annualized discount rate (same rate of Ritter)", "\n", "        ", "rho", "=", "1", "-", "np", ".", "exp", "(", "-", "discount_rate", "/", "260", ")", "\n", "\n", "# kappa is the risk aversion, CostMultiplier the parameter for trading cost", "\n", "num1", "=", "kappa", "*", "(", "1", "-", "rho", ")", "+", "CostMultiplier", "*", "rho", "\n", "num2", "=", "np", ".", "sqrt", "(", "num1", "**", "2", "+", "4", "*", "kappa", "*", "CostMultiplier", "*", "(", "1", "-", "rho", ")", "**", "2", ")", "\n", "den", "=", "2", "*", "(", "1", "-", "rho", ")", "\n", "a", "=", "(", "-", "num1", "+", "num2", ")", "/", "den", "\n", "\n", "OptRate", "=", "a", "/", "CostMultiplier", "\n", "DiscFactorLoads", "=", "f_param", "/", "(", "1", "+", "f_speed", "*", "(", "(", "OptRate", "*", "CostMultiplier", ")", "/", "kappa", ")", ")", "\n", "\n", "return", "OptRate", ",", "DiscFactorLoads", "\n", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "        ", "actions", "=", "ResActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE_RES\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", "\n", ")", ".", "values", "\n", "", "else", ":", "\n", "        ", "actions", "=", "ActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", ",", "query", "(", "\"%SIDE_ONLY\"", ")", "\n", ")", ".", "values", "\n", "\n", "", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", ")", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"ret\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha\"", ":", "\n", "        ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha\"", ":", "\n", "            ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "query", "(", "'%LEN_SERIES'", ")", ",", "rng", "=", "rng", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "sample_Ret", "=", "data_handler", ".", "returns", "\n", "sample_Ret", ".", "sort", "(", ")", "\n", "", "else", ":", "\n", "            ", "sample_Ret", "=", "np", ".", "linspace", "(", "-", "0.05", ",", "0.05", ",", "query", "(", "'%LEN_SERIES'", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "\n", "", "if", "holding", "==", "0", ":", "\n", "            ", "holdings", "=", "np", ".", "zeros", "(", "len", "(", "sample_Ret", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "", "else", ":", "\n", "            ", "holdings", "=", "np", ".", "ones", "(", "len", "(", "sample_Ret", ")", ",", "dtype", "=", "\"float\"", ")", "*", "holding", "\n", "\n", "\n", "", "if", "model", ".", "modelname", "==", "\"DQN\"", ":", "\n", "            ", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "sample_Ret", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "pred", "=", "model", "(", "states", ",", "training", "=", "False", ")", "\n", "\n", "max_action", "=", "actions", "[", "tf", ".", "math", ".", "argmax", "(", "pred", ",", "axis", "=", "1", ")", "]", "\n", "\n", "", "elif", "model", ".", "modelname", "==", "\"PPO\"", ":", "\n", "            ", "states", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "hstack", "(", "(", "sample_Ret", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", ")", ".", "float", "(", ")", "\n", "\n", "# model.eval()", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "dist", ",", "_", "=", "model", "(", "states", ")", "\n", "\n", "", "if", "stochastic", ":", "\n", "                ", "unscaled_max_action", "=", "torch", ".", "nn", ".", "Tanh", "(", ")", "(", "dist", ".", "sample", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "unscaled_max_action", "=", "torch", ".", "nn", ".", "Tanh", "(", ")", "(", "dist", ".", "mean", ")", "\n", "", "pdb", ".", "set_trace", "(", ")", "\n", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "                ", "max_action", "=", "unscale_asymmetric_action", "(", "actions", "[", "0", "]", ",", "actions", "[", "-", "1", "]", ",", "unscaled_max_action", ")", "\n", "", "else", ":", "\n", "                ", "max_action", "=", "unscale_action", "(", "actions", "[", "-", "1", "]", ",", "unscaled_max_action", ")", "\n", "\n", "\n", "# ci = 1.96 * model.log_std.exp().detach().numpy()", "\n", "", "", "ax", ".", "plot", "(", "\n", "sample_Ret", ",", "\n", "max_action", ",", "\n", "linewidth", "=", "1.5", ",", "\n", "label", "=", "\"{} Policy\"", ".", "format", "(", "model", ".", "modelname", ")", ",", "\n", "color", "=", "color", "\n", ")", "\n", "# ax.fill_between(sample_Ret, (max_action-ci).reshape(-1), (max_action+ci).reshape(-1), color='b', alpha=.1)", "\n", "\n", "\n", "", "elif", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "\n", "        ", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "# gin.bind_parameter('%HALFLIFE',[100])", "\n", "            ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "query", "(", "'%LEN_SERIES'", ")", ",", "rng", "=", "rng", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "factors", "=", "data_handler", ".", "factors", "\n", "\n", "# factors.sort()", "\n", "", "else", ":", "\n", "            ", "n_factors", "=", "len", "(", "query", "(", "\"%F_PARAM\"", ")", ")", "\n", "\n", "f_to_concat", "=", "[", "\n", "np", ".", "linspace", "(", "-", "0.5", "-", "i", ",", "0.5", "+", "i", ",", "query", "(", "'%LEN_SERIES'", ")", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "range", "(", "n_factors", ")", ")", "\n", "]", "\n", "\n", "factors", "=", "np", ".", "concatenate", "(", "f_to_concat", ",", "axis", "=", "1", ")", "\n", "\n", "", "if", "holding", "==", "0", ":", "\n", "            ", "holdings", "=", "np", ".", "zeros", "(", "len", "(", "factors", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "", "elif", "holding", "==", "None", ":", "\n", "            ", "holdings", "=", "np", ".", "linspace", "(", "-", "1e+5", ",", "1e+5", ",", "len", "(", "factors", ")", ",", "dtype", "=", "\"float\"", ")", "\n", "if", "factors", ".", "shape", "[", "1", "]", ">", "1", ":", "\n", "                ", "factors", "=", "np", ".", "array", "(", "[", "np", ".", "repeat", "(", "0.004", ",", "factors", ".", "shape", "[", "1", "]", ")", "]", "*", "len", "(", "factors", ")", ")", "\n", "", "else", ":", "\n", "                ", "factors", "=", "np", ".", "repeat", "(", "0.004", ",", "len", "(", "factors", ")", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "holdings", "=", "np", ".", "ones", "(", "len", "(", "factors", ")", ",", "dtype", "=", "\"float\"", ")", "*", "holding", "\n", "\n", "", "if", "query", "(", "\"%TIME_DEPENDENT\"", ")", ":", "\n", "            ", "times", "=", "np", ".", "ones", "(", "len", "(", "factors", ")", ",", "dtype", "=", "\"float\"", ")", "*", "time_to_stop", "\n", "\n", "", "if", "model", ".", "modelname", "==", "\"DQN\"", ":", "\n", "            ", "if", "query", "(", "\"%TIME_DEPENDENT\"", ")", ":", "\n", "                ", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ",", "times", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "states", "=", "tf", ".", "constant", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", ")", "\n", "\n", "", "pred", "=", "model", "(", "states", ",", "training", "=", "False", ")", "\n", "\n", "max_action", "=", "actions", "[", "tf", ".", "math", ".", "argmax", "(", "pred", ",", "axis", "=", "1", ")", "]", "\n", "", "elif", "model", ".", "modelname", "==", "\"PPO\"", ":", "\n", "            ", "if", "query", "(", "\"%TIME_DEPENDENT\"", ")", ":", "\n", "                ", "states", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ",", "times", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", ")", ".", "float", "(", ")", "\n", "\n", "", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "dist", ",", "_", "=", "model", "(", "states", ")", "\n", "\n", "", "if", "stochastic", ":", "\n", "                ", "unscaled_max_action", "=", "torch", ".", "nn", ".", "Tanh", "(", ")", "(", "dist", ".", "sample", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "unscaled_max_action", "=", "torch", ".", "nn", ".", "Tanh", "(", ")", "(", "dist", ".", "mean", ")", "\n", "# pdb.set_trace()", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "                ", "max_action", "=", "unscale_asymmetric_action", "(", "actions", "[", "0", "]", ",", "actions", "[", "-", "1", "]", ",", "unscaled_max_action", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "else", ":", "\n", "                ", "max_action", "=", "unscale_action", "(", "actions", "[", "-", "1", "]", ",", "unscaled_max_action", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "\n", "\n", "", "", "if", "query", "(", "\"%COST_TYPE\"", ")", "==", "'nondiff'", ":", "\n", "            ", "cm", "=", "query", "(", "\"%CM2\"", ")", "/", "(", "0.01", "*", "query", "(", "\"%DAILY_PRICE\"", ")", "*", "query", "(", "\"%DAILY_VOLUME\"", ")", "*", "query", "(", "\"%SIGMA\"", ")", "**", "2", ")", "\n", "gin", ".", "bind_parameter", "(", "'%COSTMULTIPLIER'", ",", "cm", ")", "\n", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "            ", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", "=", "(", "\n", "query", "(", "\"%DISCOUNT_RATE\"", ")", ",", "\n", "query", "(", "\"%KAPPA\"", ")", ",", "\n", "query", "(", "\"%COSTMULTIPLIER\"", ")", ",", "\n", "query", "(", "\"%F_PARAM\"", ")", ",", "\n", "query", "(", "\"%HALFLIFE\"", ")", ",", "\n", "query", "(", "\"%SIGMA\"", ")", ",", "\n", ")", "\n", "\n", "OptRate", ",", "DiscFactorLoads", "=", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "\n", "kappa", ",", "\n", "costmultiplier", ",", "\n", "f_param", ",", "\n", "np", ".", "around", "(", "np", ".", "log", "(", "2", ")", "/", "halflife", ",", "4", ")", ",", "\n", ")", "\n", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "kappa", "*", "(", "sigma", ")", "**", "2", ")", ")", "*", "np", ".", "sum", "(", "\n", "f_param", "*", "factors", ",", "axis", "=", "1", "\n", ")", "\n", "# Compute optimal markovitz action", "\n", "MV_action", "=", "OptNextHolding", "-", "holdings", "\n", "\n", "max_action", "=", "MV_action", "*", "(", "1", "-", "max_action", ")", "\n", "\n", "", "if", "holding", "==", "None", ":", "\n", "            ", "ax", ".", "plot", "(", "\n", "holdings", ",", "\n", "max_action", ",", "\n", "linewidth", "=", "1.5", ",", "\n", "label", "=", "\"{} Policy\"", ".", "format", "(", "model", ".", "modelname", ")", ",", "\n", "color", "=", "color", "\n", ")", "\n", "", "else", ":", "\n", "            ", "ax", ".", "plot", "(", "\n", "factors", "[", ":", ",", "0", "]", "*", "10", "**", "4", ",", "#to express in bps", "\n", "max_action", ",", "\n", "linewidth", "=", "1.5", ",", "\n", "label", "=", "\"{} Policy\"", ".", "format", "(", "model", ".", "modelname", ")", ",", "\n", "color", "=", "color", "\n", ")", "\n", "\n", "# ci = 1.96 * model.log_std.exp().detach().numpy()", "\n", "# ax.fill_between(factors[:, 0], (max_action-ci*max_action).reshape(-1), (max_action+ci*max_action).reshape(-1), color='b', alpha=.1)", "\n", "\n", "", "", "if", "optimal", ":", "\n", "\n", "        ", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", "=", "(", "\n", "query", "(", "\"%DISCOUNT_RATE\"", ")", ",", "\n", "query", "(", "\"%KAPPA\"", ")", ",", "\n", "query", "(", "\"%COSTMULTIPLIER\"", ")", ",", "\n", "query", "(", "\"%F_PARAM\"", ")", ",", "\n", "query", "(", "\"%HALFLIFE\"", ")", ",", "\n", "query", "(", "\"%SIGMA\"", ")", ",", "\n", ")", "\n", "\n", "OptRate", ",", "DiscFactorLoads", "=", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "\n", "kappa", ",", "\n", "costmultiplier", ",", "\n", "f_param", ",", "\n", "np", ".", "around", "(", "np", ".", "log", "(", "2", ")", "/", "halflife", ",", "4", ")", ",", "\n", ")", "\n", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"ret\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha\"", ":", "\n", "\n", "            ", "OptNextHolding", "=", "(", "1", "-", "OptRate", ")", "*", "holding", "+", "OptRate", "*", "(", "\n", "1", "/", "(", "kappa", "*", "(", "sigma", ")", "**", "2", ")", "\n", ")", "*", "sample_Ret", "\n", "optimal_policy", "=", "OptNextHolding", "-", "holding", "\n", "\n", "ax", ".", "plot", "(", "sample_Ret", ",", "optimal_policy", ",", "linewidth", "=", "1.5", ",", "label", "=", "\"GP Policy\"", ",", "ls", "=", "'--'", ")", "\n", "", "elif", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "\n", "            ", "OptNextHolding", "=", "(", "1", "-", "OptRate", ")", "*", "holdings", "+", "OptRate", "*", "(", "\n", "1", "/", "(", "kappa", "*", "(", "sigma", ")", "**", "2", ")", "\n", ")", "*", "np", ".", "sum", "(", "DiscFactorLoads", "*", "factors", ",", "axis", "=", "1", ")", "\n", "optimal_policy", "=", "OptNextHolding", "-", "holdings", "\n", "\n", "if", "holding", "==", "None", ":", "\n", "                ", "ax", ".", "plot", "(", "holdings", ",", "optimal_policy", ",", "linewidth", "=", "1.5", ",", "label", "=", "\"GP Policy\"", ",", "color", "=", "'tab:orange'", ",", "ls", "=", "'--'", ")", "\n", "", "else", ":", "\n", "                ", "ax", ".", "plot", "(", "factors", "[", ":", ",", "0", "]", "*", "10", "**", "4", ",", "optimal_policy", ",", "linewidth", "=", "1.5", ",", "label", "=", "\"GP Policy\"", ",", "color", "=", "'tab:orange'", ",", "ls", "=", "'--'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_BestActions_time": [[879, 1044], ["gin.bind_parameter", "query", "numpy.random.RandomState", "numpy.arange", "torch.from_numpy().float", "model.eval", "query", "query", "plot.optimal_vf.opt_trading_rate_disc_loads"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads"], ["", "", "", "", "def", "plot_BestActions_time", "(", "\n", "model", ",", "holding", ":", "float", ",", "alpha", ":", "float", ",", "ax", ":", "object", "=", "None", ",", "optimal", ":", "bool", "=", "False", ",", "\n", "stochastic", ":", "bool", "=", "True", ",", "seed", ":", "int", "=", "324345", ",", "color", "=", "'tab:blue'", ",", "generate_plot", "=", "False", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Ploduce plots of learned action-value function of DQN\n\n    Parameters\n    ----------\n    p: dict\n        Parameter passed as config files\n\n    model\n        Loaded model\n\n    holding: float\n        Fixed holding at which we produce the value function plot\n\n    ax: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "\n", "query", "=", "gin", ".", "query_parameter", "\n", "# gin.bind_parameter('%DOUBLE_NOISE', False)", "\n", "# gin.bind_parameter('%SIGMAF', [None])", "\n", "# gin.bind_parameter('%INITIAL_ALPHA', [0.009])", "\n", "# gin.bind_parameter('%HALFLIFE', [35])", "\n", "gin", ".", "bind_parameter", "(", "'alpha_term_structure_sampler.generate_plot'", ",", "generate_plot", ")", "\n", "\n", "\n", "def", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "kappa", ",", "CostMultiplier", ",", "f_param", ",", "f_speed", "\n", ")", ":", "\n", "\n", "# 1 percent annualized discount rate (same rate of Ritter)", "\n", "        ", "rho", "=", "1", "-", "np", ".", "exp", "(", "-", "discount_rate", "/", "260", ")", "\n", "\n", "# kappa is the risk aversion, CostMultiplier the parameter for trading cost", "\n", "num1", "=", "kappa", "*", "(", "1", "-", "rho", ")", "+", "CostMultiplier", "*", "rho", "\n", "num2", "=", "np", ".", "sqrt", "(", "num1", "**", "2", "+", "4", "*", "kappa", "*", "CostMultiplier", "*", "(", "1", "-", "rho", ")", "**", "2", ")", "\n", "den", "=", "2", "*", "(", "1", "-", "rho", ")", "\n", "a", "=", "(", "-", "num1", "+", "num2", ")", "/", "den", "\n", "\n", "OptRate", "=", "a", "/", "CostMultiplier", "\n", "DiscFactorLoads", "=", "f_param", "/", "(", "1", "+", "f_speed", "*", "(", "(", "OptRate", "*", "CostMultiplier", ")", "/", "kappa", ")", ")", "\n", "\n", "return", "OptRate", ",", "DiscFactorLoads", "\n", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "        ", "actions", "=", "ResActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE_RES\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", "\n", ")", ".", "values", "\n", "", "else", ":", "\n", "        ", "actions", "=", "ActionSpace", "(", "\n", "query", "(", "\"%ACTION_RANGE\"", ")", ",", "query", "(", "\"%ZERO_ACTION\"", ")", ",", "query", "(", "\"%SIDE_ONLY\"", ")", "\n", ")", ".", "values", "\n", "\n", "", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", ")", "\n", "\n", "\n", "holdings", "=", "np", ".", "ones", "(", "query", "(", "'%LEN_SERIES'", ")", ",", "dtype", "=", "\"float\"", ")", "*", "holding", "\n", "factors", "=", "np", ".", "ones", "(", "query", "(", "'%LEN_SERIES'", ")", ",", "dtype", "=", "\"float\"", ")", "*", "alpha", "\n", "times", "=", "np", ".", "arange", "(", "query", "(", "'%LEN_SERIES'", ")", ")", "\n", "\n", "\n", "states", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "hstack", "(", "(", "factors", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "times", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "holdings", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", ")", ".", "float", "(", ")", "\n", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "dist", ",", "_", "=", "model", "(", "states", ")", "\n", "\n", "", "if", "stochastic", ":", "\n", "        ", "unscaled_max_action", "=", "torch", ".", "nn", ".", "Tanh", "(", ")", "(", "dist", ".", "sample", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "unscaled_max_action", "=", "torch", ".", "nn", ".", "Tanh", "(", ")", "(", "dist", ".", "mean", ")", "\n", "# pdb.set_trace()", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "        ", "max_action", "=", "unscale_asymmetric_action", "(", "actions", "[", "0", "]", ",", "actions", "[", "-", "1", "]", ",", "unscaled_max_action", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "", "else", ":", "\n", "        ", "max_action", "=", "unscale_action", "(", "actions", "[", "-", "1", "]", ",", "unscaled_max_action", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", ")", "\n", "\n", "\n", "", "if", "query", "(", "\"%COST_TYPE\"", ")", "==", "'nondiff'", ":", "\n", "        ", "cm", "=", "query", "(", "\"%CM2\"", ")", "/", "(", "0.01", "*", "query", "(", "\"%DAILY_PRICE\"", ")", "*", "query", "(", "\"%DAILY_VOLUME\"", ")", "*", "query", "(", "\"%SIGMA\"", ")", "**", "2", ")", "\n", "gin", ".", "bind_parameter", "(", "'%COSTMULTIPLIER'", ",", "cm", ")", "\n", "\n", "# pdb.set_trace()", "\n", "", "if", "query", "(", "\"%MV_RES\"", ")", ":", "\n", "        ", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", "=", "(", "\n", "query", "(", "\"%DISCOUNT_RATE\"", ")", ",", "\n", "query", "(", "\"%KAPPA\"", ")", ",", "\n", "query", "(", "\"%COSTMULTIPLIER\"", ")", ",", "\n", "query", "(", "\"%F_PARAM\"", ")", ",", "\n", "query", "(", "\"%HALFLIFE\"", ")", ",", "\n", "query", "(", "\"%SIGMA\"", ")", ",", "\n", ")", "\n", "\n", "OptRate", ",", "DiscFactorLoads", "=", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "\n", "kappa", ",", "\n", "costmultiplier", ",", "\n", "f_param", ",", "\n", "np", ".", "around", "(", "np", ".", "log", "(", "2", ")", "/", "halflife", ",", "4", ")", ",", "\n", ")", "\n", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "kappa", "*", "(", "sigma", ")", "**", "2", ")", ")", "*", "np", ".", "sum", "(", "\n", "f_param", "*", "factors", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "axis", "=", "1", "\n", ")", "\n", "# Compute optimal markovitz action", "\n", "MV_action", "=", "OptNextHolding", "-", "holdings", "\n", "\n", "max_action", "=", "MV_action", "*", "(", "1", "-", "max_action", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "\n", "", "discount_rate", ",", "kappa", ",", "costmultiplier", ",", "f_param", ",", "halflife", ",", "sigma", "=", "(", "\n", "query", "(", "\"%DISCOUNT_RATE\"", ")", ",", "\n", "query", "(", "\"%KAPPA\"", ")", ",", "\n", "query", "(", "\"%COSTMULTIPLIER\"", ")", ",", "\n", "query", "(", "\"%F_PARAM\"", ")", ",", "\n", "query", "(", "\"%HALFLIFE\"", ")", ",", "\n", "query", "(", "\"%SIGMA\"", ")", ",", "\n", ")", "\n", "\n", "OptRate", ",", "DiscFactorLoads", "=", "opt_trading_rate_disc_loads", "(", "\n", "discount_rate", ",", "\n", "kappa", ",", "\n", "costmultiplier", ",", "\n", "f_param", ",", "\n", "np", ".", "around", "(", "np", ".", "log", "(", "2", ")", "/", "halflife", ",", "4", ")", ",", "\n", ")", "\n", "\n", "if", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"ret\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha\"", ":", "\n", "        ", "pass", "\n", "# OptNextHolding = (1 - OptRate) * holding + OptRate * (", "\n", "#     1 / (kappa * (sigma) ** 2)", "\n", "# ) * sample_Ret", "\n", "# optimal_policy = OptNextHolding - holding", "\n", "\n", "# ax.plot(sample_Ret, optimal_policy, linewidth=1.5, label=\"GP Policy\")", "\n", "", "elif", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"f\"", "or", "query", "(", "\"%INP_TYPE\"", ")", "==", "\"alpha_f\"", ":", "\n", "\n", "        ", "OptNextHolding", "=", "(", "1", "-", "OptRate", ")", "*", "holdings", "+", "OptRate", "*", "(", "\n", "1", "/", "(", "kappa", "*", "(", "sigma", ")", "**", "2", ")", "\n", ")", "*", "np", ".", "sum", "(", "DiscFactorLoads", "*", "factors", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "axis", "=", "1", ")", "\n", "optimal_policy", "=", "OptNextHolding", "-", "holdings", "\n", "\n", "# pdb.set_trace()", "\n", "# ax.plot(times, optimal_policy, linewidth=1.5, label=\"GP Policy\", color='tab:orange')", "\n", "\n", "\n", "", "pct_diff", "=", "(", "optimal_policy", "-", "max_action", ")", "/", "max_action", "*", "100", "\n", "\n", "ax", ".", "plot", "(", "\n", "times", ",", "#to express in bps", "\n", "pct_diff", ",", "\n", "linewidth", "=", "1.5", ",", "\n", "label", "=", "\"Pct diff\"", ",", "\n", "color", "=", "color", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_portfolio": [[1046, 1089], ["gin.query_parameter", "ax2.plot", "ax2.plot", "range", "ax2.legend", "ax2.plot", "ax2.plot", "ax2.lines[].set_color", "numpy.linalg.norm", "matplotlib.offsetbox.AnchoredText", "ax2.add_artist", "numpy.round", "matplotlib.offsetbox.AnchoredText", "ax2.add_artist", "r.filter", "ax2.lines[].get_color", "list", "list", "numpy.sum", "r.filter", "r.filter", "dict", "dict", "r.filter", "r.filter", "r.filter", "r.filter"], "function", ["None"], ["", "def", "plot_portfolio", "(", "r", ":", "pd", ".", "DataFrame", ",", "tag", ":", "str", ",", "ax2", ":", "object", ",", "tbox", ":", "bool", "=", "True", ",", "colors", ":", "list", "=", "[", "'tab:blue'", ",", "'tab:orange'", "]", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of portfolio holding\n\n    Parameters\n    ----------\n    r: pd.DataFrame\n        Dataframe containing the variables\n\n    tag: str\n        Name of the algorithm to plot result\n\n    ax2: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "\n", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "        ", "ax2", ".", "plot", "(", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "values", "[", "1", ":", "-", "1", "]", ")", "\n", "ax2", ".", "plot", "(", "r", ".", "filter", "(", "like", "=", "\"OptNextHolding\"", ")", ".", "values", "[", "1", ":", "-", "1", "]", ",", "alpha", "=", "0.65", ",", "ls", "=", "'--'", ")", "\n", "\n", "\n", "n_lines", "=", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "shape", "[", "-", "1", "]", "\n", "for", "i", "in", "range", "(", "n_lines", ")", ":", "\n", "            ", "ax2", ".", "lines", "[", "-", "1", "-", "i", "]", ".", "set_color", "(", "ax2", ".", "lines", "[", "n_lines", "-", "1", "-", "i", "]", ".", "get_color", "(", ")", ")", "\n", "\n", "", "ax2", ".", "legend", "(", "list", "(", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "columns", ")", "+", "\n", "list", "(", "r", ".", "filter", "(", "like", "=", "\"OptNextHolding\"", ")", ".", "columns", ")", ",", "fontsize", "=", "9", ")", "\n", "\n", "if", "tbox", ":", "\n", "            ", "hold_diff", "=", "r", ".", "filter", "(", "like", "=", "\"OptNextHolding\"", ")", ".", "values", "[", "1", ":", "-", "1", "]", "-", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "values", "[", "1", ":", "-", "1", "]", "\n", "norm", "=", "np", ".", "linalg", ".", "norm", "(", "hold_diff", ")", "\n", "norm_text", "=", "AnchoredText", "(", "\"Norm diff: {:e}\"", ".", "format", "(", "norm", ")", ",", "loc", "=", "'upper left'", ",", "prop", "=", "dict", "(", "size", "=", "10", ")", ")", "\n", "ax2", ".", "add_artist", "(", "norm_text", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "        ", "ax2", ".", "plot", "(", "r", "[", "\"OptNextHolding\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"benchmark\"", ",", "color", "=", "colors", "[", "1", "]", ",", "ls", "=", "'--'", ")", "\n", "ax2", ".", "plot", "(", "r", "[", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ",", "color", "=", "colors", "[", "0", "]", ")", "\n", "if", "tbox", ":", "\n", "            ", "mse", "=", "np", ".", "round", "(", "np", ".", "sum", "(", "r", "[", "\"OptNextHolding\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", "-", "r", "[", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ")", ",", "decimals", "=", "0", ")", "\n", "mse_text", "=", "AnchoredText", "(", "\"GP - PPO: {:e}\"", ".", "format", "(", "mse", ")", ",", "loc", "=", "1", ",", "prop", "=", "dict", "(", "size", "=", "10", ")", ")", "\n", "ax2", ".", "add_artist", "(", "mse_text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_2asset_holding": [[1090, 1121], ["seaborn.kdeplot", "seaborn.kdeplot", "seaborn.kdeplot", "seaborn.kdeplot", "ax2.set_xlabel", "ax2.set_ylabel", "ax2.legend", "list", "list", "r.filter", "r.filter", "r.filter", "r.filter", "r.filter", "r.filter"], "function", ["None"], ["", "", "", "def", "plot_2asset_holding", "(", "r", ":", "pd", ".", "DataFrame", ",", "tag", ":", "str", ",", "ax2", ":", "object", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of portfolio holding\n\n    Parameters\n    ----------\n    r: pd.DataFrame\n        Dataframe containing the variables\n\n    tag: str\n        Name of the algorithm to plot result\n\n    ax2: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "\n", "# ax2.hist(r.filter(like=\"NextHolding_{}\".format(tag)).values[1:-1], edgecolor = 'black',color=['b','y'])", "\n", "# ax2.hist(r.filter(like='OptNextHolding').values[1:-1], alpha=0.65,color=['b','y'])", "\n", "\n", "sns", ".", "kdeplot", "(", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "iloc", "[", ":", ",", "0", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax2", ",", "color", "=", "'b'", ")", "\n", "sns", ".", "kdeplot", "(", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "iloc", "[", ":", ",", "1", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax2", ",", "color", "=", "'y'", ")", "\n", "sns", ".", "kdeplot", "(", "r", ".", "filter", "(", "like", "=", "'OptNextHolding'", ")", ".", "iloc", "[", ":", ",", "0", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax2", ",", "color", "=", "'b'", ",", "alpha", "=", "0.5", ",", "ls", "=", "'--'", ")", "\n", "sns", ".", "kdeplot", "(", "r", ".", "filter", "(", "like", "=", "'OptNextHolding'", ")", ".", "iloc", "[", ":", ",", "1", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "bw_method", "=", "0.2", ",", "ax", "=", "ax2", ",", "color", "=", "'y'", ",", "alpha", "=", "0.5", ",", "ls", "=", "'--'", ")", "\n", "\n", "ax2", ".", "set_xlabel", "(", "'Position amount'", ")", "\n", "ax2", ".", "set_ylabel", "(", "'Frequency'", ")", "\n", "\n", "\n", "ax2", ".", "legend", "(", "list", "(", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "columns", ")", "+", "\n", "list", "(", "r", ".", "filter", "(", "like", "=", "\"OptNextHolding\"", ")", ".", "columns", ")", ",", "fontsize", "=", "9", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_heatmap_holding": [[1122, 1152], ["matplotlib.subplots", "sklearn.preprocessing.MinMaxScaler", "sklearn.preprocessing.MinMaxScaler.fit_transform", "seaborn.heatmap", "ax.set_title", "ax.set_xlabel", "ax.set_ylabel", "utils.common.set_size", "r.filter", "r.filter"], "function", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.set_size"], ["", "def", "plot_heatmap_holding", "(", "r", ":", "pd", ".", "DataFrame", ",", "tag", ":", "str", ",", "title", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of portfolio holding\n\n    Parameters\n    ----------\n    r: pd.DataFrame\n        Dataframe containing the variables\n\n    tag: str\n        Name of the algorithm to plot result\n\n    ax2: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "\n", "# pdb.set_trace()", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "set_size", "(", "width", "=", "1000.0", ")", ")", "\n", "\n", "hold_diff", "=", "r", ".", "filter", "(", "like", "=", "\"OptNextHolding\"", ")", ".", "values", "[", "1", ":", "-", "1", "]", "-", "r", ".", "filter", "(", "like", "=", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ")", ".", "values", "[", "1", ":", "-", "1", "]", "\n", "scaler", "=", "MinMaxScaler", "(", "(", "-", "1", ",", "1", ")", ")", "\n", "hold_diff", "=", "scaler", ".", "fit_transform", "(", "hold_diff", ")", "\n", "\n", "sns", ".", "heatmap", "(", "hold_diff", ",", "ax", "=", "ax", ",", "cmap", "=", "'viridis'", ")", "\n", "\n", "ax", ".", "set_title", "(", "title", ",", "fontsize", "=", "9", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "'Assets'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Time'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_action": [[1156, 1181], ["ax2.plot", "ax2.plot", "ax2.hist", "ax2.hist", "ax2.hist"], "function", ["None"], ["", "def", "plot_action", "(", "r", ":", "pd", ".", "DataFrame", ",", "tag", ":", "str", ",", "ax2", ":", "object", ",", "hist", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of portfolio holding\n\n    Parameters\n    ----------\n    r: pd.DataFrame\n        Dataframe containing the variables\n\n    tag: str\n        Name of the algorithm to plot result\n\n    ax2: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "if", "hist", ":", "\n", "        ", "if", "\"ResAction_{}\"", ".", "format", "(", "tag", ")", "in", "r", ".", "columns", ":", "\n", "            ", "ax2", ".", "hist", "(", "r", "[", "\"ResAction_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ")", "\n", "", "else", ":", "\n", "            ", "ax2", ".", "hist", "(", "r", "[", "\"Action_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ")", "\n", "ax2", ".", "hist", "(", "r", "[", "\"OptNextAction\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"benchmark\"", ",", "alpha", "=", "0.5", ")", "\n", "", "", "else", ":", "\n", "        ", "ax2", ".", "plot", "(", "r", "[", "\"Action_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ")", "\n", "ax2", ".", "plot", "(", "r", "[", "\"OptNextAction\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"benchmark\"", ",", "alpha", "=", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.plot_costs": [[1183, 1205], ["ax2.hist", "ax2.hist", "ax2.plot", "ax2.plot", "r[].cumsum", "r[].cumsum"], "function", ["None"], ["", "", "def", "plot_costs", "(", "r", ":", "pd", ".", "DataFrame", ",", "tag", ":", "str", ",", "ax2", ":", "object", ",", "hist", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Ploduce plots of portfolio holding\n\n    Parameters\n    ----------\n    r: pd.DataFrame\n        Dataframe containing the variables\n\n    tag: str\n        Name of the algorithm to plot result\n\n    ax2: matplotlib.axes.Axes\n        Axes to draw in\n\n    \"\"\"", "\n", "if", "hist", ":", "\n", "        ", "ax2", ".", "hist", "(", "r", "[", "\"Cost_{}\"", ".", "format", "(", "tag", ")", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ")", "\n", "ax2", ".", "hist", "(", "r", "[", "\"OptCost\"", "]", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"benchmark\"", ",", "alpha", "=", "0.5", ")", "\n", "", "else", ":", "\n", "        ", "ax2", ".", "plot", "(", "r", "[", "\"Cost_{}\"", ".", "format", "(", "tag", ")", "]", ".", "cumsum", "(", ")", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "tag", ")", "\n", "ax2", ".", "plot", "(", "r", "[", "\"OptCost\"", "]", ".", "cumsum", "(", ")", ".", "values", "[", "1", ":", "-", "1", "]", ",", "label", "=", "\"benchmark\"", ",", "alpha", "=", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.plot.move_sn_x": [[1206, 1272], ["matplotlib.xticks", "int", "matplotlib.xticks", "matplotlib.text", "list", "str().split", "str", "map", "matplotlib.gca", "str"], "function", ["None"], ["", "", "def", "move_sn_x", "(", "offs", "=", "0", ",", "dig", "=", "0", ",", "side", "=", "'left'", ",", "omit_last", "=", "False", ")", ":", "\n", "    ", "\"\"\"Move scientific notation exponent from top to the side.\n    \n    Additionally, one can set the number of digits after the comma\n    for the y-ticks, hence if it should state 1, 1.0, 1.00 and so forth.\n\n    Parameters\n    ----------\n    offs : float, optional; <0>\n        Horizontal movement additional to default.\n    dig : int, optional; <0>\n        Number of decimals after the comma.\n    side : string, optional; {<'left'>, 'right'}\n        To choose the side of the y-axis notation.\n    omit_last : bool, optional; <False>\n        If True, the top y-axis-label is omitted.\n\n    Returns\n    -------\n    locs : list\n        List of y-tick locations.\n\n    Note\n    ----\n    This is kind of a non-satisfying hack, which should be handled more\n    properly. But it works. Functions to look at for a better implementation:\n    ax.ticklabel_format\n    ax.yaxis.major.formatter.set_offset_string\n    \"\"\"", "\n", "\n", "# Get the ticks", "\n", "locs", ",", "_", "=", "plt", ".", "xticks", "(", ")", "\n", "# pdb.set_trace()", "\n", "\n", "# Put the last entry into a string, ensuring it is in scientific notation", "\n", "# E.g: 123456789 => '1.235e+08'", "\n", "llocs", "=", "'%.3e'", "%", "locs", "[", "-", "1", "]", "\n", "\n", "# Get the magnitude, hence the number after the 'e'", "\n", "# E.g: '1.235e+08' => 8", "\n", "yoff", "=", "int", "(", "str", "(", "llocs", ")", ".", "split", "(", "'e'", ")", "[", "1", "]", ")", "\n", "\n", "\n", "# If omit_last, remove last entry", "\n", "if", "omit_last", ":", "\n", "        ", "slocs", "=", "locs", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "slocs", "=", "locs", "\n", "\n", "# Set ticks to the requested precision", "\n", "", "form", "=", "r'$%.'", "+", "str", "(", "dig", ")", "+", "'f$'", "\n", "\n", "plt", ".", "xticks", "(", "locs", ",", "list", "(", "map", "(", "lambda", "x", ":", "form", "%", "x", ",", "slocs", "/", "(", "10", "**", "yoff", ")", ")", ")", ")", "\n", "\n", "# Define offset depending on the side", "\n", "if", "side", "==", "'left'", ":", "\n", "        ", "offs", "=", "-", ".18", "-", "offs", "# Default left: -0.18", "\n", "", "elif", "side", "==", "'right'", ":", "\n", "        ", "offs", "=", "1", "+", "offs", "# Default right: 1.0", "\n", "\n", "# Plot the exponent", "\n", "", "plt", ".", "text", "(", "offs", ",", "0.05", ",", "r'$\\times10^{%i}$'", "%", "yoff", ",", "transform", "=", "\n", "plt", ".", "gca", "(", ")", ".", "transAxes", ",", "verticalalignment", "=", "'top'", ",", "fontsize", "=", "11", ")", "\n", "\n", "# Return the locs", "\n", "return", "locs", "\n", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.__init__": [[32, 64], ["variables.append", "variables.append", "variables.append", "variables.append"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "n_seeds", ":", "int", ",", "\n", "N_test", ":", "int", ",", "\n", "rnd_state", ":", "int", ",", "\n", "savedpath", ":", "str", ",", "\n", "tag", ":", "str", ",", "\n", "experiment_type", ":", "str", ",", "\n", "env_cls", ":", "object", ",", "\n", "MV_res", ":", "bool", ",", "\n", "universal_train", ":", "bool", "=", "False", ",", "\n", "mv_solution", ":", "bool", "=", "False", ",", "\n", "stochastic_policy", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "\n", "        ", "variables", "=", "[", "]", "\n", "variables", ".", "append", "(", "\"NetPNL_{}\"", ".", "format", "(", "tag", ")", ")", "\n", "variables", ".", "append", "(", "\"Reward_{}\"", ".", "format", "(", "tag", ")", ")", "\n", "variables", ".", "append", "(", "\"OptNetPNL\"", ")", "\n", "variables", ".", "append", "(", "\"OptReward\"", ")", "\n", "\n", "self", ".", "variables", "=", "variables", "\n", "self", ".", "rnd_state", "=", "rnd_state", "\n", "self", ".", "n_seeds", "=", "n_seeds", "\n", "self", ".", "N_test", "=", "N_test", "\n", "self", ".", "tag", "=", "tag", "\n", "self", ".", "savedpath", "=", "savedpath", "\n", "self", ".", "experiment_type", "=", "experiment_type", "\n", "self", ".", "env_cls", "=", "env_cls", "\n", "self", ".", "MV_res", "=", "MV_res", "\n", "self", ".", "mv_solution", "=", "mv_solution", "\n", "self", ".", "stochastic_policy", "=", "stochastic_policy", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test": [[66, 343], ["numpy.random.RandomState", "range", "test.Out_sample_vs_gp._collect_results", "test.Out_sample_vs_gp.env_cls", "test.Out_sample_vs_gp.test_env.reset", "test.Out_sample_vs_gp.test_env.opt_reset", "test.Out_sample_vs_gp.test_env.opt_trading_rate_disc_loads", "tqdm.tqdm.tqdm", "gin.query_parameter", "utils.simulation.DataHandler", "utils.simulation.DataHandler", "utils.simulation.DataHandler.generate_returns", "gin.bind_parameter", "utils.simulation.DataHandler.generate_returns", "utils.simulation.DataHandler.estimate_parameters", "utils.tools.get_action_boundaries", "utils.spaces.ActionSpace", "test.Out_sample_vs_gp.test_env.opt_reset", "test.Out_sample_vs_gp.test_env.opt_step", "test.Out_sample_vs_gp.test_env.store_results", "list", "list", "list", "list", "pnl.cumsum", "rew.cumsum", "numpy.array().mean", "numpy.array().std", "numpy.array().mean", "numpy.array().std", "avg_pnls.append", "avg_rews.append", "avg_srs.append", "abs_pnl_rl.append", "abs_pnl_gp.append", "abs_rew_rl.append", "abs_rew_gp.append", "abs_sr_rl.append", "abs_sr_gp.append", "abs_hold_rl.append", "abs_hold_gp.append", "avg_pnlstd.append", "avg_pdist.append", "gin.query_parameter", "range", "test_agent.greedy_action", "test.Out_sample_vs_gp.test_env.store_results", "test.Out_sample_vs_gp.test_env.mv_step", "test.Out_sample_vs_gp.test_env.store_results", "filter", "filter", "filter", "filter", "abs_wealth_rl.append", "abs_wealth_gp.append", "utils.tools.get_bet_size", "test.Out_sample_vs_gp.test_env.MV_res_step", "test.Out_sample_vs_gp.test_env.step", "test_agent.model.eval", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "CurrState.to.to.to", "test.Out_sample_vs_gp.test_env.store_results", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "test_agent.model", "utils.tools.get_bet_size", "test.Out_sample_vs_gp.test_env.MV_res_step", "test.Out_sample_vs_gp.test_env.step", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "CurrState.to.to.unsqueeze", "dist.sample", "utils.math_tools.unscale_asymmetric_action", "CurrState.to.to.cpu", "CurrState.to.to.cpu", "utils.math_tools.unscale_asymmetric_action", "utils.math_tools.unscale_action", "torch.max", "torch.max", "torch.max", "torch.max", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp._collect_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.estimate_parameters", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_action_boundaries", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.greedy_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.mv_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_bet_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_bet_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_asymmetric_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_asymmetric_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_action"], ["", "def", "run_test", "(", "self", ",", "test_agent", ":", "object", ",", "it", ":", "int", "=", "0", ",", "return_output", ":", "bool", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "rng_test", "=", "np", ".", "random", ".", "RandomState", "(", "self", ".", "rnd_state", ")", "\n", "\n", "avg_pnls", "=", "[", "]", "\n", "avg_rews", "=", "[", "]", "\n", "avg_srs", "=", "[", "]", "\n", "abs_pnl_rl", "=", "[", "]", "\n", "abs_pnl_gp", "=", "[", "]", "\n", "abs_rew_rl", "=", "[", "]", "\n", "abs_rew_gp", "=", "[", "]", "\n", "abs_sr_rl", "=", "[", "]", "\n", "abs_sr_gp", "=", "[", "]", "\n", "abs_hold_rl", "=", "[", "]", "\n", "abs_hold_gp", "=", "[", "]", "\n", "avg_pnlstd", "=", "[", "]", "\n", "avg_pdist", "=", "[", "]", "\n", "abs_wealth_rl", "=", "[", "]", "\n", "abs_wealth_gp", "=", "[", "]", "\n", "\n", "for", "s", "in", "range", "(", "self", ".", "n_seeds", ")", ":", "\n", "            ", "if", "'alpha'", "in", "gin", ".", "query_parameter", "(", "'%INP_TYPE'", ")", ":", "\n", "                ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "self", ".", "N_test", ",", "rng", "=", "self", ".", "rng_test", ")", "\n", "", "else", ":", "\n", "                ", "data_handler", "=", "DataHandler", "(", "N_train", "=", "self", ".", "N_test", ",", "rng", "=", "self", ".", "rng_test", ")", "\n", "\n", "", "if", "self", ".", "experiment_type", "==", "\"GP\"", ":", "\n", "                ", "data_handler", ".", "generate_returns", "(", ")", "\n", "", "else", ":", "\n", "                ", "gin", ".", "bind_parameter", "(", "'return_sampler_garch.seed'", ",", "s", ")", "\n", "data_handler", ".", "generate_returns", "(", ")", "\n", "data_handler", ".", "estimate_parameters", "(", ")", "\n", "\n", "", "if", "data_handler", ".", "datatype", "==", "\"alpha_term_structure\"", "and", "not", "self", ".", "MV_res", ":", "\n", "                ", "action_range", ",", "_", ",", "_", "=", "get_action_boundaries", "(", "\n", "N_train", "=", "self", ".", "N_test", ",", "\n", "f_speed", "=", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "data_handler", ".", "returns", ",", "\n", "factors", "=", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "gin", ".", "query_parameter", "(", "\"%ACTION_RANGE\"", ")", "[", "0", "]", "=", "action_range", "\n", "test_agent", ".", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "self", ".", "test_env", "=", "self", ".", "env_cls", "(", "\n", "N_train", "=", "self", ".", "N_test", ",", "\n", "f_speed", "=", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "data_handler", ".", "returns", ",", "\n", "factors", "=", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "CurrState", "=", "self", ".", "test_env", ".", "reset", "(", ")", "\n", "\n", "CurrOptState", "=", "self", ".", "test_env", ".", "opt_reset", "(", ")", "\n", "OptRate", ",", "DiscFactorLoads", "=", "self", ".", "test_env", ".", "opt_trading_rate_disc_loads", "(", ")", "\n", "if", "self", ".", "mv_solution", ":", "\n", "                ", "CurrMVState", "=", "self", ".", "test_env", ".", "opt_reset", "(", ")", "\n", "\n", "", "for", "i", "in", "tqdm", "(", "iterable", "=", "range", "(", "self", ".", "N_test", "+", "1", ")", ",", "desc", "=", "\"Testing DQNetwork\"", ")", ":", "\n", "\n", "                ", "if", "self", ".", "tag", "==", "\"DQN\"", ":", "\n", "\n", "                    ", "side_only", "=", "test_agent", ".", "action_space", ".", "side_only", "\n", "\n", "action", ",", "qvalues", "=", "test_agent", ".", "greedy_action", "(", "\n", "CurrState", ",", "side_only", "=", "side_only", "\n", ")", "\n", "if", "side_only", ":", "\n", "                        ", "action", "=", "get_bet_size", "(", "\n", "qvalues", ",", "\n", "action", ",", "\n", "action_limit", "=", "test_agent", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "\n", "zero_action", "=", "test_agent", ".", "action_space", ".", "zero_action", ",", "\n", "rng", "=", "self", ".", "rng", ",", "\n", ")", "\n", "\n", "", "if", "self", ".", "MV_res", ":", "\n", "                        ", "NextState", ",", "Result", ",", "_", "=", "self", ".", "test_env", ".", "MV_res_step", "(", "\n", "CurrState", ",", "action", ",", "i", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "NextState", ",", "Result", ",", "NextFactors", "=", "self", ".", "test_env", ".", "step", "(", "\n", "CurrState", ",", "action", ",", "i", ",", "\n", ")", "\n", "", "self", ".", "test_env", ".", "store_results", "(", "Result", ",", "i", ")", "\n", "\n", "", "elif", "self", ".", "tag", "==", "\"PPO\"", ":", "\n", "                    ", "side_only", "=", "test_agent", ".", "action_space", ".", "side_only", "\n", "test_agent", ".", "model", ".", "eval", "(", ")", "\n", "CurrState", "=", "torch", ".", "from_numpy", "(", "CurrState", ")", ".", "float", "(", ")", "\n", "CurrState", "=", "CurrState", ".", "to", "(", "test_agent", ".", "device", ")", "\n", "\n", "# PPO actions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "dist", ",", "qvalues", "=", "test_agent", ".", "model", "(", "CurrState", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "if", "test_agent", ".", "policy_type", "==", "\"continuous\"", ":", "\n", "\n", "                        ", "if", "self", ".", "stochastic_policy", ":", "\n", "                            ", "action", "=", "dist", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "                            ", "action", "=", "dist", ".", "mean", "\n", "", "action", "=", "nn", ".", "Tanh", "(", ")", "(", "action", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", "[", "0", "]", "\n", "\n", "if", "self", ".", "MV_res", ":", "\n", "                            ", "action", "=", "unscale_asymmetric_action", "(", "\n", "test_agent", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "test_agent", ".", "action_space", ".", "action_range", "[", "1", "]", ",", "action", "\n", ")", "\n", "", "else", ":", "\n", "                            ", "if", "test_agent", ".", "action_space", ".", "asymmetric", ":", "\n", "                                ", "action", "=", "unscale_asymmetric_action", "(", "\n", "test_agent", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "test_agent", ".", "action_space", ".", "action_range", "[", "1", "]", ",", "action", "\n", ")", "\n", "", "else", ":", "\n", "                                ", "action", "=", "unscale_action", "(", "\n", "test_agent", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "action", "\n", ")", "\n", "\n", "\n", "", "", "", "elif", "test_agent", ".", "policy_type", "==", "\"discrete\"", ":", "\n", "# action = test_agent.action_space.values[dist.sample()]", "\n", "                        ", "action", "=", "test_agent", ".", "action_space", ".", "values", "[", "torch", ".", "max", "(", "dist", ".", "logits", ",", "axis", "=", "1", ")", "[", "1", "]", "]", "\n", "\n", "", "if", "side_only", ":", "\n", "                        ", "action", "=", "get_bet_size", "(", "\n", "qvalues", ",", "\n", "action", ",", "\n", "action_limit", "=", "test_agent", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "\n", "zero_action", "=", "test_agent", ".", "action_space", ".", "zero_action", ",", "\n", "rng", "=", "self", ".", "rng", ",", "\n", ")", "\n", "", "if", "self", ".", "MV_res", ":", "\n", "                        ", "NextState", ",", "Result", "=", "self", ".", "test_env", ".", "MV_res_step", "(", "\n", "CurrState", ".", "cpu", "(", ")", ",", "action", ",", "i", ",", "tag", "=", "\"PPO\"", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "NextState", ",", "Result", ",", "_", "=", "self", ".", "test_env", ".", "step", "(", "\n", "CurrState", ".", "cpu", "(", ")", ",", "action", ",", "i", ",", "tag", "=", "\"PPO\"", "\n", ")", "\n", "\n", "", "self", ".", "test_env", ".", "store_results", "(", "Result", ",", "i", ")", "\n", "\n", "\n", "", "CurrState", "=", "NextState", "\n", "\n", "# benchmark agent", "\n", "\n", "NextOptState", ",", "OptResult", "=", "self", ".", "test_env", ".", "opt_step", "(", "\n", "CurrOptState", ",", "OptRate", ",", "DiscFactorLoads", ",", "i", "\n", ")", "\n", "\n", "self", ".", "test_env", ".", "store_results", "(", "OptResult", ",", "i", ")", "\n", "\n", "CurrOptState", "=", "NextOptState", "\n", "\n", "if", "self", ".", "mv_solution", ":", "\n", "                    ", "NextMVState", ",", "MVResult", "=", "self", ".", "test_env", ".", "mv_step", "(", "\n", "CurrMVState", ",", "i", "\n", ")", "\n", "self", ".", "test_env", ".", "store_results", "(", "MVResult", ",", "i", ")", "\n", "CurrMVState", "=", "NextMVState", "\n", "\n", "\n", "", "", "if", "return_output", ":", "\n", "                ", "return", "self", ".", "test_env", ".", "res_df", "\n", "\n", "", "else", ":", "\n", "\n", "# select interesting variables and express as a percentage of the GP results", "\n", "                ", "pnl_str", "=", "list", "(", "\n", "filter", "(", "lambda", "x", ":", "\"NetPNL_{}\"", ".", "format", "(", "self", ".", "tag", ")", "in", "x", ",", "self", ".", "variables", ")", "\n", ")", "\n", "opt_pnl_str", "=", "list", "(", "filter", "(", "lambda", "x", ":", "\"OptNetPNL\"", "in", "x", ",", "self", ".", "variables", ")", ")", "\n", "rew_str", "=", "list", "(", "\n", "filter", "(", "lambda", "x", ":", "\"Reward_{}\"", ".", "format", "(", "self", ".", "tag", ")", "in", "x", ",", "self", ".", "variables", ")", "\n", ")", "\n", "opt_rew_str", "=", "list", "(", "filter", "(", "lambda", "x", ":", "\"OptReward\"", "in", "x", ",", "self", ".", "variables", ")", ")", "\n", "\n", "# pnl", "\n", "pnl", "=", "self", ".", "test_env", ".", "res_df", "[", "pnl_str", "+", "opt_pnl_str", "]", ".", "iloc", "[", ":", "-", "1", "]", "\n", "cum_pnl", "=", "pnl", ".", "cumsum", "(", ")", "\n", "\n", "if", "(", "\n", "data_handler", ".", "datatype", "==", "\"garch\"", "\n", "or", "data_handler", ".", "datatype", "==", "\"garch_mr\"", "\n", ")", ":", "\n", "                    ", "ref_pnl", "=", "np", ".", "array", "(", "cum_pnl", "[", "pnl_str", "]", ")", "-", "np", ".", "array", "(", "\n", "cum_pnl", "[", "opt_pnl_str", "]", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "ref_pnl", "=", "(", "\n", "np", ".", "array", "(", "cum_pnl", "[", "pnl_str", "]", ")", "/", "np", ".", "array", "(", "cum_pnl", "[", "opt_pnl_str", "]", ")", "\n", ")", "*", "100", "\n", "\n", "# rewards", "\n", "", "rew", "=", "self", ".", "test_env", ".", "res_df", "[", "rew_str", "+", "opt_rew_str", "]", ".", "iloc", "[", ":", "-", "1", "]", "\n", "cum_rew", "=", "rew", ".", "cumsum", "(", ")", "\n", "if", "(", "\n", "data_handler", ".", "datatype", "==", "\"garch\"", "\n", "or", "data_handler", ".", "datatype", "==", "\"garch_mr\"", "\n", ")", ":", "\n", "                    ", "ref_rew", "=", "np", ".", "array", "(", "cum_rew", "[", "rew_str", "]", ")", "-", "np", ".", "array", "(", "\n", "cum_rew", "[", "opt_rew_str", "]", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "ref_rew", "=", "(", "\n", "np", ".", "array", "(", "cum_rew", "[", "rew_str", "]", ")", "/", "np", ".", "array", "(", "cum_rew", "[", "opt_rew_str", "]", ")", "\n", ")", "*", "100", "\n", "\n", "# SR", "\n", "", "mean", "=", "np", ".", "array", "(", "pnl", "[", "pnl_str", "]", ")", ".", "mean", "(", ")", "\n", "std", "=", "np", ".", "array", "(", "pnl", "[", "pnl_str", "]", ")", ".", "std", "(", ")", "\n", "sr", "=", "(", "mean", "/", "std", ")", "*", "(", "252", "**", "0.5", ")", "\n", "\n", "# # Holding", "\n", "# hold = self.test_env.res_df[\"NextHolding_{}\".format(self.tag)].iloc[", "\n", "#     -2", "\n", "# ]  # avoid last observation", "\n", "# opthold = self.test_env.res_df[\"OptNextHolding\"].iloc[-2]", "\n", "\n", "# pdist_avg = (", "\n", "#     (", "\n", "#         self.test_env.res_df[\"NextHolding_{}\".format(self.tag)].values", "\n", "#         - self.test_env.res_df[\"OptNextHolding\"].values", "\n", "#     )", "\n", "#     ** 2", "\n", "# ).mean()", "\n", "\n", "opt_mean", "=", "np", ".", "array", "(", "pnl", "[", "opt_pnl_str", "]", ")", ".", "mean", "(", ")", "\n", "opt_std", "=", "np", ".", "array", "(", "pnl", "[", "opt_pnl_str", "]", ")", ".", "std", "(", ")", "\n", "optsr", "=", "(", "opt_mean", "/", "opt_std", ")", "*", "(", "252", "**", "0.5", ")", "\n", "\n", "perc_SR", "=", "(", "sr", "/", "optsr", ")", "*", "100", "\n", "pnl_std", "=", "(", "std", "/", "opt_std", ")", "*", "100", "\n", "\n", "avg_pnls", ".", "append", "(", "ref_pnl", "[", "-", "1", "]", ")", "\n", "avg_rews", ".", "append", "(", "ref_rew", "[", "-", "1", "]", ")", "\n", "avg_srs", ".", "append", "(", "perc_SR", ")", "\n", "abs_pnl_rl", ".", "append", "(", "cum_pnl", ".", "iloc", "[", "-", "1", "]", ".", "values", "[", "0", "]", ")", "\n", "abs_pnl_gp", ".", "append", "(", "cum_pnl", ".", "iloc", "[", "-", "1", "]", ".", "values", "[", "1", "]", ")", "\n", "abs_rew_rl", ".", "append", "(", "cum_rew", ".", "iloc", "[", "-", "1", "]", ".", "values", "[", "0", "]", ")", "\n", "abs_rew_gp", ".", "append", "(", "cum_rew", ".", "iloc", "[", "-", "1", "]", ".", "values", "[", "1", "]", ")", "\n", "abs_sr_rl", ".", "append", "(", "sr", ")", "\n", "abs_sr_gp", ".", "append", "(", "optsr", ")", "\n", "abs_hold_rl", ".", "append", "(", "0.0", ")", "\n", "# abs_hold_rl.append(hold)", "\n", "abs_hold_gp", ".", "append", "(", "0.0", ")", "\n", "# abs_hold_gp.append(opthold)", "\n", "avg_pnlstd", ".", "append", "(", "pnl_std", ")", "\n", "avg_pdist", ".", "append", "(", "0.0", ")", "\n", "# avg_pdist.append(pdist_avg)", "\n", "\n", "if", "self", ".", "test_env", ".", "cash", ":", "\n", "# Wealth", "\n", "                    ", "wealth", "=", "self", ".", "test_env", ".", "res_df", "[", "\"Wealth_{}\"", ".", "format", "(", "self", ".", "tag", ")", "]", ".", "iloc", "[", ":", "-", "1", "]", "# avoid last observation", "\n", "abs_wealth_rl", ".", "append", "(", "wealth", ".", "iloc", "[", "-", "1", "]", ")", "\n", "optwealth", "=", "self", ".", "test_env", ".", "res_df", "[", "\"OptWealth\"", "]", ".", "iloc", "[", ":", "-", "1", "]", "\n", "abs_wealth_gp", ".", "append", "(", "optwealth", ".", "iloc", "[", "-", "1", "]", ")", "\n", "\n", "\n", "", "", "", "self", ".", "_collect_results", "(", "\n", "avg_pnls", ",", "\n", "avg_rews", ",", "\n", "avg_srs", ",", "\n", "abs_pnl_rl", ",", "\n", "abs_pnl_gp", ",", "\n", "abs_rew_rl", ",", "\n", "abs_rew_gp", ",", "\n", "abs_sr_rl", ",", "\n", "abs_sr_gp", ",", "\n", "abs_hold_rl", ",", "\n", "abs_hold_gp", ",", "\n", "avg_pnlstd", ",", "\n", "avg_pdist", ",", "\n", "it", "=", "it", ",", "\n", "abs_wealthrl", "=", "abs_wealth_rl", ",", "\n", "abs_wealthgp", "=", "abs_wealth_gp", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.init_series_to_fill": [[345, 367], ["pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range"], "methods", ["None"], ["", "def", "init_series_to_fill", "(", "self", ",", "iterations", ")", ":", "\n", "        ", "self", ".", "mean_series_pnl", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "mean_series_rew", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "mean_series_sr", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "mean_series_pnl_std", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n", "self", ".", "abs_series_pnl_rl", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "abs_series_pnl_gp", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n", "self", ".", "abs_series_rew_rl", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "abs_series_rew_gp", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n", "self", ".", "abs_series_sr_rl", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "abs_series_sr_gp", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n", "self", ".", "abs_series_hold_rl", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "abs_series_hold_gp", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n", "self", ".", "mean_series_pdist", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n", "self", ".", "abs_series_wealth_rl", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "self", ".", "abs_series_wealth_gp", "=", "pd", ".", "DataFrame", "(", "index", "=", "range", "(", "1", ")", ",", "columns", "=", "iterations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp._collect_results": [[368, 410], ["numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "_collect_results", "(", "\n", "self", ",", "\n", "pnl", ",", "\n", "rew", ",", "\n", "sr", ",", "\n", "abs_prl", ",", "\n", "abs_pgp", ",", "\n", "abs_rewrl", ",", "\n", "abs_rewgp", ",", "\n", "abs_srrl", ",", "\n", "abs_srgp", ",", "\n", "abs_hold", ",", "\n", "abs_opthold", ",", "\n", "pnl_std", ",", "\n", "pdist_avg", ",", "\n", "it", ",", "\n", "abs_wealthrl", "=", "None", ",", "\n", "abs_wealthgp", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "mean_series_pnl", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "pnl", ")", "\n", "self", ".", "mean_series_rew", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "rew", ")", "\n", "self", ".", "mean_series_sr", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "sr", ")", "\n", "self", ".", "mean_series_pnl_std", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "pnl_std", ")", "\n", "\n", "self", ".", "abs_series_pnl_rl", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_prl", ")", "\n", "self", ".", "abs_series_pnl_gp", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_pgp", ")", "\n", "\n", "self", ".", "abs_series_rew_rl", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_rewrl", ")", "\n", "self", ".", "abs_series_rew_gp", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_rewgp", ")", "\n", "\n", "self", ".", "abs_series_sr_rl", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_srrl", ")", "\n", "self", ".", "abs_series_sr_gp", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_srgp", ")", "\n", "\n", "self", ".", "abs_series_hold_rl", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_hold", ")", "\n", "self", ".", "abs_series_hold_gp", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_opthold", ")", "\n", "\n", "self", ".", "mean_series_pdist", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "pdist_avg", ")", "\n", "\n", "if", "abs_wealthrl", ":", "\n", "            ", "self", ".", "abs_series_wealth_rl", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_wealthrl", ")", "\n", "self", ".", "abs_series_wealth_gp", ".", "loc", "[", "0", ",", "str", "(", "it", ")", "]", "=", "np", ".", "mean", "(", "abs_wealthgp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.save_series": [[413, 542], ["test.Out_sample_vs_gp.mean_series_pnl.to_parquet", "test.Out_sample_vs_gp.mean_series_rew.to_parquet", "test.Out_sample_vs_gp.mean_series_sr.to_parquet", "test.Out_sample_vs_gp.mean_series_pnl_std.to_parquet", "test.Out_sample_vs_gp.abs_series_pnl_rl.to_parquet", "test.Out_sample_vs_gp.abs_series_pnl_gp.to_parquet", "test.Out_sample_vs_gp.abs_series_rew_rl.to_parquet", "test.Out_sample_vs_gp.abs_series_rew_gp.to_parquet", "test.Out_sample_vs_gp.abs_series_sr_rl.to_parquet", "test.Out_sample_vs_gp.abs_series_sr_gp.to_parquet", "test.Out_sample_vs_gp.abs_series_hold_rl.to_parquet", "test.Out_sample_vs_gp.abs_series_hold_gp.to_parquet", "test.Out_sample_vs_gp.mean_series_pdist.to_parquet", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "test.Out_sample_vs_gp.abs_series_wealth_rl.to_parquet", "test.Out_sample_vs_gp.abs_series_wealth_gp.to_parquet", "os.path.join", "os.path.join", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands"], ["", "", "def", "save_series", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "mean_series_pnl", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"NetPnl_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "mean_series_rew", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"Reward_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "mean_series_sr", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"SR_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "\n", "self", ".", "mean_series_pnl_std", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"PnLstd_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "\n", "self", ".", "abs_series_pnl_rl", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsNetPnl_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_pnl_gp", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsNetPnl_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_test", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_rew_rl", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsRew_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_rew_gp", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsRew_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_test", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_sr_rl", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsSR_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_sr_gp", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsSR_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_test", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_hold_rl", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsHold_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_hold_gp", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsHold_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_test", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "\n", "self", ".", "mean_series_pdist", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"Pdist_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_test", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "\n", "if", "self", ".", "test_env", ".", "cash", ":", "\n", "            ", "self", ".", "abs_series_wealth_rl", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsWealth_OOS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_test", ")", ",", "self", ".", "tag", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "self", ".", "abs_series_wealth_gp", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\n", "\"AbsWealth_OOS_{}_GP.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_test", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.scale_action": [[12, 22], ["None"], "function", ["None"], ["def", "scale_action", "(", "action_limit", ",", "action", ")", ":", "\n", "    ", "\"\"\"\n    Rescale the action from [low, high] to [-1, 1]\n    (no need for symmetric action space)\n    :param action_space: (gym.spaces.box.Box)\n    :param action: (np.ndarray)\n    :return: (np.ndarray)\n    \"\"\"", "\n", "low", ",", "high", "=", "-", "action_limit", ",", "action_limit", "\n", "return", "2.0", "*", "(", "(", "action", "-", "low", ")", "/", "(", "high", "-", "low", ")", ")", "-", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_action": [[24, 34], ["None"], "function", ["None"], ["", "def", "unscale_action", "(", "action_limit", ",", "scaled_action", ")", ":", "\n", "    ", "\"\"\"\n    Rescale the action from [-1, 1] to [low, high]\n    (no need for symmetric action space)\n    :param action_space: (gym.spaces.box.Box)\n    :param action: (np.ndarray)\n    :return: (np.ndarray)\n    \"\"\"", "\n", "low", ",", "high", "=", "-", "action_limit", ",", "action_limit", "\n", "return", "low", "+", "(", "0.5", "*", "(", "scaled_action", "+", "1.0", ")", "*", "(", "high", "-", "low", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_asymmetric_action": [[35, 44], ["None"], "function", ["None"], ["", "def", "unscale_asymmetric_action", "(", "low", ",", "high", ",", "scaled_action", ")", ":", "\n", "    ", "\"\"\"\n    Rescale the action from [-1, 1] to [low, high]\n    (no need for symmetric action space)\n    :param action_space: (gym.spaces.box.Box)\n    :param action: (np.ndarray)\n    :return: (np.ndarray)\n    \"\"\"", "\n", "return", "low", "+", "(", "0.5", "*", "(", "scaled_action", "+", "1.0", ")", "*", "(", "high", "-", "low", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.boltzmann": [[46, 68], ["numpy.exp", "np.exp.sum().reshape", "numpy.max", "np.exp.sum"], "function", ["None"], ["", "def", "boltzmann", "(", "x", ":", "np", ".", "ndarray", ",", "T", ":", "Union", "[", "float", "or", "int", "]", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"\n    Get array of Q values and compute the boltmann equation on it\n\n    Parameters\n    ----------\n    x: np.ndarray\n        Array of Q values\n\n    T: float or int\n        Temperature parameter. The greater is the more equal different actions are treated\n\n    Returns\n    ----------\n    y: np.ndarray\n        Array of Q values transformed by the boltmann equation\n\n    \"\"\"", "\n", "e_x", "=", "np", ".", "exp", "(", "(", "x", "-", "np", ".", "max", "(", "x", ")", ")", "/", "T", ")", "\n", "y", "=", "e_x", "/", "e_x", ".", "sum", "(", "axis", "=", "1", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n", "return", "y", "\n", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.__init__": [[21, 73], ["numpy.power", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "PER_e", ",", "\n", "PER_a", ",", "\n", "PER_b", ",", "\n", "final_PER_b", ",", "\n", "PER_b_steps", ",", "\n", "PER_b_growth", ",", "\n", "final_PER_a", ",", "\n", "PER_a_steps", ",", "\n", "PER_a_growth", ",", "\n", "max_experiences", ",", "\n", "rng", ",", "\n", "sample_type", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "PER_e", "=", "PER_e", "\n", "self", ".", "PER_a", "=", "PER_a", "\n", "self", ".", "PER_b", "=", "PER_b", "\n", "self", ".", "final_PER_b", "=", "final_PER_b", "\n", "self", ".", "PER_b_steps", "=", "PER_b_steps", "\n", "self", ".", "PER_b_growth", "=", "PER_b_growth", "\n", "self", ".", "final_PER_a", "=", "final_PER_a", "\n", "self", ".", "PER_a_steps", "=", "PER_a_steps", "\n", "self", ".", "PER_a_growth", "=", "PER_a_growth", "\n", "self", ".", "absolute_error_upper", "=", "np", ".", "power", "(", "10", ",", "6", ")", "\n", "# initialize the counter", "\n", "self", ".", "data_pointer", "=", "0", "\n", "self", ".", "rng", "=", "rng", "\n", "self", ".", "sample_type", "=", "sample_type", "\n", "\n", "# Number of leaf nodes (final nodes) that contains experiences", "\n", "self", ".", "max_experiences", "=", "max_experiences", "# equal to max experiences", "\n", "\n", "# Generate the tree with all nodes values = 0", "\n", "# To understand this calculation (2 * capacity - 1) look at the schema below", "\n", "# Remember we are in a binary node (each node has max 2 children)", "\n", "# so 2x size of leaf (capacity) - 1 (root node)", "\n", "# Parent nodes = capacity - 1", "\n", "# Leaf nodes = capacity", "\n", "self", ".", "tree", "=", "np", ".", "zeros", "(", "2", "*", "max_experiences", "-", "1", ")", "\n", "\n", "# Contains the experiences (so the size of data is capacity)", "\n", "# self.data = np.zeros(capacity, dtype=object)", "\n", "self", ".", "experience", "=", "{", "\n", "\"s\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "\"a\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "\"r\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "\"s2\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "\"f\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "\"unsc_a\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "\"opt_a\"", ":", "np", ".", "zeros", "(", "max_experiences", ",", "dtype", "=", "object", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.add": [[75, 117], ["numpy.max", "exp.items", "exploration.PER_buffer.update"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.update"], ["", "def", "add", "(", "self", ",", "exp", ")", ":", "\n", "        ", "\"\"\"Define add function that will add our priority score in the sumtree leaf and \n        add the experience in data.\n        tree:\n                    0\n                   / \\\n                  0   0\n                 / \\ / \\\n        tree_index  0 0  0  We fill the leaves from left to right\"\"\"", "\n", "\n", "# Find the max priority", "\n", "max_priority", "=", "np", ".", "max", "(", "self", ".", "tree", "[", "-", "self", ".", "max_experiences", ":", "]", ")", "\n", "\n", "# If the max priority = 0 we can't put priority = 0 since this experience will never have a chance to be selected", "\n", "# So we use a minimum priority", "\n", "\n", "if", "max_priority", "==", "0", ":", "\n", "            ", "max_priority", "=", "self", ".", "absolute_error_upper", "\n", "\n", "# Look at what index we want to put the experience", "\n", "# You can check that len(self.tree) - tree_index when self.data_pointer==0", "\n", "# is equal to self.capacity", "\n", "", "tree_index", "=", "self", ".", "data_pointer", "+", "self", ".", "max_experiences", "-", "1", "\n", "\n", "# Update experience", "\n", "# self.data[self.data_pointer] = data", "\n", "# if len(self.experiences['s']) >= self.max_experiences:", "\n", "#     for key in self.experience.keys():", "\n", "#         self.experience[key].pop(0)", "\n", "for", "key", ",", "value", "in", "exp", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "experience", "[", "key", "]", "[", "self", ".", "data_pointer", "]", "=", "value", "\n", "\n", "# Update the leaf", "\n", "", "self", ".", "update", "(", "tree_index", ",", "max_priority", ")", "\n", "\n", "# Add 1 to data_pointer", "\n", "self", ".", "data_pointer", "+=", "1", "\n", "\n", "if", "(", "\n", "self", ".", "data_pointer", ">=", "self", ".", "max_experiences", "\n", ")", ":", "# If we're above the capacity, we go back to first index (we overwrite)", "\n", "            ", "self", ".", "data_pointer", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.update": [[118, 129], ["None"], "methods", ["None"], ["", "", "def", "update", "(", "self", ",", "tree_index", ",", "priority", ")", ":", "\n", "        ", "\"\"\"Create function to update the leaf priority score and propagate the change through tree\"\"\"", "\n", "# Change = new priority score - former priority score", "\n", "change", "=", "priority", "-", "self", ".", "tree", "[", "tree_index", "]", "\n", "self", ".", "tree", "[", "tree_index", "]", "=", "priority", "\n", "\n", "# then propagate the change through tree", "\n", "# this method is faster than the recursive loop", "\n", "while", "tree_index", "!=", "0", ":", "\n", "            ", "tree_index", "=", "(", "tree_index", "-", "1", ")", "//", "2", "\n", "self", ".", "tree", "[", "tree_index", "]", "+=", "change", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.get_leaf": [[130, 154], ["len", "exploration.PER_buffer.experience.items"], "methods", ["None"], ["", "", "def", "get_leaf", "(", "self", ",", "v", ")", ":", "\n", "        ", "parent_index", "=", "0", "\n", "\n", "while", "True", ":", "\n", "            ", "left_child_index", "=", "2", "*", "parent_index", "+", "1", "\n", "right_child_index", "=", "left_child_index", "+", "1", "\n", "\n", "# If we reach bottom, end the search", "\n", "if", "left_child_index", ">=", "len", "(", "self", ".", "tree", ")", ":", "\n", "                ", "leaf_index", "=", "parent_index", "\n", "break", "\n", "", "else", ":", "# downward search, always search for a higher priority node", "\n", "                ", "if", "v", "<=", "self", ".", "tree", "[", "left_child_index", "]", ":", "\n", "                    ", "parent_index", "=", "left_child_index", "\n", "", "else", ":", "\n", "                    ", "v", "-=", "self", ".", "tree", "[", "left_child_index", "]", "\n", "parent_index", "=", "right_child_index", "\n", "\n", "", "", "", "exp_index", "=", "leaf_index", "-", "self", ".", "max_experiences", "+", "1", "\n", "self", ".", "exp_leaf", "=", "{", "\n", "key", ":", "value", "[", "exp_index", "]", "for", "key", ",", "value", "in", "self", ".", "experience", ".", "items", "(", ")", "\n", "}", "\n", "\n", "return", "leaf_index", ",", "self", ".", "tree", "[", "leaf_index", "]", ",", "self", ".", "exp_leaf", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.total_priority": [[155, 158], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "total_priority", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tree", "[", "0", "]", "# Returns the root node", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.sample_batch": [[159, 183], ["numpy.empty", "range", "exploration.PER_buffer.rng.uniform", "exploration.PER_buffer.get_leaf", "data.items", "minibatch[].append"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.get_leaf"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "# Create a minibatch array that will contains the minibatch", "\n", "        ", "minibatch", "=", "{", "\"s\"", ":", "[", "]", ",", "\"a\"", ":", "[", "]", ",", "\"r\"", ":", "[", "]", ",", "\"s2\"", ":", "[", "]", ",", "\"f\"", ":", "[", "]", "}", "\n", "\n", "# one could use also np zeros", "\n", "b_idx", "=", "np", ".", "empty", "(", "(", "batch_size", ",", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# Calculate the priority segment", "\n", "# Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges", "\n", "priority_segment", "=", "self", ".", "total_priority", "/", "batch_size", "# priority segment", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# A value is uniformly sample from each range", "\n", "            ", "a", ",", "b", "=", "priority_segment", "*", "i", ",", "priority_segment", "*", "(", "i", "+", "1", ")", "\n", "value", "=", "self", ".", "rng", ".", "uniform", "(", "a", ",", "b", ")", "\n", "\n", "# Experience that correspond to each value is retrieved", "\n", "index", ",", "priority", ",", "data", "=", "self", ".", "get_leaf", "(", "value", ")", "\n", "\n", "b_idx", "[", "i", "]", "=", "index", "\n", "for", "key", ",", "value", "in", "data", ".", "items", "(", ")", ":", "\n", "                ", "minibatch", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "return", "b_idx", ",", "minibatch", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.batch_update": [[184, 213], ["min", "zip", "numpy.power", "exploration.PER_buffer.update", "numpy.where", "print", "sys.exit", "numpy.power", "print", "sys.exit", "numpy.abs", "numpy.power"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.update"], ["", "def", "batch_update", "(", "self", ",", "tree_idx", ",", "abs_errors", ")", ":", "\n", "\n", "        ", "if", "self", ".", "sample_type", "==", "\"TDerror\"", "or", "self", ".", "sample_type", "==", "\"rewards\"", ":", "\n", "            ", "abs_errors", "+=", "self", ".", "PER_e", "# convert to abs and avoid 0", "\n", "", "elif", "self", ".", "sample_type", "==", "\"diffTDerror\"", ":", "\n", "# variant specified in original paper for stochastic and partially observable env", "\n", "            ", "abs_diff_error", "=", "(", "\n", "np", ".", "abs", "(", "abs_errors", "-", "self", ".", "tree", "[", "tree_idx", "]", ")", "+", "self", ".", "PER_e", "\n", ")", "# abs value of the difference of abs values", "\n", "first_play_idx", "=", "np", ".", "where", "(", "self", ".", "tree", "[", "tree_idx", "]", "==", "np", ".", "power", "(", "10", ",", "6", ")", ")", "\n", "if", "first_play_idx", ":", "\n", "                ", "first_priority", "=", "abs_errors", "[", "first_play_idx", "]", "+", "self", ".", "PER_e", "\n", "abs_diff_error", "[", "first_play_idx", "]", "=", "first_priority", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Sample type for PER not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "# clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)", "\n", "", "self", ".", "PER_a", "=", "min", "(", "self", ".", "final_PER_a", ",", "self", ".", "PER_a", "+", "self", ".", "PER_a_growth", ")", "\n", "if", "self", ".", "sample_type", "==", "\"TDerror\"", "or", "\"rewards\"", ":", "\n", "            ", "ps", "=", "np", ".", "power", "(", "abs_errors", ",", "self", ".", "PER_a", ")", "\n", "", "elif", "self", ".", "sample_type", "==", "\"diffTDerror\"", ":", "\n", "            ", "ps", "=", "np", ".", "power", "(", "abs_diff_error", ",", "self", ".", "PER_a", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Sample type for PER not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "for", "ti", ",", "p", "in", "zip", "(", "tree_idx", ",", "ps", ")", ":", "\n", "            ", "self", ".", "update", "(", "ti", ",", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.GaussianActionNoise.__init__": [[217, 221], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mu", ",", "sigma", ",", "rng", ")", ":", "\n", "        ", "self", ".", "mu", "=", "mu", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "rng", "=", "rng", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.GaussianActionNoise.__call__": [[222, 224], ["exploration.GaussianActionNoise.rng.normal"], "methods", ["None"], ["", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "rng", ".", "normal", "(", "self", ".", "mu", ",", "self", ".", "sigma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.GaussianActionNoise.__repr__": [[225, 227], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"NormalActionNoise(mu={}, sigma={})\"", ".", "format", "(", "self", ".", "mu", ",", "self", ".", "sigma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.OrnsteinUhlenbeckActionNoise.__init__": [[231, 239], ["exploration.OrnsteinUhlenbeckActionNoise.reset"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.reset"], ["    ", "def", "__init__", "(", "self", ",", "mu", ",", "sigma", ",", "theta", ",", "rng", ",", "dt", "=", "1", ",", "x0", "=", "None", ")", ":", "\n", "        ", "self", ".", "theta", "=", "theta", "\n", "self", ".", "mu", "=", "mu", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "dt", "=", "dt", "\n", "self", ".", "x0", "=", "x0", "\n", "self", ".", "rng", "=", "rng", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.OrnsteinUhlenbeckActionNoise.__call__": [[240, 248], ["exploration.OrnsteinUhlenbeckActionNoise.rng.normal", "numpy.sqrt"], "methods", ["None"], ["", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "x", "=", "(", "\n", "self", ".", "x_prev", "\n", "+", "self", ".", "theta", "*", "(", "self", ".", "mu", "-", "self", ".", "x_prev", ")", "*", "self", ".", "dt", "\n", "+", "self", ".", "sigma", "*", "np", ".", "sqrt", "(", "self", ".", "dt", ")", "*", "self", ".", "rng", ".", "normal", "(", "size", "=", "self", ".", "mu", ".", "shape", ")", "\n", ")", "\n", "self", ".", "x_prev", "=", "x", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.OrnsteinUhlenbeckActionNoise.reset": [[249, 251], ["numpy.zeros_like"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "x_prev", "=", "self", ".", "x0", "if", "self", ".", "x0", "is", "not", "None", "else", "np", ".", "zeros_like", "(", "self", ".", "mu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.OrnsteinUhlenbeckActionNoise.__repr__": [[252, 255], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"OrnsteinUhlenbeckActionNoise(mu={}, sigma={})\"", ".", "format", "(", "\n", "self", ".", "mu", ",", "self", ".", "sigma", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._setattrs": [[11, 27], ["cytoolz.dicttoolz.keyfilter", "mixin_core.MixinCore.logging.debug", "cytoolz.dicttoolz.keyfilter.items", "inspect.getargvalues", "mixin_core.MixinCore.logging.debug", "setattr", "inspect.stack"], "methods", ["None"], ["    ", "def", "_setattrs", "(", "self", ",", "name", "=", "None", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Setting attributes of itself by calling function's named\n        arguments.\n        \"\"\"", "\n", "args", "=", "getargvalues", "(", "stack", "(", ")", "[", "1", "]", "[", "0", "]", ")", "[", "-", "1", "]", "\n", "args", "=", "dz", ".", "keyfilter", "(", "lambda", "k", ":", "k", "not", "in", "[", "\"self\"", ",", "\"__class__\"", "]", ",", "args", ")", "\n", "\n", "if", "name", "is", "None", ":", "\n", "            ", "name", "=", "self", ".", "__class__", ".", "__name__", "\n", "\n", "", "self", ".", "logging", ".", "debug", "(", "f\"Creating {name}\"", ")", "\n", "\n", "for", "k", ",", "v", "in", "args", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "logging", ".", "debug", "(", "f\" Setting key-value pair {k}: {v}\"", ")", "\n", "setattr", "(", "self", ",", "k", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore.flatten": [[28, 35], ["hasattr", "mixin_core.MixinCore.flatten", "isinstance", "str"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore.flatten"], ["", "", "def", "flatten", "(", "self", ",", "to_flat", ")", ":", "\n", "        ", "for", "x", "in", "to_flat", ":", "\n", "            ", "if", "hasattr", "(", "x", ",", "\"__iter__\"", ")", "and", "not", "isinstance", "(", "x", ",", "str", ")", ":", "\n", "                ", "for", "y", "in", "self", ".", "flatten", "(", "x", ")", ":", "\n", "                    ", "yield", "y", "\n", "", "", "else", ":", "\n", "                ", "yield", "str", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._save_json": [[36, 45], ["dict", "open", "json.dump", "getattr", "ValueError"], "methods", ["None"], ["", "", "", "def", "_save_json", "(", "self", ",", "save_to", ",", "keys", "=", "None", ",", "trial_dict", "=", "None", ")", ":", "\n", "        ", "if", "keys", ":", "\n", "            ", "trial_dict", "=", "dict", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "                ", "trial_dict", "[", "key", "]", "=", "getattr", "(", "self", ",", "key", ")", "\n", "", "", "elif", "trial_dict", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Provide keys or parameters dictionary.\"", ")", "\n", "", "with", "open", "(", "save_to", ",", "\"w\"", ")", "as", "outfile", ":", "\n", "            ", "json", ".", "dump", "(", "trial_dict", ",", "outfile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._make_dict": [[46, 54], ["dict", "ValueError", "getattr"], "methods", ["None"], ["", "", "def", "_make_dict", "(", "self", ",", "keys", ")", ":", "\n", "        ", "if", "keys", ":", "\n", "            ", "trial_dict", "=", "dict", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "                ", "trial_dict", "[", "key", "]", "=", "getattr", "(", "self", ",", "key", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Provide a list of keys\"", ")", "\n", "", "return", "trial_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._make_uuid": [[55, 86], ["hashlib.sha256().hexdigest", "list", "isinstance", "isinstance", "list.append", "mixin_core.MixinCore.flatten", "hashlib.sha256", "AttributeError", "getattr", "ValueError", "unhashed.encode"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore.flatten"], ["", "def", "_make_uuid", "(", "\n", "self", ",", "keys", "=", "None", ",", "truncate", "=", "False", ",", "string", "=", "None", ",", "additional", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "keys", ":", "\n", "            ", "attributes", "=", "[", "]", "\n", "\n", "for", "key", "in", "keys", ":", "\n", "                ", "attributes", ".", "append", "(", "getattr", "(", "self", ",", "key", ")", ")", "\n", "", "attributes", "=", "list", "(", "self", ".", "flatten", "(", "attributes", ")", ")", "\n", "unhashed", "=", "\"-\"", ".", "join", "(", "attributes", ")", "\n", "", "elif", "string", ":", "\n", "            ", "unhashed", "=", "string", "\n", "", "else", ":", "\n", "            ", "if", "not", "additional", ":", "\n", "                ", "raise", "ValueError", "(", "\"Provide a list of keys or a string of values\"", ")", "\n", "", "else", ":", "\n", "                ", "unhashed", "=", "\"\"", "\n", "\n", "", "", "if", "additional", ":", "\n", "            ", "unhashed", "=", "\"-\"", ".", "join", "(", "[", "unhashed", ",", "\"-\"", ".", "join", "(", "additional", ")", "]", ")", "\n", "\n", "", "hashed", "=", "hashlib", ".", "sha256", "(", "unhashed", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "\n", "\n", "if", "truncate", ":", "\n", "            ", "is_int", "=", "isinstance", "(", "truncate", ",", "int", ")", "\n", "is_bool", "=", "isinstance", "(", "truncate", ",", "bool", ")", "\n", "if", "is_bool", "or", "not", "is_int", ":", "\n", "                ", "raise", "AttributeError", "(", "\"truncation has to be an integer\"", ")", "\n", "", "hashed", "=", "hashed", "[", ":", "truncate", "]", "\n", "\n", "", "return", "hashed", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._set_hash": [[87, 90], ["mixin_core.MixinCore._make_uuid"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._make_uuid"], ["", "def", "_set_hash", "(", "self", ",", "keys", ",", "additional", "=", "None", ")", ":", "\n", "        ", "model_hash", "=", "self", ".", "_make_uuid", "(", "keys", "=", "keys", ",", "additional", "=", "additional", ",", "truncate", "=", "10", ",", ")", "\n", "return", "model_hash", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.__init__": [[132, 278], ["gym.Env.__init__", "pandas.DataFrame.astype", "pandas.DataFrame", "len", "len", "pandas.Series", "cols[].unique", "pandas.DataFrame.reindex", "isinstance", "numpy.allclose", "pandas.DataFrame", "len", "numpy.concatenate", "isinstance", "print", "sys.exit", "numpy.concatenate", "range", "numpy.eye", "numpy.zeros", "str", "str", "numpy.array", "numpy.array", "cols[].index.values.tolist", "range", "list", "numpy.array", "str", "numpy.array().reshape", "numpy.array", "pandas.Series.duplicated", "str", "sum", "numpy.triu_indices", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "HalfLife", ":", "Union", "[", "int", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "Startholding", ":", "Union", "[", "int", ",", "float", "]", ",", "\n", "sigma", ":", "float", ",", "\n", "CostMultiplier", ":", "float", ",", "\n", "kappa", ":", "float", ",", "\n", "N_train", ":", "int", ",", "\n", "discount_rate", ":", "float", ",", "\n", "f_param", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "f_speed", ":", "Union", "[", "float", ",", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "returns", ":", "Union", "[", "list", ",", "np", ".", "ndarray", "]", ",", "\n", "factors", ":", "Union", "[", "list", ",", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "action_limit", ":", "int", "=", "None", ",", "\n", "inp_type", ":", "str", "=", "\"ret\"", ",", "\n", "cost_type", ":", "str", "=", "'quadratic'", ",", "\n", "cm1", ":", "float", "=", "2.89E-4", ",", "\n", "cm2", ":", "float", "=", "7.91E-4", ",", "\n", "reward_type", ":", "str", "=", "'mean_var'", ",", "\n", "dates", ":", "pd", ".", "DatetimeIndex", "=", "None", ",", "\n", "cash", ":", "int", "=", "None", ",", "\n", "multiasset", ":", "bool", "=", "False", ",", "\n", "corr", ":", "int", "=", "None", ",", "\n", "inputs", ":", "list", "=", "None", ",", "\n", "mv_penalty", ":", "bool", "=", "False", ",", "\n", "mv_penalty_coef", ":", "float", "=", "None", ",", "\n", "daily_volume", ":", "float", "=", "None", ",", "\n", "daily_price", ":", "float", "=", "None", ",", "\n", "time_dependent", ":", "bool", "=", "False", "\n", ")", ":", "\n", "\n", "# super(MarketEnv, self).__init__()", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "HalfLife", "=", "HalfLife", "\n", "self", ".", "Startholding", "=", "Startholding", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "cm1", "=", "cm1", "\n", "self", ".", "cm2", "=", "cm2", "\n", "self", ".", "kappa", "=", "kappa", "\n", "self", ".", "N_train", "=", "N_train", "\n", "self", ".", "discount_rate", "=", "discount_rate", "\n", "self", ".", "f_param", "=", "f_param", "\n", "self", ".", "f_speed", "=", "f_speed", "\n", "self", ".", "returns", "=", "returns", "\n", "self", ".", "factors", "=", "factors", "\n", "self", ".", "action_limit", "=", "action_limit", "\n", "self", ".", "inp_type", "=", "inp_type", "\n", "self", ".", "cost_type", "=", "cost_type", "\n", "self", ".", "reward_type", "=", "reward_type", "\n", "self", ".", "multiasset", "=", "multiasset", "\n", "self", ".", "corr", "=", "corr", "\n", "self", ".", "cash", "=", "cash", "\n", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "mv_penalty", "=", "mv_penalty", "\n", "self", ".", "mv_penalty_coef", "=", "mv_penalty_coef", "\n", "self", ".", "daily_volume", "=", "daily_volume", "\n", "self", ".", "daily_price", "=", "daily_price", "\n", "self", ".", "time_dependent", "=", "time_dependent", "\n", "\n", "\n", "if", "cost_type", "==", "'nondiff'", ":", "\n", "            ", "self", ".", "CostMultiplier", "=", "self", ".", "cm2", "/", "(", "0.01", "*", "self", ".", "daily_price", "*", "self", ".", "daily_volume", "*", "self", ".", "sigma", "**", "2", ")", "\n", "", "elif", "cost_type", "==", "'quadratic'", ":", "\n", "            ", "self", ".", "CostMultiplier", "=", "CostMultiplier", "\n", "\n", "", "if", "multiasset", ":", "\n", "            ", "colnames", "=", "(", "[", "\"returns\"", "+", "str", "(", "hl", ")", "for", "hl", "in", "HalfLife", "]", "+", "\n", "[", "\"factor_\"", "+", "str", "(", "h", ")", "for", "hl", "in", "HalfLife", "for", "h", "in", "hl", "]", ")", "\n", "\n", "res_df", "=", "pd", ".", "DataFrame", "(", "\n", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "array", "(", "self", ".", "returns", ")", ",", "np", ".", "array", "(", "self", ".", "factors", ")", "]", ",", "axis", "=", "1", "\n", ")", ",", "\n", "columns", "=", "colnames", ",", "\n", ")", "\n", "self", ".", "n_assets", "=", "len", "(", "HalfLife", ")", "\n", "self", ".", "n_factors", "=", "len", "(", "HalfLife", "[", "0", "]", ")", "\n", "\n", "# Initialize all the names for the columns", "\n", "cols", "=", "pd", ".", "Series", "(", "res_df", ".", "columns", ")", "\n", "for", "dup", "in", "cols", "[", "cols", ".", "duplicated", "(", ")", "]", ".", "unique", "(", ")", ":", "\n", "                ", "cols", "[", "cols", "[", "cols", "==", "dup", "]", ".", "index", ".", "values", ".", "tolist", "(", ")", "]", "=", "[", "dup", "+", "'.'", "+", "str", "(", "i", ")", "if", "i", "!=", "0", "else", "dup", "for", "i", "in", "range", "(", "sum", "(", "cols", "==", "dup", ")", ")", "]", "\n", "", "res_df", ".", "columns", "=", "cols", "\n", "# create names of the new columns", "\n", "names1", "=", "[", "'CurrHolding_PPO'", ",", "'NextHolding_PPO'", ",", "'Action_PPO'", ",", "'OptNextAction'", ",", "'OptNextHolding'", ",", "'MVNextHolding'", "]", "\n", "names1", "=", "[", "n", "+", "'_{}'", ".", "format", "(", "i", ")", "for", "n", "in", "names1", "for", "i", "in", "range", "(", "self", ".", "n_assets", ")", "]", "\n", "names2", "=", "[", "'GrossPNL_PPO'", ",", "'NetPNL_PPO'", ",", "'Risk_PPO'", ",", "'Cost_PPO'", ",", "\n", "'Reward_PPO'", ",", "'TradedAmount_PPO'", ",", "'Cash_PPO'", ",", "'Wealth_PPO'", ",", "\n", "'OptGrossPNL'", ",", "'OptNetPNL'", ",", "'OptRisk'", ",", "'OptCost'", ",", "'OptReward'", ",", "\n", "'OptTradedAmount'", ",", "'OptCash'", ",", "'OptWealth'", ",", "'MVReward'", ",", "'MVWealth'", "]", "\n", "names", "=", "names1", "+", "names2", "\n", "res_df", "=", "res_df", ".", "reindex", "(", "columns", "=", "list", "(", "res_df", ".", "columns", ")", "+", "names", ")", "\n", "\n", "self", ".", "currholding_rl", ",", "self", ".", "nextholding_rl", ",", "self", ".", "action_rl", ",", "self", ".", "optaction", ",", "self", ".", "optholding", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "(", "self", ".", "grosspnl_rl", ",", "self", ".", "netpnl_rl", ",", "self", ".", "risk_rl", ",", "self", ".", "cost_rl", ",", "self", ".", "reward_rl", ",", "\n", "self", ".", "tradedamount_rl", ",", "self", ".", "cash_rl", ",", "self", ".", "wealth_rl", ")", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "(", "self", ".", "grosspnl_opt", ",", "self", ".", "netpnl_opt", ",", "self", ".", "risk_opt", ",", "self", ".", "cost_opt", ",", "\n", "self", ".", "reward_opt", ",", "self", ".", "tradedamount_opt", ",", "self", ".", "cash_opt", ",", "self", ".", "wealth_opt", ")", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "self", ".", "mvholding", ",", "self", ".", "mvreward", ",", "self", ".", "mvwealth", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "if", "self", ".", "cash", ":", "\n", "\n", "                ", "self", ".", "holding_ts", "=", "[", "[", "self", ".", "Startholding", "]", "*", "self", ".", "n_assets", "]", "\n", "self", ".", "cash_ts", "=", "[", "cash", "]", "\n", "self", ".", "traded_amount", "=", "0.0", "\n", "self", ".", "costs", "=", "0.0", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "corr", ",", "float", ")", ":", "\n", "                ", "cov_matrix", "=", "np", ".", "eye", "(", "self", ".", "n_assets", ",", "self", ".", "n_assets", ")", "*", "self", ".", "sigma", "**", "2", "\n", "cov_matrix", "[", "cov_matrix", "==", "0", "]", "=", "self", ".", "corr", "*", "self", ".", "sigma", "**", "2", "\n", "", "elif", "isinstance", "(", "self", ".", "corr", ",", "list", ")", ":", "\n", "                ", "cov_matrix", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_assets", ",", "self", ".", "n_assets", ")", ")", "\n", "cov_matrix", "[", "np", ".", "triu_indices", "(", "cov_matrix", ".", "shape", "[", "0", "]", ",", "k", "=", "1", ")", "]", "=", "np", ".", "array", "(", "self", ".", "corr", ")", "*", "self", ".", "sigma", "**", "2", "\n", "cov_matrix", "=", "cov_matrix", "+", "cov_matrix", ".", "T", "\n", "cov_matrix", "[", "np", ".", "where", "(", "cov_matrix", "==", "0", ")", "]", "=", "self", ".", "sigma", "**", "2", "\n", "", "if", "np", ".", "allclose", "(", "cov_matrix", ",", "cov_matrix", ".", "T", ")", ":", "\n", "                ", "self", ".", "cov_matrix", "=", "cov_matrix", "\n", "", "else", ":", "\n", "                ", "print", "(", "'Created a Covariance matrix which is not symmetric!'", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "colnames", "=", "[", "\"returns\"", "]", "+", "[", "\"factor_\"", "+", "str", "(", "hl", ")", "for", "hl", "in", "HalfLife", "]", "\n", "\n", "res_df", "=", "pd", ".", "DataFrame", "(", "\n", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "array", "(", "self", ".", "returns", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "np", ".", "array", "(", "self", ".", "factors", ")", "]", ",", "axis", "=", "1", "\n", ")", ",", "\n", "columns", "=", "colnames", ",", "\n", ")", "\n", "\n", "self", ".", "n_assets", "=", "1", "\n", "self", ".", "n_factors", "=", "len", "(", "HalfLife", ")", "\n", "\n", "if", "cash", ":", "\n", "                ", "self", ".", "cash", "=", "cash", "\n", "self", ".", "holding_ts", "=", "[", "self", ".", "Startholding", "]", "\n", "self", ".", "cash_ts", "=", "[", "cash", "]", "\n", "self", ".", "traded_amount", "=", "0.0", "\n", "self", ".", "costs", "=", "0.0", "\n", "\n", "", "", "self", ".", "dates", "=", "dates", "\n", "res_df", "=", "res_df", ".", "astype", "(", "np", ".", "float32", ")", "\n", "self", ".", "res_df", "=", "res_df", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.get_state_dim": [[280, 283], ["env.MarketEnv.reset"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.reset"], ["", "def", "get_state_dim", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "reset", "(", ")", "\n", "return", "state", ".", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.reset": [[284, 298], ["numpy.array", "numpy.array", "numpy.append", "numpy.append", "len", "len"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "        ", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "            ", "if", "self", ".", "time_dependent", ":", "\n", "                ", "currState", "=", "np", ".", "array", "(", "[", "self", ".", "returns", "[", "0", "]", ",", "len", "(", "self", ".", "returns", ")", "-", "2", ",", "self", ".", "Startholding", "]", ")", "\n", "", "else", ":", "\n", "                ", "currState", "=", "np", ".", "array", "(", "[", "self", ".", "returns", "[", "0", "]", ",", "self", ".", "Startholding", "]", ")", "\n", "", "return", "currState", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "            ", "if", "self", ".", "time_dependent", ":", "\n", "                ", "currState", "=", "np", ".", "append", "(", "self", ".", "factors", "[", "0", "]", ",", "[", "len", "(", "self", ".", "returns", ")", "-", "2", ",", "self", ".", "Startholding", "]", ")", "\n", "", "else", ":", "\n", "                ", "currState", "=", "np", ".", "append", "(", "self", ".", "factors", "[", "0", "]", ",", "self", ".", "Startholding", ")", "\n", "", "return", "currState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.step": [[299, 328], ["env.MarketEnv._getreward", "numpy.array", "numpy.array", "numpy.append", "numpy.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward"], ["", "", "def", "step", "(", "\n", "self", ",", "\n", "currState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "shares_traded", ":", "int", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"DQN\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "]", ":", "\n", "# This is the only environment in which we can run tests with alpha decay inputs in a model free setting", "\n", "# It is not implemented in the enviroment with cash because we focused on Res RL setting", "\n", "\n", "        ", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "\n", "nextHolding", "=", "currState", "[", "-", "1", "]", "+", "shares_traded", "\n", "\n", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "            ", "if", "self", ".", "time_dependent", ":", "\n", "                ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "len", "(", "self", ".", "returns", ")", "-", "2", "-", "(", "iteration", "+", "1", ")", ",", "nextHolding", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "                ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "nextHolding", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "            ", "if", "self", ".", "time_dependent", ":", "\n", "                ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "[", "len", "(", "self", ".", "returns", ")", "-", "2", "-", "(", "iteration", "+", "1", ")", ",", "nextHolding", "]", ")", "\n", "", "else", ":", "\n", "                ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "nextHolding", ")", "\n", "\n", "", "", "Result", "=", "self", ".", "_getreward", "(", "currState", ",", "nextState", ",", "iteration", ",", "tag", ")", "\n", "\n", "return", "nextState", ",", "Result", ",", "nextFactors", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.MV_res_step": [[329, 373], ["env.MarketEnv._getreward", "numpy.sum", "numpy.array", "numpy.array", "numpy.append", "numpy.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward"], ["", "def", "MV_res_step", "(", "\n", "self", ",", "\n", "currState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "shares_traded", ":", "int", ",", "\n", "iteration", ":", "int", ",", "\n", "output_action", ":", "bool", "=", "False", ",", "\n", "tag", ":", "str", "=", "\"DQN\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "        ", "CurrHolding", "=", "currState", "[", "-", "1", "]", "\n", "if", "self", ".", "inp_type", "==", "'alpha'", ":", "\n", "            ", "curr_alpha", "=", "currState", "[", "0", "]", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", ")", "*", "curr_alpha", "\n", "", "else", ":", "\n", "            ", "CurrFactors", "=", "self", ".", "factors", "[", "iteration", "]", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", ")", "*", "np", ".", "sum", "(", "\n", "self", ".", "f_param", "*", "CurrFactors", "\n", ")", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "# Compute optimal markovitz action", "\n", "", "MV_action", "=", "OptNextHolding", "-", "CurrHolding", "\n", "if", "output_action", ":", "\n", "            ", "return", "MV_action", "\n", "\n", "", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "nextHolding", "=", "currState", "[", "-", "1", "]", "+", "MV_action", "*", "(", "1", "-", "shares_traded", ")", "\n", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "            ", "if", "self", ".", "time_dependent", ":", "\n", "                ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "len", "(", "self", ".", "returns", ")", "-", "2", "-", "(", "iteration", "+", "1", ")", ",", "nextHolding", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "                ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "nextHolding", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "            ", "if", "self", ".", "time_dependent", ":", "\n", "                ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "[", "len", "(", "self", ".", "returns", ")", "-", "2", "-", "(", "iteration", "+", "1", ")", ",", "nextHolding", "]", ")", "\n", "", "else", ":", "\n", "                ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "nextHolding", ")", "\n", "\n", "", "", "Result", "=", "self", ".", "_getreward", "(", "\n", "currState", ",", "nextState", ",", "iteration", ",", "tag", ",", "res_action", "=", "shares_traded", "\n", ")", "\n", "\n", "return", "nextState", ",", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.discrete_reset": [[374, 382], ["numpy.array", "env.MarketEnv._find_nearest_return", "env.MarketEnv._find_nearest_holding"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._find_nearest_return", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._find_nearest_holding"], ["", "def", "discrete_reset", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "discretecurrState", "=", "np", ".", "array", "(", "\n", "[", "\n", "self", ".", "_find_nearest_return", "(", "self", ".", "returns", "[", "0", "]", ")", ",", "\n", "self", ".", "_find_nearest_holding", "(", "self", ".", "Startholding", ")", ",", "\n", "]", "\n", ")", "\n", "return", "discretecurrState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.discrete_step": [[383, 398], ["env.MarketEnv._find_nearest_return", "env.MarketEnv._find_nearest_holding", "numpy.array", "env.MarketEnv._getreward"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._find_nearest_return", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._find_nearest_holding", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward"], ["", "def", "discrete_step", "(", "\n", "self", ",", "\n", "discretecurrState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "shares_traded", ":", "int", ",", "\n", "iteration", ":", "int", ",", "\n", ")", "->", "Tuple", "[", "\n", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "\n", "]", ":", "# TODO implement here decoupling if needed", "\n", "        ", "discretenextRet", "=", "self", ".", "_find_nearest_return", "(", "self", ".", "returns", "[", "iteration", "+", "1", "]", ")", "\n", "discretenextHolding", "=", "self", ".", "_find_nearest_holding", "(", "\n", "discretecurrState", "[", "1", "]", "+", "shares_traded", "\n", ")", "\n", "discretenextState", "=", "np", ".", "array", "(", "[", "discretenextRet", ",", "discretenextHolding", "]", ")", "\n", "Result", "=", "self", ".", "_getreward", "(", "discretecurrState", ",", "discretenextState", ",", "iteration", ",", "\"Q\"", ")", "\n", "return", "discretenextState", ",", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.opt_reset": [[399, 404], ["numpy.array"], "methods", ["None"], ["", "def", "opt_reset", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "currOptState", "=", "np", ".", "array", "(", "\n", "[", "self", ".", "returns", "[", "0", "]", ",", "self", ".", "factors", "[", "0", "]", ",", "self", ".", "Startholding", "]", ",", "dtype", "=", "object", "\n", ")", "\n", "return", "currOptState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.opt_step": [[405, 427], ["env.MarketEnv._get_opt_reward", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._get_opt_reward"], ["", "def", "opt_step", "(", "\n", "self", ",", "\n", "currOptState", ":", "Tuple", ",", "\n", "OptRate", ":", "float", ",", "\n", "DiscFactorLoads", ":", "np", ".", "ndarray", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"Opt\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", "]", ":", "\n", "\n", "        ", "OptCurrHolding", "=", "currOptState", "[", "-", "1", "]", "\n", "CurrFactors", "=", "currOptState", "[", "1", "]", "\n", "# Optimal traded quantity between period", "\n", "OptNextHolding", "=", "(", "1", "-", "OptRate", ")", "*", "OptCurrHolding", "+", "OptRate", "*", "(", "\n", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", "\n", ")", "*", "np", ".", "sum", "(", "DiscFactorLoads", "*", "CurrFactors", ")", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextReturns", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "nextOptState", "=", "(", "nextReturns", ",", "nextFactors", ",", "OptNextHolding", ")", "\n", "\n", "OptResult", "=", "self", ".", "_get_opt_reward", "(", "currOptState", ",", "nextOptState", ",", "tag", ")", "\n", "\n", "return", "nextOptState", ",", "OptResult", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.mv_step": [[428, 446], ["env.MarketEnv._get_opt_reward", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._get_opt_reward"], ["", "def", "mv_step", "(", "\n", "self", ",", "currOptState", ":", "Tuple", ",", "iteration", ":", "int", ",", "tag", ":", "str", "=", "\"MV\"", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", "]", ":", "\n", "\n", "        ", "CurrFactors", "=", "currOptState", "[", "1", "]", "\n", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", ")", "*", "np", ".", "sum", "(", "\n", "self", ".", "f_param", "*", "CurrFactors", "\n", ")", "\n", "\n", "nextReturns", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextOptState", "=", "(", "nextReturns", ",", "nextFactors", ",", "OptNextHolding", ")", "\n", "\n", "OptResult", "=", "self", ".", "_get_opt_reward", "(", "currOptState", ",", "nextOptState", ",", "tag", ")", "\n", "\n", "return", "nextOptState", ",", "OptResult", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.store_results": [[447, 475], ["Result.keys", "env.MarketEnv.res_df.astype", "Result.keys", "isinstance", "isinstance", "enumerate", "isinstance", "isinstance", "isinstance", "isinstance", "Result[].item", "Result[].item", "range", "len"], "methods", ["None"], ["", "def", "store_results", "(", "self", ",", "Result", ":", "dict", ",", "iteration", ":", "int", ")", ":", "\n", "\n", "        ", "if", "iteration", "==", "0", ":", "\n", "            ", "for", "key", "in", "Result", ".", "keys", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "Result", "[", "key", "]", ",", "list", ")", "or", "isinstance", "(", "Result", "[", "key", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "                    ", "for", "i", ",", "k", "in", "enumerate", "(", "Result", "[", "key", "]", ")", ":", "\n", "                        ", "self", ".", "res_df", "[", "key", "+", "'_{}'", ".", "format", "(", "i", ")", "]", "=", "0.0", "\n", "self", ".", "res_df", ".", "at", "[", "iteration", ",", "key", "+", "'_{}'", ".", "format", "(", "i", ")", "]", "=", "k", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "res_df", "[", "key", "]", "=", "0.0", "\n", "if", "isinstance", "(", "Result", "[", "key", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                        ", "self", ".", "res_df", ".", "at", "[", "iteration", ",", "key", "]", "=", "Result", "[", "key", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "res_df", ".", "at", "[", "iteration", ",", "key", "]", "=", "Result", "[", "key", "]", "\n", "", "", "", "self", ".", "res_df", "=", "self", ".", "res_df", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "\n", "            ", "for", "key", "in", "Result", ".", "keys", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "Result", "[", "key", "]", ",", "list", ")", "or", "isinstance", "(", "Result", "[", "key", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "                    ", "name", "=", "key", "\n", "suffixes", "=", "[", "'_{}'", ".", "format", "(", "i", ")", "for", "i", "in", "range", "(", "len", "(", "Result", "[", "key", "]", ")", ")", "]", "\n", "names", "=", "[", "name", "+", "s", "for", "s", "in", "suffixes", "]", "\n", "self", ".", "res_df", ".", "loc", "[", "iteration", ",", "names", "]", "=", "Result", "[", "key", "]", "\n", "", "else", ":", "\n", "                    ", "if", "isinstance", "(", "Result", "[", "key", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                        ", "self", ".", "res_df", ".", "at", "[", "iteration", ",", "key", "]", "=", "Result", "[", "key", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "res_df", ".", "at", "[", "iteration", ",", "key", "]", "=", "Result", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.save_outputs": [[476, 517], ["env.MarketEnv.res_df.to_parquet", "env.MarketEnv.res_df.to_parquet", "env.MarketEnv.res_df.to_parquet", "env.MarketEnv.res_df.to_parquet", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands"], ["", "", "", "", "", "def", "save_outputs", "(", "self", ",", "savedpath", ",", "test", "=", "None", ",", "iteration", "=", "None", ",", "include_dates", "=", "False", ")", ":", "\n", "\n", "        ", "if", "not", "test", ":", "\n", "            ", "if", "include_dates", ":", "\n", "                ", "self", ".", "res_df", "[", "\"date\"", "]", "=", "self", ".", "dates", "\n", "self", ".", "res_df", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "savedpath", ",", "\n", "\"Results_{}.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_train", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "res_df", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "savedpath", ",", "\n", "\"Results_{}.parquet.gzip\"", ".", "format", "(", "format_tousands", "(", "self", ".", "N_train", ")", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "include_dates", ":", "\n", "                ", "self", ".", "res_df", "[", "\"date\"", "]", "=", "self", ".", "dates", "\n", "self", ".", "res_df", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "savedpath", ",", "\n", "\"TestResults_{}_iteration_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_train", ")", ",", "iteration", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "res_df", ".", "to_parquet", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "savedpath", ",", "\n", "\"TestResults_{}_iteration_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "self", ".", "N_train", ")", ",", "iteration", "\n", ")", ",", "\n", ")", ",", "\n", "compression", "=", "\"gzip\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.opt_trading_rate_disc_loads": [[519, 539], ["numpy.sqrt", "numpy.exp"], "methods", ["None"], ["", "", "", "def", "opt_trading_rate_disc_loads", "(", "self", ")", "->", "Tuple", "[", "float", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "# 1 percent annualized discount rate (same rate of Ritter)", "\n", "        ", "rho", "=", "1", "-", "np", ".", "exp", "(", "-", "self", ".", "discount_rate", "/", "260", ")", "\n", "\n", "# kappa is the risk aversion, CostMultiplier the parameter for trading cost", "\n", "num1", "=", "self", ".", "kappa", "*", "(", "1", "-", "rho", ")", "+", "self", ".", "CostMultiplier", "*", "rho", "\n", "num2", "=", "np", ".", "sqrt", "(", "\n", "num1", "**", "2", "+", "4", "*", "self", ".", "kappa", "*", "self", ".", "CostMultiplier", "*", "(", "1", "-", "rho", ")", "**", "2", "\n", ")", "\n", "den", "=", "2", "*", "(", "1", "-", "rho", ")", "\n", "a", "=", "(", "-", "num1", "+", "num2", ")", "/", "den", "\n", "\n", "OptRate", "=", "a", "/", "self", ".", "CostMultiplier", "\n", "\n", "DiscFactorLoads", "=", "self", ".", "f_param", "/", "(", "\n", "1", "+", "self", ".", "f_speed", "*", "(", "(", "OptRate", "*", "self", ".", "CostMultiplier", ")", "/", "self", ".", "kappa", ")", "\n", ")", "\n", "\n", "return", "OptRate", ",", "DiscFactorLoads", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._find_nearest_return": [[541, 545], ["numpy.asarray", "numpy.abs().argmin", "numpy.abs"], "methods", ["None"], ["", "def", "_find_nearest_return", "(", "self", ",", "value", ")", "->", "float", ":", "\n", "        ", "array", "=", "np", ".", "asarray", "(", "self", ".", "returns_space", ".", "values", ")", "\n", "idx", "=", "(", "np", ".", "abs", "(", "array", "-", "value", ")", ")", ".", "argmin", "(", ")", "\n", "return", "array", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._find_nearest_holding": [[546, 550], ["numpy.asarray", "numpy.abs().argmin", "numpy.abs"], "methods", ["None"], ["", "def", "_find_nearest_holding", "(", "self", ",", "value", ")", "->", "Union", "[", "float", ",", "int", "]", ":", "\n", "        ", "array", "=", "np", ".", "asarray", "(", "self", ".", "holding_space", ".", "values", ")", "\n", "idx", "=", "(", "np", ".", "abs", "(", "array", "-", "value", ")", ")", ".", "argmin", "(", ")", "\n", "return", "array", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost": [[551, 564], ["numpy.abs"], "methods", ["None"], ["", "def", "_totalcost", "(", "self", ",", "shares_traded", ":", "Union", "[", "float", ",", "int", "]", ")", "->", "Union", "[", "float", ",", "int", "]", ":", "\n", "        ", "if", "self", ".", "cost_type", "==", "'quadratic'", ":", "\n", "            ", "Lambda", "=", "self", ".", "CostMultiplier", "*", "self", ".", "sigma", "**", "2", "\n", "cost", "=", "0.5", "*", "(", "shares_traded", "**", "2", ")", "*", "Lambda", "\n", "", "elif", "self", ".", "cost_type", "==", "'nondiff'", ":", "\n", "#Kyle-Obizhaeva formulation", "\n", "            ", "p", ",", "v", "=", "self", ".", "daily_price", ",", "self", ".", "daily_volume", "\n", "quadcost", "=", "shares_traded", "**", "2", "/", "(", "0.01", "*", "p", "*", "v", ")", "\n", "# Lambda = self.cm2 * self.sigma ** 2", "\n", "# quadcost = 0.5 * (shares_traded ** 2) * Lambda", "\n", "cost", "=", "self", ".", "cm1", "*", "np", ".", "abs", "(", "shares_traded", ")", "+", "self", ".", "cm2", "*", "quadcost", "\n", "\n", "", "return", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._getreward": [[565, 627], ["env.MarketEnv._totalcost"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_getreward", "(", "\n", "self", ",", "\n", "currState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "nextState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", ",", "\n", "res_action", ":", "float", "=", "None", ",", "\n", ")", "->", "dict", ":", "\n", "\n", "# Remember that a state is a tuple (price, holding)", "\n", "# currRet = currState[0]", "\n", "\n", "        ", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "currHolding", "=", "currState", "[", "-", "1", "]", "\n", "nextHolding", "=", "nextState", "[", "-", "1", "]", "\n", "\n", "shares_traded", "=", "nextHolding", "-", "currHolding", "\n", "GrossPNL", "=", "nextHolding", "*", "nextRet", "\n", "Cost", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "NetPNL", "=", "GrossPNL", "-", "Cost", "\n", "if", "self", ".", "reward_type", "==", "'mean_var'", ":", "\n", "            ", "Risk", "=", "0.5", "*", "self", ".", "kappa", "*", "(", "(", "nextHolding", "**", "2", ")", "*", "(", "self", ".", "sigma", "**", "2", ")", ")", "\n", "Reward", "=", "GrossPNL", "-", "0.5", "*", "self", ".", "kappa", "*", "GrossPNL", "**", "2", "-", "Cost", "#GrossPNL - Risk - Cost CHANGED THE WAY WE COMPUTE THE REWARD", "\n", "", "elif", "self", ".", "reward_type", "==", "'cara'", ":", "\n", "            ", "Reward", "=", "(", "1", "-", "np", ".", "e", "**", "(", "-", "self", ".", "kappa", "*", "NetPNL", ")", ")", "/", "self", ".", "kappa", "\n", "\n", "", "if", "self", ".", "mv_penalty", ":", "\n", "# if self.inp_type == 'alpha':", "\n", "#     next_alpha = nextState[0]", "\n", "#     # Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "#     MVNextHolding = (1 / (self.kappa * (self.sigma) ** 2)) * next_alpha", "\n", "# else:", "\n", "#     NextFactors = self.factors[iteration + 1]", "\n", "#     # Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "#     MVNextHolding = (1 / (self.kappa * (self.sigma) ** 2)) * np.sum(", "\n", "#         self.f_param * NextFactors", "\n", "#     )", "\n", "\n", "# penalty = self.mv_penalty_coef * (MVNextHolding - nextHolding)**2", "\n", "            ", "penalty", "=", "self", ".", "mv_penalty_coef", "*", "(", "currHolding", "-", "nextHolding", ")", "**", "2", "\n", "\n", "Reward", "-=", "penalty", "\n", "\n", "", "Result", "=", "{", "\n", "\"CurrHolding_{}\"", ".", "format", "(", "tag", ")", ":", "currHolding", ",", "\n", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ":", "nextHolding", ",", "\n", "\"Action_{}\"", ".", "format", "(", "tag", ")", ":", "shares_traded", ",", "\n", "\"GrossPNL_{}\"", ".", "format", "(", "tag", ")", ":", "GrossPNL", ",", "\n", "\"NetPNL_{}\"", ".", "format", "(", "tag", ")", ":", "NetPNL", ",", "\n", "\"Cost_{}\"", ".", "format", "(", "tag", ")", ":", "Cost", ",", "\n", "\"Reward_{}\"", ".", "format", "(", "tag", ")", ":", "Reward", ",", "\n", "}", "\n", "if", "res_action", ":", "\n", "            ", "Result", "[", "\"ResAction_{}\"", ".", "format", "(", "tag", ")", "]", "=", "res_action", "\n", "\n", "", "if", "self", ".", "reward_type", "==", "'mean_var'", ":", "\n", "            ", "Result", "[", "\"Risk_{}\"", ".", "format", "(", "tag", ")", "]", "=", "Risk", "\n", "\n", "", "if", "self", ".", "mv_penalty", ":", "\n", "            ", "Result", "[", "\"Penalty_{}\"", ".", "format", "(", "tag", ")", "]", "=", "penalty", "\n", "\n", "", "return", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._get_opt_reward": [[628, 666], ["env.MarketEnv._totalcost"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_get_opt_reward", "(", "\n", "self", ",", "\n", "currOptState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "nextOptState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "tag", ":", "str", ",", "\n", ")", "->", "dict", ":", "\n", "\n", "# Remember that a state is a tuple (price, holding)", "\n", "# currRet = currOptState[0]", "\n", "        ", "nextRet", "=", "nextOptState", "[", "0", "]", "\n", "OptCurrHolding", "=", "currOptState", "[", "-", "1", "]", "\n", "OptNextHolding", "=", "nextOptState", "[", "-", "1", "]", "\n", "\n", "# Traded quantity between period", "\n", "OptNextAction", "=", "OptNextHolding", "-", "OptCurrHolding", "\n", "# Portfolio variation", "\n", "OptGrossPNL", "=", "OptNextHolding", "*", "nextRet", "# currRet", "\n", "# Risk", "\n", "OptRisk", "=", "0.5", "*", "self", ".", "kappa", "*", "(", "(", "OptNextHolding", "**", "2", ")", "*", "(", "self", ".", "sigma", "**", "2", ")", ")", "\n", "# Transaction costs", "\n", "OptCost", "=", "self", ".", "_totalcost", "(", "OptNextAction", ")", "\n", "# Portfolio Variation including costs", "\n", "OptNetPNL", "=", "OptGrossPNL", "-", "OptCost", "\n", "# Compute reward", "\n", "OptReward", "=", "OptGrossPNL", "-", "OptRisk", "-", "OptCost", "\n", "\n", "# Store quantities", "\n", "Result", "=", "{", "\n", "\"{}NextAction\"", ".", "format", "(", "tag", ")", ":", "OptNextAction", ",", "\n", "\"{}NextHolding\"", ".", "format", "(", "tag", ")", ":", "OptNextHolding", ",", "\n", "\"{}GrossPNL\"", ".", "format", "(", "tag", ")", ":", "OptGrossPNL", ",", "\n", "\"{}NetPNL\"", ".", "format", "(", "tag", ")", ":", "OptNetPNL", ",", "\n", "\"{}Risk\"", ".", "format", "(", "tag", ")", ":", "OptRisk", ",", "\n", "\"{}Cost\"", ".", "format", "(", "tag", ")", ":", "OptCost", ",", "\n", "\"{}Reward\"", ".", "format", "(", "tag", ")", ":", "OptReward", ",", "\n", "}", "\n", "\n", "return", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._get_inputs": [[668, 711], ["input_list.extend", "input_list.append", "isinstance", "input_list.extend", "input_list.append", "input_list.extend", "input_list.append", "isinstance", "input_list.extend", "input_list.append", "list", "input_list.extend", "input_list.append", "isinstance", "list", "input_list.extend", "input_list.append", "isinstance", "list", "input_list.extend", "list", "input_list.extend"], "methods", ["None"], ["", "def", "_get_inputs", "(", "self", ",", "reset", ",", "iteration", "=", "None", ")", ":", "\n", "\n", "# could be rewritten better https://stackoverflow.com/questions/25211924/check-every-condition-in-python-if-else-even-if-one-evaluates-to-true", "\n", "        ", "input_list", "=", "[", "]", "\n", "input_type", "=", "self", ".", "inputs", "\n", "if", "reset", ":", "\n", "            ", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "                ", "input_list", ".", "extend", "(", "list", "(", "self", ".", "returns", "[", "0", "]", ")", ")", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "                ", "input_list", ".", "extend", "(", "list", "(", "self", ".", "factors", "[", "0", "]", ")", ")", "\n", "\n", "", "if", "\"sigma\"", "in", "input_type", ":", "\n", "                ", "input_list", ".", "append", "(", "self", ".", "sigma", "**", "2", ")", "\n", "", "if", "\"corr\"", "in", "input_type", ":", "\n", "                ", "if", "isinstance", "(", "self", ".", "corr", ",", "float", ")", ":", "\n", "                    ", "input_list", ".", "append", "(", "self", ".", "corr", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "corr", ",", "list", ")", ":", "\n", "                    ", "input_list", ".", "extend", "(", "self", ".", "corr", ")", "\n", "", "", "if", "\"holding\"", "in", "input_type", ":", "\n", "                ", "input_list", ".", "extend", "(", "[", "self", ".", "Startholding", "]", "*", "self", ".", "n_assets", ")", "\n", "# entire arrays", "\n", "", "if", "\"cash\"", "in", "input_type", ":", "\n", "                ", "input_list", ".", "append", "(", "self", ".", "cash", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "                ", "input_list", ".", "extend", "(", "list", "(", "self", ".", "returns", "[", "iteration", "+", "1", "]", ")", ")", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "                ", "input_list", ".", "extend", "(", "list", "(", "self", ".", "factors", "[", "iteration", "+", "1", "]", ")", ")", "\n", "\n", "", "if", "\"sigma\"", "in", "input_type", ":", "\n", "                ", "input_list", ".", "append", "(", "self", ".", "sigma", "**", "2", ")", "\n", "", "if", "\"corr\"", "in", "input_type", ":", "\n", "                ", "if", "isinstance", "(", "self", ".", "corr", ",", "float", ")", ":", "\n", "                    ", "input_list", ".", "append", "(", "self", ".", "corr", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "corr", ",", "list", ")", ":", "\n", "                    ", "input_list", ".", "extend", "(", "self", ".", "corr", ")", "\n", "", "", "if", "\"holding\"", "in", "input_type", ":", "\n", "                ", "input_list", ".", "extend", "(", "self", ".", "holding_ts", "[", "iteration", "+", "1", "]", ")", "\n", "# entire arrays", "\n", "", "if", "\"cash\"", "in", "input_type", ":", "\n", "                ", "input_list", ".", "append", "(", "self", ".", "cash_ts", "[", "iteration", "+", "1", "]", ")", "\n", "\n", "", "", "return", "input_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv.reset": [[716, 723], ["numpy.array", "numpy.append"], "methods", ["None"], ["    ", "def", "reset", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "            ", "currState", "=", "np", ".", "array", "(", "[", "self", ".", "returns", "[", "0", "]", ",", "self", ".", "Startholding", ",", "self", ".", "cash", "]", ")", "\n", "return", "currState", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "            ", "currState", "=", "np", ".", "append", "(", "self", ".", "factors", "[", "0", "]", ",", "[", "self", ".", "Startholding", ",", "self", ".", "cash", "]", ")", "\n", "return", "currState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv.step": [[725, 759], ["env.CashMarketEnv.holding_ts.append", "env.CashMarketEnv.cash_ts.append", "env.CashMarketEnv._getreward", "env.CashMarketEnv._buy", "numpy.array", "env.CashMarketEnv._sell", "numpy.append"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._buy", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._sell"], ["", "", "def", "step", "(", "\n", "self", ",", "\n", "currState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "action", ":", "float", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"DQN\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "        ", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "\n", "# buy/sell here", "\n", "if", "action", ">", "0", ":", "\n", "            ", "shares_traded", "=", "self", ".", "_buy", "(", "index", "=", "iteration", ",", "cash", "=", "self", ".", "cash_ts", "[", "iteration", "]", ",", "action", "=", "action", ")", "\n", "", "elif", "action", "<", "0", ":", "\n", "            ", "shares_traded", "=", "self", ".", "_sell", "(", "index", "=", "iteration", ",", "holding", "=", "self", ".", "holding_ts", "[", "iteration", "]", ",", "action", "=", "action", ")", "\n", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "# update rules", "\n", "", "nextHolding", "=", "(", "1", "+", "nextRet", ")", "*", "self", ".", "holding_ts", "[", "iteration", "]", "+", "shares_traded", "\n", "self", ".", "holding_ts", ".", "append", "(", "nextHolding", ")", "\n", "nextCash", "=", "self", ".", "cash_ts", "[", "iteration", "]", "+", "self", ".", "traded_amount", "\n", "self", ".", "cash_ts", ".", "append", "(", "nextCash", ")", "\n", "\n", "if", "self", ".", "inp_type", "==", "\"ret\"", ":", "\n", "            ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "nextHolding", ",", "nextCash", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", ":", "\n", "            ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "[", "nextHolding", ",", "nextCash", "]", ")", "\n", "\n", "", "Result", "=", "self", ".", "_getreward", "(", "currState", ",", "nextState", ",", "iteration", ",", "tag", ")", "\n", "\n", "return", "nextState", ",", "Result", ",", "nextFactors", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv.MV_res_step": [[760, 800], ["env.CashMarketEnv.holding_ts.append", "env.CashMarketEnv.cash_ts.append", "env.CashMarketEnv._getreward", "numpy.array", "numpy.sum", "numpy.append"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward"], ["", "def", "MV_res_step", "(", "\n", "self", ",", "\n", "currState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "shares_traded", ":", "int", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"DQN\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "        ", "CurrHolding", "=", "self", ".", "holding_ts", "[", "iteration", "]", "\n", "if", "self", ".", "inp_type", "==", "'alpha'", ":", "\n", "            ", "curr_alpha", "=", "currState", "[", "0", "]", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", ")", "*", "curr_alpha", "\n", "", "else", ":", "\n", "            ", "CurrFactors", "=", "self", ".", "factors", "[", "iteration", "]", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "(", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", ")", "*", "np", ".", "sum", "(", "\n", "self", ".", "f_param", "*", "CurrFactors", "\n", ")", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "# Compute optimal markovitz action", "\n", "", "MV_action", "=", "OptNextHolding", "-", "CurrHolding", "\n", "\n", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "\n", "nextHolding", "=", "(", "1", "+", "nextRet", ")", "*", "CurrHolding", "+", "MV_action", "*", "(", "1", "-", "shares_traded", ")", "\n", "self", ".", "holding_ts", ".", "append", "(", "nextHolding", ")", "\n", "nextCash", "=", "self", ".", "cash_ts", "[", "iteration", "]", "+", "self", ".", "traded_amount", "\n", "self", ".", "cash_ts", ".", "append", "(", "nextCash", ")", "\n", "\n", "if", "self", ".", "inp_type", "==", "\"ret\"", "or", "self", ".", "inp_type", "==", "\"alpha\"", ":", "\n", "            ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "nextHolding", ",", "nextCash", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", "or", "self", ".", "inp_type", "==", "\"alpha_f\"", ":", "\n", "            ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "[", "nextHolding", ",", "nextCash", "]", ")", "\n", "\n", "", "Result", "=", "self", ".", "_getreward", "(", "\n", "iteration", ",", "tag", ",", "res_action", "=", "shares_traded", "\n", ")", "\n", "\n", "return", "nextState", ",", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv.opt_reset": [[802, 805], ["None"], "methods", ["None"], ["", "def", "opt_reset", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "currOptState", "=", "[", "self", ".", "factors", "[", "0", "]", ",", "self", ".", "Startholding", ",", "self", ".", "cash", "]", "\n", "return", "currOptState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv.opt_step": [[806, 839], ["env.CashMarketEnv._opt_trade", "env.CashMarketEnv._get_opt_reward", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._opt_trade", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._get_opt_reward"], ["", "def", "opt_step", "(", "\n", "self", ",", "\n", "currOptState", ":", "Tuple", ",", "\n", "OptRate", ":", "float", ",", "\n", "DiscFactorLoads", ":", "np", ".", "ndarray", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"Opt\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", "]", ":", "\n", "\n", "        ", "CurrFactors", "=", "currOptState", "[", "0", "]", "\n", "OptCurrHolding", "=", "currOptState", "[", "1", "]", "\n", "# Optimal traded quantity between period", "\n", "OptNextHolding", "=", "(", "1", "-", "OptRate", ")", "*", "OptCurrHolding", "+", "OptRate", "*", "(", "\n", "1", "/", "(", "self", ".", "kappa", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", "\n", ")", "*", "np", ".", "sum", "(", "DiscFactorLoads", "*", "CurrFactors", ")", "\n", "\n", "action", "=", "OptNextHolding", "-", "OptCurrHolding", "\n", "\n", "# buy/sell here", "\n", "OptNextHolding", "=", "self", ".", "_opt_trade", "(", "\n", "index", "=", "iteration", ",", "state", "=", "currOptState", ",", "action", "=", "action", "\n", ")", "\n", "\n", "nextCash", "=", "currOptState", "[", "-", "1", "]", "+", "self", ".", "traded_amount", "\n", "nextReturn", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextOptState", "=", "[", "nextFactors", ",", "OptNextHolding", ",", "nextCash", "]", "\n", "\n", "OptResult", "=", "self", ".", "_get_opt_reward", "(", "\n", "currOptState", ",", "nextOptState", ",", "nextReturn", ",", "iteration", ",", "tag", "\n", ")", "\n", "\n", "return", "nextOptState", ",", "OptResult", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv._getreward": [[841, 878], ["isinstance"], "methods", ["None"], ["", "def", "_getreward", "(", "\n", "self", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", ",", "\n", "res_action", ":", "float", "=", "None", ",", "\n", ")", "->", "dict", ":", "\n", "\n", "        ", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "currHolding", "=", "self", ".", "holding_ts", "[", "iteration", "]", "\n", "nextHolding", "=", "self", ".", "holding_ts", "[", "iteration", "+", "1", "]", "\n", "nextCash", "=", "self", ".", "cash_ts", "[", "iteration", "+", "1", "]", "\n", "\n", "shares_traded", "=", "nextHolding", "-", "currHolding", "\n", "NetPNL", "=", "nextHolding", "*", "nextRet", "-", "self", ".", "costs", "\n", "Risk", "=", "0.5", "*", "self", ".", "kappa", "*", "(", "(", "nextHolding", "**", "2", ")", "*", "(", "self", ".", "sigma", "**", "2", ")", ")", "\n", "Reward", "=", "NetPNL", "-", "Risk", "\n", "nextWealth", "=", "nextHolding", "+", "nextCash", "\n", "\n", "Result", "=", "{", "\n", "\"CurrHolding_{}\"", ".", "format", "(", "tag", ")", ":", "currHolding", ",", "\n", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ":", "nextHolding", ",", "\n", "\"Action_{}\"", ".", "format", "(", "tag", ")", ":", "shares_traded", ",", "\n", "\"GrossPNL_{}\"", ".", "format", "(", "tag", ")", ":", "NetPNL", "+", "self", ".", "costs", ",", "\n", "\"NetPNL_{}\"", ".", "format", "(", "tag", ")", ":", "NetPNL", ",", "\n", "\"Risk_{}\"", ".", "format", "(", "tag", ")", ":", "Risk", ",", "\n", "\"Cost_{}\"", ".", "format", "(", "tag", ")", ":", "self", ".", "costs", ",", "\n", "\"Reward_{}\"", ".", "format", "(", "tag", ")", ":", "Reward", ",", "\n", "\"TradedAmount_{}\"", ".", "format", "(", "tag", ")", ":", "self", ".", "traded_amount", ",", "\n", "\"Cash_{}\"", ".", "format", "(", "tag", ")", ":", "nextCash", ",", "\n", "\"Wealth_{}\"", ".", "format", "(", "tag", ")", ":", "nextWealth", ",", "\n", "}", "\n", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "if", "isinstance", "(", "res_action", ",", "float", ")", ":", "\n", "            ", "Result", "[", "\"ResAction_{}\"", ".", "format", "(", "tag", ")", "]", "=", "res_action", "\n", "", "return", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv._sell": [[879, 897], ["min", "env.CashMarketEnv._totalcost", "abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_sell", "(", "self", ",", "index", ":", "int", ",", "holding", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "currholding", "=", "holding", "\n", "\n", "if", "currholding", ">", "0.0", ":", "\n", "# Sell only if current asset is > 0", "\n", "            ", "shares_traded", "=", "min", "(", "abs", "(", "action", ")", ",", "currholding", ")", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "shares_traded", "-", "self", ".", "costs", "\n", "return", "-", "shares_traded", "\n", "\n", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "\n", "", "return", "-", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv._buy": [[898, 908], ["min", "env.CashMarketEnv._totalcost"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_buy", "(", "self", ",", "index", ":", "int", ",", "cash", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "max_tradable_amount", "=", "cash", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "\n", "return", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv._get_opt_reward": [[909, 952], ["None"], "methods", ["None"], ["", "def", "_get_opt_reward", "(", "\n", "self", ",", "\n", "currOptState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "nextOptState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "nextReturn", ":", "float", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", ",", "\n", ")", "->", "dict", ":", "\n", "\n", "        ", "OptCurrHolding", "=", "currOptState", "[", "1", "]", "\n", "OptNextHolding", "=", "nextOptState", "[", "1", "]", "\n", "CurrOptcash", "=", "currOptState", "[", "-", "1", "]", "\n", "NextOptcash", "=", "nextOptState", "[", "-", "1", "]", "\n", "\n", "# Traded quantity between period", "\n", "OptNextAction", "=", "OptNextHolding", "-", "OptCurrHolding", "\n", "# Portfolio variation", "\n", "OptNetPNL", "=", "OptNextHolding", "*", "nextReturn", "-", "self", ".", "costs", "\n", "# Risk", "\n", "OptRisk", "=", "0.5", "*", "self", ".", "kappa", "*", "(", "(", "OptNextHolding", ")", "**", "2", "*", "(", "self", ".", "sigma", ")", "**", "2", ")", "\n", "# Compute reward", "\n", "OptReward", "=", "OptNetPNL", "-", "OptRisk", "\n", "\n", "\n", "nextWealth", "=", "OptNextHolding", "+", "NextOptcash", "#self.prices[iteration + 1] * ", "\n", "\n", "# Store quantities", "\n", "Result", "=", "{", "\n", "\"{}NextAction\"", ".", "format", "(", "tag", ")", ":", "OptNextAction", ",", "\n", "\"{}NextHolding\"", ".", "format", "(", "tag", ")", ":", "OptNextHolding", ",", "\n", "\"{}GrossPNL\"", ".", "format", "(", "tag", ")", ":", "OptNetPNL", "+", "self", ".", "costs", ",", "\n", "\"{}NetPNL\"", ".", "format", "(", "tag", ")", ":", "OptNetPNL", ",", "\n", "\"{}Risk\"", ".", "format", "(", "tag", ")", ":", "OptRisk", ",", "\n", "\"{}Cost\"", ".", "format", "(", "tag", ")", ":", "self", ".", "costs", ",", "\n", "\"{}Reward\"", ".", "format", "(", "tag", ")", ":", "OptReward", ",", "\n", "\"{}TradedAmount\"", ".", "format", "(", "tag", ")", ":", "self", ".", "traded_amount", ",", "\n", "\"{}Cash\"", ".", "format", "(", "tag", ")", ":", "NextOptcash", ",", "\n", "\"{}Wealth\"", ".", "format", "(", "tag", ")", ":", "nextWealth", ",", "\n", "}", "\n", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.CashMarketEnv._opt_trade": [[953, 989], ["min", "env.CashMarketEnv._totalcost", "min", "env.CashMarketEnv._totalcost", "abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_opt_trade", "(", "self", ",", "index", ":", "int", ",", "state", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "if", "action", ">", "0.0", ":", "\n", "\n", "            ", "max_tradable_amount", "=", "state", "[", "-", "1", "]", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "\n", "", "elif", "action", "<", "0.0", ":", "\n", "\n", "            ", "currholding", "=", "state", "[", "1", "]", "\n", "\n", "if", "currholding", ">", "0.0", ":", "\n", "# Sell only if current asset is > 0", "\n", "                ", "shares_traded", "=", "min", "(", "abs", "(", "action", ")", ",", "currholding", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "-", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "\n", "", "else", ":", "\n", "                ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortCashMarketEnv._sell": [[995, 1003], ["env.ShortCashMarketEnv._totalcost", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["    ", "def", "_sell", "(", "self", ",", "index", ":", "int", ",", "holding", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "shares_traded", "=", "action", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "# absolute quantity because now you can sell short and shares_traded can be negative", "\n", "self", ".", "traded_amount", "=", "np", ".", "abs", "(", "shares_traded", ")", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortCashMarketEnv._buy": [[1004, 1013], ["min", "env.ShortCashMarketEnv._totalcost"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_buy", "(", "self", ",", "index", ":", "int", ",", "cash", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "max_tradable_amount", "=", "cash", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortCashMarketEnv._opt_trade": [[1014, 1039], ["min", "env.ShortCashMarketEnv._totalcost", "env.ShortCashMarketEnv._totalcost", "abs", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_opt_trade", "(", "self", ",", "index", ":", "int", ",", "state", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "if", "action", ">", "0.0", ":", "\n", "            ", "max_tradable_amount", "=", "state", "[", "-", "1", "]", "\n", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "\n", "", "elif", "action", "<", "0.0", ":", "\n", "# one could insert a stop to consider cost of short selling", "\n", "            ", "shares_traded", "=", "-", "abs", "(", "action", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "np", ".", "abs", "(", "shares_traded", ")", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "shares_traded", "+", "state", "[", "1", "]", "*", "(", "1", "+", "self", ".", "returns", "[", "index", "+", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.reset": [[1043, 1047], ["env.MultiAssetCashMarketEnv._get_inputs", "numpy.array().reshape", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._get_inputs"], ["    ", "def", "reset", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "input_list", "=", "self", ".", "_get_inputs", "(", "reset", "=", "True", ")", "\n", "currState", "=", "np", ".", "array", "(", "input_list", ")", ".", "reshape", "(", "-", "1", ",", ")", "# long vector", "\n", "return", "currState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step": [[1049, 1083], ["env.MultiAssetCashMarketEnv.holding_ts.append", "env.MultiAssetCashMarketEnv.cash_ts.append", "env.MultiAssetCashMarketEnv._getreward", "env.MultiAssetCashMarketEnv._buy", "numpy.array", "env.MultiAssetCashMarketEnv._sell", "numpy.append"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._buy", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._sell"], ["", "def", "step", "(", "\n", "self", ",", "\n", "currState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "action", ":", "float", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"DQN\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "]", ":", "\n", "# TODO Adapt to multi asset. I adapted only Mv_res step", "\n", "        ", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "\n", "# buy/sell here", "\n", "if", "action", ">", "0", ":", "\n", "            ", "shares_traded", "=", "self", ".", "_buy", "(", "index", "=", "iteration", ",", "cash", "=", "self", ".", "cash_ts", "[", "iteration", "]", ",", "action", "=", "action", ")", "\n", "", "elif", "action", "<", "0", ":", "\n", "            ", "shares_traded", "=", "self", ".", "_sell", "(", "index", "=", "iteration", ",", "holding", "=", "self", ".", "holding_ts", "[", "iteration", "]", ",", "action", "=", "action", ")", "\n", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "# update rules", "\n", "", "nextHolding", "=", "(", "1", "+", "nextRet", ")", "*", "self", ".", "holding_ts", "[", "iteration", "]", "+", "shares_traded", "\n", "self", ".", "holding_ts", ".", "append", "(", "nextHolding", ")", "\n", "nextCash", "=", "self", ".", "cash_ts", "[", "iteration", "]", "+", "self", ".", "traded_amount", "\n", "self", ".", "cash_ts", ".", "append", "(", "nextCash", ")", "\n", "\n", "if", "self", ".", "inp_type", "==", "\"ret\"", ":", "\n", "            ", "nextState", "=", "np", ".", "array", "(", "[", "nextRet", ",", "nextHolding", ",", "nextCash", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "elif", "self", ".", "inp_type", "==", "\"f\"", ":", "\n", "            ", "nextState", "=", "np", ".", "append", "(", "nextFactors", ",", "[", "nextHolding", ",", "nextCash", "]", ")", "\n", "\n", "", "Result", "=", "self", ".", "_getreward", "(", "currState", ",", "nextState", ",", "iteration", ",", "tag", ")", "\n", "\n", "return", "nextState", ",", "Result", ",", "nextFactors", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step": [[1084, 1142], ["numpy.array", "enumerate", "numpy.array", "env.MultiAssetCashMarketEnv.holding_ts.append", "env.MultiAssetCashMarketEnv.cash_ts.append", "env.MultiAssetCashMarketEnv._get_inputs", "numpy.array().reshape", "env.MultiAssetCashMarketEnv._getreward", "numpy.array", "numpy.dot", "env.MultiAssetCashMarketEnv.factors[].reshape", "numpy.dot", "numpy.array.append", "numpy.linalg.inv", "numpy.linalg.inv", "numpy.dot", "env.MultiAssetCashMarketEnv._buy", "numpy.array", "env.MultiAssetCashMarketEnv._sell"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._get_inputs", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._buy", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._sell"], ["", "def", "MV_res_step", "(", "\n", "self", ",", "\n", "currState", ":", "Union", "[", "Tuple", ",", "np", ".", "ndarray", "]", ",", "\n", "shares_traded", ":", "int", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"DQN\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "        ", "CurrHolding", "=", "np", ".", "array", "(", "self", ".", "holding_ts", "[", "iteration", "]", ")", "\n", "if", "self", ".", "inp_type", "==", "'alpha'", ":", "\n", "            ", "curr_alpha", "=", "np", ".", "array", "(", "currState", "[", ":", "self", ".", "n_assets", "]", ")", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "np", ".", "dot", "(", "np", ".", "linalg", ".", "inv", "(", "self", ".", "cov_matrix", "**", "self", ".", "kappa", ")", ",", "curr_alpha", ")", "\n", "", "else", ":", "\n", "            ", "CurrFactors", "=", "self", ".", "factors", "[", "iteration", "]", ".", "reshape", "(", "self", ".", "n_assets", ",", "self", ".", "n_factors", ")", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "np", ".", "dot", "(", "np", ".", "linalg", ".", "inv", "(", "self", ".", "cov_matrix", "*", "self", ".", "kappa", ")", ",", "np", ".", "dot", "(", "\n", "CurrFactors", ",", "self", ".", "f_param", "[", "0", "]", "\n", ")", ")", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "# Compute optimal markovitz action", "\n", "", "MV_action", "=", "OptNextHolding", "-", "CurrHolding", "\n", "MV_res_action", "=", "MV_action", "*", "(", "1", "-", "shares_traded", ")", "\n", "\n", "# buy/sell here", "\n", "res_shares_traded", "=", "[", "]", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "MV_res_action", ")", ":", "\n", "            ", "if", "a", ">", "0", ":", "\n", "                ", "trade", "=", "self", ".", "_buy", "(", "\n", "index", "=", "iteration", ",", "cash", "=", "currState", "[", "-", "1", "]", ",", "action", "=", "a", "\n", ")", "\n", "", "elif", "a", "<", "0", ":", "\n", "                ", "trade", "=", "self", ".", "_sell", "(", "\n", "index", "=", "iteration", ",", "holding", "=", "currState", "[", "-", "1", "-", "self", ".", "n_assets", "+", "i", "]", ",", "action", "=", "a", "\n", ")", "\n", "", "else", ":", "\n", "                ", "trade", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "", "res_shares_traded", ".", "append", "(", "trade", ")", "\n", "", "res_shares_traded", "=", "np", ".", "array", "(", "res_shares_traded", ")", "\n", "\n", "\n", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "\n", "nextHolding", "=", "(", "1", "+", "nextRet", ")", "*", "CurrHolding", "+", "res_shares_traded", "\n", "self", ".", "holding_ts", ".", "append", "(", "nextHolding", ")", "\n", "nextCash", "=", "self", ".", "cash_ts", "[", "iteration", "]", "+", "self", ".", "traded_amount", "\n", "self", ".", "cash_ts", ".", "append", "(", "nextCash", ")", "\n", "\n", "\n", "input_list", "=", "self", ".", "_get_inputs", "(", "reset", "=", "False", ",", "iteration", "=", "iteration", ")", "\n", "nextState", "=", "np", ".", "array", "(", "input_list", ")", ".", "reshape", "(", "-", "1", ",", ")", "# long vector", "\n", "\n", "Result", "=", "self", ".", "_getreward", "(", "\n", "iteration", ",", "tag", ",", "res_action", "=", "res_shares_traded", "\n", ")", "\n", "\n", "return", "nextState", ",", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset": [[1144, 1147], ["list"], "methods", ["None"], ["", "def", "opt_reset", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "currOptState", "=", "list", "(", "self", ".", "factors", "[", "0", "]", ")", "+", "[", "self", ".", "Startholding", "]", "*", "self", ".", "n_assets", "+", "[", "self", ".", "cash", "]", "\n", "return", "currOptState", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_step": [[1148, 1187], ["env.MultiAssetCashMarketEnv.factors[].reshape", "numpy.array", "numpy.dot", "enumerate", "env.MultiAssetCashMarketEnv._get_opt_reward", "numpy.linalg.inv", "numpy.dot", "env.MultiAssetCashMarketEnv._opt_trade", "OptTrades.append", "numpy.array", "list", "list"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._get_opt_reward", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._opt_trade"], ["", "def", "opt_step", "(", "\n", "self", ",", "\n", "currOptState", ":", "Tuple", ",", "\n", "OptRate", ":", "float", ",", "\n", "DiscFactorLoads", ":", "np", ".", "ndarray", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", "=", "\"Opt\"", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", "]", ":", "\n", "\n", "        ", "CurrFactors", "=", "self", ".", "factors", "[", "iteration", "]", ".", "reshape", "(", "self", ".", "n_assets", ",", "self", ".", "n_factors", ")", "\n", "OptCurrHolding", "=", "np", ".", "array", "(", "currOptState", "[", "-", "1", "-", "self", ".", "n_assets", ":", "-", "1", "]", ")", "\n", "# Optimal traded quantity between period", "\n", "\n", "DiscFactors", "=", "CurrFactors", "/", "(", "1", "+", "self", ".", "f_speed", "*", "(", "(", "OptRate", "*", "self", ".", "CostMultiplier", ")", "/", "self", ".", "kappa", ")", ")", "\n", "OptNextHolding", "=", "np", ".", "dot", "(", "np", ".", "linalg", ".", "inv", "(", "self", ".", "cov_matrix", "*", "self", ".", "kappa", ")", ",", "np", ".", "dot", "(", "\n", "DiscFactors", ",", "self", ".", "f_param", "[", "0", "]", "\n", ")", ")", "\n", "\n", "action", "=", "OptNextHolding", "-", "OptCurrHolding", "\n", "\n", "# buy/sell here", "\n", "OptTrades", "=", "[", "]", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "action", ")", ":", "\n", "            ", "opt_t", "=", "self", ".", "_opt_trade", "(", "\n", "index", "=", "iteration", ",", "holding", "=", "currOptState", "[", "-", "1", "-", "self", ".", "n_assets", "+", "i", "]", ",", "cash", "=", "currOptState", "[", "-", "1", "]", ",", "action", "=", "a", "\n", ")", "\n", "OptTrades", ".", "append", "(", "opt_t", ")", "\n", "", "OptNextHolding", "=", "np", ".", "array", "(", "OptTrades", ")", "+", "OptCurrHolding", "*", "(", "1", "+", "self", ".", "returns", "[", "iteration", "+", "1", "]", ")", "\n", "\n", "nextCash", "=", "currOptState", "[", "-", "1", "]", "+", "self", ".", "traded_amount", "\n", "nextReturn", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextOptState", "=", "list", "(", "nextFactors", ")", "+", "list", "(", "OptNextHolding", ")", "+", "[", "nextCash", "]", "\n", "\n", "OptResult", "=", "self", ".", "_get_opt_reward", "(", "\n", "currOptState", ",", "nextOptState", ",", "nextReturn", ",", "iteration", ",", "tag", "\n", ")", "\n", "\n", "return", "nextOptState", ",", "OptResult", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.mv_step": [[1188, 1221], ["env.MultiAssetCashMarketEnv.factors[].reshape", "numpy.array", "numpy.dot", "enumerate", "env.MultiAssetCashMarketEnv._get_opt_reward", "numpy.linalg.inv", "numpy.dot", "env.MultiAssetCashMarketEnv._opt_trade", "OptTrades.append", "numpy.array", "list", "list"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._get_opt_reward", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._opt_trade"], ["", "def", "mv_step", "(", "\n", "self", ",", "currOptState", ":", "Tuple", ",", "iteration", ":", "int", ",", "tag", ":", "str", "=", "\"MV\"", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "dict", "]", ":", "\n", "\n", "        ", "CurrFactors", "=", "self", ".", "factors", "[", "iteration", "]", ".", "reshape", "(", "self", ".", "n_assets", ",", "self", ".", "n_factors", ")", "\n", "OptCurrHolding", "=", "np", ".", "array", "(", "currOptState", "[", "-", "1", "-", "self", ".", "n_assets", ":", "-", "1", "]", ")", "\n", "\n", "# Traded quantity as for the Markovitz framework  (Mean-Variance framework)", "\n", "OptNextHolding", "=", "np", ".", "dot", "(", "np", ".", "linalg", ".", "inv", "(", "self", ".", "cov_matrix", "*", "self", ".", "kappa", ")", ",", "np", ".", "dot", "(", "\n", "CurrFactors", ",", "self", ".", "f_param", "[", "0", "]", "\n", ")", ")", "\n", "\n", "action", "=", "OptNextHolding", "-", "OptCurrHolding", "\n", "\n", "# buy/sell here", "\n", "OptTrades", "=", "[", "]", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "action", ")", ":", "\n", "            ", "opt_t", "=", "self", ".", "_opt_trade", "(", "\n", "index", "=", "iteration", ",", "holding", "=", "currOptState", "[", "-", "1", "-", "self", ".", "n_assets", "+", "i", "]", ",", "cash", "=", "currOptState", "[", "-", "1", "]", ",", "action", "=", "a", "\n", ")", "\n", "OptTrades", ".", "append", "(", "opt_t", ")", "\n", "", "OptNextHolding", "=", "np", ".", "array", "(", "OptTrades", ")", "+", "OptCurrHolding", "*", "(", "1", "+", "self", ".", "returns", "[", "iteration", "+", "1", "]", ")", "\n", "\n", "nextCash", "=", "currOptState", "[", "-", "1", "]", "+", "self", ".", "traded_amount", "\n", "nextReturn", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "nextFactors", "=", "self", ".", "factors", "[", "iteration", "+", "1", "]", "\n", "nextOptState", "=", "list", "(", "nextFactors", ")", "+", "list", "(", "OptNextHolding", ")", "+", "[", "nextCash", "]", "\n", "\n", "OptResult", "=", "self", ".", "_get_opt_reward", "(", "\n", "currOptState", ",", "nextOptState", ",", "nextReturn", ",", "iteration", ",", "tag", "\n", ")", "\n", "\n", "return", "nextOptState", ",", "OptResult", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads": [[1222, 1241], ["numpy.sqrt", "numpy.exp"], "methods", ["None"], ["", "def", "opt_trading_rate_disc_loads", "(", "self", ")", "->", "Tuple", "[", "float", ",", "np", ".", "ndarray", "]", ":", "\n", "\n", "# 1 percent annualized discount rate (same rate of Ritter)", "\n", "        ", "rho", "=", "1", "-", "np", ".", "exp", "(", "-", "self", ".", "discount_rate", "/", "260", ")", "\n", "\n", "# kappa is the risk aversion, CostMultiplier the parameter for trading cost", "\n", "num1", "=", "self", ".", "kappa", "*", "(", "1", "-", "rho", ")", "+", "self", ".", "CostMultiplier", "*", "rho", "\n", "num2", "=", "np", ".", "sqrt", "(", "\n", "num1", "**", "2", "+", "4", "*", "self", ".", "kappa", "*", "self", ".", "CostMultiplier", "*", "(", "1", "-", "rho", ")", "**", "2", "\n", ")", "\n", "den", "=", "2", "*", "(", "1", "-", "rho", ")", "\n", "a", "=", "(", "-", "num1", "+", "num2", ")", "/", "den", "\n", "\n", "OptRate", "=", "a", "/", "self", ".", "CostMultiplier", "\n", "\n", "\n", "DiscFactorLoads", "=", "self", ".", "f_param", "\n", "\n", "return", "OptRate", ",", "DiscFactorLoads", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._getreward": [[1242, 1279], ["isinstance", "numpy.dot", "numpy.dot", "nextHolding.sum", "numpy.dot"], "methods", ["None"], ["", "def", "_getreward", "(", "\n", "self", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", ",", "\n", "res_action", ":", "float", "=", "None", ",", "\n", ")", "->", "dict", ":", "\n", "\n", "        ", "nextRet", "=", "self", ".", "returns", "[", "iteration", "+", "1", "]", "\n", "currHolding", "=", "self", ".", "holding_ts", "[", "iteration", "]", "\n", "nextHolding", "=", "self", ".", "holding_ts", "[", "iteration", "+", "1", "]", "\n", "nextCash", "=", "self", ".", "cash_ts", "[", "iteration", "+", "1", "]", "\n", "\n", "shares_traded", "=", "nextHolding", "-", "currHolding", "\n", "NetPNL", "=", "np", ".", "dot", "(", "nextHolding", ",", "nextRet", ")", "-", "self", ".", "costs", "\n", "Risk", "=", "0.5", "*", "self", ".", "kappa", "*", "np", ".", "dot", "(", "np", ".", "dot", "(", "nextHolding", ".", "T", ",", "self", ".", "cov_matrix", ")", ",", "nextHolding", ")", "\n", "Reward", "=", "NetPNL", "-", "Risk", "\n", "nextWealth", "=", "nextHolding", ".", "sum", "(", ")", "+", "nextCash", "\n", "\n", "Result", "=", "{", "\n", "\"CurrHolding_{}\"", ".", "format", "(", "tag", ")", ":", "currHolding", ",", "\n", "\"NextHolding_{}\"", ".", "format", "(", "tag", ")", ":", "nextHolding", ",", "\n", "\"Action_{}\"", ".", "format", "(", "tag", ")", ":", "shares_traded", ",", "\n", "\"GrossPNL_{}\"", ".", "format", "(", "tag", ")", ":", "NetPNL", "+", "self", ".", "costs", ",", "\n", "\"NetPNL_{}\"", ".", "format", "(", "tag", ")", ":", "NetPNL", ",", "\n", "\"Risk_{}\"", ".", "format", "(", "tag", ")", ":", "Risk", ",", "\n", "\"Cost_{}\"", ".", "format", "(", "tag", ")", ":", "self", ".", "costs", ",", "\n", "\"Reward_{}\"", ".", "format", "(", "tag", ")", ":", "Reward", ",", "\n", "\"TradedAmount_{}\"", ".", "format", "(", "tag", ")", ":", "self", ".", "traded_amount", ",", "\n", "\"Cash_{}\"", ".", "format", "(", "tag", ")", ":", "nextCash", ",", "\n", "\"Wealth_{}\"", ".", "format", "(", "tag", ")", ":", "nextWealth", ",", "\n", "}", "\n", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "if", "isinstance", "(", "res_action", ",", "float", ")", ":", "\n", "            ", "Result", "[", "\"ResAction_{}\"", ".", "format", "(", "tag", ")", "]", "=", "res_action", "\n", "", "return", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._sell": [[1280, 1298], ["min", "env.MultiAssetCashMarketEnv._totalcost", "abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_sell", "(", "self", ",", "index", ":", "int", ",", "holding", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "currholding", "=", "holding", "\n", "\n", "if", "currholding", ">", "0.0", ":", "\n", "# Sell only if current asset is > 0", "\n", "            ", "shares_traded", "=", "min", "(", "abs", "(", "action", ")", ",", "currholding", ")", "\n", "self", ".", "costs", "+=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "+=", "shares_traded", "-", "self", ".", "costs", "\n", "return", "-", "shares_traded", "\n", "\n", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "\n", "", "return", "-", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._buy": [[1299, 1309], ["min", "env.MultiAssetCashMarketEnv._totalcost"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_buy", "(", "self", ",", "index", ":", "int", ",", "cash", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "max_tradable_amount", "=", "cash", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "+=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "+=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "\n", "return", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._get_opt_reward": [[1310, 1352], ["numpy.array", "numpy.array", "numpy.dot", "numpy.dot", "numpy.array.sum", "numpy.dot"], "methods", ["None"], ["", "def", "_get_opt_reward", "(", "\n", "self", ",", "\n", "currOptState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "nextOptState", ":", "Tuple", "[", "Union", "[", "float", ",", "int", "]", ",", "Union", "[", "float", ",", "int", "]", "]", ",", "\n", "nextReturn", ":", "float", ",", "\n", "iteration", ":", "int", ",", "\n", "tag", ":", "str", ",", "\n", ")", "->", "dict", ":", "\n", "\n", "        ", "OptCurrHolding", "=", "np", ".", "array", "(", "currOptState", "[", "-", "1", "-", "self", ".", "n_assets", ":", "-", "1", "]", ")", "\n", "OptNextHolding", "=", "np", ".", "array", "(", "nextOptState", "[", "-", "1", "-", "self", ".", "n_assets", ":", "-", "1", "]", ")", "\n", "CurrOptcash", "=", "currOptState", "[", "-", "1", "]", "\n", "NextOptcash", "=", "nextOptState", "[", "-", "1", "]", "\n", "\n", "# Traded quantity between period", "\n", "OptNextAction", "=", "OptNextHolding", "-", "OptCurrHolding", "\n", "# Portfolio variation", "\n", "OptNetPNL", "=", "np", ".", "dot", "(", "OptNextHolding", ",", "nextReturn", ")", "-", "self", ".", "costs", "\n", "# Risk", "\n", "OptRisk", "=", "0.5", "*", "self", ".", "kappa", "*", "np", ".", "dot", "(", "np", ".", "dot", "(", "OptNextHolding", ".", "T", ",", "self", ".", "cov_matrix", ")", ",", "OptNextHolding", ")", "\n", "# Compute reward", "\n", "OptReward", "=", "OptNetPNL", "-", "OptRisk", "\n", "\n", "nextWealth", "=", "OptNextHolding", ".", "sum", "(", ")", "+", "NextOptcash", "#self.prices[iteration + 1] * ", "\n", "\n", "# Store quantities", "\n", "Result", "=", "{", "\n", "\"{}NextAction\"", ".", "format", "(", "tag", ")", ":", "OptNextAction", ",", "\n", "\"{}NextHolding\"", ".", "format", "(", "tag", ")", ":", "OptNextHolding", ",", "\n", "\"{}GrossPNL\"", ".", "format", "(", "tag", ")", ":", "OptNetPNL", "+", "self", ".", "costs", ",", "\n", "\"{}NetPNL\"", ".", "format", "(", "tag", ")", ":", "OptNetPNL", ",", "\n", "\"{}Risk\"", ".", "format", "(", "tag", ")", ":", "OptRisk", ",", "\n", "\"{}Cost\"", ".", "format", "(", "tag", ")", ":", "self", ".", "costs", ",", "\n", "\"{}Reward\"", ".", "format", "(", "tag", ")", ":", "OptReward", ",", "\n", "\"{}TradedAmount\"", ".", "format", "(", "tag", ")", ":", "self", ".", "traded_amount", ",", "\n", "\"{}Cash\"", ".", "format", "(", "tag", ")", ":", "NextOptcash", ",", "\n", "\"{}Wealth\"", ".", "format", "(", "tag", ")", ":", "nextWealth", ",", "\n", "}", "\n", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "Result", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv._opt_trade": [[1353, 1389], ["min", "env.MultiAssetCashMarketEnv._totalcost", "min", "env.MultiAssetCashMarketEnv._totalcost", "abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_opt_trade", "(", "self", ",", "index", ":", "int", ",", "holding", ":", "float", ",", "cash", ":", "float", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "if", "action", ">", "0.0", ":", "\n", "\n", "            ", "max_tradable_amount", "=", "cash", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "+=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "+=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "\n", "\n", "", "elif", "action", "<", "0.0", ":", "\n", "\n", "            ", "currholding", "=", "holding", "\n", "\n", "if", "currholding", ">", "0.0", ":", "\n", "# Sell only if current asset is > 0", "\n", "                ", "shares_traded", "=", "min", "(", "abs", "(", "action", ")", ",", "currholding", ")", "\n", "\n", "self", ".", "costs", "=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "=", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "-", "shares_traded", "\n", "\n", "", "else", ":", "\n", "                ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "shares_traded", "\n", "\n", "", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results": [[1391, 1449], ["numpy.array", "numpy.array", "numpy.array", "env.MultiAssetCashMarketEnv.currholding_rl.append", "env.MultiAssetCashMarketEnv.nextholding_rl.append", "env.MultiAssetCashMarketEnv.action_rl.append", "env.MultiAssetCashMarketEnv.grosspnl_rl.append", "env.MultiAssetCashMarketEnv.netpnl_rl.append", "env.MultiAssetCashMarketEnv.risk_rl.append", "env.MultiAssetCashMarketEnv.cost_rl.append", "env.MultiAssetCashMarketEnv.reward_rl.append", "env.MultiAssetCashMarketEnv.tradedamount_rl.append", "env.MultiAssetCashMarketEnv.cash_rl.append", "env.MultiAssetCashMarketEnv.wealth_rl.append", "list", "numpy.array", "list", "env.MultiAssetCashMarketEnv.mvholding.append", "env.MultiAssetCashMarketEnv.mvreward.append", "env.MultiAssetCashMarketEnv.mvwealth.append", "Result.keys", "list", "numpy.array", "numpy.array", "Result.keys", "list", "env.MultiAssetCashMarketEnv.optaction.append", "env.MultiAssetCashMarketEnv.optholding.append", "env.MultiAssetCashMarketEnv.grosspnl_opt.append", "env.MultiAssetCashMarketEnv.netpnl_opt.append", "env.MultiAssetCashMarketEnv.risk_opt.append", "env.MultiAssetCashMarketEnv.cost_opt.append", "env.MultiAssetCashMarketEnv.reward_opt.append", "env.MultiAssetCashMarketEnv.tradedamount_opt.append", "env.MultiAssetCashMarketEnv.cash_opt.append", "env.MultiAssetCashMarketEnv.wealth_opt.append", "env.MultiAssetCashMarketEnv.res_df.filter", "env.MultiAssetCashMarketEnv.res_df.filter", "env.MultiAssetCashMarketEnv.res_df.filter", "Result.keys", "list", "Result.keys", "list", "env.MultiAssetCashMarketEnv.res_df.filter", "Result.keys", "Result.keys", "env.MultiAssetCashMarketEnv.res_df.filter", "env.MultiAssetCashMarketEnv.res_df.filter"], "methods", ["None"], ["", "", "def", "store_results", "(", "self", ",", "Result", ":", "dict", ",", "iteration", ":", "int", ")", ":", "\n", "        ", "if", "iteration", "==", "self", ".", "N_train", ":", "\n", "            ", "if", "'PPO'", "in", "list", "(", "Result", ".", "keys", "(", ")", ")", "[", "0", "]", ":", "\n", "                ", "self", ".", "res_df", "=", "self", ".", "res_df", ".", "iloc", "[", ":", "-", "2", ",", ":", "]", "\n", "self", ".", "res_df", "[", "self", ".", "res_df", ".", "filter", "(", "like", "=", "'CurrHolding_PPO'", ")", ".", "columns", "]", "=", "np", ".", "array", "(", "self", ".", "currholding_rl", ")", "\n", "self", ".", "res_df", "[", "self", ".", "res_df", ".", "filter", "(", "like", "=", "'NextHolding_PPO'", ")", ".", "columns", "]", "=", "np", ".", "array", "(", "self", ".", "nextholding_rl", ")", "\n", "self", ".", "res_df", "[", "self", ".", "res_df", ".", "filter", "(", "like", "=", "'Action_PPO'", ")", ".", "columns", "]", "=", "np", ".", "array", "(", "self", ".", "action_rl", ")", "\n", "self", ".", "res_df", "[", "'GrossPNL_PPO'", "]", "=", "self", ".", "grosspnl_rl", "\n", "self", ".", "res_df", "[", "'NetPNL_PPO'", "]", "=", "self", ".", "netpnl_rl", "\n", "self", ".", "res_df", "[", "'Risk_PPO'", "]", "=", "self", ".", "risk_rl", "\n", "self", ".", "res_df", "[", "'Cost_PPO'", "]", "=", "self", ".", "cost_rl", "\n", "self", ".", "res_df", "[", "'Reward_PPO'", "]", "=", "self", ".", "reward_rl", "\n", "self", ".", "res_df", "[", "'TradedAmount_PPO'", "]", "=", "self", ".", "tradedamount_rl", "\n", "self", ".", "res_df", "[", "'Cash_PPO'", "]", "=", "self", ".", "cash_rl", "\n", "self", ".", "res_df", "[", "'Wealth_PPO'", "]", "=", "self", ".", "wealth_rl", "\n", "", "elif", "'MV'", "in", "list", "(", "Result", ".", "keys", "(", ")", ")", "[", "0", "]", ":", "\n", "                ", "self", ".", "res_df", "[", "self", ".", "res_df", ".", "filter", "(", "like", "=", "'MVNextHolding'", ")", ".", "columns", "]", "=", "np", ".", "array", "(", "self", ".", "mvholding", ")", "\n", "self", ".", "res_df", "[", "'MVReward'", "]", "=", "self", ".", "mvreward", "\n", "self", ".", "res_df", "[", "'MVWealth'", "]", "=", "self", ".", "mvwealth", "\n", "", "elif", "'Opt'", "in", "list", "(", "Result", ".", "keys", "(", ")", ")", "[", "0", "]", ":", "\n", "                ", "self", ".", "res_df", "[", "self", ".", "res_df", ".", "filter", "(", "like", "=", "'OptNextAction'", ")", ".", "columns", "]", "=", "np", ".", "array", "(", "self", ".", "optaction", ")", "\n", "self", ".", "res_df", "[", "self", ".", "res_df", ".", "filter", "(", "like", "=", "'OptNextHolding'", ")", ".", "columns", "]", "=", "np", ".", "array", "(", "self", ".", "optholding", ")", "\n", "self", ".", "res_df", "[", "'OptGrossPNL'", "]", "=", "self", ".", "grosspnl_opt", "\n", "self", ".", "res_df", "[", "'OptNetPNL'", "]", "=", "self", ".", "netpnl_opt", "\n", "self", ".", "res_df", "[", "'OptRisk'", "]", "=", "self", ".", "risk_opt", "\n", "self", ".", "res_df", "[", "'OptCost'", "]", "=", "self", ".", "cost_opt", "\n", "self", ".", "res_df", "[", "'OptReward'", "]", "=", "self", ".", "reward_opt", "\n", "self", ".", "res_df", "[", "'OptTradedAmount'", "]", "=", "self", ".", "tradedamount_opt", "\n", "self", ".", "res_df", "[", "'OptCash'", "]", "=", "self", ".", "cash_opt", "\n", "self", ".", "res_df", "[", "'OptWealth'", "]", "=", "self", ".", "wealth_opt", "\n", "", "", "else", ":", "\n", "            ", "if", "'PPO'", "in", "list", "(", "Result", ".", "keys", "(", ")", ")", "[", "0", "]", ":", "\n", "                ", "self", ".", "currholding_rl", ".", "append", "(", "Result", "[", "'CurrHolding_PPO'", "]", ")", "\n", "self", ".", "nextholding_rl", ".", "append", "(", "Result", "[", "'NextHolding_PPO'", "]", ")", "\n", "self", ".", "action_rl", ".", "append", "(", "Result", "[", "'Action_PPO'", "]", ")", "\n", "self", ".", "grosspnl_rl", ".", "append", "(", "Result", "[", "'GrossPNL_PPO'", "]", ")", "\n", "self", ".", "netpnl_rl", ".", "append", "(", "Result", "[", "'NetPNL_PPO'", "]", ")", "\n", "self", ".", "risk_rl", ".", "append", "(", "Result", "[", "'Risk_PPO'", "]", ")", "\n", "self", ".", "cost_rl", ".", "append", "(", "Result", "[", "'Cost_PPO'", "]", ")", "\n", "self", ".", "reward_rl", ".", "append", "(", "Result", "[", "'Reward_PPO'", "]", ")", "\n", "self", ".", "tradedamount_rl", ".", "append", "(", "Result", "[", "'TradedAmount_PPO'", "]", ")", "\n", "self", ".", "cash_rl", ".", "append", "(", "Result", "[", "'Cash_PPO'", "]", ")", "\n", "self", ".", "wealth_rl", ".", "append", "(", "Result", "[", "'Wealth_PPO'", "]", ")", "\n", "", "elif", "'MV'", "in", "list", "(", "Result", ".", "keys", "(", ")", ")", "[", "0", "]", ":", "\n", "                ", "self", ".", "mvholding", ".", "append", "(", "Result", "[", "'MVNextHolding'", "]", ")", "\n", "self", ".", "mvreward", ".", "append", "(", "Result", "[", "'MVReward'", "]", ")", "\n", "self", ".", "mvwealth", ".", "append", "(", "Result", "[", "'MVWealth'", "]", ")", "\n", "", "elif", "'Opt'", "in", "list", "(", "Result", ".", "keys", "(", ")", ")", "[", "0", "]", ":", "\n", "                ", "self", ".", "optaction", ".", "append", "(", "Result", "[", "'OptNextAction'", "]", ")", "\n", "self", ".", "optholding", ".", "append", "(", "Result", "[", "'OptNextHolding'", "]", ")", "\n", "self", ".", "grosspnl_opt", ".", "append", "(", "Result", "[", "'OptGrossPNL'", "]", ")", "\n", "self", ".", "netpnl_opt", ".", "append", "(", "Result", "[", "'OptNetPNL'", "]", ")", "\n", "self", ".", "risk_opt", ".", "append", "(", "Result", "[", "'OptRisk'", "]", ")", "\n", "self", ".", "cost_opt", ".", "append", "(", "Result", "[", "'OptCost'", "]", ")", "\n", "self", ".", "reward_opt", ".", "append", "(", "Result", "[", "'OptReward'", "]", ")", "\n", "self", ".", "tradedamount_opt", ".", "append", "(", "Result", "[", "'OptTradedAmount'", "]", ")", "\n", "self", ".", "cash_opt", ".", "append", "(", "Result", "[", "'OptCash'", "]", ")", "\n", "self", ".", "wealth_opt", ".", "append", "(", "Result", "[", "'OptWealth'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._sell": [[1456, 1464], ["env.ShortMultiAssetCashMarketEnv._totalcost", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["    ", "def", "_sell", "(", "self", ",", "index", ":", "int", ",", "holding", ":", "np", ".", "ndarray", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "shares_traded", "=", "action", "\n", "self", ".", "costs", "+=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "# absolute quantity because now you can sell short and shares_traded can be negative", "\n", "self", ".", "traded_amount", "+=", "np", ".", "abs", "(", "shares_traded", ")", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.ShortMultiAssetCashMarketEnv._opt_trade": [[1466, 1493], ["min", "env.ShortMultiAssetCashMarketEnv._totalcost", "env.ShortMultiAssetCashMarketEnv._totalcost", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv._totalcost"], ["", "def", "_opt_trade", "(", "self", ",", "index", ":", "int", ",", "holding", ":", "float", ",", "cash", ":", "float", ",", "action", ":", "float", ")", ":", "\n", "\n", "        ", "if", "action", ">", "0.0", ":", "\n", "\n", "            ", "max_tradable_amount", "=", "cash", "\n", "shares_traded", "=", "min", "(", "max_tradable_amount", ",", "action", ")", "\n", "\n", "self", ".", "costs", "+=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "+=", "-", "shares_traded", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "\n", "\n", "", "elif", "action", "<", "0.0", ":", "\n", "\n", "# one could insert a stop to consider cost of short selling", "\n", "            ", "shares_traded", "=", "action", "\n", "\n", "self", ".", "costs", "+=", "self", ".", "_totalcost", "(", "shares_traded", ")", "\n", "self", ".", "traded_amount", "+=", "np", ".", "abs", "(", "shares_traded", ")", "-", "self", ".", "costs", "\n", "\n", "return", "shares_traded", "\n", "\n", "", "else", ":", "\n", "            ", "shares_traded", "=", "0.0", "\n", "self", ".", "costs", ",", "self", ".", "traded_amount", "=", "0.0", ",", "0.0", "\n", "\n", "return", "shares_traded", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DeepNetworkModel.__init__": [[25, 112], ["super().__init__", "tensorflow.random.set_seed", "tensorflow.keras.layers.InputLayer", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.BatchNormalization", "DQN.DeepNetworkModel.hids.append", "tensorflow.keras.layers.Dense", "DQN.DeepNetworkModel.hids.append", "DQN.DeepNetworkModel.hids.append", "tensorflow.keras.layers.Activation", "DQN.DeepNetworkModel.hids.append", "tensorflow.keras.layers.BatchNormalization", "tensorflow.keras.layers.Activation", "DQN.DeepNetworkModel.hids.append", "DQN.DeepNetworkModel.hids.append", "tensorflow.keras.layers.Activation", "tensorflow.keras.layers.Activation"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "seed", ":", "int", ",", "\n", "input_shape", ":", "int", ",", "\n", "hidden_units", ":", "list", ",", "\n", "num_actions", ":", "int", ",", "\n", "batch_norm_input", ":", "bool", ",", "\n", "batch_norm_hidden", ":", "bool", ",", "\n", "activation", ":", "str", ",", "\n", "kernel_initializer", ":", "str", ",", "\n", "modelname", ":", "str", "=", "\"Deep Q Network\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate Deep Q Network Class\n\n        Parameters\n        ----------\n        seed: int\n            Seed for experiment reproducibility\n\n        input_shape: int\n            Shape of input of the neural network\n\n        hidden_units: list\n            List of sizes of hidden layer. The length of the list determines\n            the depth of the Q network\n\n        num_actions: int\n            Number of possible action which is the size of the output\n\n        batch_norm_input: bool\n            Boolean to regulate the presence of a Batch Norm layer after the input\n\n        batch_norm_hidden: bool\n            Boolean to regulate the presence of a Batch Norm layer after each hidden layer\n\n        activation: str\n            Choice of activation function. It can be 'leaky_relu',\n            'relu6' or 'elu'\n\n        kernel_initializer: str\n            Choice of weight initialization as aliased in TF2.0 documentation\n\n        modelname: str\n            Name of the model\n\n        \"\"\"", "\n", "# call the parent constructor", "\n", "super", "(", "DeepNetworkModel", ",", "self", ")", ".", "__init__", "(", "name", "=", "modelname", ")", "\n", "\n", "# set dimensionality of input/output depending on the model", "\n", "inp_shape", "=", "input_shape", "\n", "out_shape", "=", "num_actions", "\n", "# set random seed", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "# set flag for batch norm as attribute", "\n", "self", ".", "bnflag_input", "=", "batch_norm_input", "\n", "self", ".", "batch_norm_hidden", "=", "batch_norm_hidden", "\n", "# In setting input_shape, the batch dimension is not included.", "\n", "# input layer", "\n", "self", ".", "input_layer", "=", "InputLayer", "(", "input_shape", "=", "inp_shape", ")", "\n", "# batch norm layer for inputs", "\n", "if", "self", ".", "bnflag_input", ":", "\n", "            ", "self", ".", "bnorm_layer", "=", "BatchNormalization", "(", "center", "=", "False", ",", "scale", "=", "False", ")", "\n", "\n", "# set of hidden layers", "\n", "", "self", ".", "hids", "=", "[", "]", "\n", "\n", "for", "i", "in", "hidden_units", ":", "\n", "            ", "self", ".", "hids", ".", "append", "(", "Dense", "(", "i", ",", "kernel_initializer", "=", "kernel_initializer", ")", ")", "\n", "# check what type of activation is set", "\n", "if", "activation", "==", "\"leaky_relu\"", ":", "\n", "                ", "leaky_relu", "=", "tf", ".", "nn", ".", "leaky_relu", "\n", "self", ".", "hids", ".", "append", "(", "Activation", "(", "leaky_relu", ")", ")", "\n", "", "elif", "activation", "==", "\"relu6\"", ":", "\n", "                ", "relu6", "=", "tf", ".", "nn", ".", "relu6", "\n", "self", ".", "hids", ".", "append", "(", "Activation", "(", "relu6", ")", ")", "\n", "", "elif", "activation", "==", "\"elu\"", ":", "\n", "                ", "elu", "=", "tf", ".", "nn", ".", "elu", "\n", "self", ".", "hids", ".", "append", "(", "Activation", "(", "elu", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "hids", ".", "append", "(", "Activation", "(", "activation", ")", ")", "\n", "\n", "", "if", "self", ".", "batch_norm_hidden", ":", "\n", "                ", "self", ".", "hids", ".", "append", "(", "BatchNormalization", "(", ")", ")", "\n", "# output layer with linear activation by default", "\n", "", "", "self", ".", "output_layer", "=", "Dense", "(", "out_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DeepNetworkModel.call": [[113, 149], ["DQN.DeepNetworkModel.output_layer", "DQN.DeepNetworkModel.input_layer", "DQN.DeepNetworkModel.bnorm_layer", "DQN.DeepNetworkModel.input_layer", "layer", "layer"], "methods", ["None"], ["", "def", "call", "(", "\n", "self", ",", "inputs", ":", "Union", "[", "np", ".", "ndarray", "or", "tf", ".", "Tensor", "]", ",", "training", ":", "bool", "=", "True", ",", "\n", ")", "->", "Union", "[", "np", ".", "ndarray", "or", "tf", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Instantiate Deep Q Network Class\n\n        Parameters\n        ----------\n\n        inputs: Union[np.ndarray or tf.Tensor]\n            Inputs to the neural network\n\n        training: bool\n            Boolean to regulate if inference or test time\n\n        Returns\n        ----------\n        z: Union[np.ndarray or tf.Tensor]\n            Outputs of the neural network after a forward pass\n\n        \"\"\"", "\n", "# build the input layer", "\n", "if", "self", ".", "bnflag_input", ":", "\n", "            ", "z", "=", "self", ".", "input_layer", "(", "inputs", ")", "\n", "z", "=", "self", ".", "bnorm_layer", "(", "z", ",", "training", ")", "\n", "", "else", ":", "\n", "            ", "z", "=", "self", ".", "input_layer", "(", "inputs", ")", "\n", "# build the hidden layer", "\n", "", "for", "layer", "in", "self", ".", "hids", ":", "\n", "            ", "if", "\"batch\"", "in", "layer", ".", "name", ":", "\n", "                ", "z", "=", "layer", "(", "z", ",", "training", ")", "\n", "", "else", ":", "\n", "                ", "z", "=", "layer", "(", "z", ")", "\n", "# build the output layer", "\n", "", "", "z", "=", "self", ".", "output_layer", "(", "z", ")", "\n", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.__init__": [[156, 469], ["int", "int", "len", "DQN.DeepNetworkModel", "DQN.DeepNetworkModel", "tensorflow.keras.optimizers.schedules.ExponentialDecay", "tensorflow.keras.optimizers.Adam", "utils.exploration.PER_buffer", "tensorflow.keras.losses.MeanSquaredError", "tensorflow.keras.optimizers.RMSprop", "tensorflow.keras.losses.Huber"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "seed", ":", "int", ",", "\n", "DQN_type", ":", "str", ",", "\n", "gamma", ":", "float", ",", "\n", "epsilon", ":", "float", ",", "\n", "min_eps_pct", ":", "float", ",", "\n", "min_eps", ":", "float", ",", "\n", "max_exp_pct", ":", "float", ",", "\n", "update_target", ":", "str", ",", "\n", "copy_step", ":", "int", ",", "\n", "tau", ":", "float", ",", "\n", "input_shape", ":", "int", ",", "\n", "hidden_units", ":", "list", ",", "\n", "hidden_memory_units", ":", "list", ",", "\n", "batch_size", ":", "int", ",", "\n", "selected_loss", ":", "str", ",", "\n", "lr", ":", "float", ",", "\n", "start_train", ":", "int", ",", "\n", "optimizer_name", ":", "str", ",", "\n", "batch_norm_input", ":", "str", ",", "\n", "batch_norm_hidden", ":", "str", ",", "\n", "activation", ":", "str", ",", "\n", "kernel_initializer", ":", "str", ",", "\n", "action_space", ",", "\n", "use_PER", ":", "bool", "=", "False", ",", "\n", "PER_e", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "PER_a", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "PER_b", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "final_PER_b", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "PER_b_growth", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "final_PER_a", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "PER_a_growth", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "sample_type", ":", "str", "=", "\"TDerror\"", ",", "\n", "beta_1", ":", "float", "=", "0.9", ",", "\n", "beta_2", ":", "float", "=", "0.999", ",", "\n", "eps_opt", ":", "float", "=", "1e-07", ",", "\n", "lr_schedule", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "exp_decay_pct", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "exp_decay_rate", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "rng", "=", "None", ",", "\n", "N_train", ":", "int", "=", "100000", ",", "\n", "modelname", ":", "str", "=", "\"Deep Network\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate DQN Class\n\n        Parameters\n        ----------\n        seed: int\n            Seed for experiment reproducibility\n\n        DQN_type: str\n            DQN variant choice. It can be 'DQN' or 'DDQN'\n\n        recurrent_env: bool\n            Boolean to regulate if the environment is recurrent or not\n\n        gamma: float\n            Discount parameter for the target update\n\n        max_exp_pct: int\n            Max size of the experience replay buffer as a pct of the total iterations\n\n        update_target: str\n            Choice for target update. It can be 'hard' or 'soft'\n\n        tau: float\n            When the update is 'soft', tau regulates the amount of the update\n            towards the current parameters\n\n        input_shape: int\n            Shape of input of the neural network\n\n        hidden_units: list\n            List of sizes of hidden layers. The length of the list determines\n            the depth of the Q network\n\n        hidden_memory_units: list,\n            List of sizes of recurrent hidden layers. The length of the list determines\n            the depth of the Q network\n\n        batch_size: int\n            Size of the batch to perform an update\n\n        selected_loss: str\n            Choice for the loss function. It can be 'mse' or 'huber'\n\n        lr: float\n            Initial learning rate\n\n        start_train: int\n            Number of iteration after which the training starts\n\n        optimizer_name: str\n            Choice for the optimizer. It can be 'sgd', 'sgdmom', 'sgdnest',\n            'adagrad', 'adadelta', 'adamax', 'adam', 'amsgrad', 'nadam', or 'rmsprop'\n\n        batch_norm_input: bool\n            Boolean to regulate the presence of a Batch Norm layer after the input\n\n        batch_norm_hidden: bool\n            Boolean to regulate the presence of a Batch Norm layer after each hidden layer\n\n        activation: str\n            Choice of activation function. It can be 'leaky_relu',\n            'relu6' or 'elu'\n\n        kernel_initializer: str\n            Choice of weight initialization as aliased in TF2.0 documentation\n\n        plot_hist: bool\n            Boolean to regulate if plot the histogram of intermediate outputs\n            in tensorboard\n\n        plot_steps_hist: int\n            Number of steps at which the histogram of intermediate outputs are\n            plotted in tensorboard\n\n        plot_steps: int\n            Number of steps at which all the other variables are\n            stored in tensorboard\n\n        summary_writer, #TODO need to add proper type hint\n            Tensorabord writer\n        action_space: class\n            Space of possible action as class that inherits from gym\n\n        use_PER: bool = False\n            Boolean to regulate if use Prioritized Experience Replay (PER) or not\n\n        PER_e: Optional[float]\n            Correction for priorities\n\n        PER_a: Optional[float]\n            Amount of prioritization\n\n        PER_b: Optional[float]\n            Amount of correction for introduced bias when using PER\n\n        final_PER_b: Optional[float] = None\n            Final value for b after the anneal\n\n        PER_b_growth: Optional[float]\n            Rate of increase of the b\n\n        final_PER_a: Optional[float] = None\n            Final value for a after the anneal\n\n        PER_a_growth: Optional[float]\n            Rate of increase of the a\n\n        sample_type : str\n            Type of sampling in PER. It can be 'TDerror', 'diffTDerror' or 'reward'\n\n        clipgrad: bool\n            Choice of the gradients to clip. It can be 'norm', 'value' or 'globnorm'\n\n        clipnorm: Optional[Union[str or float]]\n            Boolean for clipping the norm of the gradients\n\n        clipvalue: Optional[Union[str or float]]\n            Boolean for clipping the value of the gradients\n\n        clipglob_steps: Optional[int]\n            Boolean for clipping the global norm of the gradients\n\n        beta_1: float = 0.9\n            Parameter for adaptive optimizer\n\n        beta_2: float = 0.999\n            Parameter for adaptive optimizer\n\n        eps_opt: float = 1e-07\n            Corrective parameter for adaptive optimizer\n\n        std_rwds: bool = False\n            Boolean to regulate if standardize rewards or not\n\n        lr_schedule: Optional[str]\n            Choice for the learning rate schedule. It can be 'exponential',\n            'piecewise', 'inverse_time' or 'polynomial'\n\n        exp_decay_pct: Optional[float]\n             Amount of steps to reach the desired level of decayed learning rate as pct\n             of the total iteration\n\n        exp_decay_rate: Optional[float]\n            Rate of decay to reach the desired level of decayed learning rate\n\n        rng = None\n            Random number generator for reproducibility\n\n        modelname: str\n            Name for the model\n\n        \"\"\"", "\n", "\n", "if", "rng", "is", "not", "None", ":", "\n", "            ", "self", ".", "rng", "=", "rng", "\n", "\n", "", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "exp_decay_steps", "=", "int", "(", "N_train", "*", "exp_decay_pct", ")", "\n", "if", "lr_schedule", "==", "\"exponential\"", ":", "\n", "            ", "lr", "=", "ExponentialDecay", "(", "\n", "initial_learning_rate", "=", "lr", ",", "\n", "decay_steps", "=", "exp_decay_steps", ",", "\n", "decay_rate", "=", "exp_decay_rate", ",", "\n", ")", "\n", "\n", "", "if", "optimizer_name", "==", "\"adam\"", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "\n", "learning_rate", "=", "lr", ",", "\n", "beta_1", "=", "beta_1", ",", "\n", "beta_2", "=", "beta_2", ",", "\n", "epsilon", "=", "eps_opt", ",", "\n", "amsgrad", "=", "False", ",", "\n", ")", "\n", "", "elif", "optimizer_name", "==", "\"rmsprop\"", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "RMSprop", "(", "\n", "learning_rate", "=", "lr", ",", "\n", "rho", "=", "beta_1", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "eps_opt", ",", "\n", "centered", "=", "False", ",", "\n", ")", "\n", "\n", "", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "eps_opt", "=", "eps_opt", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n", "self", ".", "max_experiences", "=", "int", "(", "N_train", "*", "max_exp_pct", ")", "\n", "\n", "self", ".", "use_PER", "=", "use_PER", "\n", "if", "self", ".", "use_PER", ":", "\n", "\n", "            ", "if", "PER_b_growth", ":", "\n", "                ", "PER_b_steps", "=", "N_train", "\n", "PER_b_growth", "=", "(", "final_PER_b", "-", "PER_b", ")", "/", "PER_b_steps", "\n", "", "else", ":", "\n", "                ", "PER_b_growth", "=", "0.0", "\n", "PER_b_steps", "=", "None", "\n", "\n", "", "if", "PER_a_growth", ":", "\n", "                ", "PER_a_steps", "=", "PER_a_steps", "=", "N_train", "\n", "PER_a_growth", "=", "(", "final_PER_a", "-", "PER_a", ")", "/", "PER_a_steps", "\n", "", "else", ":", "\n", "                ", "PER_a_growth", "=", "0.0", "\n", "PER_a_steps", "=", "None", "\n", "\n", "", "self", ".", "PERmemory", "=", "PER_buffer", "(", "\n", "PER_e", ",", "\n", "PER_a", ",", "\n", "PER_b", ",", "\n", "final_PER_b", ",", "\n", "PER_b_steps", ",", "\n", "PER_b_growth", ",", "\n", "final_PER_a", ",", "\n", "PER_a_steps", ",", "\n", "PER_a_growth", ",", "\n", "self", ".", "max_experiences", ",", "\n", "rng", ",", "\n", "sample_type", ",", "\n", ")", "# experience is stored as object of this class", "\n", "", "else", ":", "\n", "            ", "self", ".", "experience", "=", "{", "\"s\"", ":", "[", "]", ",", "\"a\"", ":", "[", "]", ",", "\"r\"", ":", "[", "]", ",", "\"s2\"", ":", "[", "]", ",", "\"a_unsc\"", ":", "[", "]", "}", "\n", "\n", "", "self", ".", "start_train", "=", "start_train", "\n", "self", ".", "action_space", "=", "action_space", "\n", "self", ".", "num_actions", "=", "len", "(", "self", ".", "action_space", ".", "values", ")", "\n", "self", ".", "batch_norm_input", "=", "batch_norm_input", "\n", "self", ".", "batch_norm_hidden", "=", "batch_norm_hidden", "\n", "\n", "self", ".", "model", "=", "DeepNetworkModel", "(", "\n", "seed", ",", "\n", "input_shape", ",", "\n", "hidden_units", ",", "\n", "self", ".", "num_actions", ",", "\n", "batch_norm_input", ",", "\n", "batch_norm_hidden", ",", "\n", "activation", ",", "\n", "kernel_initializer", ",", "\n", "modelname", ",", "\n", ")", "\n", "\n", "self", ".", "target_model", "=", "DeepNetworkModel", "(", "\n", "seed", ",", "\n", "input_shape", ",", "\n", "hidden_units", ",", "\n", "self", ".", "num_actions", ",", "\n", "batch_norm_input", ",", "\n", "batch_norm_hidden", ",", "\n", "activation", ",", "\n", "kernel_initializer", ",", "\n", "\"Target \"", "+", "modelname", ",", "\n", ")", "\n", "\n", "self", ".", "selected_loss", "=", "selected_loss", "\n", "self", ".", "DQN_type", "=", "DQN_type", "\n", "self", ".", "update_target", "=", "update_target", "\n", "self", ".", "copy_step", "=", "copy_step", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "optimizer_name", "=", "optimizer_name", "\n", "\n", "if", "self", ".", "selected_loss", "==", "\"mse\"", ":", "\n", "            ", "self", ".", "loss", "=", "tf", ".", "keras", ".", "losses", ".", "MeanSquaredError", "(", ")", "\n", "", "elif", "self", ".", "selected_loss", "==", "\"huber\"", ":", "\n", "            ", "self", ".", "loss", "=", "tf", ".", "keras", ".", "losses", ".", "Huber", "(", ")", "\n", "\n", "", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "min_eps", "=", "min_eps", "\n", "self", ".", "min_eps_pct", "=", "min_eps_pct", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.train": [[470, 586], ["tape.gradient", "DQN.DQN.optimizer.apply_gradients", "DQN.DQN.PERmemory.sample_batch", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "DQN.DQN.rng.randint", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "tensorflow.GradientTape", "tensorflow.math.reduce_sum", "zip", "DQN.DQN.action_space.values.tolist().index", "numpy.max", "min", "DQN.DQN.loss", "DQN.DQN.loss", "len", "DQN.DQN.model", "tensorflow.one_hot", "DQN.DQN.target_model", "tensorflow.math.argmax", "tensorflow.math.reduce_sum", "numpy.max", "DQN.DQN.PERmemory.batch_update", "DQN.DQN.action_space.values.tolist", "numpy.atleast_2d", "numpy.asarray.astype", "DQN.DQN.model", "numpy.abs", "DQN.DQN.PERmemory.batch_update", "print", "sys.exit", "scaled_w_IS.reshape", "numpy.asarray.astype", "numpy.asarray.astype", "DQN.DQN.target_model", "tensorflow.one_hot", "numpy.abs", "numpy.asarray.astype"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.sample_batch", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.batch_update", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.batch_update"], ["", "def", "train", "(", "\n", "self", ",", "\n", "iteration", ":", "int", ",", "\n", "side_only", ":", "bool", "=", "False", ",", "\n", "bcm", ":", "bool", "=", "False", ",", "\n", "bcm_scale", ":", "float", "=", "0.01", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\"Parameters\n        ----------\n        iteration: int\n            Number of iteration update\n        side_only: bool\n            Regulate the decoupling between side and size of the bet\n\n        bcm: bool\n            Regulate the part of the loss relative to the behaviorl cloning of an expert\n        \"\"\"", "\n", "if", "iteration", "<", "self", ".", "start_train", ":", "\n", "            ", "return", "0", "\n", "\n", "", "if", "self", ".", "use_PER", ":", "\n", "            ", "b_idx", ",", "minibatch", "=", "self", ".", "PERmemory", ".", "sample_batch", "(", "self", ".", "batch_size", ")", "\n", "states", "=", "np", ".", "asarray", "(", "minibatch", "[", "\"s\"", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "minibatch", "[", "\"a\"", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "minibatch", "[", "\"r\"", "]", ")", "\n", "states_next", "=", "np", ".", "asarray", "(", "minibatch", "[", "\"s2\"", "]", ")", "\n", "\n", "", "else", ":", "\n", "# find the index of streams included in the experience buffer that will", "\n", "# composed the training batch", "\n", "            ", "ids", "=", "self", ".", "rng", ".", "randint", "(", "\n", "low", "=", "0", ",", "high", "=", "len", "(", "self", ".", "experience", "[", "\"s\"", "]", ")", ",", "size", "=", "self", ".", "batch_size", "\n", ")", "\n", "\n", "states", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "\"s\"", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "\"a\"", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "\"r\"", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "states_next", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "\"s2\"", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "\n", "", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "\n", "# compute current action values", "\n", "# find index of actions included in the batch", "\n", "            ", "encoded_actions", "=", "[", "\n", "self", ".", "action_space", ".", "values", ".", "tolist", "(", ")", ".", "index", "(", "act", ")", "for", "act", "in", "actions", "\n", "]", "\n", "selected_action_values", "=", "tf", ".", "math", ".", "reduce_sum", "(", "\n", "self", ".", "model", "(", "np", ".", "atleast_2d", "(", "states", ".", "astype", "(", "\"float32\"", ")", ")", ",", ")", "\n", "*", "tf", ".", "one_hot", "(", "encoded_actions", ",", "self", ".", "num_actions", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "# compute target action values", "\n", "if", "self", ".", "DQN_type", "==", "\"DQN\"", ":", "\n", "                ", "value_next", "=", "np", ".", "max", "(", "\n", "self", ".", "target_model", "(", "states_next", ".", "astype", "(", "\"float32\"", ")", ")", ",", "axis", "=", "1", "\n", ")", "\n", "", "elif", "self", ".", "DQN_type", "==", "\"DDQN\"", ":", "\n", "                ", "greedy_target_action", "=", "tf", ".", "math", ".", "argmax", "(", "\n", "self", ".", "model", "(", "states_next", ".", "astype", "(", "\"float32\"", ")", ")", ",", "1", "\n", ")", "\n", "value_next", "=", "tf", ".", "math", ".", "reduce_sum", "(", "\n", "self", ".", "target_model", "(", "states_next", ".", "astype", "(", "\"float32\"", ")", ")", "\n", "*", "tf", ".", "one_hot", "(", "greedy_target_action", ",", "self", ".", "num_actions", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "", "actual_values", "=", "rewards", "+", "self", ".", "gamma", "*", "value_next", "\n", "\n", "if", "self", ".", "use_PER", ":", "\n", "# compute weights", "\n", "                ", "if", "iteration", "<", "self", ".", "max_experiences", ":", "\n", "                    ", "N", "=", "iteration", "+", "1", "\n", "", "else", ":", "\n", "                    ", "N", "=", "self", ".", "max_experiences", "\n", "", "prob", "=", "self", ".", "PERmemory", ".", "tree", "[", "b_idx", "]", "/", "self", ".", "PERmemory", ".", "total_priority", "\n", "self", ".", "PERmemory", ".", "PER_b", "=", "min", "(", "\n", "self", ".", "PERmemory", ".", "final_PER_b", ",", "\n", "self", ".", "PERmemory", ".", "PER_b", "+", "self", ".", "PERmemory", ".", "PER_b_growth", ",", "\n", ")", "\n", "w_IS", "=", "(", "N", "*", "prob", ")", "**", "(", "-", "self", ".", "PERmemory", ".", "PER_b", ")", "\n", "scaled_w_IS", "=", "w_IS", "/", "np", ".", "max", "(", "w_IS", ")", "\n", "\n", "# update priorities", "\n", "if", "self", ".", "PERmemory", ".", "sample_type", "==", "\"rewards\"", ":", "\n", "                    ", "self", ".", "PERmemory", ".", "batch_update", "(", "b_idx", ",", "np", ".", "abs", "(", "rewards", ")", ")", "\n", "", "elif", "(", "\n", "self", ".", "PERmemory", ".", "sample_type", "==", "\"TDerror\"", "\n", "or", "self", ".", "PERmemory", ".", "sample_type", "==", "\"diffTDerror\"", "\n", ")", ":", "\n", "                    ", "self", ".", "PERmemory", ".", "batch_update", "(", "\n", "b_idx", ",", "np", ".", "abs", "(", "actual_values", "-", "selected_action_values", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"Sample type for PER not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "# compute loss function for the train model", "\n", "", "loss", "=", "self", ".", "loss", "(", "\n", "y_true", "=", "actual_values", ",", "\n", "y_pred", "=", "selected_action_values", ",", "\n", "sample_weight", "=", "scaled_w_IS", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "\n", "                ", "loss", "=", "self", ".", "loss", "(", "y_true", "=", "actual_values", ",", "y_pred", "=", "selected_action_values", ")", "\n", "\n", "", "", "variables", "=", "self", ".", "model", ".", "trainable_variables", "\n", "\n", "# compute gradient of the loss with respect to the variables (weights)", "\n", "gradients", "=", "tape", ".", "gradient", "(", "loss", ",", "variables", ")", "\n", "\n", "# provide a list of (gradient, variable) pairs.", "\n", "self", ".", "optimizer", ".", "apply_gradients", "(", "zip", "(", "gradients", ",", "variables", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.eps_greedy_action": [[587, 631], ["DQN.DQN.rng.random", "DQN.DQN.rng.choice", "DQN.DQN.rng.random", "DQN.DQN.rng.choice", "DQN.DQN.model", "numpy.atleast_2d", "numpy.argmax", "states.astype", "numpy.argmax", "DQN.DQN.model", "numpy.atleast_2d", "states.astype"], "methods", ["None"], ["", "def", "eps_greedy_action", "(", "\n", "self", ",", "states", ":", "np", ".", "ndarray", ",", "epsilon", ":", "float", ",", "side_only", ":", "bool", "=", "False", "\n", ")", "->", "Tuple", "[", "Union", "[", "float", "or", "int", "]", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"Parameters\n        ----------\n        states: np.ndarray\n            Current state representation\n\n        epsilon: float\n            Epsilon parameter for exploration\n\n        side_only: bool\n            Regulate the decoupling between side and size of the bet\n\n        Returns\n        ----------\n        action: Union[float or int]\n            Epsilon greedy selected action\n        qvalues : np.ndarray\n            Q values associated to the actions space\n        \"\"\"", "\n", "if", "not", "side_only", ":", "\n", "            ", "if", "self", ".", "rng", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "                ", "action", "=", "self", ".", "rng", ".", "choice", "(", "self", ".", "action_space", ".", "values", ")", "\n", "return", "action", ",", "None", "\n", "", "else", ":", "\n", "                ", "action", "=", "self", ".", "action_space", ".", "values", "[", "\n", "np", ".", "argmax", "(", "\n", "self", ".", "model", "(", "\n", "np", ".", "atleast_2d", "(", "states", ".", "astype", "(", "\"float32\"", ")", ")", ",", "training", "=", "False", "\n", ")", "[", "0", "]", "\n", ")", "\n", "]", "\n", "return", "action", ",", "None", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "rng", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "                ", "action", "=", "self", ".", "rng", ".", "choice", "(", "self", ".", "action_space", ".", "values", ")", "\n", "return", "action", ",", "None", "\n", "", "else", ":", "\n", "                ", "qvalues", "=", "self", ".", "model", "(", "\n", "np", ".", "atleast_2d", "(", "states", ".", "astype", "(", "\"float32\"", ")", ")", ",", "training", "=", "False", "\n", ")", "\n", "action", "=", "self", ".", "action_space", ".", "values", "[", "np", ".", "argmax", "(", "qvalues", "[", "0", "]", ")", "]", "\n", "return", "action", ",", "qvalues", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.greedy_action": [[632, 664], ["DQN.DQN.model", "DQN.DQN.model", "numpy.atleast_2d", "numpy.atleast_2d", "states.astype", "numpy.argmax", "states.astype", "numpy.argmax"], "methods", ["None"], ["", "", "", "def", "greedy_action", "(", "\n", "self", ",", "states", ":", "np", ".", "ndarray", ",", "side_only", ":", "bool", "=", "False", "\n", ")", "->", "Tuple", "[", "Union", "[", "float", "or", "int", "]", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"Parameters\n        ----------\n        states: np.ndarray\n            Current state representation\n\n        side_only: bool\n            Regulate the decoupling between side and size of the bet\n\n        Returns\n        ----------\n        action: Union[float or int]\n            Greedy selected action\n\n        qvalues : np.ndarray\n            Q values associated to the actions space\n        \"\"\"", "\n", "if", "not", "side_only", ":", "\n", "            ", "qvalues", "=", "self", ".", "model", "(", "\n", "np", ".", "atleast_2d", "(", "states", ".", "astype", "(", "\"float32\"", ")", ")", ",", "training", "=", "False", "\n", ")", "\n", "action", "=", "self", ".", "action_space", ".", "values", "[", "np", ".", "argmax", "(", "qvalues", "[", "0", "]", ")", "]", "\n", "return", "action", ",", "None", "\n", "", "else", ":", "\n", "\n", "            ", "qvalues", "=", "self", ".", "model", "(", "\n", "np", ".", "atleast_2d", "(", "states", ".", "astype", "(", "\"float32\"", ")", ")", ",", "training", "=", "False", "\n", ")", "\n", "action", "=", "self", ".", "action_space", ".", "values", "[", "np", ".", "argmax", "(", "qvalues", "[", "0", "]", ")", "]", "\n", "return", "action", ",", "qvalues", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.add_experience": [[665, 681], ["DQN.DQN.PERmemory.add", "exp.items", "len", "DQN.DQN.experience.keys", "DQN.DQN.experience[].append", "DQN.DQN.experience[].pop"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.exploration.PER_buffer.add"], ["", "", "def", "add_experience", "(", "self", ",", "exp", ")", ":", "\n", "        ", "\"\"\"Parameters\n        ----------\n        exp: dict\n            Sequences of experience to store\n\n        \"\"\"", "\n", "if", "self", ".", "use_PER", ":", "\n", "            ", "self", ".", "PERmemory", ".", "add", "(", "exp", ")", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "self", ".", "experience", "[", "\"s\"", "]", ")", ">=", "self", ".", "max_experiences", ":", "\n", "                ", "for", "key", "in", "self", ".", "experience", ".", "keys", "(", ")", ":", "\n", "                    ", "if", "self", ".", "experience", "[", "key", "]", ":", "# check if the list is not empty", "\n", "                        ", "self", ".", "experience", "[", "key", "]", ".", "pop", "(", "0", ")", "\n", "", "", "", "for", "key", ",", "value", "in", "exp", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "experience", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.copy_weights": [[682, 697], ["zip", "zip", "v1.assign", "v1.assign", "vsoft.numpy", "v2.numpy"], "methods", ["None"], ["", "", "", "def", "copy_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Parameters\n        ----------\n        \"\"\"", "\n", "if", "self", ".", "update_target", "==", "\"soft\"", ":", "\n", "            ", "variables1", "=", "self", ".", "target_model", ".", "trainable_variables", "\n", "variables2", "=", "self", ".", "model", ".", "trainable_variables", "\n", "for", "v1", ",", "v2", "in", "zip", "(", "variables1", ",", "variables2", ")", ":", "\n", "                ", "vsoft", "=", "(", "1", "-", "self", ".", "tau", ")", "*", "v1", "+", "self", ".", "tau", "*", "v2", "\n", "v1", ".", "assign", "(", "vsoft", ".", "numpy", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "variables1", "=", "self", ".", "target_model", ".", "trainable_variables", "\n", "variables2", "=", "self", ".", "model", ".", "trainable_variables", "\n", "for", "v1", ",", "v2", "in", "zip", "(", "variables1", ",", "variables2", ")", ":", "\n", "                ", "v1", ".", "assign", "(", "v2", ".", "numpy", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.update_epsilon": [[698, 700], ["max"], "methods", ["None"], ["", "", "", "def", "update_epsilon", "(", "self", ")", ":", "\n", "        ", "self", ".", "epsilon", "=", "max", "(", "self", ".", "min_eps", ",", "self", ".", "epsilon", "-", "self", ".", "eps_decay", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN._get_exploration_length": [[701, 704], ["int"], "methods", ["None"], ["", "def", "_get_exploration_length", "(", "self", ",", "N_train", ")", ":", "\n", "        ", "steps_to_min_eps", "=", "int", "(", "N_train", "*", "self", ".", "min_eps_pct", ")", "\n", "self", ".", "eps_decay", "=", "(", "self", ".", "epsilon", "-", "self", ".", "min_eps", ")", "/", "steps_to_min_eps", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPOActorCritic.__init__": [[21, 149], ["torch.Module.__init__", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "range", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "PPO.PPOActorCritic.init_weights", "critic_modules.append", "len", "actor_modules.append", "len", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "critic_modules.append", "critic_modules.append", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "actor_modules.append", "actor_modules.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "critic_modules.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "critic_modules.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "actor_modules.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "actor_modules.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Softplus", "torch.Softplus", "torch.Softplus", "torch.Softplus", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "critic_modules.append", "print", "sys.exit", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "critic_modules.append", "print", "sys.exit", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "actor_modules.append", "print", "sys.exit", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "actor_modules.append", "print", "sys.exit", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.__init__", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPOActorCritic.init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "seed", ":", "int", ",", "\n", "input_shape", ":", "int", ",", "\n", "activation", ":", "str", ",", "\n", "hidden_units_value", ":", "list", ",", "\n", "hidden_units_actor", ":", "list", ",", "\n", "num_actions", ":", "int", ",", "\n", "batch_norm_input", ":", "bool", ",", "\n", "batch_norm_value_out", ":", "bool", ",", "\n", "policy_type", ":", "str", ",", "\n", "init_std", ":", "float", "=", "0.5", ",", "\n", "min_std", ":", "float", "=", "0.003", ",", "\n", "std_transform", ":", "str", "=", "\"softplus\"", ",", "\n", "init_last_layers", ":", "str", "=", "\"rescaled\"", ",", "\n", "modelname", ":", "str", "=", "\"PPO\"", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "PPOActorCritic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seed", "=", "seed", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "self", ".", "modelname", "=", "modelname", "\n", "# set dimensionality of input/output depending on the model", "\n", "inp_dim", "=", "input_shape", "[", "0", "]", "\n", "out_dim", "=", "num_actions", "\n", "self", ".", "policy_type", "=", "policy_type", "\n", "self", ".", "std_transform", "=", "std_transform", "\n", "self", ".", "init_last_layers", "=", "init_last_layers", "\n", "\n", "# set flag for batch norm as attribute", "\n", "self", ".", "bnflag_input", "=", "batch_norm_input", "\n", "\n", "critic_modules", "=", "[", "]", "\n", "\n", "if", "self", ".", "bnflag_input", ":", "\n", "# affine false sould be equal to center and scale to False in TF2", "\n", "            ", "critic_modules", ".", "append", "(", "\n", "nn", ".", "BatchNorm1d", "(", "inp_dim", ",", "affine", "=", "False", ")", "\n", ")", "# , momentum=1.0", "\n", "# critic_modules.append(nn.LayerNorm(inp_dim, elementwise_affine=False))", "\n", "\n", "# self.input_layer = InputLayer(input_shape=inp_shape)", "\n", "# set of hidden layers", "\n", "", "for", "i", "in", "range", "(", "len", "(", "hidden_units_value", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "critic_modules", ".", "append", "(", "nn", ".", "Linear", "(", "inp_dim", ",", "hidden_units_value", "[", "i", "]", ")", ")", "\n", "if", "activation", "==", "\"relu\"", ":", "\n", "                    ", "critic_modules", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "activation", "==", "\"tanh\"", ":", "\n", "                    ", "critic_modules", ".", "append", "(", "nn", ".", "Tanh", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"Activation selected not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "critic_modules", ".", "append", "(", "\n", "nn", ".", "Linear", "(", "hidden_units_value", "[", "i", "-", "1", "]", ",", "hidden_units_value", "[", "i", "]", ")", "\n", ")", "\n", "if", "activation", "==", "\"relu\"", ":", "\n", "                    ", "critic_modules", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "activation", "==", "\"tanh\"", ":", "\n", "                    ", "critic_modules", ".", "append", "(", "nn", ".", "Tanh", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"Activation selected not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "", "", "if", "batch_norm_value_out", ":", "\n", "            ", "critic_modules", "=", "critic_modules", "+", "[", "\n", "nn", ".", "Linear", "(", "hidden_units_value", "[", "-", "1", "]", ",", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "1", ",", "affine", "=", "False", ")", ",", "\n", "# nn.LayerNorm(1, elementwise_affine=False),", "\n", "]", "\n", "", "else", ":", "\n", "            ", "critic_modules", "=", "critic_modules", "+", "[", "nn", ".", "Linear", "(", "hidden_units_value", "[", "-", "1", "]", ",", "1", ")", "]", "\n", "\n", "", "actor_modules", "=", "[", "]", "\n", "\n", "if", "self", ".", "bnflag_input", ":", "\n", "# affine false sould be equal to center and scale to False in TF2", "\n", "            ", "actor_modules", ".", "append", "(", "nn", ".", "BatchNorm1d", "(", "inp_dim", ",", "affine", "=", "False", ")", ")", "\n", "# actor_modules.append(nn.LayerNorm(inp_dim, elementwise_affine=False))", "\n", "\n", "# self.input_layer = InputLayer(input_shape=inp_shape)", "\n", "# set of hidden layers", "\n", "", "for", "i", "in", "range", "(", "len", "(", "hidden_units_actor", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "actor_modules", ".", "append", "(", "nn", ".", "Linear", "(", "inp_dim", ",", "hidden_units_actor", "[", "i", "]", ")", ")", "\n", "if", "activation", "==", "\"relu\"", ":", "\n", "                    ", "actor_modules", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "activation", "==", "\"tanh\"", ":", "\n", "                    ", "actor_modules", ".", "append", "(", "nn", ".", "Tanh", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"Activation selected not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "actor_modules", ".", "append", "(", "\n", "nn", ".", "Linear", "(", "hidden_units_actor", "[", "i", "-", "1", "]", ",", "hidden_units_actor", "[", "i", "]", ")", "\n", ")", "\n", "if", "activation", "==", "\"relu\"", ":", "\n", "                    ", "actor_modules", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "activation", "==", "\"tanh\"", ":", "\n", "                    ", "actor_modules", ".", "append", "(", "nn", ".", "Tanh", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"Activation selected not available\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "", "", "actor_modules", "=", "actor_modules", "+", "[", "nn", ".", "Linear", "(", "hidden_units_actor", "[", "-", "1", "]", ",", "out_dim", ")", "]", "\n", "\n", "self", ".", "critic", "=", "nn", ".", "Sequential", "(", "*", "critic_modules", ")", "\n", "\n", "self", ".", "actor", "=", "nn", ".", "Sequential", "(", "*", "actor_modules", ")", "\n", "\n", "# TODO insert here flag for cont/discrete", "\n", "if", "self", ".", "policy_type", "==", "\"continuous\"", ":", "\n", "            ", "if", "self", ".", "std_transform", "==", "\"exp\"", ":", "\n", "                ", "self", ".", "log_std", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "out_dim", ")", "*", "init_std", ")", "\n", "", "elif", "self", ".", "std_transform", "==", "\"softplus\"", ":", "\n", "                ", "self", ".", "log_std", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "out_dim", ")", "*", "init_std", ")", "\n", "self", ".", "softplus", "=", "nn", ".", "Softplus", "(", ")", "\n", "self", ".", "init_std", "=", "init_std", "\n", "self", ".", "min_std", "=", "min_std", "\n", "\n", "", "", "elif", "self", ".", "policy_type", "==", "\"discrete\"", ":", "\n", "            ", "pass", "\n", "# I could add an initial offset to be sure that output actions are sufficiently low", "\n", "\n", "# I didn't use apply because I wanted different init for different layer", "\n", "# I guess it should be ok anyway but it has to be tested", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPOActorCritic.forward": [[150, 182], ["PPO.PPOActorCritic.critic", "PPO.PPOActorCritic.actor", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "PPO.PPOActorCritic.log_std.exp().expand_as", "PPO.PPOActorCritic.actor", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "PPO.PPOActorCritic.log_std.exp", "PPO.PPOActorCritic.softplus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "PPO.PPOActorCritic.softplus"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "value", "=", "self", ".", "critic", "(", "x", ")", "\n", "if", "self", ".", "policy_type", "==", "\"continuous\"", ":", "\n", "            ", "mu", "=", "self", ".", "actor", "(", "x", ")", "\n", "if", "self", ".", "std_transform", "==", "\"exp\"", ":", "\n", "                ", "std", "=", "self", ".", "log_std", ".", "exp", "(", ")", ".", "expand_as", "(", "\n", "mu", "\n", ")", "# make the tensor of the same size of mu", "\n", "", "elif", "self", ".", "std_transform", "==", "\"softplus\"", ":", "\n", "                ", "init_const", "=", "1", "/", "self", ".", "softplus", "(", "\n", "torch", ".", "tensor", "(", "self", ".", "init_std", "-", "self", ".", "min_std", ")", "\n", ")", "\n", "std", "=", "(", "\n", "self", ".", "softplus", "(", "self", ".", "log_std", "+", "init_const", ")", "+", "self", ".", "min_std", "\n", ")", ".", "expand_as", "(", "mu", ")", "\n", "\n", "", "dist", "=", "Normal", "(", "mu", ",", "std", ")", "\n", "\n", "", "elif", "self", ".", "policy_type", "==", "\"discrete\"", ":", "\n", "\n", "            ", "logits", "=", "self", ".", "actor", "(", "x", ")", "\n", "# correct when logits contain nans", "\n", "# if torch.isnan(logits).sum() > 0:", "\n", "#     logits = (", "\n", "#         torch.empty(logits.shape)", "\n", "#         .uniform_(-0.01, 0.01)", "\n", "#         .type(torch.FloatTensor)", "\n", "#     )", "\n", "\n", "dist", "=", "Categorical", "(", "logits", "=", "logits", ")", "\n", "\n", "", "return", "dist", ",", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPOActorCritic.init_weights": [[183, 203], ["isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "isinstance", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "numpy.sqrt", "numpy.sqrt"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# to access module and layer of an architecture", "\n", "# https://discuss.pytorch.org/t/how-to-access-to-a-layer-by-module-name/83797/2", "\n", "        ", "for", "layer", "in", "self", ".", "actor", ":", "# [:-1]:", "\n", "            ", "if", "isinstance", "(", "layer", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "orthogonal_", "(", "layer", ".", "weight", ",", "gain", "=", "np", ".", "sqrt", "(", "2", ")", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "layer", ".", "bias", ")", "\n", "\n", "# carefully initialize last layer", "\n", "", "", "if", "self", ".", "init_last_layers", "==", "\"rescaled\"", ":", "\n", "            ", "self", ".", "actor", "[", "-", "1", "]", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "self", ".", "actor", "[", "-", "1", "]", ".", "weight", "*", "0.01", ")", "\n", "self", ".", "actor", "[", "-", "1", "]", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "self", ".", "actor", "[", "-", "1", "]", ".", "bias", "*", "0.01", ")", "\n", "", "elif", "self", ".", "init_last_layers", "==", "\"normal\"", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "actor", "[", "-", "1", "]", ".", "weight", ",", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "actor", "[", "-", "1", "]", ".", "bias", ",", "0.01", ")", "\n", "\n", "", "for", "layer", "in", "self", ".", "critic", ":", "# [:-2]:", "\n", "            ", "if", "isinstance", "(", "layer", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "orthogonal_", "(", "layer", ".", "weight", ",", "gain", "=", "np", ".", "sqrt", "(", "2", ")", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "layer", ".", "bias", ")", "\n", "# carefully initialize last layer", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.__init__": [[217, 361], ["torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "gin.query_parameter", "PPO.PPOActorCritic", "PPO.PPO.model.to", "numpy.random.RandomState", "len", "PPO.PPO.action_space.get_n_actions", "torch.Adam", "torch.Adam", "torch.Adam", "torch.Adam", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "gin.query_parameter", "PPO.PPO.model.parameters", "torch.RMSprop", "torch.RMSprop", "torch.RMSprop", "torch.RMSprop", "torch.optim.lr_scheduler.ExponentialLR", "torch.optim.lr_scheduler.ExponentialLR", "torch.optim.lr_scheduler.ExponentialLR", "torch.optim.lr_scheduler.ExponentialLR", "PPO.PPO.model.parameters"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.spaces.ResActionSpace.get_n_actions"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "seed", ":", "int", ",", "\n", "gamma", ":", "float", ",", "\n", "tau", ":", "float", ",", "\n", "clip_param", ":", "float", ",", "\n", "vf_c", ":", "float", ",", "\n", "ent_c", ":", "float", ",", "\n", "input_shape", ":", "int", ",", "\n", "hidden_units_value", ":", "list", ",", "\n", "hidden_units_actor", ":", "list", ",", "\n", "batch_size", ":", "int", ",", "\n", "lr", ":", "float", ",", "\n", "activation", ":", "str", ",", "\n", "optimizer_name", ":", "str", ",", "\n", "batch_norm_input", ":", "bool", ",", "\n", "batch_norm_value_out", ":", "bool", ",", "\n", "action_space", ",", "\n", "policy_type", ":", "str", ",", "\n", "init_pol_std", ":", "float", ",", "\n", "min_pol_std", ":", "float", ",", "\n", "beta_1", ":", "float", "=", "0.9", ",", "\n", "beta_2", ":", "float", "=", "0.999", ",", "\n", "eps_opt", ":", "float", "=", "1e-07", ",", "\n", "lr_schedule", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "exp_decay_rate", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "step_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "std_transform", ":", "str", "=", "\"softplus\"", ",", "\n", "init_last_layers", ":", "str", "=", "\"rescaled\"", ",", "\n", "rng", "=", "None", ",", "\n", "store_diagnostics", ":", "bool", "=", "False", ",", "\n", "augadv", ":", "bool", "=", "False", ",", "\n", "eta1", ":", "float", "=", "0.1", ",", "\n", "eta2", ":", "float", "=", "0.5", ",", "\n", "modelname", ":", "str", "=", "\"PPO act_crt\"", ",", "\n", ")", ":", "\n", "\n", "        ", "if", "rng", "is", "not", "None", ":", "\n", "            ", "self", ".", "rng", "=", "rng", "\n", "", "else", ":", "\n", "            ", "self", ".", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", ")", "\n", "\n", "", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "# self.device = torch.device('cpu')", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "clip_param", "=", "clip_param", "\n", "self", ".", "vf_c", "=", "vf_c", "\n", "self", ".", "ent_c", "=", "ent_c", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "eps_opt", "=", "eps_opt", "\n", "self", ".", "action_space", "=", "action_space", "\n", "self", ".", "policy_type", "=", "policy_type", "\n", "self", ".", "augadv", "=", "augadv", "\n", "self", ".", "eta1", "=", "eta1", "\n", "self", ".", "eta2", "=", "eta2", "\n", "\n", "if", "gin", ".", "query_parameter", "(", "'%MULTIASSET'", ")", ":", "\n", "            ", "self", ".", "num_actions", "=", "len", "(", "gin", ".", "query_parameter", "(", "'%HALFLIFE'", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_actions", "=", "self", ".", "action_space", ".", "get_n_actions", "(", "policy_type", "=", "self", ".", "policy_type", ")", "\n", "", "self", ".", "batch_norm_input", "=", "batch_norm_input", "\n", "self", ".", "store_diagnostics", "=", "store_diagnostics", "\n", "\n", "self", ".", "experience", "=", "{", "\n", "\"state\"", ":", "[", "]", ",", "\n", "\"action\"", ":", "[", "]", ",", "\n", "\"reward\"", ":", "[", "]", ",", "\n", "\"log_prob\"", ":", "[", "]", ",", "\n", "\"value\"", ":", "[", "]", ",", "\n", "\"returns\"", ":", "[", "]", ",", "\n", "\"advantage\"", ":", "[", "]", ",", "\n", "\"mw_action\"", ":", "[", "]", ",", "\n", "\"rl_action\"", ":", "[", "]", ",", "\n", "}", "\n", "\n", "self", ".", "model", "=", "PPOActorCritic", "(", "\n", "seed", ",", "\n", "input_shape", ",", "\n", "activation", ",", "\n", "hidden_units_value", ",", "\n", "hidden_units_actor", ",", "\n", "self", ".", "num_actions", ",", "\n", "batch_norm_input", ",", "\n", "batch_norm_value_out", ",", "\n", "self", ".", "policy_type", ",", "\n", "init_pol_std", ",", "\n", "min_pol_std", ",", "\n", "std_transform", ",", "\n", "init_last_layers", ",", "\n", "modelname", ",", "\n", ")", "\n", "\n", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "optimizer_name", "=", "optimizer_name", "\n", "if", "optimizer_name", "==", "\"adam\"", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "\n", "self", ".", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "betas", "=", "(", "beta_1", ",", "beta_2", ")", ",", "\n", "eps", "=", "eps_opt", ",", "\n", "weight_decay", "=", "0", ",", "\n", "amsgrad", "=", "False", ",", "\n", ")", "\n", "", "elif", "optimizer_name", "==", "\"rmsprop\"", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "RMSprop", "(", "\n", "self", ".", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "alpha", "=", "beta_1", ",", "\n", "eps", "=", "eps_opt", ",", "\n", "weight_decay", "=", "0", ",", "\n", "momentum", "=", "0", ",", "\n", "centered", "=", "False", ",", "\n", ")", "\n", "\n", "", "if", "lr_schedule", "==", "\"step\"", ":", "\n", "            ", "self", ".", "scheduler", "=", "StepLR", "(", "\n", "optimizer", "=", "self", ".", "optimizer", ",", "step_size", "=", "step_size", ",", "gamma", "=", "exp_decay_rate", "\n", ")", "\n", "\n", "", "elif", "lr_schedule", "==", "\"exponential\"", ":", "\n", "            ", "self", ".", "scheduler", "=", "ExponentialLR", "(", "\n", "optimizer", "=", "self", ".", "optimizer", ",", "gamma", "=", "exp_decay_rate", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "scheduler", "=", "None", "\n", "\n", "", "if", "self", ".", "policy_type", "==", "'continuous'", ":", "\n", "            ", "self", ".", "std_hist", "=", "[", "]", "\n", "self", ".", "entropy_hist", "=", "[", "]", "\n", "", "elif", "self", ".", "policy_type", "==", "'discrete'", ":", "\n", "            ", "self", ".", "logits_hist", "=", "[", "]", "\n", "self", ".", "entropy_hist", "=", "[", "]", "\n", "", "self", ".", "total_loss", "=", "[", "]", "\n", "self", ".", "policy_objective", "=", "[", "]", "\n", "self", ".", "value_objective", "=", "[", "]", "\n", "self", ".", "entropy_objective", "=", "[", "]", "\n", "self", ".", "total_loss_byepoch", "=", "[", "]", "\n", "self", ".", "policy_objective_byepoch", "=", "[", "]", "\n", "self", ".", "value_objective_byepoch", "=", "[", "]", "\n", "self", ".", "entropy_objective_byepoch", "=", "[", "]", "\n", "self", ".", "std_hist_byepoch", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.train": [[363, 447], ["PPO.PPO.model.train", "PPO.PPO.model", "dist.entropy().mean", "PPO.PPO.optimizer.zero_grad", "PPO.PPO.loss.backward", "PPO.PPO.optimizer.step", "dist.log_prob", "numpy.log().nan_to_num", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "PPO.PPO.total_loss.append", "PPO.PPO.policy_objective.append", "PPO.PPO.value_objective.append", "PPO.PPO.entropy_objective.append", "PPO.PPO.std_hist.append", "PPO.PPO.scheduler.step", "advantage.mean", "advantage.std", "dist.entropy", "dist.log_prob().reshape", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "PPO.PPO.loss.detach().cpu", "actor_loss.detach().cpu", "PPO.PPO.model.log_std.exp().detach().cpu().numpy().ravel", "PPO.PPO.total_loss_byepoch.append", "PPO.PPO.policy_objective_byepoch.append", "PPO.PPO.value_objective_byepoch.append", "PPO.PPO.entropy_objective_byepoch.append", "PPO.PPO.std_hist_byepoch.append", "numpy.log", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "critic_loss.detach().cpu", "dist.entropy().mean.detach().cpu", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "PPO.PPO.writer.add_scalar", "PPO.PPO.writer.add_scalar", "PPO.PPO.writer.add_scalar", "PPO.PPO.writer.add_scalar", "PPO.PPO.writer.add_scalar", "PPO.PPO.writer.flush", "dist.log_prob", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "PPO.PPO.loss.detach", "actor_loss.detach", "PPO.PPO.model.log_std.exp().detach().cpu().numpy", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "action.reshape", "critic_loss.detach", "dist.entropy().mean.detach", "PPO.PPO.model.log_std.exp().detach().cpu", "PPO.PPO.model.log_std.exp().detach", "PPO.PPO.model.log_std.exp"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.train", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step"], ["", "def", "train", "(", "self", ",", "state", ",", "action", ",", "old_log_probs", ",", "return_", ",", "advantage", ",", "mw_action", ",", "rl_action", ",", "iteration", ",", "epoch", ",", "episode", ")", ":", "\n", "        ", "advantage", "=", "(", "advantage", "-", "advantage", ".", "mean", "(", ")", ")", "/", "(", "advantage", ".", "std", "(", ")", "+", "1e-5", ")", "\n", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "dist", ",", "value", "=", "self", ".", "model", "(", "state", ")", "\n", "entropy", "=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "if", "self", ".", "policy_type", "==", "'continuous'", ":", "\n", "            ", "new_log_probs", "=", "dist", ".", "log_prob", "(", "action", ")", "\n", "# self.std_hist.append(self.model.log_std.exp().detach().cpu().numpy().ravel())", "\n", "# self.entropy_hist.append(entropy.detach().cpu().numpy().ravel())", "\n", "", "elif", "self", ".", "policy_type", "==", "'discrete'", ":", "\n", "            ", "new_log_probs", "=", "dist", ".", "log_prob", "(", "action", ".", "reshape", "(", "-", "1", ")", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "# self.logits_hist.append(dist.logits.detach().cpu().numpy())", "\n", "# self.entropy_hist.append(entropy.detach().cpu().numpy().ravel())", "\n", "\n", "", "if", "self", ".", "augadv", ":", "\n", "# reg = np.log(new_log_probs - old_log_probs ", "\n", "            ", "reg1", "=", "np", ".", "log", "(", "mw_action", "/", "rl_action", ")", ".", "nan_to_num", "(", "0.0", ")", "\n", "\n", "mask", "=", "torch", ".", "sign", "(", "mw_action", ")", "*", "torch", ".", "sign", "(", "rl_action", ")", "\n", "mask", "[", "mask", "==", "1.0", "]", "=", "0.0", "\n", "mask", "[", "mask", "==", "-", "1.0", "]", "=", "1.0", "\n", "reg2", "=", "mask", "*", "torch", ".", "log", "(", "torch", ".", "abs", "(", "mw_action", ")", "+", "torch", ".", "abs", "(", "rl_action", ")", ")", "\n", "\n", "advantage", "=", "advantage", "-", "self", ".", "eta1", "*", "reg1", "-", "self", ".", "eta2", "*", "reg2", "\n", "\n", "\n", "\n", "", "ratio", "=", "(", "new_log_probs", "-", "old_log_probs", ")", ".", "exp", "(", ")", "# log properties", "\n", "surr1", "=", "ratio", "*", "advantage", "\n", "surr2", "=", "(", "\n", "torch", ".", "clamp", "(", "ratio", ",", "1.0", "/", "(", "1", "+", "self", ".", "clip_param", ")", ",", "1.0", "+", "self", ".", "clip_param", ")", "\n", "*", "advantage", "\n", ")", "\n", "\n", "actor_loss", "=", "-", "torch", ".", "min", "(", "surr1", ",", "surr2", ")", ".", "mean", "(", ")", "\n", "critic_loss", "=", "(", "return_", "-", "value", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "# the loss is negated in order to be maximized", "\n", "self", ".", "loss", "=", "self", ".", "vf_c", "*", "critic_loss", "+", "actor_loss", "-", "self", ".", "ent_c", "*", "entropy", "\n", "\n", "if", "self", ".", "store_diagnostics", ":", "\n", "            ", "self", ".", "total_loss", ".", "append", "(", "self", ".", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "self", ".", "policy_objective", ".", "append", "(", "actor_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "self", ".", "value_objective", ".", "append", "(", "self", ".", "vf_c", "*", "critic_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "self", ".", "entropy_objective", ".", "append", "(", "-", "self", ".", "ent_c", "*", "entropy", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "self", ".", "std_hist", ".", "append", "(", "self", ".", "model", ".", "log_std", ".", "exp", "(", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", ")", "\n", "if", "iteration", "==", "(", "self", ".", "n_batches", "-", "1", ")", ":", "\n", "                ", "self", ".", "total_loss_byepoch", ".", "append", "(", "np", ".", "mean", "(", "self", ".", "total_loss", ")", ")", "\n", "self", ".", "policy_objective_byepoch", ".", "append", "(", "np", ".", "mean", "(", "self", ".", "policy_objective", ")", ")", "\n", "self", ".", "value_objective_byepoch", ".", "append", "(", "np", ".", "mean", "(", "self", ".", "value_objective", ")", ")", "\n", "self", ".", "entropy_objective_byepoch", ".", "append", "(", "np", ".", "mean", "(", "self", ".", "entropy_objective", ")", ")", "\n", "self", ".", "std_hist_byepoch", ".", "append", "(", "self", ".", "std_hist", ")", "\n", "self", ".", "epoch_counter", "+=", "1", "\n", "if", "epoch", "==", "(", "self", ".", "n_epochs", "-", "1", ")", ":", "\n", "                    ", "self", ".", "writer", ".", "add_scalar", "(", "\"Train/total_loss\"", ",", "np", ".", "mean", "(", "self", ".", "total_loss_byepoch", ")", ",", "\n", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Train/policy_objective\"", ",", "np", ".", "mean", "(", "self", ".", "policy_objective_byepoch", ")", ",", "\n", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Train/value_objective\"", ",", "np", ".", "mean", "(", "self", ".", "value_objective_byepoch", ")", ",", "\n", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Train/entropy_objective\"", ",", "np", ".", "mean", "(", "self", ".", "entropy_objective_byepoch", ")", ",", "\n", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Train/policy_std\"", ",", "np", ".", "mean", "(", "self", ".", "std_hist_byepoch", ")", ",", "\n", "episode", ")", "\n", "self", ".", "writer", ".", "flush", "(", ")", "\n", "\n", "self", ".", "total_loss_byepoch", "=", "[", "]", "\n", "self", ".", "policy_objective_byepoch", "=", "[", "]", "\n", "self", ".", "value_objective_byepoch", "=", "[", "]", "\n", "self", ".", "entropy_objective_byepoch", "=", "[", "]", "\n", "\n", "", "self", ".", "total_loss", "=", "[", "]", "\n", "self", ".", "policy_objective", "=", "[", "]", "\n", "self", ".", "value_objective", "=", "[", "]", "\n", "self", ".", "entropy_objective", "=", "[", "]", "\n", "\n", "\n", "", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "self", ".", "scheduler", ":", "\n", "            ", "self", ".", "scheduler", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.act": [[448, 456], ["PPO.PPO.model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "torch.from_numpy().float().unsqueeze", "states.to.to.to", "PPO.PPO.model", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "", "def", "act", "(", "self", ",", "states", ")", ":", "\n", "# useful when the states are single dimensional", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "# make 1D tensor to 2D", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "states", "=", "torch", ".", "from_numpy", "(", "states", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "states", "=", "states", ".", "to", "(", "self", ".", "device", ")", "\n", "return", "self", ".", "model", "(", "states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.compute_gae": [[457, 489], ["reversed", "PPO.PPO.model.eval", "range", "returns.insert", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "PPO.PPO.model", "numpy.array", "len", "range", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "values.detach().cpu().tolist", "len", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "values.detach().cpu", "values.detach"], "methods", ["None"], ["", "", "def", "compute_gae", "(", "self", ",", "next_value", ",", "recompute_value", "=", "False", ")", ":", "\n", "\n", "        ", "if", "recompute_value", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "_", ",", "values", "=", "self", ".", "model", "(", "\n", "torch", ".", "Tensor", "(", "self", ".", "experience", "[", "\"state\"", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", ")", "\n", "", "self", ".", "experience", "[", "\"value\"", "]", "=", "[", "\n", "np", ".", "array", "(", "v", ",", "dtype", "=", "float", ")", "for", "v", "in", "values", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "]", "\n", "# for i in range(len(self.experience[\"value\"])):", "\n", "#     _, value = self.act(self.experience[\"state\"][i])", "\n", "#     self.experience[\"value\"][i] = value.detach().cpu().numpy().ravel()", "\n", "\n", "", "rewards", "=", "self", ".", "experience", "[", "\"reward\"", "]", "\n", "values", "=", "self", ".", "experience", "[", "\"value\"", "]", "\n", "\n", "\n", "values", "=", "values", "+", "[", "next_value", "]", "\n", "gae", "=", "0", "\n", "returns", "=", "[", "]", "\n", "for", "step", "in", "reversed", "(", "range", "(", "len", "(", "rewards", ")", ")", ")", ":", "\n", "            ", "delta", "=", "rewards", "[", "step", "]", "+", "self", ".", "gamma", "*", "values", "[", "step", "+", "1", "]", "-", "values", "[", "step", "]", "\n", "gae", "=", "delta", "+", "self", ".", "gamma", "*", "self", ".", "tau", "*", "gae", "\n", "returns", ".", "insert", "(", "0", ",", "gae", "+", "values", "[", "step", "]", ")", "\n", "\n", "# add estimated returns and advantages to the experience", "\n", "", "self", ".", "experience", "[", "\"returns\"", "]", "=", "returns", "\n", "\n", "advantage", "=", "[", "returns", "[", "i", "]", "-", "values", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "returns", ")", ")", "]", "\n", "self", ".", "experience", "[", "\"advantage\"", "]", "=", "advantage", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.add_experience": [[491, 494], ["exp.items", "PPO.PPO.experience[].append"], "methods", ["None"], ["", "def", "add_experience", "(", "self", ",", "exp", ")", ":", "\n", "        ", "for", "key", ",", "value", "in", "exp", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "experience", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.reset_experience": [[495, 507], ["None"], "methods", ["None"], ["", "", "def", "reset_experience", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "experience", "=", "{", "\n", "\"state\"", ":", "[", "]", ",", "\n", "\"action\"", ":", "[", "]", ",", "\n", "\"reward\"", ":", "[", "]", ",", "\n", "\"log_prob\"", ":", "[", "]", ",", "\n", "\"value\"", ":", "[", "]", ",", "\n", "\"returns\"", ":", "[", "]", ",", "\n", "\"advantage\"", ":", "[", "]", ",", "\n", "\"mw_action\"", ":", "[", "]", ",", "\n", "\"rl_action\"", ":", "[", "]", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.ppo_iter": [[509, 533], ["numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "PPO.PPO.rng.permutation", "numpy.array_split", "len", "range", "len", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "ppo_iter", "(", "self", ")", ":", "\n", "# pick a batch from the rollout", "\n", "        ", "states", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"state\"", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"action\"", "]", ")", "\n", "log_probs", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"log_prob\"", "]", ")", "\n", "returns", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"returns\"", "]", ")", "\n", "advantage", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"advantage\"", "]", ")", "\n", "mw_actions", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"mw_action\"", "]", ")", "\n", "rl_actions", "=", "np", ".", "asarray", "(", "self", ".", "experience", "[", "\"rl_action\"", "]", ")", "\n", "\n", "len_rollout", "=", "states", ".", "shape", "[", "0", "]", "\n", "ids", "=", "self", ".", "rng", ".", "permutation", "(", "len_rollout", ")", "\n", "ids", "=", "np", ".", "array_split", "(", "ids", ",", "len_rollout", "//", "self", ".", "batch_size", ")", "\n", "self", ".", "n_batches", "=", "len", "(", "ids", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "ids", ")", ")", ":", "\n", "\n", "            ", "yield", "(", "\n", "torch", ".", "from_numpy", "(", "states", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "torch", ".", "from_numpy", "(", "actions", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "torch", ".", "from_numpy", "(", "log_probs", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "torch", ".", "from_numpy", "(", "returns", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "torch", ".", "from_numpy", "(", "advantage", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "torch", ".", "from_numpy", "(", "mw_actions", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "torch", ".", "from_numpy", "(", "rl_actions", "[", "ids", "[", "i", "]", ",", ":", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.add_tb_diagnostics": [[535, 540], ["os.path.join", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter"], "methods", ["None"], ["", "", "def", "add_tb_diagnostics", "(", "self", ",", "path", ",", "n_epochs", ")", ":", "\n", "        ", "log_dir", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"tb\"", ")", "\n", "self", ".", "writer", "=", "SummaryWriter", "(", "log_dir", ")", "\n", "self", ".", "epoch_counter", "=", "0", "\n", "self", ".", "n_epochs", "=", "n_epochs", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.getBack": [[541, 553], ["print", "getattr", "print", "print", "print", "print", "PPO.PPO.getBack"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.getBack"], ["", "def", "getBack", "(", "self", ",", "var_grad_fn", ")", ":", "\n", "        ", "print", "(", "var_grad_fn", ")", "\n", "for", "n", "in", "var_grad_fn", ".", "next_functions", ":", "\n", "            ", "if", "n", "[", "0", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "tensor", "=", "getattr", "(", "n", "[", "0", "]", ",", "\"variable\"", ")", "\n", "print", "(", "n", "[", "0", "]", ")", "\n", "print", "(", "\"Tensor with grad found:\"", ",", "tensor", ")", "\n", "print", "(", "\" - gradient:\"", ",", "tensor", ".", "grad", ")", "\n", "print", "(", ")", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "                    ", "self", ".", "getBack", "(", "n", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.save_diagnostics": [[554, 561], ["numpy.save", "numpy.save", "os.path.join", "numpy.array", "os.path.join", "numpy.array", "numpy.save", "numpy.save", "os.path.join", "numpy.array", "os.path.join", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save"], ["", "", "", "", "def", "save_diagnostics", "(", "self", ",", "path", ")", ":", "\n", "        ", "if", "self", ".", "policy_type", "==", "'continuous'", ":", "\n", "            ", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"std_hist\"", ")", ",", "np", ".", "array", "(", "self", ".", "std_hist", ")", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"entropy_hist\"", ")", ",", "np", ".", "array", "(", "self", ".", "entropy_hist", ")", ")", "\n", "", "elif", "self", ".", "policy_type", "==", "'discrete'", ":", "\n", "            ", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"logits_hist\"", ")", ",", "np", ".", "array", "(", "self", ".", "logits_hist", ",", "dtype", "=", "object", ")", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"entropy_hist\"", ")", ",", "np", ".", "array", "(", "self", ".", "entropy_hist", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.__init__": [[37, 88], ["PPO_runner.PPO_runner.logging.info", "PPO_runner.PPO_runner._setattrs", "numpy.random.RandomState", "torch.device", "torch.device", "torch.device", "torch.device", "utils.common.GeneratePathFolder", "logging.info", "int", "str", "os.makedirs", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "os.path.exists", "os.path.join", "os.path.exists", "numpy.arange", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._setattrs", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.GeneratePathFolder"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "env_cls", ":", "object", ",", "\n", "MV_res", ":", "bool", ",", "\n", "experiment_type", ":", "str", ",", "\n", "seed", ":", "int", ",", "\n", "episodes", ":", "int", ",", "\n", "epochs", ":", "int", ",", "\n", "len_series", ":", "Union", "[", "int", ",", "None", "]", ",", "\n", "dt", ":", "int", ",", "\n", "rollouts_pct_num", ":", "float", ",", "\n", "save_freq", ":", "int", ",", "\n", "use_GPU", ":", "bool", ",", "\n", "outputDir", ":", "str", "=", "\"outputs\"", ",", "\n", "outputClass", ":", "str", "=", "\"PPO\"", ",", "\n", "outputModel", ":", "str", "=", "\"test\"", ",", "\n", "varying_pars", ":", "Union", "[", "list", ",", "None", "]", "=", "None", ",", "\n", "varying_type", ":", "str", "=", "\"chunk\"", ",", "\n", "num_cores", ":", "int", "=", "None", ",", "\n", "universal_train", ":", "bool", "=", "False", ",", "\n", "store_insample", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "logging", ".", "info", "(", "\"Starting model setup\"", ")", "\n", "self", ".", "_setattrs", "(", ")", "\n", "\n", "self", ".", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "self", ".", "seed", ")", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "if", "self", ".", "dt", "!=", "1.0", ":", "\n", "            ", "self", ".", "len_series", "=", "int", "(", "self", ".", "len_series", "*", "(", "(", "1", "/", "self", ".", "dt", ")", "*", "self", ".", "rollouts_pct_num", ")", ")", "\n", "\n", "", "self", ".", "N_train", "=", "self", ".", "episodes", "*", "self", ".", "len_series", "\n", "self", ".", "col_names_oos", "=", "[", "\n", "str", "(", "e", ")", "for", "e", "in", "np", ".", "arange", "(", "0", ",", "self", ".", "episodes", "+", "1", ",", "save_freq", ")", "[", "1", ":", "]", "\n", "]", "\n", "\n", "self", ".", "savedpath", "=", "GeneratePathFolder", "(", "\n", "outputDir", ",", "\n", "outputClass", ",", "\n", "outputModel", ",", "\n", "varying_pars", ",", "\n", "varying_type", ",", "\n", "self", ".", "N_train", ",", "\n", ")", "\n", "if", "save_freq", "and", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ")", ")", "\n", "", "elif", "save_freq", "and", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ")", ")", ":", "\n", "            ", "pass", "\n", "", "logging", ".", "info", "(", "\"Successfully generated path to save outputs...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.run": [[89, 97], ["PPO_runner.PPO_runner.set_up_training", "PPO_runner.PPO_runner.training_agent", "PPO_runner.PPO_runner.logging.debug", "sys.exit"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.set_up_training", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.training_agent"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "\"\"\"Wrapper for keyboard interrupt.\"\"\"", "\n", "try", ":", "\n", "            ", "self", ".", "set_up_training", "(", ")", "\n", "self", ".", "training_agent", "(", ")", "\n", "", "except", "(", "KeyboardInterrupt", ",", "SystemExit", ")", ":", "\n", "            ", "self", ".", "logging", ".", "debug", "(", "\"Exit on KeyboardInterrupt or SystemExit\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.set_up_training": [[98, 164], ["PPO_runner.PPO_runner.logging.debug", "gin.query_parameter", "utils.simulation.DataHandler", "PPO_runner.PPO_runner.logging.debug", "PPO_runner.PPO_runner.logging.debug", "PPO_runner.PPO_runner.env_cls", "PPO_runner.PPO_runner.logging.debug", "PPO_runner.PPO_runner.env.get_state_dim", "agents.PPO.PPO", "PPO_runner.PPO_runner.train_agent.add_tb_diagnostics", "PPO_runner.PPO_runner.logging.debug", "utils.test.Out_sample_vs_gp", "PPO_runner.PPO_runner.oos_test.init_series_to_fill", "PPO_runner.PPO_runner._get_hyperparams_n_assets", "time.time", "PPO_runner.PPO_runner.data_handler.generate_returns", "PPO_runner.PPO_runner.data_handler.generate_returns", "PPO_runner.PPO_runner.data_handler.estimate_parameters", "utils.spaces.ResActionSpace", "utils.tools.get_action_boundaries", "utils.spaces.ActionSpace", "time.time", "gin.query_parameter"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.get_state_dim", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.add_tb_diagnostics", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.init_series_to_fill", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner._get_hyperparams_n_assets", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.estimate_parameters", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_action_boundaries"], ["", "", "def", "set_up_training", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "logging", ".", "debug", "(", "\"Simulating Data\"", ")", "\n", "# Modify hyperparams to deal with a large cross section", "\n", "n_assets", "=", "gin", ".", "query_parameter", "(", "'%N_ASSETS'", ")", "\n", "if", "n_assets", "and", "n_assets", ">", "3", ":", "\n", "            ", "self", ".", "_get_hyperparams_n_assets", "(", "n_assets", ",", "self", ".", "rng", ")", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "", "elif", "n_assets", "and", "n_assets", "<=", "3", ":", "\n", "            ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "self", ".", "data_handler", "=", "DataHandler", "(", "N_train", "=", "self", ".", "len_series", ",", "rng", "=", "self", ".", "rng", ")", "\n", "if", "self", ".", "experiment_type", "==", "\"GP\"", ":", "\n", "            ", "self", ".", "data_handler", ".", "generate_returns", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "data_handler", ".", "generate_returns", "(", ")", "\n", "# TODO check if these method really fit and change the parameters in the gin file", "\n", "self", ".", "data_handler", ".", "estimate_parameters", "(", ")", "\n", "\n", "", "self", ".", "logging", ".", "debug", "(", "\"Instantiating action space\"", ")", "\n", "if", "self", ".", "MV_res", ":", "\n", "            ", "self", ".", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_range", ",", "ret_quantile", ",", "holding_quantile", "=", "get_action_boundaries", "(", "\n", "N_train", "=", "self", ".", "N_train", ",", "\n", "f_speed", "=", "self", ".", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "self", ".", "data_handler", ".", "returns", ",", "\n", "factors", "=", "self", ".", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "gin", ".", "query_parameter", "(", "\"%ACTION_RANGE\"", ")", "[", "0", "]", "=", "action_range", "\n", "self", ".", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "\n", "", "self", ".", "logging", ".", "debug", "(", "\"Instantiating market environment\"", ")", "\n", "self", ".", "env", "=", "self", ".", "env_cls", "(", "\n", "N_train", "=", "self", ".", "N_train", ",", "\n", "f_speed", "=", "self", ".", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "self", ".", "data_handler", ".", "returns", ",", "\n", "factors", "=", "self", ".", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Instantiating PPO model\"", ")", "\n", "input_shape", "=", "self", ".", "env", ".", "get_state_dim", "(", ")", "\n", "\n", "# step_size = (", "\n", "#     self.len_series / gin.query_parameter(\"PPO.batch_size\")", "\n", "# ) * gin.query_parameter(\"%EPOCHS\")", "\n", "# gin.bind_parameter(\"PPO.step_size\", step_size)", "\n", "\n", "self", ".", "train_agent", "=", "PPO", "(", "\n", "input_shape", "=", "input_shape", ",", "action_space", "=", "self", ".", "action_space", ",", "rng", "=", "self", ".", "rng", "\n", ")", "\n", "\n", "self", ".", "train_agent", ".", "add_tb_diagnostics", "(", "self", ".", "savedpath", ",", "self", ".", "epochs", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Instantiating Out of sample tester\"", ")", "\n", "self", ".", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "self", ".", "savedpath", ",", "\n", "tag", "=", "\"PPO\"", ",", "\n", "experiment_type", "=", "self", ".", "experiment_type", ",", "\n", "env_cls", "=", "self", ".", "env_cls", ",", "\n", "MV_res", "=", "self", ".", "MV_res", ",", "\n", ")", "\n", "\n", "self", ".", "oos_test", ".", "init_series_to_fill", "(", "iterations", "=", "self", ".", "col_names_oos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.training_agent": [[165, 261], ["PPO_runner.PPO_runner.logging.debug", "tqdm.tqdm.tqdm", "gin.query_parameter", "utils.common.save_gin", "logging.info", "PPO_runner.PPO_runner.logging.debug", "PPO_runner.PPO_runner.collect_rollouts", "PPO_runner.PPO_runner.update", "PPO_runner.PPO_runner.oos_test.save_series", "time.time", "PPO_runner.PPO_runner.oos_test.save_series", "pandas.DataFrame", "pandas.DataFrame.to_parquet", "pandas.DataFrame", "pandas.DataFrame.to_parquet", "pandas.DataFrame", "pandas.DataFrame.to_parquet", "os.path.join", "range", "torch.save", "torch.save", "torch.save", "torch.save", "PPO_runner.PPO_runner.logging.debug", "PPO_runner.PPO_runner.oos_test.run_test", "open", "f.write", "os.path.join", "os.path.join", "os.path.join", "PPO_runner.PPO_runner.data_handler.generate_returns", "PPO_runner.PPO_runner.data_handler.generate_returns", "PPO_runner.PPO_runner.data_handler.estimate_parameters", "utils.tools.get_action_boundaries", "utils.spaces.ActionSpace", "PPO_runner.PPO_runner.train_agent.model.state_dict", "os.path.join", "os.path.join", "gin.query_parameter", "utils.common.format_tousands", "utils.common.format_tousands", "utils.common.format_tousands", "gin.query_parameter", "gin.query_parameter", "gin.query_parameter"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save_gin", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.collect_rollouts", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.update", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.save_series", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.save_series", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.estimate_parameters", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_action_boundaries", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.format_tousands"], ["", "def", "training_agent", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Main routine to train and test the DRL algorithm. The steps are:\n\n        1. Load the dataset, metadata, any model output and any pre-loaded\n        data (cached_data).\n        2. Start the Backtrader engine and initialize the broker object.\n        3. Instantiate the environment.\n        4. Instantiate the model for the agent.\n        5. Train the model according to a chosen technique.\n        6. Test the model out-of-sample.\n        7. Log the performance data, plot, save configuration file and\n            the runner logger output.\n\n        Once this is done, the backtest is over and all of the artifacts\n        are saved in `_exp/experiment_name/_backtests/`.\n        \"\"\"", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Start training...\"", ")", "\n", "\n", "if", "self", ".", "store_insample", ":", "\n", "            ", "self", ".", "ppo_rew", ",", "self", ".", "opt_rew", ",", "self", ".", "mw_rew", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "", "for", "e", "in", "tqdm", "(", "iterable", "=", "range", "(", "self", ".", "episodes", ")", ",", "desc", "=", "\"Running episodes...\"", ")", ":", "\n", "\n", "            ", "if", "e", ">", "0", "and", "self", ".", "universal_train", ":", "\n", "                ", "if", "self", ".", "experiment_type", "==", "\"GP\"", ":", "\n", "                    ", "self", ".", "data_handler", ".", "generate_returns", "(", "disable_tqdm", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "data_handler", ".", "generate_returns", "(", "disable_tqdm", "=", "True", ")", "\n", "# TODO check if these method really fit and change the parameters in the gin file", "\n", "self", ".", "data_handler", ".", "estimate_parameters", "(", ")", "\n", "\n", "", "self", ".", "env", ".", "returns", "=", "self", ".", "data_handler", ".", "returns", "\n", "self", ".", "env", ".", "factors", "=", "self", ".", "data_handler", ".", "factors", "\n", "self", ".", "env", ".", "f_speed", "=", "self", ".", "data_handler", ".", "f_speed", "\n", "if", "self", ".", "data_handler", ".", "datatype", "==", "\"alpha_term_structure\"", "and", "not", "self", ".", "MV_res", ":", "\n", "                    ", "action_range", ",", "_", ",", "_", "=", "get_action_boundaries", "(", "\n", "N_train", "=", "self", ".", "N_train", ",", "\n", "f_speed", "=", "self", ".", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "self", ".", "data_handler", ".", "returns", ",", "\n", "factors", "=", "self", ".", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "gin", ".", "query_parameter", "(", "\"%ACTION_RANGE\"", ")", "[", "0", "]", "=", "action_range", "\n", "self", ".", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "", "self", ".", "logging", ".", "debug", "(", "\"Training...\"", ")", "\n", "\n", "self", ".", "collect_rollouts", "(", ")", "\n", "\n", "self", ".", "update", "(", "e", ")", "\n", "\n", "if", "self", ".", "save_freq", "and", "(", "(", "e", "+", "1", ")", "%", "self", ".", "save_freq", "==", "0", ")", ":", "# TODO or e+1?", "\n", "\n", "                ", "torch", ".", "save", "(", "\n", "self", ".", "train_agent", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "savedpath", ",", "\"ckpt\"", ",", "\"PPO_{}_ep_weights.pth\"", ".", "format", "(", "e", "+", "1", ")", "\n", ")", ",", "\n", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Testing...\"", ")", "\n", "# n_assets = gin.query_parameter('%N_ASSETS')", "\n", "self", ".", "oos_test", ".", "run_test", "(", "it", "=", "e", "+", "1", ",", "test_agent", "=", "self", ".", "train_agent", ")", "\n", "\n", "\n", "", "", "n_assets", "=", "gin", ".", "query_parameter", "(", "'%N_ASSETS'", ")", "\n", "if", "n_assets", "==", "None", ":", "\n", "            ", "self", ".", "oos_test", ".", "save_series", "(", ")", "\n", "", "else", ":", "\n", "            ", "end_time", "=", "time", ".", "time", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"runtime.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'Runtime {} minutes'", ".", "format", "(", "(", "end_time", "-", "self", ".", "start_time", ")", "/", "60", ")", ")", "\n", "", "self", ".", "oos_test", ".", "save_series", "(", ")", "\n", "\n", "", "if", "self", ".", "store_insample", ":", "\n", "            ", "ppo_rew", "=", "pd", ".", "DataFrame", "(", "data", "=", "self", ".", "ppo_rew", ",", "columns", "=", "[", "'0'", "]", ")", "\n", "ppo_rew", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\n", "\"AbsRew_IS_{}_{}.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", ")", ",", "'PPO'", ")", ")", ",", "\n", "compression", "=", "\"gzip\"", ")", "\n", "gp_rew", "=", "pd", ".", "DataFrame", "(", "data", "=", "self", ".", "opt_rew", ",", "columns", "=", "[", "'0'", "]", ")", "\n", "gp_rew", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\n", "\"AbsRew_IS_{}_GP.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", ")", ")", ")", ",", "\n", "compression", "=", "\"gzip\"", ")", "\n", "mw_rew", "=", "pd", ".", "DataFrame", "(", "data", "=", "self", ".", "mw_rew", ",", "columns", "=", "[", "'0'", "]", ")", "\n", "mw_rew", ".", "to_parquet", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\n", "\"AbsRew_IS_{}_MW.parquet.gzip\"", ".", "format", "(", "\n", "format_tousands", "(", "gin", ".", "query_parameter", "(", "'%LEN_SERIES'", ")", ")", ")", ")", ",", "\n", "compression", "=", "\"gzip\"", ")", "\n", "\n", "\n", "\n", "", "save_gin", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"config.gin\"", ")", ")", "\n", "logging", ".", "info", "(", "\"Config file saved\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.collect_rollouts": [[262, 374], ["PPO_runner.PPO_runner.env.reset", "PPO_runner.PPO_runner.train_agent.reset_experience", "range", "PPO_runner.PPO_runner.train_agent.act", "PPO_runner.PPO_runner.train_agent.compute_gae", "PPO_runner.PPO_runner.env.opt_reset", "PPO_runner.PPO_runner.env.opt_trading_rate_disc_loads", "PPO_runner.PPO_runner.env.opt_reset", "PPO_runner.PPO_runner.train_agent.act", "numpy.array", "PPO_runner.PPO_runner.train_agent.add_experience", "PPO_runner.PPO_runner.ppo_rew.append", "PPO_runner.PPO_runner.opt_rew.append", "PPO_runner.PPO_runner.mw_rew.append", "PPO_runner.PPO_runner.next_value.detach().cpu().numpy().ravel", "len", "dist.sample", "dist.log_prob", "numpy.array.cpu().numpy().ravel", "PPO_runner.PPO_runner.env.step", "dist.log_prob.detach().cpu().numpy().ravel", "value.detach().cpu().numpy().ravel", "PPO_runner.PPO_runner.env.opt_step", "gp_temp.append", "PPO_runner.PPO_runner.env.mv_step", "mw_temp.append", "utils.math_tools.unscale_asymmetric_action", "dist.sample", "dist.log_prob", "numpy.array", "numpy.array", "print", "sys.exit", "len", "PPO_runner.PPO_runner.env.MV_res_step", "PPO_runner.PPO_runner.env.MV_res_step", "PPO_runner.PPO_runner.env.MV_res_step", "numpy.cumsum", "numpy.cumsum", "numpy.cumsum", "PPO_runner.PPO_runner.next_value.detach().cpu().numpy", "numpy.array.cpu().numpy", "utils.math_tools.unscale_asymmetric_action", "utils.math_tools.unscale_action", "dist.log_prob.detach().cpu().numpy", "value.detach().cpu().numpy", "PPO_runner.PPO_runner.next_value.detach().cpu", "numpy.array.cpu", "dist.log_prob.detach().cpu", "value.detach().cpu", "PPO_runner.PPO_runner.next_value.detach", "dist.log_prob.detach", "value.detach", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.reset_experience", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.act", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.compute_gae", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_trading_rate_disc_loads", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.act", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.add_experience", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.opt_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.mv_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_asymmetric_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_asymmetric_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.math_tools.unscale_action"], ["", "def", "collect_rollouts", "(", "self", ")", ":", "\n", "\n", "        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "if", "self", ".", "store_insample", ":", "\n", "            ", "gp_temp", "=", "[", "]", "\n", "optstate", "=", "self", ".", "env", ".", "opt_reset", "(", ")", "\n", "optrate", ",", "discfactorloads", "=", "self", ".", "env", ".", "opt_trading_rate_disc_loads", "(", ")", "\n", "\n", "mw_temp", "=", "[", "]", "\n", "mwstate", "=", "self", ".", "env", ".", "opt_reset", "(", ")", "\n", "\n", "", "self", ".", "train_agent", ".", "reset_experience", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "env", ".", "returns", ")", "-", "2", ")", ":", "\n", "            ", "dist", ",", "value", "=", "self", ".", "train_agent", ".", "act", "(", "state", ")", "\n", "\n", "if", "self", ".", "train_agent", ".", "policy_type", "==", "\"continuous\"", ":", "\n", "                ", "action", "=", "dist", ".", "sample", "(", ")", "\n", "\n", "log_prob", "=", "dist", ".", "log_prob", "(", "action", ")", "\n", "clipped_action", "=", "nn", ".", "Tanh", "(", ")", "(", "action", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", "\n", "action", "=", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", "\n", "if", "self", ".", "MV_res", ":", "\n", "                    ", "unscaled_action", "=", "unscale_asymmetric_action", "(", "\n", "self", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "self", ".", "action_space", ".", "action_range", "[", "1", "]", ",", "clipped_action", "\n", ")", "\n", "", "else", ":", "\n", "\n", "                    ", "if", "self", ".", "action_space", ".", "asymmetric", ":", "\n", "                        ", "unscaled_action", "=", "unscale_asymmetric_action", "(", "\n", "self", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "self", ".", "action_space", ".", "action_range", "[", "1", "]", ",", "clipped_action", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "unscaled_action", "=", "unscale_action", "(", "\n", "self", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "clipped_action", "\n", ")", "\n", "\n", "", "", "", "elif", "self", ".", "train_agent", ".", "policy_type", "==", "\"discrete\"", ":", "\n", "                ", "action", "=", "dist", ".", "sample", "(", ")", "\n", "log_prob", "=", "dist", ".", "log_prob", "(", "action", ")", "\n", "\n", "clipped_action", "=", "np", ".", "array", "(", "\n", "[", "self", ".", "action_space", ".", "values", "[", "action", "]", "]", ",", "dtype", "=", "\"float32\"", "\n", ")", "\n", "unscaled_action", "=", "clipped_action", "\n", "action", "=", "np", ".", "array", "(", "[", "action", "]", ",", "dtype", "=", "\"float32\"", ")", "\n", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"Select a policy as continuous or discrete\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "if", "self", ".", "MV_res", ":", "\n", "                ", "if", "len", "(", "unscaled_action", ")", ">", "1", ":", "\n", "                    ", "next_state", ",", "Result", "=", "self", ".", "env", ".", "MV_res_step", "(", "\n", "state", ",", "unscaled_action", ",", "i", ",", "tag", "=", "\"PPO\"", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "next_state", ",", "Result", "=", "self", ".", "env", ".", "MV_res_step", "(", "\n", "state", ",", "unscaled_action", "[", "0", "]", ",", "i", ",", "tag", "=", "\"PPO\"", "\n", ")", "\n", "", "", "else", ":", "\n", "                ", "next_state", ",", "Result", ",", "_", "=", "self", ".", "env", ".", "step", "(", "\n", "state", ",", "unscaled_action", "[", "0", "]", ",", "i", ",", "tag", "=", "\"PPO\"", "\n", ")", "\n", "\n", "", "exp", "=", "{", "\n", "\"state\"", ":", "state", ",", "\n", "\"action\"", ":", "action", ",", "\n", "\"reward\"", ":", "Result", "[", "\"Reward_PPO\"", "]", ",", "\n", "\"log_prob\"", ":", "log_prob", ".", "detach", "(", ")", "\n", ".", "cpu", "(", ")", "\n", ".", "numpy", "(", ")", "\n", ".", "ravel", "(", ")", ",", "# avoid require_grad and go back to numpy array", "\n", "\"value\"", ":", "value", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", ",", "\n", "\n", "}", "\n", "\n", "# pdb.set_trace()", "\n", "exp", "[", "'mw_action'", "]", "=", "np", ".", "array", "(", "[", "self", ".", "env", ".", "MV_res_step", "(", "state", ",", "unscaled_action", ",", "i", ",", "output_action", "=", "True", ")", "]", ",", "dtype", "=", "'float32'", ")", "\n", "exp", "[", "'rl_action'", "]", "=", "unscaled_action", "\n", "\n", "self", ".", "train_agent", ".", "add_experience", "(", "exp", ")", "\n", "\n", "state", "=", "next_state", "\n", "\n", "# # benchmark agent", "\n", "if", "self", ".", "store_insample", ":", "\n", "# pdb.set_trace()", "\n", "                ", "nextoptstate", ",", "optresult", "=", "self", ".", "env", ".", "opt_step", "(", "\n", "optstate", ",", "optrate", ",", "discfactorloads", ",", "i", "\n", ")", "\n", "optstate", "=", "nextoptstate", "\n", "gp_temp", ".", "append", "(", "optresult", "[", "'OptReward'", "]", ")", "\n", "\n", "nextmwstate", ",", "mwresult", "=", "self", ".", "env", ".", "mv_step", "(", "\n", "mwstate", ",", "i", "\n", ")", "\n", "mwstate", "=", "nextmwstate", "\n", "mw_temp", ".", "append", "(", "mwresult", "[", "'MVReward'", "]", ")", "\n", "\n", "\n", "", "", "if", "self", ".", "store_insample", ":", "\n", "            ", "self", ".", "ppo_rew", ".", "append", "(", "np", ".", "cumsum", "(", "self", ".", "train_agent", ".", "experience", "[", "'reward'", "]", ")", "[", "-", "1", "]", ")", "\n", "self", ".", "opt_rew", ".", "append", "(", "np", ".", "cumsum", "(", "gp_temp", ")", "[", "-", "1", "]", ")", "\n", "self", ".", "mw_rew", ".", "append", "(", "np", ".", "cumsum", "(", "mw_temp", ")", "[", "-", "1", "]", ")", "\n", "\n", "\n", "# pdb.set_trace()", "\n", "", "_", ",", "self", ".", "next_value", "=", "self", ".", "train_agent", ".", "act", "(", "next_state", ")", "\n", "# compute the advantage estimate from the given rollout", "\n", "self", ".", "train_agent", ".", "compute_gae", "(", "self", ".", "next_value", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner.update": [[376, 396], ["range", "enumerate", "PPO_runner.PPO_runner.train_agent.ppo_iter", "PPO_runner.PPO_runner.train_agent.train", "PPO_runner.PPO_runner.train_agent.compute_gae", "len", "PPO_runner.PPO_runner.next_value.detach().cpu().numpy().ravel", "range", "PPO_runner.PPO_runner.next_value.detach().cpu().numpy", "PPO_runner.PPO_runner.next_value.detach().cpu", "PPO_runner.PPO_runner.next_value.detach"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.ppo_iter", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.train", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.compute_gae"], ["", "def", "update", "(", "self", ",", "episode", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "epochs", ")", ":", "# run for more than one epochs", "\n", "            ", "for", "j", ",", "(", "\n", "state", ",", "\n", "action", ",", "\n", "old_log_probs", ",", "\n", "return_", ",", "\n", "advantage", ",", "\n", "mw_action", ",", "\n", "rl_action", ",", "\n", ")", "in", "enumerate", "(", "self", ".", "train_agent", ".", "ppo_iter", "(", ")", ")", ":", "\n", "\n", "                ", "self", ".", "train_agent", ".", "train", "(", "state", ",", "action", ",", "old_log_probs", ",", "return_", ",", "advantage", ",", "mw_action", ",", "rl_action", ",", "iteration", "=", "j", ",", "epoch", "=", "i", ",", "episode", "=", "episode", ")", "\n", "\n", "# recompute gae to avoid stale advantages", "\n", "", "if", "i", "==", "len", "(", "range", "(", "self", ".", "epochs", ")", ")", "-", "1", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "self", ".", "train_agent", ".", "compute_gae", "(", "\n", "self", ".", "next_value", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "ravel", "(", ")", ",", "recompute_value", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.PPO_runner.PPO_runner._get_hyperparams_n_assets": [[398, 412], ["numpy.random.RandomState", "gin.bind_parameter", "gin.bind_parameter", "gin.bind_parameter", "gin.bind_parameter", "list", "numpy.round", "numpy.random.RandomState.randint", "range", "numpy.round", "range", "range", "numpy.random.RandomState.uniform", "numpy.random.RandomState.uniform", "int"], "methods", ["None"], ["", "", "", "def", "_get_hyperparams_n_assets", "(", "self", ",", "n_assets", ",", "rng", ")", ":", "\n", "# gin.bind_parameter('%HALFLIFE',[[rng.randint(low=5,high=150)] for _ in range(n_assets)])", "\n", "# gin.bind_parameter('%INITIAL_ALPHA',[[np.round(rng.uniform(low=0.05,high=0.1),5)] for _ in range(n_assets)])", "\n", "# gin.bind_parameter('%F_PARAM',[[1.0] for _ in range(n_assets)])", "\n", "# gin.bind_parameter('%CORRELATION',list(np.round(rng.uniform(low=-0.8,", "\n", "#                                                             high=0.8,", "\n", "#                                                             size=(int((n_assets**2 - n_assets)/2))),5)))", "\n", "        ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "self", ".", "seed", ")", "\n", "gin", ".", "bind_parameter", "(", "'%HALFLIFE'", ",", "[", "[", "rng", ".", "randint", "(", "low", "=", "50", ",", "high", "=", "800", ")", "]", "for", "_", "in", "range", "(", "n_assets", ")", "]", ")", "\n", "gin", ".", "bind_parameter", "(", "'%INITIAL_ALPHA'", ",", "[", "[", "np", ".", "round", "(", "rng", ".", "uniform", "(", "low", "=", "0.003", ",", "high", "=", "0.01", ")", ",", "5", ")", "]", "for", "_", "in", "range", "(", "n_assets", ")", "]", ")", "\n", "gin", ".", "bind_parameter", "(", "'%F_PARAM'", ",", "[", "[", "1.0", "]", "for", "_", "in", "range", "(", "n_assets", ")", "]", ")", "\n", "gin", ".", "bind_parameter", "(", "'%CORRELATION'", ",", "list", "(", "np", ".", "round", "(", "rng", ".", "uniform", "(", "low", "=", "-", "0.8", ",", "\n", "high", "=", "0.8", ",", "\n", "size", "=", "(", "int", "(", "(", "n_assets", "**", "2", "-", "n_assets", ")", "/", "2", ")", ")", ")", ",", "5", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.__init__": [[33, 99], ["DQN_runner.DQN_runner.logging.info", "DQN_runner.DQN_runner._setattrs", "numpy.random.RandomState", "utils.common.GeneratePathFolder", "logging.info", "tensorflow.config.experimental.list_physical_devices", "tensorflow.config.experimental.list_physical_devices", "tensorflow.config.experimental.set_visible_devices", "int", "os.makedirs", "tensorflow.config.experimental.set_memory_growth", "str", "os.path.exists", "os.path.join", "os.path.exists", "str", "os.path.join", "os.path.join", "numpy.arange", "int", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.mixin_core.MixinCore._setattrs", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.GeneratePathFolder"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "env_cls", ":", "object", ",", "\n", "MV_res", ":", "bool", ",", "\n", "experiment_type", ":", "str", ",", "\n", "seed", ":", "int", ",", "\n", "episodes", ":", "int", ",", "\n", "N_train", ":", "int", ",", "\n", "len_series", ":", "Union", "[", "int", "or", "None", "]", ",", "\n", "dt", ":", "int", ",", "\n", "start_train", ":", "int", ",", "\n", "save_freq", ":", "int", ",", "\n", "use_GPU", ":", "bool", ",", "\n", "outputDir", ":", "str", "=", "\"_outputs\"", ",", "\n", "outputClass", ":", "str", "=", "\"DQN\"", ",", "\n", "outputModel", ":", "str", "=", "\"test\"", ",", "\n", "varying_pars", ":", "Union", "[", "list", "or", "None", "]", "=", "None", ",", "\n", "varying_type", ":", "str", "=", "\"chunk\"", ",", "\n", "num_cores", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "logging", ".", "info", "(", "\"Starting model setup\"", ")", "\n", "self", ".", "_setattrs", "(", ")", "\n", "\n", "self", ".", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "self", ".", "seed", ")", "\n", "\n", "if", "self", ".", "use_GPU", ":", "\n", "            ", "gpu_devices", "=", "tf", ".", "config", ".", "experimental", ".", "list_physical_devices", "(", "\"GPU\"", ")", "\n", "for", "device", "in", "gpu_devices", ":", "\n", "                ", "tf", ".", "config", ".", "experimental", ".", "set_memory_growth", "(", "device", ",", "True", ")", "\n", "", "", "else", ":", "\n", "            ", "my_devices", "=", "tf", ".", "config", ".", "experimental", ".", "list_physical_devices", "(", "device_type", "=", "\"CPU\"", ")", "\n", "tf", ".", "config", ".", "experimental", ".", "set_visible_devices", "(", "\n", "devices", "=", "my_devices", ",", "device_type", "=", "\"CPU\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "dt", "!=", "1.0", ":", "\n", "# self.len_series = self.len_series * (1/self.dt)", "\n", "            ", "self", ".", "N_train", "=", "int", "(", "self", ".", "N_train", "*", "(", "1", "/", "self", ".", "dt", ")", ")", "\n", "\n", "", "if", "self", ".", "episodes", ":", "\n", "            ", "self", ".", "N_train", "=", "self", ".", "episodes", "*", "self", ".", "len_series", "\n", "self", ".", "col_names_oos", "=", "[", "\n", "str", "(", "e", ")", "for", "e", "in", "np", ".", "arange", "(", "0", ",", "self", ".", "episodes", "+", "1", ",", "save_freq", ")", "[", "1", ":", "]", "\n", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "len_series", "=", "self", ".", "N_train", "\n", "self", ".", "save_freq_n", "=", "self", ".", "N_train", "//", "save_freq", "\n", "self", ".", "col_names_oos", "=", "[", "\n", "str", "(", "int", "(", "i", ")", ")", "for", "i", "in", "np", ".", "arange", "(", "0", ",", "self", ".", "N_train", "+", "1", ",", "self", ".", "save_freq_n", ")", "\n", "]", "[", "1", ":", "]", "\n", "\n", "", "self", ".", "savedpath", "=", "GeneratePathFolder", "(", "\n", "outputDir", ",", "\n", "outputClass", ",", "\n", "outputModel", ",", "\n", "varying_pars", ",", "\n", "varying_type", ",", "\n", "self", ".", "N_train", ",", "\n", ")", "\n", "if", "save_freq", "and", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ")", ")", "\n", "", "elif", "save_freq", "and", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ")", ")", ":", "\n", "            ", "pass", "\n", "\n", "", "logging", ".", "info", "(", "\"Successfully generated path to save outputs...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.run": [[100, 111], ["DQN_runner.DQN_runner.set_up_training", "DQN_runner.DQN_runner.training_agent", "DQN_runner.DQN_runner.logging.debug", "sys.exit"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.set_up_training", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.training_agent"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "\"\"\"Wrapper for keyboard interrupt.\"\"\"", "\n", "try", ":", "\n", "            ", "self", ".", "set_up_training", "(", ")", "\n", "# if self.episodes:", "\n", "#     self.training_episodic_agent()", "\n", "# else:", "\n", "self", ".", "training_agent", "(", ")", "\n", "", "except", "(", "KeyboardInterrupt", ",", "SystemExit", ")", ":", "\n", "            ", "self", ".", "logging", ".", "debug", "(", "\"Exit on KeyboardInterrupt or SystemExit\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.set_up_training": [[112, 170], ["DQN_runner.DQN_runner.logging.debug", "utils.simulation.DataHandler", "DQN_runner.DQN_runner.logging.debug", "DQN_runner.DQN_runner.logging.debug", "DQN_runner.DQN_runner.env_cls", "DQN_runner.DQN_runner.logging.debug", "DQN_runner.DQN_runner.env.get_state_dim", "agents.DQN.DQN", "DQN_runner.DQN_runner.logging.debug", "DQN_runner.DQN_runner.train_agent._get_exploration_length", "DQN_runner.DQN_runner.logging.debug", "utils.test.Out_sample_vs_gp", "DQN_runner.DQN_runner.oos_test.init_series_to_fill", "DQN_runner.DQN_runner.data_handler.generate_returns", "DQN_runner.DQN_runner.data_handler.generate_returns", "DQN_runner.DQN_runner.data_handler.estimate_parameters", "utils.spaces.ResActionSpace", "utils.tools.get_action_boundaries", "utils.spaces.ActionSpace", "gin.query_parameter"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MarketEnv.get_state_dim", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN._get_exploration_length", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.init_series_to_fill", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.generate_returns", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.simulation.DataHandler.estimate_parameters", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_action_boundaries"], ["", "", "def", "set_up_training", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "logging", ".", "debug", "(", "\"Simulating Data\"", ")", "\n", "\n", "self", ".", "data_handler", "=", "DataHandler", "(", "N_train", "=", "self", ".", "N_train", ",", "rng", "=", "self", ".", "rng", ")", "\n", "if", "self", ".", "experiment_type", "==", "\"GP\"", ":", "\n", "            ", "self", ".", "data_handler", ".", "generate_returns", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "data_handler", ".", "generate_returns", "(", ")", "\n", "# TODO check if these method really fit and change the parameters in the gin file", "\n", "self", ".", "data_handler", ".", "estimate_parameters", "(", ")", "\n", "\n", "", "self", ".", "logging", ".", "debug", "(", "\"Instantiating action space\"", ")", "\n", "if", "self", ".", "MV_res", ":", "\n", "            ", "self", ".", "action_space", "=", "ResActionSpace", "(", ")", "\n", "", "else", ":", "\n", "            ", "action_range", ",", "ret_quantile", ",", "holding_quantile", "=", "get_action_boundaries", "(", "\n", "N_train", "=", "self", ".", "N_train", ",", "\n", "f_speed", "=", "self", ".", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "self", ".", "data_handler", ".", "returns", ",", "\n", "factors", "=", "self", ".", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "gin", ".", "query_parameter", "(", "\"%ACTION_RANGE\"", ")", "[", "0", "]", "=", "action_range", "\n", "self", ".", "action_space", "=", "ActionSpace", "(", ")", "\n", "\n", "", "self", ".", "logging", ".", "debug", "(", "\"Instantiating market environment\"", ")", "\n", "self", ".", "env", "=", "self", ".", "env_cls", "(", "\n", "N_train", "=", "self", ".", "N_train", ",", "\n", "f_speed", "=", "self", ".", "data_handler", ".", "f_speed", ",", "\n", "returns", "=", "self", ".", "data_handler", ".", "returns", ",", "\n", "factors", "=", "self", ".", "data_handler", ".", "factors", ",", "\n", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Instantiating DQN model\"", ")", "\n", "input_shape", "=", "self", ".", "env", ".", "get_state_dim", "(", ")", "\n", "\n", "self", ".", "train_agent", "=", "DQN", "(", "\n", "input_shape", "=", "input_shape", ",", "\n", "action_space", "=", "self", ".", "action_space", ",", "\n", "rng", "=", "self", ".", "rng", ",", "\n", "N_train", "=", "self", ".", "N_train", ",", "\n", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Set up length of training and instantiate test env\"", ")", "\n", "self", ".", "train_agent", ".", "_get_exploration_length", "(", "self", ".", "N_train", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Instantiating Out of sample tester\"", ")", "\n", "self", ".", "oos_test", "=", "Out_sample_vs_gp", "(", "\n", "savedpath", "=", "self", ".", "savedpath", ",", "\n", "tag", "=", "\"DQN\"", ",", "\n", "experiment_type", "=", "self", ".", "experiment_type", ",", "\n", "env_cls", "=", "self", ".", "env_cls", ",", "\n", "MV_res", "=", "self", ".", "MV_res", ",", "\n", "N_test", "=", "2000", "\n", ")", "\n", "\n", "self", ".", "oos_test", ".", "init_series_to_fill", "(", "iterations", "=", "self", ".", "col_names_oos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.runners.DQN_runner.DQN_runner.training_agent": [[171, 262], ["DQN_runner.DQN_runner.logging.debug", "DQN_runner.DQN_runner.env.reset", "tqdm.tqdm.tqdm", "DQN_runner.DQN_runner.oos_test.save_series", "utils.common.save_gin", "logging.info", "DQN_runner.DQN_runner.train_agent.update_epsilon", "DQN_runner.DQN_runner.train_agent.eps_greedy_action", "DQN_runner.DQN_runner.env.store_results", "DQN_runner.DQN_runner.train_agent.add_experience", "DQN_runner.DQN_runner.train_agent.train", "os.path.join", "range", "utils.tools.get_bet_size", "DQN_runner.DQN_runner.env.MV_res_step", "DQN_runner.DQN_runner.env.step", "DQN_runner.DQN_runner.train_agent.copy_weights", "DQN_runner.DQN_runner.train_agent.model.save_weights", "DQN_runner.DQN_runner.logging.debug", "DQN_runner.DQN_runner.oos_test.run_test", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.reset", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.save_series", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.common.save_gin", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.update_epsilon", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.eps_greedy_action", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.store_results", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.add_experience", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.PPO.PPO.train", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.tools.get_bet_size", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.MV_res_step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.env.MultiAssetCashMarketEnv.step", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.agents.DQN.DQN.copy_weights", "home.repos.pwc.inspect_result.Alessiobrini_Deep-Reinforcement-Trading-with-Predictable-Returns.utils.test.Out_sample_vs_gp.run_test"], ["", "def", "training_agent", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Main routine to train and test the DRL algorithm. The steps are:\n\n        1. Load the dataset, metadata, any model output and any pre-loaded\n        data (cached_data).\n        2. Start the Backtrader engine and initialize the broker object.\n        3. Instantiate the environment.\n        4. Instantiate the model for the agent.\n        5. Train the model according to a chosen technique.\n        6. Test the model out-of-sample.\n        7. Log the performance data, plot, save configuration file and\n            the runner logger output.\n\n        Once this is done, the backtest is over and all of the artifacts\n        are saved in `_exp/experiment_name/_backtests/`.\n        \"\"\"", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Training...\"", ")", "\n", "# pdb.set_trace()", "\n", "CurrState", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "# CurrOptState = env.opt_reset()", "\n", "# OptRate, DiscFactorLoads = env.opt_trading_rate_disc_loads()", "\n", "\n", "for", "i", "in", "tqdm", "(", "iterable", "=", "range", "(", "self", ".", "N_train", "+", "1", ")", ",", "desc", "=", "\"Training DQNetwork\"", ")", ":", "\n", "\n", "            ", "self", ".", "train_agent", ".", "update_epsilon", "(", ")", "\n", "epsilon", "=", "self", ".", "train_agent", ".", "epsilon", "\n", "side_only", "=", "self", ".", "action_space", ".", "side_only", "\n", "copy_step", "=", "self", ".", "train_agent", ".", "copy_step", "\n", "\n", "action", ",", "qvalues", "=", "self", ".", "train_agent", ".", "eps_greedy_action", "(", "\n", "CurrState", ",", "epsilon", ",", "side_only", "=", "side_only", "\n", ")", "\n", "if", "not", "side_only", ":", "\n", "                ", "unscaled_action", "=", "action", "\n", "", "else", ":", "\n", "                ", "unscaled_action", "=", "get_bet_size", "(", "\n", "qvalues", ",", "\n", "action", ",", "\n", "action_limit", "=", "self", ".", "action_space", ".", "action_range", "[", "0", "]", ",", "\n", "zero_action", "=", "self", ".", "action_space", ".", "zero_action", ",", "\n", "rng", "=", "self", ".", "rng", ",", "\n", ")", "\n", "", "if", "self", ".", "MV_res", ":", "\n", "                ", "NextState", ",", "Result", ",", "_", "=", "self", ".", "env", ".", "MV_res_step", "(", "\n", "CurrState", ",", "unscaled_action", ",", "i", "\n", ")", "\n", "", "else", ":", "\n", "                ", "NextState", ",", "Result", ",", "_", "=", "self", ".", "env", ".", "step", "(", "CurrState", ",", "unscaled_action", ",", "i", ")", "\n", "\n", "", "self", ".", "env", ".", "store_results", "(", "Result", ",", "i", ")", "\n", "\n", "exp", "=", "{", "\n", "\"s\"", ":", "CurrState", ",", "\n", "\"a\"", ":", "action", ",", "\n", "\"a_unsc\"", ":", "unscaled_action", ",", "\n", "\"r\"", ":", "Result", "[", "\"Reward_DQN\"", "]", ",", "\n", "\"s2\"", ":", "NextState", ",", "\n", "}", "\n", "\n", "self", ".", "train_agent", ".", "add_experience", "(", "exp", ")", "\n", "\n", "self", ".", "train_agent", ".", "train", "(", "i", ",", "side_only", ")", "\n", "\n", "if", "(", "i", "%", "copy_step", "==", "0", ")", "and", "(", "i", ">", "self", ".", "train_agent", ".", "start_train", ")", ":", "\n", "                ", "self", ".", "train_agent", ".", "copy_weights", "(", ")", "\n", "\n", "", "CurrState", "=", "NextState", "\n", "\n", "if", "(", "i", "%", "self", ".", "save_freq_n", "==", "0", ")", "and", "(", "i", ">", "self", ".", "train_agent", ".", "start_train", ")", ":", "\n", "                ", "self", ".", "train_agent", ".", "model", ".", "save_weights", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"ckpt\"", ",", "\"DQN_{}_ep_weights\"", ".", "format", "(", "i", ")", ")", ",", "\n", "save_format", "=", "\"tf\"", ",", "\n", ")", "\n", "\n", "self", ".", "logging", ".", "debug", "(", "\"Testing...\"", ")", "\n", "self", ".", "oos_test", ".", "run_test", "(", "it", "=", "i", ",", "test_agent", "=", "self", ".", "train_agent", ")", "\n", "\n", "# if executeGP:", "\n", "#     NextOptState, OptResult = env.opt_step(", "\n", "#         CurrOptState, OptRate, DiscFactorLoads, i", "\n", "#     )", "\n", "#     env.store_results(OptResult, i)", "\n", "#     CurrOptState = NextOptState", "\n", "\n", "", "", "self", ".", "oos_test", ".", "save_series", "(", ")", "\n", "\n", "save_gin", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedpath", ",", "\"config.gin\"", ")", ")", "\n", "logging", ".", "info", "(", "\"Config file saved\"", ")", "\n", "", "", ""]]}