{"home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertConfig.__init__": [[87, 158], ["isinstance", "json.loads.items", "isinstance", "open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "relax_projection", "=", "0", ",", "\n", "new_pos_ids", "=", "False", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "task_idx", "=", "None", ",", "\n", "fp32_embedding", "=", "False", ",", "\n", "ffn_type", "=", "0", ",", "\n", "label_smoothing", "=", "None", ",", "\n", "num_qkv", "=", "0", ",", "\n", "seg_emb", "=", "False", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "relax_projection", "=", "relax_projection", "\n", "self", ".", "new_pos_ids", "=", "new_pos_ids", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "task_idx", "=", "task_idx", "\n", "self", ".", "fp32_embedding", "=", "fp32_embedding", "\n", "self", ".", "ffn_type", "=", "ffn_type", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "seg_emb", "=", "seg_emb", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertConfig.from_dict": [[160, 167], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertConfig.from_json_file": [[168, 174], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertConfig.__repr__": [[175, 177], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertConfig.to_dict": [[178, 182], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertConfig.to_json_string": [[183, 186], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.PositionalEmbedding.__init__": [[210, 217], ["torch.nn.Module.__init__", "modeling.PositionalEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "demb", ")", ":", "\n", "        ", "super", "(", "PositionalEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "demb", "=", "demb", "\n", "\n", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "torch", ".", "arange", "(", "0.0", ",", "demb", ",", "2.0", ")", "/", "demb", ")", ")", "\n", "self", ".", "register_buffer", "(", "'inv_freq'", ",", "inv_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.PositionalEmbedding.forward": [[218, 226], ["torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pos_emb[].expand", "torch.ger.sin", "torch.ger.sin", "torch.ger.cos", "torch.ger.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pos_seq", ",", "bsz", "=", "None", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "ger", "(", "pos_seq", ",", "self", ".", "inv_freq", ")", "\n", "pos_emb", "=", "torch", ".", "cat", "(", "[", "sinusoid_inp", ".", "sin", "(", ")", ",", "sinusoid_inp", ".", "cos", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "bsz", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEmbeddings.__init__": [[232, 254], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "hasattr", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "hasattr", "(", "config", ",", "'fp32_embedding'", ")", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "", "else", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "False", "\n", "\n", "", "if", "hasattr", "(", "config", ",", "'new_pos_ids'", ")", "and", "config", ".", "new_pos_ids", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "4", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "1", "\n", "", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", "*", "self", ".", "num_pos_emb", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEmbeddings.get_position_token_type_embedding": [[256, 284], ["input_ids.size", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "modeling.BertEmbeddings.word_embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "words_embeddings[].unsqueeze", "latent_z.type_as", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "get_position_token_type_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "if", "relace_embeddings", "==", "True", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "words_embeddings", "=", "torch", ".", "cat", "(", "(", "words_embeddings", "[", ":", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ",", "latent_z", ".", "type_as", "(", "words_embeddings", ")", ",", "\n", "words_embeddings", "[", ":", ",", "2", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "embeddings", "=", "position_embeddings", "+", "token_type_embeddings", "\n", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEmbeddings.get_word_embedding": [[286, 291], ["modeling.BertEmbeddings.word_embeddings"], "methods", ["None"], ["", "def", "get_word_embedding", "(", "self", ",", "input_ids", ")", ":", "\n", "\n", "        ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "return", "words_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEmbeddings.forward": [[330, 360], ["input_ids.size", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "modeling.BertEmbeddings.word_embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.size", "embeddings.half.half.half", "modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "words_embeddings[].unsqueeze", "latent_z.type_as", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "if", "relace_embeddings", "==", "True", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "words_embeddings", "=", "torch", ".", "cat", "(", "(", "words_embeddings", "[", ":", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ",", "latent_z", ".", "type_as", "(", "words_embeddings", ")", ",", "words_embeddings", "[", ":", ",", "2", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertSelfAttention.__init__": [[363, 401], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "ValueError", "hasattr", "os.getenv", "modeling.BertSelfAttention.register_buffer", "hasattr", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Embedding", "torch.nn.Embedding", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "\n", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "if", "hasattr", "(", "config", ",", "'num_qkv'", ")", "and", "(", "config", ".", "num_qkv", ">", "1", ")", ":", "\n", "            ", "self", ".", "num_qkv", "=", "config", ".", "num_qkv", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_qkv", "=", "1", "\n", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "\n", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n", "self", ".", "uni_debug_flag", "=", "True", "if", "os", ".", "getenv", "(", "\n", "'UNI_DEBUG_FLAG'", ",", "''", ")", "else", "False", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'debug_attention_probs'", ",", "\n", "torch", ".", "zeros", "(", "(", "512", ",", "512", ")", ")", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'seg_emb'", ")", "and", "config", ".", "seg_emb", ":", "\n", "            ", "self", ".", "b_q_s", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "1", ",", "self", ".", "num_attention_heads", ",", "1", ",", "self", ".", "attention_head_size", ")", ")", "\n", "self", ".", "seg_emb", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "self", ".", "all_head_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "b_q_s", "=", "None", "\n", "self", ".", "seg_emb", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertSelfAttention.transpose_for_scores": [[402, 426], ["x.gather().squeeze.gather().squeeze.permute", "x.gather().squeeze.gather().squeeze.view", "x.gather().squeeze.gather().squeeze.view", "isinstance", "x.gather().squeeze.gather().squeeze.size", "x.gather().squeeze.gather().squeeze.gather().squeeze", "x.gather().squeeze.gather().squeeze.size", "mask_qkv.size", "x.gather().squeeze.gather().squeeze.gather", "mask_qkv.view().expand", "mask_qkv.view"], "methods", ["None"], ["", "", "def", "transpose_for_scores", "(", "self", ",", "x", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_qkv", ",", "\n", "self", ".", "num_attention_heads", ",", "self", ".", "all_head_size", ")", "\n", "# (batch, pos, num_qkv, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "if", "mask_qkv", "is", "None", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "0", ",", ":", ",", ":", "]", "\n", "", "elif", "isinstance", "(", "mask_qkv", ",", "int", ")", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "mask_qkv", ",", ":", ",", ":", "]", "\n", "", "else", ":", "\n", "# mask_qkv: (batch, pos)", "\n", "                ", "if", "mask_qkv", ".", "size", "(", "1", ")", ">", "sz", "[", "1", "]", ":", "\n", "                    ", "mask_qkv", "=", "mask_qkv", "[", ":", ",", ":", "sz", "[", "1", "]", "]", "\n", "# -> x: (batch, pos, head, head_hid)", "\n", "", "x", "=", "x", ".", "gather", "(", "2", ",", "mask_qkv", ".", "view", "(", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "1", ",", "1", ")", ".", "expand", "(", "\n", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "sz", "[", "3", "]", ",", "sz", "[", "4", "]", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "", "", "else", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "# (batch, head, pos, head_hid)", "\n", "", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertSelfAttention.forward": [[427, 479], ["modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose", "modeling.BertSelfAttention.seg_emb", "seg_rep.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.nn.Softmax", "torch.nn.Softmax", "modeling.BertSelfAttention.size", "modeling.BertSelfAttention.debug_attention_probs[].copy_", "math.sqrt", "seg_rep.view.view.size", "seg_rep.view.view.size", "attention_probs[].mean().view", "context_layer.view.view.permute", "context_layer.view.view.size", "attention_probs[].mean"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "if", "history_states", "is", "None", ":", "\n", "            ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "", "else", ":", "\n", "            ", "x_states", "=", "torch", ".", "cat", "(", "(", "history_states", ",", "hidden_states", ")", ",", "dim", "=", "1", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "x_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "x_states", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ",", "mask_qkv", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ",", "mask_qkv", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ",", "mask_qkv", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "# (batch, head, pos, pos)", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "\n", "query_layer", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "if", "self", ".", "seg_emb", "is", "not", "None", ":", "\n", "            ", "seg_rep", "=", "self", ".", "seg_emb", "(", "seg_ids", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "seg_rep", "=", "seg_rep", ".", "view", "(", "seg_rep", ".", "size", "(", "0", ")", ",", "seg_rep", ".", "size", "(", "\n", "1", ")", ",", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "qs", "=", "torch", ".", "einsum", "(", "'bnih,bjnh->bnij'", ",", "\n", "query_layer", "+", "self", ".", "b_q_s", ",", "seg_rep", ")", "\n", "attention_scores", "=", "attention_scores", "+", "qs", "\n", "\n", "# attention_scores = attention_scores / math.sqrt(self.attention_head_size)", "\n", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "_pos", "=", "attention_probs", ".", "size", "(", "-", "1", ")", "\n", "self", ".", "debug_attention_probs", "[", ":", "_pos", ",", ":", "_pos", "]", ".", "copy_", "(", "\n", "attention_probs", "[", "0", "]", ".", "mean", "(", "0", ")", ".", "view", "(", "_pos", ",", "_pos", ")", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", "\n", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertSelfOutput.__init__": [[482, 487], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertSelfOutput.forward": [[488, 493], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertAttention.__init__": [[496, 500], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertAttention.forward": [[501, 506], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "\n", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertIntermediate.__init__": [[509, 514], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertIntermediate.forward": [[515, 519], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertOutput.__init__": [[522, 527], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertOutput.forward": [[528, 533], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerFFN.__init__": [[536, 548], ["torch.nn.Module.__init__", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TransformerFFN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "assert", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", "\n", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "wx0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "2", ",", ")", ":", "\n", "            ", "self", ".", "wx1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "output", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerFFN.forward": [[549, 560], ["modeling.TransformerFFN.dropout", "modeling.TransformerFFN.LayerNorm", "modeling.TransformerFFN.wx0", "modeling.TransformerFFN.output", "modeling.TransformerFFN.wx1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "x0", "=", "self", ".", "wx0", "(", "x", ")", "\n", "if", "self", ".", "ffn_type", "==", "1", ":", "\n", "                ", "x1", "=", "x", "\n", "", "elif", "self", ".", "ffn_type", "==", "2", ":", "\n", "                ", "x1", "=", "self", ".", "wx1", "(", "x", ")", "\n", "", "out", "=", "self", ".", "output", "(", "x0", "*", "x1", ")", "\n", "", "out", "=", "self", ".", "dropout", "(", "out", ")", "\n", "out", "=", "self", ".", "LayerNorm", "(", "out", "+", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertLayer.__init__": [[563, 572], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.TransformerFFN", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "self", ".", "ffn", "=", "TransformerFFN", "(", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertLayer.forward": [[573, 582], ["modeling.BertLayer.attention", "modeling.BertLayer.ffn", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "layer_output", "=", "self", ".", "ffn", "(", "attention_output", ")", "\n", "", "else", ":", "\n", "            ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEncoder.__init__": [[585, 590], ["torch.nn.Module.__init__", "modeling.BertLayer", "torch.nn.ModuleList", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEncoder.forward": [[591, 614], ["enumerate", "all_encoder_layers.append", "layer_module", "layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "# history embedding and encoded layer must be simultanously given", "\n", "        ", "assert", "(", "prev_embedding", "is", "None", ")", "==", "(", "prev_encoded_layers", "is", "None", ")", "\n", "\n", "all_encoder_layers", "=", "[", "]", "\n", "if", "(", "prev_embedding", "is", "not", "None", ")", "and", "(", "prev_encoded_layers", "is", "not", "None", ")", ":", "\n", "            ", "history_states", "=", "prev_embedding", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "if", "prev_encoded_layers", "is", "not", "None", ":", "\n", "                    ", "history_states", "=", "prev_encoded_layers", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPooler.__init__": [[617, 621], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPooler.forward": [[622, 629], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPredictionHeadTransform.__init__": [[632, 641], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "isinstance", "hasattr"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "hid_size", "=", "config", ".", "hidden_size", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "hid_size", "*=", "config", ".", "relax_projection", "\n", "", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "hid_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "hid_size", ",", "eps", "=", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPredictionHeadTransform.forward": [[642, 647], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertLMPredictionHead.__init__": [[650, 675], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "hasattr", "bert_model_embedding_weights.size", "tensor.half"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "self", ".", "relax_projection", "=", "config", ".", "relax_projection", "\n", "", "else", ":", "\n", "            ", "self", ".", "relax_projection", "=", "0", "\n", "", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "\n", "def", "convert_to_type", "(", "tensor", ")", ":", "\n", "            ", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "return", "tensor", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                ", "return", "tensor", "\n", "", "", "self", ".", "type_converter", "=", "convert_to_type", "\n", "self", ".", "converted", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertLMPredictionHead.forward": [[676, 694], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.type_converter", "torch.linear.size", "torch.linear.size", "torch.linear", "torch.linear", "modeling.BertLMPredictionHead.transform.half", "torch.linear.view", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.decoder", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "if", "not", "self", ".", "converted", ":", "\n", "            ", "self", ".", "converted", "=", "True", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "self", ".", "transform", ".", "half", "(", ")", "\n", "", "", "hidden_states", "=", "self", ".", "transform", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ")", "\n", "if", "self", ".", "relax_projection", ">", "1", ":", "\n", "            ", "num_batch", "=", "hidden_states", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "hidden_states", ".", "size", "(", "1", ")", "\n", "# (batch, num_pos, relax_projection*hid) -> (batch, num_pos, relax_projection, hid) -> (batch, num_pos, hid)", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "relax_projection", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "hidden_states", "=", "F", ".", "linear", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ",", "self", ".", "type_converter", "(", "\n", "self", ".", "decoder", ".", "weight", ")", ",", "self", ".", "type_converter", "(", "self", ".", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertOnlyMLMHead.__init__": [[697, 701], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertOnlyMLMHead.forward": [[702, 705], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertOnlyNSPHead.__init__": [[708, 711], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertOnlyNSPHead.forward": [[712, 715], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPreTrainingHeads.__init__": [[718, 723], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPreTrainingHeads.forward": [[724, 731], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ",", "task_idx", ")", "\n", "if", "pooled_output", "is", "None", ":", "\n", "            ", "seq_relationship_score", "=", "None", "\n", "", "else", ":", "\n", "            ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.PreTrainedBertModel.__init__": [[738, 748], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedBertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.PreTrainedBertModel.init_bert_weights": [[749, 759], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.PreTrainedBertModel.from_pretrained": [[760, 1070], ["os.path.isdir", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "modeling.PreTrainedBertModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-base-multilingual`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "\n", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "if", "(", "'config_path'", "in", "kwargs", ")", "and", "kwargs", "[", "'config_path'", "]", ":", "\n", "            ", "config_file", "=", "kwargs", "[", "'config_path'", "]", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "\n", "# define new type_vocab_size (there might be different numbers of segment ids)", "\n", "if", "'type_vocab_size'", "in", "kwargs", ":", "\n", "            ", "config", ".", "type_vocab_size", "=", "kwargs", "[", "'type_vocab_size'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'relax_projection'", "in", "kwargs", ")", "and", "kwargs", "[", "'relax_projection'", "]", ":", "\n", "            ", "config", ".", "relax_projection", "=", "kwargs", "[", "'relax_projection'", "]", "\n", "# new position embedding", "\n", "", "if", "(", "'new_pos_ids'", "in", "kwargs", ")", "and", "kwargs", "[", "'new_pos_ids'", "]", ":", "\n", "            ", "config", ".", "new_pos_ids", "=", "kwargs", "[", "'new_pos_ids'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'task_idx'", "in", "kwargs", ")", "and", "kwargs", "[", "'task_idx'", "]", ":", "\n", "            ", "config", ".", "task_idx", "=", "kwargs", "[", "'task_idx'", "]", "\n", "# define new max position embedding for length expansion", "\n", "", "if", "(", "'max_position_embeddings'", "in", "kwargs", ")", "and", "kwargs", "[", "'max_position_embeddings'", "]", ":", "\n", "            ", "config", ".", "max_position_embeddings", "=", "kwargs", "[", "'max_position_embeddings'", "]", "\n", "# use fp32 for embeddings", "\n", "", "if", "(", "'fp32_embedding'", "in", "kwargs", ")", "and", "kwargs", "[", "'fp32_embedding'", "]", ":", "\n", "            ", "config", ".", "fp32_embedding", "=", "kwargs", "[", "'fp32_embedding'", "]", "\n", "# type of FFN in transformer blocks", "\n", "", "if", "(", "'ffn_type'", "in", "kwargs", ")", "and", "kwargs", "[", "'ffn_type'", "]", ":", "\n", "            ", "config", ".", "ffn_type", "=", "kwargs", "[", "'ffn_type'", "]", "\n", "# label smoothing", "\n", "", "if", "(", "'label_smoothing'", "in", "kwargs", ")", "and", "kwargs", "[", "'label_smoothing'", "]", ":", "\n", "            ", "config", ".", "label_smoothing", "=", "kwargs", "[", "'label_smoothing'", "]", "\n", "# dropout", "\n", "", "if", "(", "'hidden_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'hidden_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "hidden_dropout_prob", "=", "kwargs", "[", "'hidden_dropout_prob'", "]", "\n", "", "if", "(", "'attention_probs_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'attention_probs_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "attention_probs_dropout_prob", "=", "kwargs", "[", "'attention_probs_dropout_prob'", "]", "\n", "# different QKV", "\n", "", "if", "(", "'num_qkv'", "in", "kwargs", ")", "and", "kwargs", "[", "'num_qkv'", "]", ":", "\n", "            ", "config", ".", "num_qkv", "=", "kwargs", "[", "'num_qkv'", "]", "\n", "# segment embedding for self-attention", "\n", "", "if", "(", "'seg_emb'", "in", "kwargs", ")", "and", "kwargs", "[", "'seg_emb'", "]", ":", "\n", "            ", "config", ".", "seg_emb", "=", "kwargs", "[", "'seg_emb'", "]", "\n", "# initialize word embeddings", "\n", "", "_word_emb_map", "=", "None", "\n", "if", "(", "'word_emb_map'", "in", "kwargs", ")", "and", "kwargs", "[", "'word_emb_map'", "]", ":", "\n", "            ", "_word_emb_map", "=", "kwargs", "[", "'word_emb_map'", "]", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "\n", "# clean the arguments in kwargs", "\n", "for", "arg_clean", "in", "(", "'config_path'", ",", "'type_vocab_size'", ",", "'relax_projection'", ",", "'new_pos_ids'", ",", "'task_idx'", ",", "'max_position_embeddings'", ",", "'fp32_embedding'", ",", "'ffn_type'", ",", "'label_smoothing'", ",", "'hidden_dropout_prob'", ",", "'attention_probs_dropout_prob'", ",", "'num_qkv'", ",", "'seg_emb'", ",", "'word_emb_map'", ")", ":", "\n", "            ", "if", "arg_clean", "in", "kwargs", ":", "\n", "                ", "del", "kwargs", "[", "arg_clean", "]", "\n", "\n", "# instantiate model.", "\n", "", "", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ")", "\n", "\n", "#for key in state_dict:", "\n", "#    logger.info(key)", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# initialize new segment embeddings", "\n", "", "_k", "=", "'bert.embeddings.token_type_embeddings.weight'", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "config", ".", "type_vocab_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] ({0} != {1})\"", ".", "format", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "type_vocab_size", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.type_vocab_size, state_dict[_k].shape[1])", "\n", "                ", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "# L2R", "\n", "if", "config", ".", "type_vocab_size", ">=", "3", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "2", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# R2L", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "4", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "3", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# S2S", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "6", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "4", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "5", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "7", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "6", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "", "elif", "config", ".", "type_vocab_size", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "type_vocab_size", ",", ":", "]", "\n", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "n_config_pos_emb", "=", "4", "if", "config", ".", "new_pos_ids", "else", "1", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_pos_emb", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_pos_emb*config.hidden_size != state_dict[bert.embeddings.position_embeddings.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_pos_emb", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_pos_emb", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_pos_emb", "==", "1", ")", "!=", "(", "n_config_pos_emb", "==", "\n", "1", ")", ",", "\"!!!!n_state_pos_emb == 1 xor n_config_pos_emb == 1!!!!\"", "\n", "if", "n_state_pos_emb", "==", "1", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "\n", "1", ",", "n_config_pos_emb", ",", "1", ")", ".", "reshape", "(", "(", "config", ".", "max_position_embeddings", ",", "n_config_pos_emb", "*", "config", ".", "hidden_size", ")", ")", "\n", "", "elif", "n_config_pos_emb", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "config", ".", "max_position_embeddings", ",", "n_state_pos_emb", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "1", ",", "_task_idx", ")", "\n", "\n", "# initialize new position embeddings", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "if", "_k", "in", "state_dict", "and", "config", ".", "max_position_embeddings", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.max_position_embeddings != state_dict[bert.embeddings.position_embeddings.weight] ({0} - {1})\"", ".", "format", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "max_position_embeddings", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "old_size", "=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.max_position_embeddings, state_dict[_k].shape[1])", "\n", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "start", "=", "old_size", "\n", "while", "start", "<", "config", ".", "max_position_embeddings", ":", "\n", "                    ", "chunk_size", "=", "min", "(", "\n", "old_size", ",", "config", ".", "max_position_embeddings", "-", "start", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "start", ":", "start", "+", "chunk_size", ",", "\n", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "chunk_size", ",", ":", "]", ")", "\n", "start", "+=", "chunk_size", "\n", "", "", "elif", "config", ".", "max_position_embeddings", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "max_position_embeddings", ",", ":", "]", "\n", "\n", "# initialize relax projection", "\n", "", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "n_config_relax", "=", "1", "if", "(", "config", ".", "relax_projection", "<", "\n", "1", ")", "else", "config", ".", "relax_projection", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_relax", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_relax*config.hidden_size != state_dict[cls.predictions.transform.dense.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_relax", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_relax", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_relax", "==", "1", ")", "!=", "(", "n_config_relax", "==", "\n", "1", ")", ",", "\"!!!!n_state_relax == 1 xor n_config_relax == 1!!!!\"", "\n", "if", "n_state_relax", "==", "1", ":", "\n", "                ", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_relax", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_relax", "*", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_relax", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "", "elif", "n_config_relax", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "\n", "# initialize QKV", "\n", "", "", "", "_all_head_size", "=", "config", ".", "num_attention_heads", "*", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "n_config_num_qkv", "=", "1", "if", "(", "config", ".", "num_qkv", "<", "1", ")", "else", "config", ".", "num_qkv", "\n", "for", "qkv_name", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", ":", "\n", "            ", "_k", "=", "'bert.encoder.layer.0.attention.self.{0}.weight'", ".", "format", "(", "\n", "qkv_name", ")", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_num_qkv", "*", "_all_head_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"n_config_num_qkv*_all_head_size != state_dict[_k] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_num_qkv", ",", "_all_head_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "for", "layer_idx", "in", "range", "(", "config", ".", "num_hidden_layers", ")", ":", "\n", "                    ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "_all_head_size", "==", "0", "\n", "n_state_qkv", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "_all_head_size", ")", "\n", "assert", "(", "n_state_qkv", "==", "1", ")", "!=", "(", "n_config_num_qkv", "==", "\n", "1", ")", ",", "\"!!!!n_state_qkv == 1 xor n_config_num_qkv == 1!!!!\"", "\n", "if", "n_state_qkv", "==", "1", ":", "\n", "                        ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_num_qkv", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_num_qkv", "*", "_all_head_size", ",", "_all_head_size", ")", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_num_qkv", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "elif", "n_config_num_qkv", "==", "1", ":", "\n", "                        ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                            ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                            ", "_task_idx", "=", "0", "\n", "", "assert", "_task_idx", "!=", "3", ",", "\"[INVALID] _task_idx=3: n_config_num_qkv=1 (should be 2)\"", "\n", "if", "_task_idx", "==", "0", ":", "\n", "                            ", "_qkv_idx", "=", "0", "\n", "", "else", ":", "\n", "                            ", "_qkv_idx", "=", "1", "\n", "", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "\n", "", "", "", "", "if", "_word_emb_map", ":", "\n", "            ", "_k", "=", "'bert.embeddings.word_embeddings.weight'", "\n", "for", "_tgt", ",", "_src", "in", "_word_emb_map", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "[", "_tgt", ",", ":", "]", ".", "copy_", "(", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "_src", ",", ":", "]", ")", "\n", "\n", "", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "\n", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "load", "(", "model", ",", "prefix", "=", "''", "if", "hasattr", "(", "model", ",", "'bert'", ")", "else", "'bert.'", ")", "\n", "model", ".", "missing_keys", "=", "missing_keys", "\n", "'''\n        if len(missing_keys) > 0:\n            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            logger.info('\\n'.join(error_msgs))\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        '''", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.__init__": [[1104, 1110], ["modeling.PreTrainedBertModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.rescale_some_parameters": [[1111, 1116], ["enumerate", "layer.attention.output.dense.weight.data.div_", "layer.output.dense.weight.data.div_", "math.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "rescale_some_parameters", "(", "self", ")", ":", "\n", "        ", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "encoder", ".", "layer", ")", ":", "\n", "            ", "layer", ".", "attention", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "\n", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "layer", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_extended_attention_mask": [[1117, 1144], ["torch.ones_like.unsqueeze.to", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "next", "modeling.BertModel.parameters"], "methods", ["None"], ["", "", "def", "get_extended_attention_mask", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "if", "attention_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "", "elif", "attention_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "return", "extended_attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_embedding": [[1145, 1149], ["modeling.BertModel.embeddings"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_position_token_type_embedding": [[1150, 1154], ["modeling.BertModel.embeddings.get_position_token_type_embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_position_token_type_embedding"], ["", "def", "get_position_token_type_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", ".", "get_position_token_type_embedding", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_word_embedding": [[1155, 1159], ["modeling.BertModel.embeddings.get_word_embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_word_embedding"], ["", "def", "get_word_embedding", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", ".", "get_word_embedding", "(", "\n", "input_ids", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.forward": [[1165, 1199], ["modeling.BertModel.get_extended_attention_mask", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler", "modeling.BertModel.get_extended_attention_mask", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ",", "\n", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ",", "decode", "=", "None", ",", "prev_embedding", "=", "None", ",", "prev_encoded_layers", "=", "None", ",", "position_ids", "=", "None", ")", ":", "\n", "        ", "if", "decode", "==", "None", ":", "\n", "            ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "                ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n", "", "else", ":", "\n", "            ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "\n", "latent_z", "=", "latent_z", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "\n", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "                ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModelIncr.__init__": [[1201, 1203], ["modeling.BertModel.__init__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModelIncr", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModelIncr.get_embedding": [[1204, 1208], ["modeling.BertModelIncr.embeddings"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModelIncr.forward": [[1209, 1226], ["modeling.BertModelIncr.get_extended_attention_mask", "modeling.BertModelIncr.embeddings", "modeling.BertModelIncr.encoder", "modeling.BertModelIncr.pooler"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "\n", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForPreTraining.__init__": [[1273, 1279], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForPreTraining.forward": [[1280, 1296], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "\n", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPreTrainingPairTransform.__init__": [[1299, 1304], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "# self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-5)", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPreTrainingPairTransform.forward": [[1306, 1312], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertPreTrainingPairTransform.dense", "modeling.BertPreTrainingPairTransform.transform_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ")", ":", "\n", "        ", "hidden_states", "=", "torch", ".", "cat", "(", "[", "pair_x", ",", "pair_y", "]", ",", "dim", "=", "-", "1", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "# hidden_states = self.LayerNorm(hidden_states)", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPreTrainingPairRel.__init__": [[1315, 1319], ["torch.nn.Module.__init__", "modeling.BertPreTrainingPairTransform", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_rel", "=", "0", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairRel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "R_xy", "=", "BertPreTrainingPairTransform", "(", "config", ")", "\n", "self", ".", "rel_emb", "=", "nn", ".", "Embedding", "(", "num_rel", ",", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertPreTrainingPairRel.forward": [[1320, 1329], ["modeling.BertPreTrainingPairRel.R_xy", "modeling.BertPreTrainingPairRel.rel_emb", "modeling.BertPreTrainingPairRel.size", "torch.logsigmoid().mul_", "torch.logsigmoid().mul_", "torch.logsigmoid", "torch.logsigmoid", "pair_pos_neg_mask.type_as"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ",", "pair_r", ",", "pair_pos_neg_mask", ")", ":", "\n", "# (batch, num_pair, hidden)", "\n", "        ", "xy", "=", "self", ".", "R_xy", "(", "pair_x", ",", "pair_y", ")", "\n", "r", "=", "self", ".", "rel_emb", "(", "pair_r", ")", "\n", "_batch", ",", "_num_pair", ",", "_hidden", "=", "xy", ".", "size", "(", ")", "\n", "pair_score", "=", "(", "xy", "*", "r", ")", ".", "sum", "(", "-", "1", ")", "\n", "# torch.bmm(xy.view(-1, 1, _hidden),r.view(-1, _hidden, 1)).view(_batch, _num_pair)", "\n", "# .mul_(-1.0): objective to loss", "\n", "return", "F", ".", "logsigmoid", "(", "pair_score", "*", "pair_pos_neg_mask", ".", "type_as", "(", "pair_score", ")", ")", ".", "mul_", "(", "-", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerEncoder.__init__": [[1333, 1338], ["torch.nn.Module.__init__", "modeling._get_clones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_clones"], ["def", "__init__", "(", "self", ",", "encoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerEncoder.forward": [[1339, 1348], ["mod", "modeling.TransformerEncoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "output", "=", "src", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "src_mask", "=", "mask", ",", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerEncoderLayer.__init__": [[1350, 1364], ["torch.nn.Module.__init__", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "modeling._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_activation_fn"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ")", ":", "\n", "        ", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerEncoderLayer.__setstate__": [[1366, 1370], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerEncoderLayer.forward": [[1371, 1380], ["modeling.TransformerEncoderLayer.norm1", "modeling.TransformerEncoderLayer.linear2", "modeling.TransformerEncoderLayer.norm2", "modeling.TransformerEncoderLayer.self_attn", "modeling.TransformerEncoderLayer.dropout1", "modeling.TransformerEncoderLayer.dropout", "modeling.TransformerEncoderLayer.dropout2", "modeling.TransformerEncoderLayer.activation", "modeling.TransformerEncoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "src2", "=", "self", ".", "self_attn", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "src_mask", ",", "\n", "key_padding_mask", "=", "src_key_padding_mask", ")", "[", "0", "]", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm1", "(", "src", ")", "\n", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "src", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm2", "(", "src", ")", "\n", "return", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerDecoderLayer.__init__": [[1403, 1420], ["torch.nn.Module.__init__", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "modeling._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_activation_fn"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "multihead_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm3", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerDecoderLayer.__setstate__": [[1421, 1425], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerDecoderLayer.forward": [[1426, 1454], ["modeling.TransformerDecoderLayer.norm1", "modeling.TransformerDecoderLayer.norm2", "modeling.TransformerDecoderLayer.linear2", "modeling.TransformerDecoderLayer.norm3", "modeling.TransformerDecoderLayer.self_attn", "modeling.TransformerDecoderLayer.dropout1", "modeling.TransformerDecoderLayer.multihead_attn", "modeling.TransformerDecoderLayer.dropout2", "modeling.TransformerDecoderLayer.dropout", "modeling.TransformerDecoderLayer.dropout3", "modeling.TransformerDecoderLayer.activation", "modeling.TransformerDecoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory", ",", "tgt_mask", "=", "None", ",", "memory_mask", "=", "None", ",", "\n", "tgt_key_padding_mask", "=", "None", ",", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "# type: (Tensor, Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]) -> Tensor", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer.\n\n        Args:\n            tgt: the sequence to the decoder layer (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "tgt2", "=", "self", ".", "self_attn", "(", "tgt", ",", "tgt", ",", "tgt", ",", "attn_mask", "=", "tgt_mask", ",", "\n", "key_padding_mask", "=", "tgt_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout1", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "multihead_attn", "(", "tgt", ",", "memory", ",", "memory", ",", "attn_mask", "=", "memory_mask", ",", "\n", "key_padding_mask", "=", "memory_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerDecoder.__init__": [[1472, 1477], ["torch.nn.Module.__init__", "modeling._get_clones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_clones"], ["def", "__init__", "(", "self", ",", "decoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "decoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.TransformerDecoder.forward": [[1478, 1507], ["mod", "modeling.TransformerDecoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory", ",", "tgt_mask", "=", "None", ",", "\n", "memory_mask", "=", "None", ",", "tgt_key_padding_mask", "=", "None", ",", "\n", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "# type: (Tensor, Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]) -> Tensor", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n\n        Args:\n            tgt: the sequence to the decoder (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "tgt", "\n", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "\n", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForPreTrainingLossMask.__init__": [[1511, 1608], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertEmbeddings", "modeling.BertEmbeddings", "modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.Linear", "torch.nn.Linear", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "modeling.TransformerDecoderLayer", "modeling.TransformerDecoder", "torch.nn.Linear", "torch.nn.Linear", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForPreTrainingLossMask.apply", "modeling.BertForPreTrainingLossMask.bert.rescale_some_parameters", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertPreTrainingPairRel", "hasattr", "pytorch_bert.loss.LabelSmoothingLoss"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.rescale_some_parameters"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "num_sentlvl_labels", "=", "0", ",", "no_nsp", "=", "False", ",", "\n", "mask_word_id", "=", "0", ",", "search_beam_size", "=", "1", ",", "length_penalty", "=", "1.0", ",", "eos_id", "=", "0", ",", "sos_id", "=", "0", ",", "\n", "forbid_duplicate_ngrams", "=", "False", ",", "forbid_ignore_set", "=", "None", ",", "not_predict_set", "=", "None", ",", "ngram_size", "=", "1", ",", "min_len", "=", "1", ",", "mode", "=", "\"s2s\"", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForPreTrainingLossMask", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "tmp_bert_emb", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "mul_bert_emb", "=", "BertEmbeddings", "(", "config", ")", "\n", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "num_sentlvl_labels", "=", "num_sentlvl_labels", "\n", "self", ".", "cls2", "=", "None", "\n", "if", "self", ".", "num_sentlvl_labels", ">", "0", ":", "\n", "            ", "self", ".", "secondary_pred_proj", "=", "nn", ".", "Embedding", "(", "\n", "num_sentlvl_labels", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "cls2", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "secondary_pred_proj", ".", "weight", ",", "num_labels", "=", "num_sentlvl_labels", ")", "\n", "", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "if", "no_nsp", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'label_smoothing'", ")", "and", "config", ".", "label_smoothing", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "LabelSmoothingLoss", "(", "\n", "config", ".", "label_smoothing", ",", "config", ".", "vocab_size", ",", "ignore_index", "=", "0", ",", "reduction", "=", "'none'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "None", "\n", "\n", "# CVAE parameter", "\n", "", "self", ".", "latent_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "mu_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "mu_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "KL_weight", "=", "1", "\n", "\n", "self", ".", "prior_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "prior_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "prior_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "self", ".", "posterior_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "posterior_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "posterior_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "# KS parameter", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n", "# Mutual Information parameter", "\n", "self", ".", "mutual_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "6", ")", "\n", "self", ".", "mutual_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "mutual_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "mutual_mlp", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "mse_fct", "=", "MSELoss", "(", ")", "\n", "self", ".", "prob_dense", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "\n", "# TopK to Top1 parameter", "\n", "self", ".", "Top_K", "=", "TopK", "\n", "self", ".", "Topk_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "6", ")", "\n", "self", ".", "Topk_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "Topk_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "Topk_classifier_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "# Transformer Decoder", "\n", "check_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "6", ")", "\n", "self", ".", "check_transformer_encoder", "=", "TransformerEncoder", "(", "check_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "check_decoder_layer", "=", "TransformerDecoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "6", ")", "\n", "self", ".", "check_transformer_decoder", "=", "TransformerDecoder", "(", "check_decoder_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "check_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ")", "\n", "\n", "# Predict Parameter", "\n", "predict_transformer_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "6", ")", "\n", "self", ".", "predict_transformer", "=", "TransformerEncoder", "(", "predict_transformer_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "predict_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "# Decode parameter", "\n", "self", ".", "mask_word_id", "=", "mask_word_id", "\n", "self", ".", "search_beam_size", "=", "search_beam_size", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "forbid_duplicate_ngrams", "=", "forbid_duplicate_ngrams", "\n", "self", ".", "forbid_ignore_set", "=", "forbid_ignore_set", "\n", "self", ".", "ngram_size", "=", "ngram_size", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "self", ".", "not_predict_set", "=", "not_predict_set", "\n", "self", ".", "min_len", "=", "min_len", "\n", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "bert", ".", "rescale_some_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForPreTrainingLossMask.forward": [[1610, 1893], ["modeling.BertForPreTrainingLossMask.tmp_bert_emb.get_full_embedding", "add_embedding.squeeze.squeeze.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum.unsqueeze", "torch.einsum.unsqueeze", "modeling.BertForPreTrainingLossMask.bert", "modeling.BertForPreTrainingLossMask.dropout", "modeling.BertForPreTrainingLossMask.classifier", "modeling.BertForPreTrainingLossMask.tmp_bert_emb.get_full_embedding", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "modeling.BertForPreTrainingLossMask.mse_fct", "modeling.BertForPreTrainingLossMask.bert", "latent_z.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForPreTrainingLossMask.prob_dense", "modeling.BertForPreTrainingLossMask.activation", "modeling.BertForPreTrainingLossMask.forward.gather_seq_out_by_pos"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_full_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_full_embedding"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "\n", "next_sentence_label", "=", "None", ",", "masked_pos", "=", "None", ",", "masked_weights", "=", "None", ",", "task_idx", "=", "None", ",", "pair_x", "=", "None", ",", "\n", "pair_x_mask", "=", "None", ",", "pair_y", "=", "None", ",", "pair_y_mask", "=", "None", ",", "pair_r", "=", "None", ",", "pair_pos_neg_mask", "=", "None", ",", "\n", "pair_loss_mask", "=", "None", ",", "masked_pos_2", "=", "None", ",", "masked_weights_2", "=", "None", ",", "masked_labels_2", "=", "None", ",", "\n", "num_tokens_a", "=", "None", ",", "num_tokens_b", "=", "None", ",", "mask_qkv", "=", "None", ",", "tgt_pos", "=", "None", ",", "labels", "=", "None", ",", "\n", "ks_labels", "=", "None", ",", "train_ks", "=", "None", ",", "check_ids", "=", "None", ",", "position_ids", "=", "None", ",", "decode", "=", "None", ")", ":", "\n", "\n", "        ", "if", "decode", "==", "True", ":", "\n", "            ", "if", "self", ".", "search_beam_size", ">", "1", ":", "\n", "                ", "return", "self", ".", "beam_search", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "", "", "else", ":", "\n", "# **************** E Step ********************************", "\n", "            ", "if", "train_ks", "==", "None", ":", "\n", "                ", "Batch_Size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "input_ids", "=", "input_ids", ".", "reshape", "(", "-", "1", ",", "input_ids", ".", "shape", "[", "2", "]", ")", "\n", "token_type_ids", "=", "token_type_ids", ".", "reshape", "(", "-", "1", ",", "token_type_ids", ".", "shape", "[", "2", "]", ")", "\n", "attention_mask", "=", "attention_mask", ".", "reshape", "(", "-", "1", ",", "attention_mask", ".", "shape", "[", "2", "]", ",", "attention_mask", ".", "shape", "[", "3", "]", ")", "\n", "masked_lm_labels", "=", "masked_lm_labels", ".", "reshape", "(", "-", "1", ",", "masked_lm_labels", ".", "shape", "[", "2", "]", ")", "\n", "masked_pos", "=", "masked_pos", ".", "reshape", "(", "-", "1", ",", "masked_pos", ".", "shape", "[", "2", "]", ")", "\n", "masked_weights", "=", "masked_weights", ".", "reshape", "(", "-", "1", ",", "masked_weights", ".", "shape", "[", "2", "]", ")", "\n", "tgt_pos", "=", "tgt_pos", ".", "reshape", "(", "-", "1", ",", "tgt_pos", ".", "shape", "[", "2", "]", ")", "\n", "labels", "=", "labels", ".", "reshape", "(", "-", "1", ")", "\n", "ks_labels", "=", "ks_labels", ".", "reshape", "(", "-", "1", ")", "\n", "\n", "\n", "\n", "#In vocab.txt 15 representing <#Z_alpha#>", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "weight_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "labels", ",", "add_embedding", ".", "expand", "(", "labels", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "labels", ")", ")", "\n", "latent_z", "=", "weight_embedding", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "_", ",", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "ks_logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "ks_logits", "=", "F", ".", "softmax", "(", "ks_logits", ",", "dim", "=", "-", "1", ")", "\n", "ks_prob", "=", "ks_logits", "[", ":", ",", "1", "]", "\n", "\n", "\n", "latent_z", "=", "latent_z", ".", "expand", "(", "-", "1", ",", "sequence_output", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "latent_z", ")", ",", "dim", "=", "2", ")", "\n", "sequence_output", "=", "self", ".", "prob_dense", "(", "sequence_output", ")", "\n", "sequence_output", "=", "self", ".", "activation", "(", "sequence_output", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "                    ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "def", "batch_loss_mask_and_normalize", "(", "loss", ",", "mask", ")", ":", "\n", "                    ", "mask", "=", "mask", ".", "type_as", "(", "loss", ")", "\n", "loss", "=", "loss", "*", "mask", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ",", "dim", "=", "1", ")", "+", "1e-5", "\n", "return", "torch", ".", "sum", "(", "loss", ",", "dim", "=", "-", "1", ")", "/", "denominator", "\n", "\n", "# masked lm", "\n", "", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores_masked", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "crit_mask_lm_smoothed", ":", "\n", "                    ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm_smoothed", "(", "\n", "F", ".", "log_softmax", "(", "prediction_scores_masked", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ",", "masked_lm_labels", ")", "\n", "", "else", ":", "\n", "                    ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm", "(", "\n", "prediction_scores_masked", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ")", "\n", "", "masked_lm_loss", "=", "batch_loss_mask_and_normalize", "(", "\n", "masked_lm_loss", ".", "float", "(", ")", ",", "masked_weights", ")", "\n", "\n", "ks_prob", "=", "ks_prob", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", ".", "detach", "(", ")", "\n", "\n", "lm_prob", "=", "F", ".", "softmax", "(", "-", "masked_lm_loss", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", ",", "dim", "=", "1", ")", "\n", "posterior", "=", "(", "ks_prob", "*", "lm_prob", "/", "torch", ".", "sum", "(", "ks_prob", "*", "lm_prob", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "lm_prob", ".", "shape", "[", "1", "]", ")", ")", ".", "detach", "(", ")", "\n", "\n", "TopK_embedding_output", "=", "self", ".", "tmp_bert_emb", ".", "get_full_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "TopK_trans_embedding_output", "=", "TopK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "TopK_out", "=", "self", ".", "Topk_transformer_network", "(", "TopK_trans_embedding_output", ")", "\n", "TopK_out", "=", "TopK_out", "[", "0", ",", ":", ",", ":", "]", "\n", "TopK_out", "=", "self", ".", "Topk_classifier_mlp", "(", "TopK_out", ")", ".", "squeeze", "(", ")", ".", "reshape", "(", "Batch_Size", ",", "-", "1", ")", "\n", "TopK_prob", "=", "torch", ".", "softmax", "(", "TopK_out", ",", "-", "1", ")", "\n", "choice_know_id", "=", "torch", ".", "argmax", "(", "TopK_prob", ",", "1", ")", ".", "detach", "(", ")", "\n", "\n", "KL_loss", "=", "torch", ".", "sum", "(", "TopK_prob", "*", "(", "torch", ".", "log", "(", "TopK_prob", ")", "-", "torch", ".", "log", "(", "posterior", ")", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# recover", "\n", "input_ids", "=", "input_ids", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "token_type_ids", "=", "token_type_ids", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "attention_mask", "=", "attention_mask", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "attention_mask", ".", "shape", "[", "2", "]", ",", "-", "1", ")", "\n", "masked_lm_labels", "=", "masked_lm_labels", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "masked_pos", "=", "masked_pos", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "masked_weights", "=", "masked_weights", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "tgt_pos", "=", "tgt_pos", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "labels", "=", "labels", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", "\n", "ks_labels", "=", "ks_labels", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", "\n", "\n", "\n", "# select ids base choice_know_id", "\n", "choice_know_id", "=", "choice_know_id", ".", "unsqueeze", "(", "1", ")", "\n", "input_ids", "=", "torch", ".", "gather", "(", "input_ids", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "input_ids", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "token_type_ids", "=", "torch", ".", "gather", "(", "token_type_ids", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "token_type_ids", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "attention_mask", "=", "torch", ".", "gather", "(", "attention_mask", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "unsqueeze", "(", "3", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "attention_mask", ".", "shape", "[", "2", "]", ",", "attention_mask", ".", "shape", "[", "3", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "masked_lm_labels", "=", "torch", ".", "gather", "(", "masked_lm_labels", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "masked_lm_labels", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "masked_pos", "=", "torch", ".", "gather", "(", "masked_pos", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "masked_pos", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "masked_weights", "=", "torch", ".", "gather", "(", "masked_weights", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "masked_weights", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "tgt_pos", "=", "torch", ".", "gather", "(", "tgt_pos", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "tgt_pos", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "labels", "=", "torch", ".", "gather", "(", "labels", ",", "1", ",", "choice_know_id", ")", ".", "squeeze", "(", "1", ")", "\n", "ks_labels", "=", "torch", ".", "gather", "(", "ks_labels", ",", "1", ",", "choice_know_id", ")", ".", "squeeze", "(", "1", ")", "\n", "check_ids", "=", "torch", ".", "gather", "(", "check_ids", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "check_ids", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# **************** M Step ********************************", "\n", "\n", "\n", "", "add_embedding", "=", "self", ".", "tmp_bert_emb", ".", "get_full_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "weight_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "labels", ",", "add_embedding", ".", "expand", "(", "labels", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "labels", ")", ")", "\n", "\n", "latent_z", "=", "weight_embedding", ".", "unsqueeze", "(", "1", ")", "\n", "_", ",", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "# Knowledge Selection Loss", "\n", "if", "ks_labels", "is", "not", "None", ":", "\n", "                ", "if", "ks_labels", ".", "dtype", "==", "torch", ".", "long", ":", "\n", "                    ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "ks_loss", "=", "loss_fct", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "ks_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "ks_labels", ".", "dtype", "==", "torch", ".", "half", "or", "ks_labels", ".", "dtype", "==", "torch", ".", "float", ":", "\n", "                    ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "ks_loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "ks_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "'unkown ks_labels.dtype'", ")", "\n", "ks_loss", "=", "None", "\n", "\n", "", "if", "train_ks", "is", "True", ":", "\n", "                    ", "return", "ks_loss", ",", "ks_loss", "\n", "", "", "else", ":", "\n", "                ", "return", "logits", "\n", "\n", "\n", "#embedding_output = self.bert.get_full_embedding(input_ids=input_ids, token_type_ids=token_type_ids)", "\n", "", "embedding_output", "=", "self", ".", "tmp_bert_emb", ".", "get_full_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "#embedding_output = torch.randn(embedding_output.shape, device=embedding_output.device) ", "\n", "#QK_embedding_output = embedding_output[:, :210, :]", "\n", "#trans_QK_embedding_output = QK_embedding_output.transpose(0, 1)", "\n", "\n", "predict_out", "=", "self", ".", "predict_transformer", "(", "embedding_output", "[", ":", ",", ":", "210", ",", ":", "]", ".", "transpose", "(", "0", ",", "1", ")", ")", "[", "0", ",", ":", ",", ":", "]", "\n", "#predict_out = self.predict_transformer(self.bert.get_full_embedding(input_ids=input_ids, token_type_ids=token_type_ids)[:, :210, :].transpose(0, 1))[0,:,:]", "\n", "#predict_out = predict_out[0, :, :]", "\n", "#predict_out = self.activation(predict_out)", "\n", "predict_probs", "=", "torch", ".", "sigmoid", "(", "self", ".", "predict_mlp", "(", "self", ".", "activation", "(", "predict_out", ")", ")", ".", "squeeze", "(", ")", ")", "\n", "predict_loss", "=", "self", ".", "mse_fct", "(", "predict_probs", ",", "labels", ")", "\n", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "                ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "def", "gather_id_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "                ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ")", "\n", "\n", "\n", "", "def", "sample_gumbel", "(", "logits", ",", "eps", "=", "1e-20", ")", ":", "\n", "                ", "\"\"\"Sample from Gumbel(0, 1)\"\"\"", "\n", "U", "=", "torch", ".", "rand_like", "(", "logits", ")", "\n", "# U = torch.empty(shape).uniform_(0, 1)", "\n", "return", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "U", "+", "eps", ")", "+", "eps", ")", "\n", "\n", "\n", "", "def", "gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", ":", "\n", "                ", "\"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"", "\n", "y", "=", "logits", "+", "torch", ".", "tensor", "(", "sample_gumbel", "(", "logits", ")", ")", "\n", "return", "F", ".", "softmax", "(", "y", "/", "temperature", ",", "dim", "=", "-", "1", ")", "\n", "\n", "\n", "", "def", "gumbel_softmax", "(", "logits", ",", "temperature", ",", "hard", "=", "False", ")", ":", "\n", "                ", "\"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n                Args:\n                  logits: [batch_size, n_class] unnormalized log-probs\n                  temperature: non-negative scalar\n                  hard: if True, take argmax, but differentiate w.r.t. soft sample y\n                Returns:\n                  [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n                  If hard=True, then the returned sample will be one-hot, otherwise it will\n                  be a probabilitiy distribution that sums to 1 across classes\n                \"\"\"", "\n", "y", "=", "gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", "\n", "if", "hard", ":", "\n", "                    ", "y_hard", "=", "torch", ".", "eq", "(", "y", ",", "torch", ".", "max", "(", "y", ",", "2", ",", "keepdim", "=", "True", ")", "[", "0", "]", ")", ".", "type_as", "(", "y", ")", "\n", "y", "=", "(", "y_hard", "-", "y", ")", ".", "detach", "(", ")", "+", "y", "\n", "\n", "", "return", "y", "\n", "\n", "", "_", ",", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "latent_z", "=", "latent_z", ".", "expand", "(", "-", "1", ",", "sequence_output", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "latent_z", ")", ",", "dim", "=", "2", ")", "\n", "sequence_output", "=", "self", ".", "prob_dense", "(", "sequence_output", ")", "\n", "sequence_output", "=", "self", ".", "activation", "(", "sequence_output", ")", "\n", "\n", "\n", "tgt_sequence_output", "=", "gather_seq_out_by_pos", "(", "sequence_output", ",", "tgt_pos", ")", "\n", "tgt_prob", ",", "_", "=", "self", ".", "cls", "(", "tgt_sequence_output", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "tgt_prob_gumbel", "=", "gumbel_softmax", "(", "tgt_prob", ",", "1", ",", "hard", "=", "False", ")", ".", "type_as", "(", "self", ".", "mul_bert_emb", ".", "word_embeddings", ".", "weight", ")", "\n", "sample_embedding", "=", "torch", ".", "einsum", "(", "\"ijk,kl->ijl\"", ",", "tgt_prob_gumbel", ",", "self", ".", "mul_bert_emb", ".", "word_embeddings", ".", "weight", ")", "\n", "\n", "\n", "# Mutual Information Loss", "\n", "position_token_type_embedding_output", "=", "self", ".", "mul_bert_emb", ".", "get_position_token_type_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "response_position_token_type_embedding", "=", "gather_seq_out_by_pos", "(", "position_token_type_embedding_output", ",", "tgt_pos", ")", "\n", "sample_embedding", "=", "sample_embedding", "+", "response_position_token_type_embedding", "\n", "sample_embedding", "=", "self", ".", "mul_bert_emb", ".", "LayerNorm", "(", "sample_embedding", ")", "\n", "sample_embedding", "=", "self", ".", "mul_bert_emb", ".", "dropout", "(", "sample_embedding", ")", "\n", "\n", "\n", "embedding_output2", "=", "self", ".", "mul_bert_emb", ".", "get_full_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "new_embedding_output", "=", "torch", ".", "cat", "(", "(", "embedding_output2", "[", ":", ",", ":", "215", ",", ":", "]", ",", "sample_embedding", ",", "embedding_output2", "[", ":", ",", "-", "1", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "trans_new_embedding_output", "=", "new_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "PAD_emb", "=", "self", ".", "mul_bert_emb", ".", "get_word_embedding", "(", "torch", ".", "tensor", "(", "[", "[", "0", "]", "*", "input_ids", ".", "shape", "[", "0", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ")", "\n", "\n", "trans_new_embedding_output", "=", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "trans_new_embedding_output", "[", ":", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "mutual_out", "=", "self", ".", "mutual_transformer_network", "(", "trans_new_embedding_output", ")", "\n", "mutual_out", "=", "mutual_out", "[", "0", ",", ":", ",", ":", "]", "\n", "mutual_out", "=", "self", ".", "activation", "(", "mutual_out", ")", "\n", "\n", "check_embedding", "=", "self", ".", "mul_bert_emb", ".", "get_full_embedding", "(", "check_ids", ",", "torch", ".", "zeros_like", "(", "check_ids", ")", ")", "\n", "trans_check_embedding", "=", "check_embedding", ".", "transpose", "(", "0", ",", "1", ")", "\n", "trans_check_embedding", "=", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "trans_check_embedding", "[", ":", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "\n", "mutual_check_out", "=", "self", ".", "mutual_transformer_network", "(", "trans_check_embedding", ")", "\n", "mutual_check_out", "=", "mutual_check_out", "[", "0", ",", ":", ",", ":", "]", "\n", "mutual_check_out", "=", "self", ".", "activation", "(", "mutual_check_out", ")", "\n", "\n", "mutual_out", "=", "torch", ".", "cat", "(", "(", "mutual_out", ",", "mutual_check_out", ")", ",", "dim", "=", "1", ")", "\n", "mutual_out", "=", "self", ".", "mutual_mlp", "(", "mutual_out", ")", "\n", "Mutual_loss", "=", "self", ".", "mse_fct", "(", "mutual_out", ",", "labels", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "golden_check_out", "=", "self", ".", "mutual_transformer_network", "(", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "self", ".", "mul_bert_emb", ".", "get_full_embedding", "(", "check_ids", ",", "torch", ".", "zeros_like", "(", "check_ids", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", ")", ",", "dim", "=", "0", ")", ")", "\n", "golden_check_out", "=", "golden_check_out", "[", "0", ",", ":", ",", ":", "]", "\n", "golden_check_out", "=", "self", ".", "activation", "(", "golden_check_out", ")", "\n", "\n", "\n", "golden_out", "=", "self", ".", "mutual_transformer_network", "(", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "self", ".", "mul_bert_emb", ".", "get_full_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", ".", "transpose", "(", "0", ",", "1", ")", "[", ":", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", ")", "\n", "golden_out", "=", "golden_out", "[", "0", ",", ":", ",", ":", "]", "\n", "golden_out", "=", "self", ".", "activation", "(", "golden_out", ")", "\n", "\n", "golden_out", "=", "torch", ".", "cat", "(", "(", "golden_out", ",", "golden_check_out", ")", ",", "dim", "=", "1", ")", "\n", "golden_out", "=", "self", ".", "mutual_mlp", "(", "golden_out", ")", "\n", "Golden_loss", "=", "self", ".", "mse_fct", "(", "golden_out", ",", "labels", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "def", "loss_mask_and_normalize", "(", "loss", ",", "mask", ")", ":", "\n", "                ", "mask", "=", "mask", ".", "type_as", "(", "loss", ")", "\n", "loss", "=", "loss", "*", "mask", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ")", "+", "1e-5", "\n", "return", "(", "loss", "/", "denominator", ")", ".", "sum", "(", ")", "\n", "\n", "", "if", "masked_lm_labels", "is", "None", ":", "\n", "                ", "if", "masked_pos", "is", "None", ":", "\n", "                    ", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "else", ":", "\n", "                    ", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n", "\n", "", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores_masked", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "crit_mask_lm_smoothed", ":", "\n", "                ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm_smoothed", "(", "F", ".", "log_softmax", "(", "prediction_scores_masked", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ",", "masked_lm_labels", ")", "\n", "", "else", ":", "\n", "                ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm", "(", "prediction_scores_masked", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ")", "\n", "", "masked_lm_loss", "=", "loss_mask_and_normalize", "(", "masked_lm_loss", ".", "float", "(", ")", ",", "masked_weights", ")", "\n", "\n", "if", "self", ".", "crit_next_sent", "is", "None", "or", "next_sentence_label", "is", "None", ":", "\n", "                ", "next_sentence_loss", "=", "0.0", "\n", "", "else", ":", "\n", "                ", "next_sentence_loss", "=", "self", ".", "crit_next_sent", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ".", "float", "(", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "\n", "", "if", "pair_x", "is", "None", "or", "pair_y", "is", "None", "or", "pair_r", "is", "None", "or", "pair_pos_neg_mask", "is", "None", "or", "pair_loss_mask", "is", "None", ":", "\n", "                ", "return", "masked_lm_loss", ",", "next_sentence_loss", ",", "KL_loss", ",", "Mutual_loss", ",", "Golden_loss", ",", "predict_loss", "\n", "#return masked_lm_loss, next_sentence_loss, KL_loss, Mutual_loss, Golden_loss, predict_loss", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForPreTrainingLossMask.beam_search": [[1896, 2221], ["list", "list", "input_ids.new().fill_", "modeling.BertForPreTrainingLossMask.bert.get_embedding", "modeling.BertForPreTrainingLossMask.transpose", "modeling.BertForPreTrainingLossMask.prior_transformer_network", "QK_embedding_output.transpose", "modeling.BertForPreTrainingLossMask.predict_transformer", "modeling.BertForPreTrainingLossMask.activation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "modeling.BertForPreTrainingLossMask.bert.get_embedding", "torch.einsum.squeeze", "torch.einsum.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForPreTrainingLossMask.mu_mlp1", "modeling.BertForPreTrainingLossMask.var_mlp1", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "range", "input_ids.size", "first_expand.size", "input_ids.new().fill_", "modeling.BertForPreTrainingLossMask.predict_mlp().squeeze", "torch.einsum.expand().type_as", "torch.einsum.expand().type_as", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForPreTrainingLossMask.prob_dense", "modeling.BertForPreTrainingLossMask.activation", "modeling.BertForPreTrainingLossMask.cls", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "step_back_ptrs.append", "step_ids.append", "beam_masks.append", "total_scores.append", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "traces[].append", "traces[].append", "traces[].append", "enumerate", "range", "sequences[].data.new().fill_", "enumerate", "_pad_sequence().to", "input_ids.new", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "latent_z.unsqueeze.unsqueeze.unsqueeze", "modeling.BertForPreTrainingLossMask.bert", "latent_z.unsqueeze.unsqueeze.unsqueeze().expand().reshape", "modeling.BertForPreTrainingLossMask.bert", "log_scores[].fill_", "len", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.div", "torch.div", "torch.div", "torch.div", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.repeat", "torch.reshape.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "list", "len", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "modeling.BertForPreTrainingLossMask.beam_search.first_expand"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding"], ["", "", "", "def", "beam_search", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "K", "=", "self", ".", "search_beam_size", "\n", "\n", "total_scores", "=", "[", "]", "\n", "beam_masks", "=", "[", "]", "\n", "step_ids", "=", "[", "]", "\n", "step_back_ptrs", "=", "[", "]", "\n", "partial_seqs", "=", "[", "]", "\n", "forbid_word_mask", "=", "None", "\n", "buf_matrix", "=", "None", "\n", "\n", "source_token_type_ids", "=", "token_type_ids", "[", ":", ",", ":", "input_length", "]", "\n", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "source_token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "prior_out", "=", "self", ".", "prior_transformer_network", "(", "trans_embedding_output", ")", "\n", "prior", "=", "prior_out", "[", "0", ",", ":", ",", ":", "]", "\n", "\n", "QK_embedding_output", "=", "embedding_output", "[", ":", ",", ":", "210", ",", ":", "]", "\n", "trans_QK_embedding_output", "=", "QK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "predict_out", "=", "self", ".", "predict_transformer", "(", "trans_QK_embedding_output", ")", "\n", "predict_out", "=", "predict_out", "[", "0", ",", ":", ",", ":", "]", "\n", "predict_out", "=", "self", ".", "activation", "(", "predict_out", ")", "\n", "predict_probs", "=", "torch", ".", "sigmoid", "(", "self", ".", "predict_mlp", "(", "predict_out", ")", ".", "squeeze", "(", ")", ")", "\n", "\n", "bleu", "=", "predict_probs", "\n", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "add_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "bleu", ",", "add_embedding", ".", "expand", "(", "bleu", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "bleu", ")", ")", "\n", "\n", "prior", "=", "torch", ".", "cat", "(", "(", "prior", ",", "add_embedding", ")", ",", "dim", "=", "1", ")", "\n", "\n", "prior_mu", "=", "self", ".", "mu_mlp1", "(", "prior", ")", "# B*768", "\n", "prior_logvar", "=", "self", ".", "var_mlp1", "(", "prior", ")", "\n", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "prior_logvar", ")", "\n", "eps", "=", "torch", ".", "randn", "(", "[", "prior_logvar", ".", "shape", "[", "0", "]", ",", "self", ".", "latent_size", "]", ",", "device", "=", "prior_mu", ".", "device", ")", "# B * hidden", "\n", "#latent_z = eps * std + prior_mu", "\n", "\n", "latent_z", "=", "add_embedding", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "\n", "if", "prev_embedding", "is", "None", ":", "\n", "                ", "latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", "\n", "beam_latent_z", "=", "latent_z", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "True", ",", "\n", "latent_z", "=", "latent_z", ",", "decode", "=", "True", ",", "position_ids", "=", "curr_position_ids", ")", "\n", "", "else", ":", "\n", "                ", "beam_latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "self", ".", "search_beam_size", ",", "-", "1", ",", "-", "1", ")", ".", "reshape", "(", "-", "1", ",", "latent_z", ".", "shape", "[", "1", "]", ",", "latent_z", ".", "shape", "[", "2", "]", ")", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "\n", "relace_embeddings", "=", "False", ",", "decode", "=", "True", ",", "position_ids", "=", "curr_position_ids", ")", "\n", "\n", "", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "last_hidden", "=", "torch", ".", "cat", "(", "(", "last_hidden", ",", "beam_latent_z", ")", ",", "dim", "=", "2", ")", "\n", "last_hidden", "=", "self", ".", "prob_dense", "(", "last_hidden", ")", "\n", "last_hidden", "=", "self", ".", "activation", "(", "last_hidden", ")", "\n", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "log_scores", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "forbid_word_mask", "is", "not", "None", ":", "\n", "                ", "log_scores", "+=", "(", "forbid_word_mask", "*", "-", "10000.0", ")", "\n", "", "if", "self", ".", "min_len", "and", "(", "next_pos", "-", "input_length", "+", "1", "<=", "self", ".", "min_len", ")", ":", "\n", "                ", "log_scores", "[", ":", ",", ":", ",", "self", ".", "eos_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "log_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "kk_scores", ",", "kk_ids", "=", "torch", ".", "topk", "(", "log_scores", ",", "k", "=", "K", ")", "\n", "if", "len", "(", "total_scores", ")", "==", "0", ":", "\n", "                ", "k_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "back_ptrs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "K", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "k_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "", "else", ":", "\n", "                ", "last_eos", "=", "torch", ".", "reshape", "(", "\n", "beam_masks", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "last_seq_scores", "=", "torch", ".", "reshape", "(", "\n", "total_scores", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "kk_scores", "+=", "last_eos", "*", "(", "-", "10000.0", ")", "+", "last_seq_scores", "\n", "kk_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_scores", ",", "k_ids", "=", "torch", ".", "topk", "(", "kk_scores", ",", "k", "=", "K", ")", "\n", "back_ptrs", "=", "torch", ".", "div", "(", "k_ids", ",", "K", ")", "\n", "kk_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_ids", "=", "torch", ".", "gather", "(", "kk_ids", ",", "1", ",", "k_ids", ")", "\n", "", "step_back_ptrs", ".", "append", "(", "back_ptrs", ")", "\n", "step_ids", ".", "append", "(", "k_ids", ")", "\n", "beam_masks", ".", "append", "(", "torch", ".", "eq", "(", "k_ids", ",", "self", ".", "eos_id", ")", ".", "float", "(", ")", ")", "\n", "total_scores", ".", "append", "(", "k_scores", ")", "\n", "\n", "def", "first_expand", "(", "x", ")", ":", "\n", "                ", "input_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "expanded_shape", "=", "input_shape", "[", ":", "1", "]", "+", "[", "1", "]", "+", "input_shape", "[", "1", ":", "]", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "\n", "repeat_count", "=", "[", "1", ",", "K", "]", "+", "[", "1", "]", "*", "(", "len", "(", "input_shape", ")", "-", "1", ")", "\n", "x", "=", "x", ".", "repeat", "(", "*", "repeat_count", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "input_shape", "[", "0", "]", "*", "K", "]", "+", "input_shape", "[", "1", ":", "]", ")", "\n", "return", "x", "\n", "\n", "", "def", "select_beam_items", "(", "x", ",", "ids", ")", ":", "\n", "                ", "id_shape", "=", "list", "(", "ids", ".", "size", "(", ")", ")", "\n", "id_rank", "=", "len", "(", "id_shape", ")", "\n", "assert", "len", "(", "id_shape", ")", "==", "2", "\n", "x_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "batch_size", ",", "K", "]", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "x_rank", "=", "len", "(", "x_shape", ")", "+", "1", "\n", "assert", "x_rank", ">=", "2", "\n", "if", "id_rank", "<", "x_rank", ":", "\n", "                    ", "ids", "=", "torch", ".", "reshape", "(", "\n", "ids", ",", "id_shape", "+", "[", "1", "]", "*", "(", "x_rank", "-", "id_rank", ")", ")", "\n", "ids", "=", "ids", ".", "expand", "(", "id_shape", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "", "y", "=", "torch", ".", "gather", "(", "x", ",", "1", ",", "ids", ")", "\n", "y", "=", "torch", ".", "reshape", "(", "y", ",", "x_shape", ")", "\n", "return", "y", "\n", "\n", "", "is_first", "=", "(", "prev_embedding", "is", "None", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "\n", "", "", "curr_ids", "=", "torch", ".", "reshape", "(", "k_ids", ",", "[", "batch_size", "*", "K", ",", "1", "]", ")", "\n", "\n", "if", "is_first", ":", "\n", "                ", "token_type_ids", "=", "first_expand", "(", "token_type_ids", ")", "\n", "position_ids", "=", "first_expand", "(", "position_ids", ")", "\n", "attention_mask", "=", "first_expand", "(", "attention_mask", ")", "\n", "mask_ids", "=", "first_expand", "(", "mask_ids", ")", "\n", "if", "mask_qkv", "is", "not", "None", ":", "\n", "                    ", "mask_qkv", "=", "first_expand", "(", "mask_qkv", ")", "\n", "\n", "", "", "if", "self", ".", "forbid_duplicate_ngrams", ":", "\n", "                ", "wids", "=", "step_ids", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "ptrs", "=", "step_back_ptrs", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "if", "is_first", ":", "\n", "                    ", "partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "partial_seqs", ".", "append", "(", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "new_partial_seqs", ".", "append", "(", "\n", "partial_seqs", "[", "ptrs", "[", "b", "]", "[", "k", "]", "+", "b", "*", "K", "]", "+", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "partial_seqs", "=", "new_partial_seqs", "\n", "\n", "", "def", "get_dup_ngram_candidates", "(", "seq", ",", "n", ")", ":", "\n", "                    ", "cands", "=", "set", "(", ")", "\n", "if", "len", "(", "seq", ")", "<", "n", ":", "\n", "                        ", "return", "[", "]", "\n", "", "tail", "=", "seq", "[", "-", "(", "n", "-", "1", ")", ":", "]", "\n", "if", "self", ".", "forbid_ignore_set", "and", "any", "(", "tk", "in", "self", ".", "forbid_ignore_set", "for", "tk", "in", "tail", ")", ":", "\n", "                        ", "return", "[", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "(", "n", "-", "1", ")", ")", ":", "\n", "                        ", "mismatch", "=", "False", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                            ", "if", "tail", "[", "j", "]", "!=", "seq", "[", "i", "+", "j", "]", ":", "\n", "                                ", "mismatch", "=", "True", "\n", "break", "\n", "", "", "if", "(", "not", "mismatch", ")", "and", "not", "(", "\n", "self", ".", "forbid_ignore_set", "and", "(", "seq", "[", "i", "+", "n", "-", "1", "]", "in", "self", ".", "forbid_ignore_set", ")", ")", ":", "\n", "                            ", "cands", ".", "add", "(", "seq", "[", "i", "+", "n", "-", "1", "]", ")", "\n", "", "", "return", "list", "(", "sorted", "(", "cands", ")", ")", "\n", "\n", "", "if", "len", "(", "partial_seqs", "[", "0", "]", ")", ">=", "self", ".", "ngram_size", ":", "\n", "                    ", "dup_cands", "=", "[", "]", "\n", "for", "seq", "in", "partial_seqs", ":", "\n", "                        ", "dup_cands", ".", "append", "(", "\n", "get_dup_ngram_candidates", "(", "seq", ",", "self", ".", "ngram_size", ")", ")", "\n", "", "if", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dup_cands", ")", ">", "0", ":", "\n", "                        ", "if", "buf_matrix", "is", "None", ":", "\n", "                            ", "vocab_size", "=", "list", "(", "log_scores", ".", "size", "(", ")", ")", "[", "-", "1", "]", "\n", "buf_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "batch_size", "*", "K", ",", "vocab_size", ")", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "                            ", "buf_matrix", ".", "fill", "(", "0", ")", "\n", "", "for", "bk", ",", "cands", "in", "enumerate", "(", "dup_cands", ")", ":", "\n", "                            ", "for", "i", ",", "wid", "in", "enumerate", "(", "cands", ")", ":", "\n", "                                ", "buf_matrix", "[", "bk", ",", "wid", "]", "=", "1.0", "\n", "", "", "forbid_word_mask", "=", "torch", ".", "tensor", "(", "\n", "buf_matrix", ",", "dtype", "=", "log_scores", ".", "dtype", ")", "\n", "forbid_word_mask", "=", "torch", ".", "reshape", "(", "\n", "forbid_word_mask", ",", "[", "batch_size", "*", "K", ",", "1", ",", "vocab_size", "]", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                        ", "forbid_word_mask", "=", "None", "\n", "", "", "", "next_pos", "+=", "1", "\n", "\n", "# [(batch, beam)]", "\n", "", "total_scores", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "total_scores", "]", "\n", "step_ids", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_ids", "]", "\n", "step_back_ptrs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_back_ptrs", "]", "\n", "# back tracking", "\n", "traces", "=", "{", "'pred_seq'", ":", "[", "]", ",", "'scores'", ":", "[", "]", ",", "'wids'", ":", "[", "]", ",", "'ptrs'", ":", "[", "]", "}", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "# [(beam,)]", "\n", "            ", "scores", "=", "[", "x", "[", "b", "]", "for", "x", "in", "total_scores", "]", "\n", "wids_list", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_ids", "]", "\n", "ptrs", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_back_ptrs", "]", "\n", "traces", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "\n", "traces", "[", "'wids'", "]", ".", "append", "(", "wids_list", ")", "\n", "traces", "[", "'ptrs'", "]", ".", "append", "(", "ptrs", ")", "\n", "# first we need to find the eos frame where all symbols are eos", "\n", "# any frames after the eos frame are invalid", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "                ", "if", "all", "(", "wid", "==", "self", ".", "eos_id", "for", "wid", "in", "wids", ")", ":", "\n", "                    ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "                ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "                    ", "if", "wid", "==", "self", ".", "eos_id", "or", "fid", "==", "last_frame_id", ":", "\n", "                        ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "self", ".", "length_penalty", ">", "0", ":", "\n", "                            ", "s", "/=", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "\n", "self", ".", "length_penalty", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "                            ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "                ", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "                    ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "pos_in_frame", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "pos_in_frame", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "seq", ")", "\n", "\n", "", "", "def", "_pad_sequence", "(", "sequences", ",", "max_len", ",", "padding_value", "=", "0", ")", ":", "\n", "            ", "trailing_dims", "=", "sequences", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "out_dims", "=", "(", "len", "(", "sequences", ")", ",", "max_len", ")", "+", "trailing_dims", "\n", "\n", "out_tensor", "=", "sequences", "[", "0", "]", ".", "data", ".", "new", "(", "*", "out_dims", ")", ".", "fill_", "(", "padding_value", ")", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "sequences", ")", ":", "\n", "                ", "length", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# use index notation to prevent duplicate references to the tensor", "\n", "out_tensor", "[", "i", ",", ":", "length", ",", "...", "]", "=", "tensor", "\n", "", "return", "out_tensor", "\n", "\n", "# convert to tensors for DataParallel", "\n", "", "for", "k", "in", "(", "'pred_seq'", ",", "'scores'", ",", "'wids'", ",", "'ptrs'", ")", ":", "\n", "            ", "ts_list", "=", "traces", "[", "k", "]", "\n", "if", "not", "isinstance", "(", "ts_list", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "dt", "=", "torch", ".", "float", "if", "k", "==", "'scores'", "else", "torch", ".", "long", "\n", "ts_list", "=", "[", "torch", ".", "tensor", "(", "it", ",", "dtype", "=", "dt", ")", "for", "it", "in", "ts_list", "]", "\n", "", "traces", "[", "k", "]", "=", "_pad_sequence", "(", "\n", "ts_list", ",", "output_length", ",", "padding_value", "=", "0", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "", "return", "traces", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForSeq2SeqDecoder.__init__": [[2226, 2270], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModelIncr", "modeling.BertPreTrainingHeads", "modeling.BertForSeq2SeqDecoder.apply", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "modeling.TransformerEncoderLayer", "modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh", "modeling.BertPreTrainingPairRel"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "mask_word_id", "=", "0", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "search_beam_size", "=", "1", ",", "length_penalty", "=", "1.0", ",", "eos_id", "=", "0", ",", "sos_id", "=", "0", ",", "forbid_duplicate_ngrams", "=", "False", ",", "forbid_ignore_set", "=", "None", ",", "not_predict_set", "=", "None", ",", "ngram_size", "=", "1", ",", "min_len", "=", "1", ",", "mode", "=", "\"s2s\"", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSeq2SeqDecoder", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModelIncr", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "self", ".", "mask_word_id", "=", "mask_word_id", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "self", ".", "search_beam_size", "=", "search_beam_size", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "forbid_duplicate_ngrams", "=", "forbid_duplicate_ngrams", "\n", "self", ".", "forbid_ignore_set", "=", "forbid_ignore_set", "\n", "self", ".", "not_predict_set", "=", "not_predict_set", "\n", "self", ".", "ngram_size", "=", "ngram_size", "\n", "self", ".", "min_len", "=", "min_len", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n", "# CVAE parameter", "\n", "self", ".", "latent_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "mu_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "mu_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "KL_weight", "=", "1", "\n", "\n", "self", ".", "prior_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "prior_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "prior_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "predict_transformer_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "predict_transformer", "=", "TransformerEncoder", "(", "predict_transformer_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "predict_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "prob_dense", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForSeq2SeqDecoder.forward": [[2271, 2381], ["list", "list", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.bert.get_embedding", "modeling.BertForSeq2SeqDecoder.transpose", "modeling.BertForSeq2SeqDecoder.prior_transformer_network", "modeling.BertForSeq2SeqDecoder.bert.get_embedding", "torch.einsum.squeeze", "torch.einsum.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.mu_mlp1", "modeling.BertForSeq2SeqDecoder.var_mlp1", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "latent_z.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.beam_search", "input_ids.size", "token_type_ids.size", "input_ids.new().fill_", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.einsum.expand().type_as", "torch.einsum.expand().type_as", "modeling.BertForSeq2SeqDecoder.bert", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.prob_dense", "modeling.BertForSeq2SeqDecoder.activation", "modeling.BertForSeq2SeqDecoder.cls", "torch.max", "torch.max", "torch.max", "torch.max", "output_ids.append", "input_ids.new", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "input_ids.new", "torch.einsum.expand", "torch.einsum.expand", "curr_ids.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prediction_scores[].fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForSeq2SeqDecoder.beam_search"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ",", "bleu", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "search_beam_size", ">", "1", ":", "\n", "            ", "return", "self", ".", "beam_search", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "source_token_type_ids", "=", "token_type_ids", "[", ":", ",", ":", "input_length", "]", "\n", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "source_token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "prior_out", "=", "self", ".", "prior_transformer_network", "(", "trans_embedding_output", ")", "\n", "prior", "=", "prior_out", "[", "0", ",", ":", ",", ":", "]", "\n", "\n", "bleu", "=", "bleu", "*", "torch", ".", "ones", "(", "size", "=", "[", "prior", ".", "shape", "[", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "prior", ".", "device", ")", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "add_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "bleu", ",", "add_embedding", ".", "expand", "(", "bleu", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "bleu", ")", ")", "\n", "\n", "prior", "=", "torch", ".", "cat", "(", "(", "prior", ",", "add_embedding", ")", ",", "dim", "=", "1", ")", "\n", "\n", "prior_mu", "=", "self", ".", "mu_mlp1", "(", "prior", ")", "\n", "prior_logvar", "=", "self", ".", "var_mlp1", "(", "prior", ")", "\n", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "prior_logvar", ")", "\n", "eps", "=", "torch", ".", "randn", "(", "[", "prior_logvar", ".", "shape", "[", "0", "]", ",", "self", ".", "latent_size", "]", ",", "device", "=", "prior_mu", ".", "device", ")", "\n", "#latent_z = eps * std + prior_mu", "\n", "\n", "latent_z", "=", "add_embedding", "\n", "latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "\n", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "\n", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "last_hidden", "=", "torch", ".", "cat", "(", "(", "last_hidden", ",", "latent_z", ")", ",", "dim", "=", "2", ")", "\n", "last_hidden", "=", "self", ".", "prob_dense", "(", "last_hidden", ")", "\n", "last_hidden", "=", "self", ".", "activation", "(", "last_hidden", ")", "\n", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "prediction_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "_", ",", "max_ids", "=", "torch", ".", "max", "(", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "output_ids", ".", "append", "(", "max_ids", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "curr_ids", "=", "max_ids", "\n", "next_pos", "+=", "1", "\n", "\n", "", "return", "torch", ".", "cat", "(", "output_ids", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertForSeq2SeqDecoder.beam_search": [[2382, 2704], ["list", "list", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.bert.get_embedding", "modeling.BertForSeq2SeqDecoder.transpose", "modeling.BertForSeq2SeqDecoder.prior_transformer_network", "QK_embedding_output.transpose", "modeling.BertForSeq2SeqDecoder.predict_transformer", "modeling.BertForSeq2SeqDecoder.activation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "modeling.BertForSeq2SeqDecoder.bert.get_embedding", "torch.einsum.squeeze", "torch.einsum.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.mu_mlp1", "modeling.BertForSeq2SeqDecoder.var_mlp1", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "range", "input_ids.size", "first_expand.size", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.predict_mlp().squeeze", "torch.einsum.expand().type_as", "torch.einsum.expand().type_as", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.prob_dense", "modeling.BertForSeq2SeqDecoder.activation", "modeling.BertForSeq2SeqDecoder.cls", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "step_back_ptrs.append", "step_ids.append", "beam_masks.append", "total_scores.append", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "traces[].append", "traces[].append", "traces[].append", "enumerate", "range", "sequences[].data.new().fill_", "enumerate", "_pad_sequence().to", "input_ids.new", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "latent_z.unsqueeze.unsqueeze.unsqueeze", "modeling.BertForSeq2SeqDecoder.bert", "latent_z.unsqueeze.unsqueeze.unsqueeze().expand().reshape", "modeling.BertForSeq2SeqDecoder.bert", "log_scores[].fill_", "len", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.div", "torch.div", "torch.div", "torch.div", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.repeat", "torch.reshape.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "list", "len", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "modeling.BertForSeq2SeqDecoder.beam_search.first_expand"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding"], ["", "def", "beam_search", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "K", "=", "self", ".", "search_beam_size", "\n", "\n", "total_scores", "=", "[", "]", "\n", "beam_masks", "=", "[", "]", "\n", "step_ids", "=", "[", "]", "\n", "step_back_ptrs", "=", "[", "]", "\n", "partial_seqs", "=", "[", "]", "\n", "forbid_word_mask", "=", "None", "\n", "buf_matrix", "=", "None", "\n", "\n", "source_token_type_ids", "=", "token_type_ids", "[", ":", ",", ":", "input_length", "]", "\n", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "source_token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "prior_out", "=", "self", ".", "prior_transformer_network", "(", "trans_embedding_output", ")", "\n", "prior", "=", "prior_out", "[", "0", ",", ":", ",", ":", "]", "\n", "\n", "\n", "QK_embedding_output", "=", "embedding_output", "[", ":", ",", ":", "210", ",", ":", "]", "\n", "trans_QK_embedding_output", "=", "QK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "predict_out", "=", "self", ".", "predict_transformer", "(", "trans_QK_embedding_output", ")", "\n", "predict_out", "=", "predict_out", "[", "0", ",", ":", ",", ":", "]", "\n", "predict_out", "=", "self", ".", "activation", "(", "predict_out", ")", "\n", "predict_probs", "=", "torch", ".", "sigmoid", "(", "self", ".", "predict_mlp", "(", "predict_out", ")", ".", "squeeze", "(", ")", ")", "\n", "bleu", "=", "predict_probs", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "add_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "bleu", ",", "add_embedding", ".", "expand", "(", "bleu", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "bleu", ")", ")", "\n", "\n", "prior", "=", "torch", ".", "cat", "(", "(", "prior", ",", "add_embedding", ")", ",", "dim", "=", "1", ")", "\n", "\n", "prior_mu", "=", "self", ".", "mu_mlp1", "(", "prior", ")", "\n", "prior_logvar", "=", "self", ".", "var_mlp1", "(", "prior", ")", "\n", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "prior_logvar", ")", "\n", "eps", "=", "torch", ".", "randn", "(", "[", "prior_logvar", ".", "shape", "[", "0", "]", ",", "self", ".", "latent_size", "]", ",", "device", "=", "prior_mu", ".", "device", ")", "\n", "latent_z", "=", "eps", "*", "std", "+", "prior_mu", "\n", "\n", "latent_z", "=", "add_embedding", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "\n", "if", "prev_embedding", "is", "None", ":", "\n", "                ", "latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", "\n", "beam_latent_z", "=", "latent_z", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "True", ",", "\n", "latent_z", "=", "latent_z", ")", "\n", "", "else", ":", "\n", "                ", "beam_latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "self", ".", "search_beam_size", ",", "-", "1", ",", "-", "1", ")", ".", "reshape", "(", "-", "1", ",", "latent_z", ".", "shape", "[", "1", "]", ",", "latent_z", ".", "shape", "[", "2", "]", ")", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "False", ")", "\n", "\n", "", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "last_hidden", "=", "torch", ".", "cat", "(", "(", "last_hidden", ",", "beam_latent_z", ")", ",", "dim", "=", "2", ")", "\n", "last_hidden", "=", "self", ".", "prob_dense", "(", "last_hidden", ")", "\n", "last_hidden", "=", "self", ".", "activation", "(", "last_hidden", ")", "\n", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "log_scores", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "forbid_word_mask", "is", "not", "None", ":", "\n", "                ", "log_scores", "+=", "(", "forbid_word_mask", "*", "-", "10000.0", ")", "\n", "", "if", "self", ".", "min_len", "and", "(", "next_pos", "-", "input_length", "+", "1", "<=", "self", ".", "min_len", ")", ":", "\n", "                ", "log_scores", "[", ":", ",", ":", ",", "self", ".", "eos_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "log_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "kk_scores", ",", "kk_ids", "=", "torch", ".", "topk", "(", "log_scores", ",", "k", "=", "K", ")", "\n", "if", "len", "(", "total_scores", ")", "==", "0", ":", "\n", "                ", "k_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "back_ptrs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "K", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "k_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "", "else", ":", "\n", "                ", "last_eos", "=", "torch", ".", "reshape", "(", "\n", "beam_masks", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "last_seq_scores", "=", "torch", ".", "reshape", "(", "\n", "total_scores", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "kk_scores", "+=", "last_eos", "*", "(", "-", "10000.0", ")", "+", "last_seq_scores", "\n", "kk_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_scores", ",", "k_ids", "=", "torch", ".", "topk", "(", "kk_scores", ",", "k", "=", "K", ")", "\n", "back_ptrs", "=", "torch", ".", "div", "(", "k_ids", ",", "K", ")", "\n", "kk_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_ids", "=", "torch", ".", "gather", "(", "kk_ids", ",", "1", ",", "k_ids", ")", "\n", "", "step_back_ptrs", ".", "append", "(", "back_ptrs", ")", "\n", "step_ids", ".", "append", "(", "k_ids", ")", "\n", "beam_masks", ".", "append", "(", "torch", ".", "eq", "(", "k_ids", ",", "self", ".", "eos_id", ")", ".", "float", "(", ")", ")", "\n", "total_scores", ".", "append", "(", "k_scores", ")", "\n", "\n", "def", "first_expand", "(", "x", ")", ":", "\n", "                ", "input_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "expanded_shape", "=", "input_shape", "[", ":", "1", "]", "+", "[", "1", "]", "+", "input_shape", "[", "1", ":", "]", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "\n", "repeat_count", "=", "[", "1", ",", "K", "]", "+", "[", "1", "]", "*", "(", "len", "(", "input_shape", ")", "-", "1", ")", "\n", "x", "=", "x", ".", "repeat", "(", "*", "repeat_count", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "input_shape", "[", "0", "]", "*", "K", "]", "+", "input_shape", "[", "1", ":", "]", ")", "\n", "return", "x", "\n", "\n", "", "def", "select_beam_items", "(", "x", ",", "ids", ")", ":", "\n", "                ", "id_shape", "=", "list", "(", "ids", ".", "size", "(", ")", ")", "\n", "id_rank", "=", "len", "(", "id_shape", ")", "\n", "assert", "len", "(", "id_shape", ")", "==", "2", "\n", "x_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "batch_size", ",", "K", "]", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "x_rank", "=", "len", "(", "x_shape", ")", "+", "1", "\n", "assert", "x_rank", ">=", "2", "\n", "if", "id_rank", "<", "x_rank", ":", "\n", "                    ", "ids", "=", "torch", ".", "reshape", "(", "\n", "ids", ",", "id_shape", "+", "[", "1", "]", "*", "(", "x_rank", "-", "id_rank", ")", ")", "\n", "ids", "=", "ids", ".", "expand", "(", "id_shape", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "", "y", "=", "torch", ".", "gather", "(", "x", ",", "1", ",", "ids", ")", "\n", "y", "=", "torch", ".", "reshape", "(", "y", ",", "x_shape", ")", "\n", "return", "y", "\n", "\n", "", "is_first", "=", "(", "prev_embedding", "is", "None", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "\n", "", "", "curr_ids", "=", "torch", ".", "reshape", "(", "k_ids", ",", "[", "batch_size", "*", "K", ",", "1", "]", ")", "\n", "\n", "if", "is_first", ":", "\n", "                ", "token_type_ids", "=", "first_expand", "(", "token_type_ids", ")", "\n", "position_ids", "=", "first_expand", "(", "position_ids", ")", "\n", "attention_mask", "=", "first_expand", "(", "attention_mask", ")", "\n", "mask_ids", "=", "first_expand", "(", "mask_ids", ")", "\n", "if", "mask_qkv", "is", "not", "None", ":", "\n", "                    ", "mask_qkv", "=", "first_expand", "(", "mask_qkv", ")", "\n", "\n", "", "", "if", "self", ".", "forbid_duplicate_ngrams", ":", "\n", "                ", "wids", "=", "step_ids", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "ptrs", "=", "step_back_ptrs", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "if", "is_first", ":", "\n", "                    ", "partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "partial_seqs", ".", "append", "(", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "new_partial_seqs", ".", "append", "(", "\n", "partial_seqs", "[", "ptrs", "[", "b", "]", "[", "k", "]", "+", "b", "*", "K", "]", "+", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "partial_seqs", "=", "new_partial_seqs", "\n", "\n", "", "def", "get_dup_ngram_candidates", "(", "seq", ",", "n", ")", ":", "\n", "                    ", "cands", "=", "set", "(", ")", "\n", "if", "len", "(", "seq", ")", "<", "n", ":", "\n", "                        ", "return", "[", "]", "\n", "", "tail", "=", "seq", "[", "-", "(", "n", "-", "1", ")", ":", "]", "\n", "if", "self", ".", "forbid_ignore_set", "and", "any", "(", "tk", "in", "self", ".", "forbid_ignore_set", "for", "tk", "in", "tail", ")", ":", "\n", "                        ", "return", "[", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "(", "n", "-", "1", ")", ")", ":", "\n", "                        ", "mismatch", "=", "False", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                            ", "if", "tail", "[", "j", "]", "!=", "seq", "[", "i", "+", "j", "]", ":", "\n", "                                ", "mismatch", "=", "True", "\n", "break", "\n", "", "", "if", "(", "not", "mismatch", ")", "and", "not", "(", "self", ".", "forbid_ignore_set", "and", "(", "seq", "[", "i", "+", "n", "-", "1", "]", "in", "self", ".", "forbid_ignore_set", ")", ")", ":", "\n", "                            ", "cands", ".", "add", "(", "seq", "[", "i", "+", "n", "-", "1", "]", ")", "\n", "", "", "return", "list", "(", "sorted", "(", "cands", ")", ")", "\n", "\n", "", "if", "len", "(", "partial_seqs", "[", "0", "]", ")", ">=", "self", ".", "ngram_size", ":", "\n", "                    ", "dup_cands", "=", "[", "]", "\n", "for", "seq", "in", "partial_seqs", ":", "\n", "                        ", "dup_cands", ".", "append", "(", "\n", "get_dup_ngram_candidates", "(", "seq", ",", "self", ".", "ngram_size", ")", ")", "\n", "", "if", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dup_cands", ")", ">", "0", ":", "\n", "                        ", "if", "buf_matrix", "is", "None", ":", "\n", "                            ", "vocab_size", "=", "list", "(", "log_scores", ".", "size", "(", ")", ")", "[", "-", "1", "]", "\n", "buf_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "batch_size", "*", "K", ",", "vocab_size", ")", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "                            ", "buf_matrix", ".", "fill", "(", "0", ")", "\n", "", "for", "bk", ",", "cands", "in", "enumerate", "(", "dup_cands", ")", ":", "\n", "                            ", "for", "i", ",", "wid", "in", "enumerate", "(", "cands", ")", ":", "\n", "                                ", "buf_matrix", "[", "bk", ",", "wid", "]", "=", "1.0", "\n", "", "", "forbid_word_mask", "=", "torch", ".", "tensor", "(", "\n", "buf_matrix", ",", "dtype", "=", "log_scores", ".", "dtype", ")", "\n", "forbid_word_mask", "=", "torch", ".", "reshape", "(", "\n", "forbid_word_mask", ",", "[", "batch_size", "*", "K", ",", "1", ",", "vocab_size", "]", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                        ", "forbid_word_mask", "=", "None", "\n", "", "", "", "next_pos", "+=", "1", "\n", "\n", "# [(batch, beam)]", "\n", "", "total_scores", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "total_scores", "]", "\n", "step_ids", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_ids", "]", "\n", "step_back_ptrs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_back_ptrs", "]", "\n", "# back tracking", "\n", "traces", "=", "{", "'pred_seq'", ":", "[", "]", ",", "'scores'", ":", "[", "]", ",", "'wids'", ":", "[", "]", ",", "'ptrs'", ":", "[", "]", "}", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "# [(beam,)]", "\n", "            ", "scores", "=", "[", "x", "[", "b", "]", "for", "x", "in", "total_scores", "]", "\n", "wids_list", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_ids", "]", "\n", "ptrs", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_back_ptrs", "]", "\n", "traces", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "\n", "traces", "[", "'wids'", "]", ".", "append", "(", "wids_list", ")", "\n", "traces", "[", "'ptrs'", "]", ".", "append", "(", "ptrs", ")", "\n", "# first we need to find the eos frame where all symbols are eos", "\n", "# any frames after the eos frame are invalid", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "                ", "if", "all", "(", "wid", "==", "self", ".", "eos_id", "for", "wid", "in", "wids", ")", ":", "\n", "                    ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "                ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "                    ", "if", "wid", "==", "self", ".", "eos_id", "or", "fid", "==", "last_frame_id", ":", "\n", "                        ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "self", ".", "length_penalty", ">", "0", ":", "\n", "                            ", "s", "/=", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "\n", "self", ".", "length_penalty", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "                            ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "                ", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "                    ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "pos_in_frame", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "pos_in_frame", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "seq", ")", "\n", "\n", "", "", "def", "_pad_sequence", "(", "sequences", ",", "max_len", ",", "padding_value", "=", "0", ")", ":", "\n", "            ", "trailing_dims", "=", "sequences", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "out_dims", "=", "(", "len", "(", "sequences", ")", ",", "max_len", ")", "+", "trailing_dims", "\n", "\n", "out_tensor", "=", "sequences", "[", "0", "]", ".", "data", ".", "new", "(", "*", "out_dims", ")", ".", "fill_", "(", "padding_value", ")", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "sequences", ")", ":", "\n", "                ", "length", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# use index notation to prevent duplicate references to the tensor", "\n", "out_tensor", "[", "i", ",", ":", "length", ",", "...", "]", "=", "tensor", "\n", "", "return", "out_tensor", "\n", "\n", "# convert to tensors for DataParallel", "\n", "", "for", "k", "in", "(", "'pred_seq'", ",", "'scores'", ",", "'wids'", ",", "'ptrs'", ")", ":", "\n", "            ", "ts_list", "=", "traces", "[", "k", "]", "\n", "if", "not", "isinstance", "(", "ts_list", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "dt", "=", "torch", ".", "float", "if", "k", "==", "'scores'", "else", "torch", ".", "long", "\n", "ts_list", "=", "[", "torch", ".", "tensor", "(", "it", ",", "dtype", "=", "dt", ")", "for", "it", "in", "ts_list", "]", "\n", "", "traces", "[", "k", "]", "=", "_pad_sequence", "(", "\n", "ts_list", ",", "output_length", ",", "padding_value", "=", "0", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "", "return", "traces", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.gelu": [[57, 63], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.swish": [[65, 67], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling._get_clones": [[71, 73], ["torch.nn.modules.container.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["def", "_get_clones", "(", "module", ",", "N", ")", ":", "\n", "    ", "return", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling._get_activation_fn": [[75, 82], ["RuntimeError"], "function", ["None"], ["", "def", "_get_activation_fn", "(", "activation", ")", ":", "\n", "    ", "if", "activation", "==", "\"relu\"", ":", "\n", "        ", "return", "F", ".", "relu", "\n", "", "elif", "activation", "==", "\"gelu\"", ":", "\n", "        ", "return", "F", ".", "gelu", "\n", "\n", "", "raise", "RuntimeError", "(", "\"activation should be relu/gelu, not {}\"", ".", "format", "(", "activation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertEmbeddings.get_full_embedding": [[292, 328], ["input_ids.size", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "get_full_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# if relace_embeddings == True:", "\n", "#     words_embeddings = self.word_embeddings(input_ids)", "\n", "#     words_embeddings = torch.cat((words_embeddings[:, 0, :].unsqueeze(1), latent_z.type_as(words_embeddings),", "\n", "#                                   words_embeddings[:, 2:, :]), dim=1)", "\n", "# else:", "\n", "#     words_embeddings = self.word_embeddings(input_ids)", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "position_embeddings", "+", "token_type_embeddings", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "#embeddings = words_embeddings + position_embeddings + token_type_embeddings", "\n", "embeddings", "+=", "words_embeddings", "\n", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "\n", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_full_embedding": [[1160, 1164], ["modeling.BertModel.embeddings.get_full_embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.modeling.BertModel.get_full_embedding"], ["", "def", "get_full_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", ".", "get_full_embedding", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieNode.__init__": [[27, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "children", "=", "{", "}", "\n", "self", ".", "is_leaf", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieNode.try_get_children": [[31, 35], ["loader_utils.TrieNode"], "methods", ["None"], ["", "def", "try_get_children", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "key", "not", "in", "self", ".", "children", ":", "\n", "            ", "self", ".", "children", "[", "key", "]", "=", "TrieNode", "(", ")", "\n", "", "return", "self", ".", "children", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.__init__": [[38, 40], ["loader_utils.TrieNode"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "root", "=", "TrieNode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add": [[41, 46], ["r.try_get_children.try_get_children.try_get_children"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieNode.try_get_children"], ["", "def", "add", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "r", "=", "self", ".", "root", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "r", "=", "r", ".", "try_get_children", "(", "token", ")", "\n", "", "r", ".", "is_leaf", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.get_pieces": [[47, 69], ["len", "pieces.append", "len", "list", "range"], "methods", ["None"], ["", "def", "get_pieces", "(", "self", ",", "tokens", ",", "offset", ")", ":", "\n", "        ", "pieces", "=", "[", "]", "\n", "r", "=", "self", ".", "root", "\n", "token_id", "=", "0", "\n", "last_valid", "=", "0", "\n", "match_count", "=", "0", "\n", "while", "last_valid", "<", "len", "(", "tokens", ")", ":", "\n", "            ", "if", "token_id", "<", "len", "(", "tokens", ")", "and", "tokens", "[", "token_id", "]", "in", "r", ".", "children", ":", "\n", "                ", "r", "=", "r", ".", "children", "[", "tokens", "[", "token_id", "]", "]", "\n", "match_count", "+=", "1", "\n", "if", "r", ".", "is_leaf", ":", "\n", "                    ", "last_valid", "=", "token_id", "\n", "", "token_id", "+=", "1", "\n", "", "else", ":", "\n", "                ", "pieces", ".", "append", "(", "\n", "list", "(", "range", "(", "token_id", "-", "match_count", "+", "offset", ",", "last_valid", "+", "1", "+", "offset", ")", ")", ")", "\n", "last_valid", "+=", "1", "\n", "token_id", "=", "last_valid", "\n", "r", "=", "self", ".", "root", "\n", "match_count", "=", "0", "\n", "\n", "", "", "return", "pieces", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.__init__": [[94, 110], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "skipgram_prb", "=", "None", "\n", "self", ".", "skipgram_size", "=", "None", "\n", "self", ".", "pre_whole_word", "=", "None", "\n", "self", ".", "mask_whole_word", "=", "None", "\n", "self", ".", "word_subsample_prb", "=", "None", "\n", "self", ".", "sp_prob", "=", "None", "\n", "self", ".", "pieces_dir", "=", "None", "\n", "self", ".", "vocab_words", "=", "None", "\n", "self", ".", "pieces_threshold", "=", "10", "\n", "self", ".", "trie", "=", "None", "\n", "self", ".", "call_count", "=", "0", "\n", "self", ".", "offline_mode", "=", "False", "\n", "self", ".", "skipgram_size_geo_list", "=", "None", "\n", "self", ".", "span_same_mask", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.init_skipgram_size_geo_list": [[111, 120], ["range", "sum", "g_list.append"], "methods", ["None"], ["", "def", "init_skipgram_size_geo_list", "(", "self", ",", "p", ")", ":", "\n", "        ", "if", "p", ">", "0", ":", "\n", "            ", "g_list", "=", "[", "]", "\n", "t", "=", "p", "\n", "for", "_", "in", "range", "(", "self", ".", "skipgram_size", ")", ":", "\n", "                ", "g_list", ".", "append", "(", "t", ")", "\n", "t", "*=", "(", "1", "-", "p", ")", "\n", "", "s", "=", "sum", "(", "g_list", ")", "\n", "self", ".", "skipgram_size_geo_list", "=", "[", "x", "/", "s", "for", "x", "in", "g_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.create_trie_tree": [[121, 140], ["print", "print", "loader_utils.TrieTree", "loader_utils.Pipeline.trie.add", "print", "open", "line.split", "loader_utils.Pipeline.trie.add", "int", "tokens.extend", "part.split"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add"], ["", "", "def", "create_trie_tree", "(", "self", ",", "pieces_dir", ")", ":", "\n", "        ", "print", "(", "\"sp_prob = {}\"", ".", "format", "(", "self", ".", "sp_prob", ")", ")", "\n", "print", "(", "\"pieces_threshold = {}\"", ".", "format", "(", "self", ".", "pieces_threshold", ")", ")", "\n", "if", "pieces_dir", "is", "not", "None", ":", "\n", "            ", "self", ".", "trie", "=", "TrieTree", "(", ")", "\n", "pieces_files", "=", "[", "pieces_dir", "]", "\n", "for", "token", "in", "self", ".", "vocab_words", ":", "\n", "                ", "self", ".", "trie", ".", "add", "(", "[", "token", "]", ")", "\n", "", "for", "piece_file", "in", "pieces_files", ":", "\n", "                ", "print", "(", "\"Load piece file: {}\"", ".", "format", "(", "piece_file", ")", ")", "\n", "with", "open", "(", "piece_file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                    ", "for", "line", "in", "reader", ":", "\n", "                        ", "parts", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "parts", "[", "-", "1", "]", ")", "<", "self", ".", "pieces_threshold", ":", "\n", "                            ", "pass", "\n", "", "tokens", "=", "[", "]", "\n", "for", "part", "in", "parts", "[", ":", "-", "1", "]", ":", "\n", "                            ", "tokens", ".", "extend", "(", "part", ".", "split", "(", "' '", ")", ")", "\n", "", "self", ".", "trie", ".", "add", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.__call__": [[141, 143], ["None"], "methods", ["None"], ["", "", "", "", "", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.get_masked_pos": [[146, 285], ["list", "set", "enumerate", "random.random.shuffle", "set", "list", "loader_utils.Pipeline.create_trie_tree", "list", "zip", "enumerate", "any", "range", "len", "random.random.shuffle", "len", "loader_utils.Pipeline.trie.get_pieces", "list", "_get_word_split_index.append", "loader_utils._get_word_split_index", "range", "loader_utils.Pipeline.trie.get_pieces", "set.add", "len", "range", "loader_utils._expand_whole_word", "set", "enumerate", "range", "len", "len", "tokens[].endswith", "tokens[].endswith", "tokens[].endswith", "cand_pos.append", "loader_utils._get_word_split_index", "list", "len", "list.add", "tokens[].startswith", "new_pieces[].extend", "new_pieces.append", "len", "cand_pos.append", "len", "range", "random.random.random", "min", "zip", "range", "list.add", "len", "numpy.random.choice", "random.random.random", "random.random.randint", "random.random.random", "set.add", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.create_trie_tree", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.get_pieces", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._get_word_split_index", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.get_pieces", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._expand_whole_word", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._get_word_split_index", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add"], ["", "def", "get_masked_pos", "(", "self", ",", "tokens", ",", "n_pred", ",", "add_skipgram", "=", "False", ",", "mask_segment", "=", "None", ",", "protect_range", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "pieces_dir", "is", "not", "None", "and", "self", ".", "trie", "is", "None", ":", "\n", "            ", "self", ".", "create_trie_tree", "(", "self", ".", "pieces_dir", ")", "\n", "", "if", "self", ".", "pre_whole_word", ":", "\n", "            ", "if", "self", ".", "trie", "is", "not", "None", ":", "\n", "                ", "pieces", "=", "self", ".", "trie", ".", "get_pieces", "(", "tokens", ",", "0", ")", "\n", "\n", "new_pieces", "=", "[", "]", "\n", "for", "piece", "in", "pieces", ":", "\n", "                    ", "if", "len", "(", "new_pieces", ")", ">", "0", "and", "tokens", "[", "piece", "[", "0", "]", "]", ".", "startswith", "(", "\"##\"", ")", ":", "\n", "                        ", "new_pieces", "[", "-", "1", "]", ".", "extend", "(", "piece", ")", "\n", "", "else", ":", "\n", "                        ", "new_pieces", ".", "append", "(", "piece", ")", "\n", "", "", "del", "pieces", "\n", "pieces", "=", "new_pieces", "\n", "\n", "pre_word_split", "=", "list", "(", "_", "[", "-", "1", "]", "for", "_", "in", "pieces", ")", "\n", "pre_word_split", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "else", ":", "\n", "                ", "pre_word_split", "=", "_get_word_split_index", "(", "tokens", ",", "0", ",", "len", "(", "tokens", ")", ")", "\n", "", "index2piece", "=", "None", "\n", "", "else", ":", "\n", "            ", "pre_word_split", "=", "list", "(", "range", "(", "0", ",", "len", "(", "tokens", ")", "+", "1", ")", ")", "\n", "\n", "if", "self", ".", "trie", "is", "not", "None", ":", "\n", "                ", "pieces", "=", "self", ".", "trie", ".", "get_pieces", "(", "tokens", ",", "0", ")", "\n", "\n", "index2piece", "=", "{", "}", "\n", "for", "piece", "in", "pieces", ":", "\n", "                    ", "for", "index", "in", "piece", ":", "\n", "                        ", "index2piece", "[", "index", "]", "=", "(", "piece", "[", "0", "]", ",", "piece", "[", "-", "1", "]", ")", "\n", "", "", "", "else", ":", "\n", "                ", "index2piece", "=", "None", "\n", "\n", "", "", "span_list", "=", "list", "(", "zip", "(", "pre_word_split", "[", ":", "-", "1", "]", ",", "pre_word_split", "[", "1", ":", "]", ")", ")", "\n", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "if", "mask_segment", ":", "\n", "            ", "for", "i", ",", "sp", "in", "enumerate", "(", "span_list", ")", ":", "\n", "                ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "(", "sp_end", "-", "sp_st", "==", "1", ")", "and", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'SEP]'", ")", ":", "\n", "                    ", "segment_index", "=", "i", "\n", "break", "\n", "", "", "", "for", "i", ",", "sp", "in", "enumerate", "(", "span_list", ")", ":", "\n", "            ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "(", "sp_end", "-", "sp_st", "==", "1", ")", "and", "(", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'CLS]'", ")", "or", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'SEP]'", ")", ")", ":", "\n", "                ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "if", "mask_segment", ":", "\n", "                    ", "if", "(", "(", "i", "<", "segment_index", ")", "and", "(", "'a'", "in", "mask_segment", ")", ")", "or", "(", "(", "i", ">", "segment_index", ")", "and", "(", "'b'", "in", "mask_segment", ")", ")", ":", "\n", "                        ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "", "else", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "for", "i_span", "in", "cand_pos", ":", "\n", "            ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                ", "break", "\n", "", "cand_st", ",", "cand_end", "=", "span_list", "[", "i_span", "]", "\n", "if", "len", "(", "masked_pos", ")", "+", "cand_end", "-", "cand_st", ">", "n_pred", ":", "\n", "                ", "continue", "\n", "", "if", "any", "(", "p", "in", "masked_pos", "for", "p", "in", "range", "(", "cand_st", ",", "cand_end", ")", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "n_span", "=", "1", "\n", "if", "index2piece", "is", "not", "None", ":", "\n", "                ", "p_start", ",", "p_end", "=", "index2piece", "[", "i_span", "]", "\n", "if", "p_start", "<", "p_end", "and", "(", "rand", "(", ")", "<", "self", ".", "sp_prob", ")", ":", "\n", "# n_span = p_end - p_start + 1", "\n", "                    ", "st_span", ",", "end_span", "=", "p_start", ",", "p_end", "+", "1", "\n", "", "else", ":", "\n", "                    ", "st_span", ",", "end_span", "=", "i_span", ",", "i_span", "+", "1", "\n", "", "", "else", ":", "\n", "                ", "rand_skipgram_size", "=", "0", "\n", "# ngram", "\n", "if", "self", ".", "skipgram_size_geo_list", ":", "\n", "# sampling ngram size from geometric distribution", "\n", "                    ", "rand_skipgram_size", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "skipgram_size_geo_list", ")", ",", "1", ",", "p", "=", "self", ".", "skipgram_size_geo_list", ")", "[", "0", "]", "+", "1", "\n", "", "else", ":", "\n", "                    ", "if", "add_skipgram", "and", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "                        ", "rand_skipgram_size", "=", "min", "(", "\n", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", ",", "len", "(", "span_list", ")", "-", "i_span", ")", "\n", "", "", "for", "n", "in", "range", "(", "2", ",", "rand_skipgram_size", "+", "1", ")", ":", "\n", "                    ", "tail_st", ",", "tail_end", "=", "span_list", "[", "i_span", "+", "n", "-", "1", "]", "\n", "if", "(", "tail_end", "-", "tail_st", "==", "1", ")", "and", "(", "tail_st", "in", "special_pos", ")", ":", "\n", "                        ", "break", "\n", "", "if", "len", "(", "masked_pos", ")", "+", "tail_end", "-", "cand_st", ">", "n_pred", ":", "\n", "                        ", "break", "\n", "", "n_span", "=", "n", "\n", "", "st_span", ",", "end_span", "=", "i_span", ",", "i_span", "+", "n_span", "\n", "\n", "", "if", "self", ".", "mask_whole_word", ":", "\n", "# pre_whole_word==False: position index of span_list is the same as tokens", "\n", "                ", "st_span", ",", "end_span", "=", "_expand_whole_word", "(", "\n", "tokens", ",", "st_span", ",", "end_span", ")", "\n", "\n", "# subsampling according to frequency", "\n", "", "if", "self", ".", "word_subsample_prb", ":", "\n", "                ", "skip_pos", "=", "set", "(", ")", "\n", "if", "self", ".", "pre_whole_word", ":", "\n", "                    ", "w_span_list", "=", "span_list", "[", "st_span", ":", "end_span", "]", "\n", "", "else", ":", "\n", "                    ", "split_idx", "=", "_get_word_split_index", "(", "\n", "tokens", ",", "st_span", ",", "end_span", ")", "\n", "w_span_list", "=", "list", "(", "\n", "zip", "(", "split_idx", "[", ":", "-", "1", "]", ",", "split_idx", "[", "1", ":", "]", ")", ")", "\n", "", "for", "i", ",", "sp", "in", "enumerate", "(", "w_span_list", ")", ":", "\n", "                    ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "sp_end", "-", "sp_st", "==", "1", ":", "\n", "                        ", "w_cat", "=", "tokens", "[", "sp_st", "]", "\n", "", "else", ":", "\n", "                        ", "w_cat", "=", "''", ".", "join", "(", "tokens", "[", "sp_st", ":", "sp_end", "]", ")", "\n", "", "if", "(", "w_cat", "in", "self", ".", "word_subsample_prb", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "word_subsample_prb", "[", "w_cat", "]", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "sp_st", ",", "sp_end", ")", ":", "\n", "                            ", "skip_pos", ".", "add", "(", "k", ")", "\n", "", "", "", "", "else", ":", "\n", "                ", "skip_pos", "=", "None", "\n", "\n", "", "for", "sp", "in", "range", "(", "st_span", ",", "end_span", ")", ":", "\n", "                ", "for", "mp", "in", "range", "(", "span_list", "[", "sp", "]", "[", "0", "]", ",", "span_list", "[", "sp", "]", "[", "1", "]", ")", ":", "\n", "                    ", "if", "not", "(", "skip_pos", "and", "(", "mp", "in", "skip_pos", ")", ")", "and", "(", "mp", "not", "in", "special_pos", ")", "and", "not", "(", "protect_range", "and", "(", "protect_range", "[", "0", "]", "<=", "mp", "<", "protect_range", "[", "1", "]", ")", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "\n", "", "", "", "", "if", "len", "(", "masked_pos", ")", "<", "n_pred", ":", "\n", "            ", "shuffle", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "not", "in", "masked_pos", ":", "\n", "                    ", "masked_pos", ".", "add", "(", "pos", ")", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "# shuffle(masked_pos)", "\n", "            ", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "", "return", "masked_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.Pipeline.replace_masked_tokens": [[286, 300], ["sorted", "list", "random.random.random", "loader_utils.get_random_word"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.get_random_word"], ["", "def", "replace_masked_tokens", "(", "self", ",", "tokens", ",", "masked_pos", ")", ":", "\n", "        ", "if", "self", ".", "span_same_mask", ":", "\n", "            ", "masked_pos", "=", "sorted", "(", "list", "(", "masked_pos", ")", ")", "\n", "", "prev_pos", ",", "prev_rand", "=", "None", ",", "None", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "            ", "if", "self", ".", "span_same_mask", "and", "(", "pos", "-", "1", "==", "prev_pos", ")", ":", "\n", "                ", "t_rand", "=", "prev_rand", "\n", "", "else", ":", "\n", "                ", "t_rand", "=", "rand", "(", ")", "\n", "", "if", "t_rand", "<", "0.8", ":", "# 80%", "\n", "                ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "t_rand", "<", "0.9", ":", "# 10%", "\n", "                ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "", "prev_pos", ",", "prev_rand", "=", "pos", ",", "t_rand", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.get_random_word": [[9, 12], ["random.randint", "len"], "function", ["None"], ["def", "get_random_word", "(", "vocab_words", ")", ":", "\n", "    ", "i", "=", "randint", "(", "0", ",", "len", "(", "vocab_words", ")", "-", "1", ")", "\n", "return", "vocab_words", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors": [[14, 24], ["zip", "batch_tensors.append", "isinstance", "batch_tensors.append", "batch_tensors.append", "torch.stack", "torch.stack", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "batch_list_to_batch_tensors", "(", "batch", ")", ":", "\n", "    ", "batch_tensors", "=", "[", "]", "\n", "for", "x", "in", "zip", "(", "*", "batch", ")", ":", "\n", "        ", "if", "x", "[", "0", "]", "is", "None", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "None", ")", "\n", "", "elif", "isinstance", "(", "x", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "torch", ".", "stack", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "torch", ".", "tensor", "(", "x", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "", "return", "batch_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._get_word_split_index": [[71, 80], ["split_idx.append", "split_idx.append", "tokens[].startswith"], "function", ["None"], ["", "", "def", "_get_word_split_index", "(", "tokens", ",", "st", ",", "end", ")", ":", "\n", "    ", "split_idx", "=", "[", "]", "\n", "i", "=", "st", "\n", "while", "i", "<", "end", ":", "\n", "        ", "if", "(", "not", "tokens", "[", "i", "]", ".", "startswith", "(", "'##'", ")", ")", "or", "(", "i", "==", "st", ")", ":", "\n", "            ", "split_idx", ".", "append", "(", "i", ")", "\n", "", "i", "+=", "1", "\n", "", "split_idx", ".", "append", "(", "end", ")", "\n", "return", "split_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._expand_whole_word": [[82, 89], ["tokens[].startswith", "tokens[].startswith", "len"], "function", ["None"], ["", "def", "_expand_whole_word", "(", "tokens", ",", "st", ",", "end", ")", ":", "\n", "    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.C_Seq2SeqDataset.__init__": [[61, 99], ["super().__init__", "print", "open", "open", "open", "zip", "len", "src.split.split.split", "tgt.split.split.split", "check.split.split.split", "range", "seq2seq_loader.C_Seq2SeqDataset.ex_list.append", "tokenizer.tokenize", "tokenizer.tokenize", "tokenizer.tokenize", "src_tk_list.append", "tgt_tk_list.append", "check_tk_list.append", "src[].strip", "tgt[].strip", "check[].strip", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["def", "__init__", "(", "self", ",", "file_src", ",", "file_tgt", ",", "file_check", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "\n", "sent_reverse_order", "=", "False", ",", "bi_uni_pipeline", "=", "[", "]", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_uni_pipeline", "=", "bi_uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "\t\t\t", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ",", "open", "(", "file_check", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_check", ":", "\n", "\n", "\t\t\t\t", "for", "src", ",", "tgt", ",", "check", "in", "zip", "(", "f_src", ",", "f_tgt", ",", "f_check", ")", ":", "\n", "\t\t\t\t\t", "src", "=", "src", ".", "split", "(", "\"[SEP]\"", ")", "\n", "tgt", "=", "tgt", ".", "split", "(", "\"[SEP]\"", ")", "\n", "\n", "check", "=", "check", ".", "split", "(", "\"[SEP]\"", ")", "\n", "\n", "src_tk_list", "=", "[", "]", "\n", "tgt_tk_list", "=", "[", "]", "\n", "check_tk_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "TopK", ")", ":", "\n", "\t\t\t\t\t\t", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", "[", "i", "]", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", "[", "i", "]", ".", "strip", "(", ")", ")", "\n", "check_tk", "=", "tokenizer", ".", "tokenize", "(", "check", "[", "i", "]", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "assert", "len", "(", "check_tk", ")", ">", "0", "\n", "\n", "src_tk_list", ".", "append", "(", "src_tk", ")", "\n", "tgt_tk_list", ".", "append", "(", "tgt_tk", ")", "\n", "check_tk_list", ".", "append", "(", "check_tk", ")", "\n", "\n", "", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk_list", ",", "tgt_tk_list", ",", "check_tk_list", ")", ")", "\n", "", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.C_Seq2SeqDataset.__len__": [[100, 102], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "\t\t", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.C_Seq2SeqDataset.__getitem__": [[103, 108], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\t\t", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "bi_uni_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.C_Seq2SeqDataset.__iter__": [[109, 117], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "loader_utils.batch_list_to_batch_tensors", "len", "float", "seq2seq_loader.C_Seq2SeqDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "\t\t", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "\t\t\t", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "\t\t\t\t", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.C_Preprocess4Seq2seq.__init__": [[122, 155], ["loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "\n", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "\n", "mask_source_words", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "\n", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ",", "train_avg_bpe_length", "=", "25", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "self", ".", "train_avg_bpe_length", "=", "train_avg_bpe_length", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.C_Preprocess4Seq2seq.__call__": [[156, 381], ["range", "torch.stack", "torch.stack", "torch.stack", "seq2seq_loader.C_Preprocess4Seq2seq.indexer", "seq2seq_loader.C_Preprocess4Seq2seq.extend", "torch.tensor", "torch.tensor", "seq2seq_loader.truncate_tokens_pair", "seq2seq_loader.C_Preprocess4Seq2seq.indexer", "seq2seq_loader.C_Preprocess4Seq2seq.extend", "segment_ids.extend", "torch.zeros", "enumerate", "tgt_pos.extend", "input_ids_list.append", "segment_ids_list.append", "torch.stack.append", "masked_ids_list.append", "masked_pos_list.append", "masked_weights_list.append", "tgt_pos_list.append", "torch.stack.append", "torch.stack.append", "check_ids_list.append", "len", "len", "float", "int", "min", "seq2seq_loader.C_Preprocess4Seq2seq.indexer", "len", "min", "random.random.shuffle", "set", "max", "list", "seq2seq_loader.C_Preprocess4Seq2seq.indexer", "len", "mask_qkv.extend", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].copy_", "len", "len", "len", "len", "max", "len", "set", "enumerate", "set", "enumerate", "range", "len", "random.random.shuffle", "len", "seq2seq_loader.C_Preprocess4Seq2seq.extend", "list.extend", "masked_weights.extend", "tgt_pos.append", "len", "len", "range", "int", "len", "random.random.randint", "random.random.random", "len", "len", "len", "len", "len", "round", "cand_pos.append", "cand_pos.append", "tokens[].startswith", "tokens[].startswith", "random.random.random", "seq2seq_loader.C_Preprocess4Seq2seq.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._expand_whole_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "\t\t", "input_ids_list", "=", "[", "]", "\n", "segment_ids_list", "=", "[", "]", "\n", "input_mask_list", "=", "[", "]", "\n", "masked_ids_list", "=", "[", "]", "\n", "masked_pos_list", "=", "[", "]", "\n", "masked_weights_list", "=", "[", "]", "\n", "tgt_pos_list", "=", "[", "]", "\n", "labels_list", "=", "[", "]", "\n", "ks_labels_list", "=", "[", "]", "\n", "check_ids_list", "=", "[", "]", "\n", "\n", "tokens_a_list", ",", "tokens_b_list", ",", "check_list", "=", "instance", "[", ":", "3", "]", "\n", "\n", "for", "rank", "in", "range", "(", "TopK", ")", ":", "\n", "\n", "\t\t\t", "tokens_a", "=", "tokens_a_list", "[", "rank", "]", "\n", "tokens_b", "=", "tokens_b_list", "[", "rank", "]", "\n", "check_tokens", "=", "check_list", "[", "rank", "]", "[", ":", "self", ".", "max_pred", "]", "\n", "\n", "#######", "\n", "check_ids", "=", "self", ".", "indexer", "(", "check_tokens", ")", "\n", "# Zero Padding", "\n", "check_n_pad", "=", "self", ".", "max_pred", "-", "len", "(", "check_ids", ")", "\n", "check_ids", ".", "extend", "(", "[", "0", "]", "*", "check_n_pad", ")", "\n", "assert", "len", "(", "check_ids", ")", "==", "self", ".", "max_pred", "\n", "########", "\n", "\n", "tokens_a", "=", "[", "\".\"", "]", "+", "tokens_a", "[", ":", "-", "1", "]", "\n", "\n", "labels", "=", "torch", ".", "tensor", "(", "float", "(", "tokens_b", "[", "-", "2", "]", ")", ")", "\n", "ks_labels", "=", "torch", ".", "tensor", "(", "int", "(", "tokens_b", "[", "-", "1", "]", ")", ")", "\n", "tokens_b", "=", "tokens_b", "[", ":", "-", "2", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t\t", "tokens_b", "=", "[", "'[S2S_SOS]'", "]", "+", "tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "", "num_truncated_a", ",", "_", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "\n", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "\t\t\t\t", "tokens", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "", "else", ":", "\n", "\t\t\t\t", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "\t\t\t\t", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t\t", "if", "self", ".", "s2s_add_segment", ":", "\n", "\t\t\t\t\t\t", "if", "self", ".", "s2s_share_segment", ":", "\n", "\t\t\t\t\t\t\t", "segment_ids", "=", "[", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t\t", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", ")", "\n", "masked_pos", "=", "[", "len", "(", "tokens_a", ")", "+", "2", "+", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", ")", "]", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "[", "1", ":", "]", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "\t\t\t\t", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "if", "self", ".", "mask_source_words", ":", "\n", "\t\t\t\t\t", "effective_length", "+=", "len", "(", "tokens_a", ")", "\n", "", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "\n", "if", "len", "(", "tokens_b", ")", ">=", "self", ".", "train_avg_bpe_length", ":", "\n", "# candidate positions of masked tokens", "\n", "\t\t\t\t\t", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol", "\n", "\t\t\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "\t\t\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "\n", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "\t\t\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "", "else", ":", "\n", "# candidate positions of masked tokens", "\n", "\t\t\t\t\t", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we don't mask [SEP] as an ending symbol", "\n", "\t\t\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "tk", "!=", "'[SEP]'", ")", ":", "\n", "\t\t\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "\n", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "\t\t\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "\t\t\t\t\t", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "\t\t\t\t\t\t", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t\t\t", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "\t\t\t\t\t\t", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "\t\t\t\t\t\t\t", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "\t\t\t\t\t\t\t", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "\t\t\t\t\t\t", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "\t\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "\t\t\t\t\t\t", "if", "self", ".", "mask_whole_word", ":", "\n", "\t\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "\t\t\t\t\t\t", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "\t\t\t\t\t\t\t", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "\t\t\t\t\t", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t\t", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "\t\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "\t\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "\t\t\t\t", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "second_st", ",", "second_end", "=", "len", "(", "\n", "tokens_a", ")", "+", "2", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "\t\t\t\t", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "tgt_pos", "=", "[", "]", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", "and", "tk", "!=", "'[SEP]'", ")", ":", "\n", "\t\t\t\t\t", "tgt_pos", ".", "append", "(", "i", ")", "\n", "\n", "", "", "tgt_pos", "=", "tgt_pos", "[", ":", "len", "(", "masked_pos", ")", "]", "\n", "tgt_pad", "=", "len", "(", "masked_pos", ")", "-", "len", "(", "tgt_pos", ")", "\n", "tgt_pos", ".", "extend", "(", "[", "0", "]", "*", "tgt_pad", ")", "\n", "\n", "input_ids_list", ".", "append", "(", "input_ids", ")", "\n", "segment_ids_list", ".", "append", "(", "segment_ids", ")", "\n", "input_mask_list", ".", "append", "(", "input_mask", ")", "\n", "masked_ids_list", ".", "append", "(", "masked_ids", ")", "\n", "masked_pos_list", ".", "append", "(", "masked_pos", ")", "\n", "masked_weights_list", ".", "append", "(", "masked_weights", ")", "\n", "tgt_pos_list", ".", "append", "(", "tgt_pos", ")", "\n", "labels_list", ".", "append", "(", "labels", ")", "\n", "ks_labels_list", ".", "append", "(", "ks_labels", ")", "\n", "check_ids_list", ".", "append", "(", "check_ids", ")", "\n", "\n", "\n", "", "input_mask_list", "=", "torch", ".", "stack", "(", "input_mask_list", ")", "\n", "labels_list", "=", "torch", ".", "stack", "(", "labels_list", ")", "\n", "ks_labels_list", "=", "torch", ".", "stack", "(", "ks_labels_list", ")", "\n", "\n", "return", "(", "input_ids_list", ",", "segment_ids_list", ",", "input_mask_list", ",", "mask_qkv", ",", "masked_ids_list", ",", "masked_pos_list", ",", "masked_weights_list", ",", "-", "1", ",", "self", ".", "task_idx", ",", "\n", "tgt_pos_list", ",", "labels_list", ",", "ks_labels_list", ",", "check_ids_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Seq2SeqDataset.__init__": [[386, 418], ["super().__init__", "print", "open", "open", "zip", "open", "open", "open", "zip", "len", "tokenizer.tokenize", "tokenizer.tokenize", "seq2seq_loader.Seq2SeqDataset.ex_list.append", "tokenizer.tokenize", "tokenizer.tokenize", "orc.split", "seq2seq_loader.Seq2SeqDataset.ex_list.append", "src.strip", "tgt.strip", "len", "len", "src.strip", "tgt.strip", "int", "int", "s_st.split", "labl.split"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["def", "__init__", "(", "self", ",", "file_src", ",", "file_tgt", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "\n", "sent_reverse_order", "=", "False", ",", "bi_uni_pipeline", "=", "[", "]", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_uni_pipeline", "=", "bi_uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "\t\t\t", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "\t\t\t\t", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "\t\t\t\t\t", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "", "", "", "else", ":", "\n", "\t\t\t", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ",", "open", "(", "file_oracle", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_orc", ":", "\n", "\t\t\t\t", "for", "src", ",", "tgt", ",", "orc", "in", "zip", "(", "f_src", ",", "f_tgt", ",", "f_orc", ")", ":", "\n", "\t\t\t\t\t", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "s_st", ",", "labl", "=", "orc", ".", "split", "(", "'\\t'", ")", "\n", "s_st", "=", "[", "int", "(", "x", ")", "for", "x", "in", "s_st", ".", "split", "(", ")", "]", "\n", "labl", "=", "[", "int", "(", "x", ")", "for", "x", "in", "labl", ".", "split", "(", ")", "]", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "s_st", ",", "labl", ")", ")", "\n", "", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Seq2SeqDataset.__len__": [[419, 421], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "\t\t", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Seq2SeqDataset.__getitem__": [[422, 427], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\t\t", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "bi_uni_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Seq2SeqDataset.__iter__": [[428, 436], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "loader_utils.batch_list_to_batch_tensors", "len", "float", "seq2seq_loader.Seq2SeqDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "\t\t", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "\t\t\t", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "\t\t\t\t", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Preprocess4Seq2seq.__init__": [[441, 473], ["loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "\n", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "\n", "mask_source_words", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "\n", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Preprocess4Seq2seq.__call__": [[474, 643], ["torch.tensor", "torch.tensor", "seq2seq_loader.truncate_tokens_pair", "seq2seq_loader.Preprocess4Seq2seq.indexer", "seq2seq_loader.Preprocess4Seq2seq.extend", "segment_ids.extend", "torch.zeros", "enumerate", "tgt_pos.extend", "float", "int", "min", "seq2seq_loader.Preprocess4Seq2seq.indexer", "len", "min", "set", "enumerate", "random.random.shuffle", "set", "max", "list", "seq2seq_loader.Preprocess4Seq2seq.indexer", "len", "mask_qkv.extend", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].copy_", "len", "len", "len", "len", "max", "range", "len", "random.random.shuffle", "len", "seq2seq_loader.Preprocess4Seq2seq.extend", "list.extend", "masked_weights.extend", "tgt_pos.append", "len", "len", "range", "int", "cand_pos.append", "len", "random.random.randint", "random.random.random", "len", "len", "len", "len", "len", "round", "cand_pos.append", "set.add", "tokens[].startswith", "tokens[].startswith", "random.random.random", "seq2seq_loader.Preprocess4Seq2seq.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._expand_whole_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "\t\t", "tokens_a", ",", "tokens_b", "=", "instance", "[", ":", "2", "]", "\n", "tokens_a", "=", "[", "\".\"", "]", "+", "tokens_a", "[", ":", "-", "1", "]", "\n", "\n", "labels", "=", "torch", ".", "tensor", "(", "float", "(", "tokens_b", "[", "-", "2", "]", ")", ")", "\n", "ks_labels", "=", "torch", ".", "tensor", "(", "int", "(", "tokens_b", "[", "-", "1", "]", ")", ")", "\n", "tokens_b", "=", "tokens_b", "[", ":", "-", "2", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t", "tokens_b", "=", "[", "'[S2S_SOS]'", "]", "+", "tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "", "num_truncated_a", ",", "_", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "\n", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "\t\t\t", "tokens", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "", "else", ":", "\n", "\t\t\t", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "\t\t\t", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t", "if", "self", ".", "s2s_add_segment", ":", "\n", "\t\t\t\t\t", "if", "self", ".", "s2s_share_segment", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "\t\t\t", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", ")", "\n", "masked_pos", "=", "[", "len", "(", "tokens_a", ")", "+", "2", "+", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", ")", "]", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "[", "1", ":", "]", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "\t\t\t", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "if", "self", ".", "mask_source_words", ":", "\n", "\t\t\t\t", "effective_length", "+=", "len", "(", "tokens_a", ")", "\n", "", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol", "\n", "\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "\n", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "\t\t\t\t", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "\t\t\t\t\t", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t\t", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "\t\t\t\t\t", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "\t\t\t\t\t\t", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "\t\t\t\t\t\t", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "\t\t\t\t\t", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "\t\t\t\t\t", "if", "self", ".", "mask_whole_word", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "\t\t\t\t\t", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "\t\t\t\t\t\t", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "\t\t\t\t", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "\t\t\t", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "\t\t\t", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "second_st", ",", "second_end", "=", "len", "(", "\n", "tokens_a", ")", "+", "2", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "\t\t\t", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "\t\t\t\t", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "\t\t\t\t", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "\t\t\t\t", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "tgt_pos", "=", "[", "]", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", "and", "tk", "!=", "'[SEP]'", ")", ":", "\n", "\t\t\t\t", "tgt_pos", ".", "append", "(", "i", ")", "\n", "\n", "", "", "tgt_pos", "=", "tgt_pos", "[", ":", "len", "(", "masked_pos", ")", "]", "\n", "tgt_pad", "=", "len", "(", "masked_pos", ")", "-", "len", "(", "tgt_pos", ")", "\n", "tgt_pos", ".", "extend", "(", "[", "0", "]", "*", "tgt_pad", ")", "\n", "\n", "\n", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ",", "\n", "tgt_pos", ",", "labels", ",", "ks_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__": [[648, 667], ["loader_utils.Pipeline.__init__", "torch.tril", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "max_tgt_length", "=", "128", ",", "new_segment_ids", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "\n", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "max_tgt_length", "=", "max_tgt_length", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Preprocess4Seq2seqDecoder.__call__": [[668, 738], ["min", "range", "range", "range", "seq2seq_loader.Preprocess4Seq2seqDecoder.indexer", "torch.zeros", "input_mask[].copy_", "len", "len", "len", "position_ids.append", "position_ids.append", "position_ids.append", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].fill_", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "\t\t", "tokens_a", ",", "max_a_len", "=", "instance", "\n", "tokens_a", "=", "[", "\".\"", "]", "+", "tokens_a", "[", ":", "-", "1", "]", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "\t\t\t", "padded_tokens_a", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "\n", "", "else", ":", "\n", "\t\t\t", "padded_tokens_a", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "\n", "", "assert", "len", "(", "padded_tokens_a", ")", "<=", "max_a_len", "+", "2", "\n", "if", "max_a_len", "+", "2", ">", "len", "(", "padded_tokens_a", ")", ":", "\n", "\t\t\t", "padded_tokens_a", "+=", "[", "'[PAD]'", "]", "*", "(", "max_a_len", "+", "2", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "assert", "len", "(", "padded_tokens_a", ")", "==", "max_a_len", "+", "2", "\n", "max_len_in_batch", "=", "min", "(", "self", ".", "max_tgt_length", "+", "\n", "max_a_len", "+", "2", ",", "self", ".", "max_len", ")", "\n", "tokens", "=", "padded_tokens_a", "\n", "if", "self", ".", "new_segment_ids", ":", "\n", "\t\t\t", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t", "_enc_seg1", "=", "0", "if", "self", ".", "s2s_share_segment", "else", "4", "\n", "if", "self", ".", "s2s_add_segment", ":", "\n", "\t\t\t\t\t", "if", "self", ".", "s2s_share_segment", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "\n", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "padded_tokens_a", ")", "-", "1", ")", "+", "[", "5", "]", "*", "(", "\n", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "\n", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "padded_tokens_a", ")", "-", "1", ")", "+", "[", "5", "]", "*", "(", "\n", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "segment_ids", "=", "[", "2", "]", "*", "max_len_in_batch", "\n", "", "", "else", ":", "\n", "\t\t\t", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "1", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "\n", "", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "\t\t\t", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "1", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t", "mask_qkv", "=", "None", "\n", "\n", "", "position_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens_a", ")", "+", "2", ")", ":", "\n", "\t\t\t", "position_ids", ".", "append", "(", "i", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "tokens_a", ")", "+", "2", ",", "max_a_len", "+", "2", ")", ":", "\n", "\t\t\t", "position_ids", ".", "append", "(", "0", ")", "\n", "", "for", "i", "in", "range", "(", "max_a_len", "+", "2", ",", "max_len_in_batch", ")", ":", "\n", "\t\t\t", "position_ids", ".", "append", "(", "i", "-", "(", "max_a_len", "+", "2", ")", "+", "len", "(", "tokens_a", ")", "+", "2", ")", "\n", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "# Zero Padding", "\n", "input_mask", "=", "torch", ".", "zeros", "(", "\n", "max_len_in_batch", ",", "max_len_in_batch", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "2", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "input_mask", "[", "end", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "", "second_st", ",", "second_end", "=", "len", "(", "padded_tokens_a", ")", ",", "max_len_in_batch", "\n", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "\n", "return", "(", "input_ids", ",", "segment_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "self", ".", "task_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Preprocess4Seq2seq_predict.__init__": [[743, 775], ["loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "\n", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "\n", "mask_source_words", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "\n", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.Preprocess4Seq2seq_predict.__call__": [[776, 930], ["seq2seq_loader.truncate_tokens_pair", "seq2seq_loader.Preprocess4Seq2seq_predict.indexer", "seq2seq_loader.Preprocess4Seq2seq_predict.extend", "segment_ids.extend", "torch.zeros", "min", "seq2seq_loader.Preprocess4Seq2seq_predict.indexer", "len", "min", "set", "enumerate", "random.random.shuffle", "set", "max", "list", "seq2seq_loader.Preprocess4Seq2seq_predict.indexer", "len", "mask_qkv.extend", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].copy_", "len", "len", "max", "range", "len", "random.random.shuffle", "len", "seq2seq_loader.Preprocess4Seq2seq_predict.extend", "list.extend", "masked_weights.extend", "len", "range", "int", "cand_pos.append", "len", "random.random.randint", "random.random.random", "len", "len", "len", "len", "len", "round", "cand_pos.append", "set.add", "tokens[].startswith", "tokens[].startswith", "random.random.random", "seq2seq_loader.Preprocess4Seq2seq_predict.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils._expand_whole_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "\t\t", "tokens_a", "=", "instance", "[", "0", "]", "\n", "tokens_a", "=", "[", "\".\"", "]", "+", "tokens_a", "[", ":", "-", "1", "]", "\n", "tokens_b", "=", "[", "\".\"", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t", "tokens_b", "=", "[", "'[S2S_SOS]'", "]", "+", "tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "", "num_truncated_a", ",", "_", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "\n", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "\t\t\t", "tokens", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "", "else", ":", "\n", "\t\t\t", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "\t\t\t", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t", "if", "self", ".", "s2s_add_segment", ":", "\n", "\t\t\t\t\t", "if", "self", ".", "s2s_share_segment", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "\t\t\t", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", ")", "\n", "masked_pos", "=", "[", "len", "(", "tokens_a", ")", "+", "2", "+", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", ")", "]", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "[", "1", ":", "]", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "\t\t\t", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "if", "self", ".", "mask_source_words", ":", "\n", "\t\t\t\t", "effective_length", "+=", "len", "(", "tokens_a", ")", "\n", "", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol", "\n", "\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "\n", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "\t\t\t\t", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "\t\t\t\t\t", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t\t", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "\t\t\t\t\t", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "\t\t\t\t\t\t", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "\t\t\t\t\t\t", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "\t\t\t\t\t", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "\t\t\t\t\t", "if", "self", ".", "mask_whole_word", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "\t\t\t\t\t", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "\t\t\t\t\t\t", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "\t\t\t\t", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "\t\t\t", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "\t\t\t", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "second_st", ",", "second_end", "=", "len", "(", "\n", "tokens_a", ")", "+", "2", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "\t\t\t", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "\t\t\t\t", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "\t\t\t\t", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "\t\t\t\t", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.seq2seq_loader.truncate_tokens_pair": [[19, 56], ["trunc_tokens.pop", "len", "len", "len", "random.random", "len", "len", "len"], "function", ["None"], ["def", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_len", ",", "max_len_a", "=", "0", ",", "max_len_b", "=", "0", ",", "trunc_seg", "=", "None", ",", "\n", "always_truncate_tail", "=", "False", ")", ":", "\n", "\t", "num_truncated_a", "=", "[", "0", ",", "0", "]", "\n", "num_truncated_b", "=", "[", "0", ",", "0", "]", "\n", "while", "True", ":", "\n", "\t\t", "if", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "<=", "max_len", ":", "\n", "\t\t\t", "break", "\n", "", "if", "(", "max_len_a", ">", "0", ")", "and", "len", "(", "tokens_a", ")", ">", "max_len_a", ":", "\n", "\t\t\t", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "elif", "(", "max_len_b", ">", "0", ")", "and", "len", "(", "tokens_b", ")", ">", "max_len_b", ":", "\n", "\t\t\t", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "elif", "trunc_seg", ":", "\n", "# truncate the specified segment", "\n", "\t\t\t", "if", "trunc_seg", "==", "'a'", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "", "else", ":", "\n", "# truncate the longer segment", "\n", "\t\t\t", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "# whether always truncate source sequences", "\n", "", "", "if", "(", "not", "always_truncate_tail", ")", "and", "(", "rand", "(", ")", "<", "0.5", ")", ":", "\n", "\t\t\t", "del", "trunc_tokens", "[", "0", "]", "\n", "num_truncated", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "\t\t\t", "trunc_tokens", ".", "pop", "(", ")", "\n", "num_truncated", "[", "1", "]", "+=", "1", "\n", "", "", "return", "num_truncated_a", ",", "num_truncated_b", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.decode.detokenize": [[35, 43], ["tk.startswith", "r_list.append", "len"], "function", ["None"], ["def", "detokenize", "(", "tk_list", ")", ":", "\n", "    ", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "        ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "            ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.decode.ascii_print": [[45, 48], ["text.encode.encode", "print"], "function", ["None"], ["", "def", "ascii_print", "(", "text", ")", ":", "\n", "    ", "text", "=", "text", ".", "encode", "(", "\"ascii\"", ",", "\"ignore\"", ")", "\n", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.decode.main": [[50, 282], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.device_count", "random.seed", "numpy.random.seed", "torch.manual_seed", "pytorch_bert.tokenization.BertTokenizer.from_pretrained", "bi_uni_pipeline.append", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "decode.main._get_token_id_set"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "main", "(", ")", ":", "\n", "\t", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_vae\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to train vae.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--bleu'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "\"Set Bleu \"", ")", "\n", "\n", "# decoding parameters", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_file\"", ",", "type", "=", "str", ",", "help", "=", "\"Input file\"", ")", "\n", "parser", ".", "add_argument", "(", "'--subset'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Decode a subset of the input dataset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "type", "=", "str", ",", "help", "=", "\"output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--split\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Data split (train/val/test).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "123", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Batch size for decoding.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--beam_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size for searching\"", ")", "\n", "parser", ".", "add_argument", "(", "'--length_penalty'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Length penalty for beam search\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--forbid_duplicate_ngrams'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--forbid_ignore_word'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Ignore the word during forbid_duplicate_ngrams\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_len\"", ",", "default", "=", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--need_score_traces'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--ngram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "default", "=", "\"s2s\"", ",", "\n", "choices", "=", "[", "\"s2s\"", ",", "\"l2r\"", ",", "\"both\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--max_tgt_length'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"maximum length of target sequence\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--not_predict_token'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Do not predict the tokens during decoding.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "need_score_traces", "and", "args", ".", "beam_size", "<=", "1", ":", "\n", "\t\t", "raise", "ValueError", "(", "\n", "\"Score trace is only available for beam search with beam size > 1.\"", ")", "\n", "", "if", "args", ".", "max_tgt_length", ">=", "args", ".", "max_seq_length", "-", "2", ":", "\n", "\t\t", "raise", "ValueError", "(", "\"Maximum tgt length exceeds max seq length - 2.\"", ")", "\n", "\n", "", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "\t\t", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "tokenizer", ".", "max_len", "=", "args", ".", "max_seq_length", "\n", "\n", "pair_num_relation", "=", "0", "\n", "bi_uni_pipeline", "=", "[", "]", "\n", "bi_uni_pipeline", ".", "append", "(", "\n", "seq2seq_loader", ".", "Preprocess4Seq2seqDecoder", "(", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "\n", "args", ".", "max_seq_length", ",", "max_tgt_length", "=", "args", ".", "max_tgt_length", ",", "\n", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "mode", "=", "\"s2s\"", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", ")", "\n", "\n", "amp_handle", "=", "None", "\n", "if", "args", ".", "fp16", "and", "args", ".", "amp", ":", "\n", "\t\t", "from", "apex", "import", "amp", "\n", "amp_handle", "=", "amp", ".", "init", "(", "enable_caching", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "\n", "# Prepare model", "\n", "", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "mask_word_id", ",", "eos_word_ids", ",", "sos_word_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "\n", "[", "\"[MASK]\"", ",", "\"[SEP]\"", ",", "\"[S2S_SOS]\"", "]", ")", "\n", "\n", "def", "_get_token_id_set", "(", "s", ")", ":", "\n", "\t\t", "r", "=", "None", "\n", "if", "s", ":", "\n", "\t\t\t", "w_list", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", "'|'", ")", ":", "\n", "\t\t\t\t", "if", "w", ".", "startswith", "(", "'['", ")", "and", "w", ".", "endswith", "(", "']'", ")", ":", "\n", "\t\t\t\t\t", "w_list", ".", "append", "(", "w", ".", "upper", "(", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "w_list", ".", "append", "(", "w", ")", "\n", "", "", "r", "=", "set", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "w_list", ")", ")", "\n", "", "return", "r", "\n", "\n", "", "forbid_ignore_set", "=", "_get_token_id_set", "(", "args", ".", "forbid_ignore_word", ")", "\n", "not_predict_set", "=", "_get_token_id_set", "(", "args", ".", "not_predict_token", ")", "\n", "logger", ".", "info", "(", "\"** ** * Continue decoding ** ** *\"", ")", "\n", "for", "model_recover_path", "in", "glob", ".", "glob", "(", "args", ".", "model_recover_path", ".", "strip", "(", ")", ")", ":", "\n", "\t\t", "logger", ".", "info", "(", "\"** ** * Recover model: %s ** ** *\"", ",", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "model_recover_path", ")", "\n", "model", "=", "BertForSeq2SeqDecoder", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "\n", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "pair_num_relation", ",", "\n", "type_vocab_size", "=", "type_vocab_size", ",", "task_idx", "=", "3", ",", "\n", "mask_word_id", "=", "mask_word_id", ",", "search_beam_size", "=", "args", ".", "beam_size", ",", "\n", "length_penalty", "=", "args", ".", "length_penalty", ",", "eos_id", "=", "eos_word_ids", ",", "\n", "sos_id", "=", "sos_word_id", ",", "\n", "forbid_duplicate_ngrams", "=", "args", ".", "forbid_duplicate_ngrams", ",", "\n", "forbid_ignore_set", "=", "forbid_ignore_set", ",", "\n", "not_predict_set", "=", "not_predict_set", ",", "ngram_size", "=", "args", ".", "ngram_size", ",", "\n", "min_len", "=", "args", ".", "min_len", ",", "mode", "=", "args", ".", "mode", ",", "\n", "max_position_embeddings", "=", "args", ".", "max_seq_length", ",", "\n", "ffn_type", "=", "args", ".", "ffn_type", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "seg_emb", "=", "args", ".", "seg_emb", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "del", "model_recover", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "\t\t\t", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "\t\t\t", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "next_i", "=", "0", "\n", "max_src_length", "=", "args", ".", "max_seq_length", "-", "2", "-", "args", ".", "max_tgt_length", "\n", "\n", "with", "open", "(", "args", ".", "input_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fin", ":", "\n", "\t\t\t", "input_lines", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "fin", ".", "readlines", "(", ")", "]", "\n", "if", "args", ".", "subset", ">", "0", ":", "\n", "\t\t\t\t", "logger", ".", "info", "(", "\"Decoding subset: %d\"", ",", "args", ".", "subset", ")", "\n", "input_lines", "=", "input_lines", "[", ":", "args", ".", "subset", "]", "\n", "", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "input_lines", "=", "[", "data_tokenizer", ".", "tokenize", "(", "\n", "x", ")", "[", ":", "max_src_length", "]", "for", "x", "in", "input_lines", "]", "\n", "input_lines", "=", "sorted", "(", "list", "(", "enumerate", "(", "input_lines", ")", ")", ",", "\n", "key", "=", "lambda", "x", ":", "-", "len", "(", "x", "[", "1", "]", ")", ")", "\n", "output_lines", "=", "[", "\"\"", "]", "*", "len", "(", "input_lines", ")", "\n", "score_trace_list", "=", "[", "None", "]", "*", "len", "(", "input_lines", ")", "\n", "total_batch", "=", "math", ".", "ceil", "(", "len", "(", "input_lines", ")", "/", "args", ".", "batch_size", ")", "\n", "\n", "with", "tqdm", "(", "total", "=", "total_batch", ")", "as", "pbar", ":", "\n", "\t\t\t", "while", "next_i", "<", "len", "(", "input_lines", ")", ":", "\n", "\t\t\t\t", "_chunk", "=", "input_lines", "[", "next_i", ":", "next_i", "+", "args", ".", "batch_size", "]", "\n", "buf_id", "=", "[", "x", "[", "0", "]", "for", "x", "in", "_chunk", "]", "\n", "buf", "=", "[", "x", "[", "1", "]", "for", "x", "in", "_chunk", "]", "\n", "next_i", "+=", "args", ".", "batch_size", "\n", "max_a_len", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "buf", "]", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "[", "(", "x", ",", "max_a_len", ")", "for", "x", "in", "buf", "]", ":", "\n", "\t\t\t\t\t", "for", "proc", "in", "bi_uni_pipeline", ":", "\n", "\t\t\t\t\t\t", "instances", ".", "append", "(", "proc", "(", "instance", ")", ")", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t\t\t\t", "batch", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "\n", "instances", ")", "\n", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "task_idx", "=", "batch", "\n", "traces", "=", "model", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "input_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ",", "bleu", "=", "args", ".", "bleu", ")", "\n", "if", "args", ".", "beam_size", ">", "1", ":", "\n", "\t\t\t\t\t\t", "traces", "=", "{", "k", ":", "v", ".", "tolist", "(", ")", "for", "k", ",", "v", "in", "traces", ".", "items", "(", ")", "}", "\n", "output_ids", "=", "traces", "[", "'pred_seq'", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "output_ids", "=", "traces", ".", "tolist", "(", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "buf", ")", ")", ":", "\n", "\t\t\t\t\t\t", "w_ids", "=", "output_ids", "[", "i", "]", "\n", "output_buf", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "w_ids", ")", "\n", "output_tokens", "=", "[", "]", "\n", "for", "t", "in", "output_buf", ":", "\n", "\t\t\t\t\t\t\t", "if", "t", "in", "(", "\"[SEP]\"", ",", "\"[PAD]\"", ")", ":", "\n", "\t\t\t\t\t\t\t\t", "break", "\n", "", "output_tokens", ".", "append", "(", "t", ")", "\n", "", "output_sequence", "=", "' '", ".", "join", "(", "detokenize", "(", "output_tokens", ")", ")", "\n", "output_lines", "[", "buf_id", "[", "i", "]", "]", "=", "output_sequence", "\n", "if", "args", ".", "need_score_traces", ":", "\n", "\t\t\t\t\t\t\t", "score_trace_list", "[", "buf_id", "[", "i", "]", "]", "=", "{", "\n", "'scores'", ":", "traces", "[", "'scores'", "]", "[", "i", "]", ",", "'wids'", ":", "traces", "[", "'wids'", "]", "[", "i", "]", ",", "'ptrs'", ":", "traces", "[", "'ptrs'", "]", "[", "i", "]", "}", "\n", "", "", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "", "if", "args", ".", "output_file", ":", "\n", "\t\t\t", "fn_out", "=", "args", ".", "output_file", "\n", "", "else", ":", "\n", "\t\t\t", "fn_out", "=", "model_recover_path", "+", "'.'", "+", "args", ".", "split", "\n", "", "with", "open", "(", "fn_out", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fout", ":", "\n", "\t\t\t", "for", "l", "in", "output_lines", ":", "\n", "\t\t\t\t", "fout", ".", "write", "(", "l", ")", "\n", "fout", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "if", "args", ".", "need_score_traces", ":", "\n", "\t\t\t", "with", "open", "(", "fn_out", "+", "\".trace.pickle\"", ",", "\"wb\"", ")", "as", "fout_trace", ":", "\n", "\t\t\t\t", "pickle", ".", "dump", "(", "\n", "{", "\"version\"", ":", "0.0", ",", "\"num_samples\"", ":", "len", "(", "input_lines", ")", "}", ",", "fout_trace", ")", "\n", "for", "x", "in", "score_trace_list", ":", "\n", "\t\t\t\t\t", "pickle", ".", "dump", "(", "x", ",", "fout_trace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq.move_stop_words": [[52, 55], ["str.split", "w.lower"], "function", ["None"], ["def", "move_stop_words", "(", "str", ")", ":", "\n", "\t", "item", "=", "\" \"", ".", "join", "(", "[", "w", "for", "w", "in", "str", ".", "split", "(", ")", "if", "not", "w", ".", "lower", "(", ")", "in", "stop_words", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq.truncate": [[56, 61], ["str.strip.strip", "len", "str.strip.split", "str.strip.split", "max"], "function", ["None"], ["", "def", "truncate", "(", "str", ",", "num", ")", ":", "\n", "\t", "str", "=", "str", ".", "strip", "(", ")", "\n", "length", "=", "len", "(", "str", ".", "split", "(", ")", ")", "\n", "list", "=", "str", ".", "split", "(", ")", "[", "max", "(", "0", ",", "length", "-", "num", ")", ":", "]", "\n", "return", "\" \"", ".", "join", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq.detokenize": [[62, 71], ["tk_str.strip().split", "tk_str.strip", "tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_str", ")", ":", "\n", "\t", "tk_list", "=", "tk_str", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "\t\t", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "\t\t\t", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "\t\t\t", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "r_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq._get_max_epoch_model": [[76, 86], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["None"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "\t", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "\t\t", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "\t\t", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "\t\t", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq.knowledge_selection": [[87, 138], ["range", "open", "file.readlines", "open", "file.readlines", "len", "[].split", "range", "len", "open", "query_know_dict.items", "print", "print", "src[].strip().split", "len", "random.shuffle", "range", "sorted", "run_seq2seq.truncate", "range", "line.replace.replace", "out.write", "out.write", "len", "query_know_dict[].append", "knows[].split", "[].split.append", "bleu_list.append", "len", "know_bleu_map.items", "[].strip", "len", "len", "src[].strip", "src[].strip().split", "[].strip", "query_know_dict[].append", "operator.itemgetter", "line.replace.strip().split", "line.replace.strip().split", "[].strip", "know.split", "know.split", "src[].strip", "query.strip().split", "line.replace.strip", "line.replace.strip", "data[].strip().split", "str", "data[].strip().split", "query.strip", "str", "data[].strip", "str", "data[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate"], ["", "", "def", "knowledge_selection", "(", "data_path", ",", "src_path", ",", "out_path", ")", ":", "\n", "\t", "with", "open", "(", "data_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "src_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "src", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "query_know_dict", "=", "{", "}", "\n", "t", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "\t\t", "query", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "for", "num", "in", "range", "(", "len", "(", "know_list", ")", ")", ":", "\n", "\t\t\t", "try", ":", "\n", "\t\t\t\t", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", ".", "append", "(", "data", "[", "t", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "t", "+=", "1", "\n", "", "except", ":", "\n", "\t\t\t\t", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", "=", "[", "]", "\n", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", ".", "append", "(", "data", "[", "t", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "t", "+=", "1", "\n", "\n", "", "", "", "assert", "t", "==", "len", "(", "data", ")", "\n", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "count", "=", "0", "\n", "for", "query", ",", "knows", "in", "query_know_dict", ".", "items", "(", ")", ":", "\n", "\t\t\t", "check_sent", "=", "knows", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", "[", "0", "]", "\n", "random", ".", "shuffle", "(", "knows", ")", "\n", "bleu_list", "=", "[", "]", "\n", "know_list", "=", "[", "]", "\n", "for", "know", "in", "knows", ":", "\n", "\t\t\t\t", "know_list", ".", "append", "(", "know", ".", "split", "(", "\"\\t\"", ")", "[", "0", "]", ")", "\n", "bleu_list", ".", "append", "(", "know", ".", "split", "(", "\"\\t\"", ")", "[", "1", "]", ")", "\n", "", "know_bleu_map", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "bleu_list", ")", ")", ":", "\n", "\t\t\t\t", "know_bleu_map", "[", "know_list", "[", "i", "]", "]", "=", "bleu_list", "[", "i", "]", "\n", "", "sorted_knows", "=", "sorted", "(", "know_bleu_map", ".", "items", "(", ")", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "line", "=", "truncate", "(", "query", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "[", "1", "]", ".", "strip", "(", ")", ",", "128", ")", "\n", "line", "+=", "\" <#Q2K#> \"", "\n", "for", "t", "in", "range", "(", "len", "(", "sorted_knows", ")", ")", ":", "\n", "\t\t\t\t", "line", "+=", "sorted_knows", "[", "t", "]", "[", "0", "]", "\n", "line", "+=", "\" <#K#> \"", "\n", "", "line", "=", "\" \"", ".", "join", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "-", "1", "]", ")", "\n", "line", "=", "line", ".", "replace", "(", "\" \u3002\"", ",", "\"\"", ")", "\n", "line", "=", "\" \"", ".", "join", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "210", "]", ")", "\n", "\n", "if", "check_sent", "in", "line", ":", "\n", "\t\t\t\t", "count", "+=", "1", "\n", "", "out", ".", "write", "(", "line", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "", "print", "(", "len", "(", "query_know_dict", ")", ")", "\n", "print", "(", "count", "/", "len", "(", "query_know_dict", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq.main": [[140, 845], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "pathlib.Path().exists", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.Formatter", "logging.FileHandler.setFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logger.addHandler", "logger.addHandler", "json.dump", "logger.info", "int", "pytorch_bert.tokenization.BertTokenizer.from_pretrained", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "modeling.BertForPreTrainingLossMask.from_pretrained", "nn.data_parallel.DataParallelImbalance.to", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "list", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "os.path.join", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.barrier", "pytorch_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "seq2seq_loader.Preprocess4Seq2seq", "seq2seq_loader.C_Preprocess4Seq2seq", "seq2seq_loader.Preprocess4Seq2seq_predict", "print", "os.path.join", "os.path.join", "os.path.join", "seq2seq_loader.C_Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "os.path.join", "os.path.join", "seq2seq_loader.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "int", "amp.init", "logger.info", "torch.barrier", "logger.info", "torch.load", "torch.load", "torch.barrier", "nn.data_parallel.DataParallelImbalance.half", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.weight.clone", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.weight.clone", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.weight.clone", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.weight.clone", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.weight.clone", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.weight.clone", "DDP", "nn.data_parallel.DataParallelImbalance.named_parameters", "FusedAdam", "pytorch_bert.optimization.BertAdam", "logger.info", "torch.load", "torch.load", "hasattr", "FP16_Optimizer_State.load_state_dict", "logger.info", "logger.info", "logger.info", "nn.data_parallel.DataParallelImbalance.train", "tqdm.trange", "nn.data_parallel.DataParallelImbalance.eval", "pathlib.Path", "os.path.join", "bool", "list", "list", "list", "os.path.join", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "FP16_Optimizer_State", "FP16_Optimizer_State", "optim_recover.state_dict.state_dict", "logger.info", "zip", "seq2seq_loader.Preprocess4Seq2seq_predict", "open", "file.readlines", "open", "file.readlines", "open", "logger.info", "tqdm.tqdm", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "torch.get_world_size", "torch.get_world_size", "ImportError", "ImportError", "torch.utils.data.distributed.DistributedSampler.set_epoch", "nn.data_parallel.DataParallelImbalance.", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "list", "os.path.join", "os.path.join", "range", "zip", "range", "seq2seq_loader.batch_list_to_batch_tensors", "torch.cuda.is_available", "torch.cuda.is_available", "len", "any", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "Mutual_loss.mean.mean", "Golden_loss.mean.mean", "KL_loss.mean.mean", "predict_kl_loss.mean.mean", "FP16_Optimizer_State.backward", "loss.backward", "FP16_Optimizer_State.step", "FP16_Optimizer_State.zero_grad", "random.randint", "nn.data_parallel.DataParallelImbalance.", "logger.info", "logger.info", "nn.data_parallel.DataParallelImbalance.eval", "logger.info", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "run_seq2seq.knowledge_selection", "logger.info", "logger.info", "batch_decode.decode_batch", "logger.info", "logger.info", "BertTokenizer.from_pretrained.vocab.keys", "data_tokenizer.tokenize", "data_tokenizer.tokenize", "ex_list.append", "len", "torch.ones", "torch.ones", "torch.no_grad", "torch.no_grad", "nn.data_parallel.DataParallelImbalance.", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "logits[].cpu().numpy", "range", "any", "t.to", "logits[].cpu().numpy.half", "amp.init._clear_cache", "pytorch_bert.optimization.warmup_linear", "ks_loss.mean.mean", "FP16_Optimizer_State.backward", "loss.backward", "FP16_Optimizer_State.step", "FP16_Optimizer_State.zero_grad", "open", "file.readlines", "open", "file.readlines", "open", "open", "file.readlines", "open", "file.readlines", "len", "len", "metrics.f_one", "logger.info", "logger.info", "logger.info", "logger.info", "os.path.join", "torch.save", "torch.save", "os.path.join", "torch.save", "torch.save", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "src.strip", "tgt.strip", "proc", "batch.append", "t.to", "len", "batch_src[].strip", "str", "out.write", "out.write", "t.to", "amp.init._clear_cache", "pytorch_bert.optimization.warmup_linear", "os.path.join", "os.path.join", "os.path.join", "len", "zip", "range", "seq2seq_loader.batch_list_to_batch_tensors", "torch.distributed.get_rank", "torch.distributed.get_rank", "hasattr", "model_to_save.state_dict", "FP16_Optimizer_State.state_dict", "len", "logits[].cpu", "data_tokenizer.tokenize", "data_tokenizer.tokenize", "ex_list.append", "len", "torch.ones", "torch.ones", "torch.no_grad", "torch.no_grad", "nn.data_parallel.DataParallelImbalance.", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "logits[].cpu().numpy", "range", "src.strip", "tgt.strip", "proc", "batch.append", "t.to", "len", "batch_src[].strip", "str", "out.write", "out.write", "logits[].cpu"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.run_seq2seq.knowledge_selection", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.batch_decode.decode_batch", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_linear", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.f_one", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_linear", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["", "", "def", "main", "(", ")", ":", "\n", "\t", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "#Train File", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data src file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data tgt file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--check_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input check knowledge data file name\"", ")", "\n", "\n", "#KS File", "\n", "parser", ".", "add_argument", "(", "\"--ks_src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input ks data src file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ks_tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input ks data tgt file name.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--predict_input_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"predict_input_file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_output_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"predict_output_file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_bleu\"", ",", "\n", "default", "=", "0.2", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The Predicted Bleu for KS Predict \"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_predict\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run ks predict.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_avg_bpe_length\"", ",", "\n", "default", "=", "25", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"average bpe length for train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion_step\"", ",", "\n", "default", "=", "300", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "67", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "Path", "(", "args", ".", "model_recover_path", ")", ".", "exists", "(", ")", ",", "\"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "handler", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "log_dir", ",", "\"train.log\"", ")", ",", "encoding", "=", "'UTF-8'", ")", "\n", "handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s - %(name)s - %(levelname)s - %(message)s'", ")", "\n", "handler", ".", "setFormatter", "(", "formatter", ")", "\n", "\n", "console", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "logger", ".", "addHandler", "(", "handler", ")", "\n", "logger", ".", "addHandler", "(", "console", ")", "\n", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "\t\t", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "\t\t", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "\t\t", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "#Random Seed", "\n", "\n", "#torch.backends.cudnn.enabled = False", "\n", "#torch.backends.cudnn.benchmark = False", "\n", "#torch.backends.cudnn.deterministic = True", "\n", "# if n_gpu > 0:", "\n", "# \ttorch.cuda.manual_seed_all(args.seed)", "\n", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "\t\t", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "\n", "#Data process pipelines", "\n", "", "bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "\n", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "\n", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "\n", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "\n", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "\n", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "\n", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "\n", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "\n", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "C_bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "C_Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "\n", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "\n", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "\n", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "\n", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "\n", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "\n", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "\n", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "\n", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "ks_predict_bi_uni_pipeline", "=", "[", "\n", "seq2seq_loader", ".", "Preprocess4Seq2seq_predict", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "\n", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "\n", "args", ".", "max_seq_length", ",", "\n", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "\n", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "\n", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "\n", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "\n", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "\n", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "\n", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "\n", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "\n", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "\n", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "\t\t", "print", "(", "\"Loading QKR Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "\t\t\t", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "", "fn_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "src_file", "if", "args", ".", "src_file", "else", "'train.src'", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "tgt_file", "if", "args", ".", "tgt_file", "else", "'train.tgt'", ")", "\n", "fn_check", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "check_file", ")", "\n", "\n", "train_dataset", "=", "seq2seq_loader", ".", "C_Seq2SeqDataset", "(", "fn_src", ",", "fn_tgt", ",", "fn_check", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "\n", "file_oracle", "=", "file_oracle", ",", "bi_uni_pipeline", "=", "C_bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "\t\t\t", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "\t\t\t", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "print", "(", "\"Loading KS Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "ks_fn_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "ks_src_file", ")", "\n", "ks_fn_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "ks_tgt_file", ")", "\n", "ks_train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "ks_fn_src", ",", "ks_fn_tgt", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "\n", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "\t\t\t", "ks_train_sampler", "=", "RandomSampler", "(", "ks_train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "\t\t\t", "ks_train_sampler", "=", "DistributedSampler", "(", "ks_train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "ks_train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "ks_train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "ks_train_sampler", ",", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "", "amp_handle", "=", "None", "\n", "if", "args", ".", "fp16", "and", "args", ".", "amp", ":", "\n", "\t\t", "from", "apex", "import", "amp", "\n", "amp_handle", "=", "amp", ".", "init", "(", "enable_caching", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "\n", "# Prepare model", "\n", "", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "\n", "#Recover model", "\n", "", "if", "args", ".", "model_recover_path", ":", "\n", "\t\t", "logger", ".", "info", "(", "\" ** ** * Recover model: %s ** ** * \"", ",", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "global_step", "=", "0", "\n", "\n", "", "mask_word_id", ",", "eos_word_ids", ",", "sos_word_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "\"[MASK]\"", ",", "\"[SEP]\"", ",", "\"[S2S_SOS]\"", "]", ")", "\n", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "\n", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "\n", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "\n", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "\n", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ",", "\n", "mask_word_id", "=", "mask_word_id", ",", "search_beam_size", "=", "5", ",", "\n", "length_penalty", "=", "0", ",", "eos_id", "=", "eos_word_ids", ",", "sos_id", "=", "sos_word_id", ",", "forbid_duplicate_ngrams", "=", "True", ",", "\n", "forbid_ignore_set", "=", "None", ",", "mode", "=", "\"s2s\"", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "\t\t", "model", ".", "half", "(", ")", "\n", "if", "args", ".", "fp32_embedding", ":", "\n", "\t\t\t", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "model", ".", "tmp_bert_emb", ".", "word_embeddings", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "model", ".", "tmp_bert_emb", ".", "token_type_embeddings", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "model", ".", "tmp_bert_emb", ".", "position_embeddings", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "model", ".", "mul_bert_emb", ".", "word_embeddings", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "model", ".", "mul_bert_emb", ".", "token_type_embeddings", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "model", ".", "mul_bert_emb", ".", "position_embeddings", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "\t\t", "try", ":", "\n", "\t\t\t", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "\t\t\t", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "\t\t", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "\t\t", "try", ":", "\n", "\t\t\t", "from", "pytorch_bert", ".", "optimization_fp16", "import", "FP16_Optimizer_State", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "\t\t\t", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "bias_correction", "=", "False", ",", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "\t\t\t", "optimizer", "=", "FP16_Optimizer_State", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "\t\t\t", "optimizer", "=", "FP16_Optimizer_State", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "", "", "else", ":", "\n", "\t\t", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "warmup", "=", "args", ".", "warmup_proportion", ",", "t_total", "=", "t_total", ")", "\n", "\n", "", "if", "args", ".", "optim_recover_path", "is", "not", "None", ":", "\n", "\t\t", "logger", ".", "info", "(", "\" ** ** * Recover optimizer from : {} ** ** * \"", ".", "format", "(", "args", ".", "optim_recover_path", ")", ")", "\n", "optim_recover", "=", "torch", ".", "load", "(", "args", ".", "optim_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "hasattr", "(", "optim_recover", ",", "'state_dict'", ")", ":", "\n", "\t\t\t", "optim_recover", "=", "optim_recover", ".", "state_dict", "(", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "optim_recover", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "\t\t\t", "logger", ".", "info", "(", "\" ** ** * Recover optimizer: dynamic_loss_scale ** ** * \"", ")", "\n", "optimizer", ".", "dynamic_loss_scale", "=", "True", "\n", "\n", "#logger.info(\" ** ** * CUDA.empty_cache() ** ** * \")", "\n", "", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "# ################# TRAIN ############################ #", "\n", "if", "args", ".", "do_train", ":", "\n", "\t\t", "max_F1", "=", "0", "\n", "best_step", "=", "0", "\n", "logger", ".", "info", "(", "\" ** ** * Running training ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "t_total", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "start_epoch", "=", "1", "\n", "\n", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "start_epoch", "+", "1", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "\t\t\t", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "\t\t\t\t", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "\n", "", "step", "=", "0", "\n", "for", "batch", ",", "ks_batch", "in", "zip", "(", "train_dataloader", ",", "ks_train_dataloader", ")", ":", "\n", "# ################# E step + M step + Mutual Information Loss ############################ #", "\n", "\t\t\t\t", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "tgt_pos", ",", "labels", ",", "ks_labels", ",", "check_ids", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "\n", "\n", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "\n", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "\n", "masked_weights_2", "=", "oracle_weights", ",", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "tgt_pos", "=", "tgt_pos", ",", "labels", "=", "labels", ".", "half", "(", ")", ",", "\n", "ks_labels", "=", "ks_labels", ",", "check_ids", "=", "check_ids", ")", "\n", "\n", "\n", "\n", "masked_lm_loss", ",", "next_sentence_loss", ",", "KL_loss", ",", "Mutual_loss", ",", "Golden_loss", ",", "predict_kl_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "\t\t\t\t\t", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "Mutual_loss", "=", "Mutual_loss", ".", "mean", "(", ")", "\n", "Golden_loss", "=", "Golden_loss", ".", "mean", "(", ")", "\n", "KL_loss", "=", "KL_loss", ".", "mean", "(", ")", "\n", "predict_kl_loss", "=", "predict_kl_loss", ".", "mean", "(", ")", "\n", "\n", "", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "+", "KL_loss", "+", "predict_kl_loss", "+", "Mutual_loss", "+", "Golden_loss", "\n", "logger", ".", "info", "(", "\"In{}step, masked_lm_loss:{}\"", ".", "format", "(", "step", ",", "masked_lm_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}step, KL_loss:{}\"", ".", "format", "(", "step", ",", "KL_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}step, Mutual_loss:{}\"", ".", "format", "(", "step", ",", "Mutual_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}step, Golden_loss:{}\"", ".", "format", "(", "step", ",", "Golden_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}step, predict_kl_loss:{}\"", ".", "format", "(", "step", ",", "predict_kl_loss", ")", ")", "\n", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "\t\t\t\t\t", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "\t\t\t\t\t", "optimizer", ".", "backward", "(", "loss", ")", "\n", "if", "amp_handle", ":", "\n", "\t\t\t\t\t\t", "amp_handle", ".", "_clear_cache", "(", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "\t\t\t\t\t", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "args", ".", "warmup_proportion_step", "/", "t_total", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "\t\t\t\t\t\t", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "\t\t\t\t\t\t\t", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "# ################# Knowledge Selection Loss ############################ #", "\n", "", "if", "random", ".", "randint", "(", "0", ",", "4", ")", "==", "0", ":", "\n", "\t\t\t\t\t", "ks_batch", "=", "[", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "ks_batch", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "_", ",", "labels", ",", "ks_labels", "=", "ks_batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "\n", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "\n", "masked_weights_2", "=", "oracle_weights", ",", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "labels", ",", "\n", "ks_labels", "=", "ks_labels", ",", "train_ks", "=", "True", ")", "\n", "\n", "ks_loss", ",", "_", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "\t\t\t\t\t\t", "ks_loss", "=", "ks_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "ks_loss", "\n", "\n", "logger", ".", "info", "(", "\"In{}step, ks_loss:{}\"", ".", "format", "(", "step", ",", "ks_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "\t\t\t\t\t\t", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "\t\t\t\t\t\t", "optimizer", ".", "backward", "(", "loss", ")", "\n", "if", "amp_handle", ":", "\n", "\t\t\t\t\t\t\t", "amp_handle", ".", "_clear_cache", "(", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t\t", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "\t\t\t\t\t\t", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "args", ".", "warmup_proportion_step", "/", "t_total", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "\t\t\t\t\t\t\t", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "\t\t\t\t\t\t\t\t", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "", "step", "+=", "1", "\n", "###################### Eval Every 5000 Step ############################ #", "\n", "if", "(", "global_step", "+", "1", ")", "%", "5000", "==", "0", ":", "\n", "\t\t\t\t\t", "next_i", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# Know Rank Stage", "\n", "logger", ".", "info", "(", "\" ** ** * DEV Know Selection Begin ** ** * \"", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "predict_input_file", ")", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t\t\t\t\t", "src_file", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"train_tgt_pad.empty\"", ")", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t\t\t\t\t", "tgt_file", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "predict_output_file", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t\t\t\t\t", "while", "next_i", "<", "len", "(", "src_file", ")", ":", "\n", "\t\t\t\t\t\t\t", "batch_src", "=", "src_file", "[", "next_i", ":", "next_i", "+", "args", ".", "eval_batch_size", "]", "\n", "batch_tgt", "=", "tgt_file", "[", "next_i", ":", "next_i", "+", "args", ".", "eval_batch_size", "]", "\n", "\n", "next_i", "+=", "args", ".", "eval_batch_size", "\n", "\n", "ex_list", "=", "[", "]", "\n", "for", "src", ",", "tgt", "in", "zip", "(", "batch_src", ",", "batch_tgt", ")", ":", "\n", "\t\t\t\t\t\t\t\t", "src_tk", "=", "data_tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "data_tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "\n", "", "batch", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "ex_list", ")", ")", ":", "\n", "\t\t\t\t\t\t\t\t", "instance", "=", "ex_list", "[", "idx", "]", "\n", "for", "proc", "in", "ks_predict_bi_uni_pipeline", ":", "\n", "\t\t\t\t\t\t\t\t\t", "instance", "=", "proc", "(", "instance", ")", "\n", "batch", ".", "append", "(", "instance", ")", "\n", "\n", "", "", "batch_tensor", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch_tensor", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "\n", "predict_bleu", "=", "args", ".", "predict_bleu", "*", "torch", ".", "ones", "(", "[", "input_ids", ".", "shape", "[", "0", "]", "]", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t\t\t\t\t\t\t", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "\n", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "\n", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "predict_bleu", ",", "train_ks", "=", "True", ")", "\n", "\n", "logits", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "labels", "=", "logits", "[", ":", ",", "1", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "\t\t\t\t\t\t\t\t\t", "line", "=", "batch_src", "[", "i", "]", ".", "strip", "(", ")", "\n", "line", "+=", "\"\\t\"", "\n", "line", "+=", "str", "(", "labels", "[", "i", "]", ")", "\n", "out", ".", "write", "(", "line", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "", "", "data_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"qkr_dev.ks_score.tk\"", ")", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"qkr_dev.src.tk\"", ")", "\n", "src_out_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"rank_qkr_dev.src.tk\"", ")", "\n", "tgt_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"qkr_dev.tgt\"", ")", "\n", "\n", "knowledge_selection", "(", "data_path", ",", "src_path", ",", "src_out_path", ")", "\n", "logger", ".", "info", "(", "\" ** ** * DEV Know Selection End ** ** * \"", ")", "\n", "\n", "# Decode Stage", "\n", "logger", ".", "info", "(", "\" ** ** * Dev Decode Begin ** ** * \"", ")", "\n", "with", "open", "(", "src_out_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t\t\t\t\t", "dev_src_lines", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "tgt_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t\t\t\t\t", "golden_response_lines", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "decode_result", "=", "decode_batch", "(", "model", ",", "dev_src_lines", ")", "\n", "logger", ".", "info", "(", "\" ** ** * Dev Decode End ** ** * \"", ")", "\n", "\n", "# Compute dev F1", "\n", "assert", "len", "(", "decode_result", ")", "==", "len", "(", "golden_response_lines", ")", "\n", "C_F1", "=", "f_one", "(", "decode_result", ",", "golden_response_lines", ")", "[", "0", "]", "\n", "logger", ".", "info", "(", "\"** ** * Current F1 is {} ** ** * \"", ".", "format", "(", "C_F1", ")", ")", "\n", "if", "C_F1", "<", "max_F1", ":", "\n", "\t\t\t\t\t\t", "logger", ".", "info", "(", "\"** ** * Current F1 is lower than Previous F1. So Stop Training ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"** ** * The best model is {} ** ** * \"", ".", "format", "(", "best_step", ")", ")", "\n", "break", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "max_F1", "=", "C_F1", "\n", "best_step", "=", "step", "\n", "logger", ".", "info", "(", "\"** ** * Current F1 is larger than Previous F1. So Continue Training ** ** * \"", ")", "\n", "\n", "# Save trained model", "\n", "", "if", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "\t\t\t\t\t\t", "logger", ".", "info", "(", "\"** ** * Saving fine-tuned model and optimizer ** ** * \"", ")", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"model.{}_{}.bin\"", ".", "format", "(", "i_epoch", ",", "global_step", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_optim_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"optim.bin\"", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "output_optim_file", ")", "\n", "\n", "#logger.info(\" ** ** * CUDA.empty_cache() ** ** * \")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "# ################# Predict ############################ #", "\n", "", "", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "\t\t", "bi_uni_pipeline", "=", "[", "\n", "seq2seq_loader", ".", "Preprocess4Seq2seq_predict", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "\n", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "\n", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "\n", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "\n", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "\n", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "\n", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "\n", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "\n", "next_i", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "predict_input_file", ")", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t\t", "src_file", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "\"train_tgt_pad.empty\"", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t\t", "tgt_file", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "predict_output_file", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t\t", "logger", ".", "info", "(", "\"** ** * Continue knowledge ranking ** ** * \"", ")", "\n", "for", "next_i", "in", "tqdm", "(", "range", "(", "len", "(", "src_file", ")", "//", "args", ".", "eval_batch_size", "+", "1", ")", ")", ":", "\n", "#while next_i < len(src_file):", "\n", "\t\t\t\t", "batch_src", "=", "src_file", "[", "next_i", "*", "args", ".", "eval_batch_size", ":", "(", "next_i", "+", "1", ")", "*", "args", ".", "eval_batch_size", "]", "\n", "batch_tgt", "=", "tgt_file", "[", "next_i", "*", "args", ".", "eval_batch_size", ":", "(", "next_i", "+", "1", ")", "*", "args", ".", "eval_batch_size", "]", "\n", "#next_i += args.eval_batch_size", "\n", "\n", "ex_list", "=", "[", "]", "\n", "for", "src", ",", "tgt", "in", "zip", "(", "batch_src", ",", "batch_tgt", ")", ":", "\n", "\t\t\t\t\t", "src_tk", "=", "data_tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "data_tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "\n", "", "batch", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "ex_list", ")", ")", ":", "\n", "\t\t\t\t\t", "instance", "=", "ex_list", "[", "idx", "]", "\n", "for", "proc", "in", "bi_uni_pipeline", ":", "\n", "\t\t\t\t\t\t", "instance", "=", "proc", "(", "instance", ")", "\n", "batch", ".", "append", "(", "instance", ")", "\n", "\n", "", "", "batch_tensor", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch_tensor", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "\n", "predict_bleu", "=", "args", ".", "predict_bleu", "*", "torch", ".", "ones", "(", "[", "input_ids", ".", "shape", "[", "0", "]", "]", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t\t\t\t", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "\n", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "\n", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "predict_bleu", ",", "train_ks", "=", "True", ")", "\n", "\n", "logits", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "labels", "=", "logits", "[", ":", ",", "1", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "\t\t\t\t\t\t", "line", "=", "batch_src", "[", "i", "]", ".", "strip", "(", ")", "\n", "line", "+=", "\"\\t\"", "\n", "line", "+=", "str", "(", "labels", "[", "i", "]", ")", "\n", "out", ".", "write", "(", "line", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.ks_process.truncate": [[10, 15], ["str.strip.strip", "len", "str.strip.split", "str.strip.split", "max"], "function", ["None"], ["def", "truncate", "(", "str", ",", "num", ")", ":", "\n", "    ", "str", "=", "str", ".", "strip", "(", ")", "\n", "length", "=", "len", "(", "str", ".", "split", "(", ")", ")", "\n", "list", "=", "str", ".", "split", "(", ")", "[", "max", "(", "0", ",", "length", "-", "num", ")", ":", "]", "\n", "return", "\" \"", ".", "join", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.ks_process.compute_selection_score": [[17, 61], ["range", "query_know_dict.items", "print", "open", "file.readlines", "open", "file.readlines", "len", "[].split", "range", "len", "random.shuffle", "map", "map", "src[].strip().split", "len", "knows[].split", "heapq.nlargest", "heapq.nlargest", "len", "query_know_dict[].append", "bleu_list.append", "[].split.append", "src[].strip", "src[].strip().split", "[].strip", "query_know_dict[].append", "float", "[].strip", "know.split", "list", "src[].strip", "know.split", "data[].strip().split", "str", "data[].strip().split", "str", "data[].strip", "str", "data[].strip"], "function", ["None"], ["", "def", "compute_selection_score", "(", "data_path", ",", "src_path", ",", "top", ")", ":", "\n", "    ", "with", "open", "(", "src_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "        ", "src", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "with", "open", "(", "data_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "        ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "query_know_dict", "=", "{", "}", "\n", "t", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "        ", "query", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "for", "num", "in", "range", "(", "len", "(", "know_list", ")", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", ".", "append", "(", "data", "[", "t", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "t", "+=", "1", "\n", "", "except", ":", "\n", "                ", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", "=", "[", "]", "\n", "\n", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", ".", "append", "(", "data", "[", "t", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "t", "+=", "1", "\n", "\n", "", "", "", "assert", "t", "==", "len", "(", "data", ")", "\n", "\n", "count", "=", "0", "\n", "for", "query", ",", "knows", "in", "query_know_dict", ".", "items", "(", ")", ":", "\n", "        ", "check_sent", "=", "knows", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", "[", "0", "]", "\n", "random", ".", "shuffle", "(", "knows", ")", "\n", "\n", "bleu_list", "=", "[", "]", "\n", "know_list", "=", "[", "]", "\n", "for", "know", "in", "knows", ":", "\n", "            ", "try", ":", "\n", "                ", "bleu_list", ".", "append", "(", "float", "(", "know", ".", "split", "(", "\"\\t\"", ")", "[", "1", "]", ")", ")", "\n", "know_list", ".", "append", "(", "know", ".", "split", "(", "\"\\t\"", ")", "[", "0", "]", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "\n", "", "", "max_num_index_list", "=", "map", "(", "bleu_list", ".", "index", ",", "heapq", ".", "nlargest", "(", "top", ",", "bleu_list", ")", ")", "\n", "if", "know_list", "[", "list", "(", "max_num_index_list", ")", "[", "0", "]", "]", "==", "check_sent", ":", "\n", "            ", "count", "+=", "1", "\n", "\n", "", "max_num_index_list", "=", "map", "(", "bleu_list", ".", "index", ",", "heapq", ".", "nlargest", "(", "top", ",", "bleu_list", ")", ")", "\n", "", "print", "(", "count", "/", "len", "(", "query_know_dict", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.ks_process.get_rank_know": [[63, 126], ["range", "open", "file.readlines", "open", "file.readlines", "len", "[].split", "range", "len", "open", "query_know_dict.items", "print", "print", "src[].strip().split", "len", "random.shuffle", "range", "sorted", "ks_process.truncate", "range", "line.replace.replace", "out.write", "out.write", "len", "query_know_dict[].append", "knows[].split", "len", "know_bleu_map.items", "[].strip", "len", "len", "src[].strip", "src[].strip().split", "[].strip", "query_know_dict[].append", "bleu_list.append", "[].split.append", "operator.itemgetter", "line.replace.strip().split", "line.replace.strip().split", "[].strip", "float", "line.replace.split", "src[].strip", "know.split", "query.strip().split", "line.replace.strip", "line.replace.strip", "data[].strip().split", "know.split", "str", "data[].strip().split", "query.strip", "str", "data[].strip", "str", "data[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate"], ["", "def", "get_rank_know", "(", "data_path", ",", "src_path", ",", "out_path", ")", ":", "\n", "    ", "with", "open", "(", "data_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "        ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "with", "open", "(", "src_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "        ", "src", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "query_know_dict", "=", "{", "}", "\n", "t", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "\n", "        ", "query", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "for", "num", "in", "range", "(", "len", "(", "know_list", ")", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", ".", "append", "(", "data", "[", "t", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "t", "+=", "1", "\n", "", "except", ":", "\n", "                ", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", "=", "[", "]", "\n", "query_know_dict", "[", "str", "(", "i", ")", "+", "\"\\t\"", "+", "query", "]", ".", "append", "(", "data", "[", "t", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "t", "+=", "1", "\n", "\n", "", "", "", "assert", "t", "==", "len", "(", "data", ")", "\n", "\n", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "        ", "count", "=", "0", "\n", "for", "query", ",", "knows", "in", "query_know_dict", ".", "items", "(", ")", ":", "\n", "            ", "check_sent", "=", "knows", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", "[", "0", "]", "\n", "\n", "random", ".", "shuffle", "(", "knows", ")", "\n", "\n", "bleu_list", "=", "[", "]", "\n", "know_list", "=", "[", "]", "\n", "for", "know", "in", "knows", ":", "\n", "                ", "try", ":", "\n", "                    ", "bleu_list", ".", "append", "(", "float", "(", "know", ".", "split", "(", "\"\\t\"", ")", "[", "1", "]", ")", ")", "\n", "know_list", ".", "append", "(", "know", ".", "split", "(", "\"\\t\"", ")", "[", "0", "]", ")", "\n", "", "except", ":", "\n", "                    ", "pass", "\n", "\n", "", "", "know_bleu_map", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "bleu_list", ")", ")", ":", "\n", "                ", "know_bleu_map", "[", "know_list", "[", "i", "]", "]", "=", "bleu_list", "[", "i", "]", "\n", "", "sorted_knows", "=", "sorted", "(", "know_bleu_map", ".", "items", "(", ")", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "line", "=", "truncate", "(", "query", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "[", "1", "]", ".", "strip", "(", ")", ",", "128", ")", "\n", "line", "+=", "\" <#Q2K#> \"", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "sorted_knows", ")", ")", ":", "\n", "                ", "line", "+=", "sorted_knows", "[", "t", "]", "[", "0", "]", "\n", "line", "+=", "\" <#K#> \"", "\n", "\n", "", "line", "=", "\" \"", ".", "join", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "-", "1", "]", ")", "\n", "line", "=", "line", ".", "replace", "(", "\" \u3002\"", ",", "\"\"", ")", "\n", "line", "=", "\" \"", ".", "join", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "210", "]", ")", "\n", "\n", "if", "check_sent", "in", "line", "or", "line", ".", "split", "(", "\"<#K#>\"", ")", "[", "-", "1", "]", "in", "check_sent", ":", "\n", "                ", "count", "+=", "1", "\n", "\n", "", "out", ".", "write", "(", "line", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "", "print", "(", "len", "(", "query_know_dict", ")", ")", "\n", "print", "(", "count", "/", "len", "(", "query_know_dict", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.distinct": [[15, 30], ["collections.Counter", "collections.Counter", "hypo.split", "unigram_counter.update", "bigram_counter.update", "len", "sum", "len", "sum", "nltk.ngrams", "unigram_counter.values", "bigram_counter.values"], "function", ["None"], ["def", "distinct", "(", "hypothesis", ")", ":", "\n", "    ", "'''\n    compute distinct metric\n    :param hypothesis: list of str\n    :return:\n    '''", "\n", "unigram_counter", ",", "bigram_counter", "=", "Counter", "(", ")", ",", "Counter", "(", ")", "\n", "for", "hypo", "in", "hypothesis", ":", "\n", "        ", "tokens", "=", "hypo", ".", "split", "(", ")", "\n", "unigram_counter", ".", "update", "(", "tokens", ")", "\n", "bigram_counter", ".", "update", "(", "ngrams", "(", "tokens", ",", "2", ")", ")", "\n", "\n", "", "distinct_1", "=", "len", "(", "unigram_counter", ")", "/", "sum", "(", "unigram_counter", ".", "values", "(", ")", ")", "#\u8d8a\u5927\u8d8a\u597d \u4e0d\u540c\u7684\u8bcd\u7ec4", "\n", "distinct_2", "=", "len", "(", "bigram_counter", ")", "/", "sum", "(", "bigram_counter", ".", "values", "(", ")", ")", "\n", "return", "distinct_1", ",", "distinct_2", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.normalize_answer": [[36, 52], ["metrics.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re_art", ".", "sub", "(", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "return", "re_punc", ".", "sub", "(", "' '", ",", "text", ")", "# convert punctuation to spaces", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics._prec_recall_f1_score": [[54, 71], ["sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["None"], ["", "def", "_prec_recall_f1_score", "(", "pred_items", ",", "gold_items", ")", ":", "\n", "    ", "\"\"\"\n    Compute precision, recall and f1 given a set of gold and prediction items.\n    :param pred_items: iterable of predicted values\n    :param gold_items: iterable of gold values\n    :return: tuple (p, r, f1) for precision, recall, f1\n    \"\"\"", "\n", "common", "=", "Counter", "(", "gold_items", ")", "&", "Counter", "(", "pred_items", ")", "\n", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "pred_items", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "gold_items", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics._f1_score": [[73, 82], ["normalize_answer().split", "metrics._prec_recall_f1_score", "max", "max", "max", "metrics.normalize_answer", "normalize_answer().split", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._prec_recall_f1_score", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_f1_score", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Return the max F1 score between the guess and *any* answer.\"\"\"", "\n", "if", "guess", "is", "None", "or", "answers", "is", "None", ":", "\n", "        ", "return", "0", "\n", "", "g_tokens", "=", "normalize_answer", "(", "guess", ")", ".", "split", "(", ")", "\n", "scores", "=", "[", "\n", "_prec_recall_f1_score", "(", "g_tokens", ",", "normalize_answer", "(", "a", ")", ".", "split", "(", ")", ")", "for", "a", "in", "answers", "\n", "]", "\n", "return", "max", "(", "f1", "for", "_", ",", "_", ",", "f1", "in", "scores", ")", ",", "max", "(", "pre", "for", "pre", ",", "_", ",", "_", "in", "scores", ")", ",", "max", "(", "rec", "for", "_", ",", "rec", ",", "_", "in", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.f_one": [[84, 100], ["zip", "metrics._f1_score", "f1.append", "pre.append", "rec.append", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._f1_score"], ["", "def", "f_one", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "'''\n    calculate f1 metric\n    :param hypothesis: list of str\n    :param references: list of str\n    :return:\n    '''", "\n", "f1", "=", "[", "]", "\n", "pre", "=", "[", "]", "\n", "rec", "=", "[", "]", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypothesis", ",", "references", ")", ":", "\n", "        ", "res", "=", "_f1_score", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "f1", ".", "append", "(", "res", "[", "0", "]", ")", "\n", "pre", ".", "append", "(", "res", "[", "1", "]", ")", "\n", "rec", ".", "append", "(", "res", "[", "2", "]", ")", "\n", "", "return", "np", ".", "mean", "(", "f1", ")", ",", "np", ".", "mean", "(", "pre", ")", ",", "np", ".", "mean", "(", "rec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics._bleu1": [[102, 118], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu1", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "1.0", ",", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics._bleu2": [[120, 136], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu2", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "2.0", ",", "1.0", "/", "2.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics._bleu3": [[138, 154], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu3", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics._bleu4": [[156, 172], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu4", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.bleu": [[174, 183], ["zip", "numpy.mean", "tuple", "metrics._bleu1", "metrics._bleu2", "metrics._bleu3", "np.mean.append"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu1", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu2", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu3"], ["", "def", "bleu", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "bleu_scores", "=", "[", "]", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypothesis", ",", "references", ")", ":", "\n", "        ", "b1", "=", "_bleu1", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "b2", "=", "_bleu2", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "b3", "=", "_bleu3", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "bleu_scores", ".", "append", "(", "[", "b1", ",", "b2", ",", "b3", "]", ")", "\n", "", "bleu_scores", "=", "np", ".", "mean", "(", "bleu_scores", ",", "axis", "=", "0", ")", "# [bleu1, bleu2, bleu3]", "\n", "return", "tuple", "(", "bleu_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.bleu_corpus": [[184, 196], ["hypothesis.copy.copy", "references.copy.copy", "corpus_bleu", "corpus_bleu", "corpus_bleu", "hyp.split", "ref.split", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.SmoothingFunction"], "function", ["None"], ["", "def", "bleu_corpus", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "from", "nltk", ".", "translate", ".", "bleu_score", "import", "corpus_bleu", "\n", "hypothesis", "=", "hypothesis", ".", "copy", "(", ")", "\n", "references", "=", "references", ".", "copy", "(", ")", "\n", "hypothesis", "=", "[", "hyp", ".", "split", "(", ")", "for", "hyp", "in", "hypothesis", "]", "\n", "references", "=", "[", "[", "ref", ".", "split", "(", ")", "]", "for", "ref", "in", "references", "]", "\n", "# hypothesis = [normalize_answer(hyp).split(\" \") for hyp in hypothesis]", "\n", "# references = [[normalize_answer(ref).split(\" \")] for ref in references]", "\n", "b1", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "1.0", ",", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "b2", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "2.0", ",", "1.0", "/", "2.0", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "b3", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "return", "(", "b1", ",", "b2", ",", "b3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.bleu_metric": [[197, 199], ["metrics.bleu_corpus"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_corpus"], ["", "def", "bleu_metric", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "return", "bleu_corpus", "(", "hypothesis", ",", "references", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.knowledge_metric": [[201, 232], ["get_stop_words", "zip", "metrics._prec_recall_f1_score", "p_scores.append", "r_scores.append", "f_scores.append", "numpy.mean", "numpy.mean", "numpy.mean", "hyp.split", "know.split"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._prec_recall_f1_score"], ["", "def", "knowledge_metric", "(", "responses", ",", "knowledges", ")", ":", "\n", "    ", "'''\n    calculate knowledge metric\n    :param responses: list of str\n    :param knowledges: list of list of str\n    :return:\n    '''", "\n", "stop_words", "=", "get_stop_words", "(", "'en'", ")", "\n", "p_scores", ",", "r_scores", ",", "f_scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "hyp", ",", "know", "in", "zip", "(", "responses", ",", "knowledges", ")", ":", "\n", "# hyp_tokens = set([w for w in hyp.split() if w not in stop_words])", "\n", "# know = ' '.join(know)", "\n", "# know_tokens = set([w for w in know.split() if w not in stop_words])", "\n", "#", "\n", "# if len(hyp_tokens & know_tokens) == 0:", "\n", "#     _p, _r, _f1 = .0, .0, .0", "\n", "# else:", "\n", "#     _p = len(hyp_tokens & know_tokens) / len(hyp_tokens)", "\n", "#     _r = len(hyp_tokens & know_tokens) / len(know_tokens)", "\n", "#     _f1 = 2 * (_p * _r) / (_p + _r)", "\n", "\n", "# hyp_tokens = list(set([w for w in hyp.split() if w not in stop_words]))", "\n", "        ", "hyp_tokens", "=", "[", "w", "for", "w", "in", "hyp", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", "\n", "know", "=", "' '", ".", "join", "(", "know", ")", "\n", "know_tokens", "=", "[", "w", "for", "w", "in", "know", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", "\n", "_p", ",", "_r", ",", "_f1", "=", "_prec_recall_f1_score", "(", "hyp_tokens", ",", "know_tokens", ")", "\n", "p_scores", ".", "append", "(", "_p", ")", "\n", "r_scores", ".", "append", "(", "_r", ")", "\n", "f_scores", ".", "append", "(", "_f1", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "r_scores", ")", ",", "np", ".", "mean", "(", "p_scores", ")", ",", "np", ".", "mean", "(", "f_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.metrics.knowledge_metric_new": [[233, 279], ["get_stop_words", "zip", "set", "set", "p_scores.append", "r_scores.append", "f_scores.append", "numpy.mean", "numpy.mean", "numpy.mean", "len", "len", "len", "len", "len", "hyp.split", "know.split"], "function", ["None"], ["", "def", "knowledge_metric_new", "(", "responses", ",", "knowledges", ")", ":", "\n", "    ", "'''\n    calculate knowledge metric\n    :param responses: list of str\n    :param knowledges: list of list of str\n    :return:\n    '''", "\n", "# stop_words = get_stop_words('en')", "\n", "# p_scores,  r_scores, f_scores = [], [], []", "\n", "# for hyp, know in zip(responses, knowledges):", "\n", "#     hyp_tokens = set([w for w in hyp.split() if w not in stop_words])", "\n", "#     know = ' '.join(know)", "\n", "#     know_tokens = set([w for w in know.split() if w not in stop_words])", "\n", "#", "\n", "#     if len(hyp_tokens & know_tokens) == 0:", "\n", "#         _p, _r, _f1 = .0, .0, .0", "\n", "#     else:", "\n", "#         _p = len(hyp_tokens & know_tokens) / len(hyp_tokens)", "\n", "#         _r = len(hyp_tokens & know_tokens) / len(know_tokens)", "\n", "#         _f1 = 2 * (_p * _r) / (_p + _r)", "\n", "#", "\n", "#     p_scores.append(_p)", "\n", "#     r_scores.append(_r)", "\n", "#     f_scores.append(_f1)", "\n", "#", "\n", "# return np.mean(r_scores), np.mean(p_scores),  np.mean(f_scores)", "\n", "\n", "stop_words", "=", "get_stop_words", "(", "'en'", ")", "\n", "p_scores", ",", "r_scores", ",", "f_scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "hyp", ",", "know", "in", "zip", "(", "responses", ",", "knowledges", ")", ":", "\n", "        ", "hyp_tokens", "=", "set", "(", "[", "w", "for", "w", "in", "hyp", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", ")", "\n", "know", "=", "' '", ".", "join", "(", "know", ")", "\n", "know_tokens", "=", "set", "(", "[", "w", "for", "w", "in", "know", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", ")", "\n", "\n", "if", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "==", "0", ":", "\n", "            ", "_p", ",", "_r", ",", "_f1", "=", ".0", ",", ".0", ",", ".0", "\n", "", "else", ":", "\n", "            ", "_p", "=", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "/", "len", "(", "hyp_tokens", ")", "\n", "_r", "=", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "/", "len", "(", "know_tokens", ")", "\n", "_f1", "=", "2", "*", "(", "_p", "*", "_r", ")", "/", "(", "_p", "+", "_r", ")", "\n", "\n", "", "p_scores", ".", "append", "(", "_p", ")", "\n", "r_scores", ".", "append", "(", "_r", ")", "\n", "f_scores", ".", "append", "(", "_f1", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "r_scores", ")", ",", "np", ".", "mean", "(", "p_scores", ")", ",", "np", ".", "mean", "(", "f_scores", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.compute_bleu.move_stop_words": [[13, 17], ["str.split", "w.lower"], "function", ["None"], ["def", "move_stop_words", "(", "str", ")", ":", "\n", "\n", "    ", "item", "=", "\" \"", ".", "join", "(", "[", "w", "for", "w", "in", "str", ".", "split", "(", ")", "if", "not", "w", ".", "lower", "(", ")", "in", "stop_words", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.compute_bleu.detokenize": [[19, 28], ["tk_str.strip().split", "tk_str.strip", "tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_str", ")", ":", "\n", "\t", "tk_list", "=", "tk_str", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "\t\t", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "\t\t\t", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "\t\t\t", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "r_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.compute_bleu.compute_bleu_between_rule_and_model": [[30, 44], ["metrics.bleu_metric", "print", "metrics.f_one", "print", "open", "rule_file.readlines", "open", "model_file.readlines", "compute_bleu.detokenize", "item.lower", "round", "round", "round", "round"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_metric", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.f_one", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize"], ["", "def", "compute_bleu_between_rule_and_model", "(", "rule_parh", ",", "model_path", ")", ":", "\n", "    ", "with", "open", "(", "rule_parh", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "rule_file", ":", "\n", "        ", "rule", "=", "rule_file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "model_path", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "model_file", ":", "\n", "        ", "model", "=", "model_file", ".", "readlines", "(", ")", "\n", "", "model", "=", "[", "detokenize", "(", "item", ".", "lower", "(", ")", ")", "for", "item", "in", "model", "]", "\n", "\n", "\n", "b1", ",", "b2", ",", "b3", "=", "bleu_metric", "(", "rule", ",", "model", ")", "\n", "\n", "print", "(", "\"b1:{},b2:{},b3:{}\"", ".", "format", "(", "round", "(", "b1", ",", "4", ")", ",", "round", "(", "b2", ",", "4", ")", ",", "round", "(", "b3", ",", "4", ")", ")", ")", "\n", "\n", "res", "=", "f_one", "(", "rule", ",", "model", ")", "\n", "print", "(", "'f1:{}'", ".", "format", "(", "round", "(", "res", "[", "0", "]", ",", "4", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.batch_decode.detokenize": [[31, 39], ["tk.startswith", "r_list.append", "len"], "function", ["None"], ["def", "detokenize", "(", "tk_list", ")", ":", "\n", "    ", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "        ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "            ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.batch_decode.ascii_print": [[40, 43], ["text.encode.encode", "print"], "function", ["None"], ["", "def", "ascii_print", "(", "text", ")", ":", "\n", "    ", "text", "=", "text", ".", "encode", "(", "\"ascii\"", ",", "\"ignore\"", ")", "\n", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.batch_decode.decode_batch": [[84, 143], ["torch.cuda.empty_cache", "model.eval", "len", "range", "x.strip", "sorted", "max", "all_output_lines.extend", "len", "len", "data_tokenizer.tokenize", "list", "len", "torch.no_grad", "seq2seq_loader.batch_list_to_batch_tensors", "model", "range", "enumerate", "len", "instances.append", "model.tolist", "len", "tokenizer.convert_ids_to_tokens", "proc", "t.to", "v.tolist", "output_tokens.append", "batch_decode.detokenize", "len", "model.items"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize"], ["def", "decode_batch", "(", "model", ",", "big_batch_data", ")", ":", "\n", "\n", "    ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "max_src_length", "=", "max_seq_length", "-", "2", "-", "max_tgt_length", "\n", "input_lines", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "big_batch_data", "]", "\n", "\n", "\n", "all_input_lines", "=", "[", "data_tokenizer", ".", "tokenize", "(", "\n", "x", ")", "[", ":", "max_src_length", "]", "for", "x", "in", "input_lines", "]", "\n", "\n", "total_length", "=", "len", "(", "input_lines", ")", "\n", "total_iter", "=", "total_length", "//", "batch_size", "\n", "if", "total_iter", "*", "batch_size", "<", "total_length", ":", "\n", "        ", "total_iter", "+=", "1", "\n", "\n", "", "all_output_lines", "=", "[", "]", "\n", "\n", "for", "cur_iter", "in", "range", "(", "total_iter", ")", ":", "\n", "        ", "input_lines", "=", "all_input_lines", "[", "cur_iter", "*", "batch_size", ":", "(", "cur_iter", "+", "1", ")", "*", "batch_size", "]", "\n", "input_lines", "=", "sorted", "(", "list", "(", "enumerate", "(", "input_lines", ")", ")", ",", "key", "=", "lambda", "x", ":", "-", "len", "(", "x", "[", "1", "]", ")", ")", "\n", "output_lines", "=", "[", "\"\"", "]", "*", "len", "(", "input_lines", ")", "\n", "_chunk", "=", "input_lines", "\n", "buf_id", "=", "[", "x", "[", "0", "]", "for", "x", "in", "_chunk", "]", "\n", "buf", "=", "[", "x", "[", "1", "]", "for", "x", "in", "_chunk", "]", "\n", "\n", "max_a_len", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "buf", "]", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "[", "(", "x", ",", "max_a_len", ")", "for", "x", "in", "buf", "]", ":", "\n", "            ", "for", "proc", "in", "bi_uni_pipeline", ":", "\n", "                ", "instances", ".", "append", "(", "proc", "(", "instance", ")", ")", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "batch", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "\n", "instances", ")", "\n", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "task_idx", "=", "batch", "\n", "traces", "=", "model", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "position_ids", "=", "position_ids", ",", "attention_mask", "=", "input_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ",", "decode", "=", "True", ")", "\n", "if", "beam_size", ">", "1", ":", "\n", "                ", "traces", "=", "{", "k", ":", "v", ".", "tolist", "(", ")", "for", "k", ",", "v", "in", "traces", ".", "items", "(", ")", "}", "\n", "output_ids", "=", "traces", "[", "'pred_seq'", "]", "\n", "", "else", ":", "\n", "                ", "output_ids", "=", "traces", ".", "tolist", "(", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "buf", ")", ")", ":", "\n", "                ", "w_ids", "=", "output_ids", "[", "i", "]", "\n", "output_buf", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "w_ids", ")", "\n", "output_tokens", "=", "[", "]", "\n", "for", "t", "in", "output_buf", ":", "\n", "                    ", "if", "t", "in", "(", "\"[SEP]\"", ",", "\"[PAD]\"", ")", ":", "\n", "                        ", "break", "\n", "", "output_tokens", ".", "append", "(", "t", ")", "\n", "", "output_sequence", "=", "' '", ".", "join", "(", "detokenize", "(", "output_tokens", ")", ")", "\n", "output_lines", "[", "buf_id", "[", "i", "]", "]", "=", "output_sequence", "\n", "\n", "", "", "all_output_lines", ".", "extend", "(", "output_lines", ")", "\n", "\n", "", "assert", "len", "(", "all_output_lines", ")", "==", "len", "(", "all_input_lines", ")", "\n", "return", "all_output_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.know_selection._get_max_epoch_model": [[37, 48], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["None"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "    ", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "        ", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "        ", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.know_selection.main": [[50, 810], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "pathlib.Path().exists", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.Formatter", "logging.FileHandler.setFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logger.addHandler", "logger.addHandler", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_bert.tokenization.BertTokenizer.from_pretrained", "print", "os.path.join", "os.path.join", "seq2seq_loader.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "os.path.join", "os.path.join", "seq2seq_loader.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "os.path.join", "os.path.join", "seq2seq_loader.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "os.path.join", "os.path.join", "seq2seq_loader.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "int", "know_selection._get_max_epoch_model", "nn.data_parallel.DataParallelImbalance.to", "list", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "os.path.join", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.barrier", "pytorch_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "seq2seq_loader.Preprocess4Seq2seq", "os.path.join", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "amp.init", "logger.info", "torch.barrier", "modeling.BertForPreTrainingLossMask.from_pretrained", "modeling.BertForPreTrainingLossMask.from_pretrained", "torch.barrier", "nn.data_parallel.DataParallelImbalance.half", "DDP", "nn.data_parallel.DataParallelImbalance.named_parameters", "FusedAdam", "pytorch_bert.optimization.BertAdam", "logger.info", "torch.load", "torch.load", "hasattr", "FP16_Optimizer_State.load_state_dict", "logger.info", "logger.info", "logger.info", "nn.data_parallel.DataParallelImbalance.train", "tqdm.trange", "nn.data_parallel.DataParallelImbalance.eval", "pathlib.Path", "os.path.join", "bool", "list", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "logger.info", "torch.load", "torch.load", "math.floor", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "FP16_Optimizer_State", "FP16_Optimizer_State", "os.path.join", "optim_recover.state_dict.state_dict", "logger.info", "zip", "seq2seq_loader.Preprocess4Seq2seq_predict", "open", "file.readlines", "open", "file.readlines", "open", "BertTokenizer.from_pretrained.vocab.keys", "len", "os.path.join", "logger.info", "torch.load", "torch.load", "ImportError", "ImportError", "int", "torch.utils.data.distributed.DistributedSampler.set_epoch", "nn.data_parallel.DataParallelImbalance.", "logger.info", "logger.info", "logger.info", "list", "os.path.join", "os.path.join", "len", "print", "zip", "range", "seq2seq_loader.batch_list_to_batch_tensors", "torch.cuda.is_available", "torch.cuda.is_available", "any", "float", "FP16_Optimizer_State.backward", "loss.backward", "FP16_Optimizer_State.step", "FP16_Optimizer_State.zero_grad", "random.randint", "nn.data_parallel.DataParallelImbalance.", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.tqdm", "enumerate", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.tqdm", "enumerate", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "BertTokenizer.from_pretrained.vocab.keys", "data_tokenizer.tokenize", "data_tokenizer.tokenize", "ex_list.append", "len", "torch.ones", "torch.ones", "torch.no_grad", "torch.no_grad", "nn.data_parallel.DataParallelImbalance.", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "logits[].cpu().numpy", "range", "any", "t.to", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "KL_loss.mean.mean", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "len", "amp.init._clear_cache", "pytorch_bert.optimization.warmup_linear", "FP16_Optimizer_State.backward", "loss.backward", "FP16_Optimizer_State.step", "FP16_Optimizer_State.zero_grad", "torch.utils.data.distributed.DistributedSampler.set_epoch", "logger.info", "os.path.join", "torch.save", "torch.save", "os.path.join", "torch.save", "torch.save", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "src.strip", "tgt.strip", "proc", "batch.append", "t.to", "len", "batch_src[].strip", "str", "out.write", "out.write", "t.to", "ks_loss.mean.mean", "KS_KL_loss.mean.mean", "ks_loss.mean.mean", "amp.init._clear_cache", "pytorch_bert.optimization.warmup_linear", "torch.no_grad", "torch.no_grad", "nn.data_parallel.DataParallelImbalance.", "tqdm.tqdm.set_description", "masked_lm_loss.mean.item", "KL_loss.mean.item", "torch.no_grad", "torch.no_grad", "nn.data_parallel.DataParallelImbalance.", "tqdm.tqdm.set_description", "ks_loss.mean.item", "KS_KL_loss.mean.item", "torch.distributed.get_rank", "torch.distributed.get_rank", "hasattr", "model_to_save.state_dict", "FP16_Optimizer_State.state_dict", "logits[].cpu", "t.to", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "KL_loss.mean.mean", "t.to", "ks_loss.mean.mean", "KS_KL_loss.mean.mean", "round", "masked_lm_loss.mean.item", "ks_loss.mean.item"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL._get_max_epoch_model", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_linear", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_linear", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--dev_src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--ks_src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ks_tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--ks_dev_src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ks_dev_tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--predict_input_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"predict_input_file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_output_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"predict_output_file\"", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--predict_bleu\"", ",", "\n", "default", "=", "0.5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The Predicted Bleu for KS Predict \"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_vae\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to train vae.\"", ")", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_predict\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run ks predict.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "Path", "(", "args", ".", "model_recover_path", ")", ".", "exists", "(", "\n", ")", ",", "\"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "handler", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "log_dir", ",", "\"train.log\"", ")", ",", "encoding", "=", "'UTF-8'", ")", "\n", "handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s - %(name)s - %(levelname)s - %(message)s'", ")", "\n", "handler", ".", "setFormatter", "(", "formatter", ")", "\n", "\n", "console", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "logger", ".", "addHandler", "(", "handler", ")", "\n", "logger", ".", "addHandler", "(", "console", ")", "\n", "\n", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "        ", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "\n", "", "print", "(", "\"Loading QKR Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "        ", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "", "fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "src_file", "if", "args", ".", "src_file", "else", "'train.src'", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "tgt_file", "if", "args", ".", "tgt_file", "else", "'train.tgt'", ")", "\n", "train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "fn_src", ",", "fn_tgt", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "        ", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "print", "(", "\"Loading KS Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "ks_fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "ks_src_file", ")", "\n", "ks_fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "ks_tgt_file", ")", "\n", "ks_train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "ks_fn_src", ",", "ks_fn_tgt", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "\n", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "ks_train_sampler", "=", "RandomSampler", "(", "ks_train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "        ", "ks_train_sampler", "=", "DistributedSampler", "(", "ks_train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "ks_train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "ks_train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "ks_train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "\n", "pin_memory", "=", "False", ")", "\n", "\n", "\n", "logger", ".", "info", "(", "\"Loading QKR Eval Dataset from {}\"", ".", "format", "(", "args", ".", "data_dir", ")", ")", "\n", "\n", "fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "dev_src_file", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "dev_tgt_file", ")", "\n", "dev_reddit_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "fn_src", ",", "fn_tgt", ",", "args", ".", "eval_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "\n", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "dev_reddit_sampler", "=", "RandomSampler", "(", "dev_reddit_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "eval_batch_size", "\n", "", "else", ":", "\n", "        ", "dev_reddit_sampler", "=", "DistributedSampler", "(", "dev_reddit_dataset", ")", "\n", "_batch_size", "=", "args", ".", "eval_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "dev_reddit_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_reddit_dataset", ",", "batch_size", "=", "_batch_size", ",", "\n", "sampler", "=", "dev_reddit_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "\n", "pin_memory", "=", "False", ")", "\n", "\n", "logger", ".", "info", "(", "\"Loading KS Eval Dataset from {}\"", ".", "format", "(", "args", ".", "data_dir", ")", ")", "\n", "\n", "ks_dev_fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "ks_dev_src_file", ")", "\n", "ks_dev_fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "ks_dev_tgt_file", ")", "\n", "ks_dev_reddit_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "ks_dev_fn_src", ",", "ks_dev_fn_tgt", ",", "args", ".", "eval_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "\n", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "ks_dev_reddit_sampler", "=", "RandomSampler", "(", "ks_dev_reddit_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "eval_batch_size", "\n", "", "else", ":", "\n", "        ", "ks_dev_reddit_sampler", "=", "DistributedSampler", "(", "ks_dev_reddit_dataset", ")", "\n", "_batch_size", "=", "args", ".", "eval_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "ks_dev_reddit_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "ks_dev_reddit_dataset", ",", "batch_size", "=", "_batch_size", ",", "\n", "sampler", "=", "ks_dev_reddit_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "\n", "pin_memory", "=", "False", ")", "\n", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "# t_total = int(math.ceil(len(train_dataset.ex_list) / args.train_batch_size)", "\n", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "\n", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "amp_handle", "=", "None", "\n", "if", "args", ".", "fp16", "and", "args", ".", "amp", ":", "\n", "        ", "from", "apex", "import", "amp", "\n", "amp_handle", "=", "amp", ".", "init", "(", "enable_caching", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "\n", "# Prepare model", "\n", "", "recover_step", "=", "_get_max_epoch_model", "(", "args", ".", "output_dir", ")", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "if", "(", "recover_step", "is", "None", ")", "and", "(", "args", ".", "model_recover_path", "is", "None", ")", ":", "\n", "# if _state_dict == {}, the parameters are randomly initialized", "\n", "# if _state_dict == None, the parameters are initialized with bert-init", "\n", "        ", "_state_dict", "=", "{", "}", "if", "args", ".", "from_scratch", "else", "None", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "_state_dict", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "global_step", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "recover_step", ":", "\n", "            ", "logger", ".", "info", "(", "\" ** ** * Recover model: %d ** ** * \"", ",", "recover_step", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "# recover_step == number of epochs", "\n", "global_step", "=", "math", ".", "floor", "(", "\n", "recover_step", "*", "t_total", "/", "args", ".", "num_train_epochs", ")", "\n", "", "elif", "args", ".", "model_recover_path", ":", "\n", "            ", "logger", ".", "info", "(", "\" ** ** * Recover model: %s ** ** * \"", ",", "\n", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "\n", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "global_step", "=", "0", "\n", "", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "if", "args", ".", "fp32_embedding", ":", "\n", "            ", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "\n", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "# model = torch.nn.DataParallel(model)", "\n", "        ", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "# from apex.optimizers import FP16_Optimizer", "\n", "            ", "from", "pytorch_bert", ".", "optimization_fp16", "import", "FP16_Optimizer_State", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer_State", "(", "\n", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer_State", "(", "\n", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "", "if", "recover_step", ":", "\n", "        ", "logger", ".", "info", "(", "\" ** ** * Recover optimizer: %d  ** ** * \"", ",", "recover_step", ")", "\n", "optim_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "hasattr", "(", "optim_recover", ",", "'state_dict'", ")", ":", "\n", "            ", "optim_recover", "=", "optim_recover", ".", "state_dict", "(", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "optim_recover", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\" ** ** * Recover optimizer: dynamic_loss_scale ** ** * \"", ")", "\n", "optimizer", ".", "dynamic_loss_scale", "=", "True", "\n", "\n", "", "", "logger", ".", "info", "(", "\" ** ** * CUDA.empty_cache() ** ** * \"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "KL_weight", "=", "0.0", "\n", "\n", "logger", ".", "info", "(", "\" ** ** * Running training ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "t_total", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "if", "recover_step", ":", "\n", "            ", "start_epoch", "=", "recover_step", "+", "1", "\n", "", "else", ":", "\n", "            ", "start_epoch", "=", "1", "\n", "", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", "+", "1", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "            ", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "\n", "\n", "", "step", "=", "0", "\n", "for", "batch", ",", "ks_batch", "in", "zip", "(", "train_dataloader", ",", "ks_train_dataloader", ")", ":", "\n", "                ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "labels", ",", "ks_labels", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "labels", ",", "ks_labels", "=", "ks_labels", ",", "train_vae", "=", "args", ".", "train_vae", ")", "\n", "\n", "if", "args", ".", "train_vae", ":", "\n", "                    ", "masked_lm_loss", ",", "next_sentence_loss", ",", "KL_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "                        ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "KL_loss", "=", "KL_loss", ".", "mean", "(", ")", "\n", "", "", "else", ":", "\n", "                    ", "masked_lm_loss", ",", "next_sentence_loss", ",", "_", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "                        ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "\n", "", "", "KL_weight", "+=", "1.0", "/", "float", "(", "len", "(", "ks_train_dataloader", ")", ")", "\n", "\n", "if", "args", ".", "train_vae", ":", "\n", "                    ", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "+", "KL_weight", "*", "KL_loss", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "\n", "", "logger", ".", "info", "(", "\"In{}step, masked_lm_loss:{}\"", ".", "format", "(", "step", ",", "masked_lm_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}step, KL_weight:{}\"", ".", "format", "(", "step", ",", "KL_weight", ")", ")", "\n", "#logger.info(\"In{}step, KL_loss:{}\".format(step, KL_loss))", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "if", "amp_handle", ":", "\n", "                        ", "amp_handle", ".", "_clear_cache", "(", ")", "\n", "", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "\n", "args", ".", "warmup_proportion", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                        ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                            ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "if", "random", ".", "randint", "(", "0", ",", "0", ")", "==", "0", ":", "\n", "                    ", "ks_batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "ks_batch", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "labels", ",", "ks_labels", "=", "ks_batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "\n", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "\n", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "labels", ",", "ks_labels", "=", "ks_labels", ",", "train_ks", "=", "True", ",", "train_vae", "=", "args", ".", "train_vae", ")", "\n", "if", "args", ".", "train_vae", ":", "\n", "                        ", "ks_loss", ",", "KS_KL_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "                            ", "ks_loss", "=", "ks_loss", ".", "mean", "(", ")", "\n", "KS_KL_loss", "=", "KS_KL_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "ks_loss", "+", "KL_weight", "*", "KS_KL_loss", "\n", "", "else", ":", "\n", "                        ", "ks_loss", ",", "_", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "                            ", "ks_loss", "=", "ks_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "ks_loss", "\n", "\n", "\n", "", "logger", ".", "info", "(", "\"In{}step, ks_loss:{}\"", ".", "format", "(", "step", ",", "ks_loss", ")", ")", "\n", "#logger.info(\"In{}step, KS_KL_loss:{}\".format(step, KS_KL_loss))", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                        ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "if", "amp_handle", ":", "\n", "                            ", "amp_handle", ".", "_clear_cache", "(", ")", "\n", "", "", "else", ":", "\n", "                        ", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                        ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "\n", "args", ".", "warmup_proportion", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                            ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                                ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "", "step", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "200", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\" ** ** * Running QKR evaling ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "                        ", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "", "dev_iter_bar", "=", "tqdm", "(", "dev_reddit_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "total_lm_loss", "=", "0", "\n", "total_kl_loss", "=", "0", "\n", "for", "qkr_dev_step", ",", "batch", "in", "enumerate", "(", "dev_iter_bar", ")", ":", "\n", "                        ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "                            ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "                            ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "labels", ",", "ks_labels", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                            ", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "\n", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "\n", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "labels", ",", "ks_labels", "=", "ks_labels", ",", "train_vae", "=", "args", ".", "train_vae", ")", "\n", "masked_lm_loss", ",", "next_sentence_loss", ",", "KL_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "# loss = loss.mean()", "\n", "                                ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "KL_loss", "=", "KL_loss", ".", "mean", "(", ")", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "", "dev_iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "masked_lm_loss", ".", "item", "(", ")", ")", "\n", "total_lm_loss", "+=", "masked_lm_loss", ".", "item", "(", ")", "\n", "total_kl_loss", "+=", "KL_loss", ".", "item", "(", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "", "", "total_mean_lm_loss", "=", "total_lm_loss", "/", "(", "qkr_dev_step", "+", "1", ")", "\n", "total_mean_kl_loss", "=", "total_kl_loss", "/", "(", "qkr_dev_step", "+", "1", ")", "\n", "\n", "logger", ".", "info", "(", "\"** ** * Evaling mean loss ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"In{}epoch,dev_lm_loss:{}\"", ".", "format", "(", "i_epoch", ",", "total_mean_lm_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}epoch,dev_kl_loss:{}\"", ".", "format", "(", "i_epoch", ",", "total_mean_kl_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "\n", "\n", "logger", ".", "info", "(", "\" ** ** * Running KS evaling ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "\n", "ks_dev_iter_bar", "=", "tqdm", "(", "ks_dev_reddit_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "total_ks_loss", "=", "0", "\n", "total_ks_kl_loss", "=", "0", "\n", "for", "ks_dev_step", ",", "batch", "in", "enumerate", "(", "ks_dev_iter_bar", ")", ":", "\n", "                        ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "labels", ",", "ks_labels", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                            ", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "\n", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "\n", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "labels", ",", "ks_labels", "=", "ks_labels", ",", "train_ks", "=", "True", ",", "train_vae", "=", "args", ".", "train_vae", ")", "\n", "ks_loss", ",", "KS_KL_loss", "=", "loss_tuple", "\n", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "# loss = loss.mean()", "\n", "                                ", "ks_loss", "=", "ks_loss", ".", "mean", "(", ")", "\n", "KS_KL_loss", "=", "KS_KL_loss", ".", "mean", "(", ")", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "", "ks_dev_iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "ks_loss", ".", "item", "(", ")", ")", "\n", "total_ks_loss", "+=", "ks_loss", ".", "item", "(", ")", "\n", "total_ks_kl_loss", "+=", "KS_KL_loss", ".", "item", "(", ")", "\n", "\n", "", "", "total_mean_ks_loss", "=", "total_ks_loss", "/", "(", "ks_dev_step", "+", "1", ")", "\n", "total_mean_ks_kl_loss", "=", "total_ks_kl_loss", "/", "(", "ks_dev_step", "+", "1", ")", "\n", "\n", "logger", ".", "info", "(", "\"** ** * Evaling mean loss ** ** * \"", ")", "\n", "logger", ".", "info", "(", "\"In{}epoch,dev_ks_loss:{}\"", ".", "format", "(", "i_epoch", ",", "total_mean_ks_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"In{}epoch,dev_ks_kl_loss:{}\"", ".", "format", "(", "i_epoch", ",", "total_mean_ks_kl_loss", ")", ")", "\n", "\n", "total_mean_loss", "=", "total_mean_lm_loss", "+", "total_mean_kl_loss", "+", "total_mean_ks_loss", "+", "total_mean_ks_kl_loss", "\n", "logger", ".", "info", "(", "\"In{}epoch,dev_loss:{}\"", ".", "format", "(", "i_epoch", ",", "total_mean_loss", ")", ")", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "\n", "# Save a trained model", "\n", "if", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "\n", "\"** ** * Saving fine-tuned model and optimizer ** ** * \"", ")", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "\n", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{}_{}_{}.bin\"", ".", "format", "(", "i_epoch", ",", "step", ",", "round", "(", "total_mean_loss", ",", "4", ")", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_optim_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.bin\"", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "output_optim_file", ")", "\n", "\n", "logger", ".", "info", "(", "\" ** ** * CUDA.empty_cache() ** ** * \"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "\n", "        ", "bi_uni_pipeline", "=", "[", "\n", "seq2seq_loader", ".", "Preprocess4Seq2seq_predict", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "\n", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "\n", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "\n", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "\n", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "\n", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "\n", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "\n", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "\n", "next_i", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "predict_input_file", ")", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "            ", "src_file", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "\"train_tgt_pad.empty\"", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "            ", "tgt_file", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "predict_output_file", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "            ", "while", "next_i", "<", "len", "(", "src_file", ")", ":", "\n", "                ", "print", "(", "next_i", ")", "\n", "batch_src", "=", "src_file", "[", "next_i", ":", "next_i", "+", "args", ".", "eval_batch_size", "]", "\n", "batch_tgt", "=", "tgt_file", "[", "next_i", ":", "next_i", "+", "args", ".", "eval_batch_size", "]", "\n", "\n", "next_i", "+=", "args", ".", "eval_batch_size", "\n", "\n", "ex_list", "=", "[", "]", "\n", "for", "src", ",", "tgt", "in", "zip", "(", "batch_src", ",", "batch_tgt", ")", ":", "\n", "                    ", "src_tk", "=", "data_tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "data_tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "\n", "", "batch", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "ex_list", ")", ")", ":", "\n", "                    ", "instance", "=", "ex_list", "[", "idx", "]", "\n", "for", "proc", "in", "bi_uni_pipeline", ":", "\n", "                        ", "instance", "=", "proc", "(", "instance", ")", "\n", "batch", ".", "append", "(", "instance", ")", "\n", "\n", "", "", "batch_tensor", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch_tensor", "]", "\n", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "\n", "predict_bleu", "=", "args", ".", "predict_bleu", "*", "torch", ".", "ones", "(", "[", "input_ids", ".", "shape", "[", "0", "]", "]", ",", "device", "=", "input_ids", ".", "device", ")", "# B", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "\n", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "\n", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "labels", "=", "predict_bleu", ",", "train_ks", "=", "True", ",", "train_vae", "=", "args", ".", "train_vae", ")", "\n", "\n", "logits", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "labels", "=", "logits", "[", ":", ",", "1", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# print(labels)", "\n", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "                        ", "line", "=", "batch_src", "[", "i", "]", ".", "strip", "(", ")", "\n", "line", "+=", "\"\\t\"", "\n", "line", "+=", "str", "(", "labels", "[", "i", "]", ")", "\n", "out", ".", "write", "(", "line", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.DataParallelImbalance.__init__": [[74, 101], ["torch.nn.DataParallel.__init__", "list", "torch.cuda._utils._get_device_index", "torch.cuda.is_available", "list", "all", "RuntimeError", "map", "len", "data_parallel.DataParallelImbalance.module.cuda", "range", "torch.cuda.device_count", "torch.cuda._utils._get_device_index", "itertools.chain", "module.parameters", "module.buffers"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "module", ",", "device_ids", "=", "None", ",", "output_device", "=", "None", ",", "dim", "=", "0", ")", ":", "\n", "        ", "super", "(", "DataParallelImbalance", ",", "self", ")", ".", "__init__", "(", "\n", "module", ",", "device_ids", ",", "output_device", ",", "dim", ")", "\n", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "module", "=", "module", "\n", "self", ".", "device_ids", "=", "[", "]", "\n", "return", "\n", "\n", "", "if", "device_ids", "is", "None", ":", "\n", "            ", "device_ids", "=", "list", "(", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ")", "\n", "", "if", "output_device", "is", "None", ":", "\n", "            ", "output_device", "=", "device_ids", "[", "0", "]", "\n", "\n", "", "if", "not", "all", "(", "t", ".", "is_cuda", "and", "t", ".", "device", ".", "index", "==", "device_ids", "[", "0", "]", "\n", "for", "t", "in", "chain", "(", "module", ".", "parameters", "(", ")", ",", "module", ".", "buffers", "(", ")", ")", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"module must have its parameters and buffers \"", "\n", "\"on device %d (device_ids[0])\"", "%", "device_ids", "[", "0", "]", ")", "\n", "\n", "", "self", ".", "dim", "=", "dim", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "device_ids", "=", "list", "(", "\n", "map", "(", "lambda", "x", ":", "_get_device_index", "(", "x", ",", "True", ")", ",", "device_ids", ")", ")", "\n", "self", ".", "output_device", "=", "_get_device_index", "(", "output_device", ",", "True", ")", "\n", "\n", "if", "len", "(", "self", ".", "device_ids", ")", "==", "1", ":", "\n", "            ", "self", ".", "module", ".", "cuda", "(", "device_ids", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.DataParallelImbalance.forward": [[102, 112], ["data_parallel.DataParallelImbalance.scatter_imbalance", "data_parallel.DataParallelImbalance.replicate", "data_parallel.DataParallelImbalance.parallel_apply", "data_parallel.DataParallelImbalance.gather", "data_parallel.DataParallelImbalance.module", "len", "data_parallel.DataParallelImbalance.module", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.scatter_imbalance"], ["", "", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "not", "self", ".", "device_ids", ":", "\n", "            ", "return", "self", ".", "module", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "", "inputs", ",", "kwargs", "=", "self", ".", "scatter_imbalance", "(", "\n", "inputs", ",", "kwargs", ",", "self", ".", "device_ids", ")", "\n", "if", "len", "(", "self", ".", "device_ids", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "module", "(", "*", "inputs", "[", "0", "]", ",", "**", "kwargs", "[", "0", "]", ")", "\n", "", "replicas", "=", "self", ".", "replicate", "(", "self", ".", "module", ",", "self", ".", "device_ids", "[", ":", "len", "(", "inputs", ")", "]", ")", "\n", "outputs", "=", "self", ".", "parallel_apply", "(", "replicas", ",", "inputs", ",", "kwargs", ")", "\n", "return", "self", ".", "gather", "(", "outputs", ",", "self", ".", "output_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.DataParallelImbalance.scatter_imbalance": [[113, 115], ["data_parallel.scatter_kwargs_imbalance"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.scatter_kwargs_imbalance"], ["", "def", "scatter_imbalance", "(", "self", ",", "inputs", ",", "kwargs", ",", "device_ids", ")", ":", "\n", "        ", "return", "scatter_kwargs_imbalance", "(", "inputs", ",", "kwargs", ",", "device_ids", ",", "dim", "=", "self", ".", "dim", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.scatter_imbalance": [[8, 58], ["isinstance", "data_parallel.scatter_imbalance.scatter_map"], "function", ["None"], ["def", "scatter_imbalance", "(", "inputs", ",", "target_gpus", ",", "dim", "=", "0", ")", ":", "\n", "    ", "r\"\"\"\n    Slices tensors into approximately equal chunks and\n    distributes them across given GPUs. Duplicates\n    references to objects that are not tensors.\n    \"\"\"", "\n", "def", "scatter_map", "(", "obj", ")", ":", "\n", "        ", "if", "isinstance", "(", "obj", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "if", "(", "len", "(", "target_gpus", ")", "==", "4", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "22", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "4", ",", "6", ",", "6", ",", "6", ")", ",", "dim", ",", "obj", ")", "\n", "", "if", "(", "len", "(", "target_gpus", ")", "==", "4", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "60", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "4", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "144", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "24", ",", "40", ",", "40", ",", "40", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "46", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "4", ",", "6", ",", "6", ",", "6", ",", "6", ",", "6", ",", "6", ",", "6", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "62", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "6", ",", "8", ",", "8", ",", "8", ",", "8", ",", "8", ",", "8", ",", "8", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "94", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "10", ",", "12", ",", "12", ",", "12", ",", "12", ",", "12", ",", "12", ",", "12", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "110", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "12", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "118", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "13", ",", "15", ",", "15", ",", "15", ",", "15", ",", "15", ",", "15", ",", "15", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "126", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "14", ",", "16", ",", "16", ",", "16", ",", "16", ",", "16", ",", "16", ",", "16", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "134", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "15", ",", "17", ",", "17", ",", "17", ",", "17", ",", "17", ",", "17", ",", "17", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "142", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "16", ",", "18", ",", "18", ",", "18", ",", "18", ",", "18", ",", "18", ",", "18", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "16", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "222", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "12", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ")", ",", "dim", ",", "obj", ")", "\n", "", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "None", ",", "dim", ",", "obj", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "tuple", ")", "and", "len", "(", "obj", ")", ">", "0", ":", "\n", "            ", "return", "list", "(", "zip", "(", "*", "map", "(", "scatter_map", ",", "obj", ")", ")", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "list", ")", "and", "len", "(", "obj", ")", ">", "0", ":", "\n", "            ", "return", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "map", "(", "scatter_map", ",", "obj", ")", ")", ")", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "dict", ")", "and", "len", "(", "obj", ")", ">", "0", ":", "\n", "            ", "return", "list", "(", "map", "(", "type", "(", "obj", ")", ",", "zip", "(", "*", "map", "(", "scatter_map", ",", "obj", ".", "items", "(", ")", ")", ")", ")", ")", "\n", "", "return", "[", "obj", "for", "targets", "in", "target_gpus", "]", "\n", "\n", "# After scatter_map is called, a scatter_map cell will exist. This cell", "\n", "# has a reference to the actual function scatter_map, which has references", "\n", "# to a closure that has a reference to the scatter_map cell (because the", "\n", "# fn is recursive). To avoid this reference cycle, we set the function to", "\n", "# None, clearing the cell", "\n", "", "try", ":", "\n", "        ", "return", "scatter_map", "(", "inputs", ")", "\n", "", "finally", ":", "\n", "        ", "scatter_map", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.scatter_kwargs_imbalance": [[60, 71], ["tuple", "tuple", "data_parallel.scatter_imbalance", "data_parallel.scatter_imbalance", "len", "len", "tuple.extend", "len", "len", "tuple.extend", "range", "range", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.scatter_imbalance", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.nn.data_parallel.scatter_imbalance"], ["", "", "def", "scatter_kwargs_imbalance", "(", "inputs", ",", "kwargs", ",", "target_gpus", ",", "dim", "=", "0", ")", ":", "\n", "    ", "r\"\"\"Scatter with support for kwargs dictionary\"\"\"", "\n", "inputs", "=", "scatter_imbalance", "(", "inputs", ",", "target_gpus", ",", "dim", ")", "if", "inputs", "else", "[", "]", "\n", "kwargs", "=", "scatter_imbalance", "(", "kwargs", ",", "target_gpus", ",", "dim", ")", "if", "kwargs", "else", "[", "]", "\n", "if", "len", "(", "inputs", ")", "<", "len", "(", "kwargs", ")", ":", "\n", "        ", "inputs", ".", "extend", "(", "[", "(", ")", "for", "_", "in", "range", "(", "len", "(", "kwargs", ")", "-", "len", "(", "inputs", ")", ")", "]", ")", "\n", "", "elif", "len", "(", "kwargs", ")", "<", "len", "(", "inputs", ")", ":", "\n", "        ", "kwargs", ".", "extend", "(", "[", "{", "}", "for", "_", "in", "range", "(", "len", "(", "inputs", ")", "-", "len", "(", "kwargs", ")", ")", "]", ")", "\n", "", "inputs", "=", "tuple", "(", "inputs", ")", "\n", "kwargs", "=", "tuple", "(", "kwargs", ")", "\n", "return", "inputs", ",", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__init__": [[58, 99], ["super().__init__", "print", "open", "open", "zip", "len", "src.split.split.split", "tgt.split.split.split", "f_check.split", "tokenizer.tokenize", "tokenizer.tokenize", "tokenizer.tokenize", "range", "src[].strip", "tgt[].strip", "check[].strip", "len", "len", "len", "len", "src_tk_list.append", "tgt_tk_list.append", "check_tk_list.append", "PPL_loader.C_Seq2SeqDataset.ex_list.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["def", "__init__", "(", "self", ",", "file_src", ",", "file_tgt", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "\n", "sent_reverse_order", "=", "False", ",", "bi_uni_pipeline", "=", "[", "]", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_uni_pipeline", "=", "bi_uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "\t\t\t", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "\n", "\t\t\t\t", "f_check", "=", "\".\"", "\n", "\n", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "\t\t\t\t\t", "src", "=", "src", ".", "split", "(", "\"[SEP]\"", ")", "\n", "tgt", "=", "tgt", ".", "split", "(", "\"[SEP]\"", ")", "\n", "check", "=", "f_check", ".", "split", "(", "\"[SEP]\"", ")", "\n", "\n", "\n", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "check_tk", "=", "tokenizer", ".", "tokenize", "(", "check", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "assert", "len", "(", "check_tk", ")", ">", "0", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "tgt_tk", ")", ")", ":", "\n", "\t\t\t\t\t\t", "src_tk_list", "=", "[", "]", "\n", "tgt_tk_list", "=", "[", "]", "\n", "check_tk_list", "=", "[", "]", "\n", "\n", "src_tk_list", ".", "append", "(", "src_tk", ")", "\n", "tgt_tk_list", ".", "append", "(", "tgt_tk", "[", ":", "t", "+", "1", "]", ")", "\n", "check_tk_list", ".", "append", "(", "check_tk", ")", "\n", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk_list", ",", "tgt_tk_list", ",", "check_tk_list", ")", ")", "\n", "", "", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__len__": [[100, 102], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "\t\t", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__getitem__": [[103, 108], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\t\t", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "bi_uni_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__iter__": [[109, 117], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "loader_utils.batch_list_to_batch_tensors", "len", "float", "PPL_loader.C_Seq2SeqDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Seq2SeqDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "\t\t", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "\t\t\t", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "\t\t\t\t", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Preprocess4Seq2seq.__init__": [[122, 154], ["loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "\n", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "\n", "mask_source_words", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "\n", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.C_Preprocess4Seq2seq.__call__": [[155, 337], ["range", "torch.stack", "torch.stack", "torch.stack", "PPL_loader.C_Preprocess4Seq2seq.indexer", "PPL_loader.C_Preprocess4Seq2seq.extend", "torch.tensor", "torch.tensor", "PPL_loader.truncate_tokens_pair", "PPL_loader.C_Preprocess4Seq2seq.indexer", "PPL_loader.C_Preprocess4Seq2seq.extend", "segment_ids.extend", "torch.zeros", "enumerate", "tgt_pos.extend", "input_ids_list.append", "segment_ids_list.append", "torch.stack.append", "masked_ids_list.append", "masked_pos_list.append", "masked_weights_list.append", "tgt_pos_list.append", "torch.stack.append", "torch.stack.append", "check_ids_list.append", "len", "len", "len", "tokens_a.extend", "min", "PPL_loader.C_Preprocess4Seq2seq.indexer", "len", "min", "set", "enumerate", "max", "list", "PPL_loader.C_Preprocess4Seq2seq.indexer", "len", "mask_qkv.extend", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].copy_", "len", "len", "len", "len", "max", "len", "random.random.shuffle", "len", "PPL_loader.C_Preprocess4Seq2seq.extend", "list.extend", "masked_weights.extend", "tgt_pos.append", "len", "len", "range", "int", "cand_pos.append", "random.random.random", "len", "len", "len", "len", "len", "round", "cand_pos.append", "set.add", "random.random.random", "loader_utils.get_random_word", "len", "len", "len", "len", "len", "len", "len", "len", "tk.startswith", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.get_random_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "\t\t", "input_ids_list", "=", "[", "]", "\n", "segment_ids_list", "=", "[", "]", "\n", "input_mask_list", "=", "[", "]", "\n", "masked_ids_list", "=", "[", "]", "\n", "masked_pos_list", "=", "[", "]", "\n", "masked_weights_list", "=", "[", "]", "\n", "tgt_pos_list", "=", "[", "]", "\n", "labels_list", "=", "[", "]", "\n", "ks_labels_list", "=", "[", "]", "\n", "check_ids_list", "=", "[", "]", "\n", "\n", "tokens_a_list", ",", "tokens_b_list", ",", "check_list", "=", "instance", "[", ":", "3", "]", "\n", "\n", "\n", "for", "rank", "in", "range", "(", "TopK", ")", ":", "\n", "\n", "\t\t\t", "tokens_a", "=", "tokens_a_list", "[", "rank", "]", "\n", "tokens_b", "=", "tokens_b_list", "[", "rank", "]", "\n", "check_tokens", "=", "check_list", "[", "rank", "]", "[", ":", "self", ".", "max_pred", "]", "\n", "\n", "#######", "\n", "check_ids", "=", "self", ".", "indexer", "(", "check_tokens", ")", "\n", "# Zero Padding", "\n", "check_n_pad", "=", "self", ".", "max_pred", "-", "len", "(", "check_ids", ")", "\n", "check_ids", ".", "extend", "(", "[", "0", "]", "*", "check_n_pad", ")", "\n", "assert", "len", "(", "check_ids", ")", "==", "self", ".", "max_pred", "\n", "########", "\n", "\n", "tokens_a", "=", "[", "\".\"", "]", "+", "tokens_a", "[", ":", "-", "1", "]", "\n", "\n", "labels", "=", "torch", ".", "tensor", "(", "0.1", ")", "\n", "ks_labels", "=", "torch", ".", "tensor", "(", "1", ")", "\n", "tokens_b", "=", "tokens_b", "\n", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t\t", "tokens_b", "=", "[", "'[S2S_SOS]'", "]", "+", "tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "", "num_truncated_a", ",", "_", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "\n", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "#process tokens_a all_len ==  213; tokens_b max len = 40", "\n", "tokens_a", "=", "tokens_a", "[", ":", "213", "]", "\n", "while", "len", "(", "tokens_a", ")", "<", "213", ":", "\n", "\t\t\t\t", "tokens_a", ".", "extend", "(", "[", "\"[PAD]\"", "]", ")", "\n", "", "tokens_b", "=", "tokens_b", "[", ":", "40", "]", "\n", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "\t\t\t\t", "tokens", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "", "else", ":", "\n", "\t\t\t\t", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "\t\t\t\t", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t\t", "if", "self", ".", "s2s_add_segment", ":", "\n", "\t\t\t\t\t\t", "if", "self", ".", "s2s_share_segment", ":", "\n", "\t\t\t\t\t\t\t", "segment_ids", "=", "[", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t\t", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "\t\t\t\t", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", ")", "\n", "masked_pos", "=", "[", "len", "(", "tokens_a", ")", "+", "2", "+", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", ")", "]", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "[", "1", ":", "]", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "\t\t\t\t", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "if", "self", ".", "mask_source_words", ":", "\n", "\t\t\t\t\t", "effective_length", "+=", "len", "(", "tokens_a", ")", "\n", "", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol", "\n", "\t\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "\t\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "\n", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "\t\t\t\t\t\t", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "list", "(", "[", "max_cand_pos", "-", "1", "]", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "\t\t\t\t\t", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "\t\t\t\t\t", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "\t\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "\t\t\t\t\t\t", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "\t\t\t\t", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "\t\t\t\t", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "second_st", ",", "second_end", "=", "len", "(", "\n", "tokens_a", ")", "+", "2", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "\t\t\t\t", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "tgt_pos", "=", "[", "]", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "\t\t\t\t", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", "and", "tk", "!=", "'[SEP]'", ")", ":", "\n", "\t\t\t\t\t", "tgt_pos", ".", "append", "(", "i", ")", "\n", "\n", "", "", "tgt_pos", "=", "tgt_pos", "[", ":", "len", "(", "masked_pos", ")", "]", "\n", "tgt_pad", "=", "len", "(", "masked_pos", ")", "-", "len", "(", "tgt_pos", ")", "\n", "tgt_pos", ".", "extend", "(", "[", "0", "]", "*", "tgt_pad", ")", "\n", "\n", "input_ids_list", ".", "append", "(", "input_ids", ")", "\n", "segment_ids_list", ".", "append", "(", "segment_ids", ")", "\n", "input_mask_list", ".", "append", "(", "input_mask", ")", "\n", "masked_ids_list", ".", "append", "(", "masked_ids", ")", "\n", "masked_pos_list", ".", "append", "(", "masked_pos", ")", "\n", "masked_weights_list", ".", "append", "(", "masked_weights", ")", "\n", "tgt_pos_list", ".", "append", "(", "tgt_pos", ")", "\n", "labels_list", ".", "append", "(", "labels", ")", "\n", "ks_labels_list", ".", "append", "(", "ks_labels", ")", "\n", "check_ids_list", ".", "append", "(", "check_ids", ")", "\n", "\n", "\n", "", "input_mask_list", "=", "torch", ".", "stack", "(", "input_mask_list", ")", "\n", "labels_list", "=", "torch", ".", "stack", "(", "labels_list", ")", "\n", "ks_labels_list", "=", "torch", ".", "stack", "(", "ks_labels_list", ")", "\n", "\n", "return", "(", "input_ids_list", ",", "segment_ids_list", ",", "input_mask_list", ",", "mask_qkv", ",", "masked_ids_list", ",", "masked_pos_list", ",", "masked_weights_list", ",", "-", "1", ",", "self", ".", "task_idx", ",", "\n", "tgt_pos_list", ",", "labels_list", ",", "ks_labels_list", ",", "check_ids_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_loader.truncate_tokens_pair": [[17, 53], ["trunc_tokens.pop", "len", "len", "len", "random.random", "len", "len", "len"], "function", ["None"], ["def", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_len", ",", "max_len_a", "=", "0", ",", "max_len_b", "=", "0", ",", "trunc_seg", "=", "None", ",", "always_truncate_tail", "=", "False", ")", ":", "\n", "\t", "num_truncated_a", "=", "[", "0", ",", "0", "]", "\n", "num_truncated_b", "=", "[", "0", ",", "0", "]", "\n", "while", "True", ":", "\n", "\t\t", "if", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "<=", "max_len", ":", "\n", "\t\t\t", "break", "\n", "", "if", "(", "max_len_a", ">", "0", ")", "and", "len", "(", "tokens_a", ")", ">", "max_len_a", ":", "\n", "\t\t\t", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "elif", "(", "max_len_b", ">", "0", ")", "and", "len", "(", "tokens_b", ")", ">", "max_len_b", ":", "\n", "\t\t\t", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "elif", "trunc_seg", ":", "\n", "# truncate the specified segment", "\n", "\t\t\t", "if", "trunc_seg", "==", "'a'", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "", "else", ":", "\n", "# truncate the longer segment", "\n", "\t\t\t", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "\t\t\t\t", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "# whether always truncate source sequences", "\n", "", "", "if", "(", "not", "always_truncate_tail", ")", "and", "(", "rand", "(", ")", "<", "0.5", ")", ":", "\n", "\t\t\t", "del", "trunc_tokens", "[", "0", "]", "\n", "num_truncated", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "\t\t\t", "trunc_tokens", ".", "pop", "(", ")", "\n", "num_truncated", "[", "1", "]", "+=", "1", "\n", "", "", "return", "num_truncated_a", ",", "num_truncated_b", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL._get_max_epoch_model": [[39, 50], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["None"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "\t", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "\t\t", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "\t\t", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "\t\t", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL.main": [[52, 485], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "pathlib.Path().exists", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.Formatter", "logging.FileHandler.setFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logger.addHandler", "logger.addHandler", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_bert.tokenization.BertTokenizer.from_pretrained", "logger.info", "os.path.join", "os.path.join", "PPL_loader.C_Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "PPL._get_max_epoch_model", "PPL_modeling.BertForPreTrainingLossMask.from_pretrained", "nn.data_parallel.DataParallelImbalance.to", "list", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "os.path.join", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.barrier", "pytorch_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "PPL_loader.C_Preprocess4Seq2seq", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "amp.init", "logger.info", "torch.barrier", "logger.info", "torch.load", "torch.load", "torch.barrier", "nn.data_parallel.DataParallelImbalance.half", "DDP", "nn.data_parallel.DataParallelImbalance.named_parameters", "FusedAdam", "pytorch_bert.optimization.BertAdam", "logger.info", "torch.load", "torch.load", "hasattr", "FP16_Optimizer_State.load_state_dict", "nn.data_parallel.DataParallelImbalance.train", "tqdm.trange", "pathlib.Path", "os.path.join", "bool", "list", "torch.get_world_size", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "FP16_Optimizer_State", "FP16_Optimizer_State", "optim_recover.state_dict.state_dict", "logger.info", "logger.info", "logger.info", "tqdm.tqdm", "enumerate", "logger.info", "logger.info", "BertTokenizer.from_pretrained.vocab.keys", "ImportError", "ImportError", "int", "train_sampler.set_epoch", "train_sampler.set_epoch", "torch.cuda.is_available", "torch.cuda.is_available", "any", "torch.no_grad", "torch.no_grad", "nn.data_parallel.DataParallelImbalance.", "masked_lm_loss.mean.item", "numpy.exp", "any", "t.to", "masked_lm_loss.mean.mean"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL._get_max_epoch_model", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict"], ["", "", "def", "main", "(", ")", ":", "\n", "\t", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--dev_src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--predict_bleu\"", ",", "\n", "default", "=", "0.5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The Predicted Bleu for KS Predict \"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_vae\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to train vae.\"", ")", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_predict\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run ks predict.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion_step\"", ",", "\n", "default", "=", "300", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "Path", "(", "args", ".", "model_recover_path", ")", ".", "exists", "(", "\n", ")", ",", "\"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "handler", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "log_dir", ",", "\"train.log\"", ")", ",", "encoding", "=", "'UTF-8'", ")", "\n", "handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s - %(name)s - %(levelname)s - %(message)s'", ")", "\n", "handler", ".", "setFormatter", "(", "formatter", ")", "\n", "\n", "console", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "logger", ".", "addHandler", "(", "handler", ")", "\n", "logger", ".", "addHandler", "(", "console", ")", "\n", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "\t\t", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "\t\t", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "\t\t", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "\t\t", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "\t\t", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "\n", "\n", "", "C_bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "C_Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "\n", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "\n", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "\n", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "\n", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "\n", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "\n", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "\n", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "\n", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "\n", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "\n", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "\n", "logger", ".", "info", "(", "\"Loading Dataset from {}\"", ".", "format", "(", "args", ".", "data_dir", ")", ")", "\n", "\n", "fn_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "dev_src_file", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "dev_tgt_file", ")", "\n", "dev_reddit_dataset", "=", "seq2seq_loader", ".", "C_Seq2SeqDataset", "(", "\n", "fn_src", ",", "fn_tgt", ",", "args", ".", "eval_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "None", ",", "\n", "bi_uni_pipeline", "=", "C_bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "\t\t", "dev_reddit_sampler", "=", "RandomSampler", "(", "dev_reddit_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "eval_batch_size", "\n", "", "else", ":", "\n", "\t\t", "dev_reddit_sampler", "=", "DistributedSampler", "(", "dev_reddit_dataset", ")", "\n", "_batch_size", "=", "args", ".", "eval_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "dev_reddit_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_reddit_dataset", ",", "batch_size", "=", "_batch_size", ",", "\n", "sampler", "=", "dev_reddit_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "\n", "pin_memory", "=", "False", ")", "\n", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "\n", "amp_handle", "=", "None", "\n", "if", "args", ".", "fp16", "and", "args", ".", "amp", ":", "\n", "\t\t", "from", "apex", "import", "amp", "\n", "amp_handle", "=", "amp", ".", "init", "(", "enable_caching", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "\n", "# Prepare model", "\n", "", "recover_step", "=", "_get_max_epoch_model", "(", "args", ".", "output_dir", ")", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "\n", "\n", "", "if", "args", ".", "model_recover_path", ":", "\n", "\t\t", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "\n", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "\n", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "\n", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "\n", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "\t\t", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "\t\t", "model", ".", "half", "(", ")", "\n", "if", "args", ".", "fp32_embedding", ":", "\n", "\t\t\t", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "\t\t", "try", ":", "\n", "\t\t\t", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "\t\t\t", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "\n", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "# model = torch.nn.DataParallel(model)", "\n", "\t\t", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "\t\t", "try", ":", "\n", "# from apex.optimizers import FP16_Optimizer", "\n", "\t\t\t", "from", "pytorch_bert", ".", "optimization_fp16", "import", "FP16_Optimizer_State", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "\t\t\t", "raise", "ImportError", "(", "\n", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "\t\t\t", "optimizer", "=", "FP16_Optimizer_State", "(", "\n", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "\t\t\t", "optimizer", "=", "FP16_Optimizer_State", "(", "\n", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "", "", "else", ":", "\n", "\t    ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "", "if", "args", ".", "optim_recover_path", "is", "not", "None", ":", "\n", "\t\t", "logger", ".", "info", "(", "\"***** Recover optimizer from : {} *****\"", ".", "format", "(", "args", ".", "optim_recover_path", ")", ")", "\n", "optim_recover", "=", "torch", ".", "load", "(", "args", ".", "optim_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "hasattr", "(", "optim_recover", ",", "'state_dict'", ")", ":", "\n", "\t\t\t", "optim_recover", "=", "optim_recover", ".", "state_dict", "(", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "optim_recover", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "\t\t\t", "logger", ".", "info", "(", "\"***** Recover optimizer: dynamic_loss_scale *****\"", ")", "\n", "optimizer", ".", "dynamic_loss_scale", "=", "True", "\n", "\n", "", "", "logger", ".", "info", "(", "\"***** CUDA.empty_cache() *****\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "\t\t", "model", ".", "train", "(", ")", "\n", "if", "recover_step", ":", "\n", "\t\t\t", "start_epoch", "=", "recover_step", "+", "1", "\n", "", "else", ":", "\n", "\t\t\t", "start_epoch", "=", "1", "\n", "", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", "+", "1", ",", "desc", "=", "\"Epoch\"", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "\t\t\t", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "\t\t\t\t", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"***** Evaling PPL *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "\t\t\t\t", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "", "dev_iter_bar", "=", "tqdm", "(", "dev_reddit_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "total_lm_loss", "=", "0", "\n", "for", "qkr_dev_step", ",", "batch", "in", "enumerate", "(", "dev_iter_bar", ")", ":", "\n", "\t\t\t\t", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "\t\t\t\t\t", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "tgt_pos", ",", "labels", ",", "ks_labels", ",", "check_ids", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "\t\t\t\t\t", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "\n", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "\n", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ",", "tgt_pos", "=", "tgt_pos", ",", "\n", "labels", "=", "labels", ",", "ks_labels", "=", "ks_labels", ",", "check_ids", "=", "check_ids", ")", "\n", "\n", "masked_lm_loss", ",", "next_sentence_loss", ",", "KL_loss", ",", "Mutual_loss", ",", "Golden_loss", ",", "predict_kl_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "\t\t\t\t\t\t", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "", "total_lm_loss", "+=", "masked_lm_loss", ".", "item", "(", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "total_mean_lm_loss", "=", "total_lm_loss", "/", "(", "qkr_dev_step", "+", "1", ")", "\n", "\n", "#logger.info(\"** ** * Evaling mean loss ** ** * \")", "\n", "", "", "logger", ".", "info", "(", "\"PPL is :{}\"", ".", "format", "(", "np", ".", "exp", "(", "total_mean_lm_loss", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"******************************************* \"", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.__init__": [[88, 159], ["isinstance", "json.loads.items", "isinstance", "open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "relax_projection", "=", "0", ",", "\n", "new_pos_ids", "=", "False", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "task_idx", "=", "None", ",", "\n", "fp32_embedding", "=", "False", ",", "\n", "ffn_type", "=", "0", ",", "\n", "label_smoothing", "=", "None", ",", "\n", "num_qkv", "=", "0", ",", "\n", "seg_emb", "=", "False", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "relax_projection", "=", "relax_projection", "\n", "self", ".", "new_pos_ids", "=", "new_pos_ids", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "task_idx", "=", "task_idx", "\n", "self", ".", "fp32_embedding", "=", "fp32_embedding", "\n", "self", ".", "ffn_type", "=", "ffn_type", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "seg_emb", "=", "seg_emb", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.from_dict": [[161, 168], ["PPL_modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.from_json_file": [[169, 175], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.__repr__": [[176, 178], ["str", "PPL_modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.to_dict": [[179, 183], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.to_json_string": [[184, 187], ["json.dumps", "PPL_modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.PositionalEmbedding.__init__": [[211, 218], ["torch.nn.Module.__init__", "PPL_modeling.PositionalEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "demb", ")", ":", "\n", "        ", "super", "(", "PositionalEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "demb", "=", "demb", "\n", "\n", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "torch", ".", "arange", "(", "0.0", ",", "demb", ",", "2.0", ")", "/", "demb", ")", ")", "\n", "self", ".", "register_buffer", "(", "'inv_freq'", ",", "inv_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.PositionalEmbedding.forward": [[219, 227], ["torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pos_emb[].expand", "torch.ger.sin", "torch.ger.sin", "torch.ger.cos", "torch.ger.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pos_seq", ",", "bsz", "=", "None", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "ger", "(", "pos_seq", ",", "self", ".", "inv_freq", ")", "\n", "pos_emb", "=", "torch", ".", "cat", "(", "[", "sinusoid_inp", ".", "sin", "(", ")", ",", "sinusoid_inp", ".", "cos", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "bsz", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertEmbeddings.__init__": [[233, 255], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "hasattr", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "hasattr", "(", "config", ",", "'fp32_embedding'", ")", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "", "else", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "False", "\n", "\n", "", "if", "hasattr", "(", "config", ",", "'new_pos_ids'", ")", "and", "config", ".", "new_pos_ids", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "4", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "1", "\n", "", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", "*", "self", ".", "num_pos_emb", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertEmbeddings.get_position_token_type_embedding": [[257, 286], ["input_ids.size", "PPL_modeling.BertEmbeddings.position_embeddings", "PPL_modeling.BertEmbeddings.token_type_embeddings", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "PPL_modeling.BertEmbeddings.word_embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertEmbeddings.word_embeddings", "PPL_modeling.BertEmbeddings.size", "PPL_modeling.BertEmbeddings.size", "PPL_modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "words_embeddings[].unsqueeze", "latent_z.type_as", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "get_position_token_type_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "if", "relace_embeddings", "==", "True", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "words_embeddings", "=", "torch", ".", "cat", "(", "(", "words_embeddings", "[", ":", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ",", "latent_z", ".", "type_as", "(", "words_embeddings", ")", ",", "\n", "words_embeddings", "[", ":", ",", "2", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "#print(\"replace latent_z\")", "\n", "", "else", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "embeddings", "=", "position_embeddings", "+", "token_type_embeddings", "\n", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertEmbeddings.get_word_embedding": [[288, 293], ["PPL_modeling.BertEmbeddings.word_embeddings"], "methods", ["None"], ["", "def", "get_word_embedding", "(", "self", ",", "input_ids", ")", ":", "\n", "\n", "        ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "return", "words_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertEmbeddings.forward": [[295, 326], ["input_ids.size", "PPL_modeling.BertEmbeddings.position_embeddings", "PPL_modeling.BertEmbeddings.token_type_embeddings", "PPL_modeling.BertEmbeddings.LayerNorm", "PPL_modeling.BertEmbeddings.dropout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "PPL_modeling.BertEmbeddings.word_embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertEmbeddings.word_embeddings", "PPL_modeling.BertEmbeddings.size", "PPL_modeling.BertEmbeddings.size", "embeddings.half.half.half", "PPL_modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "words_embeddings[].unsqueeze", "latent_z.type_as", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "if", "relace_embeddings", "==", "True", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "words_embeddings", "=", "torch", ".", "cat", "(", "(", "words_embeddings", "[", ":", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ",", "latent_z", ".", "type_as", "(", "words_embeddings", ")", ",", "words_embeddings", "[", ":", ",", "2", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "#print(\"replace latent_z\")", "\n", "", "else", ":", "\n", "            ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.__init__": [[329, 367], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "ValueError", "hasattr", "os.getenv", "PPL_modeling.BertSelfAttention.register_buffer", "hasattr", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Embedding", "torch.nn.Embedding", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "\n", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "if", "hasattr", "(", "config", ",", "'num_qkv'", ")", "and", "(", "config", ".", "num_qkv", ">", "1", ")", ":", "\n", "            ", "self", ".", "num_qkv", "=", "config", ".", "num_qkv", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_qkv", "=", "1", "\n", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "\n", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n", "self", ".", "uni_debug_flag", "=", "True", "if", "os", ".", "getenv", "(", "\n", "'UNI_DEBUG_FLAG'", ",", "''", ")", "else", "False", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'debug_attention_probs'", ",", "\n", "torch", ".", "zeros", "(", "(", "512", ",", "512", ")", ")", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'seg_emb'", ")", "and", "config", ".", "seg_emb", ":", "\n", "            ", "self", ".", "b_q_s", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "1", ",", "self", ".", "num_attention_heads", ",", "1", ",", "self", ".", "attention_head_size", ")", ")", "\n", "self", ".", "seg_emb", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "self", ".", "all_head_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "b_q_s", "=", "None", "\n", "self", ".", "seg_emb", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores": [[368, 392], ["x.gather().squeeze.gather().squeeze.permute", "x.gather().squeeze.gather().squeeze.view", "x.gather().squeeze.gather().squeeze.view", "isinstance", "x.gather().squeeze.gather().squeeze.size", "x.gather().squeeze.gather().squeeze.gather().squeeze", "x.gather().squeeze.gather().squeeze.size", "mask_qkv.size", "x.gather().squeeze.gather().squeeze.gather", "mask_qkv.view().expand", "mask_qkv.view"], "methods", ["None"], ["", "", "def", "transpose_for_scores", "(", "self", ",", "x", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_qkv", ",", "\n", "self", ".", "num_attention_heads", ",", "self", ".", "all_head_size", ")", "\n", "# (batch, pos, num_qkv, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "if", "mask_qkv", "is", "None", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "0", ",", ":", ",", ":", "]", "\n", "", "elif", "isinstance", "(", "mask_qkv", ",", "int", ")", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "mask_qkv", ",", ":", ",", ":", "]", "\n", "", "else", ":", "\n", "# mask_qkv: (batch, pos)", "\n", "                ", "if", "mask_qkv", ".", "size", "(", "1", ")", ">", "sz", "[", "1", "]", ":", "\n", "                    ", "mask_qkv", "=", "mask_qkv", "[", ":", ",", ":", "sz", "[", "1", "]", "]", "\n", "# -> x: (batch, pos, head, head_hid)", "\n", "", "x", "=", "x", ".", "gather", "(", "2", ",", "mask_qkv", ".", "view", "(", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "1", ",", "1", ")", ".", "expand", "(", "\n", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "sz", "[", "3", "]", ",", "sz", "[", "4", "]", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "", "", "else", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "# (batch, head, pos, head_hid)", "\n", "", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.forward": [[393, 445], ["PPL_modeling.BertSelfAttention.transpose_for_scores", "PPL_modeling.BertSelfAttention.transpose_for_scores", "PPL_modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "PPL_modeling.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "PPL_modeling.BertSelfAttention.query", "PPL_modeling.BertSelfAttention.key", "PPL_modeling.BertSelfAttention.value", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertSelfAttention.query", "PPL_modeling.BertSelfAttention.key", "PPL_modeling.BertSelfAttention.value", "PPL_modeling.BertSelfAttention.transpose", "PPL_modeling.BertSelfAttention.seg_emb", "seg_rep.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.nn.Softmax", "torch.nn.Softmax", "PPL_modeling.BertSelfAttention.size", "PPL_modeling.BertSelfAttention.debug_attention_probs[].copy_", "math.sqrt", "seg_rep.view.view.size", "seg_rep.view.view.size", "attention_probs[].mean().view", "context_layer.view.view.permute", "context_layer.view.view.size", "attention_probs[].mean"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "if", "history_states", "is", "None", ":", "\n", "            ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "", "else", ":", "\n", "            ", "x_states", "=", "torch", ".", "cat", "(", "(", "history_states", ",", "hidden_states", ")", ",", "dim", "=", "1", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "x_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "x_states", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ",", "mask_qkv", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ",", "mask_qkv", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ",", "mask_qkv", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "# (batch, head, pos, pos)", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "\n", "query_layer", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "if", "self", ".", "seg_emb", "is", "not", "None", ":", "\n", "            ", "seg_rep", "=", "self", ".", "seg_emb", "(", "seg_ids", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "seg_rep", "=", "seg_rep", ".", "view", "(", "seg_rep", ".", "size", "(", "0", ")", ",", "seg_rep", ".", "size", "(", "\n", "1", ")", ",", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "qs", "=", "torch", ".", "einsum", "(", "'bnih,bjnh->bnij'", ",", "\n", "query_layer", "+", "self", ".", "b_q_s", ",", "seg_rep", ")", "\n", "attention_scores", "=", "attention_scores", "+", "qs", "\n", "\n", "# attention_scores = attention_scores / math.sqrt(self.attention_head_size)", "\n", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "_pos", "=", "attention_probs", ".", "size", "(", "-", "1", ")", "\n", "self", ".", "debug_attention_probs", "[", ":", "_pos", ",", ":", "_pos", "]", ".", "copy_", "(", "\n", "attention_probs", "[", "0", "]", ".", "mean", "(", "0", ")", ".", "view", "(", "_pos", ",", "_pos", ")", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", "\n", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfOutput.__init__": [[448, 453], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertSelfOutput.forward": [[454, 459], ["PPL_modeling.BertSelfOutput.dense", "PPL_modeling.BertSelfOutput.dropout", "PPL_modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertAttention.__init__": [[462, 466], ["torch.nn.Module.__init__", "PPL_modeling.BertSelfAttention", "PPL_modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertAttention.forward": [[467, 472], ["PPL_modeling.BertAttention.self", "PPL_modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "\n", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertIntermediate.__init__": [[475, 480], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertIntermediate.forward": [[481, 485], ["PPL_modeling.BertIntermediate.dense", "PPL_modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertOutput.__init__": [[488, 493], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertOutput.forward": [[494, 499], ["PPL_modeling.BertOutput.dense", "PPL_modeling.BertOutput.dropout", "PPL_modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerFFN.__init__": [[502, 514], ["torch.nn.Module.__init__", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TransformerFFN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "assert", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", "\n", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "wx0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "2", ",", ")", ":", "\n", "            ", "self", ".", "wx1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "output", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerFFN.forward": [[515, 526], ["PPL_modeling.TransformerFFN.dropout", "PPL_modeling.TransformerFFN.LayerNorm", "PPL_modeling.TransformerFFN.wx0", "PPL_modeling.TransformerFFN.output", "PPL_modeling.TransformerFFN.wx1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "x0", "=", "self", ".", "wx0", "(", "x", ")", "\n", "if", "self", ".", "ffn_type", "==", "1", ":", "\n", "                ", "x1", "=", "x", "\n", "", "elif", "self", ".", "ffn_type", "==", "2", ":", "\n", "                ", "x1", "=", "self", ".", "wx1", "(", "x", ")", "\n", "", "out", "=", "self", ".", "output", "(", "x0", "*", "x1", ")", "\n", "", "out", "=", "self", ".", "dropout", "(", "out", ")", "\n", "out", "=", "self", ".", "LayerNorm", "(", "out", "+", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertLayer.__init__": [[529, 538], ["torch.nn.Module.__init__", "PPL_modeling.BertAttention", "PPL_modeling.TransformerFFN", "PPL_modeling.BertIntermediate", "PPL_modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "self", ".", "ffn", "=", "TransformerFFN", "(", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertLayer.forward": [[539, 548], ["PPL_modeling.BertLayer.attention", "PPL_modeling.BertLayer.ffn", "PPL_modeling.BertLayer.intermediate", "PPL_modeling.BertLayer.output"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "layer_output", "=", "self", ".", "ffn", "(", "attention_output", ")", "\n", "", "else", ":", "\n", "            ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertEncoder.__init__": [[551, 556], ["torch.nn.Module.__init__", "PPL_modeling.BertLayer", "torch.nn.ModuleList", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertEncoder.forward": [[557, 580], ["enumerate", "all_encoder_layers.append", "layer_module", "layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "# history embedding and encoded layer must be simultanously given", "\n", "        ", "assert", "(", "prev_embedding", "is", "None", ")", "==", "(", "prev_encoded_layers", "is", "None", ")", "\n", "\n", "all_encoder_layers", "=", "[", "]", "\n", "if", "(", "prev_embedding", "is", "not", "None", ")", "and", "(", "prev_encoded_layers", "is", "not", "None", ")", ":", "\n", "            ", "history_states", "=", "prev_embedding", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "if", "prev_encoded_layers", "is", "not", "None", ":", "\n", "                    ", "history_states", "=", "prev_encoded_layers", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPooler.__init__": [[583, 587], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPooler.forward": [[588, 595], ["PPL_modeling.BertPooler.dense", "PPL_modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPredictionHeadTransform.__init__": [[598, 607], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "isinstance", "hasattr"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "hid_size", "=", "config", ".", "hidden_size", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "hid_size", "*=", "config", ".", "relax_projection", "\n", "", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "hid_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "hid_size", ",", "eps", "=", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPredictionHeadTransform.forward": [[608, 613], ["PPL_modeling.BertPredictionHeadTransform.dense", "PPL_modeling.BertPredictionHeadTransform.transform_act_fn", "PPL_modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertLMPredictionHead.__init__": [[616, 641], ["torch.nn.Module.__init__", "PPL_modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "hasattr", "bert_model_embedding_weights.size", "tensor.half"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "self", ".", "relax_projection", "=", "config", ".", "relax_projection", "\n", "", "else", ":", "\n", "            ", "self", ".", "relax_projection", "=", "0", "\n", "", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "\n", "def", "convert_to_type", "(", "tensor", ")", ":", "\n", "            ", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "return", "tensor", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                ", "return", "tensor", "\n", "", "", "self", ".", "type_converter", "=", "convert_to_type", "\n", "self", ".", "converted", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertLMPredictionHead.forward": [[642, 660], ["PPL_modeling.BertLMPredictionHead.transform", "PPL_modeling.BertLMPredictionHead.type_converter", "torch.linear.size", "torch.linear.size", "torch.linear", "torch.linear", "PPL_modeling.BertLMPredictionHead.transform.half", "torch.linear.view", "PPL_modeling.BertLMPredictionHead.type_converter", "PPL_modeling.BertLMPredictionHead.type_converter", "PPL_modeling.BertLMPredictionHead.type_converter", "PPL_modeling.BertLMPredictionHead.decoder", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "if", "not", "self", ".", "converted", ":", "\n", "            ", "self", ".", "converted", "=", "True", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "self", ".", "transform", ".", "half", "(", ")", "\n", "", "", "hidden_states", "=", "self", ".", "transform", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ")", "\n", "if", "self", ".", "relax_projection", ">", "1", ":", "\n", "            ", "num_batch", "=", "hidden_states", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "hidden_states", ".", "size", "(", "1", ")", "\n", "# (batch, num_pos, relax_projection*hid) -> (batch, num_pos, relax_projection, hid) -> (batch, num_pos, hid)", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "relax_projection", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "hidden_states", "=", "F", ".", "linear", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ",", "self", ".", "type_converter", "(", "\n", "self", ".", "decoder", ".", "weight", ")", ",", "self", ".", "type_converter", "(", "self", ".", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertOnlyMLMHead.__init__": [[663, 667], ["torch.nn.Module.__init__", "PPL_modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertOnlyMLMHead.forward": [[668, 671], ["PPL_modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertOnlyNSPHead.__init__": [[674, 677], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertOnlyNSPHead.forward": [[678, 681], ["PPL_modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPreTrainingHeads.__init__": [[684, 689], ["torch.nn.Module.__init__", "PPL_modeling.BertLMPredictionHead", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPreTrainingHeads.forward": [[690, 697], ["PPL_modeling.BertPreTrainingHeads.predictions", "PPL_modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ",", "task_idx", ")", "\n", "if", "pooled_output", "is", "None", ":", "\n", "            ", "seq_relationship_score", "=", "None", "\n", "", "else", ":", "\n", "            ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.PreTrainedBertModel.__init__": [[704, 714], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedBertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.PreTrainedBertModel.init_bert_weights": [[715, 725], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.PreTrainedBertModel.from_pretrained": [[726, 1031], ["os.path.isdir", "PPL_modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "PPL_modeling.PreTrainedBertModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-base-multilingual`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "\n", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "if", "(", "'config_path'", "in", "kwargs", ")", "and", "kwargs", "[", "'config_path'", "]", ":", "\n", "            ", "config_file", "=", "kwargs", "[", "'config_path'", "]", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "\n", "# define new type_vocab_size (there might be different numbers of segment ids)", "\n", "if", "'type_vocab_size'", "in", "kwargs", ":", "\n", "            ", "config", ".", "type_vocab_size", "=", "kwargs", "[", "'type_vocab_size'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'relax_projection'", "in", "kwargs", ")", "and", "kwargs", "[", "'relax_projection'", "]", ":", "\n", "            ", "config", ".", "relax_projection", "=", "kwargs", "[", "'relax_projection'", "]", "\n", "# new position embedding", "\n", "", "if", "(", "'new_pos_ids'", "in", "kwargs", ")", "and", "kwargs", "[", "'new_pos_ids'", "]", ":", "\n", "            ", "config", ".", "new_pos_ids", "=", "kwargs", "[", "'new_pos_ids'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'task_idx'", "in", "kwargs", ")", "and", "kwargs", "[", "'task_idx'", "]", ":", "\n", "            ", "config", ".", "task_idx", "=", "kwargs", "[", "'task_idx'", "]", "\n", "# define new max position embedding for length expansion", "\n", "", "if", "(", "'max_position_embeddings'", "in", "kwargs", ")", "and", "kwargs", "[", "'max_position_embeddings'", "]", ":", "\n", "            ", "config", ".", "max_position_embeddings", "=", "kwargs", "[", "'max_position_embeddings'", "]", "\n", "# use fp32 for embeddings", "\n", "", "if", "(", "'fp32_embedding'", "in", "kwargs", ")", "and", "kwargs", "[", "'fp32_embedding'", "]", ":", "\n", "            ", "config", ".", "fp32_embedding", "=", "kwargs", "[", "'fp32_embedding'", "]", "\n", "# type of FFN in transformer blocks", "\n", "", "if", "(", "'ffn_type'", "in", "kwargs", ")", "and", "kwargs", "[", "'ffn_type'", "]", ":", "\n", "            ", "config", ".", "ffn_type", "=", "kwargs", "[", "'ffn_type'", "]", "\n", "# label smoothing", "\n", "", "if", "(", "'label_smoothing'", "in", "kwargs", ")", "and", "kwargs", "[", "'label_smoothing'", "]", ":", "\n", "            ", "config", ".", "label_smoothing", "=", "kwargs", "[", "'label_smoothing'", "]", "\n", "# dropout", "\n", "", "if", "(", "'hidden_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'hidden_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "hidden_dropout_prob", "=", "kwargs", "[", "'hidden_dropout_prob'", "]", "\n", "", "if", "(", "'attention_probs_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'attention_probs_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "attention_probs_dropout_prob", "=", "kwargs", "[", "'attention_probs_dropout_prob'", "]", "\n", "# different QKV", "\n", "", "if", "(", "'num_qkv'", "in", "kwargs", ")", "and", "kwargs", "[", "'num_qkv'", "]", ":", "\n", "            ", "config", ".", "num_qkv", "=", "kwargs", "[", "'num_qkv'", "]", "\n", "# segment embedding for self-attention", "\n", "", "if", "(", "'seg_emb'", "in", "kwargs", ")", "and", "kwargs", "[", "'seg_emb'", "]", ":", "\n", "            ", "config", ".", "seg_emb", "=", "kwargs", "[", "'seg_emb'", "]", "\n", "# initialize word embeddings", "\n", "", "_word_emb_map", "=", "None", "\n", "if", "(", "'word_emb_map'", "in", "kwargs", ")", "and", "kwargs", "[", "'word_emb_map'", "]", ":", "\n", "            ", "_word_emb_map", "=", "kwargs", "[", "'word_emb_map'", "]", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "\n", "# clean the arguments in kwargs", "\n", "for", "arg_clean", "in", "(", "'config_path'", ",", "'type_vocab_size'", ",", "'relax_projection'", ",", "'new_pos_ids'", ",", "'task_idx'", ",", "'max_position_embeddings'", ",", "'fp32_embedding'", ",", "'ffn_type'", ",", "'label_smoothing'", ",", "'hidden_dropout_prob'", ",", "'attention_probs_dropout_prob'", ",", "'num_qkv'", ",", "'seg_emb'", ",", "'word_emb_map'", ")", ":", "\n", "            ", "if", "arg_clean", "in", "kwargs", ":", "\n", "                ", "del", "kwargs", "[", "arg_clean", "]", "\n", "\n", "# Instantiate model.", "\n", "", "", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# initialize new segment embeddings", "\n", "", "_k", "=", "'bert.embeddings.token_type_embeddings.weight'", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "config", ".", "type_vocab_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] ({0} != {1})\"", ".", "format", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "type_vocab_size", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.type_vocab_size, state_dict[_k].shape[1])", "\n", "                ", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "# L2R", "\n", "if", "config", ".", "type_vocab_size", ">=", "3", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "2", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# R2L", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "4", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "3", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# S2S", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "6", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "4", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "5", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "7", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "6", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "", "elif", "config", ".", "type_vocab_size", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "type_vocab_size", ",", ":", "]", "\n", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "n_config_pos_emb", "=", "4", "if", "config", ".", "new_pos_ids", "else", "1", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_pos_emb", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_pos_emb*config.hidden_size != state_dict[bert.embeddings.position_embeddings.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_pos_emb", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_pos_emb", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_pos_emb", "==", "1", ")", "!=", "(", "n_config_pos_emb", "==", "\n", "1", ")", ",", "\"!!!!n_state_pos_emb == 1 xor n_config_pos_emb == 1!!!!\"", "\n", "if", "n_state_pos_emb", "==", "1", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "\n", "1", ",", "n_config_pos_emb", ",", "1", ")", ".", "reshape", "(", "(", "config", ".", "max_position_embeddings", ",", "n_config_pos_emb", "*", "config", ".", "hidden_size", ")", ")", "\n", "", "elif", "n_config_pos_emb", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "config", ".", "max_position_embeddings", ",", "n_state_pos_emb", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "1", ",", "_task_idx", ")", "\n", "\n", "# initialize new position embeddings", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "if", "_k", "in", "state_dict", "and", "config", ".", "max_position_embeddings", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.max_position_embeddings != state_dict[bert.embeddings.position_embeddings.weight] ({0} - {1})\"", ".", "format", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "max_position_embeddings", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "old_size", "=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.max_position_embeddings, state_dict[_k].shape[1])", "\n", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "start", "=", "old_size", "\n", "while", "start", "<", "config", ".", "max_position_embeddings", ":", "\n", "                    ", "chunk_size", "=", "min", "(", "\n", "old_size", ",", "config", ".", "max_position_embeddings", "-", "start", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "start", ":", "start", "+", "chunk_size", ",", "\n", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "chunk_size", ",", ":", "]", ")", "\n", "start", "+=", "chunk_size", "\n", "", "", "elif", "config", ".", "max_position_embeddings", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "max_position_embeddings", ",", ":", "]", "\n", "\n", "# initialize relax projection", "\n", "", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "n_config_relax", "=", "1", "if", "(", "config", ".", "relax_projection", "<", "\n", "1", ")", "else", "config", ".", "relax_projection", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_relax", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_relax*config.hidden_size != state_dict[cls.predictions.transform.dense.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_relax", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_relax", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_relax", "==", "1", ")", "!=", "(", "n_config_relax", "==", "\n", "1", ")", ",", "\"!!!!n_state_relax == 1 xor n_config_relax == 1!!!!\"", "\n", "if", "n_state_relax", "==", "1", ":", "\n", "                ", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_relax", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_relax", "*", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_relax", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "", "elif", "n_config_relax", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "\n", "# initialize QKV", "\n", "", "", "", "_all_head_size", "=", "config", ".", "num_attention_heads", "*", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "n_config_num_qkv", "=", "1", "if", "(", "config", ".", "num_qkv", "<", "1", ")", "else", "config", ".", "num_qkv", "\n", "for", "qkv_name", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", ":", "\n", "            ", "_k", "=", "'bert.encoder.layer.0.attention.self.{0}.weight'", ".", "format", "(", "\n", "qkv_name", ")", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_num_qkv", "*", "_all_head_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"n_config_num_qkv*_all_head_size != state_dict[_k] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_num_qkv", ",", "_all_head_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "for", "layer_idx", "in", "range", "(", "config", ".", "num_hidden_layers", ")", ":", "\n", "                    ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "_all_head_size", "==", "0", "\n", "n_state_qkv", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "_all_head_size", ")", "\n", "assert", "(", "n_state_qkv", "==", "1", ")", "!=", "(", "n_config_num_qkv", "==", "\n", "1", ")", ",", "\"!!!!n_state_qkv == 1 xor n_config_num_qkv == 1!!!!\"", "\n", "if", "n_state_qkv", "==", "1", ":", "\n", "                        ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_num_qkv", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_num_qkv", "*", "_all_head_size", ",", "_all_head_size", ")", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_num_qkv", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "elif", "n_config_num_qkv", "==", "1", ":", "\n", "                        ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                            ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                            ", "_task_idx", "=", "0", "\n", "", "assert", "_task_idx", "!=", "3", ",", "\"[INVALID] _task_idx=3: n_config_num_qkv=1 (should be 2)\"", "\n", "if", "_task_idx", "==", "0", ":", "\n", "                            ", "_qkv_idx", "=", "0", "\n", "", "else", ":", "\n", "                            ", "_qkv_idx", "=", "1", "\n", "", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "\n", "", "", "", "", "if", "_word_emb_map", ":", "\n", "            ", "_k", "=", "'bert.embeddings.word_embeddings.weight'", "\n", "for", "_tgt", ",", "_src", "in", "_word_emb_map", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "[", "_tgt", ",", ":", "]", ".", "copy_", "(", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "_src", ",", ":", "]", ")", "\n", "\n", "", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "\n", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "load", "(", "model", ",", "prefix", "=", "''", "if", "hasattr", "(", "model", ",", "'bert'", ")", "else", "'bert.'", ")", "\n", "model", ".", "missing_keys", "=", "missing_keys", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'\\n'", ".", "join", "(", "error_msgs", ")", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.__init__": [[1065, 1071], ["PPL_modeling.PreTrainedBertModel.__init__", "PPL_modeling.BertEmbeddings", "PPL_modeling.BertEncoder", "PPL_modeling.BertPooler", "PPL_modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.rescale_some_parameters": [[1072, 1077], ["enumerate", "layer.attention.output.dense.weight.data.div_", "layer.output.dense.weight.data.div_", "math.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "rescale_some_parameters", "(", "self", ")", ":", "\n", "        ", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "encoder", ".", "layer", ")", ":", "\n", "            ", "layer", ".", "attention", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "\n", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "layer", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask": [[1078, 1105], ["torch.ones_like.unsqueeze.to", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "next", "PPL_modeling.BertModel.parameters"], "methods", ["None"], ["", "", "def", "get_extended_attention_mask", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "if", "attention_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "", "elif", "attention_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "return", "extended_attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_embedding": [[1106, 1110], ["PPL_modeling.BertModel.embeddings"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_position_token_type_embedding": [[1111, 1115], ["PPL_modeling.BertModel.embeddings.get_position_token_type_embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_position_token_type_embedding"], ["", "def", "get_position_token_type_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", ".", "get_position_token_type_embedding", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_word_embedding": [[1116, 1120], ["PPL_modeling.BertModel.embeddings.get_word_embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_word_embedding"], ["", "def", "get_word_embedding", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", ".", "get_word_embedding", "(", "\n", "input_ids", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.forward": [[1121, 1155], ["PPL_modeling.BertModel.get_extended_attention_mask", "PPL_modeling.BertModel.embeddings", "PPL_modeling.BertModel.encoder", "PPL_modeling.BertModel.pooler", "PPL_modeling.BertModel.get_extended_attention_mask", "PPL_modeling.BertModel.embeddings", "PPL_modeling.BertModel.encoder", "PPL_modeling.BertModel.pooler"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ",", "\n", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ",", "decode", "=", "None", ",", "prev_embedding", "=", "None", ",", "prev_encoded_layers", "=", "None", ",", "position_ids", "=", "None", ")", ":", "\n", "        ", "if", "decode", "==", "None", ":", "\n", "            ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "                ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n", "", "else", ":", "\n", "            ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "\n", "latent_z", "=", "latent_z", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "\n", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "                ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.__init__": [[1157, 1159], ["PPL_modeling.BertModel.__init__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModelIncr", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding": [[1160, 1164], ["PPL_modeling.BertModelIncr.embeddings"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.forward": [[1165, 1182], ["PPL_modeling.BertModelIncr.get_extended_attention_mask", "PPL_modeling.BertModelIncr.embeddings", "PPL_modeling.BertModelIncr.encoder", "PPL_modeling.BertModelIncr.pooler"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.get_extended_attention_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "\n", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ",", "relace_embeddings", "=", "None", ",", "latent_z", "=", "None", ")", ":", "\n", "        ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "relace_embeddings", ",", "latent_z", "=", "latent_z", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForPreTraining.__init__": [[1229, 1235], ["PPL_modeling.PreTrainedBertModel.__init__", "PPL_modeling.BertModel", "PPL_modeling.BertPreTrainingHeads", "PPL_modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForPreTraining.forward": [[1236, 1252], ["PPL_modeling.BertForPreTraining.bert", "PPL_modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "\n", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPreTrainingPairTransform.__init__": [[1255, 1260], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "# self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-5)", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPreTrainingPairTransform.forward": [[1262, 1268], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertPreTrainingPairTransform.dense", "PPL_modeling.BertPreTrainingPairTransform.transform_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ")", ":", "\n", "        ", "hidden_states", "=", "torch", ".", "cat", "(", "[", "pair_x", ",", "pair_y", "]", ",", "dim", "=", "-", "1", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "# hidden_states = self.LayerNorm(hidden_states)", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPreTrainingPairRel.__init__": [[1271, 1275], ["torch.nn.Module.__init__", "PPL_modeling.BertPreTrainingPairTransform", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_rel", "=", "0", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairRel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "R_xy", "=", "BertPreTrainingPairTransform", "(", "config", ")", "\n", "self", ".", "rel_emb", "=", "nn", ".", "Embedding", "(", "num_rel", ",", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertPreTrainingPairRel.forward": [[1276, 1285], ["PPL_modeling.BertPreTrainingPairRel.R_xy", "PPL_modeling.BertPreTrainingPairRel.rel_emb", "PPL_modeling.BertPreTrainingPairRel.size", "torch.logsigmoid().mul_", "torch.logsigmoid().mul_", "torch.logsigmoid", "torch.logsigmoid", "pair_pos_neg_mask.type_as"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ",", "pair_r", ",", "pair_pos_neg_mask", ")", ":", "\n", "# (batch, num_pair, hidden)", "\n", "        ", "xy", "=", "self", ".", "R_xy", "(", "pair_x", ",", "pair_y", ")", "\n", "r", "=", "self", ".", "rel_emb", "(", "pair_r", ")", "\n", "_batch", ",", "_num_pair", ",", "_hidden", "=", "xy", ".", "size", "(", ")", "\n", "pair_score", "=", "(", "xy", "*", "r", ")", ".", "sum", "(", "-", "1", ")", "\n", "# torch.bmm(xy.view(-1, 1, _hidden),r.view(-1, _hidden, 1)).view(_batch, _num_pair)", "\n", "# .mul_(-1.0): objective to loss", "\n", "return", "F", ".", "logsigmoid", "(", "pair_score", "*", "pair_pos_neg_mask", ".", "type_as", "(", "pair_score", ")", ")", ".", "mul_", "(", "-", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerEncoder.__init__": [[1289, 1294], ["torch.nn.Module.__init__", "PPL_modeling._get_clones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_clones"], ["def", "__init__", "(", "self", ",", "encoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerEncoder.forward": [[1295, 1304], ["mod", "PPL_modeling.TransformerEncoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "output", "=", "src", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "src_mask", "=", "mask", ",", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerEncoderLayer.__init__": [[1306, 1320], ["torch.nn.Module.__init__", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "PPL_modeling._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_activation_fn"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ")", ":", "\n", "        ", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerEncoderLayer.__setstate__": [[1322, 1326], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerEncoderLayer.forward": [[1327, 1336], ["PPL_modeling.TransformerEncoderLayer.norm1", "PPL_modeling.TransformerEncoderLayer.linear2", "PPL_modeling.TransformerEncoderLayer.norm2", "PPL_modeling.TransformerEncoderLayer.self_attn", "PPL_modeling.TransformerEncoderLayer.dropout1", "PPL_modeling.TransformerEncoderLayer.dropout", "PPL_modeling.TransformerEncoderLayer.dropout2", "PPL_modeling.TransformerEncoderLayer.activation", "PPL_modeling.TransformerEncoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "src2", "=", "self", ".", "self_attn", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "src_mask", ",", "\n", "key_padding_mask", "=", "src_key_padding_mask", ")", "[", "0", "]", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm1", "(", "src", ")", "\n", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "src", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm2", "(", "src", ")", "\n", "return", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__init__": [[1359, 1376], ["torch.nn.Module.__init__", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "PPL_modeling._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_activation_fn"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "multihead_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm3", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__setstate__": [[1377, 1381], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.forward": [[1382, 1410], ["PPL_modeling.TransformerDecoderLayer.norm1", "PPL_modeling.TransformerDecoderLayer.norm2", "PPL_modeling.TransformerDecoderLayer.linear2", "PPL_modeling.TransformerDecoderLayer.norm3", "PPL_modeling.TransformerDecoderLayer.self_attn", "PPL_modeling.TransformerDecoderLayer.dropout1", "PPL_modeling.TransformerDecoderLayer.multihead_attn", "PPL_modeling.TransformerDecoderLayer.dropout2", "PPL_modeling.TransformerDecoderLayer.dropout", "PPL_modeling.TransformerDecoderLayer.dropout3", "PPL_modeling.TransformerDecoderLayer.activation", "PPL_modeling.TransformerDecoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory", ",", "tgt_mask", "=", "None", ",", "memory_mask", "=", "None", ",", "\n", "tgt_key_padding_mask", "=", "None", ",", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "# type: (Tensor, Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]) -> Tensor", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer.\n\n        Args:\n            tgt: the sequence to the decoder layer (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "tgt2", "=", "self", ".", "self_attn", "(", "tgt", ",", "tgt", ",", "tgt", ",", "attn_mask", "=", "tgt_mask", ",", "\n", "key_padding_mask", "=", "tgt_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout1", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "multihead_attn", "(", "tgt", ",", "memory", ",", "memory", ",", "attn_mask", "=", "memory_mask", ",", "\n", "key_padding_mask", "=", "memory_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoder.__init__": [[1428, 1433], ["torch.nn.Module.__init__", "PPL_modeling._get_clones"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_clones"], ["def", "__init__", "(", "self", ",", "decoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "decoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoder.forward": [[1434, 1463], ["mod", "PPL_modeling.TransformerDecoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory", ",", "tgt_mask", "=", "None", ",", "\n", "memory_mask", "=", "None", ",", "tgt_key_padding_mask", "=", "None", ",", "\n", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "# type: (Tensor, Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]) -> Tensor", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n\n        Args:\n            tgt: the sequence to the decoder (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "tgt", "\n", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "\n", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForPreTrainingLossMask.__init__": [[1467, 1561], ["PPL_modeling.PreTrainedBertModel.__init__", "PPL_modeling.BertModel", "PPL_modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "PPL_modeling.TransformerDecoderLayer", "PPL_modeling.TransformerDecoder", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.BertForPreTrainingLossMask.apply", "PPL_modeling.BertForPreTrainingLossMask.bert.rescale_some_parameters", "torch.nn.Embedding", "torch.nn.Embedding", "PPL_modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "PPL_modeling.BertPreTrainingPairRel", "hasattr", "pytorch_bert.loss.LabelSmoothingLoss"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModel.rescale_some_parameters"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "num_sentlvl_labels", "=", "0", ",", "no_nsp", "=", "False", ",", "\n", "mask_word_id", "=", "0", ",", "search_beam_size", "=", "1", ",", "length_penalty", "=", "1.0", ",", "eos_id", "=", "0", ",", "sos_id", "=", "0", ",", "\n", "forbid_duplicate_ngrams", "=", "False", ",", "forbid_ignore_set", "=", "None", ",", "not_predict_set", "=", "None", ",", "ngram_size", "=", "1", ",", "min_len", "=", "1", ",", "mode", "=", "\"s2s\"", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForPreTrainingLossMask", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "num_sentlvl_labels", "=", "num_sentlvl_labels", "\n", "self", ".", "cls2", "=", "None", "\n", "if", "self", ".", "num_sentlvl_labels", ">", "0", ":", "\n", "            ", "self", ".", "secondary_pred_proj", "=", "nn", ".", "Embedding", "(", "\n", "num_sentlvl_labels", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "cls2", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "secondary_pred_proj", ".", "weight", ",", "num_labels", "=", "num_sentlvl_labels", ")", "\n", "", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "if", "no_nsp", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'label_smoothing'", ")", "and", "config", ".", "label_smoothing", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "LabelSmoothingLoss", "(", "\n", "config", ".", "label_smoothing", ",", "config", ".", "vocab_size", ",", "ignore_index", "=", "0", ",", "reduction", "=", "'none'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "None", "\n", "\n", "# CVAE parameter", "\n", "", "self", ".", "latent_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "mu_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "mu_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "KL_weight", "=", "1", "\n", "\n", "self", ".", "prior_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "prior_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "prior_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "self", ".", "posterior_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "posterior_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "posterior_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "#KS parameter", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n", "self", ".", "mutual_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "mutual_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "mutual_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "mutual_mlp", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "mse_fct", "=", "MSELoss", "(", ")", "\n", "\n", "self", ".", "prob_dense", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "\n", "#TopK to Top1 parameter", "\n", "self", ".", "Top_K", "=", "1", "\n", "self", ".", "Topk_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "Topk_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "Topk_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "Topk_classifier_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "#Transformer Decoder", "\n", "check_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "check_transformer_encoder", "=", "TransformerEncoder", "(", "check_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "check_decoder_layer", "=", "TransformerDecoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "check_transformer_decoder", "=", "TransformerDecoder", "(", "check_decoder_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "check_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ")", "\n", "\n", "#Predict Parameter", "\n", "predict_transformer_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "predict_transformer", "=", "TransformerEncoder", "(", "predict_transformer_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "predict_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "# Decode parameter", "\n", "self", ".", "mask_word_id", "=", "mask_word_id", "\n", "self", ".", "search_beam_size", "=", "search_beam_size", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "forbid_duplicate_ngrams", "=", "forbid_duplicate_ngrams", "\n", "self", ".", "forbid_ignore_set", "=", "forbid_ignore_set", "\n", "self", ".", "ngram_size", "=", "ngram_size", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "self", ".", "not_predict_set", "=", "not_predict_set", "\n", "self", ".", "min_len", "=", "min_len", "\n", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "bert", ".", "rescale_some_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForPreTrainingLossMask.forward": [[1562, 1831], ["PPL_modeling.BertForPreTrainingLossMask.bert.get_embedding", "PPL_modeling.BertForPreTrainingLossMask.transpose", "PPL_modeling.BertForPreTrainingLossMask.bert.get_embedding", "add_embedding.squeeze.squeeze.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum.unsqueeze", "torch.einsum.unsqueeze", "PPL_modeling.BertForPreTrainingLossMask.bert", "PPL_modeling.BertForPreTrainingLossMask.dropout", "PPL_modeling.BertForPreTrainingLossMask.classifier", "QK_embedding_output.transpose", "PPL_modeling.BertForPreTrainingLossMask.predict_transformer", "PPL_modeling.BertForPreTrainingLossMask.activation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "PPL_modeling.BertForPreTrainingLossMask.mse_fct", "latent_z.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForPreTrainingLossMask.prob_dense", "PPL_modeling.BertForPreTrainingLossMask.activation", "PPL_modeling.BertForPreTrainingLossMask.forward.gather_seq_out_by_pos"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "\n", "next_sentence_label", "=", "None", ",", "masked_pos", "=", "None", ",", "masked_weights", "=", "None", ",", "task_idx", "=", "None", ",", "pair_x", "=", "None", ",", "\n", "pair_x_mask", "=", "None", ",", "pair_y", "=", "None", ",", "pair_y_mask", "=", "None", ",", "pair_r", "=", "None", ",", "pair_pos_neg_mask", "=", "None", ",", "\n", "pair_loss_mask", "=", "None", ",", "masked_pos_2", "=", "None", ",", "masked_weights_2", "=", "None", ",", "masked_labels_2", "=", "None", ",", "\n", "num_tokens_a", "=", "None", ",", "num_tokens_b", "=", "None", ",", "mask_qkv", "=", "None", ",", "tgt_pos", "=", "None", ",", "labels", "=", "None", ",", "\n", "ks_labels", "=", "None", ",", "train_ks", "=", "None", ",", "check_ids", "=", "None", ",", "position_ids", "=", "None", ",", "decode", "=", "None", ")", ":", "\n", "\n", "        ", "if", "decode", "==", "True", ":", "\n", "            ", "if", "self", ".", "search_beam_size", ">", "1", ":", "\n", "                ", "return", "self", ".", "beam_search", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "", "", "else", ":", "\n", "# **************** E Step ********************************", "\n", "            ", "if", "train_ks", "==", "None", ":", "\n", "                ", "Batch_Size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "input_ids", "=", "input_ids", ".", "reshape", "(", "-", "1", ",", "input_ids", ".", "shape", "[", "2", "]", ")", "\n", "token_type_ids", "=", "token_type_ids", ".", "reshape", "(", "-", "1", ",", "token_type_ids", ".", "shape", "[", "2", "]", ")", "\n", "attention_mask", "=", "attention_mask", ".", "reshape", "(", "-", "1", ",", "attention_mask", ".", "shape", "[", "2", "]", ",", "attention_mask", ".", "shape", "[", "3", "]", ")", "\n", "masked_lm_labels", "=", "masked_lm_labels", ".", "reshape", "(", "-", "1", ",", "masked_lm_labels", ".", "shape", "[", "2", "]", ")", "\n", "masked_pos", "=", "masked_pos", ".", "reshape", "(", "-", "1", ",", "masked_pos", ".", "shape", "[", "2", "]", ")", "\n", "masked_weights", "=", "masked_weights", ".", "reshape", "(", "-", "1", ",", "masked_weights", ".", "shape", "[", "2", "]", ")", "\n", "tgt_pos", "=", "tgt_pos", ".", "reshape", "(", "-", "1", ",", "tgt_pos", ".", "shape", "[", "2", "]", ")", "\n", "labels", "=", "labels", ".", "reshape", "(", "-", "1", ")", "\n", "ks_labels", "=", "ks_labels", ".", "reshape", "(", "-", "1", ")", "\n", "\n", "\n", "TopK_embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "TopK_trans_embedding_output", "=", "TopK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "TopK_out", "=", "self", ".", "Topk_transformer_network", "(", "TopK_trans_embedding_output", ")", "\n", "TopK_out", "=", "TopK_out", "[", "0", ",", ":", ",", ":", "]", "\n", "TopK_out", "=", "self", ".", "Topk_classifier_mlp", "(", "TopK_out", ")", ".", "squeeze", "(", ")", ".", "reshape", "(", "Batch_Size", ",", "-", "1", ")", "\n", "TopK_prob", "=", "torch", ".", "softmax", "(", "TopK_out", ",", "-", "1", ")", "\n", "choice_know_id", "=", "torch", ".", "argmax", "(", "TopK_prob", ",", "1", ")", ".", "detach", "(", ")", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "weight_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "labels", ",", "add_embedding", ".", "expand", "(", "labels", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "labels", ")", ")", "\n", "latent_z", "=", "weight_embedding", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "ks_logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "ks_logits", "=", "F", ".", "softmax", "(", "ks_logits", ",", "dim", "=", "-", "1", ")", "\n", "ks_prob", "=", "ks_logits", "[", ":", ",", "1", "]", "\n", "\n", "\n", "latent_z", "=", "latent_z", ".", "expand", "(", "-", "1", ",", "sequence_output", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "latent_z", ")", ",", "dim", "=", "2", ")", "\n", "sequence_output", "=", "self", ".", "prob_dense", "(", "sequence_output", ")", "\n", "sequence_output", "=", "self", ".", "activation", "(", "sequence_output", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "                    ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "def", "batch_loss_mask_and_normalize", "(", "loss", ",", "mask", ")", ":", "\n", "                    ", "mask", "=", "mask", ".", "type_as", "(", "loss", ")", "\n", "loss", "=", "loss", "*", "mask", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ",", "dim", "=", "1", ")", "+", "1e-5", "\n", "return", "torch", ".", "sum", "(", "loss", ",", "dim", "=", "-", "1", ")", "/", "denominator", "\n", "\n", "# masked lm", "\n", "", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores_masked", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "crit_mask_lm_smoothed", ":", "\n", "                    ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm_smoothed", "(", "\n", "F", ".", "log_softmax", "(", "prediction_scores_masked", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ",", "masked_lm_labels", ")", "\n", "", "else", ":", "\n", "                    ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm", "(", "\n", "prediction_scores_masked", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ")", "\n", "", "masked_lm_loss", "=", "batch_loss_mask_and_normalize", "(", "\n", "masked_lm_loss", ".", "float", "(", ")", ",", "masked_weights", ")", "\n", "\n", "ks_prob", "=", "ks_prob", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", ".", "detach", "(", ")", "\n", "\n", "lm_prob", "=", "F", ".", "softmax", "(", "-", "masked_lm_loss", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", ",", "dim", "=", "1", ")", "\n", "posterior", "=", "(", "ks_prob", "*", "lm_prob", "/", "torch", ".", "sum", "(", "ks_prob", "*", "lm_prob", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "lm_prob", ".", "shape", "[", "1", "]", ")", ")", ".", "detach", "(", ")", "\n", "\n", "KL_loss", "=", "torch", ".", "sum", "(", "TopK_prob", "*", "(", "torch", ".", "log", "(", "TopK_prob", ")", "-", "torch", ".", "log", "(", "posterior", ")", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# recover", "\n", "input_ids", "=", "input_ids", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "token_type_ids", "=", "token_type_ids", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "attention_mask", "=", "attention_mask", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "attention_mask", ".", "shape", "[", "2", "]", ",", "-", "1", ")", "\n", "masked_lm_labels", "=", "masked_lm_labels", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "masked_pos", "=", "masked_pos", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "masked_weights", "=", "masked_weights", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "tgt_pos", "=", "tgt_pos", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ",", "-", "1", ")", "\n", "labels", "=", "labels", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", "\n", "ks_labels", "=", "ks_labels", ".", "reshape", "(", "Batch_Size", ",", "self", ".", "Top_K", ")", "\n", "\n", "\n", "#select ids base choice_know_id", "\n", "choice_know_id", "=", "choice_know_id", ".", "unsqueeze", "(", "1", ")", "\n", "input_ids", "=", "torch", ".", "gather", "(", "input_ids", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "input_ids", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "token_type_ids", "=", "torch", ".", "gather", "(", "token_type_ids", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "token_type_ids", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "attention_mask", "=", "torch", ".", "gather", "(", "attention_mask", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "unsqueeze", "(", "3", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "attention_mask", ".", "shape", "[", "2", "]", ",", "attention_mask", ".", "shape", "[", "3", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "masked_lm_labels", "=", "torch", ".", "gather", "(", "masked_lm_labels", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "masked_lm_labels", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "masked_pos", "=", "torch", ".", "gather", "(", "masked_pos", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "masked_pos", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "masked_weights", "=", "torch", ".", "gather", "(", "masked_weights", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "masked_weights", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "tgt_pos", "=", "torch", ".", "gather", "(", "tgt_pos", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "tgt_pos", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "labels", "=", "torch", ".", "gather", "(", "labels", ",", "1", ",", "choice_know_id", ")", ".", "squeeze", "(", "1", ")", "\n", "ks_labels", "=", "torch", ".", "gather", "(", "ks_labels", ",", "1", ",", "choice_know_id", ")", ".", "squeeze", "(", "1", ")", "\n", "check_ids", "=", "torch", ".", "gather", "(", "check_ids", ",", "1", ",", "choice_know_id", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "check_ids", ".", "shape", "[", "2", "]", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# **************** M Step ********************************", "\n", "", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "weight_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "labels", ",", "add_embedding", ".", "expand", "(", "labels", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "labels", ")", ")", "\n", "\n", "latent_z", "=", "weight_embedding", ".", "unsqueeze", "(", "1", ")", "\n", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "#Knowledge Selection Loss", "\n", "if", "ks_labels", "is", "not", "None", ":", "\n", "                ", "if", "ks_labels", ".", "dtype", "==", "torch", ".", "long", ":", "\n", "                    ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "ks_loss", "=", "loss_fct", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "ks_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "ks_labels", ".", "dtype", "==", "torch", ".", "half", "or", "ks_labels", ".", "dtype", "==", "torch", ".", "float", ":", "\n", "                    ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "ks_loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "ks_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "'unkown ks_labels.dtype'", ")", "\n", "ks_loss", "=", "None", "\n", "\n", "", "if", "train_ks", "is", "True", ":", "\n", "                    ", "return", "ks_loss", ",", "ks_loss", "\n", "", "", "else", ":", "\n", "                ", "return", "logits", "\n", "\n", "\n", "", "QK_embedding_output", "=", "embedding_output", "[", ":", ",", ":", "210", ",", ":", "]", "\n", "trans_QK_embedding_output", "=", "QK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "predict_out", "=", "self", ".", "predict_transformer", "(", "trans_QK_embedding_output", ")", "\n", "predict_out", "=", "predict_out", "[", "0", ",", ":", ",", ":", "]", "\n", "predict_out", "=", "self", ".", "activation", "(", "predict_out", ")", "\n", "predict_probs", "=", "torch", ".", "sigmoid", "(", "self", ".", "predict_mlp", "(", "predict_out", ")", ".", "squeeze", "(", ")", ")", "\n", "predict_loss", "=", "self", ".", "mse_fct", "(", "predict_probs", ",", "labels", ")", "\n", "\n", "\n", "latent_z", "=", "latent_z", ".", "expand", "(", "-", "1", ",", "sequence_output", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "latent_z", ")", ",", "dim", "=", "2", ")", "\n", "sequence_output", "=", "self", ".", "prob_dense", "(", "sequence_output", ")", "\n", "sequence_output", "=", "self", ".", "activation", "(", "sequence_output", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "                ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "def", "gather_id_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "                ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ")", "\n", "\n", "\n", "", "def", "sample_gumbel", "(", "shape", ",", "eps", "=", "1e-20", ")", ":", "\n", "                ", "\"\"\"Sample from Gumbel(0, 1)\"\"\"", "\n", "U", "=", "torch", ".", "rand", "(", "shape", ")", "\n", "return", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "U", "+", "eps", ")", "+", "eps", ")", "\n", "\n", "", "def", "gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", ":", "\n", "                ", "\"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"", "\n", "y", "=", "logits", "+", "torch", ".", "tensor", "(", "sample_gumbel", "(", "logits", ".", "shape", ")", ",", "device", "=", "logits", ".", "device", ")", "\n", "return", "F", ".", "softmax", "(", "y", "/", "temperature", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "def", "gumbel_softmax", "(", "logits", ",", "temperature", ",", "hard", "=", "False", ")", ":", "\n", "                ", "\"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n                Args:\n                  logits: [batch_size, n_class] unnormalized log-probs\n                  temperature: non-negative scalar\n                  hard: if True, take argmax, but differentiate w.r.t. soft sample y\n                Returns:\n                  [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n                  If hard=True, then the returned sample will be one-hot, otherwise it will\n                  be a probabilitiy distribution that sums to 1 across classes\n                \"\"\"", "\n", "y", "=", "gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", "\n", "return", "y", "\n", "\n", "", "tgt_sequence_output", "=", "gather_seq_out_by_pos", "(", "sequence_output", ",", "tgt_pos", ")", "\n", "tgt_prob", ",", "_", "=", "self", ".", "cls", "(", "tgt_sequence_output", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "#tgt_prob_gumbel = gumbel_softmax(tgt_prob, 0.1, hard=False).type_as(self.bert.embeddings.word_embeddings.weight)", "\n", "tgt_prob_gumbel", "=", "gumbel_softmax", "(", "tgt_prob", ",", "200", ",", "hard", "=", "True", ")", ".", "type_as", "(", "input_ids", ")", ".", "type_as", "(", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "# B*T*V", "\n", "\n", "sample_embedding", "=", "torch", ".", "einsum", "(", "\"ijk,kl->ijl\"", ",", "tgt_prob_gumbel", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "check_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "check_ids", ",", "torch", ".", "zeros_like", "(", "check_ids", ")", ")", "\n", "trans_check_embedding", "=", "check_embedding", ".", "transpose", "(", "0", ",", "1", ")", "\n", "PAD_emb", "=", "self", ".", "bert", ".", "get_word_embedding", "(", "torch", ".", "tensor", "(", "[", "[", "0", "]", "*", "input_ids", ".", "shape", "[", "0", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ")", "\n", "trans_check_embedding", "=", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "trans_check_embedding", "[", ":", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "\n", "\n", "# Mutual Information Loss", "\n", "", "position_token_type_embedding_output", "=", "self", ".", "bert", ".", "get_position_token_type_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "response_position_token_type_embedding", "=", "gather_seq_out_by_pos", "(", "position_token_type_embedding_output", ",", "tgt_pos", ")", "\n", "sample_embedding", "=", "sample_embedding", "+", "response_position_token_type_embedding", "\n", "\n", "tgt_pos", "=", "tgt_pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "sample_embedding", ".", "shape", "[", "2", "]", ")", "\n", "#new_embedding_output = embedding_output.scatter(1,tgt_pos,sample_embedding)", "\n", "new_embedding_output", "=", "torch", ".", "cat", "(", "(", "embedding_output", "[", ":", ",", ":", "215", ",", ":", "]", ",", "sample_embedding", ",", "embedding_output", "[", ":", ",", "-", "1", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "trans_new_embedding_output", "=", "new_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "trans_new_embedding_output", "=", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "trans_new_embedding_output", "[", ":", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "mutual_out", "=", "self", ".", "mutual_transformer_network", "(", "trans_new_embedding_output", ")", "\n", "mutual_out", "=", "mutual_out", "[", "0", ",", ":", ",", ":", "]", "\n", "mutual_out", "=", "self", ".", "activation", "(", "mutual_out", ")", "\n", "\n", "mutual_check_out", "=", "self", ".", "mutual_transformer_network", "(", "trans_check_embedding", ")", "\n", "mutual_check_out", "=", "mutual_check_out", "[", "0", ",", ":", ",", ":", "]", "\n", "mutual_check_out", "=", "self", ".", "activation", "(", "mutual_check_out", ")", "\n", "\n", "mutual_out", "=", "torch", ".", "cat", "(", "(", "mutual_out", ",", "mutual_check_out", ")", ",", "dim", "=", "1", ")", "\n", "mutual_out", "=", "self", ".", "mutual_mlp", "(", "mutual_out", ")", "\n", "Mutual_loss", "=", "self", ".", "mse_fct", "(", "mutual_out", ",", "labels", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "trans_embedding_output", "=", "torch", ".", "cat", "(", "(", "PAD_emb", ",", "trans_embedding_output", "[", ":", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "0", ")", "\n", "golden_out", "=", "self", ".", "mutual_transformer_network", "(", "trans_embedding_output", ")", "\n", "golden_out", "=", "golden_out", "[", "0", ",", ":", ",", ":", "]", "\n", "golden_out", "=", "self", ".", "activation", "(", "golden_out", ")", "\n", "\n", "golden_check_out", "=", "self", ".", "mutual_transformer_network", "(", "trans_check_embedding", ")", "\n", "golden_check_out", "=", "golden_check_out", "[", "0", ",", ":", ",", ":", "]", "\n", "golden_check_out", "=", "self", ".", "activation", "(", "golden_check_out", ")", "\n", "\n", "golden_out", "=", "torch", ".", "cat", "(", "(", "golden_out", ",", "golden_check_out", ")", ",", "dim", "=", "1", ")", "\n", "golden_out", "=", "self", ".", "mutual_mlp", "(", "golden_out", ")", "\n", "Golden_loss", "=", "self", ".", "mse_fct", "(", "golden_out", ",", "labels", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "def", "loss_mask_and_normalize", "(", "loss", ",", "mask", ")", ":", "\n", "                ", "mask", "=", "mask", ".", "type_as", "(", "loss", ")", "\n", "loss", "=", "loss", "*", "mask", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ")", "+", "1e-5", "\n", "return", "(", "loss", "/", "denominator", ")", ".", "sum", "(", ")", "\n", "\n", "", "if", "masked_lm_labels", "is", "None", ":", "\n", "                ", "if", "masked_pos", "is", "None", ":", "\n", "                    ", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "else", ":", "\n", "                    ", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n", "\n", "", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores_masked", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "crit_mask_lm_smoothed", ":", "\n", "                ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm_smoothed", "(", "F", ".", "log_softmax", "(", "prediction_scores_masked", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ",", "masked_lm_labels", ")", "\n", "", "else", ":", "\n", "                ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm", "(", "prediction_scores_masked", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ")", "\n", "", "masked_lm_loss", "=", "loss_mask_and_normalize", "(", "masked_lm_loss", ".", "float", "(", ")", ",", "masked_weights", ")", "\n", "\n", "if", "self", ".", "crit_next_sent", "is", "None", "or", "next_sentence_label", "is", "None", ":", "\n", "                ", "next_sentence_loss", "=", "0.0", "\n", "", "else", ":", "\n", "                ", "next_sentence_loss", "=", "self", ".", "crit_next_sent", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ".", "float", "(", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "\n", "", "if", "pair_x", "is", "None", "or", "pair_y", "is", "None", "or", "pair_r", "is", "None", "or", "pair_pos_neg_mask", "is", "None", "or", "pair_loss_mask", "is", "None", ":", "\n", "                ", "return", "masked_lm_loss", ",", "next_sentence_loss", ",", "KL_loss", ",", "Mutual_loss", ",", "Golden_loss", ",", "predict_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForPreTrainingLossMask.beam_search": [[1833, 2158], ["list", "list", "input_ids.new().fill_", "PPL_modeling.BertForPreTrainingLossMask.bert.get_embedding", "PPL_modeling.BertForPreTrainingLossMask.transpose", "PPL_modeling.BertForPreTrainingLossMask.prior_transformer_network", "QK_embedding_output.transpose", "PPL_modeling.BertForPreTrainingLossMask.predict_transformer", "PPL_modeling.BertForPreTrainingLossMask.activation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "PPL_modeling.BertForPreTrainingLossMask.bert.get_embedding", "torch.einsum.squeeze", "torch.einsum.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForPreTrainingLossMask.mu_mlp1", "PPL_modeling.BertForPreTrainingLossMask.var_mlp1", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "range", "input_ids.size", "first_expand.size", "input_ids.new().fill_", "PPL_modeling.BertForPreTrainingLossMask.predict_mlp().squeeze", "torch.einsum.expand().type_as", "torch.einsum.expand().type_as", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForPreTrainingLossMask.prob_dense", "PPL_modeling.BertForPreTrainingLossMask.activation", "PPL_modeling.BertForPreTrainingLossMask.cls", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "step_back_ptrs.append", "step_ids.append", "beam_masks.append", "total_scores.append", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "traces[].append", "traces[].append", "traces[].append", "enumerate", "range", "sequences[].data.new().fill_", "enumerate", "_pad_sequence().to", "input_ids.new", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "latent_z.unsqueeze.unsqueeze.unsqueeze", "PPL_modeling.BertForPreTrainingLossMask.bert", "latent_z.unsqueeze.unsqueeze.unsqueeze().expand().reshape", "PPL_modeling.BertForPreTrainingLossMask.bert", "log_scores[].fill_", "len", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.div", "torch.div", "torch.div", "torch.div", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.repeat", "torch.reshape.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "list", "len", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "PPL_modeling.BertForPreTrainingLossMask.beam_search.first_expand"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding"], ["", "", "", "def", "beam_search", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "K", "=", "self", ".", "search_beam_size", "\n", "\n", "total_scores", "=", "[", "]", "\n", "beam_masks", "=", "[", "]", "\n", "step_ids", "=", "[", "]", "\n", "step_back_ptrs", "=", "[", "]", "\n", "partial_seqs", "=", "[", "]", "\n", "forbid_word_mask", "=", "None", "\n", "buf_matrix", "=", "None", "\n", "\n", "source_token_type_ids", "=", "token_type_ids", "[", ":", ",", ":", "input_length", "]", "\n", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "source_token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "prior_out", "=", "self", ".", "prior_transformer_network", "(", "trans_embedding_output", ")", "\n", "prior", "=", "prior_out", "[", "0", ",", ":", ",", ":", "]", "\n", "\n", "QK_embedding_output", "=", "embedding_output", "[", ":", ",", ":", "210", ",", ":", "]", "\n", "trans_QK_embedding_output", "=", "QK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "predict_out", "=", "self", ".", "predict_transformer", "(", "trans_QK_embedding_output", ")", "\n", "predict_out", "=", "predict_out", "[", "0", ",", ":", ",", ":", "]", "\n", "predict_out", "=", "self", ".", "activation", "(", "predict_out", ")", "\n", "predict_probs", "=", "torch", ".", "sigmoid", "(", "self", ".", "predict_mlp", "(", "predict_out", ")", ".", "squeeze", "(", ")", ")", "\n", "\n", "bleu", "=", "predict_probs", "\n", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "add_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "bleu", ",", "add_embedding", ".", "expand", "(", "bleu", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "bleu", ")", ")", "\n", "\n", "prior", "=", "torch", ".", "cat", "(", "(", "prior", ",", "add_embedding", ")", ",", "dim", "=", "1", ")", "\n", "\n", "prior_mu", "=", "self", ".", "mu_mlp1", "(", "prior", ")", "# B*768", "\n", "prior_logvar", "=", "self", ".", "var_mlp1", "(", "prior", ")", "\n", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "prior_logvar", ")", "\n", "eps", "=", "torch", ".", "randn", "(", "[", "prior_logvar", ".", "shape", "[", "0", "]", ",", "self", ".", "latent_size", "]", ",", "device", "=", "prior_mu", ".", "device", ")", "# B * hidden", "\n", "latent_z", "=", "eps", "*", "std", "+", "prior_mu", "\n", "\n", "latent_z", "=", "add_embedding", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "\n", "if", "prev_embedding", "is", "None", ":", "\n", "                ", "latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", "\n", "beam_latent_z", "=", "latent_z", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "True", ",", "\n", "latent_z", "=", "latent_z", ",", "decode", "=", "True", ",", "position_ids", "=", "curr_position_ids", ")", "\n", "", "else", ":", "\n", "                ", "beam_latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "self", ".", "search_beam_size", ",", "-", "1", ",", "-", "1", ")", ".", "reshape", "(", "-", "1", ",", "latent_z", ".", "shape", "[", "1", "]", ",", "latent_z", ".", "shape", "[", "2", "]", ")", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "\n", "relace_embeddings", "=", "False", ",", "decode", "=", "True", ",", "position_ids", "=", "curr_position_ids", ")", "\n", "\n", "", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "last_hidden", "=", "torch", ".", "cat", "(", "(", "last_hidden", ",", "beam_latent_z", ")", ",", "dim", "=", "2", ")", "\n", "last_hidden", "=", "self", ".", "prob_dense", "(", "last_hidden", ")", "\n", "last_hidden", "=", "self", ".", "activation", "(", "last_hidden", ")", "\n", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "log_scores", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "forbid_word_mask", "is", "not", "None", ":", "\n", "                ", "log_scores", "+=", "(", "forbid_word_mask", "*", "-", "10000.0", ")", "\n", "", "if", "self", ".", "min_len", "and", "(", "next_pos", "-", "input_length", "+", "1", "<=", "self", ".", "min_len", ")", ":", "\n", "                ", "log_scores", "[", ":", ",", ":", ",", "self", ".", "eos_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "log_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "kk_scores", ",", "kk_ids", "=", "torch", ".", "topk", "(", "log_scores", ",", "k", "=", "K", ")", "\n", "if", "len", "(", "total_scores", ")", "==", "0", ":", "\n", "                ", "k_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "back_ptrs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "K", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "k_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "", "else", ":", "\n", "                ", "last_eos", "=", "torch", ".", "reshape", "(", "\n", "beam_masks", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "last_seq_scores", "=", "torch", ".", "reshape", "(", "\n", "total_scores", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "kk_scores", "+=", "last_eos", "*", "(", "-", "10000.0", ")", "+", "last_seq_scores", "\n", "kk_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_scores", ",", "k_ids", "=", "torch", ".", "topk", "(", "kk_scores", ",", "k", "=", "K", ")", "\n", "back_ptrs", "=", "torch", ".", "div", "(", "k_ids", ",", "K", ")", "\n", "kk_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_ids", "=", "torch", ".", "gather", "(", "kk_ids", ",", "1", ",", "k_ids", ")", "\n", "", "step_back_ptrs", ".", "append", "(", "back_ptrs", ")", "\n", "step_ids", ".", "append", "(", "k_ids", ")", "\n", "beam_masks", ".", "append", "(", "torch", ".", "eq", "(", "k_ids", ",", "self", ".", "eos_id", ")", ".", "float", "(", ")", ")", "\n", "total_scores", ".", "append", "(", "k_scores", ")", "\n", "\n", "def", "first_expand", "(", "x", ")", ":", "\n", "                ", "input_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "expanded_shape", "=", "input_shape", "[", ":", "1", "]", "+", "[", "1", "]", "+", "input_shape", "[", "1", ":", "]", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "\n", "repeat_count", "=", "[", "1", ",", "K", "]", "+", "[", "1", "]", "*", "(", "len", "(", "input_shape", ")", "-", "1", ")", "\n", "x", "=", "x", ".", "repeat", "(", "*", "repeat_count", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "input_shape", "[", "0", "]", "*", "K", "]", "+", "input_shape", "[", "1", ":", "]", ")", "\n", "return", "x", "\n", "\n", "", "def", "select_beam_items", "(", "x", ",", "ids", ")", ":", "\n", "                ", "id_shape", "=", "list", "(", "ids", ".", "size", "(", ")", ")", "\n", "id_rank", "=", "len", "(", "id_shape", ")", "\n", "assert", "len", "(", "id_shape", ")", "==", "2", "\n", "x_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "batch_size", ",", "K", "]", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "x_rank", "=", "len", "(", "x_shape", ")", "+", "1", "\n", "assert", "x_rank", ">=", "2", "\n", "if", "id_rank", "<", "x_rank", ":", "\n", "                    ", "ids", "=", "torch", ".", "reshape", "(", "\n", "ids", ",", "id_shape", "+", "[", "1", "]", "*", "(", "x_rank", "-", "id_rank", ")", ")", "\n", "ids", "=", "ids", ".", "expand", "(", "id_shape", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "", "y", "=", "torch", ".", "gather", "(", "x", ",", "1", ",", "ids", ")", "\n", "y", "=", "torch", ".", "reshape", "(", "y", ",", "x_shape", ")", "\n", "return", "y", "\n", "\n", "", "is_first", "=", "(", "prev_embedding", "is", "None", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "\n", "", "", "curr_ids", "=", "torch", ".", "reshape", "(", "k_ids", ",", "[", "batch_size", "*", "K", ",", "1", "]", ")", "\n", "\n", "if", "is_first", ":", "\n", "                ", "token_type_ids", "=", "first_expand", "(", "token_type_ids", ")", "\n", "position_ids", "=", "first_expand", "(", "position_ids", ")", "\n", "attention_mask", "=", "first_expand", "(", "attention_mask", ")", "\n", "mask_ids", "=", "first_expand", "(", "mask_ids", ")", "\n", "if", "mask_qkv", "is", "not", "None", ":", "\n", "                    ", "mask_qkv", "=", "first_expand", "(", "mask_qkv", ")", "\n", "\n", "", "", "if", "self", ".", "forbid_duplicate_ngrams", ":", "\n", "                ", "wids", "=", "step_ids", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "ptrs", "=", "step_back_ptrs", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "if", "is_first", ":", "\n", "                    ", "partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "partial_seqs", ".", "append", "(", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "new_partial_seqs", ".", "append", "(", "\n", "partial_seqs", "[", "ptrs", "[", "b", "]", "[", "k", "]", "+", "b", "*", "K", "]", "+", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "partial_seqs", "=", "new_partial_seqs", "\n", "\n", "", "def", "get_dup_ngram_candidates", "(", "seq", ",", "n", ")", ":", "\n", "                    ", "cands", "=", "set", "(", ")", "\n", "if", "len", "(", "seq", ")", "<", "n", ":", "\n", "                        ", "return", "[", "]", "\n", "", "tail", "=", "seq", "[", "-", "(", "n", "-", "1", ")", ":", "]", "\n", "if", "self", ".", "forbid_ignore_set", "and", "any", "(", "tk", "in", "self", ".", "forbid_ignore_set", "for", "tk", "in", "tail", ")", ":", "\n", "                        ", "return", "[", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "(", "n", "-", "1", ")", ")", ":", "\n", "                        ", "mismatch", "=", "False", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                            ", "if", "tail", "[", "j", "]", "!=", "seq", "[", "i", "+", "j", "]", ":", "\n", "                                ", "mismatch", "=", "True", "\n", "break", "\n", "", "", "if", "(", "not", "mismatch", ")", "and", "not", "(", "\n", "self", ".", "forbid_ignore_set", "and", "(", "seq", "[", "i", "+", "n", "-", "1", "]", "in", "self", ".", "forbid_ignore_set", ")", ")", ":", "\n", "                            ", "cands", ".", "add", "(", "seq", "[", "i", "+", "n", "-", "1", "]", ")", "\n", "", "", "return", "list", "(", "sorted", "(", "cands", ")", ")", "\n", "\n", "", "if", "len", "(", "partial_seqs", "[", "0", "]", ")", ">=", "self", ".", "ngram_size", ":", "\n", "                    ", "dup_cands", "=", "[", "]", "\n", "for", "seq", "in", "partial_seqs", ":", "\n", "                        ", "dup_cands", ".", "append", "(", "\n", "get_dup_ngram_candidates", "(", "seq", ",", "self", ".", "ngram_size", ")", ")", "\n", "", "if", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dup_cands", ")", ">", "0", ":", "\n", "                        ", "if", "buf_matrix", "is", "None", ":", "\n", "                            ", "vocab_size", "=", "list", "(", "log_scores", ".", "size", "(", ")", ")", "[", "-", "1", "]", "\n", "buf_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "batch_size", "*", "K", ",", "vocab_size", ")", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "                            ", "buf_matrix", ".", "fill", "(", "0", ")", "\n", "", "for", "bk", ",", "cands", "in", "enumerate", "(", "dup_cands", ")", ":", "\n", "                            ", "for", "i", ",", "wid", "in", "enumerate", "(", "cands", ")", ":", "\n", "                                ", "buf_matrix", "[", "bk", ",", "wid", "]", "=", "1.0", "\n", "", "", "forbid_word_mask", "=", "torch", ".", "tensor", "(", "\n", "buf_matrix", ",", "dtype", "=", "log_scores", ".", "dtype", ")", "\n", "forbid_word_mask", "=", "torch", ".", "reshape", "(", "\n", "forbid_word_mask", ",", "[", "batch_size", "*", "K", ",", "1", ",", "vocab_size", "]", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                        ", "forbid_word_mask", "=", "None", "\n", "", "", "", "next_pos", "+=", "1", "\n", "\n", "# [(batch, beam)]", "\n", "", "total_scores", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "total_scores", "]", "\n", "step_ids", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_ids", "]", "\n", "step_back_ptrs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_back_ptrs", "]", "\n", "# back tracking", "\n", "traces", "=", "{", "'pred_seq'", ":", "[", "]", ",", "'scores'", ":", "[", "]", ",", "'wids'", ":", "[", "]", ",", "'ptrs'", ":", "[", "]", "}", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "# [(beam,)]", "\n", "            ", "scores", "=", "[", "x", "[", "b", "]", "for", "x", "in", "total_scores", "]", "\n", "wids_list", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_ids", "]", "\n", "ptrs", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_back_ptrs", "]", "\n", "traces", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "\n", "traces", "[", "'wids'", "]", ".", "append", "(", "wids_list", ")", "\n", "traces", "[", "'ptrs'", "]", ".", "append", "(", "ptrs", ")", "\n", "# first we need to find the eos frame where all symbols are eos", "\n", "# any frames after the eos frame are invalid", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "                ", "if", "all", "(", "wid", "==", "self", ".", "eos_id", "for", "wid", "in", "wids", ")", ":", "\n", "                    ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "                ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "                    ", "if", "wid", "==", "self", ".", "eos_id", "or", "fid", "==", "last_frame_id", ":", "\n", "                        ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "self", ".", "length_penalty", ">", "0", ":", "\n", "                            ", "s", "/=", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "\n", "self", ".", "length_penalty", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "                            ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "                ", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "                    ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "pos_in_frame", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "pos_in_frame", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "seq", ")", "\n", "\n", "", "", "def", "_pad_sequence", "(", "sequences", ",", "max_len", ",", "padding_value", "=", "0", ")", ":", "\n", "            ", "trailing_dims", "=", "sequences", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "out_dims", "=", "(", "len", "(", "sequences", ")", ",", "max_len", ")", "+", "trailing_dims", "\n", "\n", "out_tensor", "=", "sequences", "[", "0", "]", ".", "data", ".", "new", "(", "*", "out_dims", ")", ".", "fill_", "(", "padding_value", ")", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "sequences", ")", ":", "\n", "                ", "length", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# use index notation to prevent duplicate references to the tensor", "\n", "out_tensor", "[", "i", ",", ":", "length", ",", "...", "]", "=", "tensor", "\n", "", "return", "out_tensor", "\n", "\n", "# convert to tensors for DataParallel", "\n", "", "for", "k", "in", "(", "'pred_seq'", ",", "'scores'", ",", "'wids'", ",", "'ptrs'", ")", ":", "\n", "            ", "ts_list", "=", "traces", "[", "k", "]", "\n", "if", "not", "isinstance", "(", "ts_list", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "dt", "=", "torch", ".", "float", "if", "k", "==", "'scores'", "else", "torch", ".", "long", "\n", "ts_list", "=", "[", "torch", ".", "tensor", "(", "it", ",", "dtype", "=", "dt", ")", "for", "it", "in", "ts_list", "]", "\n", "", "traces", "[", "k", "]", "=", "_pad_sequence", "(", "\n", "ts_list", ",", "output_length", ",", "padding_value", "=", "0", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "", "return", "traces", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForSeq2SeqDecoder.__init__": [[2163, 2210], ["PPL_modeling.PreTrainedBertModel.__init__", "PPL_modeling.BertModelIncr", "PPL_modeling.BertPreTrainingHeads", "PPL_modeling.BertForSeq2SeqDecoder.apply", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "PPL_modeling.TransformerEncoderLayer", "PPL_modeling.TransformerEncoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh", "PPL_modeling.BertPreTrainingPairRel"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "mask_word_id", "=", "0", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "\n", "search_beam_size", "=", "1", ",", "length_penalty", "=", "1.0", ",", "eos_id", "=", "0", ",", "sos_id", "=", "0", ",", "\n", "forbid_duplicate_ngrams", "=", "False", ",", "forbid_ignore_set", "=", "None", ",", "not_predict_set", "=", "None", ",", "ngram_size", "=", "3", ",", "min_len", "=", "0", ",", "mode", "=", "\"s2s\"", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSeq2SeqDecoder", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModelIncr", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "self", ".", "mask_word_id", "=", "mask_word_id", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "self", ".", "search_beam_size", "=", "search_beam_size", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "forbid_duplicate_ngrams", "=", "forbid_duplicate_ngrams", "\n", "self", ".", "forbid_ignore_set", "=", "forbid_ignore_set", "\n", "self", ".", "not_predict_set", "=", "not_predict_set", "\n", "self", ".", "ngram_size", "=", "ngram_size", "\n", "self", ".", "min_len", "=", "min_len", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n", "# CVAE parameter  768", "\n", "self", ".", "latent_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "mu_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp1", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "mu_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "var_mlp2", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "KL_weight", "=", "1", "\n", "\n", "self", ".", "prior_encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "prior_transformer_network", "=", "TransformerEncoder", "(", "self", ".", "prior_encoder_layer", ",", "num_layers", "=", "3", ")", "\n", "\n", "########", "\n", "predict_transformer_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "768", ",", "nhead", "=", "12", ")", "\n", "self", ".", "predict_transformer", "=", "TransformerEncoder", "(", "predict_transformer_layer", ",", "num_layers", "=", "3", ")", "\n", "self", ".", "predict_mlp", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "prob_dense", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "latent_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForSeq2SeqDecoder.forward": [[2211, 2321], ["list", "list", "input_ids.new().fill_", "PPL_modeling.BertForSeq2SeqDecoder.bert.get_embedding", "PPL_modeling.BertForSeq2SeqDecoder.transpose", "PPL_modeling.BertForSeq2SeqDecoder.prior_transformer_network", "PPL_modeling.BertForSeq2SeqDecoder.bert.get_embedding", "torch.einsum.squeeze", "torch.einsum.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForSeq2SeqDecoder.mu_mlp1", "PPL_modeling.BertForSeq2SeqDecoder.var_mlp1", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "latent_z.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForSeq2SeqDecoder.beam_search", "input_ids.size", "token_type_ids.size", "input_ids.new().fill_", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.einsum.expand().type_as", "torch.einsum.expand().type_as", "PPL_modeling.BertForSeq2SeqDecoder.bert", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForSeq2SeqDecoder.prob_dense", "PPL_modeling.BertForSeq2SeqDecoder.activation", "PPL_modeling.BertForSeq2SeqDecoder.cls", "torch.max", "torch.max", "torch.max", "torch.max", "output_ids.append", "input_ids.new", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "input_ids.new", "torch.einsum.expand", "torch.einsum.expand", "curr_ids.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prediction_scores[].fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForSeq2SeqDecoder.beam_search"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ",", "bleu", "=", "None", ",", "train_vae", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "search_beam_size", ">", "1", ":", "\n", "            ", "return", "self", ".", "beam_search", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ",", "bleu", "=", "bleu", ",", "train_vae", "=", "train_vae", ")", "\n", "\n", "", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "source_token_type_ids", "=", "token_type_ids", "[", ":", ",", ":", "input_length", "]", "\n", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "source_token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "prior_out", "=", "self", ".", "prior_transformer_network", "(", "trans_embedding_output", ")", "# S B H", "\n", "prior", "=", "prior_out", "[", "0", ",", ":", ",", ":", "]", "\n", "\n", "bleu", "=", "bleu", "*", "torch", ".", "ones", "(", "size", "=", "[", "prior", ".", "shape", "[", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "prior", ".", "device", ")", "# B", "\n", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "add_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "bleu", ",", "add_embedding", ".", "expand", "(", "bleu", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "bleu", ")", ")", "# B H", "\n", "\n", "prior", "=", "torch", ".", "cat", "(", "(", "prior", ",", "add_embedding", ")", ",", "dim", "=", "1", ")", "\n", "\n", "prior_mu", "=", "self", ".", "mu_mlp1", "(", "prior", ")", "# B*768", "\n", "prior_logvar", "=", "self", ".", "var_mlp1", "(", "prior", ")", "\n", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "prior_logvar", ")", "\n", "eps", "=", "torch", ".", "randn", "(", "[", "prior_logvar", ".", "shape", "[", "0", "]", ",", "self", ".", "latent_size", "]", ",", "device", "=", "prior_mu", ".", "device", ")", "# B * hidden", "\n", "latent_z", "=", "eps", "*", "std", "+", "prior_mu", "\n", "\n", "latent_z", "=", "add_embedding", "\n", "latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "\n", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "True", ",", "latent_z", "=", "latent_z", ")", "\n", "\n", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "last_hidden", "=", "torch", ".", "cat", "(", "(", "last_hidden", ",", "latent_z", ")", ",", "dim", "=", "2", ")", "# B*1*2H", "\n", "last_hidden", "=", "self", ".", "prob_dense", "(", "last_hidden", ")", "# B*1*H", "\n", "last_hidden", "=", "self", ".", "activation", "(", "last_hidden", ")", "\n", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "prediction_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "_", ",", "max_ids", "=", "torch", ".", "max", "(", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "output_ids", ".", "append", "(", "max_ids", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "curr_ids", "=", "max_ids", "\n", "next_pos", "+=", "1", "\n", "\n", "", "return", "torch", ".", "cat", "(", "output_ids", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertForSeq2SeqDecoder.beam_search": [[2322, 2649], ["list", "list", "input_ids.new().fill_", "PPL_modeling.BertForSeq2SeqDecoder.bert.get_embedding", "PPL_modeling.BertForSeq2SeqDecoder.transpose", "PPL_modeling.BertForSeq2SeqDecoder.prior_transformer_network", "QK_embedding_output.transpose", "PPL_modeling.BertForSeq2SeqDecoder.predict_transformer", "PPL_modeling.BertForSeq2SeqDecoder.activation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "PPL_modeling.BertForSeq2SeqDecoder.bert.get_embedding", "torch.einsum.squeeze", "torch.einsum.squeeze", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForSeq2SeqDecoder.mu_mlp1", "PPL_modeling.BertForSeq2SeqDecoder.var_mlp1", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "range", "input_ids.size", "first_expand.size", "input_ids.new().fill_", "PPL_modeling.BertForSeq2SeqDecoder.predict_mlp().squeeze", "torch.einsum.expand().type_as", "torch.einsum.expand().type_as", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "PPL_modeling.BertForSeq2SeqDecoder.prob_dense", "PPL_modeling.BertForSeq2SeqDecoder.activation", "PPL_modeling.BertForSeq2SeqDecoder.cls", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "step_back_ptrs.append", "step_ids.append", "beam_masks.append", "total_scores.append", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "traces[].append", "traces[].append", "traces[].append", "enumerate", "range", "sequences[].data.new().fill_", "enumerate", "_pad_sequence().to", "input_ids.new", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "latent_z.unsqueeze.unsqueeze.unsqueeze", "PPL_modeling.BertForSeq2SeqDecoder.bert", "latent_z.unsqueeze.unsqueeze.unsqueeze().expand().reshape", "PPL_modeling.BertForSeq2SeqDecoder.bert", "log_scores[].fill_", "len", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.div", "torch.div", "torch.div", "torch.div", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.repeat", "torch.reshape.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "list", "len", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "PPL_modeling.BertForSeq2SeqDecoder.beam_search.first_expand"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.BertModelIncr.get_embedding"], ["", "def", "beam_search", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ",", "bleu", "=", "None", ",", "train_vae", "=", "None", ")", ":", "\n", "        ", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "K", "=", "self", ".", "search_beam_size", "\n", "\n", "total_scores", "=", "[", "]", "\n", "beam_masks", "=", "[", "]", "\n", "step_ids", "=", "[", "]", "\n", "step_back_ptrs", "=", "[", "]", "\n", "partial_seqs", "=", "[", "]", "\n", "forbid_word_mask", "=", "None", "\n", "buf_matrix", "=", "None", "\n", "\n", "source_token_type_ids", "=", "token_type_ids", "[", ":", ",", ":", "input_length", "]", "\n", "embedding_output", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "source_token_type_ids", ")", "\n", "trans_embedding_output", "=", "embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "prior_out", "=", "self", ".", "prior_transformer_network", "(", "trans_embedding_output", ")", "# S B H", "\n", "prior", "=", "prior_out", "[", "0", ",", ":", ",", ":", "]", "\n", "\n", "#######", "\n", "QK_embedding_output", "=", "embedding_output", "[", ":", ",", ":", "210", ",", ":", "]", "\n", "trans_QK_embedding_output", "=", "QK_embedding_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "predict_out", "=", "self", ".", "predict_transformer", "(", "trans_QK_embedding_output", ")", "\n", "predict_out", "=", "predict_out", "[", "0", ",", ":", ",", ":", "]", "# B*H", "\n", "predict_out", "=", "self", ".", "activation", "(", "predict_out", ")", "\n", "predict_probs", "=", "torch", ".", "sigmoid", "(", "self", ".", "predict_mlp", "(", "predict_out", ")", ".", "squeeze", "(", ")", ")", "# B", "\n", "\n", "bleu", "=", "predict_probs", "\n", "#print(bleu[0:5])", "\n", "#########", "\n", "\n", "#bleu = bleu * torch.ones(size=[prior.shape[0]], dtype=torch.float, device=prior.device)  # B", "\n", "add_embedding", "=", "self", ".", "bert", ".", "get_embedding", "(", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "15", "]", "]", ")", ".", "type_as", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", "]", "]", ")", ".", "type_as", "(", "token_type_ids", ")", ")", "\n", "add_embedding", "=", "add_embedding", ".", "squeeze", "(", "1", ")", "\n", "add_embedding", "=", "torch", ".", "einsum", "(", "\"i,ij->ij\"", ",", "bleu", ",", "add_embedding", ".", "expand", "(", "bleu", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ".", "type_as", "(", "bleu", ")", ")", "#B H", "\n", "\n", "prior", "=", "torch", ".", "cat", "(", "(", "prior", ",", "add_embedding", ")", ",", "dim", "=", "1", ")", "\n", "\n", "prior_mu", "=", "self", ".", "mu_mlp1", "(", "prior", ")", "# B*768", "\n", "prior_logvar", "=", "self", ".", "var_mlp1", "(", "prior", ")", "\n", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "prior_logvar", ")", "\n", "eps", "=", "torch", ".", "randn", "(", "[", "prior_logvar", ".", "shape", "[", "0", "]", ",", "self", ".", "latent_size", "]", ",", "device", "=", "prior_mu", ".", "device", ")", "# B * hidden", "\n", "latent_z", "=", "eps", "*", "std", "+", "prior_mu", "\n", "#print(latent_z[0, :])", "\n", "\n", "latent_z", "=", "add_embedding", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "\n", "if", "prev_embedding", "is", "None", ":", "\n", "                ", "latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", "\n", "beam_latent_z", "=", "latent_z", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "True", ",", "\n", "latent_z", "=", "latent_z", ")", "\n", "", "else", ":", "\n", "                ", "beam_latent_z", "=", "latent_z", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "self", ".", "search_beam_size", ",", "-", "1", ",", "-", "1", ")", ".", "reshape", "(", "-", "1", ",", "latent_z", ".", "shape", "[", "1", "]", ",", "latent_z", ".", "shape", "[", "2", "]", ")", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "relace_embeddings", "=", "False", ")", "\n", "\n", "", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "last_hidden", "=", "torch", ".", "cat", "(", "(", "last_hidden", ",", "beam_latent_z", ")", ",", "dim", "=", "2", ")", "# B*1*2H", "\n", "last_hidden", "=", "self", ".", "prob_dense", "(", "last_hidden", ")", "# B*1*H", "\n", "last_hidden", "=", "self", ".", "activation", "(", "last_hidden", ")", "\n", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "log_scores", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "forbid_word_mask", "is", "not", "None", ":", "\n", "                ", "log_scores", "+=", "(", "forbid_word_mask", "*", "-", "10000.0", ")", "\n", "", "if", "self", ".", "min_len", "and", "(", "next_pos", "-", "input_length", "+", "1", "<=", "self", ".", "min_len", ")", ":", "\n", "                ", "log_scores", "[", ":", ",", ":", ",", "self", ".", "eos_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "log_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "kk_scores", ",", "kk_ids", "=", "torch", ".", "topk", "(", "log_scores", ",", "k", "=", "K", ")", "\n", "if", "len", "(", "total_scores", ")", "==", "0", ":", "\n", "                ", "k_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "back_ptrs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "K", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "k_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "", "else", ":", "\n", "                ", "last_eos", "=", "torch", ".", "reshape", "(", "\n", "beam_masks", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "last_seq_scores", "=", "torch", ".", "reshape", "(", "\n", "total_scores", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "kk_scores", "+=", "last_eos", "*", "(", "-", "10000.0", ")", "+", "last_seq_scores", "\n", "kk_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_scores", ",", "k_ids", "=", "torch", ".", "topk", "(", "kk_scores", ",", "k", "=", "K", ")", "\n", "back_ptrs", "=", "torch", ".", "div", "(", "k_ids", ",", "K", ")", "\n", "kk_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_ids", "=", "torch", ".", "gather", "(", "kk_ids", ",", "1", ",", "k_ids", ")", "\n", "", "step_back_ptrs", ".", "append", "(", "back_ptrs", ")", "\n", "step_ids", ".", "append", "(", "k_ids", ")", "\n", "beam_masks", ".", "append", "(", "torch", ".", "eq", "(", "k_ids", ",", "self", ".", "eos_id", ")", ".", "float", "(", ")", ")", "\n", "total_scores", ".", "append", "(", "k_scores", ")", "\n", "\n", "def", "first_expand", "(", "x", ")", ":", "\n", "                ", "input_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "expanded_shape", "=", "input_shape", "[", ":", "1", "]", "+", "[", "1", "]", "+", "input_shape", "[", "1", ":", "]", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "\n", "repeat_count", "=", "[", "1", ",", "K", "]", "+", "[", "1", "]", "*", "(", "len", "(", "input_shape", ")", "-", "1", ")", "\n", "x", "=", "x", ".", "repeat", "(", "*", "repeat_count", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "input_shape", "[", "0", "]", "*", "K", "]", "+", "input_shape", "[", "1", ":", "]", ")", "\n", "return", "x", "\n", "\n", "", "def", "select_beam_items", "(", "x", ",", "ids", ")", ":", "\n", "                ", "id_shape", "=", "list", "(", "ids", ".", "size", "(", ")", ")", "\n", "id_rank", "=", "len", "(", "id_shape", ")", "\n", "assert", "len", "(", "id_shape", ")", "==", "2", "\n", "x_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "batch_size", ",", "K", "]", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "x_rank", "=", "len", "(", "x_shape", ")", "+", "1", "\n", "assert", "x_rank", ">=", "2", "\n", "if", "id_rank", "<", "x_rank", ":", "\n", "                    ", "ids", "=", "torch", ".", "reshape", "(", "\n", "ids", ",", "id_shape", "+", "[", "1", "]", "*", "(", "x_rank", "-", "id_rank", ")", ")", "\n", "ids", "=", "ids", ".", "expand", "(", "id_shape", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "", "y", "=", "torch", ".", "gather", "(", "x", ",", "1", ",", "ids", ")", "\n", "y", "=", "torch", ".", "reshape", "(", "y", ",", "x_shape", ")", "\n", "return", "y", "\n", "\n", "", "is_first", "=", "(", "prev_embedding", "is", "None", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "\n", "", "", "curr_ids", "=", "torch", ".", "reshape", "(", "k_ids", ",", "[", "batch_size", "*", "K", ",", "1", "]", ")", "\n", "\n", "if", "is_first", ":", "\n", "                ", "token_type_ids", "=", "first_expand", "(", "token_type_ids", ")", "\n", "position_ids", "=", "first_expand", "(", "position_ids", ")", "\n", "attention_mask", "=", "first_expand", "(", "attention_mask", ")", "\n", "mask_ids", "=", "first_expand", "(", "mask_ids", ")", "\n", "if", "mask_qkv", "is", "not", "None", ":", "\n", "                    ", "mask_qkv", "=", "first_expand", "(", "mask_qkv", ")", "\n", "\n", "", "", "if", "self", ".", "forbid_duplicate_ngrams", ":", "\n", "                ", "wids", "=", "step_ids", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "ptrs", "=", "step_back_ptrs", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "if", "is_first", ":", "\n", "                    ", "partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "partial_seqs", ".", "append", "(", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "new_partial_seqs", ".", "append", "(", "\n", "partial_seqs", "[", "ptrs", "[", "b", "]", "[", "k", "]", "+", "b", "*", "K", "]", "+", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "partial_seqs", "=", "new_partial_seqs", "\n", "\n", "", "def", "get_dup_ngram_candidates", "(", "seq", ",", "n", ")", ":", "\n", "                    ", "cands", "=", "set", "(", ")", "\n", "if", "len", "(", "seq", ")", "<", "n", ":", "\n", "                        ", "return", "[", "]", "\n", "", "tail", "=", "seq", "[", "-", "(", "n", "-", "1", ")", ":", "]", "\n", "if", "self", ".", "forbid_ignore_set", "and", "any", "(", "tk", "in", "self", ".", "forbid_ignore_set", "for", "tk", "in", "tail", ")", ":", "\n", "                        ", "return", "[", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "(", "n", "-", "1", ")", ")", ":", "\n", "                        ", "mismatch", "=", "False", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                            ", "if", "tail", "[", "j", "]", "!=", "seq", "[", "i", "+", "j", "]", ":", "\n", "                                ", "mismatch", "=", "True", "\n", "break", "\n", "", "", "if", "(", "not", "mismatch", ")", "and", "not", "(", "self", ".", "forbid_ignore_set", "and", "(", "seq", "[", "i", "+", "n", "-", "1", "]", "in", "self", ".", "forbid_ignore_set", ")", ")", ":", "\n", "                            ", "cands", ".", "add", "(", "seq", "[", "i", "+", "n", "-", "1", "]", ")", "\n", "", "", "return", "list", "(", "sorted", "(", "cands", ")", ")", "\n", "\n", "", "if", "len", "(", "partial_seqs", "[", "0", "]", ")", ">=", "self", ".", "ngram_size", ":", "\n", "                    ", "dup_cands", "=", "[", "]", "\n", "for", "seq", "in", "partial_seqs", ":", "\n", "                        ", "dup_cands", ".", "append", "(", "\n", "get_dup_ngram_candidates", "(", "seq", ",", "self", ".", "ngram_size", ")", ")", "\n", "", "if", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dup_cands", ")", ">", "0", ":", "\n", "                        ", "if", "buf_matrix", "is", "None", ":", "\n", "                            ", "vocab_size", "=", "list", "(", "log_scores", ".", "size", "(", ")", ")", "[", "-", "1", "]", "\n", "buf_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "batch_size", "*", "K", ",", "vocab_size", ")", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "                            ", "buf_matrix", ".", "fill", "(", "0", ")", "\n", "", "for", "bk", ",", "cands", "in", "enumerate", "(", "dup_cands", ")", ":", "\n", "                            ", "for", "i", ",", "wid", "in", "enumerate", "(", "cands", ")", ":", "\n", "                                ", "buf_matrix", "[", "bk", ",", "wid", "]", "=", "1.0", "\n", "", "", "forbid_word_mask", "=", "torch", ".", "tensor", "(", "\n", "buf_matrix", ",", "dtype", "=", "log_scores", ".", "dtype", ")", "\n", "forbid_word_mask", "=", "torch", ".", "reshape", "(", "\n", "forbid_word_mask", ",", "[", "batch_size", "*", "K", ",", "1", ",", "vocab_size", "]", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                        ", "forbid_word_mask", "=", "None", "\n", "", "", "", "next_pos", "+=", "1", "\n", "\n", "# [(batch, beam)]", "\n", "", "total_scores", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "total_scores", "]", "\n", "step_ids", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_ids", "]", "\n", "step_back_ptrs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_back_ptrs", "]", "\n", "# back tracking", "\n", "traces", "=", "{", "'pred_seq'", ":", "[", "]", ",", "'scores'", ":", "[", "]", ",", "'wids'", ":", "[", "]", ",", "'ptrs'", ":", "[", "]", "}", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "# [(beam,)]", "\n", "            ", "scores", "=", "[", "x", "[", "b", "]", "for", "x", "in", "total_scores", "]", "\n", "wids_list", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_ids", "]", "\n", "ptrs", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_back_ptrs", "]", "\n", "traces", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "\n", "traces", "[", "'wids'", "]", ".", "append", "(", "wids_list", ")", "\n", "traces", "[", "'ptrs'", "]", ".", "append", "(", "ptrs", ")", "\n", "# first we need to find the eos frame where all symbols are eos", "\n", "# any frames after the eos frame are invalid", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "                ", "if", "all", "(", "wid", "==", "self", ".", "eos_id", "for", "wid", "in", "wids", ")", ":", "\n", "                    ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "                ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "                    ", "if", "wid", "==", "self", ".", "eos_id", "or", "fid", "==", "last_frame_id", ":", "\n", "                        ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "self", ".", "length_penalty", ">", "0", ":", "\n", "                            ", "s", "/=", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "\n", "self", ".", "length_penalty", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "                            ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "                ", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "                    ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "pos_in_frame", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "pos_in_frame", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "seq", ")", "\n", "\n", "", "", "def", "_pad_sequence", "(", "sequences", ",", "max_len", ",", "padding_value", "=", "0", ")", ":", "\n", "            ", "trailing_dims", "=", "sequences", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "out_dims", "=", "(", "len", "(", "sequences", ")", ",", "max_len", ")", "+", "trailing_dims", "\n", "\n", "out_tensor", "=", "sequences", "[", "0", "]", ".", "data", ".", "new", "(", "*", "out_dims", ")", ".", "fill_", "(", "padding_value", ")", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "sequences", ")", ":", "\n", "                ", "length", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# use index notation to prevent duplicate references to the tensor", "\n", "out_tensor", "[", "i", ",", ":", "length", ",", "...", "]", "=", "tensor", "\n", "", "return", "out_tensor", "\n", "\n", "# convert to tensors for DataParallel", "\n", "", "for", "k", "in", "(", "'pred_seq'", ",", "'scores'", ",", "'wids'", ",", "'ptrs'", ")", ":", "\n", "            ", "ts_list", "=", "traces", "[", "k", "]", "\n", "if", "not", "isinstance", "(", "ts_list", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "dt", "=", "torch", ".", "float", "if", "k", "==", "'scores'", "else", "torch", ".", "long", "\n", "ts_list", "=", "[", "torch", ".", "tensor", "(", "it", ",", "dtype", "=", "dt", ")", "for", "it", "in", "ts_list", "]", "\n", "", "traces", "[", "k", "]", "=", "_pad_sequence", "(", "\n", "ts_list", ",", "output_length", ",", "padding_value", "=", "0", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "", "return", "traces", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.gelu": [[58, 64], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.swish": [[66, 68], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_clones": [[72, 74], ["torch.nn.modules.container.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["def", "_get_clones", "(", "module", ",", "N", ")", ":", "\n", "    ", "return", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling._get_activation_fn": [[76, 83], ["RuntimeError"], "function", ["None"], ["", "def", "_get_activation_fn", "(", "activation", ")", ":", "\n", "    ", "if", "activation", "==", "\"relu\"", ":", "\n", "        ", "return", "F", ".", "relu", "\n", "", "elif", "activation", "==", "\"gelu\"", ":", "\n", "        ", "return", "F", ".", "gelu", "\n", "\n", "", "raise", "RuntimeError", "(", "\"activation should be relu/gelu, not {}\"", ".", "format", "(", "activation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdam.__init__": [[69, 91], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdam.get_lr": [[92, 107], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "\n", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdam.step": [[108, 179], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "\n", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.__init__": [[182, 186], ["optimization.BertAdam.__init__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "init_param_group", "=", "[", "]", "\n", "super", "(", "BertAdamFineTune", ",", "self", ")", ".", "__init__", "(", "params", ",", "lr", ",", "warmup", ",", "\n", "t_total", ",", "schedule", ",", "b1", ",", "b2", ",", "e", ",", "weight_decay", ",", "max_grad_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.save_init_param_group": [[187, 203], ["zip", "zip", "optimization.BertAdamFineTune.init_param_group.append", "optimization.BertAdamFineTune.init_param_group.append", "p.data.clone().detach", "any", "init_p_list.append", "print", "p.data.clone().detach.zero_", "p.data.clone"], "methods", ["None"], ["", "def", "save_init_param_group", "(", "self", ",", "param_groups", ",", "name_groups", ",", "missing_keys", ")", ":", "\n", "        ", "self", ".", "init_param_group", "=", "[", "]", "\n", "for", "group", ",", "name", "in", "zip", "(", "param_groups", ",", "name_groups", ")", ":", "\n", "            ", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                ", "init_p_list", "=", "[", "]", "\n", "for", "p", ",", "n", "in", "zip", "(", "group", "[", "'params'", "]", ",", "name", ")", ":", "\n", "                    ", "init_p", "=", "p", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "any", "(", "mk", "in", "n", "for", "mk", "in", "missing_keys", ")", ":", "\n", "                        ", "print", "(", "\"[no finetuning weight decay]\"", ",", "n", ")", "\n", "# should use the original weight decay", "\n", "init_p", ".", "zero_", "(", ")", "\n", "", "init_p_list", ".", "append", "(", "init_p", ")", "\n", "", "self", ".", "init_param_group", ".", "append", "(", "init_p_list", ")", "\n", "", "else", ":", "\n", "# placeholder", "\n", "                ", "self", ".", "init_param_group", ".", "append", "(", "[", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.step": [[204, 280], ["enumerate", "closure", "enumerate", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "i_group", ",", "group", "in", "enumerate", "(", "self", ".", "param_groups", ")", ":", "\n", "            ", "for", "i_p", ",", "p", "in", "enumerate", "(", "group", "[", "'params'", "]", ")", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "if", "self", ".", "init_param_group", ":", "\n", "                        ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "(", "2.0", "*", "p", ".", "data", "-", "\n", "self", ".", "init_param_group", "[", "i_group", "]", "[", "i_p", "]", ")", "\n", "", "else", ":", "\n", "                        ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "\n", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.BertAdamFineTune.load_state_dict_subset_finetune": [[281, 350], ["copy.deepcopy", "any", "collections.defaultdict", "state_dict[].items", "optimization.BertAdamFineTune.__setstate__", "ValueError", "len", "len", "ValueError", "isinstance", "len", "len", "zip", "param.is_floating_point", "value.to.to.to", "isinstance", "optimization.BertAdamFineTune.load_state_dict_subset_finetune.cast"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.PPL.PPL_modeling.TransformerDecoderLayer.__setstate__"], ["", "def", "load_state_dict_subset_finetune", "(", "self", ",", "state_dict", ",", "num_load_group", ")", ":", "\n", "        ", "r\"\"\"Loads the optimizer state.\n\n        Arguments:\n            state_dict (dict): optimizer state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"", "\n", "# deepcopy, to be consistent with module API", "\n", "state_dict", "=", "deepcopy", "(", "state_dict", ")", "\n", "# Validate the state_dict", "\n", "groups", "=", "self", ".", "param_groups", "\n", "saved_groups", "=", "state_dict", "[", "'param_groups'", "]", "\n", "\n", "if", "len", "(", "groups", ")", "<", "num_load_group", "or", "len", "(", "saved_groups", ")", "<", "num_load_group", ":", "\n", "            ", "raise", "ValueError", "(", "\"loaded state dict has a different number of \"", "\n", "\"parameter groups\"", ")", "\n", "", "param_lens", "=", "(", "len", "(", "g", "[", "'params'", "]", ")", "for", "g", "in", "groups", "[", ":", "num_load_group", "]", ")", "\n", "saved_lens", "=", "(", "len", "(", "g", "[", "'params'", "]", ")", "for", "g", "in", "saved_groups", "[", ":", "num_load_group", "]", ")", "\n", "if", "any", "(", "p_len", "!=", "s_len", "for", "p_len", ",", "s_len", "in", "zip", "(", "param_lens", ",", "saved_lens", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"loaded state dict contains a parameter group \"", "\n", "\"that doesn't match the size of optimizer's group\"", ")", "\n", "\n", "# Update the state", "\n", "", "id_map", "=", "{", "old_id", ":", "p", "for", "old_id", ",", "p", "in", "\n", "zip", "(", "chain", "(", "*", "(", "g", "[", "'params'", "]", "for", "g", "in", "saved_groups", "[", ":", "num_load_group", "]", ")", ")", ",", "\n", "chain", "(", "*", "(", "g", "[", "'params'", "]", "for", "g", "in", "groups", "[", ":", "num_load_group", "]", ")", ")", ")", "}", "\n", "\n", "def", "cast", "(", "param", ",", "value", ")", ":", "\n", "            ", "r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"", "\n", "if", "isinstance", "(", "value", ",", "torch", ".", "Tensor", ")", ":", "\n", "# Floating-point types are a bit special here. They are the only ones", "\n", "# that are assumed to always match the type of params.", "\n", "                ", "if", "param", ".", "is_floating_point", "(", ")", ":", "\n", "                    ", "value", "=", "value", ".", "to", "(", "param", ".", "dtype", ")", "\n", "", "value", "=", "value", ".", "to", "(", "param", ".", "device", ")", "\n", "return", "value", "\n", "", "elif", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "                ", "return", "{", "k", ":", "cast", "(", "param", ",", "v", ")", "for", "k", ",", "v", "in", "value", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "value", ",", "container_abcs", ".", "Iterable", ")", ":", "\n", "                ", "return", "type", "(", "value", ")", "(", "cast", "(", "param", ",", "v", ")", "for", "v", "in", "value", ")", "\n", "", "else", ":", "\n", "                ", "return", "value", "\n", "\n", "# Copy state assigned to params (and cast tensors to appropriate types).", "\n", "# State that is not assigned to params is copied as is (needed for", "\n", "# backward compatibility).", "\n", "", "", "state", "=", "defaultdict", "(", "dict", ")", "\n", "for", "k", ",", "v", "in", "state_dict", "[", "'state'", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "id_map", ":", "\n", "                ", "param", "=", "id_map", "[", "k", "]", "\n", "state", "[", "param", "]", "=", "cast", "(", "param", ",", "v", ")", "\n", "", "else", ":", "\n", "                ", "state", "[", "k", "]", "=", "v", "\n", "# handle additional params", "\n", "", "", "for", "k", ",", "v", "in", "self", ".", "state", ":", "\n", "            ", "if", "k", "not", "in", "state", ":", "\n", "                ", "state", "[", "k", "]", "=", "v", "\n", "\n", "# do not change groups: {'weight_decay': 0.01, 'lr': 9.995e-06, 'schedule': 'warmup_linear', 'warmup': 0.1, 't_total': 400000, 'b1': 0.9, 'b2': 0.999, 'e': 1e-06, 'max_grad_norm': 1.0, 'params': [...]}", "\n", "# # Update parameter groups, setting their 'params' value", "\n", "# def update_group(group, new_group):", "\n", "#     new_group['params'] = group['params']", "\n", "#     return new_group", "\n", "# param_groups = [", "\n", "#     update_group(g, ng) for g, ng in zip(groups[:num_load_group], saved_groups[:num_load_group])]", "\n", "# # handle additional params", "\n", "# param_groups.extend(groups[num_load_group:])", "\n", "\n", "", "", "self", ".", "__setstate__", "(", "{", "'state'", ":", "state", ",", "'param_groups'", ":", "groups", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_cosine": [[29, 33], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_constant": [[35, 39], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.warmup_linear": [[41, 45], ["max"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "max", "(", "(", "x", "-", "1.", ")", "/", "(", "warmup", "-", "1.", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization.find_state_dict_subset_finetune": [[352, 402], ["set", "zip", "new_state_dict[].items", "zip", "optimization.find_state_dict_subset_finetune._filter_group"], "function", ["None"], ["", "", "def", "find_state_dict_subset_finetune", "(", "org_state_dict", ",", "org_name_list", ",", "no_decay", ",", "param_optimizer", ")", ":", "\n", "# only use the bert encoder and embeddings", "\n", "    ", "want_name_set", "=", "set", "(", ")", "\n", "for", "n", "in", "org_name_list", ":", "\n", "        ", "if", "(", "'bert.encoder'", "in", "n", ")", "or", "(", "'bert.embeddings'", "in", "n", ")", ":", "\n", "            ", "want_name_set", ".", "add", "(", "n", ")", "\n", "# original: name to pid, pid to name", "\n", "", "", "org_grouped_names", "=", "[", "[", "n", "for", "n", "in", "org_name_list", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "n", "for", "n", "in", "org_name_list", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", "]", "\n", "org_n2id", ",", "org_id2n", "=", "{", "}", ",", "{", "}", "\n", "for", "ng", ",", "pg", "in", "zip", "(", "org_grouped_names", ",", "org_state_dict", "[", "'param_groups'", "]", ")", ":", "\n", "        ", "for", "n", ",", "pid", "in", "zip", "(", "ng", ",", "pg", "[", "'params'", "]", ")", ":", "\n", "            ", "org_n2id", "[", "n", "]", "=", "pid", "\n", "org_id2n", "[", "pid", "]", "=", "n", "\n", "# group by: whether pretrained; whether weight decay", "\n", "", "", "g_np_list", "=", "[", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "in", "want_name_set", "and", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "in", "want_name_set", "and", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "not", "in", "want_name_set", "and", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "not", "in", "want_name_set", "and", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "0", "]", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "1", "]", "]", ",", "'weight_decay'", ":", "0.0", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "2", "]", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "3", "]", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "new_state_dict", "=", "{", "}", "\n", "# regroup the original state_dict", "\n", "new_state_dict", "[", "'state'", "]", "=", "{", "pid", ":", "v", "for", "pid", ",", "v", "in", "org_state_dict", "[", "'state'", "]", ".", "items", "(", "\n", ")", "if", "pid", "not", "in", "org_id2n", "or", "org_id2n", "[", "pid", "]", "in", "want_name_set", "}", "\n", "# reset step count to 0", "\n", "for", "pid", ",", "st", "in", "new_state_dict", "[", "'state'", "]", ".", "items", "(", ")", ":", "\n", "        ", "st", "[", "'step'", "]", "=", "0", "\n", "\n", "", "def", "_filter_group", "(", "group", ",", "g_np_list", ",", "i", ",", "org_n2id", ")", ":", "\n", "        ", "packed", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "group", ".", "items", "(", ")", "if", "k", "!=", "'params'", "}", "\n", "packed", "[", "'params'", "]", "=", "[", "pid", "for", "pid", "in", "group", "[", "'params'", "]", "\n", "if", "pid", "in", "org_id2n", "and", "org_id2n", "[", "pid", "]", "in", "want_name_set", "]", "\n", "assert", "len", "(", "g_np_list", "[", "i", "]", ")", "==", "len", "(", "packed", "[", "'params'", "]", ")", "\n", "# keep them the same order", "\n", "packed", "[", "'params'", "]", "=", "[", "org_n2id", "[", "n", "]", "for", "n", ",", "p", "in", "g_np_list", "[", "i", "]", "]", "\n", "return", "packed", "\n", "", "new_state_dict", "[", "'param_groups'", "]", "=", "[", "_filter_group", "(", "\n", "g", ",", "g_np_list", ",", "i", ",", "org_n2id", ")", "for", "i", ",", "g", "in", "enumerate", "(", "org_state_dict", "[", "'param_groups'", "]", ")", "]", "\n", "return", "new_state_dict", ",", "optimizer_grouped_parameters", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.url_to_filename": [[30, 46], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.filename_to_url": [[48, 72], ["isinstance", "os.path.join", "str", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.cached_path": [[74, 102], ["isinstance", "isinstance", "urllib.parse.urlparse", "str", "str", "file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.split_s3_path": [[104, 115], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.s3_request": [[117, 134], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.s3_etag": [[136, 143], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.s3_get": [[145, 151], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.http_get": [[153, 163], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.get_from_cache": [[165, 222], ["isinstance", "os.makedirs", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.url_to_filename", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_etag", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_get", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.read_set_from_file": [[224, 234], ["set", "open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.file_utils.get_file_extension": [[236, 240], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BertTokenizer.__init__": [[93, 105], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[X_SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ",", "\"<#Q2K#>\"", ",", "\"<#K#>\"", ",", "\"<#Q#>\"", ",", "\"<#Q2R#>\"", ",", "\"[#2R#]\"", ",", "\"[#2K#],<#K2R#>\"", ")", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "\n", "do_lower_case", "=", "do_lower_case", ",", "never_split", "=", "never_split", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BertTokenizer.tokenize": [[106, 112], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BertTokenizer.convert_tokens_to_ids": [[113, 126], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "\n", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BertTokenizer.convert_ids_to_tokens": [[127, 133], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BertTokenizer.from_pretrained": [[134, 171], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.WhitespaceTokenizer.tokenize": [[174, 176], ["tokenization.whitespace_tokenize"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize"], ["    ", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "whitespace_tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer.__init__": [[181, 189], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ",", "\"<#Q2K#>\"", ",", "\"<#K#>\"", ",", "\"<#Q#>\"", ",", "\"<#Q2R#>\"", ",", "\"[#2R#]\"", ",", "\"[#2K#],<#K2R#>\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer.tokenize": [[190, 210], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer._run_strip_accents": [[211, 221], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer._run_split_on_punc": [[222, 243], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[244, 256], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer._is_chinese_char": [[257, 278], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.BasicTokenizer._clean_text": [[279, 291], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_whitespace", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.WordpieceTokenizer.__init__": [[296, 300], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.WordpieceTokenizer.tokenize": [[301, 351], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.load_vocab": [[51, 79], ["range", "collections.OrderedDict", "open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "# mapping unused tokens to special tokens", "\n", "extra_map", "=", "{", "}", "\n", "extra_map", "[", "'[unused1]'", "]", "=", "'[X_SEP]'", "\n", "for", "i", "in", "range", "(", "30", ")", ":", "\n", "        ", "extra_map", "[", "'[unused{}]'", ".", "format", "(", "i", "+", "2", ")", "]", "=", "'[SEP_{}]'", ".", "format", "(", "i", "-", "20", ")", "\n", "", "extra_map", "[", "'[unused32]'", "]", "=", "'[S2S_SEP]'", "\n", "extra_map", "[", "'[unused33]'", "]", "=", "'[S2S_CLS]'", "\n", "extra_map", "[", "'[unused34]'", "]", "=", "'[L2R_SEP]'", "\n", "extra_map", "[", "'[unused35]'", "]", "=", "'[L2R_CLS]'", "\n", "extra_map", "[", "'[unused36]'", "]", "=", "'[R2L_SEP]'", "\n", "extra_map", "[", "'[unused37]'", "]", "=", "'[R2L_CLS]'", "\n", "extra_map", "[", "'[unused38]'", "]", "=", "'[S2S_SOS]'", "\n", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "if", "token", "in", "extra_map", ":", "\n", "                ", "token", "=", "extra_map", "[", "token", "]", "\n", "", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization.whitespace_tokenize": [[81, 88], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization._is_whitespace": [[353, 363], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization._is_control": [[365, 375], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.tokenization._is_punctuation": [[377, 391], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.loss.LabelSmoothingLoss.__init__": [[19, 34], ["torch.nn.modules.loss._Loss.__init__", "torch.full", "torch.full", "torch.full", "torch.full", "loss.LabelSmoothingLoss.register_buffer", "torch.full.unsqueeze", "torch.full.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "label_smoothing", "=", "0", ",", "tgt_vocab_size", "=", "0", ",", "ignore_index", "=", "0", ",", "size_average", "=", "None", ",", "reduce", "=", "None", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "assert", "0.0", "<", "label_smoothing", "<=", "1.0", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "super", "(", "LabelSmoothingLoss", ",", "self", ")", ".", "__init__", "(", "\n", "size_average", "=", "size_average", ",", "reduce", "=", "reduce", ",", "reduction", "=", "reduction", ")", "\n", "\n", "assert", "label_smoothing", ">", "0", "\n", "assert", "tgt_vocab_size", ">", "0", "\n", "\n", "smoothing_value", "=", "label_smoothing", "/", "(", "tgt_vocab_size", "-", "2", ")", "\n", "one_hot", "=", "torch", ".", "full", "(", "(", "tgt_vocab_size", ",", ")", ",", "smoothing_value", ")", "\n", "one_hot", "[", "self", ".", "ignore_index", "]", "=", "0", "\n", "self", ".", "register_buffer", "(", "'one_hot'", ",", "one_hot", ".", "unsqueeze", "(", "0", ")", ")", "\n", "self", ".", "confidence", "=", "1.0", "-", "label_smoothing", "\n", "self", ".", "tgt_vocab_size", "=", "tgt_vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.loss.LabelSmoothingLoss.forward": [[35, 49], ["output.view.view.view", "target.view.view.view", "loss.LabelSmoothingLoss.one_hot.repeat", "loss.LabelSmoothingLoss.scatter_", "loss.LabelSmoothingLoss.masked_fill_", "torch.kl_div().view().sum", "torch.kl_div().view().sum", "output.view.view.size", "target.view.view.size", "target.view.view.size", "target.view.view.size", "target.view.view.unsqueeze", "torch.kl_div().view", "torch.kl_div().view", "torch.kl_div", "torch.kl_div"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        output (FloatTensor): batch_size * num_pos * n_classes\n        target (LongTensor): batch_size * num_pos\n        \"\"\"", "\n", "assert", "self", ".", "tgt_vocab_size", "==", "output", ".", "size", "(", "2", ")", "\n", "batch_size", ",", "num_pos", "=", "target", ".", "size", "(", "0", ")", ",", "target", ".", "size", "(", "1", ")", "\n", "output", "=", "output", ".", "view", "(", "-", "1", ",", "self", ".", "tgt_vocab_size", ")", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "model_prob", "=", "self", ".", "one_hot", ".", "repeat", "(", "target", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "model_prob", ".", "scatter_", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "confidence", ")", "\n", "model_prob", ".", "masked_fill_", "(", "(", "target", "==", "self", ".", "ignore_index", ")", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "return", "F", ".", "kl_div", "(", "output", ",", "model_prob", ",", "reduction", "=", "'none'", ")", ".", "view", "(", "batch_size", ",", "num_pos", ",", "-", "1", ")", ".", "sum", "(", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.__init__": [[8, 16], ["apex.optimizers.FP16_Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "init_optimizer", ",", "\n", "static_loss_scale", "=", "1.0", ",", "\n", "dynamic_loss_scale", "=", "False", ",", "\n", "dynamic_loss_args", "=", "None", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "super", "(", "FP16_Optimizer_State", ",", "self", ")", ".", "__init__", "(", "init_optimizer", ",", "\n", "static_loss_scale", ",", "dynamic_loss_scale", ",", "dynamic_loss_args", ",", "verbose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict": [[17, 39], ["optimization_fp16.FP16_Optimizer_State.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"", "\n", "state_dict", "=", "{", "}", "\n", "state_dict", "[", "'dynamic_loss_scale'", "]", "=", "self", ".", "dynamic_loss_scale", "\n", "state_dict", "[", "'cur_scale'", "]", "=", "self", ".", "cur_scale", "\n", "state_dict", "[", "'cur_iter'", "]", "=", "self", ".", "cur_iter", "\n", "if", "state_dict", "[", "'dynamic_loss_scale'", "]", ":", "\n", "            ", "state_dict", "[", "'last_overflow_iter'", "]", "=", "self", ".", "last_overflow_iter", "\n", "state_dict", "[", "'scale_factor'", "]", "=", "self", ".", "scale_factor", "\n", "state_dict", "[", "'scale_window'", "]", "=", "self", ".", "scale_window", "\n", "", "state_dict", "[", "'optimizer_state_dict'", "]", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'fp32_groups_flat'", "]", "=", "self", ".", "fp32_groups_flat", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict": [[40, 81], ["optimization_fp16.FP16_Optimizer_State.optimizer.load_state_dict", "zip", "current.data.copy_"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.pytorch_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"\n        Loads a state_dict created by an earlier call to state_dict(). \n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, \n        whose parameters in turn came from ``model``, it is expected that the user \n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"", "\n", "# I think it should actually be ok to reload the optimizer before the model.", "\n", "self", ".", "dynamic_loss_scale", "=", "state_dict", "[", "'dynamic_loss_scale'", "]", "\n", "self", ".", "cur_scale", "=", "state_dict", "[", "'cur_scale'", "]", "\n", "self", ".", "cur_iter", "=", "state_dict", "[", "'cur_iter'", "]", "\n", "if", "state_dict", "[", "'dynamic_loss_scale'", "]", ":", "\n", "            ", "self", ".", "last_overflow_iter", "=", "state_dict", "[", "'last_overflow_iter'", "]", "\n", "self", ".", "scale_factor", "=", "state_dict", "[", "'scale_factor'", "]", "\n", "self", ".", "scale_window", "=", "state_dict", "[", "'scale_window'", "]", "\n", "", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", "[", "'optimizer_state_dict'", "]", ")", "\n", "# At this point, the optimizer's references to the model's fp32 parameters are up to date.", "\n", "# The optimizer's hyperparameters and internal buffers are also up to date.", "\n", "# However, the fp32 master copies of the model's fp16 params stored by the optimizer are still", "\n", "# out of date.  There are two options.", "\n", "# 1:  Refresh the master params from the model's fp16 params.", "\n", "# This requires less storage but incurs precision loss.", "\n", "# 2:  Save and restore the fp32 master copies separately.", "\n", "# We choose option 2.", "\n", "#", "\n", "# Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device", "\n", "# of their associated parameters, because it's possible those buffers might not exist yet in", "\n", "# the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been", "\n", "# constructed in the same way as the one whose state_dict we are loading, the same master params", "\n", "# are guaranteed to exist, so we can just copy_() from the saved master params.", "\n", "for", "current", ",", "saved", "in", "zip", "(", "self", ".", "fp32_groups_flat", ",", "state_dict", "[", "'fp32_groups_flat'", "]", ")", ":", "\n", "            ", "current", ".", "data", ".", "copy_", "(", "saved", ".", "data", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.move_stop_words": [[14, 17], ["str.split", "w.lower"], "function", ["None"], ["def", "move_stop_words", "(", "str", ")", ":", "\n", "\t", "item", "=", "\" \"", ".", "join", "(", "[", "w", "for", "w", "in", "str", ".", "split", "(", ")", "if", "not", "w", ".", "lower", "(", ")", "in", "stop_words", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.normalize_answer": [[21, 37], ["cmu_dog_preprocess.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "\t", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "\t\t", "return", "re_art", ".", "sub", "(", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "\t\t", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "\t\t", "return", "re_punc", ".", "sub", "(", "' '", ",", "text", ")", "# convert punctuation to spaces", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "\t\t", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.truncate": [[38, 43], ["str.strip.strip", "len", "str.strip.split", "str.strip.split", "max"], "function", ["None"], ["", "def", "truncate", "(", "str", ",", "num", ")", ":", "\n", "\t", "str", "=", "str", ".", "strip", "(", ")", "\n", "length", "=", "len", "(", "str", ".", "split", "(", ")", ")", "\n", "list", "=", "str", ".", "split", "(", ")", "[", "max", "(", "0", ",", "length", "-", "num", ")", ":", "]", "\n", "return", "\" \"", ".", "join", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.detokenize": [[44, 53], ["tk_str.strip().split", "tk_str.strip", "tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_str", ")", ":", "\n", "\t", "tk_list", "=", "tk_str", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "\t\t", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "\t\t\t", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "\t\t\t", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "r_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.main": [[65, 105], ["open", "open", "range", "len", "SRC[].strip().replace().replace", "TGT[].strip().replace", "len", "out1.write", "out2.write", "nltk.sent_tokenize", "cmu_dog_preprocess.normalize_answer", "cmu_dog_preprocess.normalize_answer", "metrics.bleu_metric", "use_know_list.index", "use_know_list.index", "print", "SRC[].strip().replace", "TGT[].strip", "nltk.sent_tokenize", "nltk.sent_tokenize", "[].replace", "cmu_dog_preprocess.move_stop_words", "cmu_dog_preprocess.move_stop_words", "[].replace", "[].replace", "know_line.strip", "TGT[].strip().replace.strip", "tokenizer.tokenize", "tokenizer.tokenize", "len", "SRC[].strip", "src_line.strip", "TGT[].strip().replace.strip", "KNL[].strip().split", "KNL[].strip().split", "KNL[].strip().split", "KNL[].strip", "KNL[].strip", "KNL[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_metric", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.move_stop_words", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.move_stop_words", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "main", "(", ")", ":", "\n", "\t", "mean_len_knows", "=", "0", "\n", "with", "open", "(", "\"test_data/cmu_dog/test_cmu_dog.src.tk\"", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out1", ",", "open", "(", "\"test_data/cmu_dog/test_cmu_dog.tgt.tk\"", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out2", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "SRC", ")", ")", ":", "\n", "\t\t\t", "query_line", "=", "SRC", "[", "i", "]", ".", "strip", "(", ")", ".", "replace", "(", "\" &lt; SEP &gt; \"", ",", "\"<#Q#>\"", ")", ".", "replace", "(", "\"&apos;\"", ",", "\"'\"", ")", "\n", "tgt_line", "=", "TGT", "[", "i", "]", ".", "strip", "(", ")", ".", "replace", "(", "\"&apos;\"", ",", "\"'\"", ")", "\n", "# choice no.3", "\n", "knows", "=", "nltk", ".", "sent_tokenize", "(", "\n", "KNL", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" &lt; SEP &gt; \"", ")", "[", "2", "]", ".", "replace", "(", "\"&apos;\"", ",", "\"'\"", ")", ")", "+", "nltk", ".", "sent_tokenize", "(", "\n", "KNL", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" &lt; SEP &gt; \"", ")", "[", "0", "]", ".", "replace", "(", "\"&apos;\"", ",", "\"'\"", ")", ")", "+", "nltk", ".", "sent_tokenize", "(", "KNL", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" &lt; SEP &gt; \"", ")", "[", "1", "]", ".", "replace", "(", "\"&apos;\"", ",", "\"'\"", ")", ")", "\n", "\n", "max_b2", "=", "0", "\n", "check_sentence", "=", "\"\"", "\n", "\n", "for", "know_line", "in", "knows", ":", "\n", "\t\t\t\t", "pro_know", "=", "normalize_answer", "(", "move_stop_words", "(", "know_line", ".", "strip", "(", ")", ")", ")", "\n", "pro_response", "=", "normalize_answer", "(", "move_stop_words", "(", "tgt_line", ".", "strip", "(", ")", ")", ")", "\n", "b1", ",", "b2", ",", "b3", "=", "bleu_metric", "(", "[", "pro_know", "]", ",", "[", "pro_response", "]", ")", "\n", "if", "b2", ">=", "max_b2", ":", "\n", "\t\t\t\t\t", "max_b2", "=", "b2", "\n", "check_sentence", "=", "know_line", "\n", "\n", "", "", "mean_len_knows", "+=", "len", "(", "knows", ")", "\n", "use_know_list", "=", "knows", "\n", "if", "check_sentence", "in", "use_know_list", ":", "\n", "\t\t\t\t", "index", "=", "use_know_list", ".", "index", "(", "check_sentence", ")", "\n", "use_know_list", "[", "0", "]", ",", "use_know_list", "[", "index", "]", "=", "use_know_list", "[", "index", "]", ",", "use_know_list", "[", "0", "]", "\n", "", "else", ":", "\n", "\t\t\t\t", "use_know_list", "[", "0", "]", "=", "check_sentence", "\n", "", "assert", "use_know_list", ".", "index", "(", "check_sentence", ")", "==", "0", "\n", "\n", "used_know_line", "=", "\" <#K#> \"", ".", "join", "(", "use_know_list", ")", "\n", "\n", "src_line", "=", "query_line", "+", "\" <#Q2K#> \"", "+", "used_know_line", "\n", "\n", "out1", ".", "write", "(", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "src_line", ".", "strip", "(", ")", ")", ")", "+", "\"\\n\"", ")", "\n", "out2", ".", "write", "(", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "tgt_line", ".", "strip", "(", ")", ")", ")", "+", "\"\\n\"", ")", "\n", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "\t\t\t\t", "print", "(", "\"have process {} data / {}\"", ".", "format", "(", "i", ",", "len", "(", "SRC", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.process_ks": [[107, 124], ["print", "open", "file.readlines", "open", "range", "len", "cmu_dog_preprocess.truncate", "[].split", "range", "[].strip", "len", "truncate.strip", "know_list[].strip", "out.write", "out.write", "query.strip.strip", "src[].strip().split", "src[].strip().split", "src[].strip", "src[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate"], ["", "", "", "", "def", "process_ks", "(", "src_path", ",", "out_path", ")", ":", "\n", "\t", "with", "open", "(", "src_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "src", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "\t\t\t", "query", "=", "truncate", "(", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", ".", "strip", "(", ")", ",", "128", ")", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "know_list", ")", ")", ":", "\n", "\t\t\t\t", "line", "=", "query", ".", "strip", "(", ")", "\n", "line", "+=", "\" <#Q2K#> \"", "\n", "line", "+=", "know_list", "[", "t", "]", ".", "strip", "(", ")", "\n", "\n", "out", ".", "write", "(", "line", ".", "strip", "(", ")", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "", "print", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.cmu_dog_preprocess.detokenize_file": [[126, 132], ["open", "file.readlines", "open", "range", "len", "out.write", "cmu_dog_preprocess.detokenize", "data[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize"], ["", "def", "detokenize_file", "(", "file_path", ")", ":", "\n", "\t", "with", "open", "(", "file_path", "+", "\".tk\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "\t\t\t", "out", ".", "write", "(", "detokenize", "(", "data", "[", "i", "]", ".", "strip", "(", ")", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.distinct": [[15, 30], ["collections.Counter", "collections.Counter", "hypo.split", "unigram_counter.update", "bigram_counter.update", "len", "sum", "len", "sum", "nltk.ngrams", "unigram_counter.values", "bigram_counter.values"], "function", ["None"], ["def", "distinct", "(", "hypothesis", ")", ":", "\n", "    ", "'''\n    compute distinct metric\n    :param hypothesis: list of str\n    :return:\n    '''", "\n", "unigram_counter", ",", "bigram_counter", "=", "Counter", "(", ")", ",", "Counter", "(", ")", "\n", "for", "hypo", "in", "hypothesis", ":", "\n", "        ", "tokens", "=", "hypo", ".", "split", "(", ")", "\n", "unigram_counter", ".", "update", "(", "tokens", ")", "\n", "bigram_counter", ".", "update", "(", "ngrams", "(", "tokens", ",", "2", ")", ")", "\n", "\n", "", "distinct_1", "=", "len", "(", "unigram_counter", ")", "/", "sum", "(", "unigram_counter", ".", "values", "(", ")", ")", "#\u8d8a\u5927\u8d8a\u597d \u4e0d\u540c\u7684\u8bcd\u7ec4", "\n", "distinct_2", "=", "len", "(", "bigram_counter", ")", "/", "sum", "(", "bigram_counter", ".", "values", "(", ")", ")", "\n", "return", "distinct_1", ",", "distinct_2", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer": [[36, 52], ["metrics.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re_art", ".", "sub", "(", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "return", "re_punc", ".", "sub", "(", "' '", ",", "text", ")", "# convert punctuation to spaces", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._prec_recall_f1_score": [[54, 71], ["sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["None"], ["", "def", "_prec_recall_f1_score", "(", "pred_items", ",", "gold_items", ")", ":", "\n", "    ", "\"\"\"\n    Compute precision, recall and f1 given a set of gold and prediction items.\n    :param pred_items: iterable of predicted values\n    :param gold_items: iterable of gold values\n    :return: tuple (p, r, f1) for precision, recall, f1\n    \"\"\"", "\n", "common", "=", "Counter", "(", "gold_items", ")", "&", "Counter", "(", "pred_items", ")", "\n", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "pred_items", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "gold_items", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._f1_score": [[73, 82], ["normalize_answer().split", "metrics._prec_recall_f1_score", "max", "max", "max", "metrics.normalize_answer", "normalize_answer().split", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._prec_recall_f1_score", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_f1_score", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Return the max F1 score between the guess and *any* answer.\"\"\"", "\n", "if", "guess", "is", "None", "or", "answers", "is", "None", ":", "\n", "        ", "return", "0", "\n", "", "g_tokens", "=", "normalize_answer", "(", "guess", ")", ".", "split", "(", ")", "\n", "scores", "=", "[", "\n", "_prec_recall_f1_score", "(", "g_tokens", ",", "normalize_answer", "(", "a", ")", ".", "split", "(", ")", ")", "for", "a", "in", "answers", "\n", "]", "\n", "return", "max", "(", "f1", "for", "_", ",", "_", ",", "f1", "in", "scores", ")", ",", "max", "(", "pre", "for", "pre", ",", "_", ",", "_", "in", "scores", ")", ",", "max", "(", "rec", "for", "_", ",", "rec", ",", "_", "in", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.f_one": [[84, 100], ["zip", "metrics._f1_score", "f1.append", "pre.append", "rec.append", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._f1_score"], ["", "def", "f_one", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "'''\n    calculate f1 metric\n    :param hypothesis: list of str\n    :param references: list of str\n    :return:\n    '''", "\n", "f1", "=", "[", "]", "\n", "pre", "=", "[", "]", "\n", "rec", "=", "[", "]", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypothesis", ",", "references", ")", ":", "\n", "        ", "res", "=", "_f1_score", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "f1", ".", "append", "(", "res", "[", "0", "]", ")", "\n", "pre", ".", "append", "(", "res", "[", "1", "]", ")", "\n", "rec", ".", "append", "(", "res", "[", "2", "]", ")", "\n", "", "return", "np", ".", "mean", "(", "f1", ")", ",", "np", ".", "mean", "(", "pre", ")", ",", "np", ".", "mean", "(", "rec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu1": [[102, 118], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu1", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "1.0", ",", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu2": [[120, 136], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu2", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "2.0", ",", "1.0", "/", "2.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu3": [[138, 154], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu3", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu4": [[156, 172], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer"], ["", "def", "_bleu4", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu": [[174, 183], ["zip", "numpy.mean", "tuple", "metrics._bleu1", "metrics._bleu2", "metrics._bleu3", "np.mean.append"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu1", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu2", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._bleu3"], ["", "def", "bleu", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "bleu_scores", "=", "[", "]", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypothesis", ",", "references", ")", ":", "\n", "        ", "b1", "=", "_bleu1", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "b2", "=", "_bleu2", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "b3", "=", "_bleu3", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "bleu_scores", ".", "append", "(", "[", "b1", ",", "b2", ",", "b3", "]", ")", "\n", "", "bleu_scores", "=", "np", ".", "mean", "(", "bleu_scores", ",", "axis", "=", "0", ")", "# [bleu1, bleu2, bleu3]", "\n", "return", "tuple", "(", "bleu_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_corpus": [[184, 196], ["hypothesis.copy.copy", "references.copy.copy", "corpus_bleu", "corpus_bleu", "corpus_bleu", "hyp.split", "ref.split", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.SmoothingFunction"], "function", ["None"], ["", "def", "bleu_corpus", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "from", "nltk", ".", "translate", ".", "bleu_score", "import", "corpus_bleu", "\n", "hypothesis", "=", "hypothesis", ".", "copy", "(", ")", "\n", "references", "=", "references", ".", "copy", "(", ")", "\n", "hypothesis", "=", "[", "hyp", ".", "split", "(", ")", "for", "hyp", "in", "hypothesis", "]", "\n", "references", "=", "[", "[", "ref", ".", "split", "(", ")", "]", "for", "ref", "in", "references", "]", "\n", "# hypothesis = [normalize_answer(hyp).split(\" \") for hyp in hypothesis]", "\n", "# references = [[normalize_answer(ref).split(\" \")] for ref in references]", "\n", "b1", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "1.0", ",", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "b2", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "2.0", ",", "1.0", "/", "2.0", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "b3", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "return", "(", "b1", ",", "b2", ",", "b3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_metric": [[197, 199], ["metrics.bleu_corpus"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_corpus"], ["", "def", "bleu_metric", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "return", "bleu_corpus", "(", "hypothesis", ",", "references", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.knowledge_metric": [[201, 232], ["get_stop_words", "zip", "metrics._prec_recall_f1_score", "p_scores.append", "r_scores.append", "f_scores.append", "numpy.mean", "numpy.mean", "numpy.mean", "hyp.split", "know.split"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics._prec_recall_f1_score"], ["", "def", "knowledge_metric", "(", "responses", ",", "knowledges", ")", ":", "\n", "    ", "'''\n    calculate knowledge metric\n    :param responses: list of str\n    :param knowledges: list of list of str\n    :return:\n    '''", "\n", "stop_words", "=", "get_stop_words", "(", "'en'", ")", "\n", "p_scores", ",", "r_scores", ",", "f_scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "hyp", ",", "know", "in", "zip", "(", "responses", ",", "knowledges", ")", ":", "\n", "# hyp_tokens = set([w for w in hyp.split() if w not in stop_words])", "\n", "# know = ' '.join(know)", "\n", "# know_tokens = set([w for w in know.split() if w not in stop_words])", "\n", "#", "\n", "# if len(hyp_tokens & know_tokens) == 0:", "\n", "#     _p, _r, _f1 = .0, .0, .0", "\n", "# else:", "\n", "#     _p = len(hyp_tokens & know_tokens) / len(hyp_tokens)", "\n", "#     _r = len(hyp_tokens & know_tokens) / len(know_tokens)", "\n", "#     _f1 = 2 * (_p * _r) / (_p + _r)", "\n", "\n", "# hyp_tokens = list(set([w for w in hyp.split() if w not in stop_words]))", "\n", "        ", "hyp_tokens", "=", "[", "w", "for", "w", "in", "hyp", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", "\n", "know", "=", "' '", ".", "join", "(", "know", ")", "\n", "know_tokens", "=", "[", "w", "for", "w", "in", "know", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", "\n", "_p", ",", "_r", ",", "_f1", "=", "_prec_recall_f1_score", "(", "hyp_tokens", ",", "know_tokens", ")", "\n", "p_scores", ".", "append", "(", "_p", ")", "\n", "r_scores", ".", "append", "(", "_r", ")", "\n", "f_scores", ".", "append", "(", "_f1", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "r_scores", ")", ",", "np", ".", "mean", "(", "p_scores", ")", ",", "np", ".", "mean", "(", "f_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.knowledge_metric_new": [[233, 279], ["get_stop_words", "zip", "set", "set", "p_scores.append", "r_scores.append", "f_scores.append", "numpy.mean", "numpy.mean", "numpy.mean", "len", "len", "len", "len", "len", "hyp.split", "know.split"], "function", ["None"], ["", "def", "knowledge_metric_new", "(", "responses", ",", "knowledges", ")", ":", "\n", "    ", "'''\n    calculate knowledge metric\n    :param responses: list of str\n    :param knowledges: list of list of str\n    :return:\n    '''", "\n", "# stop_words = get_stop_words('en')", "\n", "# p_scores,  r_scores, f_scores = [], [], []", "\n", "# for hyp, know in zip(responses, knowledges):", "\n", "#     hyp_tokens = set([w for w in hyp.split() if w not in stop_words])", "\n", "#     know = ' '.join(know)", "\n", "#     know_tokens = set([w for w in know.split() if w not in stop_words])", "\n", "#", "\n", "#     if len(hyp_tokens & know_tokens) == 0:", "\n", "#         _p, _r, _f1 = .0, .0, .0", "\n", "#     else:", "\n", "#         _p = len(hyp_tokens & know_tokens) / len(hyp_tokens)", "\n", "#         _r = len(hyp_tokens & know_tokens) / len(know_tokens)", "\n", "#         _f1 = 2 * (_p * _r) / (_p + _r)", "\n", "#", "\n", "#     p_scores.append(_p)", "\n", "#     r_scores.append(_r)", "\n", "#     f_scores.append(_f1)", "\n", "#", "\n", "# return np.mean(r_scores), np.mean(p_scores),  np.mean(f_scores)", "\n", "\n", "stop_words", "=", "get_stop_words", "(", "'en'", ")", "\n", "p_scores", ",", "r_scores", ",", "f_scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "hyp", ",", "know", "in", "zip", "(", "responses", ",", "knowledges", ")", ":", "\n", "        ", "hyp_tokens", "=", "set", "(", "[", "w", "for", "w", "in", "hyp", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", ")", "\n", "know", "=", "' '", ".", "join", "(", "know", ")", "\n", "know_tokens", "=", "set", "(", "[", "w", "for", "w", "in", "know", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", ")", "\n", "\n", "if", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "==", "0", ":", "\n", "            ", "_p", ",", "_r", ",", "_f1", "=", ".0", ",", ".0", ",", ".0", "\n", "", "else", ":", "\n", "            ", "_p", "=", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "/", "len", "(", "hyp_tokens", ")", "\n", "_r", "=", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "/", "len", "(", "know_tokens", ")", "\n", "_f1", "=", "2", "*", "(", "_p", "*", "_r", ")", "/", "(", "_p", "+", "_r", ")", "\n", "\n", "", "p_scores", ".", "append", "(", "_p", ")", "\n", "r_scores", ".", "append", "(", "_r", ")", "\n", "f_scores", ".", "append", "(", "_f1", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "r_scores", ")", ",", "np", ".", "mean", "(", "p_scores", ")", ",", "np", ".", "mean", "(", "f_scores", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.url_to_filename": [[30, 46], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.filename_to_url": [[48, 72], ["isinstance", "os.path.join", "str", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.cached_path": [[74, 102], ["isinstance", "isinstance", "urllib.parse.urlparse", "str", "str", "file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.split_s3_path": [[104, 115], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_request": [[117, 134], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_etag": [[136, 143], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_get": [[145, 151], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.http_get": [[153, 163], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.get_from_cache": [[165, 222], ["isinstance", "os.makedirs", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.url_to_filename", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_etag", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.s3_get", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.read_set_from_file": [[224, 234], ["set", "open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.ZRKGC.loader_utils.TrieTree.add"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.get_file_extension": [[236, 240], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.__init__": [[93, 105], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[X_SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ",", "\"<#Q2K#>\"", ",", "\"<#K#>\"", ",", "\"<#Q#>\"", ",", "\"<#Q2R#>\"", ",", "\"[#2R#]\"", ",", "\"[#2K#],<#K2R#>\"", ")", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "\n", "do_lower_case", "=", "do_lower_case", ",", "never_split", "=", "never_split", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.tokenize": [[106, 112], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.convert_tokens_to_ids": [[113, 126], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "\n", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.convert_ids_to_tokens": [[127, 133], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BertTokenizer.from_pretrained": [[134, 171], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WhitespaceTokenizer.tokenize": [[174, 176], ["tokenization.whitespace_tokenize"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize"], ["    ", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "whitespace_tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer.__init__": [[181, 189], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ",", "\"<#Q2K#>\"", ",", "\"<#K#>\"", ",", "\"<#Q#>\"", ",", "\"<#Q2R#>\"", ",", "\"[#2R#]\"", ",", "\"[#2K#],<#K2R#>\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer.tokenize": [[190, 210], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._run_strip_accents": [[211, 221], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._run_split_on_punc": [[222, 243], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._tokenize_chinese_chars": [[244, 256], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._is_chinese_char": [[257, 278], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.BasicTokenizer._clean_text": [[279, 291], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_whitespace", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.__init__": [[296, 300], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize": [[301, 351], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.load_vocab": [[51, 79], ["range", "collections.OrderedDict", "open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "# mapping unused tokens to special tokens", "\n", "extra_map", "=", "{", "}", "\n", "extra_map", "[", "'[unused1]'", "]", "=", "'[X_SEP]'", "\n", "for", "i", "in", "range", "(", "30", ")", ":", "\n", "        ", "extra_map", "[", "'[unused{}]'", ".", "format", "(", "i", "+", "2", ")", "]", "=", "'[SEP_{}]'", ".", "format", "(", "i", "-", "20", ")", "\n", "", "extra_map", "[", "'[unused32]'", "]", "=", "'[S2S_SEP]'", "\n", "extra_map", "[", "'[unused33]'", "]", "=", "'[S2S_CLS]'", "\n", "extra_map", "[", "'[unused34]'", "]", "=", "'[L2R_SEP]'", "\n", "extra_map", "[", "'[unused35]'", "]", "=", "'[L2R_CLS]'", "\n", "extra_map", "[", "'[unused36]'", "]", "=", "'[R2L_SEP]'", "\n", "extra_map", "[", "'[unused37]'", "]", "=", "'[R2L_CLS]'", "\n", "extra_map", "[", "'[unused38]'", "]", "=", "'[S2S_SOS]'", "\n", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "if", "token", "in", "extra_map", ":", "\n", "                ", "token", "=", "extra_map", "[", "token", "]", "\n", "", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.whitespace_tokenize": [[81, 88], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_whitespace": [[353, 363], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_control": [[365, 375], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization._is_punctuation": [[377, 391], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_preprocess.truncate": [[16, 21], ["str.strip.strip", "len", "str.strip.split", "str.strip.split", "max"], "function", ["None"], ["def", "truncate", "(", "str", ",", "num", ")", ":", "\n", "\t", "str", "=", "str", ".", "strip", "(", ")", "\n", "length", "=", "len", "(", "str", ".", "split", "(", ")", ")", "\n", "list", "=", "str", ".", "split", "(", ")", "[", "max", "(", "0", ",", "length", "-", "num", ")", ":", "]", "\n", "return", "\" \"", ".", "join", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_preprocess.detokenize": [[22, 31], ["tk_str.strip().split", "tk_str.strip", "tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_str", ")", ":", "\n", "\t", "tk_list", "=", "tk_str", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "\t\t", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "\t\t\t", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "\t\t\t", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "r_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_preprocess.process_wizard": [[40, 90], ["wizard_generator.data_generator", "history.split", "HISTORY.append", "KNOWLEDGE.append", "label.split", "LABEL.append", "len", "len", "len", "open", "open", "range", "len", "len", "k.split", "[].strip", "knowledge_list.append", "len", "len", "out1.write", "out2.write", "np.random.choice", "len", "len", "HISTORY[].strip", "len", "HISTORY[].strip().split", "HISTORY[].strip.strip().split", "wizard_preprocess.truncate", "len", "HISTORY[].strip().split", "tokenizer.tokenize", "HISTORY[].strip", "HISTORY[].strip.strip", "tokenizer.tokenize", "LABEL[].strip", "HISTORY[].strip", "KNOWLEDGE[].strip", "tokenizer.tokenize"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.data_generator", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "process_wizard", "(", "data_file", ")", ":", "\n", "\n", "\t", "HISTORY", "=", "[", "]", "\n", "KNOWLEDGE", "=", "[", "]", "\n", "LABEL", "=", "[", "]", "\n", "\n", "for", "(", "history", ",", "label", ",", "knowledge_sentences", ")", "in", "data_generator", "(", "data_file", ")", ":", "\n", "# process context", "\n", "\t\t", "history_words", "=", "history", ".", "split", "(", ")", "\n", "if", "len", "(", "history_words", ")", ">", "text_truncate", ":", "\n", "\t\t\t", "history_words", "=", "history_words", "[", "-", "text_truncate", ":", "]", "\n", "", "HISTORY", ".", "append", "(", "\" \"", ".", "join", "(", "history_words", ")", ".", "strip", "(", ")", ")", "\n", "\n", "# process knowledge", "\n", "knowledge_list", "=", "[", "]", "\n", "if", "len", "(", "knowledge_sentences", ")", ">", "max_knowledge", ":", "\n", "\t\t\t", "keepers", "=", "1", "+", "np", ".", "random", ".", "choice", "(", "len", "(", "knowledge_sentences", ")", "-", "1", ",", "max_knowledge", ",", "False", ")", "\n", "keepers", "[", "0", "]", "=", "0", "\n", "knowledge_sentences", "=", "[", "knowledge_sentences", "[", "i", "]", "for", "i", "in", "keepers", "]", "\n", "\n", "", "for", "k", "in", "knowledge_sentences", ":", "\n", "\t\t\t", "fw_words", "=", "k", ".", "split", "(", ")", "\n", "if", "len", "(", "fw_words", ")", ">", "knowledge_truncate", ":", "\n", "\t\t\t\t", "fw_words", "=", "fw_words", "[", ":", "knowledge_truncate", "]", "\n", "", "sentence", "=", "\" \"", ".", "join", "(", "fw_words", ")", ".", "strip", "(", ")", ".", "split", "(", "\"__knowledge__\"", ")", "[", "1", "]", ".", "strip", "(", ")", "\n", "knowledge_list", ".", "append", "(", "sentence", ")", "\n", "", "KNOWLEDGE", ".", "append", "(", "\" <#K#> \"", ".", "join", "(", "knowledge_list", ")", ")", "\n", "\n", "# process response", "\n", "label_words", "=", "label", ".", "split", "(", ")", "\n", "if", "len", "(", "label_words", ")", ">", "label_truncate", ":", "\n", "\t\t\t", "label_words", "=", "label_words", "[", ":", "label_truncate", "]", "\n", "", "LABEL", ".", "append", "(", "\" \"", ".", "join", "(", "label_words", ")", ".", "strip", "(", ")", ")", "\n", "\n", "", "assert", "len", "(", "HISTORY", ")", "==", "len", "(", "KNOWLEDGE", ")", "==", "len", "(", "LABEL", ")", "\n", "\n", "with", "open", "(", "\"test_data/{}/test_{}.src.tk\"", ".", "format", "(", "data_type", ",", "data_type", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out1", ",", "open", "(", "\"test_data/{}/test_{}.tgt.tk\"", ".", "format", "(", "data_type", ",", "data_type", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out2", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "HISTORY", ")", ")", ":", "\n", "\n", "\t\t\t", "if", "len", "(", "HISTORY", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" <#Q#> \"", ")", ")", ">", "max_query_turn", ":", "\n", "\t\t\t\t", "history_line", "=", "\" <#Q#> \"", ".", "join", "(", "HISTORY", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" <#Q#> \"", ")", "[", "-", "max_query_turn", ":", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "history_line", "=", "HISTORY", "[", "i", "]", ".", "strip", "(", ")", "\n", "", "assert", "len", "(", "history_line", ".", "strip", "(", ")", ".", "split", "(", "\" <#Q#> \"", ")", ")", "<=", "max_query_turn", "\n", "\n", "src_line", "=", "truncate", "(", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "history_line", ")", ")", ".", "strip", "(", ")", ",", "text_truncate", ")", "+", "\" <#Q2K#> \"", "+", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "KNOWLEDGE", "[", "i", "]", ".", "strip", "(", ")", ")", ")", ".", "strip", "(", ")", "\n", "tgt_line", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "LABEL", "[", "i", "]", ".", "strip", "(", ")", ")", ")", ".", "strip", "(", ")", "\n", "\n", "out1", ".", "write", "(", "src_line", "+", "\"\\n\"", ")", "\n", "out2", ".", "write", "(", "tgt_line", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_preprocess.process_ks": [[93, 110], ["print", "open", "file.readlines", "open", "range", "len", "wizard_preprocess.truncate", "[].split", "range", "[].strip", "len", "truncate.strip", "know_list[].strip", "out.write", "out.write", "query.strip.strip", "src[].strip().split", "src[].strip().split", "src[].strip", "src[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate"], ["", "", "", "def", "process_ks", "(", "src_path", ",", "out_path", ")", ":", "\n", "\t", "with", "open", "(", "src_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "src", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "\t\t\t", "query", "=", "truncate", "(", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", ".", "strip", "(", ")", ",", "128", ")", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "know_list", ")", ")", ":", "\n", "\t\t\t\t", "line", "=", "query", ".", "strip", "(", ")", "\n", "line", "+=", "\" <#Q2K#> \"", "\n", "line", "+=", "know_list", "[", "t", "]", ".", "strip", "(", ")", "\n", "\n", "out", ".", "write", "(", "line", ".", "strip", "(", ")", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "", "print", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_preprocess.detokenize_file": [[111, 117], ["open", "file.readlines", "open", "range", "len", "out.write", "wizard_preprocess.detokenize", "data[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize"], ["", "def", "detokenize_file", "(", "file_path", ")", ":", "\n", "\t", "with", "open", "(", "file_path", "+", "\".tk\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "\t\t\t", "out", ".", "write", "(", "detokenize", "(", "data", "[", "i", "]", ".", "strip", "(", ")", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._first_val": [[11, 13], ["list", "dictionary.values"], "function", ["None"], ["def", "_first_val", "(", "dictionary", ")", ":", "\n", "    ", "return", "list", "(", "dictionary", ".", "values", "(", ")", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._first_key": [[15, 17], ["list", "dictionary.keys"], "function", ["None"], ["", "def", "_first_key", "(", "dictionary", ")", ":", "\n", "    ", "return", "list", "(", "dictionary", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._get_chosen_title_and_sent": [[19, 54], ["wizard_entry.get", "wizard_entry.get", "wizard_generator._first_val", "wizard_generator._first_val", "_first_key().split", "k_dict.items", "wizard_generator._first_key"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._first_val", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._first_val", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._first_key"], ["", "def", "_get_chosen_title_and_sent", "(", "wizard_entry", ",", "k_dict", ")", ":", "\n", "    ", "\"\"\"\n    Return a nicely extracted title and chosen sentence.\n    :return: pair (title, sentence)\n    \"\"\"", "\n", "title_dict", "=", "wizard_entry", ".", "get", "(", "'checked_passage'", ",", "'none'", ")", "\n", "sentence_dict", "=", "wizard_entry", ".", "get", "(", "'checked_sentence'", ",", "{", "}", ")", "\n", "title", "=", "None", "\n", "sentence", "=", "None", "\n", "if", "sentence_dict", "==", "{", "}", ":", "\n", "        ", "title", "=", "sentence", "=", "TOKEN_NOCHOSEN", "\n", "", "else", ":", "\n", "        ", "sentence", "=", "_first_val", "(", "sentence_dict", ")", "\n", "if", "sentence", "==", "TOKEN_NOCHOSEN", ":", "\n", "            ", "title", "=", "TOKEN_NOCHOSEN", "\n", "", "else", ":", "\n", "            ", "title", "=", "''", "\n", "# cand_title1 is the title from the `checked_passage`", "\n", "cand_title1", "=", "_first_val", "(", "title_dict", ")", "if", "title_dict", "else", "''", "\n", "# cand_title2 is the extracted title of the passage from the", "\n", "#   sentence dict, which is e.g. `self_Vermont_Syrup_0`", "\n", "cand_title2", "=", "' '", ".", "join", "(", "_first_key", "(", "sentence_dict", ")", ".", "split", "(", "'_'", ")", "[", "1", ":", "-", "1", "]", ")", "\n", "if", "(", "cand_title1", "\n", "and", "cand_title1", "in", "k_dict", "\n", "and", "sentence", "in", "k_dict", "[", "cand_title1", "]", ")", ":", "\n", "                ", "title", "=", "cand_title1", "\n", "", "elif", "cand_title2", "in", "k_dict", "and", "sentence", "in", "k_dict", "[", "cand_title2", "]", ":", "\n", "                ", "title", "=", "cand_title2", "\n", "", "else", ":", "# neither candidate title is the right one", "\n", "                ", "for", "t", ",", "passage", "in", "k_dict", ".", "items", "(", ")", ":", "\n", "                    ", "if", "sentence", "in", "passage", ":", "\n", "                        ", "title", "=", "t", "\n", "break", "\n", "\n", "", "", "", "", "", "return", "title", ",", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._check_truncate": [[56, 65], ["len"], "function", ["None"], ["", "def", "_check_truncate", "(", "vec", ",", "truncate", ",", "truncate_left", "=", "False", ")", ":", "\n", "    ", "if", "truncate", "is", "None", ":", "\n", "        ", "return", "vec", "\n", "", "if", "len", "(", "vec", ")", "<=", "truncate", ":", "\n", "        ", "return", "vec", "\n", "", "if", "truncate_left", ":", "\n", "        ", "return", "vec", "[", "-", "truncate", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "vec", "[", ":", "truncate", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._parse_knowledge": [[67, 94], ["list", "k.strip", "obs_know.index", "obs.get().split", "obs.get"], "function", ["None"], ["", "", "def", "_parse_knowledge", "(", "obs", ")", ":", "\n", "    ", "if", "'knowledge_parsed'", "in", "obs", ":", "\n", "# make a copy of the list to prevent the future padding step from", "\n", "# being destructive", "\n", "        ", "return", "list", "(", "obs", "[", "'knowledge_parsed'", "]", ")", "\n", "\n", "", "checked_sentence", "=", "'{} {} {}'", ".", "format", "(", "\n", "obs", "[", "'title'", "]", ",", "TOKEN_KNOWLEDGE", ",", "obs", "[", "'checked_sentence'", "]", "\n", ")", "\n", "# grab all the nonempty knowledge", "\n", "obs_know", "=", "[", "k", ".", "strip", "(", ")", "for", "k", "in", "obs", ".", "get", "(", "'knowledge'", ",", "''", ")", ".", "split", "(", "'\\n'", ")", "]", "\n", "obs_know", "=", "[", "k", "for", "k", "in", "obs_know", "if", "k", "]", "\n", "\n", "# we want the correct knowledge to always be in index 0", "\n", "try", ":", "\n", "        ", "i", "=", "obs_know", ".", "index", "(", "checked_sentence", ")", "\n", "", "except", "ValueError", ":", "\n", "# uh oh, couldn't find the sentence in the knowledge. This happens for", "\n", "# one or two examples in the training set. We can just artificially", "\n", "# put it back in", "\n", "        ", "i", "=", "0", "\n", "obs_know", "[", "0", "]", "=", "checked_sentence", "\n", "", "obs_know", "[", "0", "]", ",", "obs_know", "[", "i", "]", "=", "obs_know", "[", "i", "]", ",", "obs_know", "[", "0", "]", "\n", "\n", "obs", "[", "'knowledge_parsed'", "]", "=", "obs_know", "\n", "obs", "[", "'checked_sentence_parsed'", "]", "=", "checked_sentence", "\n", "return", "obs", "[", "'knowledge_parsed'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.len_episode": [[96, 101], ["len", "len"], "function", ["None"], ["", "def", "len_episode", "(", "d", ")", ":", "\n", "    ", "wizard_first", "=", "'Wizard'", "in", "d", "[", "'dialog'", "]", "[", "0", "]", "[", "'speaker'", "]", "\n", "if", "wizard_first", ":", "\n", "        ", "return", "(", "len", "(", "d", "[", "'dialog'", "]", ")", "-", "1", ")", "//", "2", "\n", "", "return", "len", "(", "d", "[", "'dialog'", "]", ")", "//", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.load_data": [[103, 184], ["print", "enumerate", "print", "open", "json.load", "range", "wizard_generator.len_episode", "d.get", "knowledge_dict.items", "wizard_generator._get_chosen_title_and_sent", "examples.append", "len", "knowledge_str.startswith", "wizard_generator.len_episode", "passage.items", "knowledge_dict.keys"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.len_episode", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._get_chosen_title_and_sent", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.len_episode"], ["", "def", "load_data", "(", "data_path", ")", ":", "\n", "# 1. load from source file", "\n", "    ", "print", "(", "'loading: {}'", ".", "format", "(", "data_path", ")", ")", "\n", "with", "open", "(", "data_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# 2. split into multiple turns", "\n", "", "examples", "=", "[", "]", "\n", "for", "episode_idx", ",", "d", "in", "enumerate", "(", "data", ")", ":", "\n", "        ", "for", "entry_idx", "in", "range", "(", "len_episode", "(", "d", ")", ")", ":", "\n", "            ", "episode_done", "=", "entry_idx", "==", "(", "len_episode", "(", "d", ")", "-", "1", ")", "\n", "\n", "wizard_first", "=", "'Wizard'", "in", "d", "[", "'dialog'", "]", "[", "0", "]", "[", "'speaker'", "]", "\n", "idx", "=", "entry_idx", "*", "2", "if", "wizard_first", "else", "(", "entry_idx", "*", "2", ")", "+", "1", "\n", "\n", "# 2.1 get knowledge", "\n", "apprentice_ret_passages", "=", "wizard_ret_passages", "=", "{", "}", "\n", "\n", "if", "not", "wizard_first", "or", "idx", "!=", "0", ":", "\n", "                ", "apprentice_entry", "=", "d", "[", "'dialog'", "]", "[", "idx", "-", "1", "]", "\n", "apprentice_ret_passages", "=", "apprentice_entry", "[", "'retrieved_passages'", "]", "\n", "", "if", "idx", "-", "2", ">=", "0", ":", "\n", "                ", "wizard_prev_entry", "=", "d", "[", "'dialog'", "]", "[", "idx", "-", "2", "]", "\n", "wizard_ret_passages", "=", "wizard_prev_entry", "[", "'retrieved_passages'", "]", "\n", "\n", "", "chosen_topic_passages", "=", "d", "[", "'chosen_topic_passage'", "]", "\n", "chosen_topic", "=", "d", ".", "get", "(", "'chosen_topic'", ",", "''", ")", "\n", "\n", "#wizard_ret_passages = {}", "\n", "\n", "knowledge_dict", "=", "{", "chosen_topic", ":", "chosen_topic_passages", "}", "\n", "#knowledge_dict = {}", "\n", "for", "ret_passes", "in", "[", "apprentice_ret_passages", ",", "wizard_ret_passages", "]", ":", "\n", "                ", "for", "passage", "in", "ret_passes", ":", "\n", "                    ", "for", "k", ",", "v", "in", "passage", ".", "items", "(", ")", ":", "\n", "                        ", "if", "k", "not", "in", "knowledge_dict", ".", "keys", "(", ")", ":", "\n", "                            ", "knowledge_dict", "[", "k", "]", "=", "v", "\n", "\n", "# 2.2 get text", "\n", "", "", "", "", "if", "idx", "==", "0", ":", "\n", "                ", "text", "=", "chosen_topic", "\n", "", "elif", "idx", "==", "1", ":", "\n", "                ", "text", "=", "'{}\\n{}'", ".", "format", "(", "chosen_topic", ",", "apprentice_entry", "[", "'text'", "]", ")", "\n", "", "else", ":", "\n", "                ", "text", "=", "apprentice_entry", "[", "'text'", "]", "\n", "\n", "# 2.3 get label", "\n", "", "wizard_entry", "=", "d", "[", "'dialog'", "]", "[", "idx", "]", "\n", "labels", "=", "[", "wizard_entry", "[", "'text'", "]", "]", "\n", "\n", "# 2.4 get label_candidates", "\n", "knowledge_str", "=", "''", "\n", "for", "title", ",", "passage", "in", "knowledge_dict", ".", "items", "(", ")", ":", "\n", "                ", "for", "p", "in", "passage", ":", "\n", "                    ", "cand", "=", "'{} {} {}'", ".", "format", "(", "title", ",", "TOKEN_KNOWLEDGE", ",", "p", ")", "\n", "knowledge_str", "+=", "cand", "+", "'\\n'", "\n", "", "", "if", "not", "knowledge_str", ".", "startswith", "(", "TOKEN_NOCHOSEN", ")", ":", "\n", "                ", "knowledge_str", "=", "(", "\n", "TOKEN_NOCHOSEN", "\n", "+", "' '", "\n", "+", "TOKEN_KNOWLEDGE", "\n", "+", "' '", "\n", "+", "TOKEN_NOCHOSEN", "\n", "+", "'\\n'", "\n", "+", "knowledge_str", "\n", ")", "\n", "\n", "# 2.5 get title and checked_sentences", "\n", "", "title", ",", "sentence", "=", "_get_chosen_title_and_sent", "(", "wizard_entry", ",", "knowledge_dict", ")", "\n", "\n", "examples", ".", "append", "(", "{", "\n", "'text'", ":", "text", ",", "\n", "'labels'", ":", "labels", ",", "\n", "'chosen_topic'", ":", "chosen_topic", ",", "\n", "'episode_done'", ":", "episode_done", ",", "\n", "'knowledge'", ":", "knowledge_str", ",", "\n", "'title'", ":", "title", ",", "\n", "'checked_sentence'", ":", "sentence", ",", "\n", "}", ")", "\n", "", "", "print", "(", "'loaded {} episodes with a total of {} examples'", ".", "format", "(", "episode_idx", "+", "1", ",", "len", "(", "examples", ")", ")", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.fix_missing_period": [[189, 194], ["None"], "function", ["None"], ["def", "fix_missing_period", "(", "line", ")", ":", "\n", "    ", "\"\"\"Adds a period to a line that is missing a period\"\"\"", "\n", "if", "line", "==", "\"\"", ":", "return", "line", "\n", "if", "line", "[", "-", "1", "]", "in", "END_TOKENS", ":", "return", "line", "\n", "return", "line", "+", "\" .\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.data_generator": [[196, 251], ["wizard_generator.load_data", "enumerate", "ex.copy", "[].lower", "wizard_generator._parse_knowledge", "print", "[].lower", "wizard_generator.fix_missing_period", "history_strings.append", "observation[].lower", "wizard_generator.fix_missing_period", "history_strings.append", "nltk.tokenize.word_tokenize", "k.lower", "nltk.tokenize.word_tokenize", "nltk.tokenize.word_tokenize", "nltk.tokenize.word_tokenize", "len", "float", "float", "len"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.load_data", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator._parse_knowledge", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.fix_missing_period", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.fix_missing_period"], ["", "def", "data_generator", "(", "in_file", ")", ":", "\n", "\n", "    ", "examples", "=", "load_data", "(", "in_file", ")", "\n", "observation", "=", "None", "\n", "\n", "history_strings", "=", "[", "]", "\n", "\n", "reset_on_next_update", "=", "False", "\n", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "i", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"Processing {} of {}; {:0.2f} percent done\"", ".", "format", "(", "\n", "i", ",", "len", "(", "examples", ")", ",", "float", "(", "i", ")", "*", "100.0", "/", "float", "(", "len", "(", "examples", ")", ")", ")", ")", "\n", "\n", "", "if", "not", "observation", "or", "observation", "[", "'episode_done'", "]", ":", "\n", "            ", "last_reply", "=", "None", "\n", "", "else", ":", "\n", "            ", "last_reply", "=", "observation", "[", "'labels'", "]", "[", "0", "]", ".", "lower", "(", ")", "\n", "# TODO: use BPE", "\n", "last_reply", "=", "' '", ".", "join", "(", "word_tokenize", "(", "last_reply", ")", ")", "\n", "# last_reply = ' '.join(last_reply.split())", "\n", "last_reply", "=", "fix_missing_period", "(", "last_reply", ")", "\n", "", "observation", "=", "ex", ".", "copy", "(", ")", "\n", "\n", "# 1. update the history using the observation", "\n", "if", "reset_on_next_update", ":", "\n", "            ", "history_strings", "=", "[", "]", "\n", "reset_on_next_update", "=", "False", "\n", "\n", "", "if", "last_reply", "is", "not", "None", ":", "\n", "            ", "history_strings", ".", "append", "(", "last_reply", ")", "\n", "\n", "", "if", "'text'", "in", "observation", "and", "observation", "[", "'text'", "]", "is", "not", "None", ":", "\n", "            ", "next_text", "=", "observation", "[", "'text'", "]", ".", "lower", "(", ")", "\n", "next_text", "=", "' '", ".", "join", "(", "word_tokenize", "(", "next_text", ")", ")", "\n", "# next_text = ' '.join(next_text.split())", "\n", "next_text", "=", "fix_missing_period", "(", "next_text", ")", "\n", "history_strings", ".", "append", "(", "next_text", ")", "\n", "\n", "", "if", "observation", "[", "'episode_done'", "]", ":", "\n", "            ", "reset_on_next_update", "=", "True", "\n", "\n", "# 2. parse history, label and knowledge", "\n", "", "history", "=", "' <#Q#> '", ".", "join", "(", "history_strings", ")", "\n", "\n", "label", "=", "observation", "[", "'labels'", "]", "[", "0", "]", ".", "lower", "(", ")", "\n", "label", "=", "' '", ".", "join", "(", "word_tokenize", "(", "label", ")", ")", "\n", "# label = ' '.join(label.split())", "\n", "\n", "knowledge", "=", "_parse_knowledge", "(", "observation", ")", "\n", "knowledge", "=", "[", "k", ".", "lower", "(", ")", "for", "k", "in", "knowledge", "]", "\n", "knowledge", "=", "[", "' '", ".", "join", "(", "word_tokenize", "(", "k", ")", ")", "for", "k", "in", "knowledge", "]", "\n", "# knowledge = [' '.join(k.split()) for k in knowledge]", "\n", "\n", "yield", "(", "history", ",", "label", ",", "knowledge", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.wizard_generator.topical_generator": [[254, 286], ["range", "open", "file.readlines", "open", "file.readlines", "len", "[].strip().encode().decode", "[].strip().split", "print", "knowledge.append", "print", "print", "print", "print", "print", "[].strip().encode", "nltk.tokenize.word_tokenize", "[].strip().encode().decode.split", "[].strip", "len", "item.strip", "nltk.tokenize.word_tokenize", "float", "[].strip", "tgt[].strip().encode().decode", "float", "len", "src[].strip().split", "tgt[].strip().encode", "nltk.tokenize.word_tokenize", "src[].strip().split", "src[].strip", "know.strip().encode().decode", "tgt[].strip", "src[].strip", "know.strip().encode", "know.strip"], "function", ["None"], ["def", "topical_generator", "(", "src_file", ",", "tgt_file", ")", ":", "\n", "    ", "with", "open", "(", "src_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "        ", "src", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "tgt_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "        ", "tgt", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "tgt", ")", ")", ":", "\n", "        ", "if", "i", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"Processing {} of {}; {:0.2f} percent done\"", ".", "format", "(", "\n", "i", ",", "len", "(", "tgt", ")", ",", "float", "(", "i", ")", "*", "100.0", "/", "float", "(", "len", "(", "tgt", ")", ")", ")", ")", "\n", "\n", "", "history", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" <#Q2K#> \"", ")", "[", "0", "]", ".", "strip", "(", ")", ".", "encode", "(", "'unicode_escape'", ")", ".", "decode", "(", "'utf-8'", ")", "\n", "history_list", "=", "[", "\" \"", ".", "join", "(", "word_tokenize", "(", "item", ".", "strip", "(", ")", ")", ")", "for", "item", "in", "history", ".", "split", "(", "\"<#Q#>\"", ")", "]", "\n", "\n", "history", "=", "\" \"", ".", "join", "(", "history_list", ")", ".", "lower", "(", ")", "\n", "\n", "label", "=", "\" \"", ".", "join", "(", "word_tokenize", "(", "tgt", "[", "i", "]", ".", "strip", "(", ")", ".", "encode", "(", "'unicode_escape'", ")", ".", "decode", "(", "'utf-8'", ")", ")", ")", ".", "lower", "(", ")", "\n", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\" <#Q2K#> \"", ")", "[", "1", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#K#>\"", ")", "\n", "knowledge", "=", "[", "]", "\n", "for", "know", "in", "know_list", ":", "\n", "            ", "knowledge", ".", "append", "(", "\"__knowledge__ \"", "+", "\" \"", ".", "join", "(", "word_tokenize", "(", "know", ".", "strip", "(", ")", ".", "encode", "(", "'unicode_escape'", ")", ".", "decode", "(", "'utf-8'", ")", ")", ")", ".", "lower", "(", ")", ")", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "            ", "print", "(", "\"history:\"", ",", "history", ")", "\n", "print", "(", "'\\n'", ")", "\n", "print", "(", "\"label:\"", ",", "label", ")", "\n", "print", "(", "'\\n'", ")", "\n", "print", "(", "knowledge", ")", "\n", "\n", "\n", "", "yield", "(", "history", ",", "label", ",", "knowledge", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.move_stop_words": [[15, 18], ["str.split", "w.lower"], "function", ["None"], ["def", "move_stop_words", "(", "str", ")", ":", "\n", "\t", "item", "=", "\" \"", ".", "join", "(", "[", "w", "for", "w", "in", "str", ".", "split", "(", ")", "if", "not", "w", ".", "lower", "(", ")", "in", "stop_words", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate": [[20, 25], ["str.strip.strip", "len", "str.strip.split", "str.strip.split", "max"], "function", ["None"], ["", "def", "truncate", "(", "str", ",", "num", ")", ":", "\n", "\t", "str", "=", "str", ".", "strip", "(", ")", "\n", "length", "=", "len", "(", "str", ".", "split", "(", ")", ")", "\n", "list", "=", "str", ".", "split", "(", ")", "[", "max", "(", "0", ",", "length", "-", "num", ")", ":", "]", "\n", "return", "\" \"", ".", "join", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize": [[26, 35], ["tk_str.strip().split", "tk_str.strip", "tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_str", ")", ":", "\n", "\t", "tk_list", "=", "tk_str", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "\t\t", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "\t\t\t", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "\t\t\t", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "r_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.main": [[51, 196], ["conversations_file.items", "range", "all_query.append", "len", "len", "len", "range", "len", "open", "open", "range", "sentence.encode().decode.encode().decode", "list", "all_know.append", "all_other_know.append", "all_query[].strip().split", "len", "all_know[].strip().split", "all_know[].strip().split.index", "query_list[].strip", "src.append", "tgt.append", "len", "[].split", "[].split", "len", "src_out.write", "src_out.write", "tgt_out.write", "tgt_out.write", "k.encode().decode.encode().decode", "k.encode().decode.encode().decode", "history.strip", "metrics.bleu_metric", "all_other_know[].strip().split", "know_str.strip", "tokenizer.tokenize", "tokenizer.tokenize", "print", "sentence.encode().decode.encode", "set", "set", "all_query[].strip", "all_know[].strip", "history.strip", "nltk.tokenize.word_tokenize", "nltk.tokenize.word_tokenize", "other_knowledge_list.extend", "other_knowledge_list.extend", "other_knowledge_list.append", "other_knowledge_list.extend", "k.encode().decode.encode", "k.encode().decode.encode", "metrics.normalize_answer", "metrics.normalize_answer", "all_other_know[].strip", "src[].strip().split", "item.strip", "src[].strip().split", "item.strip", "nltk.tokenize.word_tokenize", "len", "knowledge_list.extend", "knowledge_list.extend", "knowledge_list.append", "knowledge_list.extend", "nltk.sent_tokenize", "nltk.sent_tokenize", "nltk.sent_tokenize", "topical_preprocess.move_stop_words", "topical_preprocess.move_stop_words", "tgt[].strip", "nltk.sent_tokenize", "nltk.sent_tokenize", "nltk.sent_tokenize", "query_list[].strip", "src[].strip", "src[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.bleu_metric", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.metrics.normalize_answer", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.move_stop_words", "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.move_stop_words"], ["", "def", "main", "(", ")", ":", "\n", "\t", "all_query", "=", "[", "]", "\n", "all_know", "=", "[", "]", "\n", "all_other_know", "=", "[", "]", "\n", "\n", "count", "=", "0", "\n", "no_know", "=", "0", "\n", "for", "dialog_id", ",", "dialog_turn", "in", "conversations_file", ".", "items", "(", ")", ":", "\n", "\t\t", "count", "+=", "1", "\n", "dialog_content", "=", "dialog_turn", "[", "\"content\"", "]", "\n", "knowledge_turn", "=", "know_file", "[", "dialog_id", "]", "\n", "\n", "query_line", "=", "\"\"", "\n", "for", "every_content", "in", "dialog_content", ":", "\n", "\t\t\t", "sentence", "=", "every_content", "[", "\"message\"", "]", "\n", "sentence", "=", "sentence", ".", "encode", "(", "'unicode_escape'", ")", ".", "decode", "(", "'utf-8'", ")", "\n", "\n", "know_ids", "=", "every_content", "[", "\"knowledge_source\"", "]", "\n", "\n", "single_knowledges", "=", "knowledge_turn", "[", "every_content", "[", "\"agent\"", "]", "]", "\n", "article_knowledge", "=", "knowledge_turn", "[", "\"article\"", "]", "\n", "other_know_ids", "=", "[", "\"FS1\"", ",", "\"FS2\"", ",", "\"FS3\"", ",", "\"AS1\"", ",", "\"AS2\"", ",", "\"AS3\"", "]", "\n", "other_know_ids", "=", "list", "(", "set", "(", "other_know_ids", ")", "-", "set", "(", "know_ids", ")", ")", "\n", "knowledge_list", "=", "[", "]", "\n", "other_knowledge_list", "=", "[", "]", "\n", "for", "know_id", "in", "know_ids", ":", "\n", "\t\t\t\t", "if", "know_id", "==", "\"Personal Knowledge\"", ":", "\n", "\t\t\t\t\t", "pass", "\n", "", "elif", "know_id", "in", "single_knowledges", ":", "\n", "\t\t\t\t\t", "assert", "\"shortened_wiki_lead_section\"", "or", "\"summarized_wiki_lead_section\"", "in", "single_knowledges", "[", "\n", "know_id", "]", "\n", "if", "\"shortened_wiki_lead_section\"", "in", "single_knowledges", "[", "know_id", "]", ":", "\n", "\t\t\t\t\t\t", "knowledge_list", ".", "extend", "(", "\n", "nltk", ".", "sent_tokenize", "(", "single_knowledges", "[", "know_id", "]", "[", "\"shortened_wiki_lead_section\"", "]", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "knowledge_list", ".", "extend", "(", "\n", "nltk", ".", "sent_tokenize", "(", "single_knowledges", "[", "know_id", "]", "[", "\"summarized_wiki_lead_section\"", "]", ")", ")", "\n", "", "for", "item", "in", "single_knowledges", "[", "know_id", "]", "[", "\"fun_facts\"", "]", ":", "\n", "\t\t\t\t\t\t", "knowledge_list", ".", "append", "(", "item", ")", "\n", "", "", "elif", "know_id", "in", "article_knowledge", ":", "\n", "\t\t\t\t\t", "knowledge_list", ".", "extend", "(", "nltk", ".", "sent_tokenize", "(", "article_knowledge", "[", "know_id", "]", ")", ")", "\n", "\n", "", "", "for", "other_know_id", "in", "other_know_ids", ":", "\n", "\t\t\t\t", "if", "other_know_id", "in", "single_knowledges", ":", "\n", "\t\t\t\t\t", "assert", "\"shortened_wiki_lead_section\"", "or", "\"summarized_wiki_lead_section\"", "in", "single_knowledges", "[", "know_id", "]", "\n", "if", "\"shortened_wiki_lead_section\"", "in", "single_knowledges", "[", "other_know_id", "]", ":", "\n", "\t\t\t\t\t\t", "other_knowledge_list", ".", "extend", "(", "\n", "nltk", ".", "sent_tokenize", "(", "single_knowledges", "[", "other_know_id", "]", "[", "\"shortened_wiki_lead_section\"", "]", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "other_knowledge_list", ".", "extend", "(", "\n", "nltk", ".", "sent_tokenize", "(", "single_knowledges", "[", "other_know_id", "]", "[", "\"summarized_wiki_lead_section\"", "]", ")", ")", "\n", "", "for", "item", "in", "single_knowledges", "[", "other_know_id", "]", "[", "\"fun_facts\"", "]", ":", "\n", "\t\t\t\t\t\t", "other_knowledge_list", ".", "append", "(", "item", ")", "\n", "\n", "", "", "elif", "(", "dialog_turn", "[", "\"config\"", "]", "!=", "\"C\"", ")", "and", "other_know_id", "in", "article_knowledge", ":", "\n", "\t\t\t\t\t", "other_knowledge_list", ".", "extend", "(", "nltk", ".", "sent_tokenize", "(", "article_knowledge", "[", "other_know_id", "]", ")", ")", "\n", "\n", "", "", "if", "knowledge_list", "==", "[", "]", ":", "\n", "\t\t\t\t", "no_know", "+=", "1", "\n", "knowledge_list", "=", "[", "\"__no_knowledge__\"", "]", "\n", "\n", "", "know_line", "=", "\"\"", "\n", "for", "k", "in", "knowledge_list", ":", "\n", "\t\t\t\t", "k", "=", "k", ".", "encode", "(", "'unicode_escape'", ")", ".", "decode", "(", "'utf-8'", ")", "\n", "know_line", "+=", "k", "\n", "know_line", "+=", "\"\\t\"", "\n", "", "all_know", ".", "append", "(", "know_line", ")", "\n", "\n", "other_know_line", "=", "\"\"", "\n", "for", "k", "in", "other_knowledge_list", ":", "\n", "\t\t\t\t", "k", "=", "k", ".", "encode", "(", "'unicode_escape'", ")", ".", "decode", "(", "'utf-8'", ")", "\n", "other_know_line", "+=", "k", "\n", "other_know_line", "+=", "\"\\t\"", "\n", "", "all_other_know", ".", "append", "(", "other_know_line", ")", "\n", "\n", "query_line", "+=", "sentence", "\n", "query_line", "+=", "\" <#Q#> \"", "\n", "", "all_query", ".", "append", "(", "query_line", ")", "\n", "\n", "", "assert", "len", "(", "all_other_know", ")", "==", "len", "(", "all_know", ")", "\n", "\n", "num", "=", "0", "\n", "src", "=", "[", "]", "\n", "tgt", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "all_query", ")", ")", ":", "\n", "\n", "\t\t", "query_list", "=", "all_query", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q#>\"", ")", "[", ":", "-", "1", "]", "\n", "for", "t", "in", "range", "(", "len", "(", "query_list", ")", ")", ":", "\n", "\t\t\t", "history", "=", "\" <#Q#> \"", ".", "join", "(", "query_list", "[", ":", "t", "]", ")", "\n", "if", "history", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "\t\t\t\t", "history", "=", "\"__no_history__\"", "\n", "\n", "", "knows", "=", "all_know", "[", "num", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "\n", "max_b2", "=", "0", "\n", "for", "one_know", "in", "knows", ":", "\n", "\t\t\t\t", "b1", ",", "b2", ",", "b3", "=", "bleu_metric", "(", "[", "normalize_answer", "(", "move_stop_words", "(", "query_list", "[", "t", "]", ".", "strip", "(", ")", ")", ")", "]", ",", "[", "normalize_answer", "(", "move_stop_words", "(", "one_know", ")", ")", "]", ")", "\n", "if", "b2", ">=", "max_b2", ":", "\n", "\t\t\t\t\t", "max_b2", "=", "b2", "\n", "check", "=", "one_know", "\n", "", "", "assert", "check", "in", "knows", "\n", "\n", "loc", "=", "knows", ".", "index", "(", "check", ")", "\n", "knows", "[", "loc", "]", ",", "knows", "[", "0", "]", "=", "knows", "[", "0", "]", ",", "knows", "[", "loc", "]", "\n", "\n", "other_knows", "=", "all_other_know", "[", "num", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "[", ":", "-", "1", "]", "\n", "\n", "know_str", "=", "\" <#K#> \"", ".", "join", "(", "knows", ")", "\n", "src_line", "=", "history", ".", "strip", "(", ")", "+", "\" <#Q2K#> \"", "+", "know_str", ".", "strip", "(", ")", "\n", "tgt_line", "=", "query_list", "[", "t", "]", ".", "strip", "(", ")", "\n", "\n", "src", ".", "append", "(", "src_line", ")", "\n", "tgt", ".", "append", "(", "tgt_line", ")", "\n", "num", "+=", "1", "\n", "\n", "\n", "", "", "assert", "num", "==", "len", "(", "all_know", ")", "\n", "\n", "\n", "with", "open", "(", "\"test_data/{}/test_{}.src.tk\"", ".", "format", "(", "part", ",", "part", ")", ",", "\"w\"", ")", "as", "src_out", ",", "open", "(", "\"test_data/{}/test_{}.tgt.tk\"", ".", "format", "(", "part", ",", "part", ")", ",", "\"w\"", ")", "as", "tgt_out", ":", "\n", "\n", "\t\t", "mean_know", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "\n", "\t\t\t", "query_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", ".", "split", "(", "\"<#Q#>\"", ")", "\n", "query_list", "=", "[", "\" \"", ".", "join", "(", "word_tokenize", "(", "item", ".", "strip", "(", ")", ")", ")", "for", "item", "in", "query_list", "]", "\n", "query_line", "=", "\" <#Q#> \"", ".", "join", "(", "query_list", ")", ".", "strip", "(", ")", "\n", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "know_list", "=", "[", "\" \"", ".", "join", "(", "word_tokenize", "(", "item", ".", "strip", "(", ")", ")", ")", "for", "item", "in", "know_list", "]", "\n", "mean_know", "+=", "len", "(", "know_list", ")", "\n", "know_line", "=", "\" <#K#> \"", ".", "join", "(", "know_list", ")", ".", "strip", "(", ")", "\n", "\n", "pro_src_line", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "query_line", "+", "\" <#Q2K#> \"", "+", "know_line", ")", ")", "\n", "pro_tgt_line", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "\" \"", ".", "join", "(", "word_tokenize", "(", "tgt", "[", "i", "]", ".", "strip", "(", ")", ")", ")", ")", ")", "\n", "\n", "src_out", ".", "write", "(", "pro_src_line", ")", "\n", "src_out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "tgt_out", ".", "write", "(", "pro_tgt_line", ")", "\n", "tgt_out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "\t\t\t\t", "print", "(", "\"have process {} data / {}\"", ".", "format", "(", "i", ",", "len", "(", "src", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.process_ks": [[198, 215], ["print", "open", "file.readlines", "open", "range", "len", "topical_preprocess.truncate", "[].split", "range", "[].strip", "len", "truncate.strip", "know_list[].strip", "out.write", "out.write", "query.strip.strip", "src[].strip().split", "src[].strip().split", "src[].strip", "src[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.truncate"], ["", "", "", "", "def", "process_ks", "(", "src_path", ",", "out_path", ")", ":", "\n", "\t", "with", "open", "(", "src_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "src", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "src", ")", ")", ":", "\n", "\t\t\t", "query", "=", "truncate", "(", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "0", "]", ".", "strip", "(", ")", ",", "128", ")", "\n", "know_list", "=", "src", "[", "i", "]", ".", "strip", "(", ")", ".", "split", "(", "\"<#Q2K#>\"", ")", "[", "1", "]", ".", "split", "(", "\"<#K#>\"", ")", "\n", "\n", "for", "t", "in", "range", "(", "len", "(", "know_list", ")", ")", ":", "\n", "\t\t\t\t", "line", "=", "query", ".", "strip", "(", ")", "\n", "line", "+=", "\" <#Q2K#> \"", "\n", "line", "+=", "know_list", "[", "t", "]", ".", "strip", "(", ")", "\n", "\n", "out", ".", "write", "(", "line", ".", "strip", "(", ")", ")", "\n", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "", "print", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize_file": [[216, 222], ["open", "file.readlines", "open", "range", "len", "out.write", "topical_preprocess.detokenize", "data[].strip"], "function", ["home.repos.pwc.inspect_result.nlpxucan_ZRKGC.preprocess.topical_preprocess.detokenize"], ["", "def", "detokenize_file", "(", "file_path", ")", ":", "\n", "\t", "with", "open", "(", "file_path", "+", "\".tk\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\t\t", "data", "=", "file", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "\t\t", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "\t\t\t", "out", ".", "write", "(", "detokenize", "(", "data", "[", "i", "]", ".", "strip", "(", ")", ")", "+", "\"\\n\"", ")", "\n", "\n"]]}