{"home.repos.pwc.inspect_result.xinyadu_eeqa.scripts.analysis_arg.print_order_dict": [[11, 17], ["open", "sorted", "os.path.join", "order_dict.keys", "open.write"], "function", ["None"], ["def", "print_order_dict", "(", "order_dict", ")", ":", "\n", "    ", "writer", "=", "open", "(", "path", ".", "join", "(", "output_dir", ",", "\"description_queries.txt\"", ")", ",", "\"w\"", ")", "\n", "for", "key", "in", "sorted", "(", "order_dict", ".", "keys", "(", ")", ")", ":", "\n", "# print(key, order_dict[key])", "\n", "        ", "for", "arg", "in", "order_dict", "[", "key", "]", ":", "\n", "            ", "writer", ".", "write", "(", "\"_\"", ".", "join", "(", "[", "key", ",", "arg", "]", ")", "+", "\", \\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.TokSpan.align": [[41, 46], ["parse_ace_event.get_token_indices", "parse_ace_event.get_token_indices", "parse_ace_event.get_token_indices", "str", "sent.as_doc", "sent.as_doc"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_indices", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_indices", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_indices"], ["def", "align", "(", "self", ",", "sent", ")", ":", "\n", "        ", "self", ".", "span_doc", "=", "get_token_indices", "(", "self", ",", "sent", ")", "\n", "self", ".", "span_sentence", "=", "get_token_indices", "(", "self", ",", "sent", ".", "as_doc", "(", ")", ")", "\n", "self", ".", "adjusted_span_sentence", "=", "get_token_indices", "(", "self", ",", "sent", ".", "as_doc", "(", ")", ")", "\n", "self", ".", "adjusted_text_string", "=", "str", "(", "self", ".", "text_string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.TokSpan.adjust": [[47, 55], ["parse_ace_event.in_between", "parse_ace_event.TokSpan.adjusted_text_string.replace", "tuple"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.in_between"], ["", "def", "adjust", "(", "self", ",", "tok", ")", ":", "\n", "        ", "if", "in_between", "(", "tok", ".", "i", ",", "self", ".", "span_sentence", ")", ":", "\n", "            ", "assert", "tok", ".", "text", "==", "\"\\n\"", "or", "tok", ".", "text", "==", "\" \"", "# Either a newline or an occasional whitespace.", "\n", "self", ".", "adjusted_text_string", "=", "self", ".", "adjusted_text_string", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "self", ".", "adjusted_span_sentence", "=", "(", "self", ".", "adjusted_span_sentence", "[", "0", "]", ",", "\n", "self", ".", "adjusted_span_sentence", "[", "1", "]", "-", "1", ")", "\n", "", "elif", "tok", ".", "i", "<", "self", ".", "span_sentence", "[", "0", "]", ":", "\n", "            ", "self", ".", "adjusted_span_sentence", "=", "tuple", "(", "[", "x", "-", "1", "for", "x", "in", "self", ".", "adjusted_span_sentence", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.TokSpan.adjust_spans_doc": [[56, 58], ["tuple"], "methods", ["None"], ["", "", "def", "adjust_spans_doc", "(", "self", ",", "entry_start", ")", ":", "\n", "        ", "self", ".", "adjusted_span_doc", "=", "tuple", "(", "[", "x", "+", "entry_start", "for", "x", "in", "self", ".", "adjusted_span_sentence", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entity.to_json": [[66, 68], ["None"], "methods", ["None"], ["def", "to_json", "(", "self", ")", ":", "\n", "        ", "return", "[", "*", "self", ".", "adjusted_span_doc", ",", "self", ".", "mention_type", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Relation.align": [[82, 85], ["parse_ace_event.Relation.arg1.align", "parse_ace_event.Relation.arg2.align"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align"], ["def", "align", "(", "self", ",", "sent", ")", ":", "\n", "        ", "self", ".", "arg1", ".", "align", "(", "sent", ")", "\n", "self", ".", "arg2", ".", "align", "(", "sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Relation.adjust": [[86, 89], ["parse_ace_event.Relation.arg1.adjust", "parse_ace_event.Relation.arg2.adjust"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust"], ["", "def", "adjust", "(", "self", ",", "tok", ")", ":", "\n", "        ", "self", ".", "arg1", ".", "adjust", "(", "tok", ")", "\n", "self", ".", "arg2", ".", "adjust", "(", "tok", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Relation.adjust_spans_doc": [[90, 93], ["parse_ace_event.Relation.arg1.adjust_spans_doc", "parse_ace_event.Relation.arg2.adjust_spans_doc"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc"], ["", "def", "adjust_spans_doc", "(", "self", ",", "entry_start", ")", ":", "\n", "        ", "self", ".", "arg1", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "self", ".", "arg2", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Relation.to_json": [[94, 96], ["None"], "methods", ["None"], ["", "def", "to_json", "(", "self", ")", ":", "\n", "        ", "return", "[", "*", "self", ".", "arg1", ".", "adjusted_span_doc", ",", "*", "self", ".", "arg2", ".", "adjusted_span_doc", ",", "self", ".", "relation_type", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Event.align": [[115, 119], ["parse_ace_event.Event.trigger.align", "arg.align"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align"], ["def", "align", "(", "self", ",", "sent", ")", ":", "\n", "        ", "self", ".", "trigger", ".", "align", "(", "sent", ")", "\n", "for", "arg", "in", "self", ".", "arguments", ":", "\n", "            ", "arg", ".", "align", "(", "sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Event.adjust": [[120, 124], ["parse_ace_event.Event.trigger.adjust", "arg.adjust"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust"], ["", "", "def", "adjust", "(", "self", ",", "tok", ")", ":", "\n", "        ", "self", ".", "trigger", ".", "adjust", "(", "tok", ")", "\n", "for", "arg", "in", "self", ".", "arguments", ":", "\n", "            ", "arg", ".", "adjust", "(", "tok", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Event.adjust_spans_doc": [[125, 129], ["parse_ace_event.Event.trigger.adjust_spans_doc", "arg.adjust_spans_doc"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc"], ["", "", "def", "adjust_spans_doc", "(", "self", ",", "entry_start", ")", ":", "\n", "        ", "self", ".", "trigger", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "for", "arg", "in", "self", ".", "arguments", ":", "\n", "            ", "arg", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Event.to_json": [[130, 141], ["args.append", "sorted"], "methods", ["None"], ["", "", "def", "to_json", "(", "self", ")", ":", "\n", "        ", "trigger_span", "=", "self", ".", "trigger", ".", "adjusted_span_doc", "\n", "assert", "trigger_span", "[", "0", "]", "==", "trigger_span", "[", "1", "]", "\n", "trigger", "=", "[", "[", "trigger_span", "[", "0", "]", ",", "self", ".", "trigger", ".", "trigger_type", "]", "]", "\n", "args", "=", "[", "]", "\n", "for", "arg", "in", "self", ".", "arguments", ":", "\n", "# Collapse time argument roles following Bishan.", "\n", "            ", "arg_role", "=", "\"Time\"", "if", "\"Time\"", "in", "arg", ".", "argument_role", "else", "arg", ".", "argument_role", "\n", "args", ".", "append", "(", "[", "*", "arg", ".", "adjusted_span_doc", ",", "arg_role", "]", ")", "\n", "", "res", "=", "trigger", "+", "sorted", "(", "args", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align": [[150, 157], ["entity.align", "relation.align", "event.align"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align"], ["def", "align", "(", "self", ")", ":", "\n", "        ", "for", "entity", "in", "self", ".", "entities", ":", "\n", "            ", "entity", ".", "align", "(", "self", ".", "sent", ")", "\n", "", "for", "relation", "in", "self", ".", "relations", ":", "\n", "            ", "relation", ".", "align", "(", "self", ".", "sent", ")", "\n", "", "for", "event", "in", "self", ".", "events", ":", "\n", "            ", "event", ".", "align", "(", "self", ".", "sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.remove_whitespace": [[158, 167], ["parse_ace_event.Entry.align", "parse_ace_event.Entry.sent.as_doc", "parse_ace_event.Entry.adjust", "final_toks.append"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.align", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust"], ["", "", "def", "remove_whitespace", "(", "self", ")", ":", "\n", "        ", "final_toks", "=", "[", "]", "\n", "self", ".", "align", "(", ")", "\n", "for", "tok", "in", "self", ".", "sent", ".", "as_doc", "(", ")", ":", "\n", "            ", "if", "tok", ".", "is_space", ":", "\n", "                ", "self", ".", "adjust", "(", "tok", ")", "\n", "", "else", ":", "\n", "                ", "final_toks", ".", "append", "(", "tok", ")", "\n", "", "", "self", ".", "final_toks", "=", "final_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust": [[168, 175], ["entity.adjust", "relation.adjust", "event.adjust"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust"], ["", "def", "adjust", "(", "self", ",", "tok", ")", ":", "\n", "        ", "for", "entity", "in", "self", ".", "entities", ":", "\n", "            ", "entity", ".", "adjust", "(", "tok", ")", "\n", "", "for", "relation", "in", "self", ".", "relations", ":", "\n", "            ", "relation", ".", "adjust", "(", "tok", ")", "\n", "", "for", "event", "in", "self", ".", "events", ":", "\n", "            ", "event", ".", "adjust", "(", "tok", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.adjust_spans_doc": [[176, 184], ["entity.adjust_spans_doc", "relation.adjust_spans_doc", "event.adjust_spans_doc"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc"], ["", "", "def", "adjust_spans_doc", "(", "self", ",", "entry_start", ")", ":", "\n", "        ", "self", ".", "adjusted_start", "=", "entry_start", "\n", "for", "entity", "in", "self", ".", "entities", ":", "\n", "            ", "entity", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "", "for", "relation", "in", "self", ".", "relations", ":", "\n", "            ", "relation", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "", "for", "event", "in", "self", ".", "events", ":", "\n", "            ", "event", ".", "adjust_spans_doc", "(", "entry_start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.to_json": [[185, 194], ["sorted", "sorted", "sorted", "dict", "entity.to_json", "relation.to_json", "event.to_json"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json"], ["", "", "def", "to_json", "(", "self", ")", ":", "\n", "        ", "self", ".", "entities", "=", "sorted", "(", "self", ".", "entities", ",", "key", "=", "lambda", "x", ":", "x", ".", "span_sentence", ")", "\n", "ner", "=", "[", "entity", ".", "to_json", "(", ")", "for", "entity", "in", "self", ".", "entities", "]", "\n", "ner_flavors", "=", "[", "entity", ".", "flavor", "for", "entity", "in", "self", ".", "entities", "]", "\n", "relations", "=", "sorted", "(", "[", "relation", ".", "to_json", "(", ")", "for", "relation", "in", "self", ".", "relations", "]", ")", "\n", "events", "=", "sorted", "(", "[", "event", ".", "to_json", "(", ")", "for", "event", "in", "self", ".", "events", "]", ")", "\n", "sentences", "=", "[", "tok", ".", "text", "for", "tok", "in", "self", ".", "final_toks", "]", "\n", "return", "dict", "(", "sentences", "=", "sentences", ",", "ner", "=", "ner", ",", "relations", "=", "relations", ",", "events", "=", "events", ",", "\n", "sentence_start", "=", "self", ".", "adjusted_start", ",", "ner_flavor", "=", "ner_flavors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.is_real": [[195, 207], ["len", "len", "len", "len"], "methods", ["None"], ["", "def", "is_real", "(", "self", ")", ":", "\n", "# If no tokens, make sure it's got no entities or anything.", "\n", "        ", "n_toks", "=", "len", "(", "self", ".", "final_toks", ")", "\n", "# Get rid of empty sentences", "\n", "n_entities", "=", "len", "(", "self", ".", "entities", ")", "\n", "n_relations", "=", "len", "(", "self", ".", "relations", ")", "\n", "n_events", "=", "len", "(", "self", ".", "events", ")", "\n", "if", "n_toks", "==", "0", ":", "\n", "            ", "assert", "n_entities", "==", "n_relations", "==", "n_events", "==", "0", "\n", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.__init__": [[210, 213], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "entries", ",", "doc_key", ")", ":", "\n", "        ", "self", ".", "entries", "=", "entries", "\n", "self", ".", "doc_key", "=", "doc_key", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.remove_whitespace": [[214, 218], ["entry.remove_whitespace", "entry.is_real"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.remove_whitespace", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Entry.is_real"], ["", "def", "remove_whitespace", "(", "self", ")", ":", "\n", "        ", "for", "entry", "in", "self", ".", "entries", ":", "\n", "            ", "entry", ".", "remove_whitespace", "(", ")", "\n", "", "self", ".", "entries", "=", "[", "entry", "for", "entry", "in", "self", ".", "entries", "if", "entry", ".", "is_real", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc": [[219, 227], ["numpy.cumsum", "numpy.roll", "zip", "len", "entry.adjust_spans_doc"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc"], ["", "def", "adjust_spans_doc", "(", "self", ")", ":", "\n", "# Get the token starts of the sentence", "\n", "        ", "entry_lengths", "=", "[", "len", "(", "entry", ".", "final_toks", ")", "for", "entry", "in", "self", ".", "entries", "]", "\n", "entry_starts", "=", "np", ".", "cumsum", "(", "entry_lengths", ")", "\n", "entry_starts", "=", "np", ".", "roll", "(", "entry_starts", ",", "1", ")", "\n", "entry_starts", "[", "0", "]", "=", "0", "\n", "for", "entry", ",", "start", "in", "zip", "(", "self", ".", "entries", ",", "entry_starts", ")", ":", "\n", "            ", "entry", ".", "adjust_spans_doc", "(", "start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.to_json": [[228, 237], ["parse_ace_event.Doc.remove_whitespace", "parse_ace_event.Doc.adjust_spans_doc", "entry.to_json"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.remove_whitespace", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Doc.adjust_spans_doc", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json"], ["", "", "def", "to_json", "(", "self", ")", ":", "\n", "        ", "self", ".", "remove_whitespace", "(", ")", "\n", "self", ".", "adjust_spans_doc", "(", ")", "\n", "by_entry", "=", "[", "entry", ".", "to_json", "(", ")", "for", "entry", "in", "self", ".", "entries", "]", "\n", "res", "=", "{", "}", "\n", "for", "field", "in", "[", "\"sentences\"", ",", "\"ner\"", ",", "\"relations\"", ",", "\"events\"", ",", "\"sentence_start\"", "]", ":", "\n", "            ", "res", "[", "field", "]", "=", "[", "entry", "[", "field", "]", "for", "entry", "in", "by_entry", "]", "\n", "", "res", "[", "\"doc_key\"", "]", "=", "self", ".", "doc_key", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.__init__": [[267, 293], ["xml.parse", "parse_ace_event.Document._load_text", "parse_ace_event.Document._make_nlp", "parse_ace_event.Document._populate_entity_list", "parse_ace_event.Document._populate_entity_lookup", "parse_ace_event.Document._populate_event_list", "parse_ace_event.Document._populate_relation_list"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._load_text", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._make_nlp", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_entity_list", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_entity_lookup", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_event_list", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_relation_list"], ["    ", "def", "__init__", "(", "self", ",", "annotation_path", ",", "text_path", ",", "doc_key", ",", "fold", ",", "heads_only", "=", "True", ",", "\n", "real_entities_only", "=", "True", ",", "include_pronouns", "=", "False", ")", ":", "\n", "        ", "'''\n        A base class for ACE xml annotation\n        :param annotation_path:\n        :param text_path:\n        '''", "\n", "self", ".", "_heads_only", "=", "heads_only", "\n", "self", ".", "_real_entities_only", "=", "real_entities_only", "\n", "self", ".", "_doc_key", "=", "doc_key", "\n", "self", ".", "_annotation_path", "=", "annotation_path", "\n", "self", ".", "_annotation_xml", "=", "ET", ".", "parse", "(", "self", ".", "_annotation_path", ")", "\n", "self", ".", "_text_path", "=", "text_path", "\n", "self", ".", "_text", "=", "self", ".", "_load_text", "(", "text_path", ")", "\n", "self", ".", "doc", "=", "self", ".", "_make_nlp", "(", "self", ".", "_text", ")", "\n", "assert", "self", ".", "doc", ".", "text", "==", "self", ".", "_text", "\n", "self", ".", "entity_list", ",", "self", ".", "entity_ids", "=", "self", ".", "_populate_entity_list", "(", ")", "\n", "self", ".", "entity_lookup", "=", "self", ".", "_populate_entity_lookup", "(", ")", "\n", "if", "self", ".", "_real_entities_only", ":", "\n", "            ", "self", ".", "_allowed_flavors", "=", "[", "\"entity\"", ",", "\"pronoun\"", "]", "if", "include_pronouns", "else", "[", "\"entity\"", "]", "\n", "self", ".", "entity_list", "=", "[", "x", "for", "x", "in", "self", ".", "entity_list", "if", "x", ".", "flavor", "in", "self", ".", "_allowed_flavors", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "_allowed_flavors", "=", "None", "\n", "", "self", ".", "event_list", "=", "self", ".", "_populate_event_list", "(", ")", "\n", "self", ".", "relation_list", "=", "self", ".", "_populate_relation_list", "(", ")", "\n", "self", ".", "_fold", "=", "fold", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._make_nlp": [[294, 376], ["spacy.load", "spacy.load.add_pipe", "spacy.load.", "enumerate", "spacy.load.tokenizer.add_special_case", "special_case.upper", "spacy.load.tokenizer.add_special_case", "special_case.capitalize", "spacy.load.tokenizer.add_special_case", "dict", "dict", "dict", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_make_nlp", "(", "self", ",", "text", ")", ":", "\n", "        ", "'''\n        Add a few special cases to spacy tokenizer so it works with ACe mistakes.\n        '''", "\n", "# Prevent edge case where there are sentence breaks in bad places", "\n", "def", "custom_seg", "(", "doc", ")", ":", "\n", "            ", "for", "index", ",", "token", "in", "enumerate", "(", "doc", ")", ":", "\n", "                ", "if", "self", ".", "_doc_key", "==", "\"AFP_ENG_20030417.0307\"", ":", "\n", "                    ", "if", "token", ".", "text", "==", "\"Ivanov\"", ":", "\n", "                        ", "token", ".", "sent_start", "=", "False", "\n", "", "", "if", "'--'", "in", "token", ".", "text", ":", "\n", "                    ", "doc", "[", "index", "]", ".", "sent_start", "=", "False", "\n", "doc", "[", "index", "+", "1", "]", ".", "sent_start", "=", "False", "\n", "", "if", "token", ".", "text", "==", "\"things\"", "and", "doc", "[", "index", "+", "1", "]", ".", "text", "==", "\"their\"", ":", "\n", "                    ", "doc", "[", "index", "+", "1", "]", ".", "sent_start", "=", "False", "\n", "", "if", "(", "token", ".", "text", "==", "\"Explosions\"", "and", "\n", "token", ".", "i", "<", "len", "(", "doc", ")", "and", "\n", "doc", "[", "index", "-", "1", "]", ".", "text", "==", "\".\"", "and", "\n", "doc", "[", "index", "-", "2", "]", ".", "text", "==", "\"Baghdad\"", ")", ":", "\n", "                    ", "token", ".", "sent_start", "=", "True", "\n", "# Comma followed by whitespace doesn't end a sentence.", "\n", "", "if", "token", ".", "text", "==", "\",\"", "and", "doc", "[", "index", "+", "1", "]", ".", "is_space", ":", "\n", "                    ", "doc", "[", "index", "+", "2", "]", ".", "sent_start", "=", "False", "\n", "# \"And\" only starts a sentence if preceded by period or question mark.", "\n", "", "if", "token", ".", "text", "in", "[", "\"and\"", ",", "\"but\"", "]", "and", "doc", "[", "index", "-", "1", "]", ".", "text", "not", "in", "[", "\".\"", ",", "\"?\"", ",", "\"!\"", "]", ":", "\n", "                    ", "doc", "[", "index", "]", ".", "sent_start", "=", "False", "\n", "", "if", "(", "not", "(", "(", "token", ".", "is_punct", "and", "token", ".", "text", "not", "in", "[", "\",\"", ",", "\"_\"", ",", "\";\"", ",", "\"...\"", ",", "\":\"", ",", "\"(\"", ",", "\")\"", ",", "'\"'", "]", ")", "or", "token", ".", "is_space", ")", "\n", "and", "index", "<", "len", "(", "doc", ")", "-", "1", ")", ":", "\n", "                    ", "doc", "[", "index", "+", "1", "]", ".", "sent_start", "=", "False", "\n", "", "if", "\"\\n\"", "in", "token", ".", "text", ":", "\n", "                    ", "if", "index", "+", "1", "<", "len", "(", "doc", ")", ":", "\n", "                        ", "next_token", "=", "doc", "[", "index", "+", "1", "]", "\n", "if", "len", "(", "token", ")", ">", "1", ":", "\n", "                            ", "next_token", ".", "sent_start", "=", "True", "\n", "", "else", ":", "\n", "                            ", "next_token", ".", "sent_start", "=", "False", "\n", "", "", "", "if", "token", ".", "text", "==", "\"-\"", ":", "\n", "                    ", "before", "=", "doc", "[", "index", "-", "1", "]", "\n", "after", "=", "doc", "[", "index", "+", "1", "]", "\n", "if", "not", "(", "before", ".", "is_space", "or", "before", ".", "is_punct", "or", "after", ".", "is_space", "or", "after", ".", "is_punct", ")", ":", "\n", "                        ", "after", ".", "sent_start", "=", "False", "\n", "", "", "", "return", "doc", "\n", "\n", "", "nlp", "=", "spacy", ".", "load", "(", "'en'", ")", "\n", "nlp", ".", "add_pipe", "(", "custom_seg", ",", "before", "=", "'parser'", ")", "\n", "\n", "single_tokens", "=", "[", "'sgt.'", ",", "\n", "'sen.'", ",", "\n", "'col.'", ",", "\n", "'brig.'", ",", "\n", "'gen.'", ",", "\n", "'maj.'", ",", "\n", "'sr.'", ",", "\n", "'lt.'", ",", "\n", "'cmdr.'", ",", "\n", "'u.s.'", ",", "\n", "'mr.'", ",", "\n", "'p.o.w.'", ",", "\n", "'u.k.'", ",", "\n", "'u.n.'", ",", "\n", "'ft.'", ",", "\n", "'dr.'", ",", "\n", "'d.c.'", ",", "\n", "'mt.'", ",", "\n", "'st.'", ",", "\n", "'snr.'", ",", "\n", "'rep.'", ",", "\n", "'ms.'", ",", "\n", "'capt.'", ",", "\n", "'sq.'", ",", "\n", "'jr.'", ",", "\n", "'ave.'", "]", "\n", "for", "special_case", "in", "single_tokens", ":", "\n", "            ", "nlp", ".", "tokenizer", ".", "add_special_case", "(", "special_case", ",", "[", "dict", "(", "ORTH", "=", "special_case", ")", "]", ")", "\n", "upped", "=", "special_case", ".", "upper", "(", ")", "\n", "nlp", ".", "tokenizer", ".", "add_special_case", "(", "upped", ",", "[", "dict", "(", "ORTH", "=", "upped", ")", "]", ")", "\n", "capped", "=", "special_case", ".", "capitalize", "(", ")", "\n", "nlp", ".", "tokenizer", ".", "add_special_case", "(", "capped", ",", "[", "dict", "(", "ORTH", "=", "capped", ")", "]", ")", "\n", "\n", "", "doc", "=", "nlp", "(", "text", ")", "\n", "assert", "doc", ".", "text", "==", "text", "\n", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._load_text": [[377, 397], ["re.compile", "re.compile.sub", "text_data.replace.replace.replace", "text_data.replace.replace.replace", "open", "f.read", "text_data.replace.replace.replace"], "methods", ["None"], ["", "def", "_load_text", "(", "self", ",", "text_path", ")", ":", "\n", "        ", "'''\n        Load in text and strip out tags.\n        '''", "\n", "with", "open", "(", "text_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "text_data", "=", "f", ".", "read", "(", ")", "\n", "\n", "# Get rid of XML tags.", "\n", "", "remove_tags", "=", "re", ".", "compile", "(", "'<.*?>'", ",", "re", ".", "DOTALL", ")", "# Also match expressions with a newline in the middle.", "\n", "text_data", "=", "remove_tags", ".", "sub", "(", "\"\"", ",", "text_data", ")", "\n", "\n", "# Fix errors in ACE.", "\n", "text_data", "=", "text_data", ".", "replace", "(", "\"dr. germ. the\"", ",", "\"dr. germ, the\"", ")", "\n", "text_data", "=", "text_data", ".", "replace", "(", "\"arms inspectors. 300 miles west\"", ",", "\n", "\"arms inspectors, 300 miles west\"", ")", "\n", "\n", "if", "self", ".", "_doc_key", "in", "[", "\"APW_ENG_20030327.0376\"", ",", "\"APW_ENG_20030519.0367\"", "]", ":", "\n", "            ", "text_data", "=", "text_data", ".", "replace", "(", "\"_\"", ",", "\"-\"", ")", "\n", "\n", "", "return", "text_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars": [[398, 416], ["parse_ace_event.Document.doc.char_span", "parse_ace_event.get_token_of", "parse_ace_event.get_token_of", "parse_ace_event.MultiTokenTrigerException", "parse_ace_event.Document.doc.char_span", "len"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_of", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_of"], ["", "def", "_get_chars", "(", "self", ",", "start_char", ",", "end_char", ",", "trigger", "=", "False", ")", ":", "\n", "        ", "the_text", "=", "self", ".", "doc", ".", "char_span", "(", "start_char", ",", "end_char", "+", "1", ")", "\n", "start_tok", "=", "get_token_of", "(", "self", ".", "doc", ",", "start_char", ")", "\n", "end_tok", "=", "get_token_of", "(", "self", ".", "doc", ",", "end_char", ")", "\n", "if", "trigger", "and", "start_tok", "!=", "end_tok", ":", "\n", "            ", "raise", "MultiTokenTrigerException", "(", ")", "\n", "# # If the trigger is multiple words, get the highest token in the dependency parse.", "\n", "# the_root = self.doc[start_tok.i:end_tok.i + 1].root", "\n", "# start_char = the_root.idx", "\n", "# end_char = start_char + len(the_root) - 1", "\n", "# the_text = the_root.text", "\n", "", "elif", "the_text", "is", "None", ":", "\n", "# Otherwise, just take all spans containing the entity.", "\n", "            ", "start_char", "=", "start_tok", ".", "idx", "\n", "end_char", "=", "end_tok", ".", "idx", "+", "len", "(", "end_tok", ")", "-", "1", "\n", "the_text", "=", "self", ".", "doc", ".", "char_span", "(", "start_char", ",", "end_char", "+", "1", ")", "\n", "\n", "", "return", "start_char", ",", "end_char", ",", "the_text", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_entity_list": [[417, 495], ["parse_ace_event.Document._annotation_xml.getroot", "xml_root[].findall", "xml_root[].findall", "xml_root[].findall", "entity_ids.append", "one_entity.findall", "entity_ids.append", "one_value.findall", "entity_ids.append", "one_timex2.findall", "int", "int", "parse_ace_event.Document._get_chars", "parse_ace_event.Entity", "res.append", "int", "int", "parse_ace_event.Document._get_chars", "parse_ace_event.Entity", "res.append", "int", "int", "parse_ace_event.Document._get_chars", "set", "parse_ace_event.Entity", "res.append", "one_entity_mention.find", "one_entity_mention.find", "one_value_mention.find", "one_value_mention.find", "one_timex2_mention.find", "one_timex2_mention.find"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars"], ["", "def", "_populate_entity_list", "(", "self", ")", ":", "\n", "        ", "entity_ids", "=", "[", "]", "\n", "res", "=", "[", "]", "\n", "xml_root", "=", "self", ".", "_annotation_xml", ".", "getroot", "(", ")", "\n", "field_to_find", "=", "\"head\"", "if", "self", ".", "_heads_only", "else", "\"extent\"", "\n", "for", "one_entity", "in", "xml_root", "[", "0", "]", ".", "findall", "(", "'entity'", ")", ":", "\n", "            ", "entity_id", "=", "one_entity", ".", "attrib", "[", "\"ID\"", "]", "\n", "entity_ids", ".", "append", "(", "entity_id", ")", "\n", "for", "one_entity_mention", "in", "one_entity", ".", "findall", "(", "'entity_mention'", ")", ":", "\n", "                ", "mention_id", "=", "one_entity_mention", ".", "attrib", "[", "'ID'", "]", "\n", "mention_type", "=", "one_entity", ".", "attrib", "[", "'TYPE'", "]", "\n", "# Others have only looked at the head.", "\n", "tentative_start", "=", "int", "(", "one_entity_mention", ".", "find", "(", "field_to_find", ")", "[", "0", "]", ".", "attrib", "[", "'START'", "]", ")", "\n", "tentative_end", "=", "int", "(", "one_entity_mention", ".", "find", "(", "field_to_find", ")", "[", "0", "]", ".", "attrib", "[", "'END'", "]", ")", "\n", "\n", "start_char", ",", "end_char", ",", "text_string", "=", "self", ".", "_get_chars", "(", "tentative_start", ",", "tentative_end", ")", "\n", "\n", "# Parser chokes on the space.", "\n", "if", "(", "self", ".", "_doc_key", "==", "\"soc.history.war.world-war-ii_20050127.2403\"", "and", "\n", "text_string", ".", "text", "==", "\"lesliemills2002@netscape. net\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "# Keep option to ignore pronouns.", "\n", "", "flavor", "=", "\"pronoun\"", "if", "one_entity_mention", ".", "attrib", "[", "\"TYPE\"", "]", "==", "\"PRO\"", "else", "\"entity\"", "\n", "\n", "entry", "=", "Entity", "(", "start_char", ",", "end_char", ",", "text_string", ",", "mention_id", "=", "mention_id", ",", "\n", "mention_type", "=", "mention_type", ",", "flavor", "=", "flavor", ")", "\n", "res", ".", "append", "(", "entry", ")", "\n", "\n", "# Values. Values don't have heads.", "\n", "", "", "field_to_find", "=", "\"extent\"", "\n", "for", "one_value", "in", "xml_root", "[", "0", "]", ".", "findall", "(", "'value'", ")", ":", "\n", "            ", "value_id", "=", "one_value", ".", "attrib", "[", "\"ID\"", "]", "\n", "entity_ids", ".", "append", "(", "value_id", ")", "\n", "for", "one_value_mention", "in", "one_value", ".", "findall", "(", "'value_mention'", ")", ":", "\n", "                ", "mention_id", "=", "one_value_mention", ".", "attrib", "[", "'ID'", "]", "\n", "# In the AAAI 2019 paper, they lump all the values together into one label.", "\n", "mention_type", "=", "'VALUE'", "\n", "\n", "tentative_start", "=", "int", "(", "one_value_mention", ".", "find", "(", "field_to_find", ")", "[", "0", "]", ".", "attrib", "[", "'START'", "]", ")", "\n", "tentative_end", "=", "int", "(", "one_value_mention", ".", "find", "(", "field_to_find", ")", "[", "0", "]", ".", "attrib", "[", "'END'", "]", ")", "\n", "start_char", ",", "end_char", ",", "text_string", "=", "self", ".", "_get_chars", "(", "tentative_start", ",", "tentative_end", ")", "\n", "\n", "# Parser chokes on the space.", "\n", "if", "(", "self", ".", "_doc_key", "==", "\"soc.history.war.world-war-ii_20050127.2403\"", "and", "\n", "text_string", ".", "text", "==", "\"lesliemills2002@netscape. net\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "entry", "=", "Entity", "(", "start_char", ",", "end_char", ",", "text_string", ",", "mention_id", "=", "mention_id", ",", "\n", "mention_type", "=", "mention_type", ",", "flavor", "=", "\"value\"", ")", "\n", "res", ".", "append", "(", "entry", ")", "\n", "\n", "# Also timex2. These also don't have heads.", "\n", "", "", "field_to_find", "=", "\"extent\"", "\n", "for", "one_timex2", "in", "xml_root", "[", "0", "]", ".", "findall", "(", "'timex2'", ")", ":", "\n", "            ", "timex2_id", "=", "one_timex2", ".", "attrib", "[", "\"ID\"", "]", "\n", "entity_ids", ".", "append", "(", "timex2_id", ")", "\n", "for", "one_timex2_mention", "in", "one_timex2", ".", "findall", "(", "'timex2_mention'", ")", ":", "\n", "                ", "mention_id", "=", "one_timex2_mention", ".", "attrib", "[", "'ID'", "]", "\n", "mention_type", "=", "'TIMEX2'", "\n", "# Others have only looked at the head.", "\n", "tentative_start", "=", "int", "(", "one_timex2_mention", ".", "find", "(", "field_to_find", ")", "[", "0", "]", ".", "attrib", "[", "'START'", "]", ")", "\n", "tentative_end", "=", "int", "(", "one_timex2_mention", ".", "find", "(", "field_to_find", ")", "[", "0", "]", ".", "attrib", "[", "'END'", "]", ")", "\n", "start_char", ",", "end_char", ",", "text_string", "=", "self", ".", "_get_chars", "(", "tentative_start", ",", "tentative_end", ")", "\n", "\n", "# Crosses a sentence boundary.", "\n", "if", "self", ".", "_doc_key", "==", "\"CNN_ENG_20030508_210555.5\"", "and", "start_char", "==", "1316", "and", "end_char", "==", "1335", ":", "\n", "                    ", "continue", "\n", "# This is just ridiculous.", "\n", "", "weird_times", "=", "set", "(", "[", "\"BACONSREBELLION_20050127.1017\"", ",", "\"MARKBACKER_20041103.1300\"", "]", ")", "\n", "if", "self", ".", "_doc_key", "in", "weird_times", "and", "\"????\"", "in", "text_string", ".", "text", ":", "\n", "                    ", "continue", "\n", "\n", "", "entry", "=", "Entity", "(", "start_char", ",", "end_char", ",", "text_string", ",", "mention_id", "=", "mention_id", ",", "\n", "mention_type", "=", "mention_type", ",", "flavor", "=", "\"timex2\"", ")", "\n", "res", ".", "append", "(", "entry", ")", "\n", "\n", "", "", "return", "res", ",", "entity_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_entity_lookup": [[496, 498], ["None"], "methods", ["None"], ["", "def", "_populate_entity_lookup", "(", "self", ")", ":", "\n", "        ", "return", "{", "entry", ".", "mention_id", ":", "entry", "for", "entry", "in", "self", ".", "entity_list", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_event_list": [[499, 554], ["parse_ace_event.Document._annotation_xml.getroot", "xml_root[].findall", "one_event.findall", "one_event_mention.find", "parse_ace_event.EventTrigger", "one_event_mention.findall", "parse_ace_event.Document._get_chars", "parse_ace_event.EventArgument", "argument_list.append", "res.append", "int", "int", "one_event_mention_argument.find", "one_event_mention_argument.find", "parse_ace_event.Document._get_chars", "parse_ace_event.Event", "int", "int", "argument_id.split"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars"], ["", "def", "_populate_event_list", "(", "self", ")", ":", "\n", "        ", "res", "=", "[", "]", "\n", "xml_root", "=", "self", ".", "_annotation_xml", ".", "getroot", "(", ")", "\n", "for", "one_event", "in", "xml_root", "[", "0", "]", ".", "findall", "(", "'event'", ")", ":", "\n", "            ", "for", "one_event_mention", "in", "one_event", ".", "findall", "(", "'event_mention'", ")", ":", "\n", "                ", "include", "=", "True", "\n", "trigger_id", "=", "one_event_mention", ".", "attrib", "[", "'ID'", "]", "\n", "trigger_type", "=", "'%s.%s'", "%", "(", "one_event", ".", "attrib", "[", "'TYPE'", "]", ",", "one_event", ".", "attrib", "[", "'SUBTYPE'", "]", ")", "\n", "trigger_tag", "=", "one_event_mention", ".", "find", "(", "'anchor'", ")", "\n", "try", ":", "\n", "                    ", "start_char", ",", "end_char", ",", "text_string", "=", "self", ".", "_get_chars", "(", "\n", "int", "(", "trigger_tag", "[", "0", "]", ".", "attrib", "[", "'START'", "]", ")", ",", "\n", "int", "(", "trigger_tag", "[", "0", "]", ".", "attrib", "[", "'END'", "]", ")", ",", "\n", "trigger", "=", "True", ")", "\n", "# If we hit a multi-token trigger, skip the event mention.", "\n", "", "except", "MultiTokenTrigerException", ":", "\n", "                    ", "continue", "\n", "# Buggy event. Crosses sentence. Skip it.", "\n", "", "if", "self", ".", "_doc_key", "==", "\"APW_ENG_20030308.0314\"", "and", "start_char", "==", "3263", "and", "end_char", "==", "3270", ":", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "_doc_key", "==", "\"soc.history.what-if_20050129.1404\"", "and", "start_char", "==", "554", "and", "end_char", "==", "556", ":", "\n", "                    ", "continue", "\n", "", "event_trigger", "=", "EventTrigger", "(", "start_char", ",", "end_char", ",", "text_string", ",", "trigger_id", ",", "\n", "trigger_type", ")", "\n", "argument_list", "=", "[", "]", "\n", "for", "one_event_mention_argument", "in", "one_event_mention", ".", "findall", "(", "'event_mention_argument'", ")", ":", "\n", "                    ", "argument_id", "=", "one_event_mention_argument", ".", "attrib", "[", "'REFID'", "]", "\n", "if", "self", ".", "_heads_only", ":", "\n", "                        ", "assert", "argument_id", "in", "self", ".", "entity_lookup", "\n", "this_entity", "=", "self", ".", "entity_lookup", "[", "argument_id", "]", "\n", "# If we're only doing real entities and this isn't one, don't append.", "\n", "if", "self", ".", "_real_entities_only", "and", "this_entity", ".", "flavor", "not", "in", "self", ".", "_allowed_flavors", ":", "\n", "                            ", "continue", "\n", "", "start_char", ",", "end_char", ",", "text_string", "=", "(", "this_entity", ".", "start_char", ",", "\n", "this_entity", ".", "end_char", ",", "\n", "this_entity", ".", "text_string", ")", "\n", "", "else", ":", "\n", "                        ", "event_mention_argument_tag", "=", "one_event_mention_argument", ".", "find", "(", "'extent'", ")", "\n", "relation_mention_argument_tag", "=", "one_event_mention_argument", ".", "find", "(", "'extent'", ")", "\n", "start_char", ",", "end_char", ",", "text_string", "=", "self", ".", "_get_chars", "(", "\n", "int", "(", "event_mention_argument_tag", "[", "0", "]", ".", "attrib", "[", "'START'", "]", ")", ",", "\n", "int", "(", "event_mention_argument_tag", "[", "0", "]", ".", "attrib", "[", "'END'", "]", ")", ")", "\n", "\n", "# Check that we've seen the entity. If it's a value or timex, just skip it as an", "\n", "# argument.", "\n", "", "entity_id", "=", "\"-\"", ".", "join", "(", "argument_id", ".", "split", "(", "\"-\"", ")", "[", ":", "-", "1", "]", ")", "\n", "assert", "entity_id", "in", "self", ".", "entity_ids", "\n", "\n", "argument_role", "=", "one_event_mention_argument", ".", "attrib", "[", "'ROLE'", "]", "\n", "to_append", "=", "EventArgument", "(", "start_char", ",", "end_char", ",", "text_string", ",", "argument_id", ",", "\n", "argument_role", ")", "\n", "argument_list", ".", "append", "(", "to_append", ")", "\n", "", "if", "include", ":", "\n", "                    ", "res", ".", "append", "(", "Event", "(", "event_trigger", ",", "argument_list", ")", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._populate_relation_list": [[555, 623], ["parse_ace_event.Document._annotation_xml.getroot", "xml_root[].findall", "one_relation.findall", "one_relation_mention.findall", "parse_ace_event.RelationArgument", "parse_ace_event.Relation", "one_relation_mention_argument.find", "parse_ace_event.Document._get_chars", "res.append", "int", "int", "argument_id.split"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_chars"], ["", "def", "_populate_relation_list", "(", "self", ")", ":", "\n", "        ", "res", "=", "[", "]", "\n", "xml_root", "=", "self", ".", "_annotation_xml", ".", "getroot", "(", ")", "\n", "for", "one_relation", "in", "xml_root", "[", "0", "]", ".", "findall", "(", "'relation'", ")", ":", "\n", "            ", "for", "one_relation_mention", "in", "one_relation", ".", "findall", "(", "'relation_mention'", ")", ":", "\n", "                ", "include", "=", "True", "\n", "relation_type", "=", "'%s.%s'", "%", "(", "one_relation", ".", "attrib", "[", "'TYPE'", "]", ",", "one_relation", ".", "attrib", "[", "'SUBTYPE'", "]", ")", "\n", "argument_dict", "=", "{", "}", "\n", "for", "one_relation_mention_argument", "in", "one_relation_mention", ".", "findall", "(", "\"relation_mention_argument\"", ")", ":", "\n", "                    ", "argument_id", "=", "one_relation_mention_argument", ".", "attrib", "[", "'REFID'", "]", "\n", "# If doing heads only, get the span by looking up the entity and getting its span.", "\n", "if", "self", ".", "_heads_only", ":", "\n", "                        ", "assert", "argument_id", "in", "self", ".", "entity_lookup", "\n", "this_entity", "=", "self", ".", "entity_lookup", "[", "argument_id", "]", "\n", "start_char", ",", "end_char", ",", "text_string", "=", "(", "this_entity", ".", "start_char", ",", "\n", "this_entity", ".", "end_char", ",", "\n", "this_entity", ".", "text_string", ")", "\n", "", "else", ":", "\n", "                        ", "relation_mention_argument_tag", "=", "one_relation_mention_argument", ".", "find", "(", "'extent'", ")", "\n", "start_char", ",", "end_char", ",", "text_string", "=", "self", ".", "_get_chars", "(", "\n", "int", "(", "relation_mention_argument_tag", "[", "0", "]", ".", "attrib", "[", "'START'", "]", ")", ",", "\n", "int", "(", "relation_mention_argument_tag", "[", "0", "]", ".", "attrib", "[", "'END'", "]", ")", ")", "\n", "\n", "# Check that we've seen the entity. If it's a value or timex, skip the event.", "\n", "", "entity_id", "=", "\"-\"", ".", "join", "(", "argument_id", ".", "split", "(", "\"-\"", ")", "[", ":", "-", "1", "]", ")", "\n", "assert", "entity_id", "in", "self", ".", "entity_ids", "\n", "\n", "relation_role", "=", "one_relation_mention_argument", ".", "attrib", "[", "'ROLE'", "]", "\n", "this_argument", "=", "RelationArgument", "(", "\n", "start_char", ",", "end_char", ",", "text_string", ",", "argument_id", ",", "relation_role", ")", "\n", "\n", "# Skip if not a real entity and we're only keeping real entities.", "\n", "if", "self", ".", "_heads_only", "and", "self", ".", "_real_entities_only", ":", "\n", "                        ", "this_entity", "=", "self", ".", "entity_lookup", "[", "this_argument", ".", "argument_id", "]", "\n", "if", "this_entity", ".", "flavor", "not", "in", "self", ".", "_allowed_flavors", ":", "\n", "                            ", "include", "=", "False", "\n", "\n", "", "", "if", "this_argument", ".", "relation_role", "==", "\"Arg-1\"", ":", "\n", "                        ", "argument_dict", "[", "\"arg1\"", "]", "=", "this_argument", "\n", "", "elif", "this_argument", ".", "relation_role", "==", "\"Arg-2\"", ":", "\n", "# This is a mis-annotated relation. Ignore it.", "\n", "                        ", "if", "(", "self", ".", "_doc_key", "==", "'CNN_ENG_20030430_093016.0'", "and", "\n", "text_string", ".", "text", "==", "\"the school in an\\nunderprivileged rural area\"", ")", ":", "\n", "                            ", "include", "=", "False", "\n", "", "if", "(", "self", ".", "_doc_key", "==", "\"CNN_ENG_20030430_093016.0\"", "and", "\n", "start_char", "==", "3091", "and", "end_char", "==", "3096", ")", ":", "\n", "                            ", "include", "=", "False", "\n", "# Crosses a sentence boundary.", "\n", "", "if", "(", "self", ".", "_doc_key", "==", "\"rec.travel.cruises_20050222.0313\"", "and", "\n", "start_char", "==", "1435", "and", "end_char", "==", "1442", ")", ":", "\n", "                            ", "include", "=", "False", "\n", "", "if", "(", "self", ".", "_doc_key", "==", "\"rec.travel.cruises_20050222.0313\"", "and", "\n", "start_char", "==", "1456", "and", "end_char", "==", "1458", ")", ":", "\n", "                            ", "include", "=", "False", "\n", "\n", "", "argument_dict", "[", "\"arg2\"", "]", "=", "this_argument", "\n", "", "else", ":", "\n", "                        ", "include", "=", "False", "\n", "", "", "if", "include", ":", "\n", "                    ", "relation", "=", "Relation", "(", "relation_type", ",", "argument_dict", "[", "\"arg1\"", "]", ",", "argument_dict", "[", "\"arg2\"", "]", ")", "\n", "# There are some examples where the identical relation mention shows up twice,", "\n", "# for instance \"young men and women in this country\" in", "\n", "# CNN_CF_20030304.1900.04.apf.xml. When this occurs, ignore it.", "\n", "if", "relation", "in", "res", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "                        ", "res", ".", "append", "(", "relation", ")", "\n", "", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._check_in_range": [[624, 637], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_check_in_range", "(", "span", ",", "sent", ")", ":", "\n", "# The end character inequality must be string. since end character for spans are inclusive", "\n", "# and end characters for sentences are exclusive.", "\n", "# Raise an exception if the span crosses a sentence boundary.", "\n", "        ", "if", "span", ".", "start_char", ">=", "sent", ".", "start_char", "and", "span", ".", "end_char", "<", "sent", ".", "end_char", ":", "\n", "            ", "return", "True", "\n", "", "if", "span", ".", "end_char", "<=", "sent", ".", "start_char", ":", "\n", "            ", "return", "False", "\n", "", "if", "span", ".", "start_char", ">=", "sent", ".", "end_char", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "raise", "CrossSentenceException", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._sentence_get_ner": [[638, 665], ["parse_ace_event.Document.entity_list.remove", "parse_ace_event.Document._check_in_range", "parse_ace_event.debug_if", "parse_ace_event.Document._seen_so_far[].append", "entities.append", "to_remove.append", "print"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._check_in_range", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.debug_if"], ["", "", "def", "_sentence_get_ner", "(", "self", ",", "sent", ")", ":", "\n", "        ", "entities", "=", "[", "]", "\n", "to_remove", "=", "[", "]", "# Only relevant for full extents.", "\n", "for", "entity", "in", "self", ".", "entity_list", ":", "\n", "            ", "try", ":", "\n", "                ", "in_range", "=", "self", ".", "_check_in_range", "(", "entity", ",", "sent", ")", "\n", "# If the entity crosses a sentence boundary", "\n", "", "except", "CrossSentenceException", "as", "e", ":", "\n", "# This shouldn't happen if we're only using entity heads; raise an exception.", "\n", "                ", "if", "self", ".", "_heads_only", ":", "\n", "                    ", "raise", "e", "\n", "# With full extents this may happen; notify user and skip this example.", "\n", "", "else", ":", "\n", "# Add to list of entities that will be removed.", "\n", "                    ", "to_remove", ".", "append", "(", "entity", ")", "\n", "msg", "=", "f'Entity \"{entity.text_string}\" crosses sentence boundary. Skipping.'", "\n", "print", "(", "msg", ")", "\n", "continue", "\n", "", "", "if", "in_range", ":", "\n", "                ", "debug_if", "(", "entity", "in", "self", ".", "_seen_so_far", "[", "'entity'", "]", ")", "\n", "self", ".", "_seen_so_far", "[", "\"entity\"", "]", ".", "append", "(", "entity", ")", "\n", "entities", ".", "append", "(", "entity", ")", "\n", "# If doing full entity extents, remove entities that crossed sentence boundaries.", "\n", "", "", "for", "failure", "in", "to_remove", ":", "\n", "            ", "self", ".", "entity_list", ".", "remove", "(", "failure", ")", "\n", "\n", "", "return", "entities", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._sentence_get_relations": [[666, 684], ["all", "all", "parse_ace_event.Document._sentence_get_relations.in_range"], "methods", ["None"], ["", "def", "_sentence_get_relations", "(", "self", ",", "sent", ")", ":", "\n", "        ", "def", "in_range", "(", "candidate", ")", ":", "\n", "            ", "each_one", "=", "[", "self", ".", "_check_in_range", "(", "entry", ",", "sent", ")", "for", "entry", "in", "[", "candidate", ".", "arg1", ",", "candidate", ".", "arg2", "]", "]", "\n", "if", "all", "(", "each_one", ")", ":", "\n", "                ", "debug_if", "(", "candidate", "in", "self", ".", "_seen_so_far", "[", "'relation'", "]", ")", "\n", "return", "True", "\n", "", "if", "all", "(", "[", "not", "entry", "for", "entry", "in", "each_one", "]", ")", ":", "\n", "                ", "return", "False", "\n", "", "else", ":", "\n", "                ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "\n", "", "", "relations", "=", "[", "]", "\n", "for", "relation", "in", "self", ".", "relation_list", ":", "\n", "# This is an annotation mistake and crosses sentence boundaries. Just ignore it.", "\n", "            ", "if", "in_range", "(", "relation", ")", ":", "\n", "                ", "self", ".", "_seen_so_far", "[", "\"relation\"", "]", ".", "append", "(", "relation", ")", "\n", "relations", ".", "append", "(", "relation", ")", "\n", "", "", "return", "relations", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._sentence_get_events": [[685, 706], ["all", "all", "parse_ace_event.Document._sentence_get_relations.in_range"], "methods", ["None"], ["", "def", "_sentence_get_events", "(", "self", ",", "sent", ")", ":", "\n", "        ", "def", "in_range", "(", "candidate", ")", ":", "\n", "            ", "each_one", "=", "(", "[", "self", ".", "_check_in_range", "(", "candidate", ".", "trigger", ",", "sent", ")", "]", "+", "\n", "[", "self", ".", "_check_in_range", "(", "entry", ",", "sent", ")", "for", "entry", "in", "candidate", ".", "arguments", "]", ")", "\n", "if", "all", "(", "each_one", ")", ":", "\n", "                ", "debug_if", "(", "candidate", "in", "self", ".", "_seen_so_far", "[", "'event'", "]", ")", "\n", "return", "True", "\n", "", "if", "all", "(", "[", "not", "entry", "for", "entry", "in", "each_one", "]", ")", ":", "\n", "                ", "return", "False", "\n", "", "else", ":", "\n", "                ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "\n", "", "", "events", "=", "[", "]", "\n", "for", "event", "in", "self", ".", "event_list", ":", "\n", "# Event that crosses sentence.", "\n", "            ", "if", "in_range", "(", "event", ")", ":", "\n", "                ", "self", ".", "_seen_so_far", "[", "\"event\"", "]", ".", "append", "(", "event", ")", "\n", "trigger_span", "=", "get_token_indices", "(", "event", ".", "trigger", ",", "sent", ")", "\n", "debug_if", "(", "trigger_span", "[", "0", "]", "!=", "trigger_span", "[", "1", "]", ")", "\n", "events", ".", "append", "(", "event", ")", "\n", "", "", "return", "events", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_entry": [[707, 713], ["parse_ace_event.Document._sentence_get_ner", "parse_ace_event.Document._sentence_get_relations", "parse_ace_event.Document._sentence_get_events", "parse_ace_event.Entry"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._sentence_get_ner", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._sentence_get_relations", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._sentence_get_events"], ["", "def", "_get_entry", "(", "self", ",", "sent", ")", ":", "\n", "        ", "toks", "=", "[", "tok", "for", "tok", "in", "sent", "]", "\n", "ner", "=", "self", ".", "_sentence_get_ner", "(", "sent", ")", "\n", "rel", "=", "self", ".", "_sentence_get_relations", "(", "sent", ")", "\n", "events", "=", "self", ".", "_sentence_get_events", "(", "sent", ")", "\n", "return", "Entry", "(", "sent", "=", "sent", ",", "entities", "=", "ner", ",", "relations", "=", "rel", ",", "events", "=", "events", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._check_all_seen": [[714, 718], ["len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_check_all_seen", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "_seen_so_far", "[", "\"entity\"", "]", ")", "==", "len", "(", "self", ".", "entity_list", ")", "\n", "assert", "len", "(", "self", ".", "_seen_so_far", "[", "\"relation\"", "]", ")", "==", "len", "(", "self", ".", "relation_list", ")", "\n", "assert", "len", "(", "self", ".", "_seen_so_far", "[", "\"event\"", "]", ")", "==", "len", "(", "self", ".", "event_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json": [[719, 726], ["dict", "parse_ace_event.Doc", "parse_ace_event.Document._check_all_seen", "parse_ace_event.Doc.to_json", "parse_ace_event.Document._get_entry"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._check_all_seen", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document._get_entry"], ["", "def", "to_json", "(", "self", ")", ":", "\n", "        ", "self", ".", "_seen_so_far", "=", "dict", "(", "entity", "=", "[", "]", ",", "relation", "=", "[", "]", ",", "event", "=", "[", "]", ")", "\n", "entries", "=", "[", "self", ".", "_get_entry", "(", "sent", ")", "for", "sent", "in", "self", ".", "doc", ".", "sents", "]", "\n", "doc", "=", "Doc", "(", "entries", ",", "self", ".", "_doc_key", ")", "\n", "self", ".", "_check_all_seen", "(", ")", "\n", "js", "=", "doc", ".", "to_json", "(", ")", "\n", "return", "js", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.in_between": [[30, 33], ["None"], "function", ["None"], ["", "def", "in_between", "(", "ix", ",", "pair", ")", ":", "\n", "    ", "assert", "ix", "!=", "pair", "[", "0", "]", "and", "ix", "!=", "pair", "[", "1", "]", "\n", "return", "ix", ">", "pair", "[", "0", "]", "and", "ix", "<", "pair", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.debug_if": [[239, 242], ["ipdb.set_trace"], "function", ["None"], ["", "", "def", "debug_if", "(", "cond", ")", ":", "\n", "    ", "if", "cond", ":", "\n", "        ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_indices": [[244, 254], ["parse_ace_event.debug_if", "parse_ace_event.debug_if", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.debug_if", "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.debug_if"], ["", "", "def", "get_token_indices", "(", "entity", ",", "sent", ")", ":", "\n", "    ", "start_token", "=", "[", "tok", "for", "tok", "in", "sent", "if", "tok", ".", "idx", "==", "entity", ".", "start_char", "]", "\n", "debug_if", "(", "len", "(", "start_token", ")", "!=", "1", ")", "\n", "start_token", "=", "start_token", "[", "0", "]", "\n", "end_token", "=", "[", "tok", "for", "tok", "in", "sent", "if", "tok", ".", "idx", "+", "len", "(", "tok", ")", "-", "1", "==", "entity", ".", "end_char", "]", "\n", "debug_if", "(", "len", "(", "end_token", ")", "!=", "1", ")", "\n", "end_token", "=", "end_token", "[", "0", "]", "\n", "start_ix", "=", "start_token", ".", "i", "\n", "end_ix", "=", "end_token", ".", "i", "\n", "return", "start_ix", ",", "end_ix", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.get_token_of": [[256, 262], ["Exception", "len"], "function", ["None"], ["", "def", "get_token_of", "(", "doc", ",", "char", ")", ":", "\n", "    ", "'Given a document and a character in the document, get the token that the char lives in.'", "\n", "for", "tok", "in", "doc", ":", "\n", "        ", "if", "char", ">=", "tok", ".", "idx", "and", "char", "<", "tok", ".", "idx", "+", "len", "(", "tok", ")", ":", "\n", "            ", "return", "doc", "[", "tok", ".", "i", "]", "\n", "", "", "raise", "Exception", "(", "'Should not get here.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.one_fold": [[733, 750], ["open", "open", "os.path.join", "doc_keys.append", "os.path.join", "os.path.join", "os.path.join", "parse_ace_event.Document", "parse_ace_event.Document.to_json", "g.write", "line.strip", "json.dumps"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.Document.to_json"], ["", "", "def", "one_fold", "(", "fold", ",", "output_dir", ",", "heads_only", "=", "True", ",", "real_entities_only", "=", "True", ",", "include_pronouns", "=", "False", ")", ":", "\n", "    ", "doc_path", "=", "\"./data/ace-event/raw-data\"", "\n", "split_path", "=", "\"./scripts/data/ace-event/event-split\"", "\n", "\n", "doc_keys", "=", "[", "]", "\n", "with", "open", "(", "path", ".", "join", "(", "split_path", ",", "fold", "+", "\".filelist\"", ")", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "doc_keys", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "with", "open", "(", "path", ".", "join", "(", "output_dir", ",", "fold", "+", "\".json\"", ")", ",", "\"w\"", ")", "as", "g", ":", "\n", "        ", "for", "doc_key", "in", "doc_keys", ":", "\n", "            ", "annotation_path", "=", "path", ".", "join", "(", "doc_path", ",", "doc_key", "+", "\".apf.xml\"", ")", "\n", "text_path", "=", "path", ".", "join", "(", "doc_path", ",", "doc_key", "+", "\".sgm\"", ")", "\n", "document", "=", "Document", "(", "annotation_path", ",", "text_path", ",", "doc_key", ",", "fold", ",", "heads_only", ",", "\n", "real_entities_only", ",", "include_pronouns", ")", "\n", "js", "=", "document", ".", "to_json", "(", ")", "\n", "g", ".", "write", "(", "json", ".", "dumps", "(", "js", ",", "default", "=", "int", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.main": [[752, 774], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.makedirs", "print", "parse_ace_event.one_fold"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.ace-event.parse_ace_event.one_fold"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Preprocess ACE event data.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"output_name\"", ",", "help", "=", "\"Name for output directory.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_span_extent\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Use full extent of entity mentions instead of just heads.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--include_times_and_values\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Treat times and values as entities and include them as event arguments.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--include_pronouns\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Include pronouns as entities and include them as event arguments.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "output_dir", "=", "f\"./data/ace-event/processed-data/{args.output_name}/json\"", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "for", "fold", "in", "[", "\"train\"", ",", "\"dev\"", ",", "\"test\"", "]", ":", "\n", "        ", "msg", "=", "f\"Parsing {fold} set.\"", "\n", "print", "(", "msg", ")", "\n", "one_fold", "(", "fold", ",", "\n", "output_dir", ",", "\n", "heads_only", "=", "(", "not", "args", ".", "use_span_extent", ")", ",", "\n", "real_entities_only", "=", "(", "not", "args", ".", "include_times_and_values", ")", ",", "\n", "include_pronouns", "=", "args", ".", "include_pronouns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.AceExample.__init__": [[39, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentence", ",", "events", ",", "s_start", ")", ":", "\n", "        ", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "events", "=", "events", "\n", "self", ".", "s_start", "=", "s_start", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.AceExample.__str__": [[44, 46], ["run_args_qa.AceExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.AceExample.__repr__": [[47, 59], ["event_triggers.append", "event_triggers.append", "event_triggers.append", "event_triggers.append", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"event sentence: %s\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "sentence", ")", ")", "\n", "event_triggers", "=", "[", "]", "\n", "for", "event", "in", "self", ".", "events", ":", "\n", "            ", "if", "event", ":", "\n", "                ", "event_triggers", ".", "append", "(", "self", ".", "sentence", "[", "event", "[", "0", "]", "[", "0", "]", "-", "self", ".", "s_start", "]", ")", "\n", "event_triggers", ".", "append", "(", "event", "[", "0", "]", "[", "1", "]", ")", "\n", "event_triggers", ".", "append", "(", "str", "(", "event", "[", "0", "]", "[", "0", "]", "-", "self", ".", "s_start", ")", ")", "\n", "event_triggers", ".", "append", "(", "\"|\"", ")", "\n", "", "", "s", "+=", "\" ||| event triggers: %s\"", "%", "(", "\" \"", ".", "join", "(", "event_triggers", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.InputFeatures.__init__": [[64, 84], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "example_id", ",", "tokens", ",", "token_to_orig_map", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "\n", "#", "\n", "event_type", ",", "argument_type", ",", "fea_trigger_offset", ",", "\n", "#", "\n", "start_position", "=", "None", ",", "end_position", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "example_id", "=", "example_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "if_trigger_ids", "=", "if_trigger_ids", "\n", "\n", "self", ".", "event_type", "=", "event_type", "\n", "self", ".", "argument_type", "=", "argument_type", "\n", "self", ".", "fea_trigger_offset", "=", "fea_trigger_offset", "\n", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.read_ace_examples": [[86, 97], ["io.open", "json.loads", "run_args_qa.AceExample", "examples.append"], "function", ["None"], ["", "", "def", "read_ace_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Read a ACE json file into a list of AceExample.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "sentence", ",", "events", ",", "s_start", "=", "example", "[", "\"sentence\"", "]", ",", "example", "[", "\"event\"", "]", ",", "example", "[", "\"s_start\"", "]", "\n", "example", "=", "AceExample", "(", "sentence", "=", "sentence", ",", "events", "=", "events", ",", "s_start", "=", "s_start", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.convert_examples_to_features": [[99, 179], ["enumerate", "query.replace.replace", "tokens.append", "segment_ids.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "enumerate", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "tokens.append", "segment_ids.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "features.append", "features.append", "run_args_qa.InputFeatures", "len", "features.append", "run_args_qa.InputFeatures", "run_args_qa.InputFeatures"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "query_templates", ",", "nth_query", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "for", "event", "in", "example", ".", "events", ":", "\n", "            ", "trigger_offset", "=", "event", "[", "0", "]", "[", "0", "]", "-", "example", ".", "s_start", "\n", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "trigger_token", "=", "example", ".", "sentence", "[", "trigger_offset", "]", "\n", "arguments", "=", "event", "[", "1", ":", "]", "\n", "for", "argument_type", "in", "query_templates", "[", "event_type", "]", ":", "\n", "\n", "                ", "query", "=", "query_templates", "[", "event_type", "]", "[", "argument_type", "]", "[", "nth_query", "]", "\n", "query", "=", "query", ".", "replace", "(", "\"[trigger]\"", ",", "trigger_token", ")", "\n", "\n", "# prepare [CLS] query [SEP] sentence [SEP]", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "# add [CLS]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add query", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "query", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add sentence", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "sentence", ")", ":", "\n", "                    ", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "i", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "tokens", ".", "append", "(", "sub_tokens", "[", "0", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# transform to input_ids ...", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "                    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "# start & end position", "\n", "", "start_position", ",", "end_position", "=", "None", ",", "None", "\n", "\n", "sentence_start", "=", "example", ".", "s_start", "\n", "sentence_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "fea_trigger_offset", "=", "trigger_offset", "+", "sentence_offset", "\n", "\n", "if_trigger_ids", "=", "[", "0", "]", "*", "len", "(", "segment_ids", ")", "\n", "if_trigger_ids", "[", "fea_trigger_offset", "]", "=", "1", "\n", "\n", "if", "is_training", ":", "\n", "                    ", "no_answer", "=", "True", "\n", "for", "argument", "in", "arguments", ":", "\n", "                        ", "gold_argument_type", "=", "argument", "[", "2", "]", "\n", "if", "gold_argument_type", "==", "argument_type", ":", "\n", "                            ", "no_answer", "=", "False", "\n", "answer_start", ",", "answer_end", "=", "argument", "[", "0", "]", ",", "argument", "[", "1", "]", "\n", "\n", "start_position", "=", "answer_start", "-", "sentence_start", "+", "sentence_offset", "\n", "end_position", "=", "answer_end", "-", "sentence_start", "+", "sentence_offset", "\n", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "if", "no_answer", ":", "\n", "                        ", "start_position", ",", "end_position", "=", "0", ",", "0", "\n", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.read_query_templates": [[181, 217], ["dict", "io.open", "io.open", "line.strip().split", "event_arg.split", "[].append", "[].append", "[].append", "[].append", "line.strip().split", "event_arg.split", "[].append", "[].append", "dict", "list", "len", "line.strip", "line.strip"], "function", ["None"], ["", "def", "read_query_templates", "(", "normal_file", ",", "des_file", ")", ":", "\n", "    ", "\"\"\"Load query templates\"\"\"", "\n", "query_templates", "=", "dict", "(", ")", "\n", "with", "open", "(", "normal_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "event_arg", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "event_type", ",", "arg_name", "=", "event_arg", ".", "split", "(", "\"_\"", ")", "\n", "\n", "if", "event_type", "not", "in", "query_templates", ":", "\n", "                ", "query_templates", "[", "event_type", "]", "=", "dict", "(", ")", "\n", "", "if", "arg_name", "not", "in", "query_templates", "[", "event_type", "]", ":", "\n", "                ", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", "=", "list", "(", ")", "\n", "\n", "# 0 template arg_name", "\n", "", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "arg_name", ")", "\n", "# 1 template arg_name + in trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "arg_name", "+", "\" in [trigger]\"", ")", "\n", "# 2 template arg_query", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", ")", "\n", "# 3 arg_query + trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", "[", ":", "-", "1", "]", "+", "\" in [trigger]?\"", ")", "\n", "\n", "", "", "with", "open", "(", "des_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "event_arg", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "event_type", ",", "arg_name", "=", "event_arg", ".", "split", "(", "\"_\"", ")", "\n", "# 4 template des_query", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", ")", "\n", "# 5 template des_query + trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", "[", ":", "-", "1", "]", "+", "\" in [trigger]?\"", ")", "\n", "\n", "", "", "for", "event_type", "in", "query_templates", ":", "\n", "        ", "for", "arg_name", "in", "query_templates", "[", "event_type", "]", ":", "\n", "            ", "assert", "len", "(", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ")", "==", "6", "\n", "\n", "", "", "return", "query_templates", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.make_predictions": [[221, 289], ["collections.defaultdict", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_id_to_features[].append", "example_id_to_results[].append", "collections.OrderedDict", "enumerate", "str", "run_args_qa._get_best_indexes", "run_args_qa._get_best_indexes", "sorted", "enumerate", "sorted.append", "final_all_predictions[].append", "sorted.append", "collections.namedtuple.", "collections.namedtuple.", "len", "len"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes"], ["def", "make_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "larger_than_cls", ")", ":", "\n", "    ", "example_id_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_id_to_features", "[", "feature", ".", "example_id", "]", ".", "append", "(", "feature", ")", "\n", "", "example_id_to_results", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "example_id_to_results", "[", "result", ".", "example_id", "]", ".", "append", "(", "result", ")", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "\"PrelimPrediction\"", ",", "\n", "[", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "final_all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "# all_nbest_json = collections.OrderedDict()", "\n", "# scores_diff_json = collections.OrderedDict()", "\n", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_id_to_features", "[", "example_id", "]", "\n", "results", "=", "example_id_to_results", "[", "example_id", "]", "\n", "all_predictions", "[", "example_id", "]", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "final_all_predictions", "[", "example_id", "]", "=", "[", "]", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "event_type_argument_type", "=", "\"_\"", ".", "join", "(", "[", "feature", ".", "event_type", ",", "feature", ".", "argument_type", "]", ")", "\n", "event_type_offset_argument_type", "=", "\"_\"", ".", "join", "(", "[", "feature", ".", "event_type", ",", "str", "(", "feature", ".", "token_to_orig_map", "[", "feature", ".", "fea_trigger_offset", "]", ")", ",", "feature", ".", "argument_type", "]", ")", "\n", "\n", "start_indexes", ",", "end_indexes", "=", "None", ",", "None", "\n", "prelim_predictions", "=", "[", "]", "\n", "for", "result", "in", "results", ":", "\n", "                ", "if", "result", ".", "event_type_offset_argument_type", "==", "event_type_offset_argument_type", ":", "\n", "                    ", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ",", "larger_than_cls", ",", "result", ".", "start_logits", "[", "0", "]", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ",", "larger_than_cls", ",", "result", ".", "end_logits", "[", "0", "]", ")", "\n", "# add span preds", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                        ", "for", "end_index", "in", "end_indexes", ":", "\n", "                            ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", "or", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                                ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", "or", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                                ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                                ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                                ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "start_index", "=", "start_index", ",", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "## add null pred", "\n", "", "", "if", "not", "larger_than_cls", ":", "\n", "                        ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "0", "]", "\n", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "start_index", "=", "0", ",", "end_index", "=", "0", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "0", "]", ",", "end_logit", "=", "result", ".", "end_logits", "[", "0", "]", ")", ")", "\n", "\n", "## sort", "\n", "", "prelim_predictions", "=", "sorted", "(", "prelim_predictions", ",", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "# all_predictions[example_id][event_type_offset_argument_type] = prelim_predictions", "\n", "\n", "## get final pred in format: [event_type_offset_argument_type, [start_offset, end_offset]]", "\n", "max_num_pred_per_arg", "=", "2", "\n", "for", "idx", ",", "pred", "in", "enumerate", "(", "prelim_predictions", ")", ":", "\n", "                        ", "if", "(", "idx", "+", "1", ")", ">", "max_num_pred_per_arg", ":", "break", "\n", "if", "pred", ".", "start_index", "==", "0", "and", "pred", ".", "end_index", "==", "0", ":", "break", "\n", "orig_sent_start", ",", "orig_sent_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", ",", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "final_all_predictions", "[", "example_id", "]", ".", "append", "(", "[", "event_type_argument_type", ",", "[", "orig_sent_start", ",", "orig_sent_end", "]", "]", ")", "\n", "\n", "", "", "", "", "", "return", "final_all_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa._get_best_indexes": [[291, 304], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", "=", "1", ",", "larger_than_cls", "=", "False", ",", "cls_logit", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "if", "larger_than_cls", ":", "\n", "            ", "if", "index_and_score", "[", "i", "]", "[", "1", "]", "<", "cls_logit", ":", "\n", "                ", "break", "\n", "", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.evaluate": [[305, 428], ["model.eval", "enumerate", "run_args_qa.make_predictions", "copy.deepcopy", "collections.OrderedDict", "enumerate", "enumerate", "enumerate", "enumerate", "collections.OrderedDict", "collections.OrderedDict", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "if_trigger_ids.to.to", "enumerate", "logger.info", "torch.no_grad", "batch_start_logits[].detach().cpu().tolist", "batch_end_logits[].detach().cpu().tolist", "all_results.append", "model", "model", "RawResult", "all_gold[].append", "argument[].split", "argument[].split", "batch_start_logits[].detach().cpu", "batch_end_logits[].detach().cpu", "example_index.item", "str", "len", "batch_start_logits[].detach", "batch_end_logits[].detach"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.make_predictions"], ["", "def", "evaluate", "(", "args", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ",", "na_prob_thresh", "=", "1.0", ",", "pred_only", "=", "False", ")", ":", "\n", "    ", "all_results", "=", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "for", "idx", ",", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "example_indices", ")", "in", "enumerate", "(", "eval_dataloader", ")", ":", "\n", "        ", "if", "pred_only", "and", "idx", "%", "10", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Running test: %d / %d\"", "%", "(", "idx", ",", "len", "(", "eval_dataloader", ")", ")", ")", "\n", "", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "if_trigger_ids", "=", "if_trigger_ids", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                ", "batch_start_logits", ",", "batch_end_logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "batch_start_logits", ",", "batch_end_logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "if_trigger_ids", ",", "input_mask", ")", "\n", "", "", "for", "i", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "            ", "start_logits", "=", "batch_start_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "end_logits", "=", "batch_end_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "eval_feature", "=", "eval_features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "example_id", "=", "eval_feature", ".", "example_id", "\n", "event_type_offset_argument_type", "=", "\"_\"", ".", "join", "(", "[", "eval_feature", ".", "event_type", ",", "str", "(", "eval_feature", ".", "token_to_orig_map", "[", "eval_feature", ".", "fea_trigger_offset", "]", ")", ",", "eval_feature", ".", "argument_type", "]", ")", "\n", "all_results", ".", "append", "(", "RawResult", "(", "example_id", "=", "example_id", ",", "event_type_offset_argument_type", "=", "event_type_offset_argument_type", ",", "\n", "start_logits", "=", "start_logits", ",", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "# preds, nbest_preds, na_probs = \\", "\n", "", "", "preds", "=", "make_predictions", "(", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "args", ".", "larger_than_cls", ")", "\n", "preds_init", "=", "copy", ".", "deepcopy", "(", "preds", ")", "\n", "\n", "# get all_gold in format: [event_type_argument_type, [start_offset, end_offset]]", "\n", "all_gold", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "all_gold", "[", "example_id", "]", "=", "[", "]", "\n", "for", "event", "in", "example", ".", "events", ":", "\n", "# if not event: continue", "\n", "            ", "trigger_offset", "=", "event", "[", "0", "]", "[", "0", "]", "-", "example", ".", "s_start", "\n", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "for", "argument", "in", "event", "[", "1", ":", "]", ":", "\n", "                ", "argument_start", ",", "argument_end", ",", "argument_type", "=", "argument", "[", "0", "]", "-", "example", ".", "s_start", ",", "argument", "[", "1", "]", "-", "example", ".", "s_start", ",", "argument", "[", "2", "]", "\n", "# event_type_offset_argument_type = \"_\".join([event_type, str(trigger_offset), argument_type])", "\n", "event_type_argument_type", "=", "\"_\"", ".", "join", "(", "[", "event_type", ",", "argument_type", "]", ")", "\n", "all_gold", "[", "example_id", "]", ".", "append", "(", "[", "event_type_argument_type", ",", "[", "argument_start", ",", "argument_end", "]", "]", ")", "\n", "\n", "# get results (classification)", "\n", "", "", "", "gold_arg_n", ",", "pred_arg_n", ",", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "pred_arg", "=", "preds", "[", "example_id", "]", "\n", "gold_arg", "=", "all_gold", "[", "example_id", "]", "\n", "# pred_arg_n", "\n", "for", "argument", "in", "pred_arg", ":", "pred_arg_n", "+=", "1", "\n", "# gold_arg_n     ", "\n", "for", "argument", "in", "gold_arg", ":", "gold_arg_n", "+=", "1", "\n", "# pred_in_gold_n", "\n", "for", "argument", "in", "pred_arg", ":", "\n", "            ", "if", "argument", "in", "gold_arg", ":", "\n", "                ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argument", "in", "gold_arg", ":", "\n", "            ", "if", "argument", "in", "pred_arg", ":", "\n", "                ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "", "prec_c", ",", "recall_c", ",", "f1_c", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_c", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_c", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_c", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_c", "=", "0", "\n", "if", "prec_c", "or", "recall_c", ":", "f1_c", "=", "2", "*", "prec_c", "*", "recall_c", "/", "(", "prec_c", "+", "recall_c", ")", "\n", "else", ":", "f1_c", "=", "0", "\n", "\n", "\n", "# get results (identification)", "\n", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "for", "argument", "in", "preds", "[", "example_id", "]", ":", "\n", "                ", "argument", "[", "0", "]", "=", "argument", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "# only event_type", "\n", "", "for", "argument", "in", "all_gold", "[", "example_id", "]", ":", "\n", "                ", "argument", "[", "0", "]", "=", "argument", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "# only event_type", "\n", "\n", "", "", "gold_arg_n", ",", "pred_arg_n", ",", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "pred_arg", "=", "preds", "[", "example_id", "]", "\n", "gold_arg", "=", "all_gold", "[", "example_id", "]", "\n", "# pred_arg_n", "\n", "for", "argument", "in", "pred_arg", ":", "pred_arg_n", "+=", "1", "\n", "# gold_arg_n     ", "\n", "for", "argument", "in", "gold_arg", ":", "gold_arg_n", "+=", "1", "\n", "# pred_in_gold_n", "\n", "for", "argument", "in", "pred_arg", ":", "\n", "            ", "if", "argument", "in", "gold_arg", ":", "\n", "                ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argument", "in", "gold_arg", ":", "\n", "            ", "if", "argument", "in", "pred_arg", ":", "\n", "                ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "", "prec_i", ",", "recall_i", ",", "f1_i", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_i", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_i", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_i", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_i", "=", "0", "\n", "if", "prec_i", "or", "recall_i", ":", "f1_i", "=", "2", "*", "prec_i", "*", "recall_i", "/", "(", "prec_i", "+", "recall_i", ")", "\n", "else", ":", "f1_i", "=", "0", "\n", "\n", "# # logging for DEBUG results", "\n", "# for (example_id, example) in enumerate(gold_examples):", "\n", "#     if example_id > 20: break", "\n", "#     if preds[example_id] or all_gold[example_id]:", "\n", "#         token_idx = []", "\n", "#         for idx, token in enumerate(example.sentence): token_idx.append(\" \".join([token, str(idx)]))", "\n", "#         logger.info(\"sent: {}\".format(\" | \".join(token_idx)))", "\n", "\n", "#         gold_str_list = [] ", "\n", "#         for gold in all_gold[example_id]: gold_str_list.append(\" \".join([gold[0], str(gold[1][0]), str(gold[1][1])]))", "\n", "#         logger.info(\"gold: {}\".format(\" | \".join(gold_str_list)))", "\n", "\n", "#         pred_str_list = [] ", "\n", "#         for pred in preds[example_id]: pred_str_list.append(\" \".join([pred[0], str(pred[1][0]), str(pred[1][1])]))", "\n", "#         logger.info(\"pred: {}\".format(\" | \".join(pred_str_list)))", "\n", "\n", "#         logger.info(\"\\n\")", "\n", "\n", "result", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "result", "=", "collections", ".", "OrderedDict", "(", "[", "(", "'prec_c'", ",", "prec_c", ")", ",", "(", "'recall_c'", ",", "recall_c", ")", ",", "(", "'f1_c'", ",", "f1_c", ")", ",", "(", "'prec_i'", ",", "prec_i", ")", ",", "(", "'recall_i'", ",", "recall_i", ")", ",", "(", "'f1_i'", ",", "f1_i", ")", "]", ")", "\n", "return", "result", ",", "preds_init", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa.main": [[430, 662], ["torch.device", "torch.cuda.device_count", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "logger.info", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "run_args_qa.read_query_templates", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.exists", "os.makedirs", "logger.addHandler", "logger.addHandler", "run_args_qa.read_ace_examples", "run_args_qa.read_ace_examples", "run_args_qa.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "run_args_qa.read_ace_examples", "run_args_qa.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "max", "torch.nn.DataParallel.to", "run_args_qa.evaluate", "logging.FileHandler", "logging.FileHandler", "len", "len", "torch.tensor.size", "sorted", "random.shuffle", "len", "len", "torch.nn.DataParallel.to", "list", "pytorch_pretrained_bert.optimization.BertAdam", "time.time", "range", "run_args_qa.read_ace_examples", "run_args_qa.read_ace_examples", "run_args_qa.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering.from_pretrained", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.from_pretrained", "torch.nn.DataParallel.half", "io.open", "io.open", "torch.cuda.is_available", "os.path.join", "os.path.join", "len", "len", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering.from_pretrained", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.from_pretrained", "torch.nn.DataParallel.half", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel.train", "logger.info", "enumerate", "len", "len", "torch.tensor.size", "os.path.join", "writer.write", "os.path.join", "writer.write", "random.shuffle", "loss.mean.item", "input_ids.size", "loss.mean.backward", "numpy.sum", "tuple", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "loss.mean.mean", "pytorch_pretrained_bert.optimization.BertAdam.step", "pytorch_pretrained_bert.optimization.BertAdam.zero_grad", "json.dumps", "any", "run_args_qa.evaluate", "torch.nn.DataParallel.train", "os.path.join", "os.path.join", "os.path.join", "torch.save", "model_to_save.config.to_json_file", "BertTokenizer.from_pretrained.save_vocabulary", "str", "any", "t.to", "logger.info", "logger.info", "hasattr", "os.path.exists", "os.makedirs", "model_to_save.state_dict", "io.open", "sorted", "len", "int", "len", "os.path.join", "best_result.keys", "writer.write", "time.time", "str", "str"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_query_templates", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_file", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.save_vocabulary"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "logger", ".", "info", "(", "\"device: {}, n_gpu: {}, 16-bits training: {}\"", ".", "format", "(", "device", ",", "n_gpu", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "(", "args", ".", "train_file", "is", "not", "None", ")", "and", "(", "args", ".", "dev_file", "is", "not", "None", ")", "\n", "\n", "", "if", "args", ".", "eval_test", ":", "\n", "        ", "assert", "args", ".", "test_file", "is", "not", "None", "\n", "", "else", ":", "\n", "        ", "assert", "args", ".", "dev_file", "is", "not", "None", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train.log\"", ")", ",", "'w'", ")", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval.log\"", ")", ",", "'w'", ")", ")", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "# read query templates", "\n", "query_templates", "=", "read_query_templates", "(", "normal_file", "=", "args", ".", "normal_file", ",", "des_file", "=", "args", ".", "des_file", ")", "\n", "\n", "if", "args", ".", "do_train", "or", "(", "not", "args", ".", "eval_test", ")", ":", "\n", "        ", "eval_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "dev_file", ",", "is_training", "=", "False", ")", "\n", "gold_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "gold_file", ",", "is_training", "=", "False", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"***** Dev *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "all_example_index", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "train_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "True", ")", "\n", "\n", "if", "args", ".", "train_mode", "==", "'sorted'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "            ", "train_features", "=", "sorted", "(", "train_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "train_features", ")", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_start_positions", "=", "torch", ".", "tensor", "(", "[", "f", ".", "start_position", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_end_positions", "=", "torch", ".", "tensor", "(", "[", "f", ".", "end_position", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "\n", "all_start_positions", ",", "all_end_positions", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "train_batches", "=", "[", "batch", "for", "batch", "in", "train_dataloader", "]", "\n", "\n", "num_train_optimization_steps", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "logger", ".", "info", "(", "\"***** Train *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "train_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "eval_step", "=", "max", "(", "1", ",", "len", "(", "train_batches", ")", "//", "args", ".", "eval_per_epoch", ")", "\n", "best_result", "=", "None", "\n", "lrs", "=", "[", "args", ".", "learning_rate", "]", "if", "args", ".", "learning_rate", "else", "[", "1e-6", ",", "2e-6", ",", "3e-6", ",", "5e-6", ",", "1e-5", ",", "2e-5", ",", "3e-5", ",", "5e-5", "]", "\n", "for", "lr", "in", "lrs", ":", "\n", "            ", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                ", "model", "=", "BertForQuestionAnswering", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ")", "\n", "", "else", ":", "\n", "                ", "model", "=", "BertForQuestionAnswering_withIfTriggerEmbedding", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "[", "n", "for", "n", "in", "param_optimizer", "if", "'pooler'", "not", "in", "n", "[", "0", "]", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "tr_loss", "=", "0", "\n", "nb_tr_examples", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "global_step", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "                ", "model", ".", "train", "(", ")", "\n", "logger", ".", "info", "(", "\"Start epoch #{} (lr = {})...\"", ".", "format", "(", "epoch", ",", "lr", ")", ")", "\n", "if", "args", ".", "train_mode", "==", "'random'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "                    ", "random", ".", "shuffle", "(", "train_batches", ")", "\n", "", "for", "step", ",", "batch", "in", "enumerate", "(", "train_batches", ")", ":", "\n", "                    ", "if", "n_gpu", "==", "1", ":", "\n", "                        ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "start_positions", ",", "end_positions", "=", "batch", "\n", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                        ", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "if_trigger_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                        ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "if", "(", "step", "+", "1", ")", "%", "eval_step", "==", "0", "or", "step", "==", "0", ":", "\n", "                        ", "save_model", "=", "False", "\n", "if", "args", ".", "do_eval", ":", "\n", "# result, _, _ = evaluate(args, model, device, eval_dataset, eval_dataloader, eval_examples, eval_features)", "\n", "                            ", "result", ",", "preds", "=", "evaluate", "(", "args", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ")", "\n", "# import ipdb; ipdb.set_trace()", "\n", "model", ".", "train", "(", ")", "\n", "result", "[", "'global_step'", "]", "=", "global_step", "\n", "result", "[", "'epoch'", "]", "=", "epoch", "\n", "result", "[", "'learning_rate'", "]", "=", "lr", "\n", "result", "[", "'batch_size'", "]", "=", "args", ".", "train_batch_size", "\n", "if", "(", "best_result", "is", "None", ")", "or", "(", "result", "[", "args", ".", "eval_metric", "]", ">", "best_result", "[", "args", ".", "eval_metric", "]", ")", ":", "\n", "                                ", "best_result", "=", "result", "\n", "save_model", "=", "True", "\n", "logger", ".", "info", "(", "'Epoch: {}, Step: {} / {}, used_time = {:.2f}s, loss = {:.6f}'", ".", "format", "(", "\n", "epoch", ",", "step", "+", "1", ",", "len", "(", "train_batches", ")", ",", "time", ".", "time", "(", ")", "-", "start_time", ",", "tr_loss", "/", "nb_tr_steps", ")", ")", "\n", "# logger.info(\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f\" %", "\n", "# (args.eval_metric, str(lr), epoch, result[\"prec_c\"], result[\"recall_c\"], result[\"f1_c\"]))", "\n", "logger", ".", "info", "(", "\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f, p_i: %.2f, r_i: %.2f, f1_i: %.2f\"", "%", "\n", "(", "args", ".", "eval_metric", ",", "str", "(", "lr", ")", ",", "epoch", ",", "result", "[", "\"prec_c\"", "]", ",", "result", "[", "\"recall_c\"", "]", ",", "result", "[", "\"f1_c\"", "]", ",", "result", "[", "\"prec_i\"", "]", ",", "result", "[", "\"recall_i\"", "]", ",", "result", "[", "\"f1_i\"", "]", ")", ")", "\n", "", "", "else", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "if", "(", "int", "(", "args", ".", "num_train_epochs", ")", "-", "epoch", "<", "3", "and", "(", "step", "+", "1", ")", "/", "len", "(", "train_batches", ")", ">", "0.7", ")", "or", "step", "==", "0", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "else", ":", "\n", "                            ", "save_model", "=", "False", "\n", "", "if", "save_model", ":", "\n", "                            ", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "subdir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch{epoch}-step{step}\"", ".", "format", "(", "epoch", "=", "epoch", ",", "step", "=", "step", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "subdir", ")", ":", "\n", "                                ", "os", ".", "makedirs", "(", "subdir", ")", "\n", "", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "subdir", ",", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "subdir", ",", "CONFIG_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "model_to_save", ".", "config", ".", "to_json_file", "(", "output_config_file", ")", "\n", "tokenizer", ".", "save_vocabulary", "(", "subdir", ")", "\n", "if", "best_result", ":", "\n", "                                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                                    ", "for", "key", "in", "sorted", "(", "best_result", ".", "keys", "(", ")", ")", ":", "\n", "                                        ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "best_result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "", "", "", "", "", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "if", "args", ".", "eval_test", ":", "\n", "            ", "eval_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "test_file", ",", "is_training", "=", "False", ")", "\n", "gold_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "gold_file", ",", "is_training", "=", "False", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"***** Test *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "all_example_index", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "            ", "model", "=", "BertForQuestionAnswering", ".", "from_pretrained", "(", "args", ".", "model_dir", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "BertForQuestionAnswering_withIfTriggerEmbedding", ".", "from_pretrained", "(", "args", ".", "model_dir", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "result", ",", "preds", "=", "evaluate", "(", "args", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"test_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "key", "in", "result", ":", "\n", "                ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"arg_predictions.json\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "key", "in", "preds", ":", "\n", "                ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "preds", "[", "key", "]", ",", "default", "=", "int", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.AceExample.__init__": [[38, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentence", ",", "events", ",", "s_start", ")", ":", "\n", "        ", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "events", "=", "events", "\n", "self", ".", "s_start", "=", "s_start", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.AceExample.__str__": [[43, 45], ["run_args_qa_unseen.AceExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.AceExample.__repr__": [[46, 58], ["event_triggers.append", "event_triggers.append", "event_triggers.append", "event_triggers.append", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"event sentence: %s\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "sentence", ")", ")", "\n", "event_triggers", "=", "[", "]", "\n", "for", "event", "in", "self", ".", "events", ":", "\n", "            ", "if", "event", ":", "\n", "                ", "event_triggers", ".", "append", "(", "self", ".", "sentence", "[", "event", "[", "0", "]", "[", "0", "]", "-", "self", ".", "s_start", "]", ")", "\n", "event_triggers", ".", "append", "(", "event", "[", "0", "]", "[", "1", "]", ")", "\n", "event_triggers", ".", "append", "(", "str", "(", "event", "[", "0", "]", "[", "0", "]", "-", "self", ".", "s_start", ")", ")", "\n", "event_triggers", ".", "append", "(", "\"|\"", ")", "\n", "", "", "s", "+=", "\" ||| event triggers: %s\"", "%", "(", "\" \"", ".", "join", "(", "event_triggers", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.InputFeatures.__init__": [[63, 83], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "example_id", ",", "tokens", ",", "token_to_orig_map", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "\n", "#", "\n", "event_type", ",", "argument_type", ",", "fea_trigger_offset", ",", "\n", "#", "\n", "start_position", "=", "None", ",", "end_position", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "example_id", "=", "example_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "if_trigger_ids", "=", "if_trigger_ids", "\n", "\n", "self", ".", "event_type", "=", "event_type", "\n", "self", ".", "argument_type", "=", "argument_type", "\n", "self", ".", "fea_trigger_offset", "=", "fea_trigger_offset", "\n", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.read_ace_examples": [[85, 96], ["io.open", "json.loads", "run_args_qa_unseen.AceExample", "examples.append"], "function", ["None"], ["", "", "def", "read_ace_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Read a ACE json file into a list of AceExample.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "sentence", ",", "events", ",", "s_start", "=", "example", "[", "\"sentence\"", "]", ",", "example", "[", "\"event\"", "]", ",", "example", "[", "\"s_start\"", "]", "\n", "example", "=", "AceExample", "(", "sentence", "=", "sentence", ",", "events", "=", "events", ",", "s_start", "=", "s_start", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.convert_examples_to_features": [[98, 183], ["enumerate", "query.replace.replace", "tokens.append", "segment_ids.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "enumerate", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "tokens.append", "segment_ids.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "features.append", "features.append", "run_args_qa_unseen.InputFeatures", "run_args_qa_unseen.InputFeatures"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "query_templates", ",", "unseen_arguments", ",", "nth_query", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "for", "event", "in", "example", ".", "events", ":", "\n", "            ", "trigger_offset", "=", "event", "[", "0", "]", "[", "0", "]", "-", "example", ".", "s_start", "\n", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "trigger_token", "=", "example", ".", "sentence", "[", "trigger_offset", "]", "\n", "arguments", "=", "event", "[", "1", ":", "]", "\n", "for", "argument_type", "in", "query_templates", "[", "event_type", "]", ":", "\n", "\n", "                ", "query", "=", "query_templates", "[", "event_type", "]", "[", "argument_type", "]", "[", "nth_query", "]", "\n", "query", "=", "query", ".", "replace", "(", "\"[trigger]\"", ",", "trigger_token", ")", "\n", "\n", "# prepare [CLS] query [SEP] sentence [SEP]", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "# add [CLS]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add query", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "query", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add sentence", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "sentence", ")", ":", "\n", "                    ", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "i", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "tokens", ".", "append", "(", "sub_tokens", "[", "0", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# transform to input_ids ...", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "                    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "# start & end position", "\n", "", "start_position", ",", "end_position", "=", "None", ",", "None", "\n", "\n", "sentence_start", "=", "example", ".", "s_start", "\n", "sentence_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "fea_trigger_offset", "=", "trigger_offset", "+", "sentence_offset", "\n", "\n", "if_trigger_ids", "=", "[", "0", "]", "*", "len", "(", "segment_ids", ")", "\n", "if_trigger_ids", "[", "fea_trigger_offset", "]", "=", "1", "\n", "\n", "if", "is_training", ":", "\n", "                    ", "if", "argument_type", "in", "unseen_arguments", ":", "continue", "\n", "no_answer", "=", "True", "\n", "for", "argument", "in", "arguments", ":", "\n", "                        ", "gold_argument_type", "=", "argument", "[", "2", "]", "\n", "if", "gold_argument_type", "==", "argument_type", ":", "\n", "                            ", "no_answer", "=", "False", "\n", "answer_start", ",", "answer_end", "=", "argument", "[", "0", "]", ",", "argument", "[", "1", "]", "\n", "\n", "start_position", "=", "answer_start", "-", "sentence_start", "+", "sentence_offset", "\n", "end_position", "=", "answer_end", "-", "sentence_start", "+", "sentence_offset", "\n", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "# if no_answer:", "\n", "#     start_position, end_position = 0, 0", "\n", "#     features.append(InputFeatures(example_id=example_id, tokens=tokens, token_to_orig_map=token_to_orig_map, input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, if_trigger_ids=if_trigger_ids,", "\n", "#                                   event_type=event_type, argument_type=argument_type, fea_trigger_offset=fea_trigger_offset,", "\n", "#                                   start_position=start_position, end_position=end_position))", "\n", "", "", "", "else", ":", "\n", "                    ", "if", "argument_type", "not", "in", "unseen_arguments", ":", "continue", "\n", "for", "argument", "in", "arguments", ":", "\n", "                        ", "gold_argument_type", "=", "argument", "[", "2", "]", "\n", "if", "gold_argument_type", "==", "argument_type", ":", "\n", "                            ", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "", "", "", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.read_query_templates": [[185, 221], ["dict", "io.open", "io.open", "line.strip().split", "event_arg.split", "[].append", "[].append", "[].append", "[].append", "line.strip().split", "event_arg.split", "[].append", "[].append", "dict", "list", "len", "line.strip", "line.strip"], "function", ["None"], ["", "def", "read_query_templates", "(", "normal_file", ",", "des_file", ")", ":", "\n", "    ", "\"\"\"Load query templates\"\"\"", "\n", "query_templates", "=", "dict", "(", ")", "\n", "with", "open", "(", "normal_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "event_arg", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "event_type", ",", "arg_name", "=", "event_arg", ".", "split", "(", "\"_\"", ")", "\n", "\n", "if", "event_type", "not", "in", "query_templates", ":", "\n", "                ", "query_templates", "[", "event_type", "]", "=", "dict", "(", ")", "\n", "", "if", "arg_name", "not", "in", "query_templates", "[", "event_type", "]", ":", "\n", "                ", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", "=", "list", "(", ")", "\n", "\n", "# 0 template arg_name", "\n", "", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "arg_name", ")", "\n", "# 1 template arg_name + in trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "arg_name", "+", "\" in [trigger]\"", ")", "\n", "# 2 template arg_query", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", ")", "\n", "# 3 arg_query + trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", "[", ":", "-", "1", "]", "+", "\" in [trigger]?\"", ")", "\n", "\n", "", "", "with", "open", "(", "des_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "event_arg", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "event_type", ",", "arg_name", "=", "event_arg", ".", "split", "(", "\"_\"", ")", "\n", "# 4 template des_query", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", ")", "\n", "# 5 template des_query + trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", "[", ":", "-", "1", "]", "+", "\" in [trigger]?\"", ")", "\n", "\n", "", "", "for", "event_type", "in", "query_templates", ":", "\n", "        ", "for", "arg_name", "in", "query_templates", "[", "event_type", "]", ":", "\n", "            ", "assert", "len", "(", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ")", "==", "6", "\n", "\n", "", "", "return", "query_templates", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.read_unseen_arguments": [[222, 229], ["io.open", "unseen_arguments.append", "line.strip"], "function", ["None"], ["", "def", "read_unseen_arguments", "(", "unseen_arguments_file", ")", ":", "\n", "    ", "\"\"\"Load unseen arguments\"\"\"", "\n", "unseen_arguments", "=", "[", "]", "\n", "with", "open", "(", "unseen_arguments_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "unseen_arguments", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "return", "unseen_arguments", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.make_predictions": [[233, 301], ["collections.defaultdict", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_id_to_features[].append", "example_id_to_results[].append", "collections.OrderedDict", "enumerate", "str", "run_args_qa_unseen._get_best_indexes", "run_args_qa_unseen._get_best_indexes", "sorted", "enumerate", "sorted.append", "final_all_predictions[].append", "sorted.append", "collections.namedtuple.", "collections.namedtuple.", "len", "len"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes"], ["def", "make_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "larger_than_cls", ")", ":", "\n", "    ", "example_id_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_id_to_features", "[", "feature", ".", "example_id", "]", ".", "append", "(", "feature", ")", "\n", "", "example_id_to_results", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "example_id_to_results", "[", "result", ".", "example_id", "]", ".", "append", "(", "result", ")", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "\"PrelimPrediction\"", ",", "\n", "[", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "final_all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "# all_nbest_json = collections.OrderedDict()", "\n", "# scores_diff_json = collections.OrderedDict()", "\n", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_id_to_features", "[", "example_id", "]", "\n", "results", "=", "example_id_to_results", "[", "example_id", "]", "\n", "all_predictions", "[", "example_id", "]", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "final_all_predictions", "[", "example_id", "]", "=", "[", "]", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "event_type_argument_type", "=", "\"_\"", ".", "join", "(", "[", "feature", ".", "event_type", ",", "feature", ".", "argument_type", "]", ")", "\n", "event_type_offset_argument_type", "=", "\"_\"", ".", "join", "(", "[", "feature", ".", "event_type", ",", "str", "(", "feature", ".", "token_to_orig_map", "[", "feature", ".", "fea_trigger_offset", "]", ")", ",", "feature", ".", "argument_type", "]", ")", "\n", "\n", "start_indexes", ",", "end_indexes", "=", "None", ",", "None", "\n", "prelim_predictions", "=", "[", "]", "\n", "for", "result", "in", "results", ":", "\n", "                ", "if", "result", ".", "event_type_offset_argument_type", "==", "event_type_offset_argument_type", ":", "\n", "                    ", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ",", "larger_than_cls", ",", "result", ".", "start_logits", "[", "0", "]", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ",", "larger_than_cls", ",", "result", ".", "end_logits", "[", "0", "]", ")", "\n", "# add span preds", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                        ", "for", "end_index", "in", "end_indexes", ":", "\n", "                            ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", "or", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                                ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", "or", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                                ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                                ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                                ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "start_index", "=", "start_index", ",", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "## add null pred", "\n", "", "", "if", "not", "larger_than_cls", ":", "\n", "                        ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "0", "]", "\n", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "start_index", "=", "0", ",", "end_index", "=", "0", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "0", "]", ",", "end_logit", "=", "result", ".", "end_logits", "[", "0", "]", ")", ")", "\n", "\n", "## sort", "\n", "", "prelim_predictions", "=", "sorted", "(", "prelim_predictions", ",", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "# all_predictions[example_id][event_type_offset_argument_type] = prelim_predictions", "\n", "\n", "## get final pred in format: [event_type_offset_argument_type, [start_offset, end_offset]]", "\n", "max_num_pred_per_arg", "=", "1", "\n", "for", "idx", ",", "pred", "in", "enumerate", "(", "prelim_predictions", ")", ":", "\n", "                        ", "if", "(", "idx", "+", "1", ")", ">", "max_num_pred_per_arg", ":", "break", "\n", "if", "pred", ".", "start_index", "==", "0", "and", "pred", ".", "end_index", "==", "0", ":", "break", "\n", "orig_sent_start", ",", "orig_sent_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", ",", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "final_all_predictions", "[", "example_id", "]", ".", "append", "(", "[", "event_type_argument_type", ",", "[", "orig_sent_start", ",", "orig_sent_end", "]", "]", ")", "\n", "\n", "", "", "", "", "", "return", "final_all_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen._get_best_indexes": [[303, 316], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", "=", "1", ",", "larger_than_cls", "=", "False", ",", "cls_logit", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "if", "larger_than_cls", ":", "\n", "            ", "if", "index_and_score", "[", "i", "]", "[", "1", "]", "<", "cls_logit", ":", "\n", "                ", "break", "\n", "", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.evaluate": [[317, 442], ["model.eval", "enumerate", "run_args_qa_unseen.make_predictions", "collections.OrderedDict", "enumerate", "enumerate", "enumerate", "enumerate", "collections.OrderedDict", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "if_trigger_ids.to.to", "enumerate", "enumerate", "logger.info", "torch.no_grad", "batch_start_logits[].detach().cpu().tolist", "batch_end_logits[].detach().cpu().tolist", "all_results.append", "model", "model", "RawResult", "enumerate", "logger.info", "logger.info", "logger.info", "logger.info", "argument[].split", "argument[].split", "batch_start_logits[].detach().cpu", "batch_end_logits[].detach().cpu", "example_index.item", "str", "all_gold[].append", "token_idx.append", "gold_str_list.append", "pred_str_list.append", "len", "batch_start_logits[].detach", "batch_end_logits[].detach", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.make_predictions"], ["", "def", "evaluate", "(", "args", ",", "unseen_arguments", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ",", "na_prob_thresh", "=", "1.0", ",", "pred_only", "=", "False", ")", ":", "\n", "    ", "all_results", "=", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "for", "idx", ",", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "example_indices", ")", "in", "enumerate", "(", "eval_dataloader", ")", ":", "\n", "        ", "if", "pred_only", "and", "idx", "%", "10", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Running test: %d / %d\"", "%", "(", "idx", ",", "len", "(", "eval_dataloader", ")", ")", ")", "\n", "", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "if_trigger_ids", "=", "if_trigger_ids", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                ", "batch_start_logits", ",", "batch_end_logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "batch_start_logits", ",", "batch_end_logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "if_trigger_ids", ",", "input_mask", ")", "\n", "", "", "for", "i", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "            ", "start_logits", "=", "batch_start_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "end_logits", "=", "batch_end_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "eval_feature", "=", "eval_features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "example_id", "=", "eval_feature", ".", "example_id", "\n", "event_type_offset_argument_type", "=", "\"_\"", ".", "join", "(", "[", "eval_feature", ".", "event_type", ",", "str", "(", "eval_feature", ".", "token_to_orig_map", "[", "eval_feature", ".", "fea_trigger_offset", "]", ")", ",", "eval_feature", ".", "argument_type", "]", ")", "\n", "all_results", ".", "append", "(", "RawResult", "(", "example_id", "=", "example_id", ",", "event_type_offset_argument_type", "=", "event_type_offset_argument_type", ",", "\n", "start_logits", "=", "start_logits", ",", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "# preds, nbest_preds, na_probs = \\", "\n", "", "", "preds", "=", "make_predictions", "(", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "args", ".", "larger_than_cls", ")", "\n", "\n", "# get all_gold in format: [event_type_argument_type, [start_offset, end_offset]]", "\n", "all_gold", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "all_gold", "[", "example_id", "]", "=", "[", "]", "\n", "for", "event", "in", "example", ".", "events", ":", "\n", "# if not event: continue", "\n", "            ", "trigger_offset", "=", "event", "[", "0", "]", "[", "0", "]", "-", "example", ".", "s_start", "\n", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "for", "argument", "in", "event", "[", "1", ":", "]", ":", "\n", "                ", "argument_start", ",", "argument_end", ",", "argument_type", "=", "argument", "[", "0", "]", "-", "example", ".", "s_start", ",", "argument", "[", "1", "]", "-", "example", ".", "s_start", ",", "argument", "[", "2", "]", "\n", "# event_type_offset_argument_type = \"_\".join([event_type, str(trigger_offset), argument_type])", "\n", "if", "argument_type", "in", "unseen_arguments", ":", "\n", "                    ", "event_type_argument_type", "=", "\"_\"", ".", "join", "(", "[", "event_type", ",", "argument_type", "]", ")", "\n", "all_gold", "[", "example_id", "]", ".", "append", "(", "[", "event_type_argument_type", ",", "[", "argument_start", ",", "argument_end", "]", "]", ")", "\n", "\n", "# import ipdb; ipdb.set_trace()", "\n", "# get results (classification)", "\n", "", "", "", "", "gold_arg_n", ",", "pred_arg_n", ",", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "pred_arg", "=", "preds", "[", "example_id", "]", "\n", "gold_arg", "=", "all_gold", "[", "example_id", "]", "\n", "# pred_arg_n", "\n", "for", "argument", "in", "pred_arg", ":", "pred_arg_n", "+=", "1", "\n", "# gold_arg_n     ", "\n", "for", "argument", "in", "gold_arg", ":", "gold_arg_n", "+=", "1", "\n", "# pred_in_gold_n", "\n", "for", "argument", "in", "pred_arg", ":", "\n", "            ", "if", "argument", "in", "gold_arg", ":", "\n", "                ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argument", "in", "gold_arg", ":", "\n", "            ", "if", "argument", "in", "pred_arg", ":", "\n", "                ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "", "prec_c", ",", "recall_c", ",", "f1_c", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_c", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_c", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_c", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_c", "=", "0", "\n", "if", "prec_c", "or", "recall_c", ":", "f1_c", "=", "2", "*", "prec_c", "*", "recall_c", "/", "(", "prec_c", "+", "recall_c", ")", "\n", "else", ":", "f1_c", "=", "0", "\n", "\n", "\n", "# logging for DEBUG results", "\n", "if", "pred_only", ":", "\n", "        ", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "            ", "if", "example_id", ">", "40", ":", "break", "\n", "if", "preds", "[", "example_id", "]", "or", "all_gold", "[", "example_id", "]", ":", "\n", "                ", "token_idx", "=", "[", "]", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "example", ".", "sentence", ")", ":", "token_idx", ".", "append", "(", "\" \"", ".", "join", "(", "[", "token", ",", "str", "(", "idx", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"sent: {}\"", ".", "format", "(", "\" | \"", ".", "join", "(", "token_idx", ")", ")", ")", "\n", "\n", "gold_str_list", "=", "[", "]", "\n", "for", "gold", "in", "all_gold", "[", "example_id", "]", ":", "gold_str_list", ".", "append", "(", "\" \"", ".", "join", "(", "[", "gold", "[", "0", "]", ",", "str", "(", "gold", "[", "1", "]", "[", "0", "]", ")", ",", "str", "(", "gold", "[", "1", "]", "[", "1", "]", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"gold: {}\"", ".", "format", "(", "\" | \"", ".", "join", "(", "gold_str_list", ")", ")", ")", "\n", "\n", "pred_str_list", "=", "[", "]", "\n", "for", "pred", "in", "preds", "[", "example_id", "]", ":", "pred_str_list", ".", "append", "(", "\" \"", ".", "join", "(", "[", "pred", "[", "0", "]", ",", "str", "(", "pred", "[", "1", "]", "[", "0", "]", ")", ",", "str", "(", "pred", "[", "1", "]", "[", "1", "]", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"pred: {}\"", ".", "format", "(", "\" | \"", ".", "join", "(", "pred_str_list", ")", ")", ")", "\n", "\n", "logger", ".", "info", "(", "\"\\n\"", ")", "\n", "\n", "# get results (identification)", "\n", "", "", "", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "for", "argument", "in", "preds", "[", "example_id", "]", ":", "\n", "                ", "argument", "[", "0", "]", "=", "argument", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "# only event_type", "\n", "", "for", "argument", "in", "all_gold", "[", "example_id", "]", ":", "\n", "                ", "argument", "[", "0", "]", "=", "argument", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "# only event_type", "\n", "\n", "", "", "gold_arg_n", ",", "pred_arg_n", ",", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "pred_arg", "=", "preds", "[", "example_id", "]", "\n", "gold_arg", "=", "all_gold", "[", "example_id", "]", "\n", "# pred_arg_n", "\n", "for", "argument", "in", "pred_arg", ":", "pred_arg_n", "+=", "1", "\n", "# gold_arg_n     ", "\n", "for", "argument", "in", "gold_arg", ":", "gold_arg_n", "+=", "1", "\n", "# pred_in_gold_n", "\n", "for", "argument", "in", "pred_arg", ":", "\n", "            ", "if", "argument", "in", "gold_arg", ":", "\n", "                ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argument", "in", "gold_arg", ":", "\n", "            ", "if", "argument", "in", "pred_arg", ":", "\n", "                ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "", "prec_i", ",", "recall_i", ",", "f1_i", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_i", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_i", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_i", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_i", "=", "0", "\n", "if", "prec_i", "or", "recall_i", ":", "f1_i", "=", "2", "*", "prec_i", "*", "recall_i", "/", "(", "prec_i", "+", "recall_i", ")", "\n", "else", ":", "f1_i", "=", "0", "\n", "\n", "\n", "result", "=", "collections", ".", "OrderedDict", "(", "[", "(", "'prec_c'", ",", "prec_c", ")", ",", "(", "'recall_c'", ",", "recall_c", ")", ",", "(", "'f1_c'", ",", "f1_c", ")", ",", "(", "'prec_i'", ",", "prec_i", ")", ",", "(", "'recall_i'", ",", "recall_i", ")", ",", "(", "'f1_i'", ",", "f1_i", ")", "]", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.main": [[444, 668], ["torch.device", "torch.cuda.device_count", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "logger.info", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "run_args_qa_unseen.read_query_templates", "run_args_qa_unseen.read_unseen_arguments", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.exists", "os.makedirs", "logger.addHandler", "logger.addHandler", "run_args_qa_unseen.read_ace_examples", "run_args_qa_unseen.read_ace_examples", "run_args_qa_unseen.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "run_args_qa_unseen.read_ace_examples", "run_args_qa_unseen.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "max", "torch.nn.DataParallel.to", "run_args_qa_unseen.evaluate", "logging.FileHandler", "logging.FileHandler", "len", "len", "torch.tensor.size", "sorted", "random.shuffle", "len", "len", "torch.nn.DataParallel.to", "list", "pytorch_pretrained_bert.optimization.BertAdam", "time.time", "range", "run_args_qa_unseen.read_ace_examples", "run_args_qa_unseen.read_ace_examples", "run_args_qa_unseen.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering.from_pretrained", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.from_pretrained", "torch.nn.DataParallel.half", "torch.cuda.is_available", "os.path.join", "os.path.join", "len", "len", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering.from_pretrained", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.from_pretrained", "torch.nn.DataParallel.half", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel.train", "logger.info", "enumerate", "len", "len", "torch.tensor.size", "random.shuffle", "loss.mean.item", "input_ids.size", "loss.mean.backward", "numpy.sum", "tuple", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "loss.mean.mean", "pytorch_pretrained_bert.optimization.BertAdam.step", "pytorch_pretrained_bert.optimization.BertAdam.zero_grad", "any", "run_args_qa_unseen.evaluate", "torch.nn.DataParallel.train", "os.path.join", "os.path.join", "torch.save", "model_to_save.config.to_json_file", "BertTokenizer.from_pretrained.save_vocabulary", "any", "t.to", "logger.info", "logger.info", "hasattr", "model_to_save.state_dict", "io.open", "sorted", "len", "os.path.join", "best_result.keys", "writer.write", "time.time", "str", "str"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_query_templates", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_unseen.read_unseen_arguments", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_file", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.save_vocabulary"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "logger", ".", "info", "(", "\"device: {}, n_gpu: {}, 16-bits training: {}\"", ".", "format", "(", "device", ",", "n_gpu", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "(", "args", ".", "train_file", "is", "not", "None", ")", "and", "(", "args", ".", "dev_file", "is", "not", "None", ")", "\n", "\n", "", "if", "args", ".", "eval_test", ":", "\n", "        ", "assert", "args", ".", "test_file", "is", "not", "None", "\n", "", "else", ":", "\n", "        ", "assert", "args", ".", "dev_file", "is", "not", "None", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train.log\"", ")", ",", "'w'", ")", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval.log\"", ")", ",", "'w'", ")", ")", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "# read query templates", "\n", "query_templates", "=", "read_query_templates", "(", "normal_file", "=", "args", ".", "normal_file", ",", "des_file", "=", "args", ".", "des_file", ")", "\n", "\n", "# read unseen arguments", "\n", "unseen_arguments", "=", "read_unseen_arguments", "(", "unseen_arguments_file", "=", "args", ".", "unseen_arguments_file", ")", "\n", "\n", "if", "args", ".", "do_train", "or", "(", "not", "args", ".", "eval_test", ")", ":", "\n", "        ", "eval_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "dev_file", ",", "is_training", "=", "False", ")", "\n", "gold_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "gold_file", ",", "is_training", "=", "False", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "unseen_arguments", "=", "unseen_arguments", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"***** Dev *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "all_example_index", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "train_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "unseen_arguments", "=", "unseen_arguments", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "True", ")", "\n", "\n", "if", "args", ".", "train_mode", "==", "'sorted'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "            ", "train_features", "=", "sorted", "(", "train_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "train_features", ")", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_start_positions", "=", "torch", ".", "tensor", "(", "[", "f", ".", "start_position", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_end_positions", "=", "torch", ".", "tensor", "(", "[", "f", ".", "end_position", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "\n", "all_start_positions", ",", "all_end_positions", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "train_batches", "=", "[", "batch", "for", "batch", "in", "train_dataloader", "]", "\n", "\n", "num_train_optimization_steps", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "logger", ".", "info", "(", "\"***** Train *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "train_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "eval_step", "=", "max", "(", "1", ",", "len", "(", "train_batches", ")", "//", "args", ".", "eval_per_epoch", ")", "\n", "best_result", "=", "None", "\n", "lrs", "=", "[", "args", ".", "learning_rate", "]", "if", "args", ".", "learning_rate", "else", "[", "1e-6", ",", "2e-6", ",", "3e-6", ",", "5e-6", ",", "1e-5", ",", "2e-5", ",", "3e-5", ",", "5e-5", "]", "\n", "for", "lr", "in", "lrs", ":", "\n", "            ", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                ", "model", "=", "BertForQuestionAnswering", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ")", "\n", "", "else", ":", "\n", "                ", "model", "=", "BertForQuestionAnswering_withIfTriggerEmbedding", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "[", "n", "for", "n", "in", "param_optimizer", "if", "'pooler'", "not", "in", "n", "[", "0", "]", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "tr_loss", "=", "0", "\n", "nb_tr_examples", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "global_step", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "                ", "model", ".", "train", "(", ")", "\n", "logger", ".", "info", "(", "\"Start epoch #{} (lr = {})...\"", ".", "format", "(", "epoch", ",", "lr", ")", ")", "\n", "if", "args", ".", "train_mode", "==", "'random'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "                    ", "random", ".", "shuffle", "(", "train_batches", ")", "\n", "", "for", "step", ",", "batch", "in", "enumerate", "(", "train_batches", ")", ":", "\n", "                    ", "if", "n_gpu", "==", "1", ":", "\n", "                        ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "start_positions", ",", "end_positions", "=", "batch", "\n", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                        ", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "if_trigger_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                        ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "if", "(", "step", "+", "1", ")", "%", "eval_step", "==", "0", ":", "\n", "                        ", "save_model", "=", "False", "\n", "if", "args", ".", "do_eval", ":", "\n", "# result, _, _ = evaluate(args, model, device, eval_dataset, eval_dataloader, eval_examples, eval_features)", "\n", "                            ", "result", "=", "evaluate", "(", "args", ",", "unseen_arguments", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ")", "\n", "# import ipdb; ipdb.set_trace()", "\n", "model", ".", "train", "(", ")", "\n", "result", "[", "'global_step'", "]", "=", "global_step", "\n", "result", "[", "'epoch'", "]", "=", "epoch", "\n", "result", "[", "'learning_rate'", "]", "=", "lr", "\n", "result", "[", "'batch_size'", "]", "=", "args", ".", "train_batch_size", "\n", "if", "(", "best_result", "is", "None", ")", "or", "(", "result", "[", "args", ".", "eval_metric", "]", ">", "best_result", "[", "args", ".", "eval_metric", "]", ")", ":", "\n", "                                ", "best_result", "=", "result", "\n", "save_model", "=", "True", "\n", "logger", ".", "info", "(", "'Epoch: {}, Step: {} / {}, used_time = {:.2f}s, loss = {:.6f}'", ".", "format", "(", "\n", "epoch", ",", "step", "+", "1", ",", "len", "(", "train_batches", ")", ",", "time", ".", "time", "(", ")", "-", "start_time", ",", "tr_loss", "/", "nb_tr_steps", ")", ")", "\n", "# logger.info(\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f\" %", "\n", "# (args.eval_metric, str(lr), epoch, result[\"prec_c\"], result[\"recall_c\"], result[\"f1_c\"]))", "\n", "logger", ".", "info", "(", "\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f, p_i: %.2f, r_i: %.2f, f1_i: %.2f\"", "%", "\n", "(", "args", ".", "eval_metric", ",", "str", "(", "lr", ")", ",", "epoch", ",", "result", "[", "\"prec_c\"", "]", ",", "result", "[", "\"recall_c\"", "]", ",", "result", "[", "\"f1_c\"", "]", ",", "result", "[", "\"prec_i\"", "]", ",", "result", "[", "\"recall_i\"", "]", ",", "result", "[", "\"f1_i\"", "]", ")", ")", "\n", "", "", "else", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "if", "save_model", ":", "\n", "                            ", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "model_to_save", ".", "config", ".", "to_json_file", "(", "output_config_file", ")", "\n", "tokenizer", ".", "save_vocabulary", "(", "args", ".", "output_dir", ")", "\n", "if", "best_result", ":", "\n", "                                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                                    ", "for", "key", "in", "sorted", "(", "best_result", ".", "keys", "(", ")", ")", ":", "\n", "                                        ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "best_result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "", "", "", "", "", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "if", "args", ".", "eval_test", ":", "\n", "            ", "eval_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "test_file", ",", "is_training", "=", "False", ")", "\n", "gold_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "gold_file", ",", "is_training", "=", "False", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "unseen_arguments", "=", "unseen_arguments", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"***** Test *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "all_example_index", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "            ", "model", "=", "BertForQuestionAnswering", ".", "from_pretrained", "(", "args", ".", "output_dir", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "BertForQuestionAnswering_withIfTriggerEmbedding", ".", "from_pretrained", "(", "args", ".", "output_dir", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "result", "=", "evaluate", "(", "args", ",", "unseen_arguments", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ",", "pred_only", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.trigger_category_vocab.__init__": [[49, 54], ["dict", "dict", "collections.Counter"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "category_to_index", "=", "dict", "(", ")", "\n", "self", ".", "index_to_category", "=", "dict", "(", ")", "\n", "self", ".", "counter", "=", "Counter", "(", ")", "\n", "self", ".", "max_sent_length", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.trigger_category_vocab.create_vocab": [[55, 74], ["io.open", "json.loads", "len", "len", "len"], "methods", ["None"], ["", "def", "create_vocab", "(", "self", ",", "files_list", ")", ":", "\n", "        ", "self", ".", "category_to_index", "[", "\"None\"", "]", "=", "0", "\n", "self", ".", "index_to_category", "[", "0", "]", "=", "\"None\"", "\n", "for", "file", "in", "files_list", ":", "\n", "            ", "with", "open", "(", "file", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "events", ",", "sentence", "=", "example", "[", "\"event\"", "]", ",", "example", "[", "\"sentence\"", "]", "\n", "if", "len", "(", "sentence", ")", ">", "self", ".", "max_sent_length", ":", "self", ".", "max_sent_length", "=", "len", "(", "sentence", ")", "\n", "for", "event", "in", "events", ":", "\n", "                        ", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "self", ".", "counter", "[", "event_type", "]", "+=", "1", "\n", "if", "event_type", "not", "in", "self", ".", "category_to_index", ":", "\n", "                            ", "index", "=", "len", "(", "self", ".", "category_to_index", ")", "\n", "self", ".", "category_to_index", "[", "event_type", "]", "=", "index", "\n", "self", ".", "index_to_category", "[", "index", "]", "=", "event_type", "\n", "\n", "# add [CLS]", "\n", "", "", "", "", "", "self", ".", "max_sent_length", "+=", "12", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.InputFeatures.__init__": [[79, 104], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "# unique_id,", "\n", "# example_index,", "\n", "# doc_span_index,", "\n", "sentence_id", ",", "\n", "tokens", ",", "\n", "# token_to_orig_map,", "\n", "# token_is_max_context,", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "in_sentence", ",", "\n", "labels", ")", ":", "\n", "# self.unique_id = unique_id", "\n", "# self.example_index = example_index", "\n", "# self.doc_span_index = doc_span_index", "\n", "        ", "self", ".", "sentence_id", "=", "sentence_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "# self.token_to_orig_map = token_to_orig_map", "\n", "# self.token_is_max_context = token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "in_sentence", "=", "in_sentence", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.read_ace_examples": [[106, 199], ["io.open", "json.loads", "dict", "tokens.append", "segment_ids.append", "in_sentence.append", "labels.append", "enumerate", "tokens.append", "segment_ids.append", "in_sentence.append", "labels.append", "enumerate", "tokens.append", "segment_ids.append", "in_sentence.append", "labels.append", "tokenizer.convert_tokens_to_ids", "features.append", "examples.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "in_sentence.append", "labels.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "in_sentence.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "in_sentence.append", "labels.append", "len", "len", "len", "len", "len", "run_trigger_qa.InputFeatures", "len", "labels.append", "labels.append"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "", "def", "read_ace_examples", "(", "nth_query", ",", "input_file", ",", "tokenizer", ",", "category_vocab", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Read an ACE json file, transform to features\"\"\"", "\n", "features", "=", "[", "]", "\n", "examples", "=", "[", "]", "\n", "sentence_id", "=", "0", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "sentence", ",", "events", ",", "s_start", "=", "example", "[", "\"sentence\"", "]", ",", "example", "[", "\"event\"", "]", ",", "example", "[", "\"s_start\"", "]", "\n", "offset_category", "=", "dict", "(", ")", "\n", "for", "event", "in", "events", ":", "\n", "                ", "assert", "len", "(", "event", "[", "0", "]", ")", "==", "2", "\n", "offset", ",", "category", "=", "event", "[", "0", "]", "[", "0", "]", "-", "s_start", ",", "event", "[", "0", "]", "[", "1", "]", "\n", "offset_category", "[", "offset", "]", "=", "category", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "in_sentence", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "# add [CLS]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "in_sentence", ".", "append", "(", "0", ")", "\n", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "\"None\"", "]", ")", "\n", "\n", "# add query", "\n", "query", "=", "candidate_queries", "[", "nth_query", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "query", ")", ":", "\n", "                ", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "tokens", ".", "append", "(", "sub_tokens", "[", "0", "]", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "in_sentence", ".", "append", "(", "0", ")", "\n", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "\"None\"", "]", ")", "\n", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "in_sentence", ".", "append", "(", "0", ")", "\n", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "\"None\"", "]", ")", "\n", "\n", "# add sentence", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "sentence", ")", ":", "\n", "                ", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "tokens", ".", "append", "(", "sub_tokens", "[", "0", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "in_sentence", ".", "append", "(", "1", ")", "\n", "if", "i", "in", "offset_category", ":", "\n", "                    ", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "offset_category", "[", "i", "]", "]", ")", "\n", "", "else", ":", "\n", "                    ", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "\"None\"", "]", ")", "\n", "\n", "# add [SEP]", "\n", "", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "in_sentence", ".", "append", "(", "0", ")", "\n", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "\"None\"", "]", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "category_vocab", ".", "max_sent_length", ":", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "in_sentence", ".", "append", "(", "0", ")", "\n", "labels", ".", "append", "(", "category_vocab", ".", "category_to_index", "[", "\"None\"", "]", ")", "\n", "\n", "# print(len(input_ids), category_vocab.max_sent_length)", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "category_vocab", ".", "max_sent_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "category_vocab", ".", "max_sent_length", "\n", "assert", "len", "(", "in_sentence", ")", "==", "category_vocab", ".", "max_sent_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "category_vocab", ".", "max_sent_length", "\n", "assert", "len", "(", "labels", ")", "==", "category_vocab", ".", "max_sent_length", "\n", "\n", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "# unique_id=unique_id,", "\n", "# example_index=example_index,", "\n", "sentence_id", "=", "sentence_id", ",", "\n", "tokens", "=", "tokens", ",", "\n", "# token_to_orig_map=token_to_orig_map,", "\n", "# token_is_max_context=token_is_max_context,", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "in_sentence", "=", "in_sentence", ",", "\n", "labels", "=", "labels", ")", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "# if len(tokens) > 20 and sum(labels) > 0:", "\n", "# import ipdb; ipdb.set_trace()", "\n", "sentence_id", "+=", "1", "\n", "\n", "", "", "return", "examples", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.evaluate": [[202, 333], ["model.eval", "dict", "enumerate", "enumerate", "collections.OrderedDict", "collections.OrderedDict", "copy.deepcopy", "enumerate", "sentence_id.tolist.tolist", "input_ids.to.to", "segmend_ids.to.to", "input_mask.to.to", "enumerate", "gold_triggers.append", "gold_triggers_offset.append", "pred_triggers_offset.append", "logger.info", "torch.no_grad", "torch.no_grad", "model", "logits[].detach().cpu", "torch.max", "torch.max", "tag_seq.tolist.tolist", "enumerate", "enumerate", "gold_sentence_triggers.append", "sent_triggers_offset.append", "sent_triggers_offset.append", "pred[].append", "logits[].detach", "decoded_tag_seg.append", "sentence_triggers.append", "len"], "function", ["None"], ["", "def", "evaluate", "(", "args", ",", "eval_examples", ",", "category_vocab", ",", "model", ",", "device", ",", "eval_dataloader", ",", "pred_only", "=", "False", ")", ":", "\n", "# eval_examples, eval_features, na_prob_thresh=1.0, pred_only=False):", "\n", "    ", "all_results", "=", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# get predictions", "\n", "pred_triggers", "=", "dict", "(", ")", "\n", "for", "idx", ",", "(", "sentence_id", ",", "input_ids", ",", "segmend_ids", ",", "in_sentence", ",", "input_mask", ",", "labels", ")", "in", "enumerate", "(", "eval_dataloader", ")", ":", "\n", "        ", "if", "pred_only", "and", "idx", "%", "10", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Running test: %d / %d\"", "%", "(", "idx", ",", "len", "(", "eval_dataloader", ")", ")", ")", "\n", "", "sentence_id", "=", "sentence_id", ".", "tolist", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "segmend_ids", "=", "segmend_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "labels", "=", "labels", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "logits", "=", "model", "(", "input_ids", ",", "token_type_ids", "=", "segmend_ids", ",", "attention_mask", "=", "input_mask", ")", "\n", "\n", "", "for", "i", ",", "in_sent", "in", "enumerate", "(", "in_sentence", ")", ":", "\n", "            ", "logits_i", "=", "logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "_", ",", "tag_seq", "=", "torch", ".", "max", "(", "logits_i", ",", "1", ")", "\n", "tag_seq", "=", "tag_seq", ".", "tolist", "(", ")", "\n", "\n", "decoded_tag_seg", "=", "[", "]", "\n", "for", "idj", ",", "j", "in", "enumerate", "(", "in_sent", ")", ":", "\n", "                ", "if", "j", ":", "\n", "                    ", "decoded_tag_seg", ".", "append", "(", "category_vocab", ".", "index_to_category", "[", "tag_seq", "[", "idj", "]", "]", ")", "\n", "", "", "sentence_triggers", "=", "[", "]", "\n", "for", "offset", ",", "tag", "in", "enumerate", "(", "decoded_tag_seg", ")", ":", "\n", "                ", "if", "tag", "!=", "\"None\"", ":", "\n", "                    ", "sentence_triggers", ".", "append", "(", "[", "offset", ",", "tag", "]", ")", "\n", "\n", "", "", "pred_triggers", "[", "sentence_id", "[", "i", "]", "]", "=", "sentence_triggers", "\n", "\n", "# get results (classification)", "\n", "", "", "gold_triggers", "=", "[", "]", "\n", "for", "eval_example", "in", "eval_examples", ":", "\n", "        ", "events", "=", "eval_example", "[", "\"event\"", "]", "\n", "s_start", "=", "eval_example", "[", "\"s_start\"", "]", "\n", "gold_sentence_triggers", "=", "[", "]", "\n", "for", "event", "in", "events", ":", "\n", "            ", "offset", ",", "category", "=", "event", "[", "0", "]", "\n", "offset_new", "=", "offset", "-", "s_start", "\n", "gold_sentence_triggers", ".", "append", "(", "[", "offset_new", ",", "category", "]", ")", "\n", "", "gold_triggers", ".", "append", "(", "gold_sentence_triggers", ")", "\n", "\n", "", "gold_trigger_n", ",", "pred_trigger_n", ",", "true_positive_n", "=", "0", ",", "0", ",", "0", "\n", "for", "sentence_id", "in", "pred_triggers", ":", "\n", "        ", "gold_sentence_triggers", "=", "gold_triggers", "[", "sentence_id", "]", "\n", "pred_sentence_triggers", "=", "pred_triggers", "[", "sentence_id", "]", "\n", "# for pred_trigger_n", "\n", "for", "trigger", "in", "pred_sentence_triggers", ":", "pred_trigger_n", "+=", "1", "\n", "# for gold_trigger_n     ", "\n", "for", "trigger", "in", "gold_sentence_triggers", ":", "gold_trigger_n", "+=", "1", "\n", "# for true_positive_n", "\n", "for", "trigger", "in", "pred_sentence_triggers", ":", "\n", "            ", "if", "trigger", "in", "gold_sentence_triggers", ":", "\n", "                ", "true_positive_n", "+=", "1", "\n", "\n", "", "", "", "prec_c", ",", "recall_c", ",", "f1_c", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_trigger_n", "!=", "0", ":", "\n", "        ", "prec_c", "=", "100.0", "*", "true_positive_n", "/", "pred_trigger_n", "\n", "", "else", ":", "\n", "        ", "prec_c", "=", "0", "\n", "", "if", "gold_trigger_n", "!=", "0", ":", "\n", "        ", "recall_c", "=", "100.0", "*", "true_positive_n", "/", "gold_trigger_n", "\n", "", "else", ":", "\n", "        ", "recall_c", "=", "0", "\n", "", "if", "prec_c", "or", "recall_c", ":", "\n", "        ", "f1_c", "=", "2", "*", "prec_c", "*", "recall_c", "/", "(", "prec_c", "+", "recall_c", ")", "\n", "", "else", ":", "\n", "        ", "f1_c", "=", "0", "\n", "\n", "# get results (identification)", "\n", "", "gold_triggers_offset", "=", "[", "]", "\n", "for", "sent_triggers", "in", "gold_triggers", ":", "\n", "        ", "sent_triggers_offset", "=", "[", "]", "\n", "for", "trigger", "in", "sent_triggers", ":", "\n", "            ", "sent_triggers_offset", ".", "append", "(", "trigger", "[", "0", "]", ")", "\n", "", "gold_triggers_offset", ".", "append", "(", "sent_triggers_offset", ")", "\n", "\n", "", "pred_triggers_offset", "=", "[", "]", "\n", "for", "sentence_id", "in", "pred_triggers", ":", "\n", "        ", "sent_triggers_offset", "=", "[", "]", "\n", "for", "trigger", "in", "pred_triggers", "[", "sentence_id", "]", ":", "\n", "            ", "sent_triggers_offset", ".", "append", "(", "trigger", "[", "0", "]", ")", "\n", "", "pred_triggers_offset", ".", "append", "(", "sent_triggers_offset", ")", "\n", "\n", "", "gold_trigger_n", ",", "pred_trigger_n", ",", "true_positive_n", "=", "0", ",", "0", ",", "0", "\n", "for", "sentence_id", ",", "_", "in", "enumerate", "(", "pred_triggers_offset", ")", ":", "\n", "        ", "gold_sentence_triggers", "=", "gold_triggers_offset", "[", "sentence_id", "]", "\n", "pred_sentence_triggers", "=", "pred_triggers_offset", "[", "sentence_id", "]", "\n", "# for pred_trigger_n", "\n", "for", "trigger", "in", "pred_sentence_triggers", ":", "\n", "            ", "pred_trigger_n", "+=", "1", "\n", "# for gold_trigger_n", "\n", "", "for", "trigger", "in", "gold_sentence_triggers", ":", "\n", "            ", "gold_trigger_n", "+=", "1", "\n", "# for true_positive_n", "\n", "", "for", "trigger", "in", "pred_sentence_triggers", ":", "\n", "            ", "if", "trigger", "in", "gold_sentence_triggers", ":", "\n", "                ", "true_positive_n", "+=", "1", "\n", "\n", "", "", "", "result", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "prec_i", ",", "recall_i", ",", "f1_i", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_trigger_n", "!=", "0", ":", "\n", "        ", "prec_i", "=", "100.0", "*", "true_positive_n", "/", "pred_trigger_n", "\n", "", "else", ":", "\n", "        ", "prec_i", "=", "0", "\n", "", "if", "gold_trigger_n", "!=", "0", ":", "\n", "        ", "recall_i", "=", "100.0", "*", "true_positive_n", "/", "gold_trigger_n", "\n", "", "else", ":", "\n", "        ", "recall_i", "=", "0", "\n", "", "if", "prec_i", "or", "recall_i", ":", "\n", "        ", "f1_i", "=", "2", "*", "prec_i", "*", "recall_i", "/", "(", "prec_i", "+", "recall_i", ")", "\n", "", "else", ":", "\n", "        ", "f1_i", "=", "0", "\n", "", "result", "=", "collections", ".", "OrderedDict", "(", "[", "(", "'prec_c'", ",", "prec_c", ")", ",", "(", "'recall_c'", ",", "recall_c", ")", ",", "(", "'f1_c'", ",", "f1_c", ")", ",", "(", "'prec_i'", ",", "prec_i", ")", ",", "(", "'recall_i'", ",", "recall_i", ")", ",", "(", "'f1_i'", ",", "f1_i", ")", "]", ")", "\n", "\n", "\n", "preds", "=", "copy", ".", "deepcopy", "(", "eval_examples", ")", "\n", "for", "sentence_id", ",", "pred", "in", "enumerate", "(", "preds", ")", ":", "\n", "        ", "s_start", "=", "pred", "[", "'s_start'", "]", "\n", "pred", "[", "'event'", "]", "=", "[", "]", "\n", "pred_sentence_triggers", "=", "pred_triggers", "[", "sentence_id", "]", "\n", "for", "trigger", "in", "pred_sentence_triggers", ":", "\n", "            ", "offset", "=", "s_start", "+", "trigger", "[", "0", "]", "\n", "category", "=", "trigger", "[", "1", "]", "\n", "pred", "[", "'event'", "]", ".", "append", "(", "[", "[", "offset", ",", "category", "]", "]", ")", "\n", "\n", "", "", "return", "result", ",", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.main": [[335, 583], ["torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "logger.info", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "run_trigger_qa.trigger_category_vocab", "run_trigger_qa.trigger_category_vocab.create_vocab", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.exists", "os.makedirs", "logger.addHandler", "logger.addHandler", "run_trigger_qa.read_ace_examples", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "run_trigger_qa.read_ace_examples", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "logger.info", "max", "torch.nn.DataParallel.to", "run_trigger_qa.evaluate", "logging.FileHandler", "logging.FileHandler", "sorted", "len", "sorted", "random.shuffle", "sorted", "len", "torch.nn.DataParallel.to", "time.time", "range", "run_trigger_qa.read_ace_examples", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "pytorch_pretrained_bert.modeling.BertLSTMForTriggerClassification.from_pretrained", "pytorch_pretrained_bert.modeling.BertForTriggerClassification.from_pretrained", "torch.nn.DataParallel.half", "io.open", "io.open", "torch.cuda.is_available", "torch.cuda.is_available", "os.path.join", "os.path.join", "len", "len", "pytorch_pretrained_bert.modeling.BertLSTMForTriggerClassification.from_pretrained", "pytorch_pretrained_bert.modeling.BertForTriggerClassification.from_pretrained", "torch.nn.DataParallel.half", "torch.nn.DataParallel", "torch.nn.DataParallel", "list", "torch.SGD", "int", "torch.nn.DataParallel.train", "logger.info", "enumerate", "sorted", "len", "os.path.join", "writer.write", "os.path.join", "writer.write", "torch.nn.DataParallel.named_parameters", "FusedAdam", "pytorch_pretrained_bert.optimization.BertAdam", "torch.nn.DataParallel.parameters", "random.shuffle", "torch.nn.DataParallel.", "loss.mean.item", "input_ids.size", "len", "len", "numpy.sum", "numpy.sum", "numpy.sum", "len", "len", "FP16_Optimizer", "FP16_Optimizer", "tuple", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "json.dumps", "ImportError", "run_trigger_qa.evaluate", "torch.nn.DataParallel.train", "os.path.join", "os.path.join", "os.path.join", "torch.save", "torch.save", "model_to_save.config.to_json_file", "BertTokenizer.from_pretrained.save_vocabulary", "numpy.sum", "str", "any", "t.to", "pytorch_pretrained_bert.optimization.warmup_linear", "logger.info", "logger.info", "logger.info", "hasattr", "os.path.exists", "os.makedirs", "model_to_save.state_dict", "any", "io.open", "len", "int", "len", "os.path.join", "writer.write", "time.time", "str", "str"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_trigger_qa.trigger_category_vocab.create_vocab", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_file", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.save_vocabulary", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.warmup_linear"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "logger", ".", "info", "(", "\"device: {}, n_gpu: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "(", "args", ".", "train_file", "is", "not", "None", ")", "and", "(", "args", ".", "dev_file", "is", "not", "None", ")", "\n", "\n", "", "if", "args", ".", "eval_test", ":", "\n", "        ", "assert", "args", ".", "test_file", "is", "not", "None", "\n", "", "else", ":", "\n", "        ", "assert", "args", ".", "dev_file", "is", "not", "None", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train.log\"", ")", ",", "'w'", ")", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval.log\"", ")", ",", "'w'", ")", ")", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "category_vocab", "=", "trigger_category_vocab", "(", ")", "\n", "category_vocab", ".", "create_vocab", "(", "[", "args", ".", "train_file", ",", "args", ".", "dev_file", ",", "args", ".", "test_file", "]", ")", "\n", "\n", "if", "args", ".", "do_train", "or", "(", "not", "args", ".", "eval_test", ")", ":", "\n", "        ", "eval_examples", ",", "eval_features", "=", "read_ace_examples", "(", "nth_query", "=", "args", ".", "nth_query", ",", "input_file", "=", "args", ".", "dev_file", ",", "tokenizer", "=", "tokenizer", ",", "category_vocab", "=", "category_vocab", ",", "is_training", "=", "False", ")", "\n", "if", "args", ".", "add_lstm", ":", "\n", "            ", "eval_features", "=", "sorted", "(", "eval_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ",", "reverse", "=", "True", ")", "\n", "", "logger", ".", "info", "(", "\"***** Dev *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_sentence_id", "=", "torch", ".", "tensor", "(", "[", "f", ".", "sentence_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segmend_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_in_sentence", "=", "torch", ".", "tensor", "(", "[", "f", ".", "in_sentence", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "# all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)", "\n", "eval_data", "=", "TensorDataset", "(", "all_sentence_id", ",", "all_input_ids", ",", "all_segmend_ids", ",", "all_in_sentence", ",", "all_input_mask", ",", "all_labels", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "_", ",", "train_features", "=", "read_ace_examples", "(", "nth_query", "=", "args", ".", "nth_query", ",", "input_file", "=", "args", ".", "train_file", ",", "tokenizer", "=", "tokenizer", ",", "category_vocab", "=", "category_vocab", ",", "is_training", "=", "True", ")", "\n", "if", "args", ".", "train_mode", "==", "'sorted'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "            ", "train_features", "=", "sorted", "(", "train_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "train_features", ")", "\n", "", "if", "args", ".", "add_lstm", ":", "\n", "            ", "train_features", "=", "sorted", "(", "train_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_in_sentence", "=", "torch", ".", "tensor", "(", "[", "f", ".", "in_sentence", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "labels", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_segment_ids", ",", "all_input_mask", ",", "all_labels", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "train_batches", "=", "[", "batch", "for", "batch", "in", "train_dataloader", "]", "\n", "\n", "num_train_optimization_steps", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "logger", ".", "info", "(", "\"***** Train *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "train_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "eval_step", "=", "max", "(", "1", ",", "len", "(", "train_batches", ")", "//", "args", ".", "eval_per_epoch", ")", "\n", "best_result", "=", "None", "\n", "lrs", "=", "[", "args", ".", "learning_rate", "]", "if", "args", ".", "learning_rate", "else", "[", "1e-6", ",", "2e-6", ",", "3e-6", ",", "5e-6", ",", "1e-5", ",", "2e-5", ",", "3e-5", ",", "5e-5", "]", "\n", "for", "lr", "in", "lrs", ":", "\n", "            ", "if", "args", ".", "add_lstm", ":", "\n", "                ", "model", "=", "BertLSTMForTriggerClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ",", "num_labels", "=", "len", "(", "category_vocab", ".", "index_to_category", ")", ")", "\n", "", "else", ":", "\n", "                ", "model", "=", "BertForTriggerClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ",", "num_labels", "=", "len", "(", "category_vocab", ".", "index_to_category", ")", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "if", "not", "args", ".", "add_lstm", ":", "\n", "                ", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "[", "n", "for", "n", "in", "param_optimizer", "if", "'pooler'", "not", "in", "n", "[", "0", "]", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "                    ", "try", ":", "\n", "                        ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "                        ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex\"", "\n", "\"to use distributed and fp16 training.\"", ")", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "lr", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "                        ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "                        ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "", "", "else", ":", "\n", "                    ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "", "", "else", ":", "\n", "                ", "optimizer", "=", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lstm_lr", ",", "momentum", "=", "0.9", ",", "weight_decay", "=", "1e-6", ")", "\n", "\n", "", "tr_loss", "=", "0", "\n", "nb_tr_examples", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "global_step", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "                ", "model", ".", "train", "(", ")", "\n", "logger", ".", "info", "(", "\"Start epoch #{} (lr = {})...\"", ".", "format", "(", "epoch", ",", "lr", ")", ")", "\n", "if", "args", ".", "train_mode", "==", "'random'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "                    ", "random", ".", "shuffle", "(", "train_batches", ")", "\n", "", "for", "step", ",", "batch", "in", "enumerate", "(", "train_batches", ")", ":", "\n", "                    ", "if", "n_gpu", "==", "1", ":", "\n", "                        ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "labels", "=", "batch", "\n", "loss", "=", "model", "(", "input_ids", ",", "token_type_ids", "=", "segment_ids", ",", "attention_mask", "=", "input_mask", ",", "labels", "=", "labels", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "                        ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                        ", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                        ", "if", "args", ".", "fp16", ":", "\n", "                            ", "lr_this_step", "=", "lr", "*", "warmup_linear", "(", "global_step", "/", "num_train_optimization_steps", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                                ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "if", "(", "step", "+", "1", ")", "%", "eval_step", "==", "0", "or", "step", "==", "0", ":", "\n", "# logger.info('Epoch: {}, Step: {} / {}, used_time = {:.2f}s, loss = {:.6f}'.format(", "\n", "#     epoch, step + 1, len(train_batches), time.time() - start_time, tr_loss / nb_tr_steps))", "\n", "\n", "                        ", "save_model", "=", "False", "\n", "if", "args", ".", "do_eval", ":", "\n", "                            ", "result", ",", "_", "=", "evaluate", "(", "args", ",", "eval_examples", ",", "category_vocab", ",", "model", ",", "device", ",", "eval_dataloader", ")", "\n", "model", ".", "train", "(", ")", "\n", "result", "[", "'global_step'", "]", "=", "global_step", "\n", "result", "[", "'epoch'", "]", "=", "epoch", "\n", "result", "[", "'learning_rate'", "]", "=", "lr", "\n", "result", "[", "'batch_size'", "]", "=", "args", ".", "train_batch_size", "\n", "if", "args", ".", "add_lstm", ":", "\n", "                                ", "logger", ".", "info", "(", "\"        p: %.2f, r: %.2f, f1: %.2f\"", "%", "(", "result", "[", "\"prec\"", "]", ",", "result", "[", "\"recall\"", "]", ",", "result", "[", "\"f1\"", "]", ")", ")", "\n", "", "if", "(", "best_result", "is", "None", ")", "or", "(", "result", "[", "args", ".", "eval_metric", "]", ">", "best_result", "[", "args", ".", "eval_metric", "]", ")", ":", "\n", "                                ", "best_result", "=", "result", "\n", "save_model", "=", "True", "\n", "logger", ".", "info", "(", "'Epoch: {}, Step: {} / {}, used_time = {:.2f}s, loss = {:.6f}'", ".", "format", "(", "\n", "epoch", ",", "step", "+", "1", ",", "len", "(", "train_batches", ")", ",", "time", ".", "time", "(", ")", "-", "start_time", ",", "tr_loss", "/", "nb_tr_steps", ")", ")", "\n", "logger", ".", "info", "(", "\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f, p_i: %.2f, r_i: %.2f, f1_i: %.2f\"", "%", "\n", "(", "args", ".", "eval_metric", ",", "str", "(", "lr", ")", ",", "epoch", ",", "result", "[", "\"prec_c\"", "]", ",", "result", "[", "\"recall_c\"", "]", ",", "result", "[", "\"f1_c\"", "]", ",", "result", "[", "\"prec_i\"", "]", ",", "result", "[", "\"recall_i\"", "]", ",", "result", "[", "\"f1_i\"", "]", ")", ")", "\n", "", "", "else", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "if", "(", "int", "(", "args", ".", "num_train_epochs", ")", "-", "epoch", "<", "3", "and", "(", "step", "+", "1", ")", "/", "len", "(", "train_batches", ")", ">", "0.7", ")", "or", "step", "==", "0", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "else", ":", "\n", "                            ", "save_model", "=", "False", "\n", "", "if", "save_model", ":", "\n", "                            ", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "subdir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch{epoch}-step{step}\"", ".", "format", "(", "epoch", "=", "epoch", ",", "step", "=", "step", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "subdir", ")", ":", "\n", "                                ", "os", ".", "makedirs", "(", "subdir", ")", "\n", "", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "subdir", ",", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "subdir", ",", "CONFIG_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "model_to_save", ".", "config", ".", "to_json_file", "(", "output_config_file", ")", "\n", "tokenizer", ".", "save_vocabulary", "(", "subdir", ")", "\n", "if", "best_result", ":", "\n", "                                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "# for key in sorted(best_result.keys()):", "\n", "                                    ", "for", "key", "in", "best_result", ":", "\n", "                                        ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "best_result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "", "", "", "", "del", "model", "\n", "\n", "", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "if", "args", ".", "eval_test", ":", "\n", "            ", "eval_examples", ",", "eval_features", "=", "read_ace_examples", "(", "nth_query", "=", "args", ".", "nth_query", ",", "input_file", "=", "args", ".", "test_file", ",", "tokenizer", "=", "tokenizer", ",", "category_vocab", "=", "category_vocab", ",", "is_training", "=", "False", ")", "\n", "if", "args", ".", "add_lstm", ":", "\n", "                ", "eval_features", "=", "sorted", "(", "eval_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ",", "reverse", "=", "True", ")", "\n", "", "logger", ".", "info", "(", "\"***** Test *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_sentence_id", "=", "torch", ".", "tensor", "(", "[", "f", ".", "sentence_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segmend_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_in_sentence", "=", "torch", ".", "tensor", "(", "[", "f", ".", "in_sentence", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "# all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)", "\n", "eval_data", "=", "TensorDataset", "(", "all_sentence_id", ",", "all_input_ids", ",", "all_segmend_ids", ",", "all_in_sentence", ",", "all_input_mask", ",", "all_labels", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "# BertForTriggerClassification.from_pretrained(args.model, cache_dir=PYTORCH_PRETRAINED_BERT_CACHE, num_labels=len(category_vocab.index_to_category))", "\n", "", "if", "args", ".", "add_lstm", ":", "\n", "            ", "model", "=", "BertLSTMForTriggerClassification", ".", "from_pretrained", "(", "args", ".", "model_dir", ",", "num_labels", "=", "len", "(", "category_vocab", ".", "index_to_category", ")", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "BertForTriggerClassification", ".", "from_pretrained", "(", "args", ".", "model_dir", ",", "num_labels", "=", "len", "(", "category_vocab", ".", "index_to_category", ")", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "result", ",", "preds", "=", "evaluate", "(", "args", ",", "eval_examples", ",", "category_vocab", ",", "model", ",", "device", ",", "eval_dataloader", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"test_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "key", "in", "result", ":", "\n", "                ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"trigger_predictions.json\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "line", "in", "preds", ":", "\n", "                ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "line", ",", "default", "=", "int", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.AceExample.__init__": [[39, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentence", ",", "events", ",", "s_start", ")", ":", "\n", "        ", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "events", "=", "events", "\n", "self", ".", "s_start", "=", "s_start", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.AceExample.__str__": [[44, 46], ["run_args_qa_thresh.AceExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.AceExample.__repr__": [[47, 59], ["event_triggers.append", "event_triggers.append", "event_triggers.append", "event_triggers.append", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"event sentence: %s\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "sentence", ")", ")", "\n", "event_triggers", "=", "[", "]", "\n", "for", "event", "in", "self", ".", "events", ":", "\n", "            ", "if", "event", ":", "\n", "                ", "event_triggers", ".", "append", "(", "self", ".", "sentence", "[", "event", "[", "0", "]", "[", "0", "]", "-", "self", ".", "s_start", "]", ")", "\n", "event_triggers", ".", "append", "(", "event", "[", "0", "]", "[", "1", "]", ")", "\n", "event_triggers", ".", "append", "(", "str", "(", "event", "[", "0", "]", "[", "0", "]", "-", "self", ".", "s_start", ")", ")", "\n", "event_triggers", ".", "append", "(", "\"|\"", ")", "\n", "", "", "s", "+=", "\" ||| event triggers: %s\"", "%", "(", "\" \"", ".", "join", "(", "event_triggers", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.InputFeatures.__init__": [[64, 84], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "example_id", ",", "tokens", ",", "token_to_orig_map", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "\n", "#", "\n", "event_type", ",", "argument_type", ",", "fea_trigger_offset", ",", "\n", "#", "\n", "start_position", "=", "None", ",", "end_position", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "example_id", "=", "example_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "if_trigger_ids", "=", "if_trigger_ids", "\n", "\n", "self", ".", "event_type", "=", "event_type", "\n", "self", ".", "argument_type", "=", "argument_type", "\n", "self", ".", "fea_trigger_offset", "=", "fea_trigger_offset", "\n", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples": [[86, 97], ["io.open", "json.loads", "run_args_qa_thresh.AceExample", "examples.append"], "function", ["None"], ["", "", "def", "read_ace_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Read a ACE json file into a list of AceExample.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "sentence", ",", "events", ",", "s_start", "=", "example", "[", "\"sentence\"", "]", ",", "example", "[", "\"event\"", "]", ",", "example", "[", "\"s_start\"", "]", "\n", "example", "=", "AceExample", "(", "sentence", "=", "sentence", ",", "events", "=", "events", ",", "s_start", "=", "s_start", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features": [[99, 179], ["enumerate", "query.replace.replace", "tokens.append", "segment_ids.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "enumerate", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "tokens.append", "segment_ids.append", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "features.append", "features.append", "run_args_qa_thresh.InputFeatures", "len", "features.append", "run_args_qa_thresh.InputFeatures", "run_args_qa_thresh.InputFeatures"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "query_templates", ",", "nth_query", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "for", "event", "in", "example", ".", "events", ":", "\n", "            ", "trigger_offset", "=", "event", "[", "0", "]", "[", "0", "]", "-", "example", ".", "s_start", "\n", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "trigger_token", "=", "example", ".", "sentence", "[", "trigger_offset", "]", "\n", "arguments", "=", "event", "[", "1", ":", "]", "\n", "for", "argument_type", "in", "query_templates", "[", "event_type", "]", ":", "\n", "\n", "                ", "query", "=", "query_templates", "[", "event_type", "]", "[", "argument_type", "]", "[", "nth_query", "]", "\n", "query", "=", "query", ".", "replace", "(", "\"[trigger]\"", ",", "trigger_token", ")", "\n", "\n", "# prepare [CLS] query [SEP] sentence [SEP]", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "# add [CLS]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add query", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "query", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# add sentence", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "sentence", ")", ":", "\n", "                    ", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "i", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "tokens", ".", "append", "(", "sub_tokens", "[", "0", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# add [SEP]", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# transform to input_ids ...", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "                    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "# start & end position", "\n", "", "start_position", ",", "end_position", "=", "None", ",", "None", "\n", "\n", "sentence_start", "=", "example", ".", "s_start", "\n", "sentence_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "fea_trigger_offset", "=", "trigger_offset", "+", "sentence_offset", "\n", "\n", "if_trigger_ids", "=", "[", "0", "]", "*", "len", "(", "segment_ids", ")", "\n", "if_trigger_ids", "[", "fea_trigger_offset", "]", "=", "1", "\n", "\n", "if", "is_training", ":", "\n", "                    ", "no_answer", "=", "True", "\n", "for", "argument", "in", "arguments", ":", "\n", "                        ", "gold_argument_type", "=", "argument", "[", "2", "]", "\n", "if", "gold_argument_type", "==", "argument_type", ":", "\n", "                            ", "no_answer", "=", "False", "\n", "answer_start", ",", "answer_end", "=", "argument", "[", "0", "]", ",", "argument", "[", "1", "]", "\n", "\n", "start_position", "=", "answer_start", "-", "sentence_start", "+", "sentence_offset", "\n", "end_position", "=", "answer_end", "-", "sentence_start", "+", "sentence_offset", "\n", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "if", "no_answer", ":", "\n", "                        ", "start_position", ",", "end_position", "=", "0", ",", "0", "\n", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "features", ".", "append", "(", "InputFeatures", "(", "example_id", "=", "example_id", ",", "tokens", "=", "tokens", ",", "token_to_orig_map", "=", "token_to_orig_map", ",", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "if_trigger_ids", "=", "if_trigger_ids", ",", "\n", "event_type", "=", "event_type", ",", "argument_type", "=", "argument_type", ",", "fea_trigger_offset", "=", "fea_trigger_offset", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ")", ")", "\n", "", "", "", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_query_templates": [[181, 217], ["dict", "io.open", "io.open", "line.strip().split", "event_arg.split", "[].append", "[].append", "[].append", "[].append", "line.strip().split", "event_arg.split", "[].append", "[].append", "dict", "list", "len", "line.strip", "line.strip"], "function", ["None"], ["", "def", "read_query_templates", "(", "normal_file", ",", "des_file", ")", ":", "\n", "    ", "\"\"\"Load query templates\"\"\"", "\n", "query_templates", "=", "dict", "(", ")", "\n", "with", "open", "(", "normal_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "event_arg", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "event_type", ",", "arg_name", "=", "event_arg", ".", "split", "(", "\"_\"", ")", "\n", "\n", "if", "event_type", "not", "in", "query_templates", ":", "\n", "                ", "query_templates", "[", "event_type", "]", "=", "dict", "(", ")", "\n", "", "if", "arg_name", "not", "in", "query_templates", "[", "event_type", "]", ":", "\n", "                ", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", "=", "list", "(", ")", "\n", "\n", "# 0 template arg_name", "\n", "", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "arg_name", ")", "\n", "# 1 template arg_name + in trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "arg_name", "+", "\" in [trigger]\"", ")", "\n", "# 2 template arg_query", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", ")", "\n", "# 3 arg_query + trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", "[", ":", "-", "1", "]", "+", "\" in [trigger]?\"", ")", "\n", "\n", "", "", "with", "open", "(", "des_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "event_arg", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "event_type", ",", "arg_name", "=", "event_arg", ".", "split", "(", "\"_\"", ")", "\n", "# 4 template des_query", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", ")", "\n", "# 5 template des_query + trigger (replace [trigger] when forming the instance)", "\n", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ".", "append", "(", "query", "[", ":", "-", "1", "]", "+", "\" in [trigger]?\"", ")", "\n", "\n", "", "", "for", "event_type", "in", "query_templates", ":", "\n", "        ", "for", "arg_name", "in", "query_templates", "[", "event_type", "]", ":", "\n", "            ", "assert", "len", "(", "query_templates", "[", "event_type", "]", "[", "arg_name", "]", ")", "==", "6", "\n", "\n", "", "", "return", "query_templates", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.make_predictions": [[221, 294], ["collections.defaultdict", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_id_to_features[].append", "example_id_to_results[].append", "collections.OrderedDict", "enumerate", "str", "run_args_qa_thresh._get_best_indexes", "run_args_qa_thresh._get_best_indexes", "sorted", "enumerate", "sorted.append", "final_all_predictions[].append", "sorted.append", "collections.namedtuple.", "collections.namedtuple.", "len", "len"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes"], ["def", "make_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "larger_than_cls", ")", ":", "\n", "    ", "example_id_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_id_to_features", "[", "feature", ".", "example_id", "]", ".", "append", "(", "feature", ")", "\n", "", "example_id_to_results", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "example_id_to_results", "[", "result", ".", "example_id", "]", ".", "append", "(", "result", ")", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "\"PrelimPrediction\"", ",", "\n", "[", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "final_all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "# all_nbest_json = collections.OrderedDict()", "\n", "# scores_diff_json = collections.OrderedDict()", "\n", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_id_to_features", "[", "example_id", "]", "\n", "results", "=", "example_id_to_results", "[", "example_id", "]", "\n", "all_predictions", "[", "example_id", "]", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "final_all_predictions", "[", "example_id", "]", "=", "[", "]", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "event_type_argument_type", "=", "\"_\"", ".", "join", "(", "[", "feature", ".", "event_type", ",", "feature", ".", "argument_type", "]", ")", "\n", "event_type_offset_argument_type", "=", "\"_\"", ".", "join", "(", "[", "feature", ".", "event_type", ",", "str", "(", "feature", ".", "token_to_orig_map", "[", "feature", ".", "fea_trigger_offset", "]", ")", ",", "feature", ".", "argument_type", "]", ")", "\n", "\n", "start_indexes", ",", "end_indexes", "=", "None", ",", "None", "\n", "prelim_predictions", "=", "[", "]", "\n", "for", "result", "in", "results", ":", "\n", "                ", "if", "result", ".", "event_type_offset_argument_type", "==", "event_type_offset_argument_type", ":", "\n", "                    ", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ",", "larger_than_cls", ",", "result", ".", "start_logits", "[", "0", "]", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ",", "larger_than_cls", ",", "result", ".", "end_logits", "[", "0", "]", ")", "\n", "# add span preds", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                        ", "for", "end_index", "in", "end_indexes", ":", "\n", "                            ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", "or", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                                ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", "or", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                                ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                                ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                                ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "start_index", "=", "start_index", ",", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "## add null pred", "\n", "", "", "if", "not", "larger_than_cls", ":", "\n", "                        ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "0", "]", "\n", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "start_index", "=", "0", ",", "end_index", "=", "0", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "0", "]", ",", "end_logit", "=", "result", ".", "end_logits", "[", "0", "]", ")", ")", "\n", "\n", "## sort", "\n", "", "prelim_predictions", "=", "sorted", "(", "prelim_predictions", ",", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "reverse", "=", "True", ")", "\n", "# print(len(prelim_predictions))", "\n", "# if len(prelim_predictions) > 0:", "\n", "#     print(prelim_predictions[0].start_logit + prelim_predictions[0].end_logit - result.start_logits[0] - result.end_logits[0])", "\n", "#     print(prelim_predictions[0].start_index, prelim_predictions[0].end_index)", "\n", "# all_predictions[example_id][event_type_offset_argument_type] = prelim_predictions", "\n", "\n", "## get final pred in format: [event_type_offset_argument_type, [start_offset, end_offset]]", "\n", "max_num_pred_per_arg", "=", "4", "\n", "for", "idx", ",", "pred", "in", "enumerate", "(", "prelim_predictions", ")", ":", "\n", "                        ", "if", "(", "idx", "+", "1", ")", ">", "max_num_pred_per_arg", ":", "break", "\n", "if", "pred", ".", "start_index", "==", "0", "and", "pred", ".", "end_index", "==", "0", ":", "break", "\n", "orig_sent_start", ",", "orig_sent_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", ",", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "na_prob", "=", "(", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "0", "]", ")", "-", "(", "pred", ".", "start_logit", "+", "pred", ".", "end_logit", ")", "\n", "final_all_predictions", "[", "example_id", "]", ".", "append", "(", "[", "event_type_argument_type", ",", "[", "orig_sent_start", ",", "orig_sent_end", "]", ",", "na_prob", "]", ")", "\n", "# final_all_predictions[example_id].append([event_type_argument_type, [orig_sent_start, orig_sent_end]])", "\n", "\n", "", "", "", "", "", "return", "final_all_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh._get_best_indexes": [[296, 309], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", "=", "1", ",", "larger_than_cls", "=", "False", ",", "cls_logit", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "if", "larger_than_cls", ":", "\n", "            ", "if", "index_and_score", "[", "i", "]", "[", "1", "]", "<", "cls_logit", ":", "\n", "                ", "break", "\n", "", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.find_best_thresh": [[310, 344], ["len", "candidate_preds.append"], "function", ["None"], ["", "def", "find_best_thresh", "(", "new_preds", ",", "new_all_gold", ")", ":", "\n", "    ", "best_score", "=", "0", "\n", "best_na_thresh", "=", "0", "\n", "gold_arg_n", ",", "pred_arg_n", "=", "len", "(", "new_all_gold", ")", ",", "0", "\n", "\n", "candidate_preds", "=", "[", "]", "\n", "for", "argument", "in", "new_preds", ":", "\n", "        ", "candidate_preds", ".", "append", "(", "argument", "[", ":", "-", "2", "]", "+", "argument", "[", "-", "1", ":", "]", ")", "\n", "pred_arg_n", "+=", "1", "\n", "\n", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", "\n", "# pred_in_gold_n", "\n", "for", "argu", "in", "candidate_preds", ":", "\n", "            ", "if", "argu", "in", "new_all_gold", ":", "\n", "                ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argu", "in", "new_all_gold", ":", "\n", "            ", "if", "argu", "in", "candidate_preds", ":", "\n", "                ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "prec_c", ",", "recall_c", ",", "f1_c", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_c", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_c", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_c", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_c", "=", "0", "\n", "if", "prec_c", "or", "recall_c", ":", "f1_c", "=", "2", "*", "prec_c", "*", "recall_c", "/", "(", "prec_c", "+", "recall_c", ")", "\n", "else", ":", "f1_c", "=", "0", "\n", "\n", "if", "f1_c", ">", "best_score", ":", "\n", "            ", "best_score", "=", "f1_c", "\n", "best_na_thresh", "=", "argument", "[", "-", "2", "]", "\n", "\n", "# import ipdb; ipdb.set_trace()", "\n", "", "", "return", "best_na_thresh", "+", "1e-10", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate": [[345, 505], ["model.eval", "enumerate", "run_args_qa_thresh.make_predictions", "copy.deepcopy", "collections.OrderedDict", "enumerate", "enumerate", "sorted", "run_args_qa_thresh.find_best_thresh", "collections.OrderedDict", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "if_trigger_ids.to.to", "enumerate", "final_new_preds_identification.append", "new_all_gold_identification.append", "logger.info", "torch.no_grad", "batch_start_logits[].detach().cpu().tolist", "batch_end_logits[].detach().cpu().tolist", "all_results.append", "argument.append", "sorted.append", "argument.append", "new_all_gold.append", "final_new_preds.append", "model", "model", "RawResult", "all_gold[].append", "item[].split", "item[].split", "batch_start_logits[].detach().cpu", "batch_end_logits[].detach().cpu", "example_index.item", "str", "len", "batch_start_logits[].detach", "batch_end_logits[].detach"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.make_predictions", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.find_best_thresh"], ["", "def", "evaluate", "(", "args", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ",", "na_prob_thresh", "=", "1.0", ",", "pred_only", "=", "False", ")", ":", "\n", "    ", "all_results", "=", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "for", "idx", ",", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "example_indices", ")", "in", "enumerate", "(", "eval_dataloader", ")", ":", "\n", "        ", "if", "pred_only", "and", "idx", "%", "10", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Running test: %d / %d\"", "%", "(", "idx", ",", "len", "(", "eval_dataloader", ")", ")", ")", "\n", "", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "if_trigger_ids", "=", "if_trigger_ids", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                ", "batch_start_logits", ",", "batch_end_logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "", "else", ":", "\n", "                ", "batch_start_logits", ",", "batch_end_logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "if_trigger_ids", ",", "input_mask", ")", "\n", "", "", "for", "i", ",", "example_index", "in", "enumerate", "(", "example_indices", ")", ":", "\n", "            ", "start_logits", "=", "batch_start_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "end_logits", "=", "batch_end_logits", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "eval_feature", "=", "eval_features", "[", "example_index", ".", "item", "(", ")", "]", "\n", "example_id", "=", "eval_feature", ".", "example_id", "\n", "event_type_offset_argument_type", "=", "\"_\"", ".", "join", "(", "[", "eval_feature", ".", "event_type", ",", "str", "(", "eval_feature", ".", "token_to_orig_map", "[", "eval_feature", ".", "fea_trigger_offset", "]", ")", ",", "eval_feature", ".", "argument_type", "]", ")", "\n", "all_results", ".", "append", "(", "RawResult", "(", "example_id", "=", "example_id", ",", "event_type_offset_argument_type", "=", "event_type_offset_argument_type", ",", "\n", "start_logits", "=", "start_logits", ",", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "# preds, nbest_preds, na_probs = \\", "\n", "", "", "preds", "=", "make_predictions", "(", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "args", ".", "larger_than_cls", ")", "\n", "preds_init", "=", "copy", ".", "deepcopy", "(", "preds", ")", "\n", "\n", "# get all_gold in format: [event_type_argument_type, [start_offset, end_offset]]", "\n", "all_gold", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "example_id", ",", "example", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "all_gold", "[", "example_id", "]", "=", "[", "]", "\n", "for", "event", "in", "example", ".", "events", ":", "\n", "# if not event: continue", "\n", "            ", "trigger_offset", "=", "event", "[", "0", "]", "[", "0", "]", "-", "example", ".", "s_start", "\n", "event_type", "=", "event", "[", "0", "]", "[", "1", "]", "\n", "for", "argument", "in", "event", "[", "1", ":", "]", ":", "\n", "                ", "argument_start", ",", "argument_end", ",", "argument_type", "=", "argument", "[", "0", "]", "-", "example", ".", "s_start", ",", "argument", "[", "1", "]", "-", "example", ".", "s_start", ",", "argument", "[", "2", "]", "\n", "# event_type_offset_argument_type = \"_\".join([event_type, str(trigger_offset), argument_type])", "\n", "event_type_argument_type", "=", "\"_\"", ".", "join", "(", "[", "event_type", ",", "argument_type", "]", ")", "\n", "all_gold", "[", "example_id", "]", ".", "append", "(", "[", "event_type_argument_type", ",", "[", "argument_start", ",", "argument_end", "]", "]", ")", "\n", "\n", "# linearize the preds and all_gold", "\n", "", "", "", "new_preds", "=", "[", "]", "\n", "new_all_gold", "=", "[", "]", "\n", "for", "(", "example_id", ",", "_", ")", "in", "enumerate", "(", "gold_examples", ")", ":", "\n", "        ", "pred_arg", "=", "preds", "[", "example_id", "]", "\n", "gold_arg", "=", "all_gold", "[", "example_id", "]", "\n", "for", "argument", "in", "pred_arg", ":", "\n", "            ", "argument", ".", "append", "(", "example_id", ")", "\n", "new_preds", ".", "append", "(", "argument", ")", "\n", "", "for", "argument", "in", "gold_arg", ":", "\n", "            ", "argument", ".", "append", "(", "example_id", ")", "\n", "new_all_gold", ".", "append", "(", "argument", ")", "\n", "\n", "", "", "new_preds", "=", "sorted", "(", "new_preds", ",", "key", "=", "lambda", "x", ":", "x", "[", "-", "2", "]", ")", "\n", "# best_na_thresh = 0", "\n", "best_na_thresh", "=", "find_best_thresh", "(", "new_preds", ",", "new_all_gold", ")", "\n", "\n", "final_new_preds", "=", "[", "]", "\n", "for", "argument", "in", "new_preds", ":", "\n", "        ", "if", "argument", "[", "-", "2", "]", "<", "best_na_thresh", ":", "\n", "            ", "final_new_preds", ".", "append", "(", "argument", "[", ":", "-", "2", "]", "+", "argument", "[", "-", "1", ":", "]", ")", "# no na_prob", "\n", "\n", "################################################################################################################################################", "\n", "# # logging for DEBUG results", "\n", "# if pred_only:", "\n", "#     debug_preds = collections.OrderedDict()", "\n", "#     for example_id in preds:", "\n", "#         debug_preds[example_id] = []", "\n", "#         for argument in preds[example_id]:", "\n", "#             arg_new = argument[:-2] + argument[-1:]", "\n", "#             if arg_new in final_new_preds:", "\n", "#                 debug_preds[example_id].append(argument)", "\n", "#         debug_preds[example_id] = sorted(debug_preds[example_id], key=lambda x: x[2])", "\n", "\n", "#     # import ipdb; ipdb.set_trace()", "\n", "#     for (example_id, example) in enumerate(gold_examples):", "\n", "#         # if example_id > 120: break", "\n", "#         if debug_preds[example_id] or all_gold[example_id]:", "\n", "#             token_idx = []", "\n", "#             for idx, token in enumerate(example.sentence): token_idx.append(\" \".join([token, str(idx)]))", "\n", "#             logger.info(\"sent: {}\".format(\" | \".join(token_idx)))", "\n", "\n", "#             logger.info(\"trigger: {}\".format(str(example).split(\"|||\")[1]))", "\n", "\n", "#             gold_str_list = [] ", "\n", "#             for gold in all_gold[example_id]: gold_str_list.append(\" \".join([gold[0], str(gold[1][0]), str(gold[1][1])]))", "\n", "#             logger.info(\"gold: {}\".format(\" | \".join(gold_str_list)))", "\n", "\n", "#             pred_str_list = [] ", "\n", "#             for pred in debug_preds[example_id]: pred_str_list.append(\" \".join([pred[0], str(pred[1][0]), str(pred[1][1]), str(pred[2])]))", "\n", "#             logger.info(\"pred: {} \\n\".format(\" | \".join(pred_str_list)))", "\n", "\n", "################################################################################################################################################", "\n", "\n", "# get results (classification)", "\n", "", "", "gold_arg_n", ",", "pred_arg_n", ",", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "# pred_arg_n", "\n", "for", "argument", "in", "final_new_preds", ":", "pred_arg_n", "+=", "1", "\n", "# gold_arg_n     ", "\n", "for", "argument", "in", "new_all_gold", ":", "gold_arg_n", "+=", "1", "\n", "# pred_in_gold_n", "\n", "for", "argument", "in", "final_new_preds", ":", "\n", "        ", "if", "argument", "in", "new_all_gold", ":", "\n", "            ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argument", "in", "new_all_gold", ":", "\n", "        ", "if", "argument", "in", "final_new_preds", ":", "\n", "            ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "prec_c", ",", "recall_c", ",", "f1_c", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_c", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_c", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_c", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_c", "=", "0", "\n", "if", "prec_c", "or", "recall_c", ":", "f1_c", "=", "2", "*", "prec_c", "*", "recall_c", "/", "(", "prec_c", "+", "recall_c", ")", "\n", "else", ":", "f1_c", "=", "0", "\n", "# import ipdb; ipdb.set_trace()", "\n", "\n", "################################################################################################################################################", "\n", "# get results (identification)", "\n", "final_new_preds_identification", "=", "[", "]", "\n", "for", "item", "in", "final_new_preds", ":", "\n", "        ", "new_item", "=", "[", "item", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "]", "# event_type", "\n", "new_item", "+=", "item", "[", "1", ":", "]", "# offset and example_id", "\n", "final_new_preds_identification", ".", "append", "(", "new_item", ")", "\n", "\n", "", "new_all_gold_identification", "=", "[", "]", "\n", "for", "item", "in", "new_all_gold", ":", "\n", "        ", "new_item", "=", "[", "item", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "]", "# event_type", "\n", "new_item", "+=", "item", "[", "1", ":", "]", "# offset and example_id", "\n", "new_all_gold_identification", ".", "append", "(", "new_item", ")", "\n", "\n", "", "gold_arg_n", ",", "pred_arg_n", ",", "pred_in_gold_n", ",", "gold_in_pred_n", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "# pred_arg_n", "\n", "for", "argument", "in", "final_new_preds_identification", ":", "pred_arg_n", "+=", "1", "\n", "# gold_arg_n     ", "\n", "for", "argument", "in", "new_all_gold_identification", ":", "gold_arg_n", "+=", "1", "\n", "# pred_in_gold_n", "\n", "for", "argument", "in", "final_new_preds_identification", ":", "\n", "        ", "if", "argument", "in", "new_all_gold_identification", ":", "\n", "            ", "pred_in_gold_n", "+=", "1", "\n", "# gold_in_pred_n", "\n", "", "", "for", "argument", "in", "new_all_gold_identification", ":", "\n", "        ", "if", "argument", "in", "final_new_preds_identification", ":", "\n", "            ", "gold_in_pred_n", "+=", "1", "\n", "\n", "", "", "prec_i", ",", "recall_i", ",", "f1_i", "=", "0", ",", "0", ",", "0", "\n", "if", "pred_arg_n", "!=", "0", ":", "prec_i", "=", "100.0", "*", "pred_in_gold_n", "/", "pred_arg_n", "\n", "else", ":", "prec_c", "=", "0", "\n", "if", "gold_arg_n", "!=", "0", ":", "recall_i", "=", "100.0", "*", "gold_in_pred_n", "/", "gold_arg_n", "\n", "else", ":", "recall_i", "=", "0", "\n", "if", "prec_i", "or", "recall_i", ":", "f1_i", "=", "2", "*", "prec_i", "*", "recall_i", "/", "(", "prec_i", "+", "recall_i", ")", "\n", "else", ":", "f1_i", "=", "0", "\n", "\n", "\n", "result", "=", "collections", ".", "OrderedDict", "(", "[", "(", "'prec_c'", ",", "prec_c", ")", ",", "(", "'recall_c'", ",", "recall_c", ")", ",", "(", "'f1_c'", ",", "f1_c", ")", ",", "(", "'prec_i'", ",", "prec_i", ")", ",", "(", "'recall_i'", ",", "recall_i", ")", ",", "(", "'f1_i'", ",", "f1_i", ")", ",", "(", "'best_na_thresh'", ",", "best_na_thresh", ")", "]", ")", "\n", "return", "result", ",", "preds_init", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.main": [[507, 739], ["torch.device", "torch.cuda.device_count", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "logger.info", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "run_args_qa_thresh.read_query_templates", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.exists", "os.makedirs", "logger.addHandler", "logger.addHandler", "run_args_qa_thresh.read_ace_examples", "run_args_qa_thresh.read_ace_examples", "run_args_qa_thresh.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "run_args_qa_thresh.read_ace_examples", "run_args_qa_thresh.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "max", "torch.nn.DataParallel.to", "run_args_qa_thresh.evaluate", "logging.FileHandler", "logging.FileHandler", "len", "len", "torch.tensor.size", "sorted", "random.shuffle", "len", "len", "torch.nn.DataParallel.to", "list", "pytorch_pretrained_bert.optimization.BertAdam", "time.time", "range", "run_args_qa_thresh.read_ace_examples", "run_args_qa_thresh.read_ace_examples", "run_args_qa_thresh.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering.from_pretrained", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.from_pretrained", "torch.nn.DataParallel.half", "io.open", "io.open", "torch.cuda.is_available", "os.path.join", "os.path.join", "len", "len", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering.from_pretrained", "pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.from_pretrained", "torch.nn.DataParallel.half", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel.train", "logger.info", "enumerate", "len", "len", "torch.tensor.size", "os.path.join", "writer.write", "os.path.join", "writer.write", "random.shuffle", "loss.mean.item", "input_ids.size", "loss.mean.backward", "numpy.sum", "tuple", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "loss.mean.mean", "pytorch_pretrained_bert.optimization.BertAdam.step", "pytorch_pretrained_bert.optimization.BertAdam.zero_grad", "json.dumps", "any", "run_args_qa_thresh.evaluate", "torch.nn.DataParallel.train", "os.path.join", "os.path.join", "os.path.join", "torch.save", "model_to_save.config.to_json_file", "BertTokenizer.from_pretrained.save_vocabulary", "str", "any", "t.to", "logger.info", "logger.info", "hasattr", "os.path.exists", "os.makedirs", "model_to_save.state_dict", "io.open", "sorted", "len", "int", "len", "os.path.join", "best_result.keys", "writer.write", "time.time", "str", "str"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_query_templates", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.read_ace_examples", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.convert_examples_to_features", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.xinyadu_eeqa.code.run_args_qa_thresh.evaluate", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_file", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.save_vocabulary"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "logger", ".", "info", "(", "\"device: {}, n_gpu: {}, 16-bits training: {}\"", ".", "format", "(", "device", ",", "n_gpu", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "(", "args", ".", "train_file", "is", "not", "None", ")", "and", "(", "args", ".", "dev_file", "is", "not", "None", ")", "\n", "\n", "", "if", "args", ".", "eval_test", ":", "\n", "        ", "assert", "args", ".", "test_file", "is", "not", "None", "\n", "", "else", ":", "\n", "        ", "assert", "args", ".", "dev_file", "is", "not", "None", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train.log\"", ")", ",", "'w'", ")", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval.log\"", ")", ",", "'w'", ")", ")", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "# read query templates", "\n", "query_templates", "=", "read_query_templates", "(", "normal_file", "=", "args", ".", "normal_file", ",", "des_file", "=", "args", ".", "des_file", ")", "\n", "\n", "if", "args", ".", "do_train", "or", "(", "not", "args", ".", "eval_test", ")", ":", "\n", "        ", "eval_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "dev_file", ",", "is_training", "=", "False", ")", "\n", "gold_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "gold_file", ",", "is_training", "=", "False", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"***** Dev *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "all_example_index", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "train_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "True", ")", "\n", "\n", "if", "args", ".", "train_mode", "==", "'sorted'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "            ", "train_features", "=", "sorted", "(", "train_features", ",", "key", "=", "lambda", "f", ":", "np", ".", "sum", "(", "f", ".", "input_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "train_features", ")", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_start_positions", "=", "torch", ".", "tensor", "(", "[", "f", ".", "start_position", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_end_positions", "=", "torch", ".", "tensor", "(", "[", "f", ".", "end_position", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "\n", "all_start_positions", ",", "all_end_positions", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "train_batches", "=", "[", "batch", "for", "batch", "in", "train_dataloader", "]", "\n", "\n", "num_train_optimization_steps", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "logger", ".", "info", "(", "\"***** Train *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "train_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "eval_step", "=", "max", "(", "1", ",", "len", "(", "train_batches", ")", "//", "args", ".", "eval_per_epoch", ")", "\n", "best_result", "=", "None", "\n", "lrs", "=", "[", "args", ".", "learning_rate", "]", "if", "args", ".", "learning_rate", "else", "[", "1e-6", ",", "2e-6", ",", "3e-6", ",", "5e-6", ",", "1e-5", ",", "2e-5", ",", "3e-5", ",", "5e-5", "]", "\n", "for", "lr", "in", "lrs", ":", "\n", "            ", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                ", "model", "=", "BertForQuestionAnswering", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ")", "\n", "", "else", ":", "\n", "                ", "model", "=", "BertForQuestionAnswering_withIfTriggerEmbedding", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "[", "n", "for", "n", "in", "param_optimizer", "if", "'pooler'", "not", "in", "n", "[", "0", "]", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "tr_loss", "=", "0", "\n", "nb_tr_examples", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "global_step", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "                ", "model", ".", "train", "(", ")", "\n", "logger", ".", "info", "(", "\"Start epoch #{} (lr = {})...\"", ".", "format", "(", "epoch", ",", "lr", ")", ")", "\n", "if", "args", ".", "train_mode", "==", "'random'", "or", "args", ".", "train_mode", "==", "'random_sorted'", ":", "\n", "                    ", "random", ".", "shuffle", "(", "train_batches", ")", "\n", "", "for", "step", ",", "batch", "in", "enumerate", "(", "train_batches", ")", ":", "\n", "                    ", "if", "n_gpu", "==", "1", ":", "\n", "                        ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "if_trigger_ids", ",", "start_positions", ",", "end_positions", "=", "batch", "\n", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "                        ", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "if_trigger_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                        ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                        ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "if", "(", "step", "+", "1", ")", "%", "eval_step", "==", "0", "or", "step", "==", "0", ":", "\n", "                        ", "save_model", "=", "False", "\n", "if", "args", ".", "do_eval", ":", "\n", "# result, _, _ = evaluate(args, model, device, eval_dataset, eval_dataloader, eval_examples, eval_features)", "\n", "                            ", "result", ",", "preds", "=", "evaluate", "(", "args", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ")", "\n", "# import ipdb; ipdb.set_trace()", "\n", "model", ".", "train", "(", ")", "\n", "result", "[", "'global_step'", "]", "=", "global_step", "\n", "result", "[", "'epoch'", "]", "=", "epoch", "\n", "result", "[", "'learning_rate'", "]", "=", "lr", "\n", "result", "[", "'batch_size'", "]", "=", "args", ".", "train_batch_size", "\n", "if", "(", "best_result", "is", "None", ")", "or", "(", "result", "[", "args", ".", "eval_metric", "]", ">", "best_result", "[", "args", ".", "eval_metric", "]", ")", ":", "\n", "                                ", "best_result", "=", "result", "\n", "save_model", "=", "True", "\n", "logger", ".", "info", "(", "'Epoch: {}, Step: {} / {}, used_time = {:.2f}s, loss = {:.6f}'", ".", "format", "(", "\n", "epoch", ",", "step", "+", "1", ",", "len", "(", "train_batches", ")", ",", "time", ".", "time", "(", ")", "-", "start_time", ",", "tr_loss", "/", "nb_tr_steps", ")", ")", "\n", "logger", ".", "info", "(", "\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f, p_i: %.2f, r_i: %.2f, f1_i: %.2f, best_na_thresh: %.5f\"", "%", "\n", "# logger.info(\"!!! Best dev %s (lr=%s, epoch=%d): p_c: %.2f, r_c: %.2f, f1_c: %.2f, best_na_thresh: %.10f\" %", "\n", "# (args.eval_metric, str(lr), epoch, result[\"prec_c\"], result[\"recall_c\"], result[\"f1_c\"], result[\"best_na_thresh\"]))", "\n", "(", "args", ".", "eval_metric", ",", "str", "(", "lr", ")", ",", "epoch", ",", "result", "[", "\"prec_c\"", "]", ",", "result", "[", "\"recall_c\"", "]", ",", "result", "[", "\"f1_c\"", "]", ",", "result", "[", "\"prec_i\"", "]", ",", "result", "[", "\"recall_i\"", "]", ",", "result", "[", "\"f1_i\"", "]", ",", "result", "[", "\"best_na_thresh\"", "]", ")", ")", "\n", "", "", "else", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "if", "(", "int", "(", "args", ".", "num_train_epochs", ")", "-", "epoch", "<", "3", "and", "(", "step", "+", "1", ")", "/", "len", "(", "train_batches", ")", ">", "0.7", ")", "or", "step", "==", "0", ":", "\n", "                            ", "save_model", "=", "True", "\n", "", "else", ":", "\n", "                            ", "save_model", "=", "False", "\n", "", "if", "save_model", ":", "\n", "                            ", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "subdir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch{epoch}-step{step}\"", ".", "format", "(", "epoch", "=", "epoch", ",", "step", "=", "step", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "subdir", ")", ":", "\n", "                                ", "os", ".", "makedirs", "(", "subdir", ")", "\n", "", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "subdir", ",", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "subdir", ",", "CONFIG_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "model_to_save", ".", "config", ".", "to_json_file", "(", "output_config_file", ")", "\n", "tokenizer", ".", "save_vocabulary", "(", "subdir", ")", "\n", "if", "best_result", ":", "\n", "                                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                                    ", "for", "key", "in", "sorted", "(", "best_result", ".", "keys", "(", ")", ")", ":", "\n", "                                        ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "best_result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "", "", "", "", "", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "if", "args", ".", "eval_test", ":", "\n", "            ", "eval_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "test_file", ",", "is_training", "=", "False", ")", "\n", "gold_examples", "=", "read_ace_examples", "(", "input_file", "=", "args", ".", "gold_file", ",", "is_training", "=", "False", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "query_templates", "=", "query_templates", ",", "\n", "nth_query", "=", "args", ".", "nth_query", ",", "\n", "is_training", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"***** Test *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_if_trigger_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "if_trigger_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index", "=", "torch", ".", "arange", "(", "all_input_ids", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_if_trigger_ids", ",", "all_example_index", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "", "if", "not", "args", ".", "add_if_trigger_embedding", ":", "\n", "            ", "model", "=", "BertForQuestionAnswering", ".", "from_pretrained", "(", "args", ".", "model_dir", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "BertForQuestionAnswering_withIfTriggerEmbedding", ".", "from_pretrained", "(", "args", ".", "model_dir", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "result", ",", "preds", "=", "evaluate", "(", "args", ",", "model", ",", "device", ",", "eval_dataloader", ",", "eval_examples", ",", "gold_examples", ",", "eval_features", ",", "pred_only", "=", "True", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"test_results.txt\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "key", "in", "result", ":", "\n", "                ", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"arg_predictions.json\"", ")", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "key", "in", "preds", ":", "\n", "                ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "preds", "[", "key", "]", ",", "default", "=", "int", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.__init__": [[139, 195], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.from_dict": [[197, 204], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.from_json_file": [[205, 211], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.__repr__": [[212, 214], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_dict": [[215, 219], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_string": [[220, 223], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_file": [[224, 228], ["io.open", "writer.write", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.to_json_string"], ["", "def", "to_json_file", "(", "self", ",", "json_file_path", ")", ":", "\n", "        ", "\"\"\" Save this instance to a json file.\"\"\"", "\n", "with", "open", "(", "json_file_path", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLayerNorm.__init__": [[234, 241], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BertLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLayerNorm.forward": [[242, 247], ["x.mean", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "weight", "*", "x", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertEmbeddings.__init__": [[251, 261], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertEmbeddings.forward": [[262, 277], ["input_ids.size", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfAttention.__init__": [[280, 295], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores": [[296, 300], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfAttention.forward": [[301, 331], ["modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.clamp", "modeling.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "# assert not torch.isnan(attention_scores).any()", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# assert not torch.isnan(attention_scores).any()", "\n", "attention_scores", "=", "torch", ".", "clamp", "(", "attention_scores", ",", "-", "10000.", ",", "10000.", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfOutput.__init__": [[334, 339], ["torch.nn.Module.__init__", "torch.nn.Linear", "modeling.BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertSelfOutput.forward": [[340, 345], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertAttention.__init__": [[348, 352], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertAttention.forward": [[353, 357], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertIntermediate.__init__": [[360, 367], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertIntermediate.forward": [[368, 372], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertOutput.__init__": [[375, 380], ["torch.nn.Module.__init__", "torch.nn.Linear", "modeling.BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertOutput.forward": [[381, 386], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLayer.__init__": [[389, 394], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLayer.forward": [[395, 400], ["modeling.BertLayer.attention", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertEncoder.__init__": [[403, 407], ["torch.nn.Module.__init__", "modeling.BertLayer", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertEncoder.forward": [[408, 417], ["layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPooler.__init__": [[420, 424], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPooler.forward": [[425, 432], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.__init__": [[435, 443], ["torch.nn.Module.__init__", "torch.nn.Linear", "modeling.BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.forward": [[444, 449], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLMPredictionHead.__init__": [[452, 463], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLMPredictionHead.forward": [[464, 468], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.__init__": [[471, 474], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.forward": [[475, 478], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.__init__": [[481, 484], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.forward": [[485, 488], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.__init__": [[491, 495], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.forward": [[496, 500], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPreTrainedModel.__init__": [[506, 516], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPreTrainedModel.init_bert_weights": [[517, 529], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertPreTrainedModel.from_pretrained": [[530, 657], ["os.path.join", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling.BertPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", "or", "from_tf", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "config_file", ")", ":", "\n", "# Backward compatibility with old naming format", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "BERT_CONFIG_NAME", ")", "\n", "", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "if", "pretrained_model_name_or_path", "==", "'bert-base-uncased-1024'", ":", "\n", "            ", "config", ".", "max_position_embeddings", "=", "1024", "\n", "", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate model.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ",", "map_location", "=", "'cpu'", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "TF_WEIGHTS_NAME", ")", "\n", "return", "load_tf_weights_in_bert", "(", "model", ",", "weights_path", ")", "\n", "# Load from a PyTorch state_dict", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "key", "[", "8", ":", "]", "if", "key", ".", "startswith", "(", "\"decoder.\"", ")", "else", "key", "\n", "if", "'gamma'", "in", "new_key", ":", "\n", "                ", "new_key", "=", "new_key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "new_key", ":", "\n", "                ", "new_key", "=", "new_key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "key", "!=", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "start_prefix", "=", "''", "\n", "if", "not", "hasattr", "(", "model", ",", "'bert'", ")", "and", "any", "(", "s", ".", "startswith", "(", "'bert.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "'bert.'", "\n", "", "load", "(", "model", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertModel.__init__": [[703, 709], ["modeling.BertPreTrainedModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertModel.forward": [[710, 740], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler", "torch.ones_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "next", "modeling.BertModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForPreTraining.__init__": [[792, 797], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForPreTraining.forward": [[798, 811], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForMaskedLM.__init__": [[855, 860], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "modeling.BertOnlyMLMHead", "modeling.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForMaskedLM.forward": [[861, 872], ["modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.__init__": [[917, 922], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "modeling.BertOnlyNSPHead", "modeling.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.forward": [[923, 934], ["modeling.BertForNextSentencePrediction.bert", "modeling.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForSequenceClassification.__init__": [[981, 988], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForSequenceClassification.forward": [[989, 1000], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForMultipleChoice.__init__": [[1046, 1053], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForMultipleChoice.forward": [[1054, 1069], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "modeling.BertForMultipleChoice.bert", "modeling.BertForMultipleChoice.dropout", "modeling.BertForMultipleChoice.classifier", "modeling.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForTokenClassification.__init__": [[1116, 1123], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForTokenClassification.forward": [[1124, 1142], ["modeling.BertForTokenClassification.bert", "modeling.BertForTokenClassification.dropout", "modeling.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "modeling.BertForTokenClassification.view", "labels.view", "modeling.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForTriggerClassification.__init__": [[1189, 1196], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForTriggerClassification.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForTriggerClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForTriggerClassification.forward": [[1197, 1215], ["modeling.BertForTriggerClassification.bert", "modeling.BertForTriggerClassification.dropout", "modeling.BertForTriggerClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "modeling.BertForTriggerClassification.view", "labels.view", "modeling.BertForTriggerClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLSTMForTriggerClassification.__init__": [[1220, 1233], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.LSTM", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertLSTMForTriggerClassification.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertLSTMForTriggerClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# newly add lstm initialization", "\n", "self", ".", "hidden_dim", "=", "200", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "config", ".", "hidden_size", ",", "self", ".", "hidden_dim", "//", "2", ",", "num_layers", "=", "2", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "droplstm", "=", "nn", ".", "Dropout", "(", "0.1", ")", "\n", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_dim", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertLSTMForTriggerClassification.forward": [[1234, 1273], ["modeling.BertLSTMForTriggerClassification.bert.eval", "modeling.BertLSTMForTriggerClassification.dropout", "attention_mask.sum().cpu().numpy", "torch.nn.utils.rnn.pack_padded_sequence", "modeling.BertLSTMForTriggerClassification.lstm", "torch.nn.utils.rnn.pad_packed_sequence", "modeling.BertLSTMForTriggerClassification.droplstm", "modeling.BertLSTMForTriggerClassification.classifier", "torch.no_grad", "modeling.BertLSTMForTriggerClassification.bert", "lstm_out.transpose", "torch.nn.CrossEntropyLoss", "attention_mask.sum().cpu", "int", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "input_ids.size", "attention_mask.view", "modeling.BertLSTMForTriggerClassification.view", "labels.view", "modeling.BertLSTMForTriggerClassification.view", "labels.view", "attention_mask.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "# bert layer", "\n", "        ", "self", ".", "bert", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "\n", "# pass through LSTM layer", "\n", "## option 1 (not working)", "\n", "# hidden = None", "\n", "# lstm_out, _ = self.lstm(sequence_output, hidden)", "\n", "# lstm_out = self.droplstm(lstm_out)", "\n", "## option 2 w/ (un)packing", "\n", "seq_lengths", "=", "attention_mask", ".", "sum", "(", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "packed_words", "=", "pack_padded_sequence", "(", "sequence_output", ",", "seq_lengths", ",", "batch_first", "=", "True", ")", "\n", "hidden", "=", "None", "\n", "lstm_out", ",", "hidden", "=", "self", ".", "lstm", "(", "packed_words", ",", "hidden", ")", "\n", "lstm_out", ",", "_", "=", "pad_packed_sequence", "(", "lstm_out", ",", "total_length", "=", "int", "(", "input_ids", ".", "size", "(", "1", ")", ")", ")", "\n", "## lstm_out (seq_len, seq_len, hidden_size)", "\n", "feature_out", "=", "self", ".", "droplstm", "(", "lstm_out", ".", "transpose", "(", "1", ",", "0", ")", ")", "\n", "\n", "# get logits", "\n", "logits", "=", "self", ".", "classifier", "(", "feature_out", ")", "\n", "\n", "# import ipdb; ipdb.set_trace()", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.__init__": [[1322, 1329], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Linear", "modeling.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.forward": [[1330, 1355], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.__init__": [[1359, 1368], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Embedding", "torch.nn.Linear", "modeling.BertForQuestionAnswering_withIfTriggerEmbedding.apply"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering_withIfTriggerEmbedding", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "if_trigger_embedding", "=", "nn", ".", "Embedding", "(", "2", ",", "config", ".", "hidden_size", "//", "10", ")", "\n", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "+", "config", ".", "hidden_size", "//", "10", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.BertForQuestionAnswering_withIfTriggerEmbedding.forward": [[1369, 1397], ["modeling.BertForQuestionAnswering_withIfTriggerEmbedding.bert", "modeling.BertForQuestionAnswering_withIfTriggerEmbedding.if_trigger_embedding", "modeling.BertForQuestionAnswering_withIfTriggerEmbedding.qa_outputs", "modeling.BertForQuestionAnswering_withIfTriggerEmbedding.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "torch.cat", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "if_trigger_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "if_trigger_embedding_output", "=", "self", ".", "if_trigger_embedding", "(", "if_trigger_ids", ")", "\n", "\n", "# import ipdb; ipdb.set_trace()", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "torch", ".", "cat", "(", "[", "sequence_output", ",", "if_trigger_embedding_output", "]", ",", "dim", "=", "2", ")", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.load_tf_weights_in_bert": [[53, 118], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr", "getattr", "print"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", ",", "\"global_step\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'squad'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'classifier'", ")", "\n", "", "else", ":", "\n", "                ", "try", ":", "\n", "                    ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "except", "AttributeError", ":", "\n", "                    ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.gelu": [[120, 127], ["torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.modeling.swish": [[129, 131], ["torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.__init__": [[68, 87], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.get_lr": [[88, 102], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.BertAdam.step": [[103, 182], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct", "logger.warning"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "warned_for_t_total", "=", "False", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "progress", "=", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "progress", ",", "group", "[", "'warmup'", "]", ")", "\n", "# warning for exceeding t_total (only active with warmup_linear", "\n", "if", "group", "[", "'schedule'", "]", "==", "\"warmup_linear\"", "and", "progress", ">", "1.", "and", "not", "warned_for_t_total", ":", "\n", "                        ", "logger", ".", "warning", "(", "\n", "\"Training beyond specified 't_total' steps with schedule '{}'. Learning rate set to {}. \"", "\n", "\"Please set 't_total' of {} correctly.\"", ".", "format", "(", "group", "[", "'schedule'", "]", ",", "lr_scheduled", ",", "self", ".", "__class__", ".", "__name__", ")", ")", "\n", "warned_for_t_total", "=", "True", "\n", "# end warning", "\n", "", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.warmup_cosine": [[27, 32], ["math.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "x_", "=", "(", "x", "-", "warmup", ")", "/", "(", "1", "-", "warmup", ")", "# progress after warmup -", "\n", "return", "0.5", "*", "(", "1.", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "x_", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.warmup_constant": [[33, 39], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "\"\"\" Linearly increases learning rate over `warmup`*`t_total` (as provided to BertAdam) training steps.\n        Learning rate is 1. afterwards. \"\"\"", "\n", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.optimization.warmup_linear": [[40, 46], ["max"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "\"\"\" Specifies a triangular learning rate schedule where peak is reached at `warmup`*`t_total`-th (as provided to BertAdam) training step.\n        After `t_total`-th training step, learning rate is zero. \"\"\"", "\n", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "max", "(", "(", "x", "-", "1.", ")", "/", "(", "warmup", "-", "1.", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.url_to_filename": [[44, 60], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.filename_to_url": [[62, 86], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.cached_path": [[88, 116], ["urlparse", "isinstance", "str", "isinstance", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.split_s3_path": [[118, 129], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.s3_request": [[131, 148], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.s3_etag": [[150, 157], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.s3_get": [[159, 165], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.http_get": [[167, 177], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.get_from_cache": [[179, 253], ["url.startswith", "file_utils.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "response.headers.get.decode", "fnmatch.filter", "list", "os.path.exists", "requests.head", "os.path.exists", "os.listdir", "filter", "os.path.join", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "requests.head.headers.get", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dumps", "meta_file.write", "isinstance", "unicode", "s.endswith"], "function", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.url_to_filename", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.s3_etag", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.s3_get", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "            ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "                ", "etag", "=", "None", "\n", "", "else", ":", "\n", "                ", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "", "", "except", "EnvironmentError", ":", "\n", "            ", "etag", "=", "None", "\n", "\n", "", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "etag", "is", "not", "None", ":", "\n", "        ", "etag", "=", "etag", ".", "decode", "(", "'utf-8'", ")", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "# If we don't have a connection (etag is None) and can't identify the file", "\n", "# try to get the last downloaded one", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", "and", "etag", "is", "None", ":", "\n", "        ", "matching_files", "=", "fnmatch", ".", "filter", "(", "os", ".", "listdir", "(", "cache_dir", ")", ",", "filename", "+", "'.*'", ")", "\n", "matching_files", "=", "list", "(", "filter", "(", "lambda", "s", ":", "not", "s", ".", "endswith", "(", "'.json'", ")", ",", "matching_files", ")", ")", "\n", "if", "matching_files", ":", "\n", "            ", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "matching_files", "[", "-", "1", "]", ")", "\n", "\n", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "output_string", "=", "json", ".", "dumps", "(", "meta", ")", "\n", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "output_string", ",", "str", ")", ":", "\n", "                    ", "output_string", "=", "unicode", "(", "output_string", ",", "'utf-8'", ")", "# The beauty of python 2", "\n", "", "meta_file", ".", "write", "(", "output_string", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.read_set_from_file": [[255, 265], ["set", "io.open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.get_file_extension": [[267, 271], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.__init__": [[77, 106], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "tokenization.BasicTokenizer", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "do_basic_tokenize", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BertTokenizer.\n\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model's\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "do_basic_tokenize", "=", "do_basic_tokenize", "\n", "if", "do_basic_tokenize", ":", "\n", "            ", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ",", "\n", "never_split", "=", "never_split", ")", "\n", "", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.tokenize": [[107, 116], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization.BertTokenizer.append"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "if", "self", ".", "do_basic_tokenize", ":", "\n", "            ", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "                ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                    ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "", "else", ":", "\n", "            ", "split_tokens", "=", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids": [[117, 129], ["ids.append", "len", "logger.warning", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_ids_to_tokens": [[130, 136], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.save_vocabulary": [[137, 151], ["os.path.isdir", "os.path.join", "io.open", "sorted", "tokenization.BertTokenizer.vocab.items", "writer.write", "logger.warning"], "methods", ["None"], ["", "def", "save_vocabulary", "(", "self", ",", "vocab_path", ")", ":", "\n", "        ", "\"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"", "\n", "index", "=", "0", "\n", "if", "os", ".", "path", ".", "isdir", "(", "vocab_path", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_path", ",", "VOCAB_NAME", ")", "\n", "", "with", "open", "(", "vocab_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "writer", ":", "\n", "            ", "for", "token", ",", "token_index", "in", "sorted", "(", "self", ".", "vocab", ".", "items", "(", ")", ",", "key", "=", "lambda", "kv", ":", "kv", "[", "1", "]", ")", ":", "\n", "                ", "if", "index", "!=", "token_index", ":", "\n", "                    ", "logger", ".", "warning", "(", "\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"", "\n", "\" Please check that the vocabulary is not corrupted!\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "index", "=", "token_index", "\n", "", "writer", ".", "write", "(", "token", "+", "u'\\n'", ")", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab_file", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained": [[152, 199], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "kwargs.get", "logger.warning", "logger.error", "kwargs.get", "logger.warning", "int", "kwargs.get", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "if", "'-cased'", "in", "pretrained_model_name_or_path", "and", "kwargs", ".", "get", "(", "'do_lower_case'", ",", "True", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\"The pre-trained model you are loading is a cased model but you have not set \"", "\n", "\"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \"", "\n", "\"you may want to check this behavior.\"", ")", "\n", "kwargs", "[", "'do_lower_case'", "]", "=", "False", "\n", "", "elif", "'-cased'", "not", "in", "pretrained_model_name_or_path", "and", "not", "kwargs", ".", "get", "(", "'do_lower_case'", ",", "True", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\"The pre-trained model you are loading is an uncased model but you have set \"", "\n", "\"`do_lower_case` to False. We are setting `do_lower_case=True` for you \"", "\n", "\"but you may want to check this behavior.\"", ")", "\n", "kwargs", "[", "'do_lower_case'", "]", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name_or_path", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer.__init__": [[204, 214], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer.tokenize": [[215, 235], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents": [[236, 246], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc": [[247, 268], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[269, 281], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char": [[282, 303], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text": [[304, 316], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__": [[321, 325], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize": [[326, 376], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.load_vocab": [[50, 63], ["collections.OrderedDict", "io.open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization.whitespace_tokenize": [[65, 72], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization._is_whitespace": [[378, 388], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization._is_control": [[390, 400], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xinyadu_eeqa.pytorch_pretrained_bert.tokenization._is_punctuation": [[402, 416], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]]}