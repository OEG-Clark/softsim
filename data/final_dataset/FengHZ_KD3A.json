{"home.repos.pwc.inspect_result.FengHZ_KD3A.None.main.main": [[65, 326], ["os.path.join", "print", "os.path.exists", "torch.utils.tensorboard.SummaryWriter", "print", "lib.utils.federated_utils.create_domain_weight", "lib.utils.federated_utils.decentralized_training_strategy", "range", "print", "datasets.DigitFive.digit5_dataset_read", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "domains.remove", "print", "print", "zip", "optimizers.append", "classifier_optimizers.append", "optimizer_schedulers.append", "classifier_optimizer_schedulers.append", "input", "len", "train.train.train", "train.train.test", "model.digit5.CNN().cuda", "model.digit5.Classifier().cuda", "datasets.DigitFive.digit5_dataset_read", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "print", "print", "datasets.AmazonReview.amazon_dataset_read", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "domains.remove", "print", "print", "model.named_parameters", "models[].named_parameters", "target_weight[].data.clone", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.optim.lr_scheduler.CosineAnnealingLR", "shutil.rmtree", "scheduler.step", "scheduler.step", "main.save_checkpoint", "model.digit5.CNN().cuda", "model.digit5.Classifier().cuda", "model.amazon.AmazonMLP().cuda", "model.amazon.AmazonClassifier().cuda", "datasets.AmazonReview.amazon_dataset_read", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "print", "datasets.OfficeCaltech10.get_office_caltech10_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "domains.remove", "model.parameters", "classifier.parameters", "model.digit5.CNN", "model.digit5.Classifier", "model.amazon.AmazonMLP().cuda", "model.amazon.AmazonClassifier().cuda", "model.officecaltech10.OfficeCaltechNet().cuda", "model.officecaltech10.OfficeCaltechClassifier().cuda", "datasets.OfficeCaltech10.get_office_caltech10_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "datasets.Office31.get_office31_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "domains.remove", "models[].state_dict", "classifiers[].state_dict", "optimizers[].state_dict", "classifier_optimizers[].state_dict", "model.digit5.CNN", "model.digit5.Classifier", "model.amazon.AmazonMLP", "model.amazon.AmazonClassifier", "model.officecaltech10.OfficeCaltechNet().cuda", "model.officecaltech10.OfficeCaltechClassifier().cuda", "model.officecaltech10.OfficeCaltechNet().cuda", "model.officecaltech10.OfficeCaltechClassifier().cuda", "datasets.Office31.get_office31_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "datasets.MiniDomainNet.get_mini_domainnet_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "domains.remove", "model.amazon.AmazonMLP", "model.amazon.AmazonClassifier", "model.officecaltech10.OfficeCaltechNet", "model.officecaltech10.OfficeCaltechClassifier", "model.officecaltech10.OfficeCaltechNet().cuda", "model.officecaltech10.OfficeCaltechClassifier().cuda", "model.domainnet.DomainNet().cuda", "model.domainnet.DomainNetClassifier().cuda", "datasets.MiniDomainNet.get_mini_domainnet_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "datasets.DomainNet.get_domainnet_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "domains.remove", "NotImplementedError", "model.officecaltech10.OfficeCaltechNet", "model.officecaltech10.OfficeCaltechClassifier", "model.officecaltech10.OfficeCaltechNet", "model.officecaltech10.OfficeCaltechClassifier", "model.domainnet.DomainNet().cuda", "model.domainnet.DomainNetClassifier().cuda", "model.domainnet.DomainNet().cuda", "model.domainnet.DomainNetClassifier().cuda", "datasets.DomainNet.get_domainnet_dloader", "train_dloaders.append", "test_dloaders.append", "models.append", "classifiers.append", "model.officecaltech10.OfficeCaltechNet", "model.officecaltech10.OfficeCaltechClassifier", "model.domainnet.DomainNet", "model.domainnet.DomainNetClassifier", "model.domainnet.DomainNet().cuda", "model.domainnet.DomainNetClassifier().cuda", "model.domainnet.DomainNet", "model.domainnet.DomainNetClassifier", "model.domainnet.DomainNet", "model.domainnet.DomainNetClassifier", "model.domainnet.DomainNet", "model.domainnet.DomainNetClassifier"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.create_domain_weight", "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.decentralized_training_strategy", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.digit5_dataset_read", "home.repos.pwc.inspect_result.FengHZ_KD3A.train.train.train", "home.repos.pwc.inspect_result.FengHZ_KD3A.train.train.test", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.digit5_dataset_read", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.amazon_dataset_read", "home.repos.pwc.inspect_result.FengHZ_KD3A.None.main.save_checkpoint", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.amazon_dataset_read", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.get_office_caltech10_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.get_office_caltech10_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.get_office31_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.get_office31_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.get_mini_domainnet_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.get_mini_domainnet_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.get_domainnet_dloader", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.get_domainnet_dloader"], ["def", "main", "(", "args", "=", "args", ",", "configs", "=", "configs", ")", ":", "\n", "# set the dataloader list, model list, optimizer list, optimizer schedule list", "\n", "    ", "train_dloaders", "=", "[", "]", "\n", "test_dloaders", "=", "[", "]", "\n", "models", "=", "[", "]", "\n", "classifiers", "=", "[", "]", "\n", "optimizers", "=", "[", "]", "\n", "classifier_optimizers", "=", "[", "]", "\n", "optimizer_schedulers", "=", "[", "]", "\n", "classifier_optimizer_schedulers", "=", "[", "]", "\n", "# build dataset", "\n", "if", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "==", "\"DigitFive\"", ":", "\n", "        ", "domains", "=", "[", "'mnistm'", ",", "'mnist'", ",", "'syn'", ",", "'usps'", ",", "'svhn'", "]", "\n", "# [0]: target dataset, target backbone, [1:-1]: source dataset, source backbone", "\n", "# generate dataset for train and target", "\n", "print", "(", "\"load target domain {}\"", ".", "format", "(", "args", ".", "target_domain", ")", ")", "\n", "target_train_dloader", ",", "target_test_dloader", "=", "digit5_dataset_read", "(", "args", ".", "base_path", ",", "\n", "args", ".", "target_domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ")", "\n", "train_dloaders", ".", "append", "(", "target_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "target_test_dloader", ")", "\n", "# generate CNN and Classifier for target domain", "\n", "models", ".", "append", "(", "CNN", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "Classifier", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "domains", ".", "remove", "(", "args", ".", "target_domain", ")", "\n", "args", ".", "source_domains", "=", "domains", "\n", "print", "(", "\"target domain {} loaded\"", ".", "format", "(", "args", ".", "target_domain", ")", ")", "\n", "# create DigitFive dataset", "\n", "print", "(", "\"Source Domains :{}\"", ".", "format", "(", "domains", ")", ")", "\n", "for", "domain", "in", "domains", ":", "\n", "# generate dataset for source domain", "\n", "            ", "source_train_dloader", ",", "source_test_dloader", "=", "digit5_dataset_read", "(", "args", ".", "base_path", ",", "domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ")", "\n", "train_dloaders", ".", "append", "(", "source_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "source_test_dloader", ")", "\n", "# generate CNN and Classifier for source domain", "\n", "models", ".", "append", "(", "CNN", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "Classifier", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "print", "(", "\"Domain {} Preprocess Finished\"", ".", "format", "(", "domain", ")", ")", "\n", "", "num_classes", "=", "10", "\n", "", "elif", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "==", "\"AmazonReview\"", ":", "\n", "        ", "domains", "=", "[", "\"books\"", ",", "\"dvd\"", ",", "\"electronics\"", ",", "\"kitchen\"", "]", "\n", "print", "(", "\"load target domain {}\"", ".", "format", "(", "args", ".", "target_domain", ")", ")", "\n", "target_train_dloader", ",", "target_test_dloader", "=", "amazon_dataset_read", "(", "args", ".", "base_path", ",", "\n", "args", ".", "target_domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ")", "\n", "train_dloaders", ".", "append", "(", "target_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "target_test_dloader", ")", "\n", "# generate MLP and Classifier for target domain", "\n", "models", ".", "append", "(", "AmazonMLP", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "AmazonClassifier", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "domains", ".", "remove", "(", "args", ".", "target_domain", ")", "\n", "args", ".", "source_domains", "=", "domains", "\n", "print", "(", "\"target domain {} loaded\"", ".", "format", "(", "args", ".", "target_domain", ")", ")", "\n", "# create DigitFive dataset", "\n", "print", "(", "\"Source Domains :{}\"", ".", "format", "(", "domains", ")", ")", "\n", "for", "domain", "in", "domains", ":", "\n", "# generate dataset for source domain", "\n", "            ", "source_train_dloader", ",", "source_test_dloader", "=", "amazon_dataset_read", "(", "args", ".", "base_path", ",", "domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ")", "\n", "train_dloaders", ".", "append", "(", "source_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "source_test_dloader", ")", "\n", "# generate CNN and Classifier for source domain", "\n", "models", ".", "append", "(", "AmazonMLP", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "AmazonClassifier", "(", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "print", "(", "\"Domain {} Preprocess Finished\"", ".", "format", "(", "domain", ")", ")", "\n", "", "num_classes", "=", "2", "\n", "", "elif", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "==", "\"OfficeCaltech10\"", ":", "\n", "        ", "domains", "=", "[", "'amazon'", ",", "'webcam'", ",", "'dslr'", ",", "\"caltech\"", "]", "\n", "target_train_dloader", ",", "target_test_dloader", "=", "get_office_caltech10_dloader", "(", "args", ".", "base_path", ",", "\n", "args", ".", "target_domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", "\n", ",", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "target_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "target_test_dloader", ")", "\n", "models", ".", "append", "(", "\n", "OfficeCaltechNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "bn_momentum", "=", "args", ".", "bn_momentum", ",", "\n", "pretrained", "=", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "data_parallel", "=", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "\n", "OfficeCaltechClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "10", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", "\n", ")", "\n", "domains", ".", "remove", "(", "args", ".", "target_domain", ")", "\n", "args", ".", "source_domains", "=", "domains", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "source_train_dloader", ",", "source_test_dloader", "=", "get_office_caltech10_dloader", "(", "args", ".", "base_path", ",", "domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\n", "\"batch_size\"", "]", ",", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "source_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "source_test_dloader", ")", "\n", "models", ".", "append", "(", "\n", "OfficeCaltechNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "args", ".", "bn_momentum", ",", "\n", "pretrained", "=", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "data_parallel", "=", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "\n", "OfficeCaltechClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "10", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", "\n", ")", "\n", "", "num_classes", "=", "10", "\n", "", "elif", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "==", "\"Office31\"", ":", "\n", "        ", "domains", "=", "[", "'amazon'", ",", "'webcam'", ",", "'dslr'", "]", "\n", "target_train_dloader", ",", "target_test_dloader", "=", "get_office31_dloader", "(", "args", ".", "base_path", ",", "\n", "args", ".", "target_domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ",", "\n", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "target_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "target_test_dloader", ")", "\n", "models", ".", "append", "(", "\n", "OfficeCaltechNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "bn_momentum", "=", "args", ".", "bn_momentum", ",", "\n", "pretrained", "=", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "data_parallel", "=", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "\n", "OfficeCaltechClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "31", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", "\n", ")", "\n", "domains", ".", "remove", "(", "args", ".", "target_domain", ")", "\n", "args", ".", "source_domains", "=", "domains", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "source_train_dloader", ",", "source_test_dloader", "=", "get_office31_dloader", "(", "args", ".", "base_path", ",", "domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ",", "\n", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "source_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "source_test_dloader", ")", "\n", "models", ".", "append", "(", "\n", "OfficeCaltechNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "args", ".", "bn_momentum", ",", "\n", "pretrained", "=", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "data_parallel", "=", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "\n", "OfficeCaltechClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "31", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", "\n", ")", "\n", "", "num_classes", "=", "31", "\n", "", "elif", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "==", "\"MiniDomainNet\"", ":", "\n", "        ", "domains", "=", "[", "'clipart'", ",", "'painting'", ",", "'real'", ",", "'sketch'", "]", "\n", "target_train_dloader", ",", "target_test_dloader", "=", "get_mini_domainnet_dloader", "(", "args", ".", "base_path", ",", "args", ".", "target_domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ",", "\n", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "target_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "target_test_dloader", ")", "\n", "models", ".", "append", "(", "\n", "DomainNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "args", ".", "bn_momentum", ",", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "DomainNetClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "126", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "domains", ".", "remove", "(", "args", ".", "target_domain", ")", "\n", "args", ".", "source_domains", "=", "domains", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "source_train_dloader", ",", "source_test_dloader", "=", "get_mini_domainnet_dloader", "(", "args", ".", "base_path", ",", "domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\n", "\"batch_size\"", "]", ",", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "source_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "source_test_dloader", ")", "\n", "models", ".", "append", "(", "DomainNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "args", ".", "bn_momentum", ",", "\n", "pretrained", "=", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "data_parallel", "=", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "DomainNetClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "126", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "", "num_classes", "=", "126", "\n", "", "elif", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "==", "\"DomainNet\"", ":", "\n", "        ", "domains", "=", "[", "'clipart'", ",", "'infograph'", ",", "'painting'", ",", "'quickdraw'", ",", "'real'", ",", "'sketch'", "]", "\n", "target_train_dloader", ",", "target_test_dloader", "=", "get_domainnet_dloader", "(", "args", ".", "base_path", ",", "\n", "args", ".", "target_domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ",", "\n", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "target_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "target_test_dloader", ")", "\n", "models", ".", "append", "(", "\n", "DomainNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "args", ".", "bn_momentum", ",", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "DomainNetClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "345", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "domains", ".", "remove", "(", "args", ".", "target_domain", ")", "\n", "args", ".", "source_domains", "=", "domains", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "source_train_dloader", ",", "source_test_dloader", "=", "get_domainnet_dloader", "(", "args", ".", "base_path", ",", "domain", ",", "\n", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ",", "\n", "args", ".", "workers", ")", "\n", "train_dloaders", ".", "append", "(", "source_train_dloader", ")", "\n", "test_dloaders", ".", "append", "(", "source_test_dloader", ")", "\n", "models", ".", "append", "(", "DomainNet", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "args", ".", "bn_momentum", ",", "\n", "pretrained", "=", "configs", "[", "\"ModelConfig\"", "]", "[", "\"pretrained\"", "]", ",", "\n", "data_parallel", "=", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "classifiers", ".", "append", "(", "DomainNetClassifier", "(", "configs", "[", "\"ModelConfig\"", "]", "[", "\"backbone\"", "]", ",", "345", ",", "args", ".", "data_parallel", ")", ".", "cuda", "(", ")", ")", "\n", "", "num_classes", "=", "345", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Dataset {} not implemented\"", ".", "format", "(", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", ")", ")", "\n", "# federated learning step 1: initialize model with the same parameter (use target as standard)", "\n", "", "for", "model", "in", "models", "[", "1", ":", "]", ":", "\n", "        ", "for", "source_weight", ",", "target_weight", "in", "zip", "(", "model", ".", "named_parameters", "(", ")", ",", "models", "[", "0", "]", ".", "named_parameters", "(", ")", ")", ":", "\n", "# consistent parameters", "\n", "            ", "source_weight", "[", "1", "]", ".", "data", "=", "target_weight", "[", "1", "]", ".", "data", ".", "clone", "(", ")", "\n", "# create the optimizer for each model", "\n", "", "", "for", "model", "in", "models", ":", "\n", "        ", "optimizers", ".", "append", "(", "\n", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "momentum", "=", "args", ".", "momentum", ",", "\n", "lr", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"learning_rate_begin\"", "]", ",", "weight_decay", "=", "args", ".", "wd", ")", ")", "\n", "", "for", "classifier", "in", "classifiers", ":", "\n", "        ", "classifier_optimizers", ".", "append", "(", "\n", "torch", ".", "optim", ".", "SGD", "(", "classifier", ".", "parameters", "(", ")", ",", "momentum", "=", "args", ".", "momentum", ",", "\n", "lr", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"learning_rate_begin\"", "]", ",", "weight_decay", "=", "args", ".", "wd", ")", ")", "\n", "# create the optimizer scheduler with cosine annealing schedule", "\n", "", "for", "optimizer", "in", "optimizers", ":", "\n", "        ", "optimizer_schedulers", ".", "append", "(", "\n", "CosineAnnealingLR", "(", "optimizer", ",", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"total_epochs\"", "]", ",", "\n", "eta_min", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"learning_rate_end\"", "]", ")", ")", "\n", "", "for", "classifier_optimizer", "in", "classifier_optimizers", ":", "\n", "        ", "classifier_optimizer_schedulers", ".", "append", "(", "\n", "CosineAnnealingLR", "(", "classifier_optimizer", ",", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"total_epochs\"", "]", ",", "\n", "eta_min", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"learning_rate_end\"", "]", ")", ")", "\n", "# create the event to save log info", "\n", "", "writer_log_dir", "=", "path", ".", "join", "(", "args", ".", "base_path", ",", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", ",", "\"runs\"", ",", "\n", "\"train_time:{}\"", ".", "format", "(", "args", ".", "train_time", ")", "+", "\"_\"", "+", "\n", "args", ".", "target_domain", "+", "\"_\"", "+", "\"_\"", ".", "join", "(", "args", ".", "source_domains", ")", ")", "\n", "print", "(", "\"create writer in {}\"", ".", "format", "(", "writer_log_dir", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "writer_log_dir", ")", ":", "\n", "        ", "flag", "=", "input", "(", "\"{} train_time:{} will be removed, input yes to continue:\"", ".", "format", "(", "\n", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", ",", "args", ".", "train_time", ")", ")", "\n", "if", "flag", "==", "\"yes\"", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "writer_log_dir", ",", "ignore_errors", "=", "True", ")", "\n", "", "", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "writer_log_dir", ")", "\n", "# begin train", "\n", "print", "(", "\"Begin the {} time's training, Dataset:{}, Source Domains {}, Target Domain {}\"", ".", "format", "(", "args", ".", "train_time", ",", "\n", "configs", "[", "\n", "\"DataConfig\"", "]", "[", "\n", "\"dataset\"", "]", ",", "\n", "args", ".", "source_domains", ",", "\n", "args", ".", "target_domain", ")", ")", "\n", "\n", "# create the initialized domain weight", "\n", "domain_weight", "=", "create_domain_weight", "(", "len", "(", "args", ".", "source_domains", ")", ")", "\n", "# adjust training strategy with communication round", "\n", "batch_per_epoch", ",", "total_epochs", "=", "decentralized_training_strategy", "(", "\n", "communication_rounds", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"communication_rounds\"", "]", ",", "\n", "epoch_samples", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"epoch_samples\"", "]", ",", "\n", "batch_size", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"batch_size\"", "]", ",", "\n", "total_epochs", "=", "configs", "[", "\"TrainingConfig\"", "]", "[", "\"total_epochs\"", "]", ")", "\n", "# train model", "\n", "for", "epoch", "in", "range", "(", "args", ".", "start_epoch", ",", "total_epochs", ")", ":", "\n", "        ", "domain_weight", "=", "train", "(", "train_dloaders", ",", "models", ",", "classifiers", ",", "optimizers", ",", "\n", "classifier_optimizers", ",", "epoch", ",", "writer", ",", "num_classes", "=", "num_classes", ",", "\n", "domain_weight", "=", "domain_weight", ",", "source_domains", "=", "args", ".", "source_domains", ",", "\n", "batch_per_epoch", "=", "batch_per_epoch", ",", "total_epochs", "=", "total_epochs", ",", "\n", "batchnorm_mmd", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"batchnorm_mmd\"", "]", ",", "\n", "communication_rounds", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"communication_rounds\"", "]", ",", "\n", "confidence_gate_begin", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"confidence_gate_begin\"", "]", ",", "\n", "confidence_gate_end", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"confidence_gate_end\"", "]", ",", "\n", "malicious_domain", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"malicious\"", "]", "[", "\"attack_domain\"", "]", ",", "\n", "attack_level", "=", "configs", "[", "\"UMDAConfig\"", "]", "[", "\"malicious\"", "]", "[", "\"attack_level\"", "]", ",", "\n", "mix_aug", "=", "(", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", "!=", "\"AmazonReview\"", ")", ")", "\n", "test", "(", "args", ".", "target_domain", ",", "args", ".", "source_domains", ",", "test_dloaders", ",", "models", ",", "classifiers", ",", "epoch", ",", "\n", "writer", ",", "num_classes", "=", "num_classes", ",", "top_5_accuracy", "=", "(", "num_classes", ">", "10", ")", ")", "\n", "for", "scheduler", "in", "optimizer_schedulers", ":", "\n", "            ", "scheduler", ".", "step", "(", "epoch", ")", "\n", "", "for", "scheduler", "in", "classifier_optimizer_schedulers", ":", "\n", "            ", "scheduler", ".", "step", "(", "epoch", ")", "\n", "# save models every 10 epochs", "\n", "", "if", "(", "epoch", "+", "1", ")", "%", "10", "==", "0", ":", "\n", "# save target model with epoch, domain, model, optimizer", "\n", "            ", "save_checkpoint", "(", "\n", "{", "\"epoch\"", ":", "epoch", "+", "1", ",", "\n", "\"domain\"", ":", "args", ".", "target_domain", ",", "\n", "\"backbone\"", ":", "models", "[", "0", "]", ".", "state_dict", "(", ")", ",", "\n", "\"classifier\"", ":", "classifiers", "[", "0", "]", ".", "state_dict", "(", ")", ",", "\n", "\"optimizer\"", ":", "optimizers", "[", "0", "]", ".", "state_dict", "(", ")", ",", "\n", "\"classifier_optimizer\"", ":", "classifier_optimizers", "[", "0", "]", ".", "state_dict", "(", ")", "\n", "}", ",", "\n", "filename", "=", "\"{}.pth.tar\"", ".", "format", "(", "args", ".", "target_domain", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.None.main.save_checkpoint": [[328, 334], ["torch.save", "torch.save", "os.path.exists", "os.makedirs", "os.path.join"], "function", ["None"], ["", "", "", "def", "save_checkpoint", "(", "state", ",", "filename", ")", ":", "\n", "    ", "filefolder", "=", "\"{}/{}/parameter/train_time:{}\"", ".", "format", "(", "args", ".", "base_path", ",", "configs", "[", "\"DataConfig\"", "]", "[", "\"dataset\"", "]", ",", "\n", "args", ".", "train_time", ")", "\n", "if", "not", "path", ".", "exists", "(", "filefolder", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "filefolder", ")", "\n", "", "torch", ".", "save", "(", "state", ",", "path", ".", "join", "(", "filefolder", ",", "filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.cifar10_label_transform": [[7, 9], ["None"], "function", ["None"], ["def", "cifar10_label_transform", "(", "label", ")", ":", "\n", "    ", "return", "cifar10_label_map", "[", "label", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.stl10_dataset": [[11, 28], ["torchvision.transforms.Compose", "torchvision.datasets.STL10", "torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.ToTensor", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.RandomVerticalFlip"], "function", ["None"], ["", "def", "stl10_dataset", "(", "dataset_base_path", ",", "train_flag", "=", "True", ")", ":", "\n", "    ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "128", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "if", "train_flag", ":", "\n", "        ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomVerticalFlip", "(", ")", ",", "\n", "transform", "\n", "]", ")", "\n", "", "if", "train_flag", ":", "\n", "        ", "split", "=", "\"train\"", "\n", "", "else", ":", "\n", "        ", "split", "=", "\"test\"", "\n", "", "dataset", "=", "datasets", ".", "STL10", "(", "root", "=", "dataset_base_path", ",", "transform", "=", "transform", ",", "split", "=", "split", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.cifar10_dataset": [[30, 49], ["torchvision.transforms.Compose", "torchvision.datasets.CIFAR10", "torchvision.transforms.Compose", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Pad", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.RandomCrop"], "function", ["None"], ["", "def", "cifar10_dataset", "(", "dataset_base_path", ",", "train_flag", "=", "True", ",", "target_domain", "=", "False", ")", ":", "\n", "    ", "if", "target_domain", ":", "\n", "        ", "target_transform", "=", "cifar10_label_transform", "\n", "", "else", ":", "\n", "        ", "target_transform", "=", "None", "\n", "", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "mean", "=", "(", "0.4914", ",", "0.4822", ",", "0.4465", ")", ",", "std", "=", "(", "0.2023", ",", "0.1994", ",", "0.2010", ")", ")", "\n", "]", ")", "\n", "if", "train_flag", ":", "\n", "        ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Pad", "(", "4", ",", "padding_mode", "=", "'reflect'", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomCrop", "(", "32", ")", ",", "\n", "transform", "\n", "]", ")", "\n", "", "dataset", "=", "datasets", ".", "CIFAR10", "(", "root", "=", "dataset_base_path", ",", "train", "=", "train_flag", ",", "\n", "download", "=", "False", ",", "transform", "=", "transform", ",", "target_transform", "=", "target_transform", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.cifar100_dataset": [[51, 66], ["torchvision.transforms.Compose", "torchvision.datasets.CIFAR100", "torchvision.transforms.Compose", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Pad", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.RandomCrop"], "function", ["None"], ["", "def", "cifar100_dataset", "(", "dataset_base_path", ",", "train_flag", "=", "True", ")", ":", "\n", "    ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "mean", "=", "(", "0.5071", ",", "0.4867", ",", "0.4408", ")", ",", "std", "=", "(", "0.2675", ",", "0.2565", ",", "0.2761", ")", ")", "\n", "]", ")", "\n", "if", "train_flag", ":", "\n", "        ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Pad", "(", "4", ",", "padding_mode", "=", "'reflect'", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomCrop", "(", "32", ")", ",", "\n", "transform", "\n", "]", ")", "\n", "", "dataset", "=", "datasets", ".", "CIFAR100", "(", "root", "=", "dataset_base_path", ",", "train", "=", "train_flag", ",", "\n", "download", "=", "False", ",", "transform", "=", "transform", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.svhn_dataset": [[68, 85], ["torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.datasets.SVHN", "torchvision.datasets.SVHN", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Pad", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.RandomCrop"], "function", ["None"], ["", "def", "svhn_dataset", "(", "dataset_base_path", ",", "train_flag", "=", "True", ")", ":", "\n", "    ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "mean", "=", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "std", "=", "(", "0.5", ",", "0.5", ",", "0.5", ")", ")", "\n", "]", ")", "\n", "if", "train_flag", ":", "\n", "        ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Pad", "(", "4", ",", "padding_mode", "=", "'reflect'", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomCrop", "(", "32", ")", ",", "\n", "transform", "\n", "]", ")", "\n", "", "if", "train_flag", ":", "\n", "        ", "dataset", "=", "datasets", ".", "SVHN", "(", "root", "=", "dataset_base_path", ",", "split", "=", "'train'", ",", "transform", "=", "transform", ",", "download", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "datasets", ".", "SVHN", "(", "root", "=", "dataset_base_path", ",", "split", "=", "'test'", ",", "transform", "=", "transform", ",", "download", "=", "True", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.get_ssl_sampler": [[87, 112], ["range", "torch.utils.data.sampler.SubsetRandomSampler", "torch.utils.data.sampler.SubsetRandomSampler", "torch.utils.data.sampler.SubsetRandomSampler", "torch.nonzero", "loc.view.view", "torch.utils.data.sampler.SubsetRandomSampler.extend", "torch.utils.data.sampler.SubsetRandomSampler.extend", "torch.utils.data.sampler.SubsetRandomSampler.extend", "loc[].tolist", "loc[].tolist", "loc[].tolist", "torch.randperm", "loc.view.size"], "function", ["None"], ["", "def", "get_ssl_sampler", "(", "labels", ",", "valid_num_per_class", ",", "annotated_num_per_class", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    :param labels: torch.array(int tensor)\n    :param valid_num_per_class: the number of validation for each class\n    :param annotated_num_per_class: the number of annotation we use for each classes\n    :param num_classes: the total number of classes\n    :return: sampler_l,sampler_u\n    \"\"\"", "\n", "sampler_valid", "=", "[", "]", "\n", "sampler_train_l", "=", "[", "]", "\n", "sampler_train_u", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "        ", "loc", "=", "torch", ".", "nonzero", "(", "labels", "==", "i", ")", "\n", "loc", "=", "loc", ".", "view", "(", "-", "1", ")", "\n", "# do random perm to make sure uniform sample", "\n", "loc", "=", "loc", "[", "torch", ".", "randperm", "(", "loc", ".", "size", "(", "0", ")", ")", "]", "\n", "sampler_valid", ".", "extend", "(", "loc", "[", ":", "valid_num_per_class", "]", ".", "tolist", "(", ")", ")", "\n", "sampler_train_l", ".", "extend", "(", "loc", "[", "valid_num_per_class", ":", "valid_num_per_class", "+", "annotated_num_per_class", "]", ".", "tolist", "(", ")", ")", "\n", "# sampler_train_u.extend(loc[num_valid + annotated_num_per_class:].tolist())", "\n", "# here the unsampled part also include the train_l part", "\n", "sampler_train_u", ".", "extend", "(", "loc", "[", "valid_num_per_class", ":", "]", ".", "tolist", "(", ")", ")", "\n", "", "sampler_valid", "=", "SubsetRandomSampler", "(", "sampler_valid", ")", "\n", "sampler_train_l", "=", "SubsetRandomSampler", "(", "sampler_train_l", ")", "\n", "sampler_train_u", "=", "SubsetRandomSampler", "(", "sampler_train_u", ")", "\n", "return", "sampler_valid", ",", "sampler_train_l", ",", "sampler_train_u", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.lib.dataloader.get_sl_sampler": [[114, 133], ["range", "torch.utils.data.sampler.SubsetRandomSampler", "torch.utils.data.sampler.SubsetRandomSampler", "torch.nonzero", "loc.view.view", "torch.utils.data.sampler.SubsetRandomSampler.extend", "torch.utils.data.sampler.SubsetRandomSampler.extend", "loc[].tolist", "loc[].tolist", "torch.randperm", "loc.view.size"], "function", ["None"], ["", "def", "get_sl_sampler", "(", "labels", ",", "valid_num_per_class", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    :param labels: torch.array(int tensor)\n    :param valid_num_per_class: the number of validation for each class\n    :param num_classes: the total number of classes\n    :return: sampler_l,sampler_u\n    \"\"\"", "\n", "sampler_valid", "=", "[", "]", "\n", "sampler_train", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "        ", "loc", "=", "torch", ".", "nonzero", "(", "labels", "==", "i", ")", "\n", "loc", "=", "loc", ".", "view", "(", "-", "1", ")", "\n", "# do random perm to make sure uniform sample", "\n", "loc", "=", "loc", "[", "torch", ".", "randperm", "(", "loc", ".", "size", "(", "0", ")", ")", "]", "\n", "sampler_valid", ".", "extend", "(", "loc", "[", ":", "valid_num_per_class", "]", ".", "tolist", "(", ")", ")", "\n", "sampler_train", ".", "extend", "(", "loc", "[", "valid_num_per_class", ":", "]", ".", "tolist", "(", ")", ")", "\n", "", "sampler_valid", "=", "SubsetRandomSampler", "(", "sampler_valid", ")", "\n", "sampler_train", "=", "SubsetRandomSampler", "(", "sampler_train", ")", "\n", "return", "sampler_valid", ",", "sampler_train", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.create_domain_weight": [[5, 8], ["None"], "function", ["None"], ["def", "create_domain_weight", "(", "source_domain_num", ")", ":", "\n", "    ", "global_federated_matrix", "=", "[", "1", "/", "(", "source_domain_num", "+", "1", ")", "]", "*", "(", "source_domain_num", "+", "1", ")", "\n", "return", "global_federated_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.update_domain_weight": [[10, 14], ["round", "range", "len"], "function", ["None"], ["", "def", "update_domain_weight", "(", "global_domain_weight", ",", "epoch_domain_weight", ",", "momentum", "=", "0.9", ")", ":", "\n", "    ", "global_domain_weight", "=", "[", "round", "(", "global_domain_weight", "[", "i", "]", "*", "momentum", "+", "epoch_domain_weight", "[", "i", "]", "*", "(", "1", "-", "momentum", ")", ",", "4", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "epoch_domain_weight", ")", ")", "]", "\n", "return", "global_domain_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.federated_average": [[16, 41], ["zip", "zip", "it.state_dict", "dic.items", "sum", "model.load_state_dict", "model.named_parameters", "sum", "[].data.clone", "enumerate", "parameter[].data.clone", "enumerate"], "function", ["None"], ["", "def", "federated_average", "(", "model_list", ",", "coefficient_matrix", ",", "batchnorm_mmd", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    :param model_list: a list of all models needed in federated average. [0]: model for target domain,\n    [1:-1] model for source domains\n    :param coefficient_matrix: the coefficient for each model in federate average, list or 1-d np.array\n    :param batchnorm_mmd: bool, if true, we use the batchnorm mmd\n    :return model list after federated average\n    \"\"\"", "\n", "if", "batchnorm_mmd", ":", "\n", "        ", "dict_list", "=", "[", "it", ".", "state_dict", "(", ")", "for", "it", "in", "model_list", "]", "\n", "dict_item_list", "=", "[", "dic", ".", "items", "(", ")", "for", "dic", "in", "dict_list", "]", "\n", "for", "key_data_pair_list", "in", "zip", "(", "*", "dict_item_list", ")", ":", "\n", "            ", "source_data_list", "=", "[", "pair", "[", "1", "]", "*", "coefficient_matrix", "[", "idx", "]", "for", "idx", ",", "pair", "in", "\n", "enumerate", "(", "key_data_pair_list", ")", "]", "\n", "dict_list", "[", "0", "]", "[", "key_data_pair_list", "[", "0", "]", "[", "0", "]", "]", "=", "sum", "(", "source_data_list", ")", "\n", "", "for", "model", "in", "model_list", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "dict_list", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "named_parameter_list", "=", "[", "model", ".", "named_parameters", "(", ")", "for", "model", "in", "model_list", "]", "\n", "for", "parameter_list", "in", "zip", "(", "*", "named_parameter_list", ")", ":", "\n", "            ", "source_parameters", "=", "[", "parameter", "[", "1", "]", ".", "data", ".", "clone", "(", ")", "*", "coefficient_matrix", "[", "idx", "]", "for", "idx", ",", "parameter", "in", "\n", "enumerate", "(", "parameter_list", ")", "]", "\n", "parameter_list", "[", "0", "]", "[", "1", "]", ".", "data", "=", "sum", "(", "source_parameters", ")", "\n", "for", "parameter", "in", "parameter_list", "[", "1", ":", "]", ":", "\n", "                ", "parameter", "[", "1", "]", ".", "data", "=", "parameter_list", "[", "0", "]", "[", "1", "]", ".", "data", ".", "clone", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.knowledge_vote": [[43, 65], ["knowledge_list.max", "max_p.max", "torch.zeros().cuda", "enumerate", "torch.zeros().cuda().scatter_.max", "torch.zeros().cuda().scatter_", "zip", "enumerate", "torch.zeros().cuda().scatter_.view", "torch.zeros", "torch.sum", "torch.zeros().cuda", "knowledge_list.size", "knowledge_list.size", "torch.zeros", "torch.zeros().cuda().scatter_.size"], "function", ["None"], ["", "", "", "", "def", "knowledge_vote", "(", "knowledge_list", ",", "confidence_gate", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    :param torch.tensor knowledge_list : recording the knowledge from each source domain model\n    :param float confidence_gate: the confidence gate to judge which sample to use\n    :return: consensus_confidence,consensus_knowledge,consensus_knowledge_weight\n    \"\"\"", "\n", "max_p", ",", "max_p_class", "=", "knowledge_list", ".", "max", "(", "2", ")", "\n", "max_conf", ",", "_", "=", "max_p", ".", "max", "(", "1", ")", "\n", "max_p_mask", "=", "(", "max_p", ">", "confidence_gate", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "consensus_knowledge", "=", "torch", ".", "zeros", "(", "knowledge_list", ".", "size", "(", "0", ")", ",", "knowledge_list", ".", "size", "(", "2", ")", ")", ".", "cuda", "(", ")", "\n", "for", "batch_idx", ",", "(", "p", ",", "p_class", ",", "p_mask", ")", "in", "enumerate", "(", "zip", "(", "max_p", ",", "max_p_class", ",", "max_p_mask", ")", ")", ":", "\n", "# to solve the [0,0,0] situation", "\n", "        ", "if", "torch", ".", "sum", "(", "p_mask", ")", ">", "0", ":", "\n", "            ", "p", "=", "p", "*", "p_mask", "\n", "", "for", "source_idx", ",", "source_class", "in", "enumerate", "(", "p_class", ")", ":", "\n", "            ", "consensus_knowledge", "[", "batch_idx", ",", "source_class", "]", "+=", "p", "[", "source_idx", "]", "\n", "", "", "consensus_knowledge_conf", ",", "consensus_knowledge", "=", "consensus_knowledge", ".", "max", "(", "1", ")", "\n", "consensus_knowledge_mask", "=", "(", "max_conf", ">", "confidence_gate", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "consensus_knowledge", "=", "torch", ".", "zeros", "(", "consensus_knowledge", ".", "size", "(", "0", ")", ",", "num_classes", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "1", ",", "\n", "consensus_knowledge", ".", "view", "(", "\n", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "return", "consensus_knowledge_conf", ",", "consensus_knowledge", ",", "consensus_knowledge_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.calculate_consensus_focus": [[67, 95], ["range", "list", "len", "frozenset", "list", "itertools.permutations", "list", "range", "itertools.combinations", "federated_utils.knowledge_vote", "torch.sum().item", "range", "range", "frozenset", "torch.sum", "frozenset", "frozenset", "list.index", "list.index"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.knowledge_vote"], ["", "def", "calculate_consensus_focus", "(", "consensus_focus_dict", ",", "knowledge_list", ",", "confidence_gate", ",", "source_domain_numbers", ",", "\n", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    :param consensus_focus_dict: record consensus_focus for each domain\n    :param torch.tensor knowledge_list : recording the knowledge from each source domain model\n    :param float confidence_gate: the confidence gate to judge which sample to use\n    :param source_domain_numbers: the numbers of source domains\n    \"\"\"", "\n", "domain_contribution", "=", "{", "frozenset", "(", ")", ":", "0", "}", "\n", "for", "combination_num", "in", "range", "(", "1", ",", "source_domain_numbers", "+", "1", ")", ":", "\n", "        ", "combination_list", "=", "list", "(", "combinations", "(", "range", "(", "source_domain_numbers", ")", ",", "combination_num", ")", ")", "\n", "for", "combination", "in", "combination_list", ":", "\n", "            ", "consensus_knowledge_conf", ",", "consensus_knowledge", ",", "consensus_knowledge_mask", "=", "knowledge_vote", "(", "\n", "knowledge_list", "[", ":", ",", "combination", ",", ":", "]", ",", "confidence_gate", ",", "num_classes", ")", "\n", "domain_contribution", "[", "frozenset", "(", "combination", ")", "]", "=", "torch", ".", "sum", "(", "\n", "consensus_knowledge_conf", "*", "consensus_knowledge_mask", ")", ".", "item", "(", ")", "\n", "", "", "permutation_list", "=", "list", "(", "permutations", "(", "range", "(", "source_domain_numbers", ")", ",", "source_domain_numbers", ")", ")", "\n", "permutation_num", "=", "len", "(", "permutation_list", ")", "\n", "for", "permutation", "in", "permutation_list", ":", "\n", "        ", "permutation", "=", "list", "(", "permutation", ")", "\n", "for", "source_idx", "in", "range", "(", "source_domain_numbers", ")", ":", "\n", "            ", "consensus_focus_dict", "[", "source_idx", "+", "1", "]", "+=", "(", "\n", "domain_contribution", "[", "frozenset", "(", "\n", "permutation", "[", ":", "permutation", ".", "index", "(", "source_idx", ")", "+", "1", "]", ")", "]", "\n", "-", "domain_contribution", "[", "\n", "frozenset", "(", "permutation", "[", ":", "permutation", ".", "index", "(", "source_idx", ")", "]", ")", "]", "\n", ")", "/", "permutation_num", "\n", "", "", "return", "consensus_focus_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.decentralized_training_strategy": [[97, 117], ["round", "round", "round", "round", "round", "NotImplementedError"], "function", ["None"], ["", "def", "decentralized_training_strategy", "(", "communication_rounds", ",", "epoch_samples", ",", "batch_size", ",", "total_epochs", ")", ":", "\n", "    ", "\"\"\"\n    Split one epoch into r rounds and perform model aggregation\n    :param communication_rounds: the communication rounds in training process\n    :param epoch_samples: the samples for each epoch\n    :param batch_size: the batch_size for each epoch\n    :param total_epochs: the total epochs for training\n    :return: batch_per_epoch, total_epochs with communication rounds r\n    \"\"\"", "\n", "if", "communication_rounds", ">=", "1", ":", "\n", "        ", "epoch_samples", "=", "round", "(", "epoch_samples", "/", "communication_rounds", ")", "\n", "total_epochs", "=", "round", "(", "total_epochs", "*", "communication_rounds", ")", "\n", "batch_per_epoch", "=", "round", "(", "epoch_samples", "/", "batch_size", ")", "\n", "", "elif", "communication_rounds", "in", "[", "0.2", ",", "0.5", "]", ":", "\n", "        ", "total_epochs", "=", "round", "(", "total_epochs", "*", "communication_rounds", ")", "\n", "batch_per_epoch", "=", "round", "(", "epoch_samples", "/", "batch_size", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"The communication round {} illegal, should be 0.2 or 0.5\"", ".", "format", "(", "communication_rounds", ")", ")", "\n", "", "return", "batch_per_epoch", ",", "total_epochs", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.mixup.mixup_data": [[5, 21], ["numpy.random.beta", "image.size", "torch.randperm().cuda", "torch.randperm", "torch.randperm"], "function", ["None"], ["def", "mixup_data", "(", "image", ",", "label", ",", "alpha", "=", "1.0", ",", "use_cuda", "=", "True", ")", ":", "\n", "    ", "'''Returns mixed inputs, pairs of targets, and lambda'''", "\n", "if", "alpha", ">", "0", ":", "\n", "        ", "lam", "=", "np", ".", "random", ".", "beta", "(", "alpha", ",", "alpha", ")", "\n", "", "else", ":", "\n", "        ", "lam", "=", "1", "\n", "\n", "", "batch_size", "=", "image", ".", "size", "(", ")", "[", "0", "]", "\n", "if", "use_cuda", ":", "\n", "        ", "index", "=", "torch", ".", "randperm", "(", "batch_size", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "index", "=", "torch", ".", "randperm", "(", "batch_size", ")", "\n", "\n", "", "mixed_image", "=", "lam", "*", "image", "+", "(", "1", "-", "lam", ")", "*", "image", "[", "index", ",", ":", "]", "\n", "label_a", ",", "label_b", "=", "label", ",", "label", "[", "index", "]", "\n", "return", "mixed_image", ",", "label_a", ",", "label_b", ",", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.mixup.mixup_criterion": [[23, 33], ["criterion", "criterion"], "function", ["None"], ["", "def", "mixup_criterion", "(", "criterion", ",", "prediction", ",", "label_a", ",", "label_b", ",", "lam", ")", ":", "\n", "    ", "\"\"\"\n    :param criterion: the cross entropy criterion\n    :param prediction: y_pred\n    :param label_a: label = lam * label_a + (1-lam)* label_b\n    :param label_b: label = lam * label_a + (1-lam)* label_b\n    :param lam: label = lam * label_a + (1-lam)* label_b\n    :return:  cross_entropy(pred,label)\n    \"\"\"", "\n", "return", "lam", "*", "criterion", "(", "prediction", ",", "label_a", ")", "+", "(", "1", "-", "lam", ")", "*", "criterion", "(", "prediction", ",", "label_b", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.avgmeter.AverageMeter.__init__": [[4, 6], ["avgmeter.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.utils.avgmeter.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.avgmeter.AverageMeter.reset": [[7, 12], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.avgmeter.AverageMeter.update": [[13, 18], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.DomainNet.__init__": [[26, 32], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_paths", ",", "data_labels", ",", "transforms", ",", "domain_name", ")", ":", "\n", "        ", "super", "(", "DomainNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data_paths", "=", "data_paths", "\n", "self", ".", "data_labels", "=", "data_labels", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "domain_name", "=", "domain_name", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.DomainNet.__getitem__": [[33, 41], ["PIL.Image.open", "DomainNet.DomainNet.transforms", "img.convert.convert.convert"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", "=", "Image", ".", "open", "(", "self", ".", "data_paths", "[", "index", "]", ")", "\n", "if", "not", "img", ".", "mode", "==", "\"RGB\"", ":", "\n", "            ", "img", "=", "img", ".", "convert", "(", "\"RGB\"", ")", "\n", "", "label", "=", "self", ".", "data_labels", "[", "index", "]", "\n", "img", "=", "self", ".", "transforms", "(", "img", ")", "\n", "\n", "return", "img", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.DomainNet.__len__": [[42, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.read_domainnet_data": [[9, 23], ["os.path.join", "open", "f.readlines", "line.strip.strip", "line.strip.split", "os.path.join", "int", "data_paths.append", "data_labels.append"], "function", ["None"], ["def", "read_domainnet_data", "(", "dataset_path", ",", "domain_name", ",", "split", "=", "\"train\"", ")", ":", "\n", "    ", "data_paths", "=", "[", "]", "\n", "data_labels", "=", "[", "]", "\n", "split_file", "=", "path", ".", "join", "(", "dataset_path", ",", "\"splits\"", ",", "\"{}_{}.txt\"", ".", "format", "(", "domain_name", ",", "split", ")", ")", "\n", "with", "open", "(", "split_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "data_path", ",", "label", "=", "line", ".", "split", "(", "' '", ")", "\n", "data_path", "=", "path", ".", "join", "(", "dataset_path", ",", "data_path", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "data_paths", ".", "append", "(", "data_path", ")", "\n", "data_labels", ".", "append", "(", "label", ")", "\n", "", "", "return", "data_paths", ",", "data_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.get_domainnet_dloader": [[46, 84], ["os.path.join", "DomainNet.read_domainnet_data", "DomainNet.read_domainnet_data", "torchvision.Compose", "torchvision.Compose", "DomainNet.DomainNet", "DomainNet.DomainNet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "torchvision.ToTensor", "torchvision.Resize", "torchvision.ToTensor"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.read_domainnet_data", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DomainNet.read_domainnet_data"], ["", "", "def", "get_domainnet_dloader", "(", "base_path", ",", "domain_name", ",", "batch_size", ",", "num_workers", ")", ":", "\n", "    ", "dataset_path", "=", "path", ".", "join", "(", "base_path", ",", "'dataset'", ",", "'DomainNet'", ")", "\n", "train_data_paths", ",", "train_data_labels", "=", "read_domainnet_data", "(", "dataset_path", ",", "domain_name", ",", "split", "=", "\"train\"", ")", "\n", "test_data_paths", ",", "test_data_labels", "=", "read_domainnet_data", "(", "dataset_path", ",", "domain_name", ",", "split", "=", "\"test\"", ")", "\n", "transforms_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ",", "scale", "=", "(", "0.75", ",", "1", ")", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "transforms_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "(", "224", ",", "224", ")", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "# transforms_train = transforms.Compose([", "\n", "#     transforms.RandomResizedCrop(96, scale=(0.75, 1)),", "\n", "#     transforms.RandomHorizontalFlip(),", "\n", "#     transforms.ToTensor()", "\n", "# ])", "\n", "# transforms_test = transforms.Compose([", "\n", "#     transforms.Resize((96,96)),", "\n", "#     transforms.ToTensor()", "\n", "# ])", "\n", "# transforms_train = transforms.Compose([", "\n", "#     transforms.RandomHorizontalFlip(),", "\n", "#     transforms.RandomVerticalFlip(),", "\n", "#     transforms.ToTensor()", "\n", "# ])", "\n", "# transforms_test = transforms.Compose([", "\n", "#     transforms.ToTensor()", "\n", "# ])", "\n", "\n", "train_dataset", "=", "DomainNet", "(", "train_data_paths", ",", "train_data_labels", ",", "transforms_train", ",", "domain_name", ")", "\n", "test_dataset", "=", "DomainNet", "(", "test_data_paths", ",", "test_data_labels", ",", "transforms_test", ",", "domain_name", ")", "\n", "train_dloader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ")", "\n", "test_dloader", "=", "DataLoader", "(", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ")", "\n", "return", "train_dloader", ",", "test_dloader", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.DigitFiveDataset.__init__": [[11, 17], ["torch.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "labels", ",", "transform", "=", "None", ",", "target_transform", "=", "None", ")", ":", "\n", "        ", "super", "(", "DigitFiveDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.DigitFiveDataset.__getitem__": [[18, 36], ["PIL.Image.fromarray", "DigitFive.DigitFiveDataset.transform", "DigitFive.DigitFiveDataset.target_transform", "numpy.uint8", "numpy.uint8", "numpy.vstack().transpose", "PIL.Image.fromarray", "numpy.asarray", "numpy.asarray", "PIL.Image.fromarray.transpose", "numpy.vstack"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", ",", "label", "=", "self", ".", "data", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "\n", "if", "img", ".", "shape", "[", "0", "]", "!=", "1", ":", "\n", "# transpose to Image type,so that the transform function can be used", "\n", "            ", "img", "=", "Image", ".", "fromarray", "(", "np", ".", "uint8", "(", "np", ".", "asarray", "(", "img", ".", "transpose", "(", "(", "1", ",", "2", ",", "0", ")", ")", ")", ")", ")", "\n", "\n", "", "elif", "img", ".", "shape", "[", "0", "]", "==", "1", ":", "\n", "            ", "im", "=", "np", ".", "uint8", "(", "np", ".", "asarray", "(", "img", ")", ")", "\n", "# turn the raw image into 3 channels", "\n", "im", "=", "np", ".", "vstack", "(", "[", "im", ",", "im", ",", "im", "]", ")", ".", "transpose", "(", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "img", "=", "Image", ".", "fromarray", "(", "im", ")", "\n", "\n", "# do transform with PIL", "\n", "", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "if", "self", ".", "target_transform", "is", "not", "None", ":", "\n", "            ", "label", "=", "self", ".", "target_transform", "(", "label", ")", "\n", "", "return", "img", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.DigitFiveDataset.__len__": [[37, 39], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "data", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_mnist": [[41, 65], ["scipy.io.loadmat", "numpy.reshape", "numpy.reshape", "numpy.concatenate", "numpy.concatenate", "mnist_train.transpose().astype.transpose().astype", "mnist_test.transpose().astype.transpose().astype", "numpy.argmax", "numpy.random.permutation", "numpy.argmax", "os.path.join", "mnist_train.transpose().astype.transpose", "mnist_test.transpose().astype.transpose"], "function", ["None"], ["", "", "def", "load_mnist", "(", "base_path", ")", ":", "\n", "    ", "mnist_data", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"mnist_data.mat\"", ")", ")", "\n", "mnist_train", "=", "np", ".", "reshape", "(", "mnist_data", "[", "'train_32'", "]", ",", "(", "55000", ",", "32", ",", "32", ",", "1", ")", ")", "\n", "mnist_test", "=", "np", ".", "reshape", "(", "mnist_data", "[", "'test_32'", "]", ",", "(", "10000", ",", "32", ",", "32", ",", "1", ")", ")", "\n", "# turn to the 3 channel image with C*H*W", "\n", "mnist_train", "=", "np", ".", "concatenate", "(", "[", "mnist_train", ",", "mnist_train", ",", "mnist_train", "]", ",", "3", ")", "\n", "mnist_test", "=", "np", ".", "concatenate", "(", "[", "mnist_test", ",", "mnist_test", ",", "mnist_test", "]", ",", "3", ")", "\n", "mnist_train", "=", "mnist_train", ".", "transpose", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "mnist_test", "=", "mnist_test", ".", "transpose", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "# get labels", "\n", "mnist_labels_train", "=", "mnist_data", "[", "'label_train'", "]", "\n", "mnist_labels_test", "=", "mnist_data", "[", "'label_test'", "]", "\n", "# random sample 25000 from train dataset and random sample 9000 from test dataset", "\n", "train_label", "=", "np", ".", "argmax", "(", "mnist_labels_train", ",", "axis", "=", "1", ")", "\n", "inds", "=", "np", ".", "random", ".", "permutation", "(", "mnist_train", ".", "shape", "[", "0", "]", ")", "\n", "mnist_train", "=", "mnist_train", "[", "inds", "]", "\n", "train_label", "=", "train_label", "[", "inds", "]", "\n", "test_label", "=", "np", ".", "argmax", "(", "mnist_labels_test", ",", "axis", "=", "1", ")", "\n", "\n", "mnist_train", "=", "mnist_train", "[", ":", "25000", "]", "\n", "train_label", "=", "train_label", "[", ":", "25000", "]", "\n", "mnist_test", "=", "mnist_test", "[", ":", "9000", "]", "\n", "test_label", "=", "test_label", "[", ":", "9000", "]", "\n", "return", "mnist_train", ",", "train_label", ",", "mnist_test", ",", "test_label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_mnist_m": [[67, 87], ["scipy.io.loadmat", "mnistm_train.transpose().astype.transpose().astype", "mnistm_test.transpose().astype.transpose().astype", "numpy.argmax", "numpy.random.permutation", "numpy.argmax", "os.path.join", "mnistm_train.transpose().astype.transpose", "mnistm_test.transpose().astype.transpose"], "function", ["None"], ["", "def", "load_mnist_m", "(", "base_path", ")", ":", "\n", "    ", "mnistm_data", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"mnistm_with_label.mat\"", ")", ")", "\n", "mnistm_train", "=", "mnistm_data", "[", "'train'", "]", "\n", "mnistm_test", "=", "mnistm_data", "[", "'test'", "]", "\n", "mnistm_train", "=", "mnistm_train", ".", "transpose", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "mnistm_test", "=", "mnistm_test", ".", "transpose", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "# get labels", "\n", "mnistm_labels_train", "=", "mnistm_data", "[", "'label_train'", "]", "\n", "mnistm_labels_test", "=", "mnistm_data", "[", "'label_test'", "]", "\n", "# random sample 25000 from train dataset and random sample 9000 from test dataset", "\n", "train_label", "=", "np", ".", "argmax", "(", "mnistm_labels_train", ",", "axis", "=", "1", ")", "\n", "inds", "=", "np", ".", "random", ".", "permutation", "(", "mnistm_train", ".", "shape", "[", "0", "]", ")", "\n", "mnistm_train", "=", "mnistm_train", "[", "inds", "]", "\n", "train_label", "=", "train_label", "[", "inds", "]", "\n", "test_label", "=", "np", ".", "argmax", "(", "mnistm_labels_test", ",", "axis", "=", "1", ")", "\n", "mnistm_train", "=", "mnistm_train", "[", ":", "25000", "]", "\n", "train_label", "=", "train_label", "[", ":", "25000", "]", "\n", "mnistm_test", "=", "mnistm_test", "[", ":", "9000", "]", "\n", "test_label", "=", "test_label", "[", ":", "9000", "]", "\n", "return", "mnistm_train", ",", "train_label", ",", "mnistm_test", ",", "test_label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_svhn": [[89, 108], ["scipy.io.loadmat", "scipy.io.loadmat", "svhn_train.transpose().astype.transpose().astype", "svhn_test.transpose().astype.transpose().astype", "svhn_train_data[].reshape", "svhn_test_data[].reshape", "numpy.random.permutation", "os.path.join", "os.path.join", "svhn_train.transpose().astype.transpose", "svhn_test.transpose().astype.transpose"], "function", ["None"], ["", "def", "load_svhn", "(", "base_path", ")", ":", "\n", "    ", "svhn_train_data", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"svhn_train_32x32.mat\"", ")", ")", "\n", "svhn_test_data", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"svhn_test_32x32.mat\"", ")", ")", "\n", "svhn_train", "=", "svhn_train_data", "[", "'X'", "]", "\n", "svhn_train", "=", "svhn_train", ".", "transpose", "(", "3", ",", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "svhn_test", "=", "svhn_test_data", "[", "'X'", "]", "\n", "svhn_test", "=", "svhn_test", ".", "transpose", "(", "3", ",", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "train_label", "=", "svhn_train_data", "[", "\"y\"", "]", ".", "reshape", "(", "-", "1", ")", "\n", "test_label", "=", "svhn_test_data", "[", "\"y\"", "]", ".", "reshape", "(", "-", "1", ")", "\n", "inds", "=", "np", ".", "random", ".", "permutation", "(", "svhn_train", ".", "shape", "[", "0", "]", ")", "\n", "svhn_train", "=", "svhn_train", "[", "inds", "]", "\n", "train_label", "=", "train_label", "[", "inds", "]", "\n", "svhn_train", "=", "svhn_train", "[", ":", "25000", "]", "\n", "train_label", "=", "train_label", "[", ":", "25000", "]", "\n", "svhn_test", "=", "svhn_test", "[", ":", "9000", "]", "\n", "test_label", "=", "test_label", "[", ":", "9000", "]", "\n", "train_label", "[", "train_label", "==", "10", "]", "=", "0", "\n", "test_label", "[", "test_label", "==", "10", "]", "=", "0", "\n", "return", "svhn_train", ",", "train_label", ",", "svhn_test", ",", "test_label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_syn": [[110, 128], ["print", "scipy.io.loadmat", "print", "scipy.io.loadmat", "syn_train.transpose().astype.transpose().astype", "syn_test.transpose().astype.transpose().astype", "syn_train_data[].reshape", "syn_test_data[].reshape", "os.path.join", "os.path.join", "syn_train.transpose().astype.transpose", "syn_test.transpose().astype.transpose"], "function", ["None"], ["", "def", "load_syn", "(", "base_path", ")", ":", "\n", "    ", "print", "(", "\"load syn train\"", ")", "\n", "syn_train_data", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"synth_train_32x32.mat\"", ")", ")", "\n", "print", "(", "\"load syn test\"", ")", "\n", "syn_test_data", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"synth_test_32x32.mat\"", ")", ")", "\n", "syn_train", "=", "syn_train_data", "[", "\"X\"", "]", "\n", "syn_test", "=", "syn_test_data", "[", "\"X\"", "]", "\n", "syn_train", "=", "syn_train", ".", "transpose", "(", "3", ",", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "syn_test", "=", "syn_test", ".", "transpose", "(", "3", ",", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "train_label", "=", "syn_train_data", "[", "\"y\"", "]", ".", "reshape", "(", "-", "1", ")", "\n", "test_label", "=", "syn_test_data", "[", "\"y\"", "]", ".", "reshape", "(", "-", "1", ")", "\n", "syn_train", "=", "syn_train", "[", ":", "25000", "]", "\n", "syn_test", "=", "syn_test", "[", ":", "9000", "]", "\n", "train_label", "=", "train_label", "[", ":", "25000", "]", "\n", "test_label", "=", "test_label", "[", ":", "9000", "]", "\n", "train_label", "[", "train_label", "==", "10", "]", "=", "0", "\n", "test_label", "[", "test_label", "==", "10", "]", "=", "0", "\n", "return", "syn_train", ",", "train_label", ",", "syn_test", ",", "test_label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_usps": [[130, 150], ["scipy.io.loadmat", "np.tile.reshape", "test_label.reshape.reshape", "numpy.concatenate", "numpy.tile", "numpy.tile", "numpy.concatenate", "os.path.join"], "function", ["None"], ["", "def", "load_usps", "(", "base_path", ")", ":", "\n", "    ", "usps_dataset", "=", "loadmat", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"DigitFive\"", ",", "\"usps_28x28.mat\"", ")", ")", "\n", "usps_dataset", "=", "usps_dataset", "[", "\"dataset\"", "]", "\n", "usps_train", "=", "usps_dataset", "[", "0", "]", "[", "0", "]", "\n", "train_label", "=", "usps_dataset", "[", "0", "]", "[", "1", "]", "\n", "train_label", "=", "train_label", ".", "reshape", "(", "-", "1", ")", "\n", "train_label", "[", "train_label", "==", "10", "]", "=", "0", "\n", "usps_test", "=", "usps_dataset", "[", "1", "]", "[", "0", "]", "\n", "test_label", "=", "usps_dataset", "[", "1", "]", "[", "1", "]", "\n", "test_label", "=", "test_label", ".", "reshape", "(", "-", "1", ")", "\n", "test_label", "[", "test_label", "==", "10", "]", "=", "0", "\n", "usps_train", "=", "usps_train", "*", "255", "\n", "usps_test", "=", "usps_test", "*", "255", "\n", "usps_train", "=", "np", ".", "concatenate", "(", "[", "usps_train", ",", "usps_train", ",", "usps_train", "]", ",", "1", ")", "\n", "usps_train", "=", "np", ".", "tile", "(", "usps_train", ",", "(", "4", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "train_label", "=", "np", ".", "tile", "(", "train_label", ",", "4", ")", "\n", "usps_train", "=", "usps_train", "[", ":", "25000", "]", "\n", "train_label", "=", "train_label", "[", ":", "25000", "]", "\n", "usps_test", "=", "np", ".", "concatenate", "(", "[", "usps_test", ",", "usps_test", ",", "usps_test", "]", ",", "1", ")", "\n", "return", "usps_train", ",", "train_label", ",", "usps_test", ",", "test_label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.digit5_dataset_read": [[152, 177], ["torchvision.Compose", "DigitFive.DigitFiveDataset", "torch.utils.data.DataLoader", "DigitFive.DigitFiveDataset", "torch.utils.data.DataLoader", "DigitFive.load_mnist", "DigitFive.load_mnist_m", "torchvision.Resize", "torchvision.ToTensor", "torchvision.Normalize", "DigitFive.load_svhn", "DigitFive.load_syn", "DigitFive.load_usps", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_mnist", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_mnist_m", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_svhn", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_syn", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.DigitFive.load_usps"], ["", "def", "digit5_dataset_read", "(", "base_path", ",", "domain", ",", "batch_size", ")", ":", "\n", "    ", "if", "domain", "==", "\"mnist\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "load_mnist", "(", "base_path", ")", "\n", "", "elif", "domain", "==", "\"mnistm\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "load_mnist_m", "(", "base_path", ")", "\n", "", "elif", "domain", "==", "\"svhn\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "load_svhn", "(", "base_path", ")", "\n", "", "elif", "domain", "==", "\"syn\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "load_syn", "(", "base_path", ")", "\n", "", "elif", "domain", "==", "\"usps\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "load_usps", "(", "base_path", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Domain {} Not Implemented\"", ".", "format", "(", "domain", ")", ")", "\n", "# define the transform function", "\n", "", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "32", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "(", "0.5", ",", "0.5", ",", "0.5", ")", ")", "\n", "]", ")", "\n", "# raise train and test data loader", "\n", "train_dataset", "=", "DigitFiveDataset", "(", "data", "=", "train_image", ",", "labels", "=", "train_label", ",", "transform", "=", "transform", ")", "\n", "train_loader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "test_dataset", "=", "DigitFiveDataset", "(", "data", "=", "test_image", ",", "labels", "=", "test_label", ",", "transform", "=", "transform", ")", "\n", "test_loader", "=", "DataLoader", "(", "dataset", "=", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "return", "train_loader", ",", "test_loader", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.MiniDomainNet.__init__": [[26, 32], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_paths", ",", "data_labels", ",", "transforms", ",", "domain_name", ")", ":", "\n", "        ", "super", "(", "MiniDomainNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data_paths", "=", "data_paths", "\n", "self", ".", "data_labels", "=", "data_labels", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "domain_name", "=", "domain_name", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.MiniDomainNet.__getitem__": [[33, 41], ["PIL.Image.open", "MiniDomainNet.MiniDomainNet.transforms", "img.convert.convert.convert"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", "=", "Image", ".", "open", "(", "self", ".", "data_paths", "[", "index", "]", ")", "\n", "if", "not", "img", ".", "mode", "==", "\"RGB\"", ":", "\n", "            ", "img", "=", "img", ".", "convert", "(", "\"RGB\"", ")", "\n", "", "label", "=", "self", ".", "data_labels", "[", "index", "]", "\n", "img", "=", "self", ".", "transforms", "(", "img", ")", "\n", "\n", "return", "img", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.MiniDomainNet.__len__": [[42, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.read_mini_domainnet_data": [[9, 23], ["os.path.join", "open", "f.readlines", "line.strip.strip", "line.strip.split", "os.path.join", "int", "data_paths.append", "data_labels.append"], "function", ["None"], ["def", "read_mini_domainnet_data", "(", "dataset_path", ",", "domain_name", ",", "split", "=", "\"train\"", ")", ":", "\n", "    ", "data_paths", "=", "[", "]", "\n", "data_labels", "=", "[", "]", "\n", "split_file", "=", "path", ".", "join", "(", "dataset_path", ",", "\"splits_mini\"", ",", "\"{}_{}.txt\"", ".", "format", "(", "domain_name", ",", "split", ")", ")", "\n", "with", "open", "(", "split_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "data_path", ",", "label", "=", "line", ".", "split", "(", "' '", ")", "\n", "data_path", "=", "path", ".", "join", "(", "dataset_path", ",", "data_path", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "data_paths", ".", "append", "(", "data_path", ")", "\n", "data_labels", ".", "append", "(", "label", ")", "\n", "", "", "return", "data_paths", ",", "data_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.get_mini_domainnet_dloader": [[46, 67], ["os.path.join", "MiniDomainNet.read_mini_domainnet_data", "MiniDomainNet.read_mini_domainnet_data", "torchvision.Compose", "torchvision.Compose", "MiniDomainNet.MiniDomainNet", "MiniDomainNet.MiniDomainNet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "torchvision.ToTensor", "torchvision.Resize", "torchvision.ToTensor"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.read_mini_domainnet_data", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.MiniDomainNet.read_mini_domainnet_data"], ["", "", "def", "get_mini_domainnet_dloader", "(", "base_path", ",", "domain_name", ",", "batch_size", ",", "num_workers", ")", ":", "\n", "    ", "dataset_path", "=", "path", ".", "join", "(", "base_path", ",", "'dataset'", ",", "'DomainNet'", ")", "\n", "train_data_paths", ",", "train_data_labels", "=", "read_mini_domainnet_data", "(", "dataset_path", ",", "domain_name", ",", "split", "=", "\"train\"", ")", "\n", "test_data_paths", ",", "test_data_labels", "=", "read_mini_domainnet_data", "(", "dataset_path", ",", "domain_name", ",", "split", "=", "\"test\"", ")", "\n", "transforms_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomResizedCrop", "(", "96", ",", "scale", "=", "(", "0.75", ",", "1", ")", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "transforms_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "(", "96", ",", "96", ")", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "\n", "train_dataset", "=", "MiniDomainNet", "(", "train_data_paths", ",", "train_data_labels", ",", "transforms_train", ",", "domain_name", ")", "\n", "test_dataset", "=", "MiniDomainNet", "(", "test_data_paths", ",", "test_data_labels", ",", "transforms_test", ",", "domain_name", ")", "\n", "train_dloader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ")", "\n", "test_dloader", "=", "DataLoader", "(", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ")", "\n", "return", "train_dloader", ",", "test_dloader", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.OfficeCaltech.__init__": [[36, 42], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_paths", ",", "data_labels", ",", "transforms", ",", "domain_name", ")", ":", "\n", "        ", "super", "(", "OfficeCaltech", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data_paths", "=", "data_paths", "\n", "self", ".", "data_labels", "=", "data_labels", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "domain_name", "=", "domain_name", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.OfficeCaltech.__getitem__": [[43, 50], ["PIL.Image.open", "OfficeCaltech10.OfficeCaltech.convert", "OfficeCaltech10.OfficeCaltech.transforms"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", "=", "Image", ".", "open", "(", "self", ".", "data_paths", "[", "index", "]", ")", "\n", "img", "=", "img", ".", "convert", "(", "\"RGB\"", ")", "\n", "label", "=", "self", ".", "data_labels", "[", "index", "]", "\n", "img", "=", "self", ".", "transforms", "(", "img", ")", "\n", "\n", "return", "img", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.OfficeCaltech.__len__": [[51, 53], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.listdir_nohidden": [[10, 17], ["os.listdir", "f.startswith"], "function", ["None"], ["def", "listdir_nohidden", "(", "path", ")", ":", "\n", "    ", "\"\"\"List non-hidden items in a directory.\n\n    Args:\n         path (str): directory path.\n    \"\"\"", "\n", "return", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", "if", "not", "f", ".", "startswith", "(", "'.'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.read_office_caltech10_data": [[19, 33], ["os.path.join", "OfficeCaltech10.listdir_nohidden", "listdir_nohidden.sort", "enumerate", "os.path.join", "OfficeCaltech10.listdir_nohidden", "os.path.join", "data_paths.append", "data_labels.append"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.listdir_nohidden", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.listdir_nohidden"], ["", "def", "read_office_caltech10_data", "(", "dataset_path", ",", "domain_name", ")", ":", "\n", "    ", "data_paths", "=", "[", "]", "\n", "data_labels", "=", "[", "]", "\n", "domain_dir", "=", "path", ".", "join", "(", "dataset_path", ",", "domain_name", ")", "\n", "class_names", "=", "listdir_nohidden", "(", "domain_dir", ")", "\n", "class_names", ".", "sort", "(", ")", "\n", "for", "label", ",", "class_name", "in", "enumerate", "(", "class_names", ")", ":", "\n", "        ", "class_dir", "=", "path", ".", "join", "(", "domain_dir", ",", "class_name", ")", "\n", "item_names", "=", "listdir_nohidden", "(", "class_dir", ")", "\n", "for", "item_name", "in", "item_names", ":", "\n", "            ", "item_path", "=", "path", ".", "join", "(", "class_dir", ",", "item_name", ")", "\n", "data_paths", ".", "append", "(", "item_path", ")", "\n", "data_labels", ".", "append", "(", "label", ")", "\n", "", "", "return", "data_paths", ",", "data_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.get_office_caltech10_split_sampler": [[55, 75], ["range", "torch.utils.data.sampler.SubsetRandomSampler", "torch.utils.data.sampler.SubsetRandomSampler", "torch.nonzero", "loc.view.view", "round", "torch.utils.data.sampler.SubsetRandomSampler.extend", "torch.utils.data.sampler.SubsetRandomSampler.extend", "loc[].tolist", "loc[].tolist", "loc.view.size", "torch.randperm", "loc.view.size"], "function", ["None"], ["", "", "def", "get_office_caltech10_split_sampler", "(", "labels", ",", "test_ratio", "=", "0.2", ",", "num_classes", "=", "10", ")", ":", "\n", "    ", "\"\"\"\n    :param labels: torch.array(long tensor)\n    :param test_ratio: the ratio to split part of the data for test\n    :param num_classes: 10\n    :return: sampler_train,sampler_test\n    \"\"\"", "\n", "sampler_test", "=", "[", "]", "\n", "sampler_train", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "        ", "loc", "=", "torch", ".", "nonzero", "(", "labels", "==", "i", ")", "\n", "loc", "=", "loc", ".", "view", "(", "-", "1", ")", "\n", "# do random perm to make sure uniform sample", "\n", "test_num", "=", "round", "(", "loc", ".", "size", "(", "0", ")", "*", "test_ratio", ")", "\n", "loc", "=", "loc", "[", "torch", ".", "randperm", "(", "loc", ".", "size", "(", "0", ")", ")", "]", "\n", "sampler_test", ".", "extend", "(", "loc", "[", ":", "test_num", "]", ".", "tolist", "(", ")", ")", "\n", "sampler_train", ".", "extend", "(", "loc", "[", "test_num", ":", "]", ".", "tolist", "(", ")", ")", "\n", "", "sampler_test", "=", "SubsetRandomSampler", "(", "sampler_test", ")", "\n", "sampler_train", "=", "SubsetRandomSampler", "(", "sampler_train", ")", "\n", "return", "sampler_train", ",", "sampler_test", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.get_office_caltech10_dloader": [[77, 97], ["os.path.join", "OfficeCaltech10.read_office_caltech10_data", "torchvision.Compose", "torchvision.Compose", "OfficeCaltech10.OfficeCaltech", "OfficeCaltech10.OfficeCaltech", "OfficeCaltech10.get_office_caltech10_split_sampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.LongTensor", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "torchvision.ToTensor", "torchvision.Resize", "torchvision.ToTensor"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.read_office_caltech10_data", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.OfficeCaltech10.get_office_caltech10_split_sampler"], ["", "def", "get_office_caltech10_dloader", "(", "base_path", ",", "domain_name", ",", "batch_size", ",", "num_workers", ")", ":", "\n", "    ", "dataset_path", "=", "path", ".", "join", "(", "base_path", ",", "'dataset'", ",", "'OfficeCaltech10'", ")", "\n", "data_paths", ",", "data_labels", "=", "read_office_caltech10_data", "(", "dataset_path", ",", "domain_name", ")", "\n", "transforms_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ",", "scale", "=", "(", "0.75", ",", "1", ")", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "transforms_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "(", "224", ",", "224", ")", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "train_dataset", "=", "OfficeCaltech", "(", "data_paths", ",", "data_labels", ",", "transforms_train", ",", "domain_name", ")", "\n", "test_dataset", "=", "OfficeCaltech", "(", "data_paths", ",", "data_labels", ",", "transforms_test", ",", "domain_name", ")", "\n", "sampler_train", ",", "sampler_test", "=", "get_office_caltech10_split_sampler", "(", "torch", ".", "LongTensor", "(", "data_labels", ")", ")", "\n", "train_dloader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "sampler_train", ")", "\n", "test_dloader", "=", "DataLoader", "(", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "sampler_test", ")", "\n", "return", "train_dloader", ",", "test_dloader", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.Office31.__init__": [[36, 42], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_paths", ",", "data_labels", ",", "transforms", ",", "domain_name", ")", ":", "\n", "        ", "super", "(", "Office31", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data_paths", "=", "data_paths", "\n", "self", ".", "data_labels", "=", "data_labels", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "domain_name", "=", "domain_name", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.Office31.__getitem__": [[43, 50], ["PIL.Image.open", "Office31.Office31.convert", "Office31.Office31.transforms"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", "=", "Image", ".", "open", "(", "self", ".", "data_paths", "[", "index", "]", ")", "\n", "img", "=", "img", ".", "convert", "(", "\"RGB\"", ")", "\n", "label", "=", "self", ".", "data_labels", "[", "index", "]", "\n", "img", "=", "self", ".", "transforms", "(", "img", ")", "\n", "\n", "return", "img", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.Office31.__len__": [[51, 53], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.listdir_nohidden": [[10, 17], ["os.listdir", "f.startswith"], "function", ["None"], ["def", "listdir_nohidden", "(", "path", ")", ":", "\n", "    ", "\"\"\"List non-hidden items in a directory.\n\n    Args:\n         path (str): directory path.\n    \"\"\"", "\n", "return", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", "if", "not", "f", ".", "startswith", "(", "'.'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.read_office31_data": [[19, 33], ["os.path.join", "Office31.listdir_nohidden", "listdir_nohidden.sort", "enumerate", "os.path.join", "Office31.listdir_nohidden", "os.path.join", "data_paths.append", "data_labels.append"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.listdir_nohidden", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.listdir_nohidden"], ["", "def", "read_office31_data", "(", "dataset_path", ",", "domain_name", ")", ":", "\n", "    ", "data_paths", "=", "[", "]", "\n", "data_labels", "=", "[", "]", "\n", "domain_dir", "=", "path", ".", "join", "(", "dataset_path", ",", "domain_name", ",", "\"images\"", ")", "\n", "class_names", "=", "listdir_nohidden", "(", "domain_dir", ")", "\n", "class_names", ".", "sort", "(", ")", "\n", "for", "label", ",", "class_name", "in", "enumerate", "(", "class_names", ")", ":", "\n", "        ", "class_dir", "=", "path", ".", "join", "(", "domain_dir", ",", "class_name", ")", "\n", "item_names", "=", "listdir_nohidden", "(", "class_dir", ")", "\n", "for", "item_name", "in", "item_names", ":", "\n", "            ", "item_path", "=", "path", ".", "join", "(", "class_dir", ",", "item_name", ")", "\n", "data_paths", ".", "append", "(", "item_path", ")", "\n", "data_labels", ".", "append", "(", "label", ")", "\n", "", "", "return", "data_paths", ",", "data_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.get_office31_split_sampler": [[55, 75], ["range", "torch.utils.data.sampler.SubsetRandomSampler", "torch.utils.data.sampler.SubsetRandomSampler", "torch.nonzero", "loc.view.view", "round", "torch.utils.data.sampler.SubsetRandomSampler.extend", "torch.utils.data.sampler.SubsetRandomSampler.extend", "loc[].tolist", "loc[].tolist", "loc.view.size", "torch.randperm", "loc.view.size"], "function", ["None"], ["", "", "def", "get_office31_split_sampler", "(", "labels", ",", "test_ratio", "=", "0.2", ",", "num_classes", "=", "31", ")", ":", "\n", "    ", "\"\"\"\n    :param labels: torch.array(long tensor)\n    :param test_ratio: the ratio to split part of the data for test\n    :param num_classes: 31\n    :return: sampler_train,sampler_test\n    \"\"\"", "\n", "sampler_test", "=", "[", "]", "\n", "sampler_train", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "        ", "loc", "=", "torch", ".", "nonzero", "(", "labels", "==", "i", ")", "\n", "loc", "=", "loc", ".", "view", "(", "-", "1", ")", "\n", "# do random perm to make sure uniform sample", "\n", "test_num", "=", "round", "(", "loc", ".", "size", "(", "0", ")", "*", "test_ratio", ")", "\n", "loc", "=", "loc", "[", "torch", ".", "randperm", "(", "loc", ".", "size", "(", "0", ")", ")", "]", "\n", "sampler_test", ".", "extend", "(", "loc", "[", ":", "test_num", "]", ".", "tolist", "(", ")", ")", "\n", "sampler_train", ".", "extend", "(", "loc", "[", "test_num", ":", "]", ".", "tolist", "(", ")", ")", "\n", "", "sampler_test", "=", "SubsetRandomSampler", "(", "sampler_test", ")", "\n", "sampler_train", "=", "SubsetRandomSampler", "(", "sampler_train", ")", "\n", "return", "sampler_train", ",", "sampler_test", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.get_office31_dloader": [[77, 97], ["os.path.join", "Office31.read_office31_data", "torchvision.Compose", "torchvision.Compose", "Office31.Office31", "Office31.Office31", "Office31.get_office31_split_sampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.LongTensor", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "torchvision.ToTensor", "torchvision.Resize", "torchvision.ToTensor"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.read_office31_data", "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.Office31.get_office31_split_sampler"], ["", "def", "get_office31_dloader", "(", "base_path", ",", "domain_name", ",", "batch_size", ",", "num_workers", ")", ":", "\n", "    ", "dataset_path", "=", "path", ".", "join", "(", "base_path", ",", "'dataset'", ",", "'Office31'", ")", "\n", "data_paths", ",", "data_labels", "=", "read_office31_data", "(", "dataset_path", ",", "domain_name", ")", "\n", "transforms_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ",", "scale", "=", "(", "0.75", ",", "1", ")", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "transforms_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "(", "224", ",", "224", ")", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", "\n", "]", ")", "\n", "train_dataset", "=", "Office31", "(", "data_paths", ",", "data_labels", ",", "transforms_train", ",", "domain_name", ")", "\n", "test_dataset", "=", "Office31", "(", "data_paths", ",", "data_labels", ",", "transforms_test", ",", "domain_name", ")", "\n", "sampler_train", ",", "sampler_test", "=", "get_office31_split_sampler", "(", "torch", ".", "LongTensor", "(", "data_labels", ")", ")", "\n", "train_dloader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "sampler_train", ")", "\n", "test_dloader", "=", "DataLoader", "(", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "sampler_test", ")", "\n", "return", "train_dloader", ",", "test_dloader", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.AmazonReviewDataset.__init__": [[15, 18], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "labels", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.AmazonReviewDataset.__getitem__": [[19, 22], ["numpy.squeeze", "numpy.squeeze", "numpy.asarray", "numpy.asarray"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "tensor", ",", "label", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "self", ".", "data", "[", "index", "]", ")", ")", ",", "self", ".", "labels", "[", "index", "]", "\n", "return", "tensor", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.AmazonReviewDataset.__len__": [[23, 25], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.load_amazon": [[27, 52], ["numpy.load", "scipy.sparse.coo_matrix().tocsc", "amazon[].flatten", "range", "os.path.join", "data_insts.append", "data_labels.append", "num_insts.append", "numpy.arange", "numpy.random.shuffle", "data_insts[].todense().astype", "data_labels[].ravel().astype", "scipy.sparse.coo_matrix", "data_insts[].todense", "data_labels[].ravel"], "function", ["None"], ["", "", "def", "load_amazon", "(", "base_path", ")", ":", "\n", "    ", "dimension", "=", "5000", "\n", "amazon", "=", "np", ".", "load", "(", "path", ".", "join", "(", "base_path", ",", "\"dataset\"", ",", "\"AmazonReview\"", ",", "\"amazon.npz\"", ")", ")", "\n", "amazon_xx", "=", "coo_matrix", "(", "(", "amazon", "[", "'xx_data'", "]", ",", "(", "amazon", "[", "'xx_col'", "]", ",", "amazon", "[", "'xx_row'", "]", ")", ")", ",", "\n", "shape", "=", "amazon", "[", "'xx_shape'", "]", "[", ":", ":", "-", "1", "]", ")", ".", "tocsc", "(", ")", "\n", "amazon_xx", "=", "amazon_xx", "[", ":", ",", ":", "dimension", "]", "\n", "amazon_yy", "=", "amazon", "[", "'yy'", "]", "\n", "amazon_yy", "=", "(", "amazon_yy", "+", "1", ")", "/", "2", "\n", "amazon_offset", "=", "amazon", "[", "'offset'", "]", ".", "flatten", "(", ")", "\n", "# Partition the data into four categories and for each category partition the data set into training and test set.", "\n", "data_name", "=", "[", "\"books\"", ",", "\"dvd\"", ",", "\"electronics\"", ",", "\"kitchen\"", "]", "\n", "num_data_sets", "=", "4", "\n", "data_insts", ",", "data_labels", ",", "num_insts", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "num_data_sets", ")", ":", "\n", "        ", "data_insts", ".", "append", "(", "amazon_xx", "[", "amazon_offset", "[", "i", "]", ":", "amazon_offset", "[", "i", "+", "1", "]", ",", ":", "]", ")", "\n", "data_labels", ".", "append", "(", "amazon_yy", "[", "amazon_offset", "[", "i", "]", ":", "amazon_offset", "[", "i", "+", "1", "]", ",", ":", "]", ")", "\n", "num_insts", ".", "append", "(", "amazon_offset", "[", "i", "+", "1", "]", "-", "amazon_offset", "[", "i", "]", ")", "\n", "# Randomly shuffle.", "\n", "r_order", "=", "np", ".", "arange", "(", "num_insts", "[", "i", "]", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "r_order", ")", "\n", "data_insts", "[", "i", "]", "=", "data_insts", "[", "i", "]", "[", "r_order", ",", ":", "]", "\n", "data_labels", "[", "i", "]", "=", "data_labels", "[", "i", "]", "[", "r_order", ",", ":", "]", "\n", "data_insts", "[", "i", "]", "=", "data_insts", "[", "i", "]", ".", "todense", "(", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "data_labels", "[", "i", "]", "=", "data_labels", "[", "i", "]", ".", "ravel", "(", ")", ".", "astype", "(", "np", ".", "int64", ")", "\n", "", "return", "data_insts", ",", "data_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.amazon_dataset_read": [[54, 80], ["AmazonReview.load_amazon", "AmazonReview.AmazonReviewDataset", "torch.utils.data.DataLoader", "AmazonReview.AmazonReviewDataset", "torch.utils.data.DataLoader", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.datasets.AmazonReview.load_amazon"], ["", "def", "amazon_dataset_read", "(", "base_path", ",", "domain", ",", "batch_size", ")", ":", "\n", "    ", "data_insts", ",", "data_labels", "=", "load_amazon", "(", "base_path", ")", "\n", "if", "domain", "==", "\"books\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "data_insts", "[", "0", "]", "[", ":", "2000", "]", ",", "data_labels", "[", "0", "]", "[", ":", "2000", "]", ",", "data_insts", "[", "0", "]", "[", "\n", "2000", ":", "]", ",", "data_labels", "[", "0", "]", "[", "2000", ":", "]", "\n", "", "elif", "domain", "==", "\"dvd\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "data_insts", "[", "1", "]", "[", ":", "2000", "]", ",", "data_labels", "[", "1", "]", "[", ":", "2000", "]", ",", "data_insts", "[", "1", "]", "[", "\n", "2000", ":", "]", ",", "data_labels", "[", "1", "]", "[", "2000", ":", "]", "\n", "", "elif", "domain", "==", "\"electronics\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "data_insts", "[", "2", "]", "[", ":", "2000", "]", ",", "data_labels", "[", "2", "]", "[", ":", "2000", "]", ",", "data_insts", "[", "2", "]", "[", "\n", "2000", ":", "]", ",", "data_labels", "[", "2", "]", "[", "2000", ":", "]", "\n", "", "elif", "domain", "==", "\"kitchen\"", ":", "\n", "        ", "train_image", ",", "train_label", ",", "test_image", ",", "test_label", "=", "data_insts", "[", "3", "]", "[", ":", "2000", "]", ",", "data_labels", "[", "3", "]", "[", ":", "2000", "]", ",", "data_insts", "[", "3", "]", "[", "\n", "2000", ":", "]", ",", "data_labels", "[", "3", "]", "[", "2000", ":", "]", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Domain {} Not Implemented\"", ".", "format", "(", "domain", ")", ")", "\n", "# raise train and test data loader", "\n", "", "train_dataset", "=", "AmazonReviewDataset", "(", "data", "=", "train_image", ",", "labels", "=", "train_label", ")", "\n", "train_loader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "test_dataset", "=", "AmazonReviewDataset", "(", "data", "=", "test_image", ",", "labels", "=", "test_label", ")", "\n", "test_loader", "=", "DataLoader", "(", "dataset", "=", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "return", "train_loader", ",", "test_loader", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.officecaltech10.OfficeCaltechNet.__init__": [[10, 17], ["torch.Module.__init__", "resnet.get_resnet", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.get_resnet"], ["    ", "def", "__init__", "(", "self", ",", "backbone", ",", "bn_momentum", ",", "pretrained", "=", "True", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "OfficeCaltechNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "encoder", "=", "get_resnet", "(", "backbone", ",", "momentumn", "=", "bn_momentum", ",", "pretrained", "=", "pretrained", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "encoder", "=", "nn", ".", "DataParallel", "(", "encoder", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "=", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.officecaltech10.OfficeCaltechNet.forward": [[18, 21], ["officecaltech10.OfficeCaltechNet.encoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "feature", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.officecaltech10.OfficeCaltechClassifier.__init__": [[24, 32], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Linear", "torch.Linear", "torch.Linear", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "backbone", ",", "classes", "=", "10", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "OfficeCaltechClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "linear", "=", "nn", ".", "Sequential", "(", ")", "\n", "linear", ".", "add_module", "(", "\"fc\"", ",", "nn", ".", "Linear", "(", "feature_dict", "[", "backbone", "]", ",", "classes", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "DataParallel", "(", "linear", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.officecaltech10.OfficeCaltechClassifier.forward": [[33, 37], ["torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "officecaltech10.OfficeCaltechClassifier.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "feature", ")", ":", "\n", "        ", "feature", "=", "torch", ".", "flatten", "(", "feature", ",", "1", ")", "\n", "feature", "=", "self", ".", "linear", "(", "feature", ")", "\n", "return", "feature", "\n", "", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet._PreProcess.__init__": [[9, 21], ["torch.Sequential.__init__", "wideresnet._PreProcess.add_module", "wideresnet._PreProcess.add_module", "wideresnet._PreProcess.add_module", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.MaxPool2d", "torch.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_input_channels", ",", "num_init_features", "=", "16", ",", "small_input", "=", "True", ")", ":", "\n", "        ", "super", "(", "_PreProcess", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "small_input", ":", "\n", "            ", "self", ".", "add_module", "(", "'conv0'", ",", "\n", "nn", ".", "Conv2d", "(", "num_input_channels", ",", "num_init_features", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "\n", "bias", "=", "True", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "add_module", "(", "'conv0'", ",", "\n", "nn", ".", "Conv2d", "(", "num_input_channels", ",", "num_init_features", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "\n", "bias", "=", "True", ")", ")", "\n", "self", ".", "add_module", "(", "'pool0'", ",", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "\n", "ceil_mode", "=", "False", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet._WideResUnit.__init__": [[24, 44], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "wideresnet._WideResUnit.f_block.add_module", "wideresnet._WideResUnit.f_block.add_module", "wideresnet._WideResUnit.f_block.add_module", "wideresnet._WideResUnit.f_block.add_module", "wideresnet._WideResUnit.f_block.add_module", "wideresnet._WideResUnit.f_block.add_module", "wideresnet._WideResUnit.f_block.add_module", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Dropout", "torch.Dropout", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "wideresnet._WideResUnit.i_block.add_module", "wideresnet._WideResUnit.i_block.add_module", "wideresnet._WideResUnit.i_block.add_module", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_input_features", ",", "num_output_features", ",", "stride", "=", "1", ",", "drop_rate", "=", "0.3", ")", ":", "\n", "        ", "super", "(", "_WideResUnit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "f_block", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'norm1'", ",", "nn", ".", "BatchNorm2d", "(", "num_input_features", ")", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'relu1'", ",", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'conv1'", ",", "nn", ".", "Conv2d", "(", "num_input_features", ",", "num_output_features", ",", "\n", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'dropout'", ",", "nn", ".", "Dropout", "(", "drop_rate", ")", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'norm2'", ",", "nn", ".", "BatchNorm2d", "(", "num_output_features", ")", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'relu2'", ",", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "f_block", ".", "add_module", "(", "'conv2'", ",", "nn", ".", "Conv2d", "(", "num_output_features", ",", "num_output_features", ",", "\n", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "\n", "if", "num_input_features", "!=", "num_output_features", "or", "stride", "!=", "1", ":", "\n", "            ", "self", ".", "i_block", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "i_block", ".", "add_module", "(", "'norm'", ",", "nn", ".", "BatchNorm2d", "(", "num_input_features", ")", ")", "\n", "self", ".", "i_block", ".", "add_module", "(", "'relu'", ",", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "i_block", ".", "add_module", "(", "'conv'", ",", "\n", "nn", ".", "Conv2d", "(", "num_input_features", ",", "num_output_features", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet._WideResUnit.forward": [[45, 50], ["wideresnet._WideResUnit.f_block", "hasattr", "wideresnet._WideResUnit.i_block"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_features", "=", "self", ".", "f_block", "(", "x", ")", "\n", "if", "hasattr", "(", "self", ",", "\"i_block\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "i_block", "(", "x", ")", "\n", "", "return", "new_features", "+", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet._WideBlock.__init__": [[53, 63], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "range", "wideresnet._WideBlock.wide_block.add_module", "wideresnet._WideResUnit", "wideresnet._WideResUnit", "int"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channel", ",", "channel_width", ",", "block_depth", ",", "down_sample", "=", "False", ",", "drop_rate", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "_WideBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "wide_block", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "i", "in", "range", "(", "block_depth", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "unit", "=", "_WideResUnit", "(", "input_channel", ",", "channel_width", ",", "stride", "=", "int", "(", "1", "+", "down_sample", ")", ",", "\n", "drop_rate", "=", "drop_rate", ")", "\n", "", "else", ":", "\n", "                ", "unit", "=", "_WideResUnit", "(", "channel_width", ",", "channel_width", ",", "drop_rate", "=", "drop_rate", ")", "\n", "", "self", ".", "wide_block", ".", "add_module", "(", "\"wideunit%d\"", "%", "(", "i", "+", "1", ")", ",", "unit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet._WideBlock.forward": [[64, 66], ["wideresnet._WideBlock.wide_block"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "wide_block", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet.WideResNet.__init__": [[69, 96], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "wideresnet._PreProcess", "wideresnet.WideResNet.encoder.add_module", "enumerate", "torch.Sequential", "torch.Sequential", "torch.DataParallel.add_module", "torch.DataParallel.add_module", "wideresnet.WideResNet.encoder.add_module", "int", "torch.DataParallel", "torch.DataParallel", "wideresnet.WideResNet.encoder.add_module", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.DataParallel", "torch.DataParallel", "wideresnet._WideBlock", "wideresnet._WideBlock", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_input_channels", "=", "3", ",", "num_init_features", "=", "16", ",", "depth", "=", "28", ",", "width", "=", "2", ",", "\n", "data_parallel", "=", "True", ",", "small_input", "=", "True", ",", "drop_rate", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "WideResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "depth", "-", "4", ")", "%", "6", "==", "0", ",", "'depth should be 6n+4'", "\n", "block_depth", "=", "(", "depth", "-", "4", ")", "//", "6", "\n", "widths", "=", "[", "int", "(", "v", "*", "width", ")", "for", "v", "in", "(", "16", ",", "32", ",", "64", ")", "]", "\n", "self", ".", "_widths", "=", "widths", "\n", "self", ".", "encoder", "=", "nn", ".", "Sequential", "(", ")", "\n", "pre_process", "=", "_PreProcess", "(", "num_input_channels", ",", "num_init_features", ",", "small_input", "=", "small_input", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "pre_process", "=", "nn", ".", "DataParallel", "(", "pre_process", ")", "\n", "", "self", ".", "encoder", ".", "add_module", "(", "\"pre_process\"", ",", "pre_process", ")", "\n", "for", "idx", ",", "width", "in", "enumerate", "(", "widths", ")", ":", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "wide_block", "=", "_WideBlock", "(", "num_init_features", ",", "width", ",", "block_depth", ",", "drop_rate", "=", "drop_rate", ")", "\n", "", "else", ":", "\n", "                ", "wide_block", "=", "_WideBlock", "(", "widths", "[", "idx", "-", "1", "]", ",", "width", ",", "block_depth", ",", "down_sample", "=", "True", ",", "drop_rate", "=", "drop_rate", ")", "\n", "", "if", "data_parallel", ":", "\n", "                ", "wide_block", "=", "nn", ".", "DataParallel", "(", "wide_block", ")", "\n", "", "self", ".", "encoder", ".", "add_module", "(", "\"wideblock%d\"", "%", "(", "idx", "+", "1", ")", ",", "wide_block", ")", "\n", "", "trans", "=", "nn", ".", "Sequential", "(", ")", "\n", "trans", ".", "add_module", "(", "\"norm\"", ",", "nn", ".", "BatchNorm2d", "(", "widths", "[", "-", "1", "]", ")", ")", "\n", "trans", ".", "add_module", "(", "\"relu\"", ",", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "trans", "=", "nn", ".", "DataParallel", "(", "trans", ")", "\n", "", "self", ".", "encoder", ".", "add_module", "(", "'transition'", ",", "trans", ")", "\n", "self", ".", "num_feature_channel", "=", "widths", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet.WideResNet.forward": [[97, 100], ["wideresnet.WideResNet.encoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_img", ")", ":", "\n", "        ", "features", "=", "self", ".", "encoder", "(", "input_img", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.wideresnet.get_wide_resnet": [[102, 115], ["re.findall", "eval", "eval", "wideresnet.WideResNet"], "function", ["None"], ["", "", "def", "get_wide_resnet", "(", "name", ",", "drop_rate", "=", "0.0", ",", "input_channels", "=", "3", ",", "small_input", "=", "True", ",", "data_parallel", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    :param name: wideresnet-depth-width type e.g. wideresnet-28-2\n    :param drop_rate: the drop rate\n    :param data_parallel:\n    :param input_channels:\n    :return: widresnet encoder\n    \"\"\"", "\n", "depth", ",", "width", "=", "re", ".", "findall", "(", "r'\\d+'", ",", "name", ")", "\n", "depth", "=", "eval", "(", "depth", ")", "\n", "width", "=", "eval", "(", "width", ")", "\n", "return", "WideResNet", "(", "depth", "=", "depth", ",", "width", "=", "width", ",", "drop_rate", "=", "drop_rate", ",", "num_input_channels", "=", "input_channels", ",", "\n", "data_parallel", "=", "data_parallel", ",", "small_input", "=", "small_input", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.BasicBlock.__init__": [[28, 45], ["torch.Module.__init__", "resnet.conv3x3", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "resnet.conv3x3", "norm_layer", "ValueError", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv3x3", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv3x3"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "if", "groups", "!=", "1", "or", "base_width", "!=", "64", ":", "\n", "            ", "raise", "ValueError", "(", "'BasicBlock only supports groups=1 and base_width=64'", ")", "\n", "", "if", "dilation", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dilation > 1 not supported in BasicBlock\"", ")", "\n", "# Both self.conv1 and self.downsample layers downsample the input when stride != 1", "\n", "", "self", ".", "conv1", "=", "conv3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "planes", ",", "momentum", "=", "bn_momentum", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "conv3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "planes", ",", "momentum", "=", "bn_momentum", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.BasicBlock.forward": [[46, 63], ["resnet.BasicBlock.conv1", "resnet.BasicBlock.bn1", "resnet.BasicBlock.relu", "resnet.BasicBlock.conv2", "resnet.BasicBlock.bn2", "resnet.BasicBlock.relu", "resnet.BasicBlock.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.Bottleneck.__init__": [[74, 90], ["torch.Module.__init__", "resnet.conv1x1", "norm_layer", "resnet.conv3x3", "norm_layer", "resnet.conv1x1", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "int"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv1x1", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv3x3", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv1x1"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "width", "=", "int", "(", "planes", "*", "(", "base_width", "/", "64.", ")", ")", "*", "groups", "\n", "# Both self.conv2 and self.downsample layers downsample the input when stride != 1", "\n", "self", ".", "conv1", "=", "conv1x1", "(", "inplanes", ",", "width", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "width", ",", "momentum", "=", "bn_momentum", ")", "\n", "self", ".", "conv2", "=", "conv3x3", "(", "width", ",", "width", ",", "stride", ",", "groups", ",", "dilation", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "width", ",", "momentum", "=", "bn_momentum", ")", "\n", "self", ".", "conv3", "=", "conv1x1", "(", "width", ",", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "planes", "*", "self", ".", "expansion", ",", "momentum", "=", "bn_momentum", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.Bottleneck.forward": [[91, 112], ["resnet.Bottleneck.conv1", "resnet.Bottleneck.bn1", "resnet.Bottleneck.relu", "resnet.Bottleneck.conv2", "resnet.Bottleneck.bn2", "resnet.Bottleneck.relu", "resnet.Bottleneck.conv3", "resnet.Bottleneck.bn3", "resnet.Bottleneck.relu", "resnet.Bottleneck.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet.__init__": [[115, 163], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "resnet.ResNet.modules", "len", "ValueError", "isinstance", "resnet.ResNet.modules", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "block", ",", "layers", ",", "zero_init_residual", "=", "False", ",", "\n", "groups", "=", "1", ",", "width_per_group", "=", "64", ",", "replace_stride_with_dilation", "=", "None", ",", "\n", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "self", ".", "_norm_layer", "=", "norm_layer", "\n", "\n", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "dilation", "=", "1", "\n", "if", "replace_stride_with_dilation", "is", "None", ":", "\n", "# each element in the tuple indicates if we should replace", "\n", "# the 2x2 stride with a dilated convolution instead", "\n", "            ", "replace_stride_with_dilation", "=", "[", "False", ",", "False", ",", "False", "]", "\n", "", "if", "len", "(", "replace_stride_with_dilation", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\"replace_stride_with_dilation should be None \"", "\n", "\"or a 3-element tuple, got {}\"", ".", "format", "(", "replace_stride_with_dilation", ")", ")", "\n", "", "self", ".", "groups", "=", "groups", "\n", "self", ".", "base_width", "=", "width_per_group", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "3", ",", "self", ".", "inplanes", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "self", ".", "inplanes", ",", "momentum", "=", "bn_momentum", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "0", "]", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "1", "]", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "2", "]", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "nn", ".", "BatchNorm2d", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n", "# Zero-initialize the last BN in each residual branch,", "\n", "# so that the residual branch starts with zeros, and each residual block behaves like an identity.", "\n", "# This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677", "\n", "", "", "if", "zero_init_residual", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "Bottleneck", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn3", ".", "weight", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "BasicBlock", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn2", ".", "weight", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._make_layer": [[164, 187], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "resnet.conv1x1", "norm_layer", "block"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv1x1"], ["", "", "", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ",", "dilate", "=", "False", ")", ":", "\n", "        ", "norm_layer", "=", "self", ".", "_norm_layer", "\n", "downsample", "=", "None", "\n", "previous_dilation", "=", "self", ".", "dilation", "\n", "if", "dilate", ":", "\n", "            ", "self", ".", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "conv1x1", "(", "self", ".", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "stride", ")", ",", "\n", "norm_layer", "(", "planes", "*", "block", ".", "expansion", ",", "momentum", "=", "bn_momentum", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "self", ".", "groups", ",", "\n", "self", ".", "base_width", ",", "previous_dilation", ",", "norm_layer", ")", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "_", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "groups", "=", "self", ".", "groups", ",", "\n", "base_width", "=", "self", ".", "base_width", ",", "dilation", "=", "self", ".", "dilation", ",", "\n", "norm_layer", "=", "norm_layer", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._forward_impl": [[188, 201], ["resnet.ResNet.conv1", "resnet.ResNet.bn1", "resnet.ResNet.relu", "resnet.ResNet.maxpool", "resnet.ResNet.layer1", "resnet.ResNet.layer2", "resnet.ResNet.layer3", "resnet.ResNet.layer4", "resnet.ResNet.avgpool"], "methods", ["None"], ["", "def", "_forward_impl", "(", "self", ",", "x", ")", ":", "\n", "# See note [TorchScript super()]", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet.forward": [[202, 204], ["resnet.ResNet._forward_impl"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.ResNet._forward_impl"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "_forward_impl", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv3x3": [[14, 18], ["torch.Conv2d"], "function", ["None"], ["def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "\"\"\"3x3 convolution with padding\"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "False", ",", "dilation", "=", "dilation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.conv1x1": [[20, 23], ["torch.Conv2d"], "function", ["None"], ["", "def", "conv1x1", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"1x1 convolution\"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet._resnet": [[206, 212], ["resnet.ResNet", "torch.load_url", "ResNet.load_state_dict"], "function", ["None"], ["", "", "def", "_resnet", "(", "arch", ",", "block", ",", "layers", ",", "pretrained", ")", ":", "\n", "    ", "model", "=", "ResNet", "(", "block", ",", "layers", ")", "\n", "if", "pretrained", ":", "\n", "        ", "state_dict", "=", "model_zoo", ".", "load_url", "(", "model_urls", "[", "arch", "]", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.get_resnet": [[214, 228], ["resnet._resnet", "resnet._resnet", "resnet._resnet", "resnet._resnet", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet._resnet", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet._resnet", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet._resnet", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet._resnet"], ["", "def", "get_resnet", "(", "name", ",", "momentumn", ",", "pretrained", "=", "True", ")", ":", "\n", "    ", "global", "bn_momentum", "\n", "bn_momentum", "=", "momentumn", "\n", "if", "name", "==", "\"resnet18\"", ":", "\n", "        ", "model", "=", "_resnet", "(", "'resnet18'", ",", "BasicBlock", ",", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "pretrained", ")", "\n", "", "elif", "name", "==", "\"resnet34\"", ":", "\n", "        ", "model", "=", "_resnet", "(", "'resnet34'", ",", "BasicBlock", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "pretrained", ")", "\n", "", "elif", "name", "==", "\"resnet50\"", ":", "\n", "        ", "model", "=", "_resnet", "(", "'resnet50'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "pretrained", ")", "\n", "", "elif", "name", "==", "\"resnet101\"", ":", "\n", "        ", "model", "=", "_resnet", "(", "\"resnet101\"", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "pretrained", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"model {} not implemented\"", ".", "format", "(", "name", ")", ")", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.digit5.CNN.__init__": [[6, 36], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.DataParallel", "torch.DataParallel", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "encoder", "=", "nn", ".", "Sequential", "(", ")", "\n", "encoder", ".", "add_module", "(", "\"conv1\"", ",", "nn", ".", "Conv2d", "(", "3", ",", "64", ",", "kernel_size", "=", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"bn1\"", ",", "nn", ".", "BatchNorm2d", "(", "64", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"relu1\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"maxpool1\"", ",", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "ceil_mode", "=", "False", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"conv2\"", ",", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"bn2\"", ",", "nn", ".", "BatchNorm2d", "(", "64", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"relu2\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"maxpool2\"", ",", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "ceil_mode", "=", "False", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"conv3\"", ",", "nn", ".", "Conv2d", "(", "64", ",", "128", ",", "kernel_size", "=", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"bn3\"", ",", "nn", ".", "BatchNorm2d", "(", "128", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"relu3\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "encoder", "=", "nn", ".", "DataParallel", "(", "encoder", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "=", "encoder", "\n", "", "linear", "=", "nn", ".", "Sequential", "(", ")", "\n", "linear", ".", "add_module", "(", "\"fc1\"", ",", "nn", ".", "Linear", "(", "8192", ",", "3072", ")", ")", "\n", "linear", ".", "add_module", "(", "\"bn4\"", ",", "nn", ".", "BatchNorm1d", "(", "3072", ")", ")", "\n", "linear", ".", "add_module", "(", "\"relu4\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "linear", ".", "add_module", "(", "\"dropout\"", ",", "nn", ".", "Dropout", "(", ")", ")", "\n", "linear", ".", "add_module", "(", "\"fc2\"", ",", "nn", ".", "Linear", "(", "3072", ",", "2048", ")", ")", "\n", "linear", ".", "add_module", "(", "\"bn5\"", ",", "nn", ".", "BatchNorm1d", "(", "2048", ")", ")", "\n", "linear", ".", "add_module", "(", "\"relu5\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "DataParallel", "(", "linear", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.digit5.CNN.forward": [[37, 43], ["x.size", "digit5.CNN.encoder", "digit5.CNN.view", "digit5.CNN.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "feature", "=", "self", ".", "encoder", "(", "x", ")", "\n", "feature", "=", "feature", ".", "view", "(", "batch_size", ",", "8192", ")", "\n", "feature", "=", "self", ".", "linear", "(", "feature", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.digit5.Classifier.__init__": [[46, 54], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Linear", "torch.Linear", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "Classifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "linear", "=", "nn", ".", "Sequential", "(", ")", "\n", "linear", ".", "add_module", "(", "\"fc\"", ",", "nn", ".", "Linear", "(", "2048", ",", "10", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "DataParallel", "(", "linear", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.digit5.Classifier.forward": [[55, 58], ["digit5.Classifier.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.amazon.AmazonMLP.__init__": [[6, 22], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Sequential.add_module", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "AmazonMLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "encoder", "=", "nn", ".", "Sequential", "(", ")", "\n", "encoder", ".", "add_module", "(", "\"fc1\"", ",", "nn", ".", "Linear", "(", "5000", ",", "1000", ")", ")", "\n", "# encoder.add_module(\"bn1\", nn.BatchNorm1d(1000))", "\n", "encoder", ".", "add_module", "(", "\"relu1\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"fc2\"", ",", "nn", ".", "Linear", "(", "1000", ",", "500", ")", ")", "\n", "# encoder.add_module(\"bn2\", nn.BatchNorm1d(500))", "\n", "encoder", ".", "add_module", "(", "\"relu2\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "encoder", ".", "add_module", "(", "\"fc3\"", ",", "nn", ".", "Linear", "(", "500", ",", "100", ")", ")", "\n", "# encoder.add_module(\"bn3\", nn.BatchNorm1d(100))", "\n", "encoder", ".", "add_module", "(", "\"relu3\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "encoder", "=", "nn", ".", "DataParallel", "(", "encoder", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "=", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.amazon.AmazonMLP.forward": [[23, 26], ["amazon.AmazonMLP.encoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "feature", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.amazon.AmazonClassifier.__init__": [[29, 37], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Linear", "torch.Linear", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "AmazonClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "linear", "=", "nn", ".", "Sequential", "(", ")", "\n", "linear", ".", "add_module", "(", "\"fc\"", ",", "nn", ".", "Linear", "(", "100", ",", "2", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "DataParallel", "(", "linear", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.amazon.AmazonClassifier.forward": [[38, 41], ["amazon.AmazonClassifier.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNet.__init__": [[10, 17], ["torch.Module.__init__", "resnet.get_resnet", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__", "home.repos.pwc.inspect_result.FengHZ_KD3A.model.resnet.get_resnet"], ["    ", "def", "__init__", "(", "self", ",", "backbone", ",", "bn_momentum", ",", "pretrained", "=", "True", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "DomainNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "encoder", "=", "get_resnet", "(", "backbone", ",", "momentumn", "=", "bn_momentum", ",", "pretrained", "=", "pretrained", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "encoder", "=", "nn", ".", "DataParallel", "(", "encoder", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "=", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNet.forward": [[18, 21], ["domainnet.DomainNet.encoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "feature", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__": [[24, 32], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential.add_module", "torch.Linear", "torch.Linear", "torch.Linear", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "backbone", ",", "classes", "=", "126", ",", "data_parallel", "=", "True", ")", ":", "\n", "        ", "super", "(", "DomainNetClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "linear", "=", "nn", ".", "Sequential", "(", ")", "\n", "linear", ".", "add_module", "(", "\"fc\"", ",", "nn", ".", "Linear", "(", "feature_dict", "[", "backbone", "]", ",", "classes", ")", ")", "\n", "if", "data_parallel", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "DataParallel", "(", "linear", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.model.domainnet.DomainNetClassifier.forward": [[33, 37], ["torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "domainnet.DomainNetClassifier.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "feature", ")", ":", "\n", "        ", "feature", "=", "torch", ".", "flatten", "(", "feature", ",", "1", ")", "\n", "feature", "=", "self", ".", "linear", "(", "feature", ")", "\n", "return", "feature", "\n", "", "", ""]], "home.repos.pwc.inspect_result.FengHZ_KD3A.train.train.train": [[8, 129], ["torch.CrossEntropyLoss().cuda", "len", "range", "range", "enumerate", "round", "range", "epoch_domain_weight.insert", "federated_average", "writer.add_scalar", "range", "print", "model.train", "classifier.train", "round", "zip", "len", "optimizer_list[].zero_grad", "classifier_optimizer_list[].zero_grad", "image_t.cuda.cuda", "knowledge_vote", "torch.sum().item", "torch.sum().item", "consensus_weight.size", "image_t.cuda.size", "torch.randperm().cuda", "torch.randperm().cuda", "torch.log_softmax", "torch.log_softmax", "torch.mean", "torch.mean", "torch.mean.backward", "optimizer_list[].step", "classifier_optimizer_list[].step", "calculate_consensus_focus", "epoch_domain_weight.append", "sum", "round", "update_domain_weight", "writer.add_scalar", "torch.CrossEntropyLoss", "enumerate", "torch.no_grad", "torch.no_grad", "torch.cat", "torch.cat", "numpy.random.beta", "numpy.random.beta", "len", "image_s.cuda.cuda", "label_s.long().cuda.long().cuda", "optimizer.zero_grad", "classifier_optimizer.zero_grad", "model", "classifier", "nn.CrossEntropyLoss().cuda.", "task_criterion.backward", "optimizer.step", "classifier_optimizer.step", "torch.softmax().unsqueeze", "torch.softmax().unsqueeze", "torch.sum", "torch.sum", "torch.randperm", "torch.randperm", "torch.sum", "torch.sum", "sum", "round", "range", "label_s.long().cuda.long", "torch.softmax", "torch.softmax", "label_s.long().cuda.size"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.federated_average", "home.repos.pwc.inspect_result.FengHZ_KD3A.train.train.train", "home.repos.pwc.inspect_result.FengHZ_KD3A.train.train.train", "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.knowledge_vote", "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.calculate_consensus_focus", "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.federated_utils.update_domain_weight"], ["def", "train", "(", "train_dloader_list", ",", "model_list", ",", "classifier_list", ",", "optimizer_list", ",", "classifier_optimizer_list", ",", "epoch", ",", "writer", ",", "\n", "num_classes", ",", "domain_weight", ",", "source_domains", ",", "batchnorm_mmd", ",", "batch_per_epoch", ",", "confidence_gate_begin", ",", "\n", "confidence_gate_end", ",", "communication_rounds", ",", "total_epochs", ",", "malicious_domain", ",", "attack_level", ",", "mix_aug", "=", "True", ")", ":", "\n", "    ", "task_criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "source_domain_num", "=", "len", "(", "train_dloader_list", "[", "1", ":", "]", ")", "\n", "for", "model", "in", "model_list", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "", "for", "classifier", "in", "classifier_list", ":", "\n", "        ", "classifier", ".", "train", "(", ")", "\n", "# If communication rounds <1,", "\n", "# then we perform parameter aggregation after (1/communication_rounds) epochs", "\n", "# If communication rounds >=1:", "\n", "# then we extend the training epochs and use fewer samples in each epoch.", "\n", "", "if", "communication_rounds", "in", "[", "0.2", ",", "0.5", "]", ":", "\n", "        ", "model_aggregation_frequency", "=", "round", "(", "1", "/", "communication_rounds", ")", "\n", "", "else", ":", "\n", "        ", "model_aggregation_frequency", "=", "1", "\n", "", "for", "f", "in", "range", "(", "model_aggregation_frequency", ")", ":", "\n", "        ", "current_domain_index", "=", "0", "\n", "# Train model locally on source domains", "\n", "for", "train_dloader", ",", "model", ",", "classifier", ",", "optimizer", ",", "classifier_optimizer", "in", "zip", "(", "train_dloader_list", "[", "1", ":", "]", ",", "\n", "model_list", "[", "1", ":", "]", ",", "\n", "classifier_list", "[", "1", ":", "]", ",", "\n", "optimizer_list", "[", "1", ":", "]", ",", "\n", "classifier_optimizer_list", "[", "1", ":", "]", ")", ":", "\n", "\n", "# check if the source domain is the malicious domain with poisoning attack", "\n", "            ", "source_domain", "=", "source_domains", "[", "current_domain_index", "]", "\n", "current_domain_index", "+=", "1", "\n", "if", "source_domain", "==", "malicious_domain", "and", "attack_level", ">", "0", ":", "\n", "                ", "poisoning_attack", "=", "True", "\n", "", "else", ":", "\n", "                ", "poisoning_attack", "=", "False", "\n", "", "for", "i", ",", "(", "image_s", ",", "label_s", ")", "in", "enumerate", "(", "train_dloader", ")", ":", "\n", "                ", "if", "i", ">=", "batch_per_epoch", ":", "\n", "                    ", "break", "\n", "", "image_s", "=", "image_s", ".", "cuda", "(", ")", "\n", "label_s", "=", "label_s", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "if", "poisoning_attack", ":", "\n", "# perform poison attack on source domain", "\n", "                    ", "corrupted_num", "=", "round", "(", "label_s", ".", "size", "(", "0", ")", "*", "attack_level", ")", "\n", "# provide fake labels for those corrupted data", "\n", "label_s", "[", ":", "corrupted_num", ",", "...", "]", "=", "(", "label_s", "[", ":", "corrupted_num", ",", "...", "]", "+", "1", ")", "%", "num_classes", "\n", "# reset grad", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "classifier_optimizer", ".", "zero_grad", "(", ")", "\n", "# each source domain do optimize", "\n", "feature_s", "=", "model", "(", "image_s", ")", "\n", "output_s", "=", "classifier", "(", "feature_s", ")", "\n", "task_loss_s", "=", "task_criterion", "(", "output_s", ",", "label_s", ")", "\n", "task_loss_s", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "classifier_optimizer", ".", "step", "(", ")", "\n", "# Domain adaptation on target domain", "\n", "", "", "", "confidence_gate", "=", "(", "confidence_gate_end", "-", "confidence_gate_begin", ")", "*", "(", "epoch", "/", "total_epochs", ")", "+", "confidence_gate_begin", "\n", "# We use I(n_i>=1)/(N_T) to adjust the weight for knowledge distillation domain", "\n", "target_weight", "=", "[", "0", ",", "0", "]", "\n", "consensus_focus_dict", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "train_dloader_list", ")", ")", ":", "\n", "        ", "consensus_focus_dict", "[", "i", "]", "=", "0", "\n", "", "for", "i", ",", "(", "image_t", ",", "label_t", ")", "in", "enumerate", "(", "train_dloader_list", "[", "0", "]", ")", ":", "\n", "        ", "if", "i", ">=", "batch_per_epoch", ":", "\n", "            ", "break", "\n", "", "optimizer_list", "[", "0", "]", ".", "zero_grad", "(", ")", "\n", "classifier_optimizer_list", "[", "0", "]", ".", "zero_grad", "(", ")", "\n", "image_t", "=", "image_t", ".", "cuda", "(", ")", "\n", "# Knowledge Vote", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "knowledge_list", "=", "[", "torch", ".", "softmax", "(", "classifier_list", "[", "i", "]", "(", "model_list", "[", "i", "]", "(", "image_t", ")", ")", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "for", "\n", "i", "in", "range", "(", "1", ",", "source_domain_num", "+", "1", ")", "]", "\n", "knowledge_list", "=", "torch", ".", "cat", "(", "knowledge_list", ",", "1", ")", "\n", "", "_", ",", "consensus_knowledge", ",", "consensus_weight", "=", "knowledge_vote", "(", "knowledge_list", ",", "confidence_gate", ",", "\n", "num_classes", "=", "num_classes", ")", "\n", "target_weight", "[", "0", "]", "+=", "torch", ".", "sum", "(", "consensus_weight", ")", ".", "item", "(", ")", "\n", "target_weight", "[", "1", "]", "+=", "consensus_weight", ".", "size", "(", "0", ")", "\n", "# Perform data augmentation with mixup", "\n", "if", "mix_aug", ":", "\n", "            ", "lam", "=", "np", ".", "random", ".", "beta", "(", "2", ",", "2", ")", "\n", "", "else", ":", "\n", "# Do not perform mixup", "\n", "            ", "lam", "=", "np", ".", "random", ".", "beta", "(", "2", ",", "2", ")", "\n", "", "batch_size", "=", "image_t", ".", "size", "(", "0", ")", "\n", "index", "=", "torch", ".", "randperm", "(", "batch_size", ")", ".", "cuda", "(", ")", "\n", "mixed_image", "=", "lam", "*", "image_t", "+", "(", "1", "-", "lam", ")", "*", "image_t", "[", "index", ",", ":", "]", "\n", "mixed_consensus", "=", "lam", "*", "consensus_knowledge", "+", "(", "1", "-", "lam", ")", "*", "consensus_knowledge", "[", "index", ",", ":", "]", "\n", "feature_t", "=", "model_list", "[", "0", "]", "(", "mixed_image", ")", "\n", "output_t", "=", "classifier_list", "[", "0", "]", "(", "feature_t", ")", "\n", "output_t", "=", "torch", ".", "log_softmax", "(", "output_t", ",", "dim", "=", "1", ")", "\n", "task_loss_t", "=", "torch", ".", "mean", "(", "consensus_weight", "*", "torch", ".", "sum", "(", "-", "1", "*", "mixed_consensus", "*", "output_t", ",", "dim", "=", "1", ")", ")", "\n", "task_loss_t", ".", "backward", "(", ")", "\n", "optimizer_list", "[", "0", "]", ".", "step", "(", ")", "\n", "classifier_optimizer_list", "[", "0", "]", ".", "step", "(", ")", "\n", "# Calculate consensus focus", "\n", "consensus_focus_dict", "=", "calculate_consensus_focus", "(", "consensus_focus_dict", ",", "knowledge_list", ",", "confidence_gate", ",", "\n", "source_domain_num", ",", "num_classes", ")", "\n", "# Consensus Focus Re-weighting", "\n", "", "target_parameter_alpha", "=", "target_weight", "[", "0", "]", "/", "target_weight", "[", "1", "]", "\n", "target_weight", "=", "round", "(", "target_parameter_alpha", "/", "(", "source_domain_num", "+", "1", ")", ",", "4", ")", "\n", "epoch_domain_weight", "=", "[", "]", "\n", "source_total_weight", "=", "1", "-", "target_weight", "\n", "for", "i", "in", "range", "(", "1", ",", "source_domain_num", "+", "1", ")", ":", "\n", "        ", "epoch_domain_weight", ".", "append", "(", "consensus_focus_dict", "[", "i", "]", ")", "\n", "", "if", "sum", "(", "epoch_domain_weight", ")", "==", "0", ":", "\n", "        ", "epoch_domain_weight", "=", "[", "v", "+", "1e-3", "for", "v", "in", "epoch_domain_weight", "]", "\n", "", "epoch_domain_weight", "=", "[", "round", "(", "source_total_weight", "*", "v", "/", "sum", "(", "epoch_domain_weight", ")", ",", "4", ")", "for", "v", "in", "\n", "epoch_domain_weight", "]", "\n", "epoch_domain_weight", ".", "insert", "(", "0", ",", "target_weight", ")", "\n", "# Update domain weight with moving average", "\n", "if", "epoch", "==", "0", ":", "\n", "        ", "domain_weight", "=", "epoch_domain_weight", "\n", "", "else", ":", "\n", "        ", "domain_weight", "=", "update_domain_weight", "(", "domain_weight", ",", "epoch_domain_weight", ")", "\n", "# Model aggregation and Batchnorm MMD", "\n", "", "federated_average", "(", "model_list", ",", "domain_weight", ",", "batchnorm_mmd", "=", "batchnorm_mmd", ")", "\n", "# Recording domain weight in logs", "\n", "writer", ".", "add_scalar", "(", "tag", "=", "\"Train/target_domain_weight\"", ",", "scalar_value", "=", "target_weight", ",", "global_step", "=", "epoch", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "train_dloader_list", ")", "-", "1", ")", ":", "\n", "        ", "writer", ".", "add_scalar", "(", "tag", "=", "\"Train/source_domain_{}_weight\"", ".", "format", "(", "source_domains", "[", "i", "]", ")", ",", "\n", "scalar_value", "=", "domain_weight", "[", "i", "+", "1", "]", ",", "global_step", "=", "epoch", "+", "1", ")", "\n", "", "print", "(", "\"Source Domains:{}, Domain Weight :{}\"", ".", "format", "(", "source_domains", ",", "domain_weight", "[", "1", ":", "]", ")", ")", "\n", "return", "domain_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.FengHZ_KD3A.train.train.test": [[131, 209], ["lib.utils.avgmeter.AverageMeter", "torch.CrossEntropyLoss().cuda", "enumerate", "writer.add_scalar", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.topk", "torch.topk", "writer.add_scalar", "enumerate", "lib.utils.avgmeter.AverageMeter", "model.eval", "classifier.eval", "image_t.cuda.cuda", "label_t.long().cuda.long().cuda", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "nn.CrossEntropyLoss().cuda.", "lib.utils.avgmeter.AverageMeter.update", "torch.cat().detach.append", "torch.cat().detach.append", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "float", "y_true.size", "writer.add_scalar", "print", "print", "enumerate", "writer.add_scalar", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.topk", "torch.topk", "writer.add_scalar", "torch.CrossEntropyLoss", "torch.no_grad", "torch.no_grad", "label_t.long().cuda.view", "float", "image_t.cuda.size", "torch.softmax", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sum().item", "torch.sum().item", "float", "y_true.size", "image_s.cuda.cuda", "label_s.long().cuda.long().cuda", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "nn.CrossEntropyLoss().cuda.", "source_domain_losses[].update", "torch.cat().detach.append", "torch.cat().detach.append", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "float", "y_true.size", "writer.add_scalar", "label_t.long().cuda.long", "torch.zeros().cuda", "torch.zeros().cuda", "task_criterion.item", "torch.sum().item", "torch.sum().item", "torch.no_grad", "torch.no_grad", "label_s.long().cuda.view", "float", "image_s.cuda.size", "torch.softmax", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sum().item", "torch.sum().item", "float", "y_true.size", "torch.sum", "torch.sum", "label_s.long().cuda.long", "torch.zeros().cuda", "torch.zeros().cuda", "task_criterion.item", "torch.sum().item", "torch.sum().item", "torch.zeros", "torch.zeros", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "label_t.long().cuda.size", "torch.zeros", "torch.zeros", "torch.sum", "torch.sum", "label_s.long().cuda.size"], "function", ["home.repos.pwc.inspect_result.FengHZ_KD3A.utils.avgmeter.AverageMeter.update", "home.repos.pwc.inspect_result.FengHZ_KD3A.utils.avgmeter.AverageMeter.update"], ["", "def", "test", "(", "target_domain", ",", "source_domains", ",", "test_dloader_list", ",", "model_list", ",", "classifier_list", ",", "epoch", ",", "writer", ",", "num_classes", "=", "126", ",", "\n", "top_5_accuracy", "=", "True", ")", ":", "\n", "    ", "source_domain_losses", "=", "[", "AverageMeter", "(", ")", "for", "i", "in", "source_domains", "]", "\n", "target_domain_losses", "=", "AverageMeter", "(", ")", "\n", "task_criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "for", "model", "in", "model_list", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "", "for", "classifier", "in", "classifier_list", ":", "\n", "        ", "classifier", ".", "eval", "(", ")", "\n", "# calculate loss, accuracy for target domain", "\n", "", "tmp_score", "=", "[", "]", "\n", "tmp_label", "=", "[", "]", "\n", "test_dloader_t", "=", "test_dloader_list", "[", "0", "]", "\n", "for", "_", ",", "(", "image_t", ",", "label_t", ")", "in", "enumerate", "(", "test_dloader_t", ")", ":", "\n", "        ", "image_t", "=", "image_t", ".", "cuda", "(", ")", "\n", "label_t", "=", "label_t", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "output_t", "=", "classifier_list", "[", "0", "]", "(", "model_list", "[", "0", "]", "(", "image_t", ")", ")", "\n", "", "label_onehot_t", "=", "torch", ".", "zeros", "(", "label_t", ".", "size", "(", "0", ")", ",", "num_classes", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "1", ",", "label_t", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "task_loss_t", "=", "task_criterion", "(", "output_t", ",", "label_t", ")", "\n", "target_domain_losses", ".", "update", "(", "float", "(", "task_loss_t", ".", "item", "(", ")", ")", ",", "image_t", ".", "size", "(", "0", ")", ")", "\n", "tmp_score", ".", "append", "(", "torch", ".", "softmax", "(", "output_t", ",", "dim", "=", "1", ")", ")", "\n", "# turn label into one-hot code", "\n", "tmp_label", ".", "append", "(", "label_onehot_t", ")", "\n", "", "writer", ".", "add_scalar", "(", "tag", "=", "\"Test/target_domain_{}_loss\"", ".", "format", "(", "target_domain", ")", ",", "scalar_value", "=", "target_domain_losses", ".", "avg", ",", "\n", "global_step", "=", "epoch", "+", "1", ")", "\n", "tmp_score", "=", "torch", ".", "cat", "(", "tmp_score", ",", "dim", "=", "0", ")", ".", "detach", "(", ")", "\n", "tmp_label", "=", "torch", ".", "cat", "(", "tmp_label", ",", "dim", "=", "0", ")", ".", "detach", "(", ")", "\n", "_", ",", "y_true", "=", "torch", ".", "topk", "(", "tmp_label", ",", "k", "=", "1", ",", "dim", "=", "1", ")", "\n", "if", "top_5_accuracy", ":", "\n", "        ", "_", ",", "y_pred", "=", "torch", ".", "topk", "(", "tmp_score", ",", "k", "=", "5", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "_", ",", "y_pred", "=", "torch", ".", "topk", "(", "tmp_score", ",", "k", "=", "1", ",", "dim", "=", "1", ")", "\n", "", "top_1_accuracy_t", "=", "float", "(", "torch", ".", "sum", "(", "y_true", "==", "y_pred", "[", ":", ",", ":", "1", "]", ")", ".", "item", "(", ")", ")", "/", "y_true", ".", "size", "(", "0", ")", "\n", "writer", ".", "add_scalar", "(", "tag", "=", "\"Test/target_domain_{}_accuracy_top1\"", ".", "format", "(", "target_domain", ")", ".", "format", "(", "target_domain", ")", ",", "\n", "scalar_value", "=", "top_1_accuracy_t", ",", "\n", "global_step", "=", "epoch", "+", "1", ")", "\n", "if", "top_5_accuracy", ":", "\n", "        ", "top_5_accuracy_t", "=", "float", "(", "torch", ".", "sum", "(", "y_true", "==", "y_pred", ")", ".", "item", "(", ")", ")", "/", "y_true", ".", "size", "(", "0", ")", "\n", "writer", ".", "add_scalar", "(", "tag", "=", "\"Test/target_domain_{}_accuracy_top5\"", ".", "format", "(", "target_domain", ")", ".", "format", "(", "target_domain", ")", ",", "\n", "scalar_value", "=", "top_5_accuracy_t", ",", "\n", "global_step", "=", "epoch", "+", "1", ")", "\n", "print", "(", "\"Target Domain {} Accuracy Top1 :{:.3f} Top5:{:.3f}\"", ".", "format", "(", "target_domain", ",", "top_1_accuracy_t", ",", "\n", "top_5_accuracy_t", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Target Domain {} Accuracy {:.3f}\"", ".", "format", "(", "target_domain", ",", "top_1_accuracy_t", ")", ")", "\n", "# calculate loss, accuracy for source domains", "\n", "", "for", "s_i", ",", "domain_s", "in", "enumerate", "(", "source_domains", ")", ":", "\n", "        ", "tmp_score", "=", "[", "]", "\n", "tmp_label", "=", "[", "]", "\n", "test_dloader_s", "=", "test_dloader_list", "[", "s_i", "+", "1", "]", "\n", "for", "_", ",", "(", "image_s", ",", "label_s", ")", "in", "enumerate", "(", "test_dloader_s", ")", ":", "\n", "            ", "image_s", "=", "image_s", ".", "cuda", "(", ")", "\n", "label_s", "=", "label_s", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "output_s", "=", "classifier_list", "[", "s_i", "+", "1", "]", "(", "model_list", "[", "s_i", "+", "1", "]", "(", "image_s", ")", ")", "\n", "", "label_onehot_s", "=", "torch", ".", "zeros", "(", "label_s", ".", "size", "(", "0", ")", ",", "num_classes", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "1", ",", "label_s", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "task_loss_s", "=", "task_criterion", "(", "output_s", ",", "label_s", ")", "\n", "source_domain_losses", "[", "s_i", "]", ".", "update", "(", "float", "(", "task_loss_s", ".", "item", "(", ")", ")", ",", "image_s", ".", "size", "(", "0", ")", ")", "\n", "tmp_score", ".", "append", "(", "torch", ".", "softmax", "(", "output_s", ",", "dim", "=", "1", ")", ")", "\n", "# turn label into one-hot code", "\n", "tmp_label", ".", "append", "(", "label_onehot_s", ")", "\n", "", "writer", ".", "add_scalar", "(", "tag", "=", "\"Test/source_domain_{}_loss\"", ".", "format", "(", "domain_s", ")", ",", "scalar_value", "=", "source_domain_losses", "[", "s_i", "]", ".", "avg", ",", "\n", "global_step", "=", "epoch", "+", "1", ")", "\n", "tmp_score", "=", "torch", ".", "cat", "(", "tmp_score", ",", "dim", "=", "0", ")", ".", "detach", "(", ")", "\n", "tmp_label", "=", "torch", ".", "cat", "(", "tmp_label", ",", "dim", "=", "0", ")", ".", "detach", "(", ")", "\n", "_", ",", "y_true", "=", "torch", ".", "topk", "(", "tmp_label", ",", "k", "=", "1", ",", "dim", "=", "1", ")", "\n", "if", "top_5_accuracy", ":", "\n", "            ", "_", ",", "y_pred", "=", "torch", ".", "topk", "(", "tmp_score", ",", "k", "=", "5", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "_", ",", "y_pred", "=", "torch", ".", "topk", "(", "tmp_score", ",", "k", "=", "1", ",", "dim", "=", "1", ")", "\n", "", "top_1_accuracy_s", "=", "float", "(", "torch", ".", "sum", "(", "y_true", "==", "y_pred", "[", ":", ",", ":", "1", "]", ")", ".", "item", "(", ")", ")", "/", "y_true", ".", "size", "(", "0", ")", "\n", "writer", ".", "add_scalar", "(", "tag", "=", "\"Test/source_domain_{}_accuracy_top1\"", ".", "format", "(", "domain_s", ")", ",", "scalar_value", "=", "top_1_accuracy_s", ",", "\n", "global_step", "=", "epoch", "+", "1", ")", "\n", "if", "top_5_accuracy", ":", "\n", "            ", "top_5_accuracy_s", "=", "float", "(", "torch", ".", "sum", "(", "y_true", "==", "y_pred", ")", ".", "item", "(", ")", ")", "/", "y_true", ".", "size", "(", "0", ")", "\n", "writer", ".", "add_scalar", "(", "tag", "=", "\"Test/source_domain_{}_accuracy_top5\"", ".", "format", "(", "domain_s", ")", ",", "scalar_value", "=", "top_5_accuracy_s", ",", "\n", "global_step", "=", "epoch", "+", "1", ")", "\n", "", "", "", ""]]}