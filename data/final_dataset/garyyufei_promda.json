{"home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_label_dict": [[17, 24], ["open", "out.readlines", "l.strip.strip", "len"], "function", ["None"], ["def", "get_label_dict", "(", "label_path", ")", ":", "\n", "    ", "label_dict", "=", "{", "}", "\n", "with", "open", "(", "label_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ".", "readlines", "(", ")", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "label_dict", "[", "l", "]", "=", "len", "(", "label_dict", ")", "\n", "", "", "return", "label_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_bio_seq": [[25, 70], ["gen.split", "enumerate", "w.startswith", "len", "word.append", "len", "len", "len", "label.append", "label.append", "current_label.startswith", "label_seq.append"], "function", ["None"], ["", "def", "get_bio_seq", "(", "config", ",", "index2label", ",", "gen", ",", "input_seq", ")", ":", "\n", "\t", "gen_words", "=", "gen", ".", "split", "(", ")", "\n", "label", "=", "[", "]", "\n", "word", "=", "[", "]", "\n", "label_seq", "=", "[", "]", "\n", "current_label", "=", "None", "\n", "has_error", "=", "False", "\n", "for", "index", ",", "w", "in", "enumerate", "(", "gen_words", ")", ":", "\n", "\t\t", "if", "w", ".", "startswith", "(", "\"B-\"", ")", ":", "\n", "\t\t\t", "entity_label", "=", "w", "[", "2", ":", "]", "\n", "if", "current_label", "is", "not", "None", ":", "\n", "\t\t\t\t", "has_error", "=", "True", "\n", "break", "\n", "", "if", "len", "(", "w", ")", "==", "2", ":", "\n", "\t\t\t\t", "has_error", "=", "True", "\n", "break", "\n", "", "if", "entity_label", "not", "in", "index2label", ":", "\n", "\t\t\t\t", "has_error", "=", "True", "\n", "break", "\n", "", "entity_label", "=", "index2label", "[", "entity_label", "]", "[", "2", ":", "]", "\n", "current_label", "=", "\"B-\"", "+", "entity_label", "\n", "", "elif", "w", "==", "\"&&\"", ":", "\n", "\t\t\t", "if", "current_label", "is", "None", ":", "\n", "\t\t\t\t", "has_error", "=", "True", "\n", "break", "\n", "", "current_label", "=", "None", "\n", "", "else", ":", "\n", "\t\t\t", "word", ".", "append", "(", "w", ")", "\n", "if", "current_label", "is", "None", ":", "\n", "\t\t\t\t", "label", ".", "append", "(", "'O'", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "label", ".", "append", "(", "current_label", ")", "\n", "if", "current_label", ".", "startswith", "(", "'B-'", ")", ":", "\n", "\t\t\t\t\t", "label_seq", ".", "append", "(", "current_label", ")", "\n", "current_label", "=", "'I-'", "+", "current_label", "[", "2", ":", "]", "\n", "\n", "", "", "", "", "if", "current_label", "is", "not", "None", ":", "\n", "\t\t", "has_error", "=", "True", "\n", "\n", "", "if", "not", "(", "len", "(", "word", ")", "==", "len", "(", "label", ")", "and", "len", "(", "word", ")", ">", "0", ")", ":", "\n", "\t\t", "has_error", "=", "True", "\n", "# input_seq = ' '.join(input_seq.split()[1:])", "\n", "# if not ' && '.join(label_seq) == input_seq:", "\n", "# \thas_error = True", "\n", "", "return", "word", ",", "label", ",", "has_error", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_sentence_classification_output": [[71, 79], ["gen.split", "len"], "function", ["None"], ["", "def", "get_sentence_classification_output", "(", "label_list", ",", "gen", ")", ":", "\n", "\t", "words", "=", "gen", ".", "split", "(", ")", "\n", "if", "len", "(", "words", ")", ">", "2", ":", "\n", "\t\t", "word", "=", "' '", ".", "join", "(", "words", "[", "1", ":", "]", ")", "\n", "label", "=", "words", "[", "0", "]", "\n", "return", "word", ",", "label", ",", "label", "not", "in", "label_list", "\n", "", "else", ":", "\n", "\t\t", "return", "None", ",", "None", ",", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_pair_sentence_classification_output": [[80, 105], ["gen.split", "gen.startswith", "len", "word.split"], "function", ["None"], ["", "", "def", "get_pair_sentence_classification_output", "(", "label_list", ",", "gen", ")", ":", "\n", "\t", "has_error", "=", "True", "\n", "for", "l", "in", "label_list", ":", "\n", "\t\t", "if", "gen", ".", "startswith", "(", "l", ")", ":", "\n", "\t\t\t", "has_error", "=", "False", "\n", "break", "\n", "\n", "", "", "words", "=", "gen", ".", "split", "(", ")", "\n", "if", "len", "(", "words", ")", ">", "3", ":", "\n", "\t\t", "if", "words", "[", "0", "]", "==", "'not'", ":", "\n", "\t\t\t", "sentence_pair", "=", "words", "[", "2", ":", "]", "\n", "label", "=", "' '", ".", "join", "(", "words", "[", ":", "2", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t", "sentence_pair", "=", "words", "[", "1", ":", "]", "\n", "label", "=", "words", "[", "0", "]", "\n", "\n", "", "has_error", "=", "'[SEP]'", "not", "in", "sentence_pair", "or", "label", "not", "in", "label_list", "\n", "\n", "word", "=", "' '", ".", "join", "(", "sentence_pair", ")", "\n", "if", "'[SEP]'", "in", "sentence_pair", ":", "\n", "\t\t\t", "word", "=", "'\\t'", ".", "join", "(", "word", ".", "split", "(", "'[SEP]'", ")", ")", "\n", "\n", "", "return", "word", ",", "label", ",", "has_error", "\n", "", "else", ":", "\n", "\t\t", "return", "None", ",", "None", ",", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.evaluation": [[107, 234], ["model.eval", "zip", "zip", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "iSen2oSen[].append", "list", "iSen2gt[].append", "list", "print", "zip", "print", "print", "zip", "print", "label_index.items", "set", "set", "sum", "len", "sacrebleu.corpus_bleu", "open", "zip", "model", "loss_list.append", "model.generate", "outputs.view.view", "range", "train_data_agumentation.get_sentence_classification_output", "bio_labels.append", "bio_words.append", "words_to_tags[].append", "len", "len", "batch[].to", "loss.item", "len", "len", "range", "train_data_agumentation.get_pair_sentence_classification_output", "train_data_agumentation.get_bio_seq", "len", "new_bio_words.append", "new_bio_labels.append", "new_bio_words.append", "new_bio_labels.append", "out.write", "zip", "out.write", "tokenizer.decode", "gen_ner.append", "input_sentences.append", "gt_tags.append", "key.split", "[].split", "out.write"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_sentence_classification_output", "home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_pair_sentence_classification_output", "home.repos.pwc.inspect_result.garyyufei_promda.None.train_data_agumentation.get_bio_seq"], ["", "", "def", "evaluation", "(", "config", ",", "eval_data", ",", "model", ",", "label_list", ",", "label_index", ",", "device", ",", "show_detail", "=", "False", ",", "output_path", "=", "None", ")", ":", "\n", "\t", "model", ".", "eval", "(", ")", "\n", "preds", "=", "None", "\n", "input_sentences", "=", "[", "]", "\n", "gen_ner", "=", "[", "]", "\n", "gt_tags", "=", "[", "]", "\n", "loss_list", "=", "[", "]", "\n", "index2label", "=", "{", "v", ":", "k", "for", "(", "k", ",", "v", ")", "in", "label_index", ".", "items", "(", ")", "}", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t", "for", "batch", "in", "tqdm", "(", "eval_data", ")", ":", "\n", "\n", "\t\t\t", "for", "n", "in", "batch", ":", "\n", "\t\t\t\t", "if", "n", "not", "in", "eval_data", ".", "dataset", ".", "SKIP_ATTRIBUTES", "and", "batch", "[", "n", "]", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "batch", "[", "n", "]", "=", "batch", "[", "n", "]", ".", "to", "(", "device", ")", "\n", "\n", "", "", "if", "config", ".", "select_model_by_ppl", ":", "\n", "\t\t\t\t", "outputs", "=", "model", "(", "\n", "input_ids", "=", "batch", "[", "'encoder_input_ids'", "]", ",", "\n", "task_ids", "=", "batch", "[", "'task_index'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'encoder_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'decoder_input_ids'", "]", ",", "\n", ")", "\n", "loss", "=", "outputs", ".", "loss", "\n", "loss_list", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "outputs", "=", "model", ".", "generate", "(", "\n", "input_ids", "=", "batch", "[", "'encoder_input_ids'", "]", ",", "\n", "task_ids", "=", "batch", "[", "'task_index'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'encoder_mask'", "]", ",", "\n", "max_length", "=", "config", ".", "max_length", ",", "\n", "min_length", "=", "config", ".", "min_length", ",", "\n", "eos_token_id", "=", "tokenizer", ".", "eos_token_id", ",", "\n", "num_return_sequences", "=", "config", ".", "sample_num", ",", "\n", "do_sample", "=", "True", ",", "\n", "top_p", "=", "0.9", ",", "\n", "early_stopping", "=", "True", "\n", ")", "\n", "\n", "outputs", "=", "outputs", ".", "view", "(", "len", "(", "batch", "[", "'gt_x'", "]", ")", ",", "config", ".", "sample_num", ",", "-", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", "[", "'gt_x'", "]", ")", ")", ":", "\n", "\t\t\t\t\t", "for", "j", "in", "range", "(", "config", ".", "sample_num", ")", ":", "\n", "\t\t\t\t\t\t", "sen", "=", "tokenizer", ".", "decode", "(", "outputs", "[", "i", "]", "[", "j", "]", ",", "skip_special_tokens", "=", "True", ")", "\n", "gen_ner", ".", "append", "(", "sen", ")", "\n", "input_sentences", ".", "append", "(", "batch", "[", "'gt_x'", "]", "[", "i", "]", ")", "\n", "gt_tags", ".", "append", "(", "batch", "[", "'gt_y'", "]", "[", "i", "]", ")", "\n", "\n", "", "", "", "", "", "iSen2oSen", "=", "{", "}", "\n", "for", "i_sen", ",", "o_sen", "in", "zip", "(", "input_sentences", ",", "gen_ner", ")", ":", "\n", "\t\t", "if", "i_sen", "not", "in", "iSen2oSen", ":", "\n", "\t\t\t", "iSen2oSen", "[", "i_sen", "]", "=", "[", "]", "\n", "", "iSen2oSen", "[", "i_sen", "]", ".", "append", "(", "o_sen", ")", "\n", "\n", "", "for", "key", "in", "iSen2oSen", ":", "\n", "\t\t", "iSen2oSen", "[", "key", "]", "=", "list", "(", "set", "(", "iSen2oSen", "[", "key", "]", ")", ")", "\n", "\n", "", "iSen2gt", "=", "{", "}", "\n", "for", "i_sen", ",", "gt", "in", "zip", "(", "input_sentences", ",", "gt_tags", ")", ":", "\n", "\t\t", "if", "i_sen", "not", "in", "iSen2gt", ":", "\n", "\t\t\t", "iSen2gt", "[", "i_sen", "]", "=", "[", "]", "\n", "", "iSen2gt", "[", "i_sen", "]", ".", "append", "(", "gt", ")", "\n", "\n", "", "for", "key", "in", "iSen2gt", ":", "\n", "\t\t", "iSen2gt", "[", "key", "]", "=", "list", "(", "set", "(", "iSen2gt", "[", "key", "]", ")", ")", "\n", "\n", "", "bio_labels", "=", "[", "]", "\n", "bio_words", "=", "[", "]", "\n", "new_F", "=", "0", "\n", "if", "config", ".", "select_model_by_ppl", ":", "\n", "\t\t", "new_F", "=", "sum", "(", "loss_list", ")", "/", "len", "(", "loss_list", ")", "\n", "print", "(", "\"PPL %.2f\"", "%", "new_F", ")", "\n", "new_F", "=", "-", "1", "*", "new_F", "\n", "", "else", ":", "\n", "\t\t", "total_count", "=", "0", "\n", "correct_count", "=", "0", "\n", "for", "gen", ",", "input_sen", "in", "zip", "(", "gen_ner", ",", "input_sentences", ")", ":", "\n", "\t\t\t", "total_count", "+=", "1", "\n", "if", "config", ".", "enable_sentence_classification", ":", "\n", "\t\t\t\t", "word", ",", "label", ",", "has_error", "=", "get_sentence_classification_output", "(", "label_list", ",", "gen", ")", "\n", "", "elif", "config", ".", "enable_pair_sentence_classification", ":", "\n", "\t\t\t\t", "word", ",", "label", ",", "has_error", "=", "get_pair_sentence_classification_output", "(", "label_list", ",", "gen", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "word", ",", "label", ",", "has_error", "=", "get_bio_seq", "(", "config", ",", "index2label", ",", "gen", ",", "input_sen", ")", "\n", "", "if", "(", "not", "has_error", ")", "or", "(", "not", "config", ".", "enable_filtering_error", ")", ":", "\n", "\t\t\t\t", "bio_labels", ".", "append", "(", "label", ")", "\n", "bio_words", ".", "append", "(", "word", ")", "\n", "", "correct_count", "+=", "1", "if", "not", "has_error", "else", "0", "\n", "", "print", "(", "\"Sentence Sample Successful Ratio %.2f, remaining %d instances\"", "%", "(", "100", "*", "correct_count", "/", "total_count", ",", "len", "(", "bio_words", ")", ")", ")", "\n", "\n", "sen_score", "=", "sacrebleu", ".", "corpus_bleu", "(", "gen_ner", ",", "[", "gt_tags", "]", ")", ".", "score", "\n", "print", "(", "\"BLEU %.2f\"", "%", "sen_score", ")", "\n", "new_F", "=", "sen_score", "\n", "\n", "words_to_tags", "=", "{", "}", "\n", "for", "word", ",", "tag", "in", "zip", "(", "bio_words", ",", "bio_labels", ")", ":", "\n", "\t\t\t", "if", "config", ".", "enable_sentence_classification", ":", "\n", "\t\t\t\t", "word_key", "=", "word", "\n", "tag_value", "=", "tag", "\n", "", "else", ":", "\n", "\t\t\t\t", "word_key", "=", "' '", ".", "join", "(", "word", ")", "\n", "tag_value", "=", "' '", ".", "join", "(", "tag", ")", "\n", "", "if", "word_key", "not", "in", "words_to_tags", ":", "\n", "\t\t\t\t", "words_to_tags", "[", "word_key", "]", "=", "[", "]", "\n", "", "if", "tag_value", "not", "in", "words_to_tags", "[", "word_key", "]", ":", "\n", "\t\t\t\t", "words_to_tags", "[", "word_key", "]", ".", "append", "(", "tag_value", ")", "\n", "\n", "", "", "new_bio_labels", ",", "new_bio_words", "=", "[", "]", ",", "[", "]", "\n", "for", "key", "in", "words_to_tags", ":", "\n", "\t\t\t", "if", "len", "(", "words_to_tags", "[", "key", "]", ")", "==", "1", ":", "\n", "\t\t\t\t", "if", "config", ".", "enable_sentence_classification", ":", "\n", "\t\t\t\t\t", "new_bio_words", ".", "append", "(", "key", ")", "\n", "new_bio_labels", ".", "append", "(", "words_to_tags", "[", "key", "]", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "new_bio_words", ".", "append", "(", "key", ".", "split", "(", ")", ")", "\n", "new_bio_labels", ".", "append", "(", "words_to_tags", "[", "key", "]", "[", "0", "]", ".", "split", "(", ")", ")", "\n", "", "", "", "print", "(", "\"unique count %d\"", "%", "len", "(", "new_bio_words", ")", ")", "\n", "\n", "", "if", "output_path", "is", "not", "None", ":", "\n", "\t\t", "with", "open", "(", "output_path", ",", "'w'", ")", "as", "out", ":", "\n", "\t\t\t", "for", "gen", ",", "labels", "in", "zip", "(", "bio_words", ",", "bio_labels", ")", ":", "\n", "\t\t\t\t", "if", "config", ".", "enable_sentence_classification", "or", "config", ".", "enable_pair_sentence_classification", ":", "\n", "\t\t\t\t\t", "out", ".", "write", "(", "\"%s\\t%s\\n\"", "%", "(", "gen", ",", "labels", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "for", "g", ",", "l", "in", "zip", "(", "gen", ",", "labels", ")", ":", "\n", "\t\t\t\t\t\t", "out", ".", "write", "(", "\"%s %s\\n\"", "%", "(", "g", ",", "l", ")", ")", "\n", "", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "", "", "return", "new_F", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.__init__": [[6, 61], ["yacs.config.CfgNode", "config.Config._C.merge_from_file", "config.Config._C.merge_from_list", "config.Config._C.freeze"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "config_yaml", ":", "str", ",", "config_override", ":", "List", "[", "Any", "]", "=", "[", "]", ")", ":", "\n", "\t\t", "self", ".", "_C", "=", "CN", "(", ")", "\n", "self", ".", "_C", ".", "random_seed", "=", "0", "\n", "self", ".", "_C", ".", "train_path", "=", "\"\"", "\n", "self", ".", "_C", ".", "dev_path", "=", "\"\"", "\n", "self", ".", "_C", ".", "test_path", "=", "\"\"", "\n", "self", ".", "_C", ".", "lm_type", "=", "\"\"", "\n", "self", ".", "_C", ".", "tokenizer_type", "=", "\"\"", "\n", "self", ".", "_C", ".", "label_path", "=", "\"\"", "\n", "self", ".", "_C", ".", "nlu_model_path", "=", "\"\"", "\n", "self", ".", "_C", ".", "batch_size", "=", "10", "\n", "self", ".", "_C", ".", "val_batch_size", "=", "-", "1", "\n", "self", ".", "_C", ".", "beam_size", "=", "5", "\n", "self", ".", "_C", ".", "sample_num", "=", "5", "\n", "self", ".", "_C", ".", "eval_data_replication", "=", "1", "\n", "self", ".", "_C", ".", "max_length", "=", "50", "\n", "self", ".", "_C", ".", "min_length", "=", "5", "\n", "self", ".", "_C", ".", "learning_rate", "=", "1e-3", "\n", "self", ".", "_C", ".", "adam_epsilon", "=", "1e-8", "\n", "self", ".", "_C", ".", "weight_decay", "=", "1e-6", "\n", "self", ".", "_C", ".", "num_training_steps", "=", "0", "\n", "self", ".", "_C", ".", "gradient_accumulation_steps", "=", "1", "\n", "self", ".", "_C", ".", "warmup_ratio", "=", "0.0", "\n", "self", ".", "_C", ".", "warmup_step", "=", "0", "\n", "self", ".", "_C", ".", "oversample", "=", "1", "\n", "self", ".", "_C", ".", "max_epoch", "=", "10", "\n", "self", ".", "_C", ".", "checkpoint_every_step", "=", "500", "\n", "self", ".", "_C", ".", "max_grad_norm", "=", "1.0", "\n", "self", ".", "_C", ".", "filter_by_min_length", "=", "-", "1", "\n", "self", ".", "_C", ".", "prefix_length", "=", "0", "\n", "self", ".", "_C", ".", "prefix_set_number", "=", "0", "\n", "self", ".", "_C", ".", "enable_layer_wise_prefix", "=", "True", "\n", "self", ".", "_C", ".", "load_from_pretrained", "=", "False", "\n", "self", ".", "_C", ".", "max_length", "=", "128", "\n", "self", ".", "_C", ".", "enable_full_finetune", "=", "False", "\n", "self", ".", "_C", ".", "enable_filtering_error", "=", "False", "\n", "self", ".", "_C", ".", "enable_consistency_filtering", "=", "False", "\n", "self", ".", "_C", ".", "enable_full_pretrain", "=", "False", "\n", "self", ".", "_C", ".", "enable_adam_opt", "=", "False", "\n", "self", ".", "_C", ".", "enable_sentence_classification", "=", "False", "\n", "self", ".", "_C", ".", "enable_pair_sentence_classification", "=", "False", "\n", "self", ".", "_C", ".", "select_model_by_ppl", "=", "False", "\n", "self", ".", "_C", ".", "score_top_ratio", "=", "-", "1.0", "\n", "self", ".", "_C", ".", "enable_eval_oversample", "=", "False", "\n", "self", ".", "_C", ".", "pre_training_modes", "=", "[", "]", "\n", "self", ".", "_C", ".", "training_da_mode", "=", "[", "]", "\n", "self", ".", "_C", ".", "eval_da_mode", "=", "[", "]", "\n", "self", ".", "_C", ".", "lm_gen_train_path_list", "=", "[", "]", "\n", "\n", "# Override parameter values from YAML file first, then from override list.", "\n", "self", ".", "_C", ".", "merge_from_file", "(", "config_yaml", ")", "\n", "self", ".", "_C", ".", "merge_from_list", "(", "config_override", ")", "\n", "\n", "# Make an instantiated object of this class immutable.", "\n", "self", ".", "_C", ".", "freeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.dump": [[62, 64], ["config.Config._C.dump", "open"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.dump"], ["", "def", "dump", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "\t\t", "self", ".", "_C", ".", "dump", "(", "stream", "=", "open", "(", "file_path", ",", "\"w\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.__getattr__": [[65, 69], ["config.Config._C.__getattr__", "AttributeError"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.__getattr__"], ["", "def", "__getattr__", "(", "self", ",", "attr", ":", "str", ")", ":", "\n", "\t\t", "if", "attr", "==", "\"__setstate__\"", ":", "\n", "\t\t\t", "raise", "AttributeError", "(", "attr", ")", "\n", "", "return", "self", ".", "_C", ".", "__getattr__", "(", "attr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.__str__": [[70, 72], ["config._config_str"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.config._config_str"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "\t\t", "return", "_config_str", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.__repr__": [[73, 75], ["config.Config._C.__repr__"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.config.Config.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "_C", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.config._config_str": [[77, 93], ["yacs.config.CfgNode", "str"], "function", ["None"], ["", "", "def", "_config_str", "(", "config", ":", "Config", ")", "->", "str", ":", "\n", "    ", "r\"\"\"\n    Collect a subset of config in sensible order (not alphabetical) according to phase. Used by\n    :func:`Config.__str__()`.\n\n    Parameters\n    ----------\n    config: Config\n        A :class:`Config` object which is to be printed.\n    \"\"\"", "\n", "_C", "=", "config", "\n", "\n", "__C", ":", "CN", "=", "CN", "(", "{", "\"RANDOM_SEED\"", ":", "_C", ".", "random_seed", "}", ")", "\n", "common_string", ":", "str", "=", "str", "(", "__C", ")", "+", "\"\\n\"", "\n", "\n", "return", "common_string", "", "", ""]], "home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering.read_conll": [[9, 29], ["pathlib.Path", "pathlib.Path.read_text().strip", "re.split", "doc.split", "token_docs.append", "tag_docs.append", "pathlib.Path.read_text", "line.split", "len", "tokens.append", "tags.append"], "function", ["None"], ["def", "read_conll", "(", "file_path", ")", ":", "\n", "    ", "file_path", "=", "Path", "(", "file_path", ")", "\n", "\n", "raw_text", "=", "file_path", ".", "read_text", "(", ")", ".", "strip", "(", ")", "\n", "raw_docs", "=", "re", ".", "split", "(", "r'\\n\\t?\\n'", ",", "raw_text", ")", "\n", "token_docs", "=", "[", "]", "\n", "tag_docs", "=", "[", "]", "\n", "for", "doc", "in", "raw_docs", ":", "\n", "        ", "tokens", "=", "[", "]", "\n", "tags", "=", "[", "]", "\n", "for", "line", "in", "doc", ".", "split", "(", "'\\n'", ")", ":", "\n", "            ", "items", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "items", ")", "==", "2", ":", "\n", "                ", "token", ",", "tag", "=", "items", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "tags", ".", "append", "(", "tag", ")", "\n", "", "", "token_docs", ".", "append", "(", "tokens", ")", "\n", "tag_docs", ".", "append", "(", "tags", ")", "\n", "\n", "", "return", "token_docs", ",", "tag_docs", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data_sen_classification.read_sentence_classification": [[6, 17], ["open", "l.strip.strip", "l.strip.split", "token_docs.append", "tag_docs.append"], "function", ["None"], ["def", "read_sentence_classification", "(", "file_path", ")", ":", "\n", "    ", "token_docs", "=", "[", "]", "\n", "tag_docs", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "token_docs", ".", "append", "(", "items", "[", "0", "]", ")", "\n", "tag_docs", ".", "append", "(", "items", "[", "1", "]", ")", "\n", "\n", "", "", "return", "token_docs", ",", "tag_docs", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data_sen_classification.read_pair_sentence_label_data": [[18, 29], ["open", "l.strip.strip", "l.strip.split", "token_docs.append", "output_labels.append", "len"], "function", ["None"], ["", "def", "read_pair_sentence_label_data", "(", "data_path", ")", ":", "\n", "    ", "token_docs", "=", "[", "]", "\n", "output_labels", "=", "[", "]", "\n", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "3", ":", "continue", "\n", "token_docs", ".", "append", "(", "\"%s\\t%s\"", "%", "(", "items", "[", "0", "]", ",", "items", "[", "1", "]", ")", ")", "\n", "output_labels", ".", "append", "(", "items", "[", "2", "]", ")", "\n", "", "", "return", "token_docs", ",", "output_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data_sen_classification.extract_data": [[30, 95], ["random.seed", "os.makedirs", "zip", "print", "print", "set", "print", "os.path.join", "generate_few_shot_data_sen_classification.read_sentence_classification", "generate_few_shot_data_sen_classification.read_pair_sentence_label_data", "all_data.append", "tag_dict[].append", "open", "zip", "os.path.join", "print", "len", "len", "random.shuffle", "random.shuffle", "ValueError", "len", "out.write", "open", "zip", "set.add", "selected_sen.append", "selected_tag.append", "set.add", "selected_sen.append", "selected_tag.append", "len", "out.write", "len", "len", "sen.split", "sen.split"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering_sen_cls.read_sentence_classification", "home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering_sen_cls.read_pair_sentence_label_data"], ["", "def", "extract_data", "(", "_A", ",", "data_path", ",", "split_name", ",", "save_unlabel_data", "=", "True", ")", ":", "\n", "    ", "if", "not", "_A", ".", "enable_sentence_pair", ":", "\n", "        ", "token_docs", ",", "tag_docs", "=", "read_sentence_classification", "(", "data_path", ")", "\n", "", "else", ":", "\n", "        ", "token_docs", ",", "tag_docs", "=", "read_pair_sentence_label_data", "(", "data_path", ")", "\n", "\n", "", "random", ".", "seed", "(", "_A", ".", "random_seed", ")", "\n", "os", ".", "makedirs", "(", "_A", ".", "output_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "tag_dict", "=", "{", "}", "\n", "all_data", "=", "[", "]", "\n", "for", "tokens", ",", "tag", "in", "zip", "(", "token_docs", ",", "tag_docs", ")", ":", "\n", "        ", "all_data", ".", "append", "(", "(", "tokens", ",", "tag", ")", ")", "\n", "if", "tag", "not", "in", "tag_dict", ":", "\n", "            ", "tag_dict", "[", "tag", "]", "=", "[", "]", "\n", "", "tag_dict", "[", "tag", "]", ".", "append", "(", "(", "tokens", ",", "tag", ")", ")", "\n", "\n", "", "print", "(", "\"found %d tag slots\"", "%", "len", "(", "tag_dict", ")", ")", "\n", "print", "(", "\"found %d instances\"", "%", "len", "(", "all_data", ")", ")", "\n", "\n", "\n", "selected_sen", "=", "[", "]", "\n", "selected_tag", "=", "[", "]", "\n", "used_sen", "=", "set", "(", ")", "\n", "if", "_A", ".", "few_shot_k", ">", "0", ":", "\n", "        ", "for", "chunk", "in", "tag_dict", ":", "\n", "            ", "random", ".", "shuffle", "(", "tag_dict", "[", "chunk", "]", ")", "\n", "count", "=", "0", "\n", "for", "(", "sen", ",", "tags", ")", "in", "tag_dict", "[", "chunk", "]", ":", "\n", "                ", "if", "(", "sen", "not", "in", "used_sen", ")", "and", "len", "(", "sen", ".", "split", "(", ")", ")", ">", "_A", ".", "min_length", ":", "\n", "                    ", "used_sen", ".", "add", "(", "sen", ")", "\n", "selected_sen", ".", "append", "(", "sen", ")", "\n", "selected_tag", ".", "append", "(", "tags", ")", "\n", "count", "+=", "1", "\n", "", "if", "count", "==", "_A", ".", "few_shot_k", ":", "break", "\n", "", "", "", "elif", "_A", ".", "total_training_num", ">", "0", ":", "\n", "        ", "random", ".", "shuffle", "(", "all_data", ")", "\n", "for", "(", "sen", ",", "tags", ")", "in", "all_data", ":", "\n", "            ", "if", "(", "sen", "not", "in", "used_sen", ")", "and", "len", "(", "sen", ".", "split", "(", ")", ")", ">", "_A", ".", "min_length", ":", "\n", "                ", "used_sen", ".", "add", "(", "sen", ")", "\n", "selected_sen", ".", "append", "(", "sen", ")", "\n", "selected_tag", ".", "append", "(", "tags", ")", "\n", "\n", "", "if", "len", "(", "selected_sen", ")", "==", "_A", ".", "total_training_num", ":", "break", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"few_shot_k and total_training_num cannot be non-positive!\"", ")", "\n", "\n", "\n", "", "print", "(", "\"select %d labeled instances\"", "%", "len", "(", "selected_tag", ")", ")", "\n", "num_", "=", "_A", ".", "few_shot_k", "if", "_A", ".", "few_shot_k", ">", "0", "else", "_A", ".", "total_training_num", "\n", "\n", "output_path", "=", "os", ".", "path", ".", "join", "(", "_A", ".", "output_path", ",", "'%s_whole_%d.txt'", "%", "(", "split_name", ",", "num_", ")", ")", "\n", "with", "open", "(", "output_path", ",", "'w'", ")", "as", "out", ":", "\n", "        ", "for", "(", "gen", ",", "labels", ")", "in", "zip", "(", "selected_sen", ",", "selected_tag", ")", ":", "\n", "            ", "out", ".", "write", "(", "\"%s\\t%s\\n\"", "%", "(", "gen", ",", "labels", ")", ")", "\n", "\n", "", "", "if", "save_unlabel_data", ":", "\n", "        ", "unlabeled_output_path", "=", "os", ".", "path", ".", "join", "(", "_A", ".", "output_path", ",", "'unlabeled_train_whole_%d.txt'", "%", "num_", ")", "\n", "unlabeled_count", "=", "0", "\n", "with", "open", "(", "unlabeled_output_path", ",", "'w'", ")", "as", "out", ":", "\n", "            ", "for", "(", "sen", ",", "labels", ")", "in", "zip", "(", "token_docs", ",", "tag_docs", ")", ":", "\n", "                ", "if", "sen", "not", "in", "used_sen", ":", "\n", "                    ", "unlabeled_count", "+=", "1", "\n", "out", ".", "write", "(", "\"%s\\t%s\\n\"", "%", "(", "sen", ",", "labels", ")", ")", "\n", "", "", "", "print", "(", "\"select %d unlabeled instances\"", "%", "unlabeled_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.checkpointing.CheckpointManager.__init__": [[50, 74], ["isinstance", "isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "models", ":", "Union", "[", "nn", ".", "Module", ",", "Dict", "[", "str", ",", "nn", ".", "Module", "]", "]", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "mode", ":", "str", "=", "\"max\"", ",", "\n", "filename_prefix", ":", "str", "=", "\"model\"", ",", "\n", ")", ":", "\n", "\n", "# Convert single model to a dict.", "\n", "        ", "if", "isinstance", "(", "models", ",", "nn", ".", "Module", ")", ":", "\n", "            ", "models", "=", "{", "\"model\"", ":", "models", "}", "\n", "\n", "", "for", "key", "in", "models", ":", "\n", "            ", "if", "not", "isinstance", "(", "models", "[", "key", "]", ",", "nn", ".", "Module", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\"{} is not a Module\"", ".", "format", "(", "type", "(", "models", ")", ".", "__name__", ")", ")", "\n", "\n", "", "", "self", ".", "_models", "=", "models", "\n", "self", ".", "_serialization_dir", "=", "serialization_dir", "\n", "\n", "self", ".", "_mode", "=", "mode", "\n", "self", ".", "_filename_prefix", "=", "filename_prefix", "\n", "\n", "# Initialize members to hold state dict of best checkpoint and its performance.", "\n", "self", ".", "_best_metric", ":", "Optional", "[", "Union", "[", "float", ",", "torch", ".", "Tensor", "]", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.checkpointing.CheckpointManager.step": [[76, 115], ["isinstance", "torch.save", "torch.save", "checkpointing.CheckpointManager._models[].module.state_dict", "checkpointing.CheckpointManager._models[].state_dict", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "metric", ":", "Union", "[", "float", ",", "torch", ".", "Tensor", "]", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Serialize checkpoint and update best checkpoint based on metric and mode.\"\"\"", "\n", "\n", "# Update best checkpoint based on metric and metric mode.", "\n", "if", "not", "self", ".", "_best_metric", ":", "\n", "            ", "self", ".", "_best_metric", "=", "metric", "\n", "\n", "", "models_state_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "_models", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "_models", "[", "key", "]", ",", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "models_state_dict", "[", "key", "]", "=", "self", ".", "_models", "[", "key", "]", ".", "module", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "                ", "models_state_dict", "[", "key", "]", "=", "self", ".", "_models", "[", "key", "]", ".", "state_dict", "(", ")", "\n", "\n", "", "", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "torch", ".", "save", "(", "\n", "models_state_dict", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "\"%s-epoch-%d.pth\"", "%", "(", "self", ".", "_filename_prefix", ",", "epoch", ")", "\n", ")", ",", "\n", ")", "\n", "\n", "\n", "", "if", "(", "self", ".", "_mode", "==", "\"min\"", "and", "metric", "<=", "self", ".", "_best_metric", ")", "or", "(", "\n", "self", ".", "_mode", "==", "\"max\"", "and", "metric", ">=", "self", ".", "_best_metric", "\n", ")", ":", "\n", "            ", "self", ".", "_best_metric", "=", "metric", "\n", "\n", "# Serialize checkpoint corresponding to current epoch (or iteration).", "\n", "torch", ".", "save", "(", "\n", "models_state_dict", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "f\"{self._filename_prefix}-best.pth\"", "\n", ")", ",", "\n", ")", "\n", "\n", "return", "True", "\n", "\n", "", "return", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SeqLabelConsistencyDataset.__init__": [[120, 130], ["pre_train_dataset.SeqLabelConsistencyDataset.tokenizer", "pre_train_dataset.encode_tags", "pre_train_dataset.SeqLabelConsistencyDataset.encodings.pop", "pre_train_dataset.SeqLabelConsistencyDataset.encodings.pop"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.encode_tags"], ["    ", "def", "__init__", "(", "self", ",", "token_docs", ",", "tag_docs", ",", "label_dict", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "label_dict", "=", "label_dict", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "self", ".", "words", "=", "token_docs", "\n", "self", ".", "encodings", "=", "self", ".", "tokenizer", "(", "token_docs", ",", "is_split_into_words", "=", "True", ",", "return_offsets_mapping", "=", "True", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "self", ".", "encoded_labels", "=", "encode_tags", "(", "tag_docs", ",", "self", ".", "encodings", ",", "self", ".", "label_dict", ")", "\n", "\n", "self", ".", "encodings", ".", "pop", "(", "'offset_mapping'", ")", "\n", "self", ".", "encodings", ".", "pop", "(", "'token_type_ids'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SeqLabelConsistencyDataset.__getitem__": [[131, 137], ["torch.tensor", "torch.tensor", "pre_train_dataset.SeqLabelConsistencyDataset.encodings.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "item", "=", "{", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", "]", ")", "for", "key", ",", "val", "in", "self", ".", "encodings", ".", "items", "(", ")", "}", "\n", "item", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "self", ".", "encoded_labels", "[", "idx", "]", ")", "\n", "item", "[", "'labels'", "]", "[", "item", "[", "'attention_mask'", "]", "==", "0", "]", "=", "-", "100", "\n", "item", "[", "'gt_x'", "]", "=", "self", ".", "words", "[", "idx", "]", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SeqLabelConsistencyDataset.__len__": [[138, 140], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoded_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PairSenLabelDataset.__init__": [[143, 160], ["pre_train_dataset.PairSenLabelDataset.read_pair_sentence_label_data", "print", "pre_train_dataset.PairSenLabelDataset.tokenizer", "s.replace", "pre_train_dataset.PairSenLabelDataset.read_pair_sentence_label_data", "len"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering_sen_cls.read_pair_sentence_label_data", "home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering_sen_cls.read_pair_sentence_label_data"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "data_path", ",", "label_dict", ",", "tokenizer", ",", "is_training", "=", "False", ")", ":", "\n", "        ", "self", ".", "label_dict", "=", "label_dict", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "config", "=", "config", "\n", "\n", "token_docs", ",", "output_labels", "=", "self", ".", "read_pair_sentence_label_data", "(", "data_path", ",", "enable_oversample", "=", "True", ")", "\n", "if", "is_training", ":", "\n", "            ", "for", "d_path", "in", "config", ".", "lm_gen_train_path_list", ":", "\n", "                ", "sub_token_docs", ",", "sub_output_labels", "=", "self", ".", "read_pair_sentence_label_data", "(", "d_path", ")", "\n", "token_docs", "+=", "sub_token_docs", "\n", "output_labels", "+=", "sub_output_labels", "\n", "\n", "", "", "print", "(", "\"Total instances %d\"", "%", "len", "(", "token_docs", ")", ")", "\n", "self", ".", "words", "=", "[", "s", ".", "replace", "(", "' [SEP] '", ",", "'\\t'", ")", "for", "s", "in", "token_docs", "]", "\n", "self", ".", "encoded_labels", "=", "output_labels", "\n", "self", ".", "encodings", "=", "self", ".", "tokenizer", "(", "token_docs", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PairSenLabelDataset.read_pair_sentence_label_data": [[161, 174], ["open", "l.strip.strip.strip", "l.strip.strip.split", "range", "token_docs.append", "output_labels.append", "len"], "methods", ["None"], ["", "def", "read_pair_sentence_label_data", "(", "self", ",", "data_path", ",", "enable_oversample", "=", "False", ")", ":", "\n", "        ", "token_docs", "=", "[", "]", "\n", "output_labels", "=", "[", "]", "\n", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "            ", "for", "l", "in", "out", ":", "\n", "                ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "3", ":", "continue", "\n", "if", "items", "[", "2", "]", "not", "in", "self", ".", "label_dict", ":", "continue", "\n", "for", "_", "in", "range", "(", "self", ".", "config", ".", "oversample", "if", "(", "self", ".", "is_training", "or", "self", ".", "config", ".", "enable_eval_oversample", ")", "and", "enable_oversample", "else", "1", ")", ":", "\n", "                    ", "token_docs", ".", "append", "(", "\"%s [SEP] %s\"", "%", "(", "items", "[", "0", "]", ",", "items", "[", "1", "]", ")", ")", "\n", "output_labels", ".", "append", "(", "self", ".", "label_dict", "[", "items", "[", "2", "]", "]", ")", "\n", "", "", "", "return", "token_docs", ",", "output_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PairSenLabelDataset.__getitem__": [[175, 180], ["torch.tensor", "torch.tensor", "pre_train_dataset.PairSenLabelDataset.encodings.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "item", "=", "{", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", "]", ")", "for", "key", ",", "val", "in", "self", ".", "encodings", ".", "items", "(", ")", "}", "\n", "item", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "[", "self", ".", "encoded_labels", "[", "idx", "]", "]", ")", "\n", "item", "[", "'gt_x'", "]", "=", "self", ".", "words", "[", "idx", "]", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PairSenLabelDataset.__len__": [[181, 183], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoded_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SenLabelDataset.__init__": [[187, 204], ["pre_train_dataset.SenLabelDataset.read_sentence_label_data", "print", "pre_train_dataset.SenLabelDataset.tokenizer", "pre_train_dataset.SenLabelDataset.read_sentence_label_data", "len"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SenLabelDataset.read_sentence_label_data", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SenLabelDataset.read_sentence_label_data"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "data_path", ",", "label_dict", ",", "tokenizer", ",", "is_training", "=", "False", ")", ":", "\n", "        ", "self", ".", "label_dict", "=", "label_dict", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "config", "=", "config", "\n", "\n", "token_docs", ",", "output_labels", "=", "self", ".", "read_sentence_label_data", "(", "data_path", ",", "enable_oversample", "=", "True", ")", "\n", "if", "is_training", ":", "\n", "            ", "for", "d_path", "in", "config", ".", "lm_gen_train_path_list", ":", "\n", "                ", "sub_token_docs", ",", "sub_output_labels", "=", "self", ".", "read_sentence_label_data", "(", "d_path", ")", "\n", "token_docs", "+=", "sub_token_docs", "\n", "output_labels", "+=", "sub_output_labels", "\n", "\n", "", "", "print", "(", "\"Total instances %d\"", "%", "len", "(", "token_docs", ")", ")", "\n", "self", ".", "words", "=", "token_docs", "\n", "self", ".", "encoded_labels", "=", "output_labels", "\n", "self", ".", "encodings", "=", "self", ".", "tokenizer", "(", "token_docs", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SenLabelDataset.read_sentence_label_data": [[205, 218], ["open", "l.strip.strip.strip", "l.strip.strip.split", "range", "token_docs.append", "output_labels.append", "len"], "methods", ["None"], ["", "def", "read_sentence_label_data", "(", "self", ",", "data_path", ",", "enable_oversample", "=", "False", ")", ":", "\n", "        ", "token_docs", "=", "[", "]", "\n", "output_labels", "=", "[", "]", "\n", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "            ", "for", "l", "in", "out", ":", "\n", "                ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "2", ":", "continue", "\n", "if", "items", "[", "1", "]", "not", "in", "self", ".", "label_dict", ":", "continue", "\n", "for", "_", "in", "range", "(", "self", ".", "config", ".", "oversample", "if", "(", "self", ".", "is_training", "or", "self", ".", "config", ".", "enable_eval_oversample", ")", "and", "enable_oversample", "else", "1", ")", ":", "\n", "                    ", "token_docs", ".", "append", "(", "items", "[", "0", "]", ")", "\n", "output_labels", ".", "append", "(", "self", ".", "label_dict", "[", "items", "[", "1", "]", "]", ")", "\n", "", "", "", "return", "token_docs", ",", "output_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SenLabelDataset.__getitem__": [[220, 225], ["torch.tensor", "torch.tensor", "pre_train_dataset.SenLabelDataset.encodings.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "item", "=", "{", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", "]", ")", "for", "key", ",", "val", "in", "self", ".", "encodings", ".", "items", "(", ")", "}", "\n", "item", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "[", "self", ".", "encoded_labels", "[", "idx", "]", "]", ")", "\n", "item", "[", "'gt_x'", "]", "=", "self", ".", "words", "[", "idx", "]", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SenLabelDataset.__len__": [[226, 228], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoded_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SeqLabelDataset.__init__": [[249, 281], ["pre_train_dataset.read_conll", "zip", "print", "pre_train_dataset.SeqLabelDataset.tokenizer", "pre_train_dataset.encode_tags", "pre_train_dataset.SeqLabelDataset.encodings.pop", "pre_train_dataset.SeqLabelDataset.encodings.pop", "all", "range", "pre_train_dataset.read_conll", "zip", "len", "token_docs.append", "tag_docs.append", "len", "all", "token_docs.append", "tag_docs.append", "len"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.read_conll", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.encode_tags", "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.read_conll"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "data_path", ",", "label_dict", ",", "tokenizer", ",", "is_training", "=", "False", ")", ":", "\n", "        ", "self", ".", "label_dict", "=", "label_dict", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "config", "=", "config", "\n", "\n", "token_docs", "=", "[", "]", "\n", "tag_docs", "=", "[", "]", "\n", "\n", "tokens", ",", "tags", "=", "read_conll", "(", "data_path", ")", "\n", "for", "token", ",", "tag", "in", "zip", "(", "tokens", ",", "tags", ")", ":", "\n", "            ", "if", "all", "(", "[", "t", "in", "self", ".", "label_dict", "for", "t", "in", "tag", "]", ")", "and", "(", "len", "(", "token", ")", "<", "200", "or", "not", "self", ".", "is_training", ")", ":", "\n", "                ", "for", "_", "in", "range", "(", "config", ".", "oversample", "if", "(", "self", ".", "is_training", "or", "self", ".", "config", ".", "enable_eval_oversample", ")", "else", "1", ")", ":", "\n", "                    ", "token_docs", ".", "append", "(", "token", ")", "\n", "tag_docs", ".", "append", "(", "tag", ")", "\n", "\n", "", "", "", "if", "is_training", ":", "\n", "            ", "for", "d_path", "in", "config", ".", "lm_gen_train_path_list", ":", "\n", "                ", "tokens", ",", "tags", "=", "read_conll", "(", "d_path", ")", "\n", "for", "token", ",", "tag", "in", "zip", "(", "tokens", ",", "tags", ")", ":", "\n", "                    ", "if", "all", "(", "[", "t", "in", "self", ".", "label_dict", "for", "t", "in", "tag", "]", ")", "and", "len", "(", "token", ")", "<", "200", ":", "\n", "                        ", "token_docs", ".", "append", "(", "token", ")", "\n", "tag_docs", ".", "append", "(", "tag", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Total instances %d\"", "%", "len", "(", "tag_docs", ")", ")", "\n", "\n", "self", ".", "words", "=", "token_docs", "\n", "self", ".", "encodings", "=", "self", ".", "tokenizer", "(", "token_docs", ",", "is_split_into_words", "=", "True", ",", "return_offsets_mapping", "=", "True", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "self", ".", "encoded_labels", "=", "encode_tags", "(", "tag_docs", ",", "self", ".", "encodings", ",", "self", ".", "label_dict", ")", "\n", "\n", "self", ".", "encodings", ".", "pop", "(", "'offset_mapping'", ")", "\n", "self", ".", "encodings", ".", "pop", "(", "'token_type_ids'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SeqLabelDataset.__getitem__": [[282, 288], ["torch.tensor", "torch.tensor", "pre_train_dataset.SeqLabelDataset.encodings.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "item", "=", "{", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", "]", ")", "for", "key", ",", "val", "in", "self", ".", "encodings", ".", "items", "(", ")", "}", "\n", "item", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "self", ".", "encoded_labels", "[", "idx", "]", ")", "\n", "item", "[", "'labels'", "]", "[", "item", "[", "'attention_mask'", "]", "==", "0", "]", "=", "-", "100", "\n", "item", "[", "'gt_x'", "]", "=", "self", ".", "words", "[", "idx", "]", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.SeqLabelDataset.__len__": [[289, 291], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoded_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PreTrainDataset.__init__": [[311, 325], ["rake_nltk.Rake", "open", "tqdm.tqdm.tqdm", "l.strip.strip.strip", "len", "pre_train_dataset.PreTrainDataset.record.append"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "tokenizer", ",", "data_path", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "pre_training_modes", "=", "config", ".", "pre_training_modes", "\n", "self", ".", "mode_func", "=", "{", "\n", "\"webpage_syn_keyword\"", ":", "self", ".", "webpage_syn_keyword", ",", "\n", "}", "\n", "self", ".", "r", "=", "Rake", "(", ")", "\n", "self", ".", "record", "=", "[", "]", "\n", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "            ", "for", "l", "in", "tqdm", "(", "out", ")", ":", "\n", "                ", "l", "=", "l", ".", "strip", "(", ")", "\n", "if", "len", "(", "l", ")", ">", "0", ":", "\n", "                    ", "self", ".", "record", ".", "append", "(", "l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PreTrainDataset.webpage_syn_keyword": [[326, 350], ["text.split", "random.randint", "random.randint", "random.randint", "random.randint", "random.randint", "random.randint", "random.randint", "random.randint", "pre_train_dataset.PreTrainDataset.r.extract_keywords_from_sentences", "random.randint", "random.randint", "random.randint", "random.randint", "pre_train_dataset.clean_top_features", "pre_train_dataset.PreTrainDataset.r.get_ranked_phrases", "len", "random.random", "random.random", "random.random", "random.random", "syn_keyword.append", "keyword.split", "syn_keyword.append", "pre_train_dataset.PreTrainDataset.tokenizer", "pre_train_dataset.PreTrainDataset.tokenizer", "random.choice.strip", "random.choice.strip", "nw_list.append", "len", "random.choice", "random.choice", "random.choice", "random.choice", "ws.lemma_names", "nltk.corpus.wordnet.synsets"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.clean_top_features"], ["", "", "", "", "def", "webpage_syn_keyword", "(", "self", ",", "text", ")", ":", "\n", "        ", "sen_list", "=", "text", ".", "split", "(", "'\\t'", ")", "\n", "sen_count", "=", "random", ".", "randint", "(", "1", ",", "3", ")", "\n", "start_index", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "sen_list", ")", "-", "sen_count", ")", "\n", "selected_sens", "=", "sen_list", "[", "start_index", ":", "start_index", "+", "sen_count", "]", "\n", "self", ".", "r", ".", "extract_keywords_from_sentences", "(", "selected_sens", ")", "\n", "keyword_count", "=", "random", ".", "randint", "(", "1", ",", "5", ")", "\n", "keywords", "=", "clean_top_features", "(", "self", ".", "r", ".", "get_ranked_phrases", "(", ")", ",", "top", "=", "keyword_count", ")", "\n", "syn_keyword", "=", "[", "]", "\n", "for", "keyword", "in", "keywords", ":", "\n", "            ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "syn_keyword", ".", "append", "(", "keyword", ")", "\n", "", "else", ":", "\n", "                ", "nw_list", "=", "[", "]", "\n", "for", "w", "in", "keyword", ".", "split", "(", ")", ":", "\n", "                    ", "w", "=", "w", ".", "strip", "(", ")", "\n", "w_syn", "=", "[", "ws", ".", "lemma_names", "(", ")", "[", "0", "]", "for", "ws", "in", "wn", ".", "synsets", "(", "w", ")", "]", "\n", "if", "len", "(", "w_syn", ")", ">", "0", ":", "\n", "                        ", "w", "=", "random", ".", "choice", "(", "w_syn", ")", "\n", "", "nw_list", ".", "append", "(", "w", ")", "\n", "", "syn_keyword", ".", "append", "(", "' '", ".", "join", "(", "nw_list", ")", ")", "\n", "", "", "s_doc_token", "=", "self", ".", "tokenizer", "(", "' and '", ".", "join", "(", "syn_keyword", ")", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "t_doc_token", "=", "self", ".", "tokenizer", "(", "' '", ".", "join", "(", "selected_sens", ")", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "return", "s_doc_token", ",", "t_doc_token", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PreTrainDataset.__len__": [[351, 353], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "record", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.PreTrainDataset.__getitem__": [[354, 357], ["random.choice", "random.choice", "random.choice", "random.choice"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "method", "=", "random", ".", "choice", "(", "self", ".", "pre_training_modes", ")", "\n", "return", "self", ".", "mode_func", "[", "method", "]", "(", "self", ".", "record", "[", "index", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixSenClsDataset.__init__": [[378, 444], ["rake_nltk.Rake", "print", "len", "len", "open", "len", "len", "l.strip.strip.strip", "l.strip.strip.split", "range", "set", "open", "pre_train_dataset.NLGMixSenClsDataset.data_list.append", "items[].split", "w.lower.lower.lower", "vocab2doc[].append", "l.strip.strip.strip", "l.strip.strip.split", "range", "set", "len", "pre_train_dataset.NLGMixSenClsDataset.data_list.append", "combined_sen.split", "w.lower.lower.lower", "vocab2doc[].append", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "config", ",", "data_path", ",", "tokenizer", ",", "label_index", ",", "is_training", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "label_index", "=", "label_index", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "r", "=", "Rake", "(", ")", "\n", "self", ".", "data_list", "=", "[", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "assert", "len", "(", "self", ".", "config", ".", "training_da_mode", ")", ">", "0", "\n", "self", ".", "da_mode", "=", "self", ".", "config", ".", "training_da_mode", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "self", ".", "config", ".", "eval_da_mode", ")", ">", "0", "\n", "self", ".", "da_mode", "=", "self", ".", "config", ".", "eval_da_mode", "\n", "\n", "", "self", ".", "mode_func", "=", "{", "\n", "\"keyword\"", ":", "self", ".", "gen_from_keyword_sequence", ",", "\n", "\"tag\"", ":", "self", ".", "gen_from_tag_sequence", ",", "\n", "}", "\n", "\n", "self", ".", "task_index", "=", "{", "\n", "\"keyword\"", ":", "0", ",", "\n", "\"tag\"", ":", "1", "if", "config", ".", "prefix_set_number", "==", "2", "else", "0", "\n", "}", "\n", "\n", "self", ".", "data_list", "=", "[", "]", "\n", "vocab2doc", "=", "{", "}", "\n", "\n", "if", "config", ".", "enable_sentence_classification", ":", "\n", "            ", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "                ", "doc_id", "=", "0", "\n", "for", "l", "in", "out", ":", "\n", "                    ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "2", ":", "continue", "\n", "for", "_", "in", "range", "(", "config", ".", "eval_data_replication", "if", "not", "is_training", "else", "1", ")", ":", "\n", "                        ", "self", ".", "data_list", ".", "append", "(", "(", "items", "[", "0", "]", ",", "items", "[", "1", "]", ")", ")", "\n", "", "for", "w", "in", "set", "(", "items", "[", "0", "]", ".", "split", "(", ")", ")", ":", "\n", "                        ", "w", "=", "w", ".", "lower", "(", ")", "\n", "if", "w", "not", "in", "vocab2doc", ":", "\n", "                            ", "vocab2doc", "[", "w", "]", "=", "[", "]", "\n", "", "vocab2doc", "[", "w", "]", ".", "append", "(", "doc_id", ")", "\n", "", "doc_id", "+=", "1", "\n", "", "", "", "elif", "config", ".", "enable_pair_sentence_classification", ":", "\n", "            ", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "                ", "doc_id", "=", "0", "\n", "for", "l", "in", "out", ":", "\n", "                    ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "3", ":", "continue", "\n", "sentence", "=", "\"%s [SEP] %s\"", "%", "(", "items", "[", "0", "]", ",", "items", "[", "1", "]", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "eval_data_replication", "if", "not", "is_training", "else", "1", ")", ":", "\n", "                        ", "self", ".", "data_list", ".", "append", "(", "(", "sentence", ",", "items", "[", "2", "]", ")", ")", "\n", "", "combined_sen", "=", "\"%s %s\"", "%", "(", "items", "[", "0", "]", ",", "items", "[", "1", "]", ")", "\n", "for", "w", "in", "set", "(", "combined_sen", ".", "split", "(", ")", ")", ":", "\n", "                        ", "w", "=", "w", ".", "lower", "(", ")", "\n", "if", "w", "not", "in", "vocab2doc", ":", "\n", "                            ", "vocab2doc", "[", "w", "]", "=", "[", "]", "\n", "", "vocab2doc", "[", "w", "]", ".", "append", "(", "doc_id", ")", "\n", "", "doc_id", "+=", "1", "\n", "\n", "", "", "", "self", ".", "idf_value", "=", "{", "}", "\n", "for", "w", "in", "vocab2doc", ":", "\n", "            ", "self", ".", "idf_value", "[", "w", "]", "=", "doc_id", "/", "len", "(", "vocab2doc", "[", "w", "]", ")", "\n", "\n", "", "print", "(", "\"Data Size %d\"", "%", "len", "(", "self", ".", "data_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixSenClsDataset.__len__": [[445, 447], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixSenClsDataset.__getitem__": [[448, 454], ["random.choice", "random.choice", "random.choice", "random.choice"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "(", "word", ",", "tag", ")", "=", "self", ".", "data_list", "[", "idx", "]", "\n", "current_mode", "=", "random", ".", "choice", "(", "self", ".", "da_mode", ")", "\n", "task_index", "=", "self", ".", "task_index", "[", "current_mode", "]", "\n", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "=", "self", ".", "mode_func", "[", "current_mode", "]", "(", "word", ",", "tag", ")", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", ",", "task_index", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixSenClsDataset.gen_from_tag_sequence": [[455, 461], ["pre_train_dataset.NLGMixSenClsDataset.tokenizer", "pre_train_dataset.NLGMixSenClsDataset.tokenizer"], "methods", ["None"], ["", "def", "gen_from_tag_sequence", "(", "self", ",", "token", ",", "tag", ")", ":", "\n", "        ", "input_y", "=", "tag", "+", "\" \"", "+", "token", "\n", "input_x", "=", "tag", "\n", "y_np", "=", "self", ".", "tokenizer", "(", "input_y", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "x_np", "=", "self", ".", "tokenizer", "(", "input_x", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixSenClsDataset.gen_from_keyword_sequence": [[462, 479], ["pre_train_dataset.NLGMixSenClsDataset.r.extract_keywords_from_text", "pre_train_dataset.NLGMixSenClsDataset.r.get_ranked_phrases", "random.shuffle", "random.shuffle", "random.shuffle", "random.shuffle", "len", "pre_train_dataset.clean_top_features", "token.split", "sorted", "pre_train_dataset.NLGMixSenClsDataset.tokenizer", "pre_train_dataset.NLGMixSenClsDataset.tokenizer", "w.lower", "w.lower"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.clean_top_features"], ["", "def", "gen_from_keyword_sequence", "(", "self", ",", "token", ",", "tag", ")", ":", "\n", "        ", "input_y", "=", "tag", "+", "\" \"", "+", "token", "\n", "self", ".", "r", ".", "extract_keywords_from_text", "(", "token", ")", "\n", "raw_keyword_list", "=", "self", ".", "r", ".", "get_ranked_phrases", "(", ")", "\n", "if", "len", "(", "raw_keyword_list", ")", ">", "0", ":", "\n", "            ", "mention_list", "=", "clean_top_features", "(", "raw_keyword_list", ",", "top", "=", "5", ")", "\n", "", "else", ":", "\n", "            ", "current_tokens", "=", "token", ".", "split", "(", ")", "\n", "w_idf", "=", "[", "(", "w", ",", "self", ".", "idf_value", "[", "w", ".", "lower", "(", ")", "]", ")", "for", "w", "in", "current_tokens", "if", "w", ".", "lower", "(", ")", "in", "self", ".", "idf_value", "]", "\n", "w_idf", "=", "sorted", "(", "w_idf", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "mention_list", "=", "[", "k", "[", "0", "]", "for", "k", "in", "w_idf", "[", ":", "6", "]", "]", "\n", "", "random", ".", "shuffle", "(", "mention_list", ")", "\n", "\n", "input_x", "=", "\" and \"", ".", "join", "(", "mention_list", ")", "# \"generate with keywords: \" + ", "\n", "y_np", "=", "self", ".", "tokenizer", "(", "input_y", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "x_np", "=", "self", ".", "tokenizer", "(", "input_x", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.__init__": [[485, 533], ["rake_nltk.Rake", "pre_train_dataset.read_conll", "len", "enumerate", "zip", "print", "len", "len", "set", "range", "len", "len", "w.lower.lower.lower", "vocab2doc[].append", "len", "pre_train_dataset.NLGMixDataset.data_list.append", "len"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.read_conll"], ["def", "__init__", "(", "self", ",", "config", ",", "data_path", ",", "tokenizer", ",", "label_index", ",", "is_training", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "label_index", "=", "label_index", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "r", "=", "Rake", "(", ")", "\n", "\n", "self", ".", "data_list", "=", "[", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "assert", "len", "(", "self", ".", "config", ".", "training_da_mode", ")", ">", "0", "\n", "self", ".", "da_mode", "=", "self", ".", "config", ".", "training_da_mode", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "self", ".", "config", ".", "eval_da_mode", ")", ">", "0", "\n", "self", ".", "da_mode", "=", "self", ".", "config", ".", "eval_da_mode", "\n", "\n", "", "self", ".", "mode_func", "=", "{", "\n", "\"keyword\"", ":", "self", ".", "gen_from_keyword_sequence", ",", "\n", "\"tag\"", ":", "self", ".", "gen_from_tag_sequence", ",", "\n", "\"keyword_tag_mixture\"", ":", "self", ".", "gen_from_tag_keyword_mix_sequence", "\n", "}", "\n", "\n", "self", ".", "task_index", "=", "{", "\n", "\"keyword\"", ":", "0", ",", "\n", "\"tag\"", ":", "1", "if", "config", ".", "prefix_set_number", "==", "2", "else", "0", "\n", "}", "\n", "\n", "\n", "tokens", ",", "tags", "=", "read_conll", "(", "data_path", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "tags", ")", "\n", "\n", "doc_count", "=", "len", "(", "tokens", ")", "\n", "vocab2doc", "=", "{", "}", "\n", "for", "doc_id", ",", "words", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "for", "w", "in", "set", "(", "words", ")", ":", "\n", "                ", "w", "=", "w", ".", "lower", "(", ")", "\n", "if", "w", "not", "in", "vocab2doc", ":", "\n", "                    ", "vocab2doc", "[", "w", "]", "=", "[", "]", "\n", "", "vocab2doc", "[", "w", "]", ".", "append", "(", "doc_id", ")", "\n", "", "", "self", ".", "idf_value", "=", "{", "}", "\n", "for", "w", "in", "vocab2doc", ":", "\n", "            ", "self", ".", "idf_value", "[", "w", "]", "=", "doc_count", "/", "len", "(", "vocab2doc", "[", "w", "]", ")", "\n", "\n", "", "for", "(", "word", ",", "tag", ")", "in", "zip", "(", "tokens", ",", "tags", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "config", ".", "eval_data_replication", "if", "not", "is_training", "else", "1", ")", ":", "\n", "                ", "self", ".", "data_list", ".", "append", "(", "(", "word", ",", "tag", ")", ")", "\n", "\n", "", "", "print", "(", "\"Data Size %d\"", "%", "len", "(", "self", ".", "data_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.__len__": [[534, 536], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.__getitem__": [[537, 543], ["random.choice", "random.choice", "random.choice", "random.choice"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "(", "word", ",", "tag", ")", "=", "self", ".", "data_list", "[", "idx", "]", "\n", "current_mode", "=", "random", ".", "choice", "(", "self", ".", "da_mode", ")", "\n", "task_index", "=", "self", ".", "task_index", "[", "current_mode", "]", "\n", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "=", "self", ".", "mode_func", "[", "current_mode", "]", "(", "word", ",", "tag", ")", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", ",", "task_index", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.add_annotation": [[544, 552], ["None"], "methods", ["None"], ["", "def", "add_annotation", "(", "self", ",", "tokens", ",", "chunk_info", ")", ":", "\n", "        ", "entity_label", "=", "\"B-%s\"", "%", "chunk_info", "[", "0", "]", "\n", "if", "chunk_info", "[", "1", "]", "+", "1", "==", "chunk_info", "[", "2", "]", ":", "\n", "            ", "tokens", "[", "chunk_info", "[", "1", "]", "]", "=", "\"%s %s &&\"", "%", "(", "entity_label", ",", "tokens", "[", "chunk_info", "[", "1", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "[", "chunk_info", "[", "1", "]", "]", "=", "\"%s %s\"", "%", "(", "entity_label", ",", "tokens", "[", "chunk_info", "[", "1", "]", "]", ")", "\n", "tokens", "[", "chunk_info", "[", "2", "]", "-", "1", "]", "=", "\"%s &&\"", "%", "(", "tokens", "[", "chunk_info", "[", "2", "]", "-", "1", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.gen_from_tag_sequence": [[553, 568], ["pre_train_dataset.get_chunks", "copy.deepcopy", "random.shuffle", "random.shuffle", "random.shuffle", "random.shuffle", "pre_train_dataset.NLGMixDataset.add_annotation", "label_list.append", "pre_train_dataset.NLGMixDataset.tokenizer", "pre_train_dataset.NLGMixDataset.tokenizer"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunks", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.add_annotation"], ["", "def", "gen_from_tag_sequence", "(", "self", ",", "token", ",", "tag", ")", ":", "\n", "        ", "label_list", "=", "[", "]", "\n", "chunks", "=", "get_chunks", "(", "tag", ")", "\n", "copied_token", "=", "copy", ".", "deepcopy", "(", "token", ")", "\n", "for", "v", "in", "chunks", ":", "\n", "            ", "entity_label", "=", "\"B-%s\"", "%", "v", "[", "0", "]", "\n", "copied_token", "=", "self", ".", "add_annotation", "(", "copied_token", ",", "v", ")", "\n", "label_list", ".", "append", "(", "entity_label", ")", "\n", "", "input_y", "=", "' '", ".", "join", "(", "copied_token", ")", "\n", "random", ".", "shuffle", "(", "label_list", ")", "\n", "input_x", "=", "\" and \"", ".", "join", "(", "label_list", ")", "# \"generate with tags: \" + ", "\n", "y_np", "=", "self", ".", "tokenizer", "(", "input_y", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "x_np", "=", "self", ".", "tokenizer", "(", "input_x", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.gen_from_keyword_sequence": [[569, 592], ["pre_train_dataset.get_chunks", "copy.deepcopy", "input_y.split", "sorted", "list", "random.shuffle", "random.shuffle", "random.shuffle", "random.shuffle", "pre_train_dataset.NLGMixDataset.add_annotation", "entity_mention.split", "set", "pre_train_dataset.NLGMixDataset.tokenizer", "pre_train_dataset.NLGMixDataset.tokenizer", "w.lower", "w.lower"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunks", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.add_annotation"], ["", "def", "gen_from_keyword_sequence", "(", "self", ",", "token", ",", "tag", ")", ":", "\n", "        ", "mention_list", "=", "[", "]", "\n", "chunks", "=", "get_chunks", "(", "tag", ")", "\n", "copied_token", "=", "copy", ".", "deepcopy", "(", "token", ")", "\n", "for", "v", "in", "chunks", ":", "\n", "            ", "entity_mention", "=", "' '", ".", "join", "(", "token", "[", "v", "[", "1", "]", ":", "v", "[", "2", "]", "]", ")", "\n", "copied_token", "=", "self", ".", "add_annotation", "(", "copied_token", ",", "v", ")", "\n", "mention_list", "+=", "entity_mention", ".", "split", "(", ")", "\n", "", "input_y", "=", "' '", ".", "join", "(", "copied_token", ")", "\n", "current_tokens", "=", "input_y", ".", "split", "(", ")", "\n", "w_idf", "=", "[", "(", "w", ",", "self", ".", "idf_value", "[", "w", ".", "lower", "(", ")", "]", ")", "for", "w", "in", "current_tokens", "if", "w", ".", "lower", "(", ")", "in", "self", ".", "idf_value", "]", "\n", "w_idf", "=", "sorted", "(", "w_idf", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "mention_list", "+=", "[", "k", "[", "0", "]", "for", "k", "in", "w_idf", "[", ":", "3", "]", "]", "\n", "mention_list", "=", "list", "(", "set", "(", "mention_list", ")", ")", "\n", "\n", "random", ".", "shuffle", "(", "mention_list", ")", "\n", "mention_list", "=", "mention_list", "[", ":", "3", "]", "\n", "\n", "input_x", "=", "\" and \"", ".", "join", "(", "mention_list", ")", "# \"generate with keywords: \" + ", "\n", "y_np", "=", "self", ".", "tokenizer", "(", "input_y", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "x_np", "=", "self", ".", "tokenizer", "(", "input_x", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.gen_from_tag_keyword_mix_sequence": [[593, 617], ["pre_train_dataset.get_chunks", "copy.deepcopy", "sorted", "list", "random.shuffle", "random.shuffle", "random.shuffle", "random.shuffle", "pre_train_dataset.NLGMixDataset.add_annotation", "entity_mention.split", "list.append", "set", "pre_train_dataset.NLGMixDataset.tokenizer", "pre_train_dataset.NLGMixDataset.tokenizer", "w.lower", "w.lower"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunks", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.NLGMixDataset.add_annotation"], ["", "def", "gen_from_tag_keyword_mix_sequence", "(", "self", ",", "token", ",", "tag", ")", ":", "\n", "        ", "mention_list", "=", "[", "]", "\n", "chunks", "=", "get_chunks", "(", "tag", ")", "\n", "copied_token", "=", "copy", ".", "deepcopy", "(", "token", ")", "\n", "for", "v", "in", "chunks", ":", "\n", "            ", "entity_mention", "=", "' '", ".", "join", "(", "token", "[", "v", "[", "1", "]", ":", "v", "[", "2", "]", "]", ")", "\n", "entity_label", "=", "\"B-%s\"", "%", "v", "[", "0", "]", "\n", "copied_token", "=", "self", ".", "add_annotation", "(", "copied_token", ",", "v", ")", "\n", "mention_list", "+=", "entity_mention", ".", "split", "(", ")", "\n", "mention_list", ".", "append", "(", "entity_label", ")", "\n", "", "input_y", "=", "' '", ".", "join", "(", "copied_token", ")", "\n", "current_tokens", "=", "copied_token", "\n", "w_idf", "=", "[", "(", "w", ",", "self", ".", "idf_value", "[", "w", ".", "lower", "(", ")", "]", ")", "for", "w", "in", "current_tokens", "if", "w", ".", "lower", "(", ")", "in", "self", ".", "idf_value", "]", "\n", "w_idf", "=", "sorted", "(", "w_idf", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "mention_list", "+=", "[", "k", "[", "0", "]", "for", "k", "in", "w_idf", "[", ":", "3", "]", "]", "\n", "mention_list", "=", "list", "(", "set", "(", "mention_list", ")", ")", "\n", "random", ".", "shuffle", "(", "mention_list", ")", "\n", "mention_list", "=", "mention_list", "[", ":", "4", "]", "\n", "\n", "input_x", "=", "\" and \"", ".", "join", "(", "mention_list", ")", "# \"generate with mixture of tags and keywords: \" + ", "\n", "y_np", "=", "self", ".", "tokenizer", "(", "input_y", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "x_np", "=", "self", ".", "tokenizer", "(", "input_x", ",", "return_tensors", "=", "\"np\"", ")", "[", "'input_ids'", "]", "[", "0", ",", ":", "self", ".", "config", ".", "max_length", "]", "\n", "\n", "return", "x_np", ",", "y_np", ",", "input_y", ",", "input_x", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.get_chunk_type": [[14, 18], ["tag_name.split", "tag_name.split"], "function", ["None"], ["def", "get_chunk_type", "(", "tag_name", ")", ":", "\n", "    ", "tag_class", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "tag_type", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "\n", "return", "tag_class", ",", "tag_type", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.get_chunks": [[19, 45], ["enumerate", "chunks.append", "chunks.append", "len", "pre_train_dataset.get_chunk_type", "chunks.append"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunk_type"], ["", "def", "get_chunks", "(", "seq", ")", ":", "\n", "    ", "default", "=", "\"O\"", "\n", "chunks", "=", "[", "]", "\n", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "seq", ")", ":", "\n", "        ", "if", "tok", "==", "default", "and", "chunk_type", "is", "not", "None", ":", "\n", "            ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "\n", "", "elif", "tok", "!=", "default", ":", "\n", "            ", "tok_chunk_class", ",", "tok_chunk_type", "=", "get_chunk_type", "(", "tok", ")", "\n", "if", "chunk_type", "is", "None", ":", "\n", "                ", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "elif", "tok_chunk_type", "!=", "chunk_type", "or", "tok_chunk_class", "==", "\"B\"", ":", "\n", "                ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "if", "chunk_type", "is", "not", "None", ":", "\n", "        ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "len", "(", "seq", ")", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.read_conll": [[46, 66], ["pathlib.Path", "pathlib.Path.read_text().strip", "re.split", "doc.split", "token_docs.append", "tag_docs.append", "pathlib.Path.read_text", "line.split", "len", "tokens.append", "tags.append"], "function", ["None"], ["", "def", "read_conll", "(", "file_path", ")", ":", "\n", "    ", "file_path", "=", "Path", "(", "file_path", ")", "\n", "\n", "raw_text", "=", "file_path", ".", "read_text", "(", ")", ".", "strip", "(", ")", "\n", "raw_docs", "=", "re", ".", "split", "(", "r'\\n\\t?\\n'", ",", "raw_text", ")", "\n", "token_docs", "=", "[", "]", "\n", "tag_docs", "=", "[", "]", "\n", "for", "doc", "in", "raw_docs", ":", "\n", "        ", "tokens", "=", "[", "]", "\n", "tags", "=", "[", "]", "\n", "for", "line", "in", "doc", ".", "split", "(", "'\\n'", ")", ":", "\n", "            ", "items", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "items", ")", "==", "2", ":", "\n", "                ", "token", ",", "tag", "=", "items", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "tags", ".", "append", "(", "tag", ")", "\n", "", "", "token_docs", ".", "append", "(", "tokens", ")", "\n", "tag_docs", ".", "append", "(", "tags", ")", "\n", "\n", "", "return", "token_docs", ",", "tag_docs", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.sorting": [[67, 70], ["sorted"], "function", ["None"], ["", "def", "sorting", "(", "lst", ")", ":", "\n", "    ", "lst2", "=", "sorted", "(", "lst", ",", "key", "=", "len", ")", "\n", "return", "lst2", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.clean_top_features": [[71, 83], ["pre_train_dataset.sorting", "newkeys.append", "range", "newkeys[].startswith", "newkeys.append", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.sorting"], ["", "def", "clean_top_features", "(", "keywords", ",", "top", "=", "5", ")", ":", "\n", "    ", "keywords", "=", "sorting", "(", "keywords", ")", "\n", "newkeys", "=", "[", "]", "\n", "newkeys", ".", "append", "(", "keywords", "[", "len", "(", "keywords", ")", "-", "1", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "keywords", ")", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "if", "newkeys", "[", "len", "(", "newkeys", ")", "-", "1", "]", ".", "startswith", "(", "keywords", "[", "i", "]", ")", ":", "\n", "            ", "continue", "\n", "", "newkeys", ".", "append", "(", "keywords", "[", "i", "]", ")", "\n", "\n", "", "if", "len", "(", "newkeys", ")", ">", "top", ":", "\n", "        ", "return", "newkeys", "[", ":", "top", "]", "\n", "", "return", "newkeys", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.process_tensor": [[84, 101], ["max", "numpy.zeros", "enumerate", "numpy.zeros", "numpy.zeros", "zip", "torch.from_numpy", "len", "torch.from_numpy", "torch.from_numpy", "len", "len"], "function", ["None"], ["", "def", "process_tensor", "(", "tensor_list", ",", "last_dim", ",", "output_mask", "=", "False", ")", ":", "\n", "    ", "tensor_len", "=", "[", "d", ".", "shape", "[", "0", "]", "for", "d", "in", "tensor_list", "]", "\n", "tensor_max_lenth", "=", "max", "(", "tensor_len", ")", "\n", "d_type", "=", "tensor_list", "[", "0", "]", ".", "dtype", "\n", "if", "last_dim", ">", "0", ":", "\n", "        ", "tensor_np", "=", "np", ".", "zeros", "(", "(", "len", "(", "tensor_list", ")", ",", "tensor_max_lenth", ",", "last_dim", ")", ",", "dtype", "=", "d_type", ")", "\n", "", "else", ":", "\n", "        ", "tensor_np", "=", "np", ".", "zeros", "(", "(", "len", "(", "tensor_list", ")", ",", "tensor_max_lenth", ")", ",", "dtype", "=", "d_type", ")", "\n", "", "mask_np", "=", "np", ".", "zeros", "(", "(", "len", "(", "tensor_list", ")", ",", "tensor_max_lenth", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "(", "d", ",", "l", ")", "in", "enumerate", "(", "zip", "(", "tensor_list", ",", "tensor_len", ")", ")", ":", "\n", "        ", "if", "l", ">", "0", ":", "\n", "            ", "tensor_np", "[", "i", ",", ":", "l", "]", "=", "d", "\n", "mask_np", "[", "i", ",", ":", "l", "]", "=", "1", "\n", "", "", "if", "output_mask", ":", "\n", "        ", "return", "torch", ".", "from_numpy", "(", "tensor_np", ")", ",", "torch", ".", "from_numpy", "(", "mask_np", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "from_numpy", "(", "tensor_np", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.encode_tags": [[102, 117], ["zip", "numpy.array", "numpy.ones", "len", "encoded_labels.append", "len", "doc_enc_labels.tolist"], "function", ["None"], ["", "", "def", "encode_tags", "(", "tags", ",", "encodings", ",", "tag2id", ")", ":", "\n", "    ", "labels", "=", "[", "[", "tag2id", "[", "tag", "]", "for", "tag", "in", "doc", "]", "for", "doc", "in", "tags", "]", "\n", "encoded_labels", "=", "[", "]", "\n", "for", "doc_labels", ",", "doc_offset", "in", "zip", "(", "labels", ",", "encodings", ".", "offset_mapping", ")", ":", "\n", "# create an empty array of -100", "\n", "        ", "doc_enc_labels", "=", "np", ".", "ones", "(", "len", "(", "doc_offset", ")", ",", "dtype", "=", "int", ")", "*", "-", "100", "#tag2id['O']", "\n", "arr_offset", "=", "np", ".", "array", "(", "doc_offset", ")", "\n", "\n", "preversed_doc_enc_labels", "=", "doc_enc_labels", "[", "(", "arr_offset", "[", ":", ",", "0", "]", "==", "0", ")", "&", "(", "arr_offset", "[", ":", ",", "1", "]", "!=", "0", ")", "]", "\n", "if", "preversed_doc_enc_labels", ".", "shape", "[", "0", "]", "==", "len", "(", "doc_labels", ")", ":", "\n", "# set labels whose first offset position is 0 and the second is not 0", "\n", "            ", "doc_enc_labels", "[", "(", "arr_offset", "[", ":", ",", "0", "]", "==", "0", ")", "&", "(", "arr_offset", "[", ":", ",", "1", "]", "!=", "0", ")", "]", "=", "doc_labels", "\n", "encoded_labels", ".", "append", "(", "doc_enc_labels", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "return", "encoded_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.sen_data_wrapper": [[229, 236], ["torch.cat", "torch.cat", "torch.cat().long", "d[].unsqueeze", "d[].unsqueeze", "torch.cat", "d[].unsqueeze"], "function", ["None"], ["", "", "def", "sen_data_wrapper", "(", "config", ",", "dataset", ")", ":", "\n", "    ", "input_ids", "=", "torch", ".", "cat", "(", "[", "d", "[", "'input_ids'", "]", ".", "unsqueeze", "(", "0", ")", "for", "d", "in", "dataset", "]", ",", "dim", "=", "0", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "[", "d", "[", "'attention_mask'", "]", ".", "unsqueeze", "(", "0", ")", "for", "d", "in", "dataset", "]", ",", "dim", "=", "0", ")", "\n", "labels", "=", "torch", ".", "cat", "(", "[", "d", "[", "'labels'", "]", ".", "unsqueeze", "(", "0", ")", "for", "d", "in", "dataset", "]", ",", "dim", "=", "0", ")", ".", "long", "(", ")", "\n", "gt_x", "=", "[", "d", "[", "'gt_x'", "]", "for", "d", "in", "dataset", "]", "\n", "\n", "return", "{", "\"input_ids\"", ":", "input_ids", ",", "\"attention_mask\"", ":", "attention_mask", ",", "\"labels\"", ":", "labels", ",", "\"gt_x\"", ":", "gt_x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.get_sen_data_loader": [[237, 244], ["torch.utils.data.DataLoader", "pre_train_dataset.sen_data_wrapper"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.sen_data_wrapper"], ["", "def", "get_sen_data_loader", "(", "config", ",", "dataset", ",", "batch_size", ",", "shuffle", "=", "False", ")", ":", "\n", "    ", "collate_fn", "=", "lambda", "d", ":", "sen_data_wrapper", "(", "config", ",", "d", ")", "\n", "return", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "shuffle", "=", "shuffle", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.seq_data_wrapper": [[292, 299], ["torch.cat", "torch.cat", "torch.cat", "d[].unsqueeze", "d[].unsqueeze", "d[].unsqueeze"], "function", ["None"], ["", "", "def", "seq_data_wrapper", "(", "config", ",", "dataset", ")", ":", "\n", "    ", "input_ids", "=", "torch", ".", "cat", "(", "[", "d", "[", "'input_ids'", "]", ".", "unsqueeze", "(", "0", ")", "for", "d", "in", "dataset", "]", ",", "dim", "=", "0", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "[", "d", "[", "'attention_mask'", "]", ".", "unsqueeze", "(", "0", ")", "for", "d", "in", "dataset", "]", ",", "dim", "=", "0", ")", "\n", "labels", "=", "torch", ".", "cat", "(", "[", "d", "[", "'labels'", "]", ".", "unsqueeze", "(", "0", ")", "for", "d", "in", "dataset", "]", ",", "dim", "=", "0", ")", "\n", "gt_x", "=", "[", "d", "[", "'gt_x'", "]", "for", "d", "in", "dataset", "]", "\n", "\n", "return", "{", "\"input_ids\"", ":", "input_ids", ",", "\"attention_mask\"", ":", "attention_mask", ",", "\"labels\"", ":", "labels", ",", "\"gt_x\"", ":", "gt_x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.get_seq_data_loader": [[300, 307], ["torch.utils.data.DataLoader", "pre_train_dataset.seq_data_wrapper"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.seq_data_wrapper"], ["", "def", "get_seq_data_loader", "(", "config", ",", "dataset", ",", "batch_size", ",", "shuffle", "=", "False", ")", ":", "\n", "    ", "collate_fn", "=", "lambda", "d", ":", "seq_data_wrapper", "(", "config", ",", "d", ")", "\n", "return", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "shuffle", "=", "shuffle", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.data_wrapper": [[358, 364], ["pre_train_dataset.process_tensor", "pre_train_dataset.process_tensor"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.process_tensor", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.process_tensor"], ["", "", "def", "data_wrapper", "(", "config", ",", "dataset", ")", ":", "\n", "    ", "encoder_input_ids", ",", "encoder_mask", "=", "process_tensor", "(", "[", "d", "[", "0", "]", "for", "d", "in", "dataset", "]", ",", "0", ",", "output_mask", "=", "True", ")", "\n", "decoder_input_ids", ",", "decoder_mask", "=", "process_tensor", "(", "[", "d", "[", "1", "]", "for", "d", "in", "dataset", "]", ",", "0", ",", "output_mask", "=", "True", ")", "\n", "decoder_input_ids", "[", "decoder_mask", "==", "0", "]", "=", "-", "100", "\n", "\n", "return", "{", "\"encoder_input_ids\"", ":", "encoder_input_ids", ",", "\"encoder_mask\"", ":", "encoder_mask", ",", "\"decoder_input_ids\"", ":", "decoder_input_ids", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.get_data_loader": [[365, 372], ["torch.utils.data.DataLoader", "pre_train_dataset.data_wrapper"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.data_wrapper"], ["", "def", "get_data_loader", "(", "config", ",", "dataset", ",", "batch_size", ",", "shuffle", "=", "False", ")", ":", "\n", "    ", "collate_fn", "=", "lambda", "d", ":", "data_wrapper", "(", "config", ",", "d", ")", "\n", "return", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "shuffle", "=", "shuffle", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.nlg_data_wrapper": [[618, 630], ["pre_train_dataset.process_tensor", "pre_train_dataset.process_tensor", "len", "torch.tensor().long", "torch.tensor().long", "torch.tensor", "torch.tensor", "range", "len"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.process_tensor", "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.process_tensor"], ["", "", "def", "nlg_data_wrapper", "(", "config", ",", "dataset", ")", ":", "\n", "    ", "encoder_input_ids", ",", "encoder_mask", "=", "process_tensor", "(", "[", "d", "[", "0", "]", "for", "d", "in", "dataset", "]", ",", "0", ",", "output_mask", "=", "True", ")", "\n", "decoder_input_ids", ",", "decoder_mask", "=", "process_tensor", "(", "[", "d", "[", "1", "]", "for", "d", "in", "dataset", "]", ",", "0", ",", "output_mask", "=", "True", ")", "\n", "decoder_input_ids", "[", "decoder_mask", "==", "0", "]", "=", "-", "100", "\n", "gt_y", "=", "[", "d", "[", "2", "]", "for", "d", "in", "dataset", "]", "\n", "gt_x", "=", "[", "d", "[", "3", "]", "for", "d", "in", "dataset", "]", "\n", "if", "len", "(", "dataset", "[", "0", "]", ")", "==", "5", ":", "\n", "        ", "task_index", "=", "torch", ".", "tensor", "(", "[", "d", "[", "4", "]", "for", "d", "in", "dataset", "]", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "        ", "task_index", "=", "torch", ".", "tensor", "(", "[", "0", "for", "_", "in", "range", "(", "len", "(", "dataset", ")", ")", "]", ")", ".", "long", "(", ")", "\n", "\n", "", "return", "{", "\"task_index\"", ":", "task_index", ",", "\"encoder_input_ids\"", ":", "encoder_input_ids", ",", "\"encoder_mask\"", ":", "encoder_mask", ",", "\"decoder_input_ids\"", ":", "decoder_input_ids", ",", "\"gt_y\"", ":", "gt_y", ",", "\"gt_x\"", ":", "gt_x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.nlg_get_data_loader": [[631, 638], ["torch.utils.data.DataLoader", "pre_train_dataset.nlg_data_wrapper"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_dataset.nlg_data_wrapper"], ["", "def", "nlg_get_data_loader", "(", "config", ",", "dataset", ",", "batch_size", ",", "shuffle", "=", "False", ")", ":", "\n", "    ", "collate_fn", "=", "lambda", "d", ":", "nlg_data_wrapper", "(", "config", ",", "d", ")", "\n", "return", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "shuffle", "=", "shuffle", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_SeqLabel.get_label_dict": [[14, 21], ["open", "out.readlines", "l.strip.strip", "len"], "function", ["None"], ["def", "get_label_dict", "(", "label_path", ")", ":", "\n", "    ", "label_dict", "=", "{", "}", "\n", "with", "open", "(", "label_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ".", "readlines", "(", ")", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "label_dict", "[", "l", "]", "=", "len", "(", "label_dict", ")", "\n", "", "", "return", "label_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_SeqLabel.evaluation": [[22, 101], ["model.to.eval", "model.to.to", "torch.nn.Softmax", "torch.nn.Softmax", "np.argmax", "range", "print", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "range", "seqeval.metrics.f1_score", "sorted", "int", "print", "zip", "model.to.", "torch.nn.Softmax.", "torch.max", "torch.max", "mean_score.detach().cpu().numpy().tolist", "range", "range", "selected_words.append", "selected_tags.append", "selected_pred.append", "selected_gt.append", "seqeval.metrics.f1_score", "open", "zip", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "softmax.detach().cpu().numpy", "batch[].detach().cpu().numpy", "np.append", "np.append", "preds_list[].append", "out_label_list[].append", "zip", "len", "selected_words.append", "selected_tags.append", "zip", "out.write", "batch[].to", "softmax.detach().cpu().numpy", "batch[].detach().cpu().numpy", "mean_score.detach().cpu().numpy", "selected_words.append", "selected_tags.append", "out.write", "softmax.detach().cpu", "batch[].detach().cpu", "len", "softmax.detach().cpu", "batch[].detach().cpu", "mean_score.detach().cpu", "softmax.detach", "batch[].detach", "softmax.detach", "batch[].detach", "mean_score.detach"], "function", ["None"], ["", "def", "evaluation", "(", "config", ",", "eval_data", ",", "model", ",", "label_map", ",", "device", ",", "show_detail", "=", "False", ",", "output_path", "=", "None", ")", ":", "\n", "\t", "model", ".", "eval", "(", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "preds", "=", "None", "\n", "pad_token_label_id", "=", "-", "100", "\n", "input_words", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t", "for", "batch", "in", "tqdm", "(", "eval_data", ")", ":", "\n", "\t\t\t", "for", "n", "in", "batch", ":", "\n", "\t\t\t\t", "if", "batch", "[", "n", "]", "is", "not", "None", "and", "n", "not", "in", "[", "'gt_x'", "]", ":", "\n", "\t\t\t\t\t", "batch", "[", "n", "]", "=", "batch", "[", "n", "]", ".", "to", "(", "device", ")", "\n", "\n", "", "", "outputs", "=", "model", "(", "\n", "input_ids", "=", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", "\n", ")", "\n", "\n", "logits", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "softmax", "(", "logits", ")", "\n", "max_score", ",", "_", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "mean_score", "=", "torch", ".", "sum", "(", "max_score", "*", "batch", "[", "'attention_mask'", "]", ",", "dim", "=", "1", ")", "/", "torch", ".", "sum", "(", "batch", "[", "'attention_mask'", "]", ",", "dim", "=", "1", ")", "\n", "\n", "if", "preds", "is", "None", ":", "\n", "\t\t\t\t", "preds", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "out_label_ids", "=", "batch", "[", "\"labels\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "preds", "=", "np", ".", "append", "(", "preds", ",", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "out_label_ids", "=", "np", ".", "append", "(", "out_label_ids", ",", "batch", "[", "\"labels\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "input_words", "+=", "batch", "[", "'gt_x'", "]", "\n", "scores", "+=", "mean_score", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "", "", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "2", ")", "\n", "\n", "preds_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "out_label_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "\t\t", "for", "j", "in", "range", "(", "out_label_ids", ".", "shape", "[", "1", "]", ")", ":", "\n", "\t\t\t", "if", "out_label_ids", "[", "i", ",", "j", "]", "!=", "pad_token_label_id", ":", "\n", "\t\t\t\t", "preds_list", "[", "i", "]", ".", "append", "(", "label_map", "[", "preds", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "out_label_list", "[", "i", "]", ".", "append", "(", "label_map", "[", "out_label_ids", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "\n", "", "", "", "new_F", "=", "f1_score", "(", "out_label_list", ",", "preds_list", ")", "*", "100", "\n", "print", "(", "\"all data Performance %.2f\"", "%", "new_F", ")", "\n", "selected_words", ",", "selected_tags", "=", "[", "]", ",", "[", "]", "\n", "\n", "if", "config", ".", "score_top_ratio", ">", "0", ":", "\n", "\t\t", "selected_gt", ",", "selected_pred", "=", "[", "]", ",", "[", "]", "\n", "rank_list", "=", "[", "(", "w", ",", "t", ",", "gt", ",", "s", ")", "for", "(", "w", ",", "t", ",", "gt", ",", "s", ")", "in", "zip", "(", "input_words", ",", "preds_list", ",", "out_label_list", ",", "scores", ")", "]", "\n", "sorted_rank_list", "=", "sorted", "(", "rank_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "3", "]", ",", "reverse", "=", "True", ")", "\n", "select_num", "=", "int", "(", "len", "(", "sorted_rank_list", ")", "*", "config", ".", "score_top_ratio", ")", "\n", "for", "input_word", ",", "tag", ",", "gt_tag", ",", "_", "in", "sorted_rank_list", "[", ":", "select_num", "]", ":", "\n", "\t\t\t", "selected_words", ".", "append", "(", "input_word", ")", "\n", "selected_tags", ".", "append", "(", "tag", ")", "\n", "selected_pred", ".", "append", "(", "tag", ")", "\n", "selected_gt", ".", "append", "(", "gt_tag", ")", "\n", "", "F", "=", "f1_score", "(", "selected_gt", ",", "selected_pred", ")", "*", "100", "\n", "print", "(", "\"Selected Performance %.2f\"", "%", "F", ")", "\n", "", "else", ":", "\n", "\t\t", "for", "input_word", ",", "gt_label", ",", "pred_label", "in", "zip", "(", "input_words", ",", "out_label_list", ",", "preds_list", ")", ":", "\n", "\t\t\t", "if", "config", ".", "enable_consistency_filtering", ":", "\n", "\t\t\t\t", "if", "len", "(", "input_word", ")", ">", "config", ".", "filter_by_min_length", "and", "' '", ".", "join", "(", "gt_label", ")", "==", "' '", ".", "join", "(", "pred_label", ")", ":", "\n", "\t\t\t\t\t", "selected_words", ".", "append", "(", "input_word", ")", "\n", "selected_tags", ".", "append", "(", "pred_label", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "selected_words", ".", "append", "(", "input_word", ")", "\n", "selected_tags", ".", "append", "(", "pred_label", ")", "\n", "\n", "", "", "", "if", "output_path", "is", "not", "None", ":", "\n", "\t\t", "with", "open", "(", "output_path", ",", "'w'", ")", "as", "out", ":", "\n", "\t\t\t", "for", "gen", ",", "labels", "in", "zip", "(", "selected_words", ",", "selected_tags", ")", ":", "\n", "\t\t\t\t", "for", "g", ",", "l", "in", "zip", "(", "gen", ",", "labels", ")", ":", "\n", "\t\t\t\t\t", "out", ".", "write", "(", "\"%s %s\\n\"", "%", "(", "g", ",", "l", ")", ")", "\n", "", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "", "return", "new_F", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.__init__": [[8, 37], ["T5PreTrainedModel.__init__", "nn.ModuleList", "T5LayerNorm", "nn.Dropout", "t5_model.T5Stack.init_weights", "T5Block", "nn.Embedding", "nn.Embedding", "range", "bool"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "embed_tokens", "=", "None", ",", "prefix_length", "=", "0", ",", "prefix_set_number", "=", "0", ",", "enable_layer_wise_prefix", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "enable_layer_wise_prefix", "=", "enable_layer_wise_prefix", "\n", "\n", "self", ".", "block", "=", "nn", ".", "ModuleList", "(", "\n", "[", "T5Block", "(", "config", ",", "has_relative_attention_bias", "=", "bool", "(", "i", "==", "0", ")", ")", "for", "i", "in", "range", "(", "config", ".", "num_layers", ")", "]", "\n", ")", "\n", "self", ".", "final_layer_norm", "=", "T5LayerNorm", "(", "config", ".", "d_model", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout_rate", ")", "\n", "self", ".", "use_input_prefix_embeds", "=", "prefix_length", ">", "0", "and", "prefix_set_number", ">", "0", "\n", "\n", "if", "self", ".", "use_input_prefix_embeds", "and", "not", "self", ".", "is_decoder", ":", "\n", "            ", "if", "self", ".", "enable_layer_wise_prefix", ":", "\n", "                ", "self", ".", "prefix_embedding", "=", "nn", ".", "Embedding", "(", "prefix_set_number", ",", "prefix_length", "*", "config", ".", "d_model", "*", "config", ".", "num_layers", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "prefix_embedding", "=", "nn", ".", "Embedding", "(", "prefix_set_number", ",", "prefix_length", "*", "config", ".", "d_model", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "prefix_embedding", "=", "None", "\n", "\n", "", "self", ".", "prefix_length", "=", "prefix_length", "\n", "self", ".", "prefix_set_number", "=", "prefix_set_number", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "# Model parallel", "\n", "self", ".", "model_parallel", "=", "False", "\n", "self", ".", "device_map", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.update_prefix_embedding": [[38, 43], ["nn.Embedding", "range", "t5_model.T5Stack.prefix_embedding.weight.data[].copy_", "t5_model.T5Stack.prefix_embedding.weight.detach"], "methods", ["None"], ["", "def", "update_prefix_embedding", "(", "self", ",", "prefix_set_number", ")", ":", "\n", "        ", "pre_trained_embedding", "=", "self", ".", "prefix_embedding", ".", "weight", ".", "detach", "(", ")", ".", "data", "[", "0", "]", "\n", "self", ".", "prefix_embedding", "=", "nn", ".", "Embedding", "(", "prefix_set_number", ",", "self", ".", "prefix_length", "*", "self", ".", "config", ".", "d_model", "*", "self", ".", "config", ".", "num_layers", ")", "\n", "for", "i", "in", "range", "(", "prefix_set_number", ")", ":", "\n", "            ", "self", ".", "prefix_embedding", ".", "weight", ".", "data", "[", "i", "]", ".", "copy_", "(", "pre_trained_embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.parallelize": [[44, 65], ["assert_device_map", "t5_model.T5Stack.device_map.items", "t5_model.T5Stack.embed_tokens.to", "t5_model.T5Stack.final_layer_norm.to", "get_device_map", "len", "str", "t5_model.T5Stack.prefix_embedding.to", "len", "range", "t5_model.T5Stack.device_map.keys", "str", "max", "t5_model.T5Stack.block[].to", "torch.cuda.device_count", "min", "t5_model.T5Stack.device_map.keys", "str", "t5_model.T5Stack.device_map.keys"], "methods", ["None"], ["", "", "def", "parallelize", "(", "self", ",", "device_map", "=", "None", ")", ":", "\n", "# Check validity of device_map", "\n", "        ", "self", ".", "device_map", "=", "(", "\n", "get_device_map", "(", "len", "(", "self", ".", "block", ")", ",", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ")", "if", "device_map", "is", "None", "else", "device_map", "\n", ")", "\n", "assert_device_map", "(", "self", ".", "device_map", ",", "len", "(", "self", ".", "block", ")", ")", "\n", "self", ".", "model_parallel", "=", "True", "\n", "self", ".", "first_device", "=", "\"cpu\"", "if", "\"cpu\"", "in", "self", ".", "device_map", ".", "keys", "(", ")", "else", "\"cuda:\"", "+", "str", "(", "min", "(", "self", ".", "device_map", ".", "keys", "(", ")", ")", ")", "\n", "self", ".", "last_device", "=", "\"cuda:\"", "+", "str", "(", "max", "(", "self", ".", "device_map", ".", "keys", "(", ")", ")", ")", "\n", "# Load onto devices", "\n", "for", "k", ",", "v", "in", "self", ".", "device_map", ".", "items", "(", ")", ":", "\n", "            ", "for", "layer", "in", "v", ":", "\n", "                ", "cuda_device", "=", "\"cuda:\"", "+", "str", "(", "k", ")", "\n", "self", ".", "block", "[", "layer", "]", "=", "self", ".", "block", "[", "layer", "]", ".", "to", "(", "cuda_device", ")", "\n", "\n", "# Set embed_tokens to first layer", "\n", "", "", "self", ".", "embed_tokens", "=", "self", ".", "embed_tokens", ".", "to", "(", "self", ".", "first_device", ")", "\n", "if", "self", ".", "prefix_embedding", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix_embedding", "=", "self", ".", "prefix_embedding", ".", "to", "(", "self", ".", "first_device", ")", "\n", "# Set final layer norm to last device", "\n", "", "self", ".", "final_layer_norm", "=", "self", ".", "final_layer_norm", ".", "to", "(", "self", ".", "last_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.deparallelize": [[66, 78], ["range", "t5_model.T5Stack.embed_tokens.to", "t5_model.T5Stack.final_layer_norm.to", "torch.cuda.empty_cache", "len", "t5_model.T5Stack.block[].to", "t5_model.T5Stack.prefix_embedding.to"], "methods", ["None"], ["", "def", "deparallelize", "(", "self", ")", ":", "\n", "        ", "self", ".", "model_parallel", "=", "False", "\n", "self", ".", "device_map", "=", "None", "\n", "self", ".", "first_device", "=", "\"cpu\"", "\n", "self", ".", "last_device", "=", "\"cpu\"", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "block", ")", ")", ":", "\n", "            ", "self", ".", "block", "[", "i", "]", "=", "self", ".", "block", "[", "i", "]", ".", "to", "(", "\"cpu\"", ")", "\n", "", "self", ".", "embed_tokens", "=", "self", ".", "embed_tokens", ".", "to", "(", "\"cpu\"", ")", "\n", "self", ".", "final_layer_norm", "=", "self", ".", "final_layer_norm", ".", "to", "(", "\"cpu\"", ")", "\n", "if", "self", ".", "prefix_embedding", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix_embedding", "=", "self", ".", "prefix_embedding", ".", "to", "(", "\"cpu\"", ")", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.get_input_embeddings": [[79, 81], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.set_input_embeddings": [[82, 84], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "embed_tokens", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5Stack.forward": [[85, 319], ["t5_model.T5Stack.get_extended_attention_mask", "t5_model.T5Stack.get_head_mask", "t5_model.T5Stack.get_head_mask", "t5_model.T5Stack.dropout", "enumerate", "t5_model.T5Stack.final_layer_norm", "t5_model.T5Stack.dropout", "BaseModelOutputWithPastAndCrossAttentions", "torch.cuda.set_device", "t5_model.T5Stack.embed_tokens.to", "ValueError", "t5_model.T5Stack.embed_tokens", "torch.ones().to", "torch.ones", "torch.ones().to", "torch.cat", "torch.ones().to", "torch.cat", "encoder_hidden_states.to.to.size", "t5_model.T5Stack.invert_attention_mask", "zip", "tuple", "input_ids.view.view.size", "input_ids.view.view.view", "len", "torch.zeros().long().to", "t5_model.T5Stack.prefix_embedding().view", "torch.cat", "t5_model.T5Stack.prefix_embedding().view", "torch.cat", "torch.ones", "torch.split", "prefix_embs[].to", "torch.cat", "torch.cuda.set_device", "getattr", "checkpoint", "layer_module", "t5_model.T5Stack.device_map.items", "ValueError", "torch.ones", "torch.cat.size", "torch.cat.size", "torch.ones", "torch.ones", "attention_mask.to.to.to", "position_bias.to.to.to", "encoder_hidden_states.to.to.to", "encoder_extended_attention_mask.to.to.to", "encoder_decoder_position_bias.to.to.to", "layer_head_mask.to.to.to", "cross_attn_layer_head_mask.to.to.to", "logger.warn", "t5_model.T5Stack.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "task_ids", "=", "None", ",", "\n", ")", ":", "\n", "# Model parallel", "\n", "        ", "if", "self", ".", "model_parallel", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_device", "(", "self", ".", "first_device", ")", "\n", "self", ".", "embed_tokens", "=", "self", ".", "embed_tokens", ".", "to", "(", "self", ".", "first_device", ")", "\n", "", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "err_msg_prefix", "=", "\"decoder_\"", "if", "self", ".", "is_decoder", "else", "\"\"", "\n", "raise", "ValueError", "(", "\n", "f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"", "\n", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "err_msg_prefix", "=", "\"decoder_\"", "if", "self", ".", "is_decoder", "else", "\"\"", "\n", "raise", "ValueError", "(", "f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "assert", "self", ".", "embed_tokens", "is", "not", "None", ",", "\"You have to initialize the model with valid token embeddings\"", "\n", "inputs_embeds", "=", "self", ".", "embed_tokens", "(", "input_ids", ")", "\n", "\n", "", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "\n", "# required mask seq length can be calculated via length of past", "\n", "mask_seq_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "+", "seq_length", "if", "past_key_values", "is", "not", "None", "else", "seq_length", "\n", "\n", "if", "use_cache", "is", "True", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\":obj:`use_cache` can only be set to `True` if {self} is used as a decoder\"", "\n", "\n", "", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "mask_seq_length", ")", ".", "to", "(", "inputs_embeds", ".", "device", ")", "\n", "", "if", "self", ".", "is_decoder", "and", "encoder_attention_mask", "is", "None", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_seq_length", "=", "encoder_hidden_states", ".", "shape", "[", "1", "]", "\n", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "\n", "batch_size", ",", "encoder_seq_length", ",", "device", "=", "inputs_embeds", ".", "device", ",", "dtype", "=", "torch", ".", "long", "\n", ")", "\n", "\n", "# initialize past_key_values with `None` if past does not exist", "\n", "", "if", "past_key_values", "is", "None", ":", "\n", "            ", "past_key_values", "=", "[", "None", "]", "*", "len", "(", "self", ".", "block", ")", "\n", "\n", "", "if", "self", ".", "use_input_prefix_embeds", "and", "not", "self", ".", "is_decoder", ":", "\n", "            ", "if", "task_ids", "is", "None", ":", "\n", "                ", "task_ids", "=", "torch", ".", "zeros", "(", "(", "inputs_embeds", ".", "size", "(", "0", ")", ",", ")", ")", ".", "long", "(", ")", ".", "to", "(", "inputs_embeds", ".", "device", ")", "\n", "\n", "", "if", "self", ".", "enable_layer_wise_prefix", ":", "\n", "                ", "prefix_embs", "=", "self", ".", "prefix_embedding", "(", "task_ids", ")", ".", "view", "(", "inputs_embeds", ".", "size", "(", "0", ")", ",", "self", ".", "config", ".", "num_layers", ",", "-", "1", ",", "self", ".", "config", ".", "d_model", ")", "\n", "inputs_embeds", "=", "torch", ".", "cat", "(", "[", "prefix_embs", "[", ":", ",", "0", "]", ",", "inputs_embeds", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "prefix_embs", "=", "self", ".", "prefix_embedding", "(", "task_ids", ")", ".", "view", "(", "inputs_embeds", ".", "size", "(", "0", ")", ",", "-", "1", ",", "self", ".", "config", ".", "d_model", ")", "\n", "inputs_embeds", "=", "torch", ".", "cat", "(", "[", "prefix_embs", ",", "inputs_embeds", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "prefix_mask", "=", "torch", ".", "ones", "(", "(", "attention_mask", ".", "size", "(", "0", ")", ",", "self", ".", "prefix_length", ")", ",", "dtype", "=", "attention_mask", ".", "dtype", ")", ".", "to", "(", "inputs_embeds", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "[", "prefix_mask", ",", "attention_mask", "]", ",", "axis", "=", "1", ")", "\n", "input_shape", "=", "(", "batch_size", ",", "seq_length", "+", "self", ".", "prefix_length", ")", "\n", "\n", "", "if", "self", ".", "use_input_prefix_embeds", "and", "self", ".", "is_decoder", ":", "\n", "            ", "prefix_mask", "=", "torch", ".", "ones", "(", "(", "attention_mask", ".", "size", "(", "0", ")", ",", "self", ".", "prefix_length", ")", ",", "dtype", "=", "encoder_attention_mask", ".", "dtype", ")", ".", "to", "(", "encoder_attention_mask", ".", "device", ")", "\n", "encoder_attention_mask", "=", "torch", ".", "cat", "(", "[", "prefix_mask", ",", "encoder_attention_mask", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "inputs_embeds", ".", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "inputs_embeds", ".", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_layers", ")", "\n", "cross_attn_head_mask", "=", "self", ".", "get_head_mask", "(", "cross_attn_head_mask", ",", "self", ".", "config", ".", "num_layers", ")", "\n", "present_key_value_states", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "(", "output_attentions", "and", "self", ".", "is_decoder", ")", "else", "None", "\n", "position_bias", "=", "None", "\n", "encoder_decoder_position_bias", "=", "None", "\n", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "inputs_embeds", ")", "\n", "\n", "for", "i", ",", "(", "layer_module", ",", "past_key_value", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "block", ",", "past_key_values", ")", ")", ":", "\n", "            ", "layer_head_mask", "=", "head_mask", "[", "i", "]", "\n", "cross_attn_layer_head_mask", "=", "cross_attn_head_mask", "[", "i", "]", "\n", "\n", "if", "i", ">", "0", "and", "self", ".", "use_input_prefix_embeds", "and", "not", "self", ".", "is_decoder", "and", "self", ".", "enable_layer_wise_prefix", ":", "\n", "                ", "prefix", ",", "real_hidden_states", "=", "torch", ".", "split", "(", "hidden_states", ",", "[", "self", ".", "prefix_length", ",", "hidden_states", ".", "size", "(", "1", ")", "-", "self", ".", "prefix_length", "]", ",", "dim", "=", "1", ")", "\n", "layer_wise_prefix_embs", "=", "prefix_embs", "[", ":", ",", "i", "]", ".", "to", "(", "real_hidden_states", ".", "device", ")", "\n", "hidden_states", "=", "torch", ".", "cat", "(", "[", "layer_wise_prefix_embs", ",", "real_hidden_states", "]", ",", "dim", "=", "1", ")", "\n", "# Model parallel", "\n", "", "if", "self", ".", "model_parallel", ":", "\n", "                ", "torch", ".", "cuda", ".", "set_device", "(", "hidden_states", ".", "device", ")", "\n", "# Ensure that attention_mask is always on the same device as hidden_states", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                    ", "attention_mask", "=", "attention_mask", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "if", "position_bias", "is", "not", "None", ":", "\n", "                    ", "position_bias", "=", "position_bias", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "if", "encoder_hidden_states", "is", "not", "None", ":", "\n", "                    ", "encoder_hidden_states", "=", "encoder_hidden_states", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "if", "encoder_extended_attention_mask", "is", "not", "None", ":", "\n", "                    ", "encoder_extended_attention_mask", "=", "encoder_extended_attention_mask", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "if", "encoder_decoder_position_bias", "is", "not", "None", ":", "\n", "                    ", "encoder_decoder_position_bias", "=", "encoder_decoder_position_bias", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "if", "layer_head_mask", "is", "not", "None", ":", "\n", "                    ", "layer_head_mask", "=", "layer_head_mask", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "if", "cross_attn_layer_head_mask", "is", "not", "None", ":", "\n", "                    ", "cross_attn_layer_head_mask", "=", "cross_attn_layer_head_mask", ".", "to", "(", "hidden_states", ".", "device", ")", "\n", "", "", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warn", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "tuple", "(", "module", "(", "*", "inputs", ",", "use_cache", ",", "output_attentions", ")", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "extended_attention_mask", ",", "\n", "position_bias", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_extended_attention_mask", ",", "\n", "encoder_decoder_position_bias", ",", "\n", "layer_head_mask", ",", "\n", "cross_attn_layer_head_mask", ",", "\n", "None", ",", "# past_key_value is always None with gradient checkpointing", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "position_bias", "=", "position_bias", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "encoder_decoder_position_bias", "=", "encoder_decoder_position_bias", ",", "\n", "layer_head_mask", "=", "layer_head_mask", ",", "\n", "cross_attn_layer_head_mask", "=", "cross_attn_layer_head_mask", ",", "\n", "past_key_value", "=", "past_key_value", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "\n", "# layer_outputs is a tuple with:", "\n", "# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)", "\n", "", "if", "use_cache", "is", "False", ":", "\n", "                ", "layer_outputs", "=", "layer_outputs", "[", ":", "1", "]", "+", "(", "None", ",", ")", "+", "layer_outputs", "[", "1", ":", "]", "\n", "\n", "", "hidden_states", ",", "present_key_value_state", "=", "layer_outputs", "[", ":", "2", "]", "\n", "\n", "# We share the position biases between the layers - the first layer store them", "\n", "# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),", "\n", "# (cross-attention position bias), (cross-attention weights)", "\n", "position_bias", "=", "layer_outputs", "[", "2", "]", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "                ", "encoder_decoder_position_bias", "=", "layer_outputs", "[", "4", "if", "output_attentions", "else", "3", "]", "\n", "# append next layer key value states", "\n", "", "if", "use_cache", ":", "\n", "                ", "present_key_value_states", "=", "present_key_value_states", "+", "(", "present_key_value_state", ",", ")", "\n", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "3", "]", ",", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "5", "]", ",", ")", "\n", "\n", "# Model Parallel: If it's the last layer for that device, put things on the next device", "\n", "", "", "if", "self", ".", "model_parallel", ":", "\n", "                ", "for", "k", ",", "v", "in", "self", ".", "device_map", ".", "items", "(", ")", ":", "\n", "                    ", "if", "i", "==", "v", "[", "-", "1", "]", "and", "\"cuda:\"", "+", "str", "(", "k", ")", "!=", "self", ".", "last_device", ":", "\n", "                        ", "hidden_states", "=", "hidden_states", ".", "to", "(", "\"cuda:\"", "+", "str", "(", "k", "+", "1", ")", ")", "\n", "\n", "", "", "", "", "hidden_states", "=", "self", ".", "final_layer_norm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "\n", "# Add last layer", "\n", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "present_key_value_states", ",", "\n", "all_hidden_states", ",", "\n", "all_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "present_key_value_states", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.__init__": [[331, 357], ["T5PreTrainedModel.__init__", "nn.Embedding", "copy.deepcopy", "t5_model.T5Stack", "copy.deepcopy", "t5_model.T5Stack", "nn.Linear", "t5_model.T5ForPT.init_weights"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "**", "model_args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_dim", "=", "config", ".", "d_model", "\n", "\n", "self", ".", "shared", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "d_model", ")", "\n", "self", ".", "local_config", "=", "model_args", "[", "'local_config'", "]", "\n", "\n", "encoder_config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "encoder_config", ".", "is_decoder", "=", "False", "\n", "encoder_config", ".", "use_cache", "=", "False", "\n", "encoder_config", ".", "is_encoder_decoder", "=", "False", "\n", "self", ".", "encoder", "=", "T5Stack", "(", "encoder_config", ",", "self", ".", "shared", ",", "prefix_length", "=", "self", ".", "local_config", ".", "prefix_length", ",", "prefix_set_number", "=", "self", ".", "local_config", ".", "prefix_set_number", ",", "enable_layer_wise_prefix", "=", "self", ".", "local_config", ".", "enable_layer_wise_prefix", ")", "\n", "\n", "decoder_config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "decoder_config", ".", "is_decoder", "=", "True", "\n", "decoder_config", ".", "is_encoder_decoder", "=", "False", "\n", "decoder_config", ".", "num_layers", "=", "config", ".", "num_decoder_layers", "\n", "self", ".", "decoder", "=", "T5Stack", "(", "decoder_config", ",", "self", ".", "shared", ",", "prefix_length", "=", "self", ".", "local_config", ".", "prefix_length", ",", "prefix_set_number", "=", "self", ".", "local_config", ".", "prefix_set_number", ",", "enable_layer_wise_prefix", "=", "self", ".", "local_config", ".", "enable_layer_wise_prefix", ")", "\n", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "# Model parallel", "\n", "self", ".", "model_parallel", "=", "False", "\n", "self", ".", "device_map", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.update_prefix_embedding": [[358, 360], ["t5_model.T5ForPT.encoder.update_prefix_embedding"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.update_prefix_embedding"], ["", "def", "update_prefix_embedding", "(", "self", ",", "prefix_set_number", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "update_prefix_embedding", "(", "prefix_set_number", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.parallelize": [[361, 372], ["assert_device_map", "t5_model.T5ForPT.encoder.parallelize", "t5_model.T5ForPT.decoder.parallelize", "t5_model.T5ForPT.lm_head.to", "get_device_map", "len", "len", "range", "torch.cuda.device_count"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.parallelize", "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.parallelize"], ["", "def", "parallelize", "(", "self", ",", "device_map", "=", "None", ")", ":", "\n", "        ", "self", ".", "device_map", "=", "(", "\n", "get_device_map", "(", "len", "(", "self", ".", "encoder", ".", "block", ")", ",", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ")", "\n", "if", "device_map", "is", "None", "\n", "else", "device_map", "\n", ")", "\n", "assert_device_map", "(", "self", ".", "device_map", ",", "len", "(", "self", ".", "encoder", ".", "block", ")", ")", "\n", "self", ".", "encoder", ".", "parallelize", "(", "self", ".", "device_map", ")", "\n", "self", ".", "decoder", ".", "parallelize", "(", "self", ".", "device_map", ")", "\n", "self", ".", "lm_head", "=", "self", ".", "lm_head", ".", "to", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "self", ".", "model_parallel", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.deparallelize": [[374, 383], ["t5_model.T5ForPT.encoder.deparallelize", "t5_model.T5ForPT.decoder.deparallelize", "t5_model.T5ForPT.encoder.to", "t5_model.T5ForPT.decoder.to", "t5_model.T5ForPT.lm_head.to", "torch.cuda.empty_cache"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.deparallelize", "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.deparallelize"], ["", "def", "deparallelize", "(", "self", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "deparallelize", "(", ")", "\n", "self", ".", "decoder", ".", "deparallelize", "(", ")", "\n", "self", ".", "encoder", "=", "self", ".", "encoder", ".", "to", "(", "\"cpu\"", ")", "\n", "self", ".", "decoder", "=", "self", ".", "decoder", ".", "to", "(", "\"cpu\"", ")", "\n", "self", ".", "lm_head", "=", "self", ".", "lm_head", ".", "to", "(", "\"cpu\"", ")", "\n", "self", ".", "model_parallel", "=", "False", "\n", "self", ".", "device_map", "=", "None", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.get_input_embeddings": [[385, 387], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "shared", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.set_input_embeddings": [[388, 392], ["t5_model.T5ForPT.encoder.set_input_embeddings", "t5_model.T5ForPT.decoder.set_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.set_input_embeddings", "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.set_input_embeddings"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "shared", "=", "new_embeddings", "\n", "self", ".", "encoder", ".", "set_input_embeddings", "(", "new_embeddings", ")", "\n", "self", ".", "decoder", ".", "set_input_embeddings", "(", "new_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.set_output_embeddings": [[393, 395], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "lm_head", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.get_output_embeddings": [[396, 398], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.get_encoder": [[399, 401], ["None"], "methods", ["None"], ["", "def", "get_encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.get_decoder": [[402, 404], ["None"], "methods", ["None"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.forward": [[405, 558], ["t5_model.T5ForPT.decoder", "t5_model.T5ForPT.lm_head", "Seq2SeqLMOutput", "t5_model.T5ForPT.encoder", "torch.cuda.set_device", "t5_model.T5ForPT._shift_right", "torch.cuda.set_device", "hidden_states.to.to.to", "torch.cuda.set_device", "t5_model.T5ForPT.lm_head.to", "sequence_output.to.to.to", "CrossEntropyLoss", "CrossEntropyLoss.", "warnings.warn", "BaseModelOutput", "decoder_input_ids.to.to.to", "attention_mask.to.to.to", "decoder_attention_mask.to.to.to", "t5_model.T5ForPT.view", "labels.view", "isinstance", "t5_model.T5ForPT.size", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "task_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_input_ids", "=", "None", ",", "\n", "decoder_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "decoder_head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "decoder_inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[-100, 0, ...,\n            config.vocab_size - 1]`. All labels set to ``-100`` are ignored (masked), the loss is only computed for\n            labels in ``[0, ..., config.vocab_size]``\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n            >>> model = T5ForConditionalGeneration.from_pretrained('t5-small')\n\n            >>> input_ids = tokenizer('The <extra_id_0> walks in <extra_id_1> park', return_tensors='pt').input_ids\n            >>> labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the <extra_id_2> </s>', return_tensors='pt').input_ids\n            >>> outputs = model(input_ids=input_ids, labels=labels)\n            >>> loss = outputs.loss\n            >>> logits = outputs.logits\n\n            >>> input_ids = tokenizer(\"summarize: studies have shown that owning a dog is good for you \", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> outputs = model.generate(input_ids)\n        \"\"\"", "\n", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "# FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask", "\n", "if", "head_mask", "is", "not", "None", "and", "decoder_head_mask", "is", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "num_layers", "==", "self", ".", "config", ".", "num_decoder_layers", ":", "\n", "                ", "warnings", ".", "warn", "(", "__HEAD_MASK_WARNING_MSG", ",", "FutureWarning", ")", "\n", "decoder_head_mask", "=", "head_mask", "\n", "\n", "# Encode if needed (training, first prediction pass)", "\n", "", "", "if", "encoder_outputs", "is", "None", ":", "\n", "# Convert encoder inputs in embeddings if needed", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "task_ids", "=", "task_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "", "elif", "return_dict", "and", "not", "isinstance", "(", "encoder_outputs", ",", "BaseModelOutput", ")", ":", "\n", "            ", "encoder_outputs", "=", "BaseModelOutput", "(", "\n", "last_hidden_state", "=", "encoder_outputs", "[", "0", "]", ",", "\n", "hidden_states", "=", "encoder_outputs", "[", "1", "]", "if", "len", "(", "encoder_outputs", ")", ">", "1", "else", "None", ",", "\n", "attentions", "=", "encoder_outputs", "[", "2", "]", "if", "len", "(", "encoder_outputs", ")", ">", "2", "else", "None", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "model_parallel", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_device", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "\n", "", "if", "labels", "is", "not", "None", "and", "decoder_input_ids", "is", "None", "and", "decoder_inputs_embeds", "is", "None", ":", "\n", "# get decoder inputs from shifting lm labels to the right", "\n", "            ", "decoder_input_ids", "=", "self", ".", "_shift_right", "(", "labels", ")", "\n", "\n", "# If decoding with past key value states, only the last tokens", "\n", "# should be given as an input", "\n", "", "if", "past_key_values", "is", "not", "None", ":", "\n", "            ", "assert", "labels", "is", "None", ",", "\"Decoder should not use cached key value states when training.\"", "\n", "if", "decoder_input_ids", "is", "not", "None", ":", "\n", "                ", "decoder_input_ids", "=", "decoder_input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "", "if", "decoder_inputs_embeds", "is", "not", "None", ":", "\n", "                ", "decoder_inputs_embeds", "=", "decoder_inputs_embeds", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# Set device for model parallelism", "\n", "", "", "if", "self", ".", "model_parallel", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_device", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "hidden_states", "=", "hidden_states", ".", "to", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "if", "decoder_input_ids", "is", "not", "None", ":", "\n", "                ", "decoder_input_ids", "=", "decoder_input_ids", ".", "to", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "attention_mask", "=", "attention_mask", ".", "to", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "", "if", "decoder_attention_mask", "is", "not", "None", ":", "\n", "                ", "decoder_attention_mask", "=", "decoder_attention_mask", ".", "to", "(", "self", ".", "decoder", ".", "first_device", ")", "\n", "\n", "# Decode", "\n", "", "", "decoder_outputs", "=", "self", ".", "decoder", "(", "\n", "input_ids", "=", "decoder_input_ids", ",", "\n", "attention_mask", "=", "decoder_attention_mask", ",", "\n", "inputs_embeds", "=", "decoder_inputs_embeds", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "encoder_hidden_states", "=", "hidden_states", ",", "\n", "encoder_attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "decoder_head_mask", ",", "\n", "cross_attn_head_mask", "=", "cross_attn_head_mask", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "decoder_outputs", "[", "0", "]", "\n", "\n", "# Set device for model parallelism", "\n", "if", "self", ".", "model_parallel", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_device", "(", "self", ".", "encoder", ".", "first_device", ")", "\n", "self", ".", "lm_head", "=", "self", ".", "lm_head", ".", "to", "(", "self", ".", "encoder", ".", "first_device", ")", "\n", "sequence_output", "=", "sequence_output", ".", "to", "(", "self", ".", "lm_head", ".", "weight", ".", "device", ")", "\n", "\n", "", "if", "self", ".", "config", ".", "tie_word_embeddings", ":", "\n", "# Rescale output before projecting on vocab", "\n", "# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586", "\n", "            ", "sequence_output", "=", "sequence_output", "*", "(", "self", ".", "model_dim", "**", "-", "0.5", ")", "\n", "\n", "", "lm_logits", "=", "self", ".", "lm_head", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "100", ")", "\n", "loss", "=", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "lm_logits", ".", "size", "(", "-", "1", ")", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "# TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "lm_logits", ",", ")", "+", "decoder_outputs", "[", "1", ":", "]", "+", "encoder_outputs", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "Seq2SeqLMOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "lm_logits", ",", "\n", "past_key_values", "=", "decoder_outputs", ".", "past_key_values", ",", "\n", "decoder_hidden_states", "=", "decoder_outputs", ".", "hidden_states", ",", "\n", "decoder_attentions", "=", "decoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "decoder_outputs", ".", "cross_attentions", ",", "\n", "encoder_last_hidden_state", "=", "encoder_outputs", ".", "last_hidden_state", ",", "\n", "encoder_hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "encoder_attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.prepare_inputs_for_generation": [[561, 593], ["None"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "decoder_head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "\n", "# cut decoder_input_ids if past is used", "\n", "        ", "if", "past", "is", "not", "None", ":", "\n", "            ", "input_ids", "=", "input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "", "return_dict", "=", "{", "\n", "\"decoder_input_ids\"", ":", "input_ids", ",", "\n", "\"past_key_values\"", ":", "past", ",", "\n", "\"encoder_outputs\"", ":", "encoder_outputs", ",", "\n", "\"attention_mask\"", ":", "attention_mask", ",", "\n", "\"head_mask\"", ":", "head_mask", ",", "\n", "\"decoder_head_mask\"", ":", "decoder_head_mask", ",", "\n", "\"cross_attn_head_mask\"", ":", "cross_attn_head_mask", ",", "\n", "\"use_cache\"", ":", "use_cache", ",", "\n", "}", "\n", "\n", "if", "'task_ids'", "in", "kwargs", ":", "\n", "            ", "return_dict", "[", "'task_ids'", "]", "=", "kwargs", "[", "'task_ids'", "]", "\n", "\n", "", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT.prepare_decoder_input_ids_from_labels": [[594, 596], ["t5_model.T5ForPT._shift_right"], "methods", ["None"], ["", "def", "prepare_decoder_input_ids_from_labels", "(", "self", ",", "labels", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "self", ".", "_shift_right", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT._reorder_cache": [[597, 620], ["logger.warning", "len", "len", "layer_past_state.index_select", "beam_idx.to"], "methods", ["None"], ["", "def", "_reorder_cache", "(", "self", ",", "past", ",", "beam_idx", ")", ":", "\n", "# if decoder past is not included in output", "\n", "# speedy decoding is disabled and no need to reorder", "\n", "        ", "if", "past", "is", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\"You might want to consider setting `use_cache=True` to speed up decoding\"", ")", "\n", "return", "past", "\n", "\n", "", "reordered_decoder_past", "=", "(", ")", "\n", "for", "layer_past_states", "in", "past", ":", "\n", "# get the correct batch idx from layer past batch dim", "\n", "# batch dim of `past` is at 2nd position", "\n", "            ", "reordered_layer_past_states", "=", "(", ")", "\n", "for", "layer_past_state", "in", "layer_past_states", ":", "\n", "# need to set correct `past` for each of the four key / value states", "\n", "                ", "reordered_layer_past_states", "=", "reordered_layer_past_states", "+", "(", "\n", "layer_past_state", ".", "index_select", "(", "0", ",", "beam_idx", ".", "to", "(", "layer_past_state", ".", "device", ")", ")", ",", "\n", ")", "\n", "\n", "", "assert", "reordered_layer_past_states", "[", "0", "]", ".", "shape", "==", "layer_past_states", "[", "0", "]", ".", "shape", "\n", "assert", "len", "(", "reordered_layer_past_states", ")", "==", "len", "(", "layer_past_states", ")", "\n", "\n", "reordered_decoder_past", "=", "reordered_decoder_past", "+", "(", "reordered_layer_past_states", ",", ")", "\n", "", "return", "reordered_decoder_past", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.T5ForPT._expand_inputs_for_generation": [[621, 652], ["torch.arange().view().repeat().view().to", "input_ids.index_select.index_select.index_select", "token_type_ids.index_select", "attention_mask.index_select", "model_kwargs[].index_select", "encoder_outputs.last_hidden_state.index_select", "torch.arange().view().repeat().view", "torch.arange().view().repeat().view().to.to", "torch.arange().view().repeat", "torch.arange().view", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_expand_inputs_for_generation", "(", "\n", "input_ids", ":", "torch", ".", "LongTensor", ",", "\n", "expand_size", ":", "int", "=", "1", ",", "\n", "is_encoder_decoder", ":", "bool", "=", "False", ",", "\n", "attention_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "encoder_outputs", ":", "ModelOutput", "=", "None", ",", "\n", "**", "model_kwargs", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "LongTensor", ",", "Dict", "[", "str", ",", "Any", "]", "]", ":", "\n", "        ", "expanded_return_idx", "=", "(", "\n", "torch", ".", "arange", "(", "input_ids", ".", "shape", "[", "0", "]", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "expand_size", ")", ".", "view", "(", "-", "1", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", ")", "\n", "input_ids", "=", "input_ids", ".", "index_select", "(", "0", ",", "expanded_return_idx", ")", "\n", "\n", "if", "\"token_type_ids\"", "in", "model_kwargs", ":", "\n", "            ", "token_type_ids", "=", "model_kwargs", "[", "\"token_type_ids\"", "]", "\n", "model_kwargs", "[", "\"token_type_ids\"", "]", "=", "token_type_ids", ".", "index_select", "(", "0", ",", "expanded_return_idx", ")", "\n", "\n", "", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "model_kwargs", "[", "\"attention_mask\"", "]", "=", "attention_mask", ".", "index_select", "(", "0", ",", "expanded_return_idx", ")", "\n", "\n", "", "if", "\"task_ids\"", "in", "model_kwargs", ":", "\n", "            ", "model_kwargs", "[", "\"task_ids\"", "]", "=", "model_kwargs", "[", "\"task_ids\"", "]", ".", "index_select", "(", "0", ",", "expanded_return_idx", ")", "\n", "\n", "", "if", "is_encoder_decoder", ":", "\n", "            ", "assert", "encoder_outputs", "is", "not", "None", "\n", "encoder_outputs", "[", "\"last_hidden_state\"", "]", "=", "encoder_outputs", ".", "last_hidden_state", ".", "index_select", "(", "\n", "0", ",", "expanded_return_idx", ".", "to", "(", "encoder_outputs", ".", "last_hidden_state", ".", "device", ")", "\n", ")", "\n", "model_kwargs", "[", "\"encoder_outputs\"", "]", "=", "encoder_outputs", "\n", "", "return", "input_ids", ",", "model_kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.get_t5_model": [[653, 664], ["transformers.T5Tokenizer.from_pretrained", "T5ForPT.from_pretrained", "T5ForPT.from_pretrained.parameters", "T5ForPT.from_pretrained.encoder.prefix_embedding.parameters"], "function", ["None"], ["", "", "def", "get_t5_model", "(", "config", ")", ":", "\n", "    ", "tokenizer", "=", "T5Tokenizer", ".", "from_pretrained", "(", "config", ".", "tokenizer_type", ")", "\n", "model", "=", "T5ForPT", ".", "from_pretrained", "(", "config", ".", "lm_type", ",", "local_config", "=", "config", ")", "\n", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "for", "p", "in", "model", ".", "encoder", ".", "prefix_embedding", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "True", "\n", "\n", "", "return", "tokenizer", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.get_full_finetune_t5_model": [[665, 674], ["transformers.T5Tokenizer.from_pretrained", "T5ForPT.from_pretrained", "config.lm_type.startswith", "T5ForPT.from_pretrained.shared.parameters"], "function", ["None"], ["", "def", "get_full_finetune_t5_model", "(", "config", ")", ":", "\n", "    ", "tokenizer", "=", "T5Tokenizer", ".", "from_pretrained", "(", "config", ".", "tokenizer_type", ")", "\n", "model", "=", "T5ForPT", ".", "from_pretrained", "(", "config", ".", "lm_type", ",", "local_config", "=", "config", ")", "\n", "\n", "if", "config", ".", "lm_type", ".", "startswith", "(", "'google/t5-v1.1'", ")", ":", "\n", "        ", "for", "p", "in", "model", ".", "shared", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "return", "tokenizer", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.get_full_pretrain_t5_model": [[675, 680], ["transformers.T5Tokenizer.from_pretrained", "T5ForPT.from_pretrained"], "function", ["None"], ["", "def", "get_full_pretrain_t5_model", "(", "config", ")", ":", "\n", "    ", "tokenizer", "=", "T5Tokenizer", ".", "from_pretrained", "(", "config", ".", "tokenizer_type", ")", "\n", "model", "=", "T5ForPT", ".", "from_pretrained", "(", "config", ".", "lm_type", ",", "local_config", "=", "config", ")", "\n", "\n", "return", "tokenizer", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.get_bert_model": [[681, 686], ["transformers.BertTokenizerFast.from_pretrained", "transformers.BertForTokenClassification.from_pretrained"], "function", ["None"], ["", "def", "get_bert_model", "(", "config", ",", "output_size", ")", ":", "\n", "    ", "tokenizer", "=", "BertTokenizerFast", ".", "from_pretrained", "(", "config", ".", "lm_type", ")", "\n", "model", "=", "BertForTokenClassification", ".", "from_pretrained", "(", "config", ".", "lm_type", ",", "num_labels", "=", "output_size", ",", "attention_probs_dropout_prob", "=", "0.3", ",", "hidden_dropout_prob", "=", "0.3", ")", "\n", "\n", "return", "tokenizer", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.t5_model.get_bert_sen_classification_model": [[687, 692], ["transformers.BertTokenizerFast.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained"], "function", ["None"], ["", "def", "get_bert_sen_classification_model", "(", "config", ",", "output_size", ")", ":", "\n", "    ", "tokenizer", "=", "BertTokenizerFast", ".", "from_pretrained", "(", "config", ".", "lm_type", ")", "\n", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "config", ".", "lm_type", ",", "num_labels", "=", "output_size", ")", "\n", "\n", "return", "tokenizer", ",", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering_sen_cls.read_sentence_classification": [[8, 20], ["open", "l.strip.strip", "l.strip.split", "token_docs.append", "tag_docs.append", "len"], "function", ["None"], ["def", "read_sentence_classification", "(", "file_path", ")", ":", "\n", "    ", "token_docs", "=", "[", "]", "\n", "tag_docs", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "2", ":", "continue", "\n", "token_docs", ".", "append", "(", "items", "[", "0", "]", ")", "\n", "tag_docs", ".", "append", "(", "items", "[", "1", "]", ")", "\n", "\n", "", "", "return", "token_docs", ",", "tag_docs", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.LM_data_filtering_sen_cls.read_pair_sentence_label_data": [[21, 32], ["open", "l.strip.strip", "l.strip.split", "token_docs.append", "output_labels.append", "len"], "function", ["None"], ["", "def", "read_pair_sentence_label_data", "(", "data_path", ")", ":", "\n", "    ", "token_docs", "=", "[", "]", "\n", "output_labels", "=", "[", "]", "\n", "with", "open", "(", "data_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "items", "=", "l", ".", "split", "(", "'\\t'", ")", "\n", "if", "not", "len", "(", "items", ")", "==", "3", ":", "continue", "\n", "token_docs", ".", "append", "(", "\"%s\\t%s\"", "%", "(", "items", "[", "0", "]", ",", "items", "[", "1", "]", ")", ")", "\n", "output_labels", ".", "append", "(", "items", "[", "2", "]", ")", "\n", "", "", "return", "token_docs", ",", "output_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.pre_train_t5.evaluation": [[14, 34], ["model.eval", "print", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "sum", "len", "model", "loss_list.append", "batch[].to", "loss.item"], "function", ["None"], ["def", "evaluation", "(", "_C", ",", "eval_data", ",", "model", ",", "device", ")", ":", "\n", "\t", "model", ".", "eval", "(", ")", "\n", "loss_list", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t", "for", "batch", "in", "tqdm", "(", "eval_data", ")", ":", "\n", "\t\t\t", "for", "n", "in", "batch", ":", "\n", "\t\t\t\t", "batch", "[", "n", "]", "=", "batch", "[", "n", "]", ".", "to", "(", "device", ")", "\n", "\n", "", "outputs", "=", "model", "(", "\n", "input_ids", "=", "batch", "[", "'encoder_input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'encoder_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'decoder_input_ids'", "]", ",", "\n", ")", "\n", "loss", "=", "outputs", ".", "loss", "\n", "\n", "loss_list", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "", "", "final_loss", "=", "sum", "(", "loss_list", ")", "/", "len", "(", "loss_list", ")", "\n", "print", "(", "\"EVAL LOSS %.2f\"", "%", "final_loss", ")", "\n", "return", "-", "1", "*", "final_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.__init__": [[6, 9], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "scheduler", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "scheduler", "=", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.__getattr__": [[10, 12], ["getattr"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "optimizer", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.step": [[13, 16], ["utils.AdamWOpt.optimizer.step", "utils.AdamWOpt.scheduler.step"], "methods", ["home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.step", "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.AdamWOpt.step"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "scheduler", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.build_t5_finetune_optimizer": [[17, 20], ["transformers.optimization.Adafactor", "model.parameters"], "function", ["None"], ["", "", "def", "build_t5_finetune_optimizer", "(", "opt", ",", "model", ")", ":", "\n", "\t", "optimizer", "=", "Adafactor", "(", "model", ".", "parameters", "(", ")", ",", "scale_parameter", "=", "False", ",", "relative_step", "=", "False", ",", "warmup_init", "=", "False", ",", "lr", "=", "1e-3", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.build_t5_pretraining_optimizer": [[21, 25], ["transformers.optimization.Adafactor", "transformers.optimization.AdafactorSchedule", "utils.AdamWOpt", "model.parameters"], "function", ["None"], ["", "def", "build_t5_pretraining_optimizer", "(", "opt", ",", "model", ")", ":", "\n", "\t", "optimizer", "=", "Adafactor", "(", "model", ".", "parameters", "(", ")", ",", "beta1", "=", "0", ",", "scale_parameter", "=", "True", ",", "relative_step", "=", "True", ",", "warmup_init", "=", "False", ",", "lr", "=", "None", ")", "\n", "lr_scheduler", "=", "AdafactorSchedule", "(", "optimizer", ")", "\n", "return", "AdamWOpt", "(", "optimizer", ",", "lr_scheduler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.build_t5_optimizer": [[26, 29], ["transformers.optimization.Adafactor", "model.parameters"], "function", ["None"], ["", "def", "build_t5_optimizer", "(", "opt", ",", "model", ")", ":", "\n", "\t", "optimizer", "=", "Adafactor", "(", "model", ".", "parameters", "(", ")", ",", "scale_parameter", "=", "False", ",", "relative_step", "=", "False", ",", "warmup_init", "=", "False", ",", "lr", "=", "0.3", ",", "weight_decay", "=", "1e-5", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.build_optimizer": [[30, 46], ["transformers.AdamW", "transformers.get_constant_schedule", "utils.AdamWOpt", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["", "def", "build_optimizer", "(", "opt", ",", "model", ")", ":", "\n", "    ", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "opt", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "opt", ".", "learning_rate", ",", "eps", "=", "opt", ".", "adam_epsilon", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ")", "\n", "scheduler", "=", "get_constant_schedule", "(", "optimizer", ")", "\n", "\n", "return", "AdamWOpt", "(", "optimizer", ",", "scheduler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.utils.build_warmup_optimizer": [[47, 64], ["transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "utils.AdamWOpt", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["", "def", "build_warmup_optimizer", "(", "opt", ",", "model", ")", ":", "\n", "    ", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "opt", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "opt", ".", "learning_rate", ",", "eps", "=", "opt", ".", "adam_epsilon", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ")", "\n", "warmup_step", "=", "(", "opt", ".", "num_training_steps", "*", "opt", ".", "warmup_ratio", ")", "if", "opt", ".", "warmup_ratio", ">", "0", "else", "opt", ".", "warmup_step", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "warmup_step", ",", "opt", ".", "num_training_steps", ",", "-", "1", ")", "\n", "\n", "return", "AdamWOpt", "(", "optimizer", ",", "scheduler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_SenLabel.get_label_dict": [[14, 21], ["open", "out.readlines", "l.strip.strip", "len"], "function", ["None"], ["def", "get_label_dict", "(", "label_path", ")", ":", "\n", "    ", "label_dict", "=", "{", "}", "\n", "with", "open", "(", "label_path", ")", "as", "out", ":", "\n", "        ", "for", "l", "in", "out", ".", "readlines", "(", ")", ":", "\n", "            ", "l", "=", "l", ".", "strip", "(", ")", "\n", "label_dict", "[", "l", "]", "=", "len", "(", "label_dict", ")", "\n", "", "", "return", "label_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.train_SenLabel.evaluation": [[22, 82], ["model.to.eval", "model.to.to", "torch.nn.Softmax", "torch.nn.Softmax", "print", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "sklearn.metrics.f1_score", "sorted", "int", "print", "zip", "model.to.", "torch.nn.Softmax.", "torch.max", "torch.max", "preds.detach().cpu().numpy().tolist", "max_score.detach().cpu().numpy().tolist", "selected_words.append", "selected_tags.append", "selected_pred.append", "selected_gt.append", "sklearn.metrics.f1_score", "open", "zip", "zip", "len", "selected_words.append", "selected_tags.append", "out.write", "batch[].to", "batch[].detach().cpu().numpy().tolist", "preds.detach().cpu().numpy", "max_score.detach().cpu().numpy", "selected_words.append", "selected_tags.append", "batch[].detach().cpu().numpy", "preds.detach().cpu", "max_score.detach().cpu", "batch[].detach().cpu", "preds.detach", "max_score.detach", "batch[].detach"], "function", ["None"], ["", "def", "evaluation", "(", "config", ",", "eval_data", ",", "model", ",", "label_map", ",", "device", ",", "show_detail", "=", "False", ",", "output_path", "=", "None", ")", ":", "\n", "\t", "model", ".", "eval", "(", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "input_words", "=", "[", "]", "\n", "output_tags", "=", "[", "]", "\n", "scores", "=", "[", "]", "\n", "gt_tags", "=", "[", "]", "\n", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t", "for", "batch", "in", "tqdm", "(", "eval_data", ")", ":", "\n", "\t\t\t", "for", "n", "in", "batch", ":", "\n", "\t\t\t\t", "if", "batch", "[", "n", "]", "is", "not", "None", "and", "n", "not", "in", "[", "'gt_x'", "]", ":", "\n", "\t\t\t\t\t", "batch", "[", "n", "]", "=", "batch", "[", "n", "]", ".", "to", "(", "device", ")", "\n", "\n", "", "", "outputs", "=", "model", "(", "\n", "input_ids", "=", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", "\n", ")", "\n", "\n", "logits", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "softmax", "(", "logits", ")", "\n", "max_score", ",", "preds", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "input_words", "+=", "batch", "[", "'gt_x'", "]", "\n", "gt_tags", "+=", "[", "l", "[", "0", "]", "for", "l", "in", "batch", "[", "\"labels\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "]", "\n", "output_tags", "+=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "scores", "+=", "max_score", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "", "", "new_F", "=", "f1_score", "(", "gt_tags", ",", "output_tags", ",", "average", "=", "'micro'", ")", "*", "100", "\n", "print", "(", "\"all data Micro F1 %.2f\"", "%", "new_F", ")", "\n", "selected_words", ",", "selected_tags", "=", "[", "]", ",", "[", "]", "\n", "\n", "if", "config", ".", "score_top_ratio", ">", "0", ":", "\n", "\t\t", "selected_gt", ",", "selected_pred", "=", "[", "]", ",", "[", "]", "\n", "rank_list", "=", "[", "(", "w", ",", "t", ",", "gt", ",", "s", ")", "for", "(", "w", ",", "t", ",", "gt", ",", "s", ")", "in", "zip", "(", "input_words", ",", "output_tags", ",", "gt_tags", ",", "scores", ")", "]", "\n", "sorted_rank_list", "=", "sorted", "(", "rank_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "3", "]", ",", "reverse", "=", "True", ")", "\n", "select_num", "=", "int", "(", "len", "(", "sorted_rank_list", ")", "*", "config", ".", "score_top_ratio", ")", "\n", "for", "input_word", ",", "tag", ",", "gt_tag", ",", "_", "in", "sorted_rank_list", "[", ":", "select_num", "]", ":", "\n", "\t\t\t", "selected_words", ".", "append", "(", "input_word", ")", "\n", "selected_tags", ".", "append", "(", "label_map", "[", "tag", "]", ")", "\n", "selected_pred", ".", "append", "(", "tag", ")", "\n", "selected_gt", ".", "append", "(", "gt_tag", ")", "\n", "", "F", "=", "f1_score", "(", "selected_gt", ",", "selected_pred", ",", "average", "=", "'micro'", ")", "*", "100", "\n", "print", "(", "\"Selected Micro F1 %.2f\"", "%", "F", ")", "\n", "", "else", ":", "\n", "\t\t", "for", "input_word", ",", "gt_label", ",", "pred_label", "in", "zip", "(", "input_words", ",", "gt_tags", ",", "output_tags", ")", ":", "\n", "\t\t\t", "if", "config", ".", "enable_consistency_filtering", ":", "\n", "\t\t\t\t", "if", "gt_label", "==", "pred_label", ":", "\n", "\t\t\t\t\t", "selected_words", ".", "append", "(", "input_word", ")", "\n", "selected_tags", ".", "append", "(", "label_map", "[", "pred_label", "]", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "selected_words", ".", "append", "(", "input_word", ")", "\n", "selected_tags", ".", "append", "(", "label_map", "[", "pred_label", "]", ")", "\n", "\n", "", "", "", "if", "output_path", "is", "not", "None", ":", "\n", "\t\t", "with", "open", "(", "output_path", ",", "'w'", ")", "as", "out", ":", "\n", "\t\t\t", "for", "gen", ",", "labels", "in", "zip", "(", "selected_words", ",", "selected_tags", ")", ":", "\n", "\t\t\t\t", "out", ".", "write", "(", "\"%s\\t%s\\n\"", "%", "(", "gen", ",", "labels", ")", ")", "\n", "\n", "", "", "", "return", "new_F", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunk_type": [[7, 11], ["tag_name.split", "tag_name.split"], "function", ["None"], ["def", "get_chunk_type", "(", "tag_name", ")", ":", "\n", "    ", "tag_class", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "tag_type", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "\n", "return", "tag_class", ",", "tag_type", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunks": [[12, 38], ["enumerate", "chunks.append", "chunks.append", "len", "generate_few_shot_data.get_chunk_type", "chunks.append"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunk_type"], ["", "def", "get_chunks", "(", "seq", ")", ":", "\n", "    ", "default", "=", "\"O\"", "\n", "chunks", "=", "[", "]", "\n", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "seq", ")", ":", "\n", "        ", "if", "tok", "==", "default", "and", "chunk_type", "is", "not", "None", ":", "\n", "            ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "\n", "", "elif", "tok", "!=", "default", ":", "\n", "            ", "tok_chunk_class", ",", "tok_chunk_type", "=", "get_chunk_type", "(", "tok", ")", "\n", "if", "chunk_type", "is", "None", ":", "\n", "                ", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "elif", "tok_chunk_type", "!=", "chunk_type", "or", "tok_chunk_class", "==", "\"B\"", ":", "\n", "                ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "if", "chunk_type", "is", "not", "None", ":", "\n", "        ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "len", "(", "seq", ")", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.read_conll": [[39, 57], ["pathlib.Path", "pathlib.Path.read_text().strip", "re.split", "doc.split", "token_docs.append", "tag_docs.append", "pathlib.Path.read_text", "line.split", "tokens.append", "tags.append"], "function", ["None"], ["", "def", "read_conll", "(", "file_path", ")", ":", "\n", "    ", "file_path", "=", "Path", "(", "file_path", ")", "\n", "\n", "raw_text", "=", "file_path", ".", "read_text", "(", ")", ".", "strip", "(", ")", "\n", "raw_docs", "=", "re", ".", "split", "(", "r'\\n\\t?\\n'", ",", "raw_text", ")", "\n", "token_docs", "=", "[", "]", "\n", "tag_docs", "=", "[", "]", "\n", "for", "doc", "in", "raw_docs", ":", "\n", "        ", "tokens", "=", "[", "]", "\n", "tags", "=", "[", "]", "\n", "for", "line", "in", "doc", ".", "split", "(", "'\\n'", ")", ":", "\n", "            ", "token", ",", "tag", "=", "line", ".", "split", "(", ")", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "tags", ".", "append", "(", "tag", ")", "\n", "", "token_docs", ".", "append", "(", "tokens", ")", "\n", "tag_docs", ".", "append", "(", "tags", ")", "\n", "\n", "", "return", "token_docs", ",", "tag_docs", "\n", "\n"]], "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.extract_data": [[58, 130], ["generate_few_shot_data.read_conll", "random.seed", "os.makedirs", "zip", "print", "print", "set", "print", "os.path.join", "all_data.append", "generate_few_shot_data.get_chunks", "open", "zip", "os.path.join", "print", "tag_dict[].append", "len", "random.shuffle", "random.shuffle", "ValueError", "len", "zip", "out.write", "open", "zip", "out.write", "set.add", "selected_sen.append", "selected_tag.append", "set.add", "selected_sen.append", "selected_tag.append", "len", "zip", "out.write", "len", "len", "len", "len", "len", "len", "out.write"], "function", ["home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.read_conll", "home.repos.pwc.inspect_result.garyyufei_promda.None.generate_few_shot_data.get_chunks"], ["", "def", "extract_data", "(", "_A", ",", "data_path", ",", "split_name", ",", "save_unlabel_data", "=", "True", ")", ":", "\n", "    ", "token_docs", ",", "tag_docs", "=", "read_conll", "(", "data_path", ")", "\n", "\n", "random", ".", "seed", "(", "_A", ".", "random_seed", ")", "\n", "os", ".", "makedirs", "(", "_A", ".", "output_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "tag_dict", "=", "{", "}", "\n", "data_count", "=", "0", "\n", "all_data", "=", "[", "]", "\n", "for", "tokens", ",", "tags", "in", "zip", "(", "token_docs", ",", "tag_docs", ")", ":", "\n", "        ", "all_data", ".", "append", "(", "(", "tokens", ",", "tags", ")", ")", "\n", "data_count", "+=", "1", "\n", "chunks", "=", "get_chunks", "(", "tags", ")", "\n", "for", "chunk", "in", "chunks", ":", "\n", "            ", "if", "chunk", "[", "0", "]", "not", "in", "tag_dict", ":", "\n", "                ", "tag_dict", "[", "chunk", "[", "0", "]", "]", "=", "[", "]", "\n", "", "tag_dict", "[", "chunk", "[", "0", "]", "]", ".", "append", "(", "(", "tokens", ",", "tags", ")", ")", "\n", "\n", "", "", "print", "(", "\"found %d tag slots\"", "%", "len", "(", "tag_dict", ")", ")", "\n", "print", "(", "\"found %d instances\"", "%", "data_count", ")", "\n", "\n", "selected_sen", "=", "[", "]", "\n", "selected_tag", "=", "[", "]", "\n", "used_sen", "=", "set", "(", ")", "\n", "if", "_A", ".", "few_shot_k", ">", "0", ":", "\n", "        ", "for", "chunk", "in", "tag_dict", ":", "\n", "            ", "random", ".", "shuffle", "(", "tag_dict", "[", "chunk", "]", ")", "\n", "count", "=", "0", "\n", "for", "(", "tokens", ",", "tags", ")", "in", "tag_dict", "[", "chunk", "]", ":", "\n", "                ", "sen", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "if", "(", "sen", "not", "in", "used_sen", ")", "and", "len", "(", "tokens", ")", ">", "_A", ".", "min_length", "and", "len", "(", "tokens", ")", "<", "_A", ".", "max_length", ":", "\n", "                    ", "used_sen", ".", "add", "(", "sen", ")", "\n", "selected_sen", ".", "append", "(", "tokens", ")", "\n", "selected_tag", ".", "append", "(", "tags", ")", "\n", "count", "+=", "1", "\n", "", "if", "count", "==", "_A", ".", "few_shot_k", ":", "break", "\n", "", "", "", "elif", "_A", ".", "total_training_num", ">", "0", ":", "\n", "        ", "random", ".", "shuffle", "(", "all_data", ")", "\n", "for", "(", "tokens", ",", "tags", ")", "in", "all_data", ":", "\n", "            ", "sen", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "if", "(", "sen", "not", "in", "used_sen", ")", "and", "len", "(", "tokens", ")", ">", "_A", ".", "min_length", "and", "len", "(", "tokens", ")", "<", "_A", ".", "max_length", ":", "\n", "                ", "used_sen", ".", "add", "(", "sen", ")", "\n", "selected_sen", ".", "append", "(", "tokens", ")", "\n", "selected_tag", ".", "append", "(", "tags", ")", "\n", "\n", "", "if", "len", "(", "selected_sen", ")", "==", "_A", ".", "total_training_num", ":", "break", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"few_shot_k and total_training_num cannot be non-positive!\"", ")", "\n", "\n", "\n", "", "print", "(", "\"select %d labeled instances\"", "%", "len", "(", "selected_tag", ")", ")", "\n", "num_", "=", "_A", ".", "few_shot_k", "if", "_A", ".", "few_shot_k", ">", "0", "else", "_A", ".", "total_training_num", "\n", "\n", "output_path", "=", "os", ".", "path", ".", "join", "(", "_A", ".", "output_path", ",", "'%s_whole_%d.txt'", "%", "(", "split_name", ",", "num_", ")", ")", "\n", "with", "open", "(", "output_path", ",", "'w'", ")", "as", "out", ":", "\n", "        ", "for", "(", "gen", ",", "labels", ")", "in", "zip", "(", "selected_sen", ",", "selected_tag", ")", ":", "\n", "            ", "for", "g", ",", "l", "in", "zip", "(", "gen", ",", "labels", ")", ":", "\n", "                ", "out", ".", "write", "(", "\"%s %s\\n\"", "%", "(", "g", ",", "l", ")", ")", "\n", "", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "if", "save_unlabel_data", ":", "\n", "        ", "unlabeled_output_path", "=", "os", ".", "path", ".", "join", "(", "_A", ".", "output_path", ",", "'unlabeled_train_whole_%d.txt'", "%", "num_", ")", "\n", "unlabeled_count", "=", "0", "\n", "with", "open", "(", "unlabeled_output_path", ",", "'w'", ")", "as", "out", ":", "\n", "            ", "for", "(", "gen", ",", "labels", ")", "in", "zip", "(", "token_docs", ",", "tag_docs", ")", ":", "\n", "                ", "sen", "=", "' '", ".", "join", "(", "gen", ")", "\n", "if", "sen", "not", "in", "used_sen", "and", "len", "(", "gen", ")", ">", "_A", ".", "min_length", "and", "len", "(", "gen", ")", "<", "_A", ".", "max_length", ":", "\n", "                    ", "unlabeled_count", "+=", "1", "\n", "for", "g", ",", "l", "in", "zip", "(", "gen", ",", "labels", ")", ":", "\n", "                        ", "out", ".", "write", "(", "\"%s %s\\n\"", "%", "(", "g", ",", "l", ")", ")", "\n", "", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "", "", "", "print", "(", "\"select %d unlabeled instances\"", "%", "unlabeled_count", ")", "\n", "\n"]]}