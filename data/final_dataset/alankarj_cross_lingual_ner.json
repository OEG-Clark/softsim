{"home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.val_error_msg": [[12, 23], ["type", "type", "type", "type"], "function", ["None"], ["def", "val_error_msg", "(", "var", ",", "name", ")", ":", "\n", "    ", "assert", "type", "(", "name", ")", "==", "str", "\n", "if", "type", "(", "var", ")", "==", "int", ":", "\n", "        ", "placeholder", "=", "'%d'", "\n", "", "elif", "type", "(", "var", ")", "==", "float", ":", "\n", "        ", "placeholder", "=", "'%.3f'", "\n", "", "elif", "type", "(", "var", ")", "==", "str", ":", "\n", "        ", "placeholder", "=", "'%s'", "\n", "", "else", ":", "\n", "        ", "return", "'Incorrect type for %s'", "%", "name", "\n", "", "return", "placeholder", "+", "' '", "+", "name", "+", "' not supported.'", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.prepare_data_new_format": [[25, 30], ["os.path.join", "main.data_prep_helper", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.data_prep_helper"], ["", "def", "prepare_data_new_format", "(", "tgt_lang", ")", ":", "\n", "    ", "data_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "config", ".", "DATA_FOLDER", ")", "\n", "# data_prep_helper(tgt_lang, data_path, key=\"testa\")", "\n", "# data_prep_helper(tgt_lang, data_path, key=\"testb\")", "\n", "data_prep_helper", "(", "tgt_lang", ",", "data_path", ",", "key", "=", "\"train_original\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.data_prep_helper": [[32, 38], ["main.get_annotated_list", "src.util.data_processing.prepare_train_file", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.get_annotated_list", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.prepare_train_file"], ["", "def", "data_prep_helper", "(", "tgt_lang", ",", "data_path", ",", "key", ")", ":", "\n", "    ", "annotated_list", "=", "get_annotated_list", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "tgt_lang", ",", "key", ")", ")", "\n", "# data_processing.prepare_train_file_new_format(annotated_list, os.path.join(data_path,", "\n", "#                                                                            \"en-\" + tgt_lang), key)", "\n", "data_processing", ".", "prepare_train_file", "(", "annotated_list", ",", "None", ",", "os", ".", "path", ".", "join", "(", "data_path", ",", "\n", "\"en-\"", "+", "tgt_lang", ",", "key", "+", "\"_cleaned\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.get_annotated_list": [[40, 50], ["os.path.exists", "pickle.load", "src.util.data_processing.AnnotatedData", "data_processing.AnnotatedData.process_data", "open", "open", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData.process_data"], ["", "def", "get_annotated_list", "(", "file_path", ")", ":", "\n", "    ", "annotated_list_file_path", "=", "file_path", "+", "\".pkl\"", "\n", "if", "os", ".", "path", ".", "exists", "(", "annotated_list_file_path", ")", ":", "\n", "        ", "annotated_list", "=", "pickle", ".", "load", "(", "open", "(", "annotated_list_file_path", ",", "'rb'", ")", ")", "\n", "", "else", ":", "\n", "        ", "annotated_data", "=", "data_processing", ".", "AnnotatedData", "(", "file_path", ",", "lang", "=", "None", ",", "verbosity", "=", "2", ")", "\n", "annotated_list", "=", "annotated_data", ".", "process_data", "(", ")", "\n", "with", "open", "(", "annotated_list_file_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "annotated_list", ",", "f", ")", "\n", "", "", "return", "annotated_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.parse_arguments": [[52, 155], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parse_arguments", "(", ")", ":", "\n", "    ", "parser", "=", "ArgumentParser", "(", "description", "=", "\"Argument Parser for cross-lingual \"", "\n", "\"NER.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--src_lang\"", ",", "dest", "=", "\"src_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"en\"", ",", "\n", "help", "=", "\"Source language\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--tgt_lang\"", ",", "dest", "=", "\"tgt_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"es\"", ",", "\n", "help", "=", "'Target language'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--translate_fname\"", ",", "dest", "=", "\"translate_fname\"", ",", "type", "=", "str", ",", "\n", "default", "=", "\"train\"", ",", "\n", "help", "=", "\"File name to be translated from src_lang to \"", "\n", "\"tgt_lang (train, dev, test)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--api_key_fname\"", ",", "dest", "=", "\"api_key_fname\"", ",", "type", "=", "str", ",", "\n", "default", "=", "\"api_key\"", ",", "help", "=", "\"File name for Google API key\"", ")", "\n", "\n", "# Set this flag to 1 if the input is in its raw form, i.e., in a text file", "\n", "# with each token and its corresponding tag (separated by a space) in", "\n", "# a separate line.", "\n", "parser", ".", "add_argument", "(", "\"--pre_process\"", ",", "dest", "=", "\"pre_process\"", ",", "type", "=", "int", ",", "\n", "default", "=", "0", ",", "\n", "help", "=", "\"Whether to pre-process raw data or not\"", ")", "\n", "\n", "# Set this flag to 2 if the TRANSLATE step, i.e., translation of the", "\n", "# annotated source corpus to the target language, sentence-by-sentence needs", "\n", "# to be performed. Set it to 1 if only the first step of MATCH,", "\n", "# i.e., translation of entity phrases needs to be performed.", "\n", "parser", ".", "add_argument", "(", "\"--trans_sent\"", ",", "dest", "=", "\"trans_sent\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Whether to translate full sentences or not\"", ")", "\n", "\n", "# This flag determines whether all sentences in the corpus need to be", "\n", "# translated. The flags sentence_ids_file and num_sample can be used in", "\n", "# conjunction with this flag when it is set to 0.", "\n", "parser", ".", "add_argument", "(", "\"--translate_all\"", ",", "dest", "=", "\"translate_all\"", ",", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Whether to translate all sentences or not\"", ")", "\n", "\n", "# If only a sample of the overall corpus needs to be translated,", "\n", "# a few sentence Ids can be sampled and stored as a list in a file for", "\n", "# future experiments.", "\n", "parser", ".", "add_argument", "(", "\"--sentence_ids_file\"", ",", "dest", "=", "\"sentence_ids_file\"", ",", "\n", "type", "=", "str", ",", "default", "=", "\"sentence_ids\"", ",", "\n", "help", "=", "\"Sentence IDs file name (only root of the name)\"", ")", "\n", "\n", "# The number of samples that need to be generated if the entire corpus", "\n", "# does not need to be translated. Note that this should be equal to the", "\n", "# size of the list of sentence_ids in the sentence_ids_file.", "\n", "parser", ".", "add_argument", "(", "\"--num_sample\"", ",", "dest", "=", "\"num_sample\"", ",", "type", "=", "int", ",", "\n", "default", "=", "100", ",", "help", "=", "\"Number of sentences to sample for \"", "\n", "\"partial translation.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--verbosity\"", ",", "dest", "=", "\"verbosity\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"Verbosity level of logging (0/1/2)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--sent_iter\"", ",", "dest", "=", "\"sent_iter\"", ",", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "help", "=", "\"Iteration index of the batch \"", "\n", "\"of source sentences during which a \"", "\n", "\"translation error occurred while \"", "\n", "\"translating sentences.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--phrase_iter\"", ",", "dest", "=", "\"phrase_iter\"", ",", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "help", "=", "\"Iteration index of the source \"", "\n", "\"sentence during which a translation \"", "\n", "\"error occurred while translating \"", "\n", "\"entity phrases.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--matching_score_threshold\"", ",", "\n", "dest", "=", "\"matching_score_threshold\"", ",", "\n", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "\"All token-level matches with a score below this\"", "\n", "\"threshold are ignored.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_set_size\"", ",", "\n", "dest", "=", "\"max_set_size\"", ",", "\n", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "\"Maximum number of permutations to be generated \"", "\n", "\"while determining the best span level match.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--ablation_string\"", ",", "\n", "dest", "=", "\"ablation_string\"", ",", "\n", "type", "=", "str", ",", "default", "=", "\"original\"", ",", "\n", "help", "=", "\"Ablation string to perform experiments by \"", "\n", "\"removing different features (original / \"", "\n", "\"_no_idc_ / _no_gold_ / _no_copy_ / \"", "\n", "\"_no_phonetic_ / _no_google_ / _no_google_v2_)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--topk\"", ",", "\n", "dest", "=", "\"topk\"", ",", "\n", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"For distribution-level matching, each unmatched \"", "\n", "\"entity is compared with only top-k spans sorted\"", "\n", "\"according to their tf-idf scores.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--min_occur\"", ",", "\n", "dest", "=", "\"min_occur\"", ",", "\n", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"For distribution-level matching, each potential \"", "\n", "\"span match must occur atleast min_occur number \"", "\n", "\"of times.\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.verify_arguments": [[157, 173], ["ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "main.val_error_msg", "main.val_error_msg", "main.val_error_msg", "main.val_error_msg", "main.val_error_msg"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.val_error_msg", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.val_error_msg", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.val_error_msg", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.val_error_msg", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.val_error_msg"], ["", "def", "verify_arguments", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "pre_process", "not", "in", "[", "0", ",", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "val_error_msg", "(", "args", ".", "pre_process", ",", "\"pre_process\"", ")", "%", "\n", "args", ".", "pre_process", ")", "\n", "", "if", "args", ".", "trans_sent", "not", "in", "[", "0", ",", "1", ",", "2", "]", ":", "\n", "        ", "raise", "ValueError", "(", "val_error_msg", "(", "args", ".", "trans_sent", ",", "\"trans_sent\"", ")", "%", "\n", "args", ".", "trans_sent", ")", "\n", "", "if", "args", ".", "translate_all", "not", "in", "[", "0", ",", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "val_error_msg", "(", "args", ".", "translate_all", ",", "\"translate_all\"", ")", "%", "\n", "args", ".", "translate_all", ")", "\n", "", "if", "args", ".", "verbosity", "not", "in", "[", "0", ",", "1", ",", "2", "]", ":", "\n", "        ", "raise", "ValueError", "(", "val_error_msg", "(", "args", ".", "verbosity", ",", "\"verbosity\"", ")", "%", "\n", "args", ".", "verbosity", ")", "\n", "", "if", "args", ".", "num_sample", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "val_error_msg", "(", "args", ".", "num_sample", ",", "\"num_sample\"", ")", "%", "\n", "args", ".", "num_sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.main": [[175, 222], ["main.parse_arguments", "main.verify_arguments", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "src.util.tmp.TMP", "tmp.TMP.translate_data", "os.getcwd", "os.path.join", "src.util.data_processing.AnnotatedData", "data_processing.AnnotatedData.process_data", "pickle.load", "open", "f.read().strip", "tmp.TMP.prepare_tgt_candidate_phrase_list", "tmp.TMP.get_tgt_annotations_new", "tmp.TMP.prepare_train_file", "open", "pickle.dump", "open", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "src.util.tmp.read_lexicon", "f.read"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.parse_arguments", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.None.main.verify_arguments", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.translate_data", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData.process_data", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.prepare_tgt_candidate_phrase_list", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_tgt_annotations_new", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.prepare_train_file", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.read_lexicon"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_arguments", "(", ")", "\n", "verify_arguments", "(", "args", ")", "\n", "\n", "args", ".", "data_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "config", ".", "DATA_FOLDER", ")", "\n", "args", ".", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "args", ".", "src_lang", ")", "\n", "args", ".", "tgt_file_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "args", ".", "tgt_lang", ")", "\n", "\n", "file_name", "=", "args", ".", "translate_fname", "+", "\"_processed\"", "\n", "annotated_list_file_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "src_file_path", ",", "\n", "file_name", "+", "config", ".", "PKL_EXT", ")", "\n", "\n", "# Pre-process the raw input only if the flag is set to 1.", "\n", "if", "args", ".", "pre_process", "==", "1", ":", "\n", "        ", "file_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "src_file_path", ",", "args", ".", "translate_fname", ")", "\n", "# Unless supported by config.gold_test_dict, set the lang always to", "\n", "# None below. This argument is only used for performing tests to ensure", "\n", "# that the preprocessing has been performed correctly.", "\n", "annotated_data", "=", "data_processing", ".", "AnnotatedData", "(", "file_path", ",", "lang", "=", "None", ",", "\n", "verbosity", "=", "args", ".", "verbosity", ")", "\n", "annotated_list", "=", "annotated_data", ".", "process_data", "(", ")", "\n", "with", "open", "(", "annotated_list_file_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "annotated_list", ",", "f", ")", "\n", "", "", "else", ":", "\n", "# Load the pre-processed annotated list.", "\n", "        ", "annotated_list", "=", "pickle", ".", "load", "(", "open", "(", "annotated_list_file_path", ",", "'rb'", ")", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "args", ".", "api_key_fname", ")", ",", "\"r\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "args", ".", "api_key", "=", "f", ".", "read", "(", ")", ".", "strip", "(", "\"\\n\"", ")", "\n", "\n", "", "translation", "=", "tmp", ".", "TMP", "(", "annotated_list", ",", "args", ")", "\n", "translation", ".", "translate_data", "(", ")", "\n", "\n", "# If no translation needs to be performed, proceed with the MATCH +", "\n", "# PROJECT steps.", "\n", "if", "args", ".", "trans_sent", "==", "0", ":", "\n", "        ", "for", "lexicon", "in", "config", ".", "LEXICON_FILE_NAMES", ":", "\n", "            ", "base_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "args", ".", "src_lang", "+", "\"-\"", "+", "\n", "args", ".", "tgt_lang", ")", "\n", "txt_path", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "lexicon", "+", "\".txt\"", ")", "\n", "pkl_path", "=", "os", ".", "path", ".", "join", "(", "base_path", ",", "lexicon", "+", "config", ".", "PKL_EXT", ")", "\n", "tmp", ".", "read_lexicon", "(", "txt_path", ",", "pkl_path", ")", "\n", "\n", "", "translation", ".", "prepare_tgt_candidate_phrase_list", "(", ")", "\n", "translation", ".", "get_tgt_annotations_new", "(", ")", "\n", "translation", ".", "prepare_train_file", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.src.config.get_allowed_tags": [[257, 263], ["allowed_tags.append"], "function", ["None"], ["def", "get_allowed_tags", "(", ")", ":", "\n", "    ", "allowed_tags", "=", "[", "]", "\n", "for", "tp", "in", "TAG_PREFIXES", ":", "\n", "        ", "for", "nt", "in", "NER_TAGS", ":", "\n", "            ", "allowed_tags", ".", "append", "(", "tp", "+", "'-'", "+", "nt", ")", "\n", "", "", "return", "allowed_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.src.config.get_epi": [[268, 273], ["epitran.Epitran", "epitran.Epitran"], "function", ["None"], ["def", "get_epi", "(", "lang", ")", ":", "\n", "    ", "if", "lang", "==", "\"zh\"", ":", "\n", "        ", "return", "epitran", ".", "Epitran", "(", "EPI_DICT", "[", "lang", "]", ",", "\n", "cedict_file", "=", "\"data/zh/cedict_ts.u8\"", ")", "\n", "", "return", "epitran", ".", "Epitran", "(", "EPI_DICT", "[", "lang", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData.__init__": [[12, 22], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "file_path", ",", "lang", "=", "None", ",", "verbosity", "=", "0", ")", ":", "\n", "        ", "self", ".", "file_path", "=", "file_path", "\n", "# Language of the input file.", "\n", "self", ".", "lang", "=", "lang", "\n", "self", ".", "verbosity", "=", "verbosity", "\n", "# List of all annotated sentences.", "\n", "self", ".", "annotated_list", "=", "None", "\n", "# Documents marked by \"-DOCSTART-\". This value is calculated and used", "\n", "# only for testing, if the lang is set to a non-None value.", "\n", "self", ".", "num_docs", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData.process_data": [[23, 60], ["open", "f.readlines", "list", "list", "enumerate", "data_processing.AnnotatedData._create_annotation", "row.rstrip().split.rstrip().split.rstrip().split", "row[].startswith", "list.append", "list.append", "print", "data_processing.AnnotatedData._processing_test", "data_processing.AnnotatedData._create_annotation", "list", "tuple", "print", "len", "row.rstrip().split.rstrip().split.rstrip", "list.append"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData._create_annotation", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData._processing_test", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData._create_annotation"], ["", "def", "process_data", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "file_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "all_rows", "=", "f", ".", "readlines", "(", ")", "\n", "# List of all annotated sentences in the input corpus.", "\n", "annotated_list", "=", "list", "(", ")", "\n", "# Each sentence is stored as a list of tuples (that contain token", "\n", "# as their first element and the tag as the last).", "\n", "sentence", "=", "list", "(", ")", "\n", "\n", "for", "i", ",", "row", "in", "enumerate", "(", "all_rows", ")", ":", "\n", "                ", "row", "=", "row", ".", "rstrip", "(", "\"\\n\"", ")", ".", "split", "(", "\" \"", ")", "\n", "if", "row", "[", "0", "]", ".", "startswith", "(", "config", ".", "DOC_BEGIN_STR", ")", ":", "\n", "                    ", "self", ".", "num_docs", "+=", "1", "\n", "# No need to parse this line.", "\n", "continue", "\n", "", "if", "row", "==", "[", "\"\"", "]", ":", "\n", "# This indicates the beginning of a new sentence. Hence,", "\n", "# annotate the sentence found so far.", "\n", "                    ", "a", "=", "self", ".", "_create_annotation", "(", "sentence", ")", "\n", "if", "a", "is", "not", "None", ":", "\n", "                        ", "annotated_list", ".", "append", "(", "a", ")", "\n", "", "sentence", "=", "list", "(", ")", "\n", "continue", "\n", "", "sentence", ".", "append", "(", "tuple", "(", "row", ")", ")", "\n", "\n", "", "a", "=", "self", ".", "_create_annotation", "(", "sentence", ")", "\n", "if", "a", "is", "not", "None", ":", "\n", "                ", "annotated_list", ".", "append", "(", "a", ")", "\n", "\n", "", "if", "self", ".", "verbosity", ">=", "1", ":", "\n", "                ", "if", "self", ".", "lang", "is", "not", "None", ":", "\n", "                    ", "print", "(", "\"Number of documents: \"", ",", "self", ".", "num_docs", ")", "\n", "", "print", "(", "\"Number of annotated sentences: \"", ",", "len", "(", "annotated_list", ")", ")", "\n", "", "self", ".", "annotated_list", "=", "annotated_list", "\n", "if", "self", ".", "lang", "is", "not", "None", ":", "\n", "                ", "self", ".", "_processing_test", "(", ")", "\n", "", "return", "annotated_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData._create_annotation": [[61, 83], ["list", "map", "data_processing.Annotation", "data_processing._get_entity_spans", "zip", "print", "print", "print"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing._get_entity_spans"], ["", "", "def", "_create_annotation", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "\"\"\"\n        :param sentence: List of tuples (that contain token as their first\n        element and the tag as the last).\n        :return: Nothing if the sentence isn't an empty list. Else, return an\n        Annotation object corresponding to the sentence.\n        \"\"\"", "\n", "sentence", "=", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "sentence", ")", ")", ")", "\n", "if", "sentence", ":", "\n", "            ", "a", "=", "Annotation", "(", ")", "\n", "a", ".", "tokens", "=", "sentence", "[", "0", "]", "\n", "a", ".", "ner_tags", "=", "sentence", "[", "-", "1", "]", "\n", "# Get Span objects for all entities in the sentence.", "\n", "a", ".", "span_list", "=", "_get_entity_spans", "(", "a", ".", "ner_tags", ")", "\n", "if", "self", ".", "verbosity", "==", "2", ":", "\n", "                ", "print", "(", "\"Sentences: \"", ",", "a", ".", "tokens", ")", "\n", "print", "(", "\"NER Tags: \"", ",", "a", ".", "ner_tags", ")", "\n", "tuple_span_list", "=", "[", "(", "spans", ".", "beg", ",", "spans", ".", "end", ",", "spans", ".", "tag_type", ")", "\n", "for", "spans", "in", "a", ".", "span_list", "]", "\n", "print", "(", "\"Span list: \"", ",", "tuple_span_list", ")", "\n", "", "return", "a", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.AnnotatedData._processing_test": [[84, 111], ["dict", "len", "len", "print", "print", "print"], "methods", ["None"], ["", "def", "_processing_test", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Perform a series of checks to test whether the input file has been\n        correctly parsed. Works only for languages for which gold counts exist\n        in config.gold_test_dict exists.\n        :return: Nothing.\n        \"\"\"", "\n", "assert", "self", ".", "lang", "in", "config", ".", "gold_test_dict", "\n", "if", "self", ".", "num_docs", ">", "0", ":", "\n", "            ", "assert", "self", ".", "num_docs", "==", "config", ".", "gold_test_dict", "[", "self", ".", "lang", "]", "[", "\"documents\"", "]", "\n", "", "test_dict", "=", "dict", "(", ")", "\n", "test_dict", "[", "\"sentences\"", "]", "=", "len", "(", "self", ".", "annotated_list", ")", "\n", "test_dict", "[", "\"tokens\"", "]", "=", "0", "\n", "for", "annotation", "in", "self", ".", "annotated_list", ":", "\n", "            ", "test_dict", "[", "\"tokens\"", "]", "+=", "len", "(", "annotation", ".", "tokens", ")", "\n", "for", "spans", "in", "annotation", ".", "span_list", ":", "\n", "                ", "if", "spans", ".", "tag_type", "not", "in", "test_dict", ":", "\n", "                    ", "test_dict", "[", "spans", ".", "tag_type", "]", "=", "0", "\n", "", "test_dict", "[", "spans", ".", "tag_type", "]", "+=", "1", "\n", "\n", "", "", "for", "key", "in", "test_dict", ":", "\n", "            ", "if", "self", ".", "verbosity", "==", "2", ":", "\n", "                ", "print", "(", "\"Key: \"", ",", "key", ",", "end", "=", "''", ")", "\n", "print", "(", "\", Test count: \"", ",", "test_dict", "[", "key", "]", ",", "end", "=", "''", ")", "\n", "print", "(", "\", Gold count: \"", ",", "config", ".", "gold_test_dict", "[", "self", ".", "lang", "]", "[", "key", "]", ")", "\n", "", "assert", "test_dict", "[", "key", "]", "==", "config", ".", "gold_test_dict", "[", "self", ".", "lang", "]", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.Annotation.__init__": [[117, 124], ["list", "list", "list"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "# List of tokens in the sentence.", "\n", "        ", "self", ".", "tokens", "=", "list", "(", ")", "\n", "# List of NER tags in the sentence, one corresponding to each token.", "\n", "self", ".", "ner_tags", "=", "list", "(", ")", "\n", "# A list of spans of all the entity phrases in the sentence.", "\n", "self", ".", "span_list", "=", "list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.Span.__init__": [[130, 137], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "beg", ",", "end", ",", "tag_type", ")", ":", "\n", "# Begin token index of the entity span", "\n", "        ", "self", ".", "beg", "=", "beg", "\n", "# End token index of the entity span", "\n", "self", ".", "end", "=", "end", "\n", "# Type of tag of the entity span", "\n", "self", ".", "tag_type", "=", "tag_type", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing._get_entity_spans": [[139, 158], ["copy.deepcopy", "copy.deepcopy.append", "list", "enumerate", "curr_tag.startswith", "data_processing._add_span", "curr_tag.startswith", "curr_tag.split"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing._add_span"], ["", "", "def", "_get_entity_spans", "(", "ner_tags", ")", ":", "\n", "    ", "\"\"\"\n    :param ner_tags: List of NER Tags in BIO format.\n    :return: List of span objects corresponding to distinct entity phrases.\n    \"\"\"", "\n", "ner_tags", "=", "copy", ".", "deepcopy", "(", "ner_tags", ")", "\n", "ner_tags", ".", "append", "(", "\"O\"", ")", "\n", "span_list", "=", "list", "(", ")", "\n", "beg", "=", "0", "\n", "prev_tag_type", "=", "None", "\n", "for", "i", ",", "curr_tag", "in", "enumerate", "(", "ner_tags", ")", ":", "\n", "        ", "curr_tag_type", "=", "curr_tag", ".", "split", "(", "\"-\"", ")", "[", "1", "]", "if", "curr_tag", "!=", "\"O\"", "else", "None", "\n", "if", "not", "curr_tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "# This means that the previous span has ended, so we add it.", "\n", "            ", "span_list", "=", "_add_span", "(", "span_list", ",", "beg", ",", "i", ",", "prev_tag_type", ")", "\n", "if", "curr_tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "beg", "=", "i", "\n", "", "", "prev_tag_type", "=", "curr_tag_type", "\n", "", "return", "span_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing._add_span": [[160, 164], ["span_list.append", "data_processing.Span"], "function", ["None"], ["", "def", "_add_span", "(", "span_list", ",", "beg", ",", "end", ",", "tag_type", ")", ":", "\n", "    ", "if", "tag_type", "is", "not", "None", ":", "\n", "        ", "span_list", ".", "append", "(", "Span", "(", "beg", ",", "end", ",", "tag_type", ")", ")", "\n", "", "return", "span_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.prepare_train_file": [[166, 187], ["open", "enumerate", "len", "enumerate", "f.writelines", "f.writelines", "annotation.ner_tags[].endswith", "len", "len", "write_str.split", "write_str.split"], "function", ["None"], ["", "def", "prepare_train_file", "(", "annotated_list", ",", "drop_list", ",", "file_path", ",", "remove_misc", "=", "False", ")", ":", "\n", "    ", "if", "drop_list", "==", "None", ":", "\n", "        ", "drop_list", "=", "[", "]", "\n", "\n", "", "with", "open", "(", "file_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "annotation", "in", "enumerate", "(", "annotated_list", ")", ":", "\n", "            ", "if", "i", "not", "in", "drop_list", ":", "\n", "                ", "if", "len", "(", "annotation", ".", "tokens", ")", ">=", "0", ":", "\n", "                    ", "for", "j", ",", "token", "in", "enumerate", "(", "annotation", ".", "tokens", ")", ":", "\n", "                        ", "if", "remove_misc", ":", "\n", "                            ", "if", "annotation", ".", "ner_tags", "[", "j", "]", ".", "endswith", "(", "\"MISC\"", ")", ":", "\n", "                                ", "write_str", "=", "token", "+", "\" \"", "+", "\"O\"", "+", "\"\\n\"", "\n", "", "else", ":", "\n", "                                ", "write_str", "=", "token", "+", "\" \"", "+", "annotation", ".", "ner_tags", "[", "j", "]", "+", "\"\\n\"", "\n", "", "", "else", ":", "\n", "                            ", "write_str", "=", "token", "+", "\" \"", "+", "annotation", ".", "ner_tags", "[", "j", "]", "+", "\"\\n\"", "\n", "", "if", "len", "(", "write_str", ".", "split", "(", "\" \"", ")", ")", "<", "2", ":", "\n", "                            ", "write_str", "=", "'-'", "+", "write_str", "\n", "", "assert", "len", "(", "write_str", ".", "split", "(", ")", ")", "==", "2", "\n", "f", ".", "writelines", "(", "write_str", ")", "\n", "", "f", ".", "writelines", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.prepare_train_file_new_format": [[189, 203], ["os.path.join", "os.path.join", "open", "enumerate", "open", "enumerate", "len", "data_processing.get_write_str", "f.writelines", "len", "data_processing.get_write_str", "f.writelines"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.get_write_str", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.get_write_str"], ["", "", "", "", "", "def", "prepare_train_file_new_format", "(", "annotated_list", ",", "data_path", ",", "key", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "key", "+", "\".words.txt\"", ")", "\n", "with", "open", "(", "file_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "annotation", "in", "enumerate", "(", "annotated_list", ")", ":", "\n", "            ", "if", "len", "(", "annotation", ".", "tokens", ")", ">=", "0", ":", "\n", "                ", "write_str", "=", "get_write_str", "(", "annotation", ".", "tokens", ")", "\n", "f", ".", "writelines", "(", "write_str", ")", "\n", "\n", "", "", "", "file_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "key", "+", "\".tags.txt\"", ")", "\n", "with", "open", "(", "file_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "annotation", "in", "enumerate", "(", "annotated_list", ")", ":", "\n", "            ", "if", "len", "(", "annotation", ".", "tokens", ")", ">=", "0", ":", "\n", "                ", "write_str", "=", "get_write_str", "(", "annotation", ".", "ner_tags", ")", "\n", "f", ".", "writelines", "(", "write_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.data_processing.get_write_str": [[205, 210], ["enumerate", "write_str.rstrip"], "function", ["None"], ["", "", "", "", "def", "get_write_str", "(", "token_list", ")", ":", "\n", "    ", "write_str", "=", "\"\"", "\n", "for", "j", ",", "token", "in", "enumerate", "(", "token_list", ")", ":", "\n", "        ", "write_str", "+=", "token", "+", "\" \"", "\n", "", "return", "write_str", ".", "rstrip", "(", "\" \"", ")", "+", "\"\\n\"", "\n", "", ""]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.__init__": [[29, 94], ["os.path.join", "list", "list", "src.config.get_epi", "datetime.datetime.datetime.today().strftime", "logging.getLogger().setLevel", "logging.getLogger().setLevel", "logging.getLogger().setLevel", "logging.basicConfig", "logging.getLogger", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.getLogger.addHandler", "set", "nltk.corpus.stopwords.words", "ValueError", "datetime.datetime.datetime.today", "logging.getLogger", "logging.getLogger", "logging.getLogger", "str", "str", "datetime.datetime.datetime.today().strftime", "datetime.datetime.datetime.today"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.src.config.get_epi"], ["def", "__init__", "(", "self", ",", "annotated_list", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "# List of annotated sentences from source language.", "\n", "self", ".", "src_annotated_list", "=", "annotated_list", "\n", "# Path where all the intermediate files would be stored.", "\n", "self", ".", "base_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "args", ".", "src_lang", "+", "\"-\"", "+", "\n", "args", ".", "tgt_lang", ")", "\n", "# List containing Ids of sentences sampled (if the entire corpus does", "\n", "# not need to be translated and processed).", "\n", "self", ".", "sentence_ids", "=", "None", "\n", "# List of sentences in the target language (obtained through the ", "\n", "# TRANSLATE step).", "\n", "self", ".", "tgt_sentence_list", "=", "None", "\n", "# List of entity phrases for each sentence in the target language.", "\n", "self", ".", "tgt_phrase_list", "=", "None", "\n", "# List of entity phrases for each sentence in the source language.", "\n", "self", ".", "src_phrase_list", "=", "None", "\n", "# List of annotated sentences in the in the target language.", "\n", "self", ".", "tgt_annotated_list", "=", "list", "(", ")", "\n", "# List of candidate phrases in the target language for each source", "\n", "# entity phrase.", "\n", "self", ".", "tgt_candidate_phrase_list", "=", "list", "(", ")", "\n", "# Suffix for intermediate files.", "\n", "self", ".", "suffix", "=", "None", "\n", "# A list of sentence Ids that need to be dropped from the final", "\n", "# training / valid / test file.", "\n", "self", ".", "drop_list", "=", "None", "\n", "self", ".", "src_lang", "=", "args", ".", "src_lang", "\n", "self", ".", "tgt_lang", "=", "args", ".", "tgt_lang", "\n", "# Get Epitran object for Transliteration.", "\n", "self", ".", "epi", "=", "config", ".", "get_epi", "(", "self", ".", "tgt_lang", ")", "\n", "\n", "# Whether or not to use Stanford CoreNLP for tokenization.", "\n", "if", "self", ".", "tgt_lang", "==", "\"zh\"", "or", "self", ".", "tgt_lang", "==", "\"ar\"", ":", "\n", "            ", "self", ".", "use_corenlp", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "use_corenlp", "=", "False", "\n", "\n", "# Stop words for the target language.", "\n", "", "if", "self", ".", "tgt_lang", "in", "config", ".", "STOP_WORD_LANG_MAP", ":", "\n", "            ", "self", ".", "stop_word_list", "=", "set", "(", "stopwords", ".", "words", "(", "\n", "config", ".", "STOP_WORD_LANG_MAP", "[", "self", ".", "tgt_lang", "]", ")", ")", "\n", "", "elif", "self", ".", "tgt_lang", "in", "config", ".", "STOP_WORD_DICT", ":", "\n", "            ", "self", ".", "stop_word_list", "=", "config", ".", "STOP_WORD_DICT", "[", "self", ".", "tgt_lang", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Language %s stop word list not available.\"", "\n", "%", "self", ".", "tgt_lang", ")", "\n", "\n", "", "self", ".", "date_today", "=", "datetime", ".", "today", "(", ")", ".", "strftime", "(", "\"%d-%m-%Y\"", ")", "\n", "\n", "logging", ".", "getLogger", "(", "\"googleapiclient.discovery_cache\"", ")", ".", "setLevel", "(", "\n", "logging", ".", "ERROR", ")", "\n", "logging", ".", "getLogger", "(", "\"googleapiclient.discovery\"", ")", ".", "setLevel", "(", "logging", ".", "ERROR", ")", "\n", "logging", ".", "getLogger", "(", "\"cloudpickle\"", ")", ".", "setLevel", "(", "logging", ".", "ERROR", ")", "\n", "\n", "logging", ".", "basicConfig", "(", "level", "=", "logging", ".", "DEBUG", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "\n", "handler", "=", "logging", ".", "FileHandler", "(", "\n", "\"translation_\"", "+", "self", ".", "src_lang", "+", "\"-\"", "+", "self", ".", "tgt_lang", "+", "\"_\"", "+", "\n", "datetime", ".", "today", "(", ")", ".", "strftime", "(", "\"%d-%m-%Y\"", ")", "+", "\"_\"", "+", "\n", "str", "(", "self", ".", "args", ".", "matching_score_threshold", ")", "+", "\"_\"", "+", "\n", "str", "(", "self", ".", "args", ".", "topk", ")", "+", "self", ".", "args", ".", "ablation_string", "+", "\".log\"", ")", "\n", "handler", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "addHandler", "(", "handler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.translate_data": [[95, 296], ["len", "os.path.join", "os.path.join", "os.path.join", "tmp._google_translate_lang_code", "tmp._google_translate_lang_code", "enumerate", "str", "os.path.join", "os.path.exists", "list", "print", "tmp._get_saved_list", "int", "range", "pickle.load", "enumerate", "pickle.load", "pickle.load", "pickle.load", "print", "print", "print", "print", "ValueError", "pickle.load", "random.sample", "range", "math.ceil", "pickle.load.extend", "time.sleep", "open", "list", "list", "pickle.load", "pickle.load", "print", "print", "pickle.load.append", "print", "print", "open", "pickle.dump", "open", "pickle.dump", "open", "open", "open", "len", "len", "open", "list", "open", "pickle.dump", "len", "print", "print", "tmp.get_google_translations", "open", "pickle.dump", "open", "open", "len", "len", "print", "print", "print", "list", "src_phrase_list[].append", "print", "pickle.load.append", "pickle.load.append", "open", "pickle.dump", "open", "pickle.dump", "print", "len", "len", "len", "range", "print", "print", "tmp.get_google_translations", "list"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._google_translate_lang_code", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._google_translate_lang_code", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._get_saved_list", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_google_translations", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_google_translations"], ["", "def", "translate_data", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        :return: Populate the self.tgt_sentence_list, self.tgt_phrase_list\n        and self.src_phrase_list with their correct values. For accomplishing\n        that, this function utilizes the self.args.trans_sent flag to\n        determine whether both sentences and entity phrases need to be\n        translated (trans_sent = 2), only entity phrases need to be\n        translated (trans_sent = 1) or both of them have already been\n        translated and only need to be loaded from files (trans_sent = 0).\n        \"\"\"", "\n", "num_sentences", "=", "len", "(", "self", ".", "src_annotated_list", ")", "\n", "\n", "if", "self", ".", "args", ".", "translate_all", "==", "0", ":", "\n", "            ", "self", ".", "suffix", "=", "str", "(", "self", ".", "args", ".", "num_sample", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "suffix", "=", "\"all\"", "\n", "\n", "# If not all sentences need to be translated, read from a file", "\n", "# containing num_sample sentence Ids or sample num_sample sentences.", "\n", "", "if", "self", ".", "args", ".", "translate_all", "==", "0", ":", "\n", "            ", "if", "self", ".", "args", ".", "num_sample", ">", "num_sentences", ":", "\n", "                ", "raise", "ValueError", "(", "\"Cannot sample more than the number of \"", "\n", "\"sentences.\"", ")", "\n", "", "sentence_ids_file_name", "=", "self", ".", "args", ".", "sentence_ids_file", "+", "\"_num_sample=\"", "+", "self", ".", "suffix", "+", "\".pkl\"", "\n", "sentence_ids_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\n", "sentence_ids_file_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "sentence_ids_path", ")", ":", "\n", "                ", "sentence_ids", "=", "pickle", ".", "load", "(", "open", "(", "sentence_ids_path", ",", "\"rb\"", ")", ")", "\n", "", "else", ":", "\n", "                ", "sentence_ids", "=", "random", ".", "sample", "(", "list", "(", "range", "(", "num_sentences", ")", ")", ",", "\n", "self", ".", "args", ".", "num_sample", ")", "\n", "with", "open", "(", "sentence_ids_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                    ", "pickle", ".", "dump", "(", "sentence_ids", ",", "f", ")", "\n", "", "", "", "else", ":", "\n", "# All sentences would be translated.", "\n", "            ", "sentence_ids", "=", "list", "(", "range", "(", "num_sentences", ")", ")", "\n", "\n", "# Re-construct sentences from tokens in a na\u00efve fashion by concatenating", "\n", "# tokens together using whitespace. In absence of more information from", "\n", "# the input, this is a cheap and reasonably accurate way of constructing", "\n", "# sentences to send them for translation.", "\n", "", "src_sentence_list", "=", "[", "\" \"", ".", "join", "(", "self", ".", "src_annotated_list", "[", "sid", "]", ".", "tokens", ")", "\n", "for", "sid", "in", "sentence_ids", "]", "\n", "\n", "if", "self", ".", "args", ".", "verbosity", "==", "2", ":", "\n", "            ", "print", "(", "\"Source sentence list loaded. Length: %d\"", "%", "\n", "len", "(", "src_sentence_list", ")", ")", "\n", "\n", "# File names to either read from (if args.trans_sent is 0) or write to ", "\n", "# (if args.trans_sent is 1).", "\n", "", "tgt_sentence_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\n", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_\"", "+", "self", ".", "suffix", "+", "config", ".", "PKL_EXT", ")", "\n", "tgt_phrase_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\n", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_tgt_phrases_\"", "+", "self", ".", "suffix", "\n", "+", "config", ".", "PKL_EXT", ")", "\n", "src_phrase_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\n", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_src_phrases_\"", "+", "self", ".", "suffix", "+", "\n", "config", ".", "PKL_EXT", ")", "\n", "\n", "src_lang_code", "=", "_google_translate_lang_code", "(", "self", ".", "src_lang", ")", "\n", "tgt_lang_code", "=", "_google_translate_lang_code", "(", "self", ".", "tgt_lang", ")", "\n", "\n", "# Perform TRANSLATE step, i.e., translate of the annotated source", "\n", "# corpus to the target language.", "\n", "if", "self", ".", "args", ".", "trans_sent", "==", "2", ":", "\n", "# Sentences need to be sent in batches to Google Translate service.", "\n", "            ", "batch_size", "=", "config", ".", "BATCH_SIZE", "\n", "# If the file exists, load the list, else create an empty list.", "\n", "tgt_sentence_list", "=", "_get_saved_list", "(", "tgt_sentence_path", ")", "\n", "\n", "max_iter", "=", "int", "(", "math", ".", "ceil", "(", "num_sentences", "/", "batch_size", ")", ")", "\n", "for", "i", "in", "range", "(", "max_iter", ")", ":", "\n", "# Ignore all batches that have already been translated.", "\n", "                ", "if", "i", "<", "self", ".", "args", ".", "sent_iter", ":", "\n", "                    ", "continue", "\n", "\n", "", "beg", "=", "i", "*", "batch_size", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "batch_size", "\n", "\n", "if", "self", ".", "args", ".", "verbosity", "==", "2", ":", "\n", "                    ", "print", "(", "\"Batch ID: %d\"", "%", "i", ")", "\n", "print", "(", "\"Sentence indices from %d to %d being sent for \"", "\n", "\"translation...\"", "%", "(", "beg", ",", "end", ")", ")", "\n", "\n", "", "src_sentences", "=", "src_sentence_list", "[", "beg", ":", "end", "]", "\n", "tgt_sentence_list", ".", "extend", "(", "get_google_translations", "(", "\n", "src_sentences", ",", "src_lang_code", ",", "\n", "tgt_lang_code", ",", "self", ".", "args", ".", "api_key", ")", ")", "\n", "\n", "# Store the target sentence list obtained so far.", "\n", "with", "open", "(", "tgt_sentence_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                    ", "pickle", ".", "dump", "(", "tgt_sentence_list", ",", "f", ")", "\n", "", "time", ".", "sleep", "(", "config", ".", "TIME_SLEEP", ")", "\n", "\n", "# Translate entity phrases only in this round.", "\n", "", "", "if", "self", ".", "args", ".", "trans_sent", ">=", "1", ":", "\n", "            ", "tgt_sentence_list", "=", "pickle", ".", "load", "(", "open", "(", "tgt_sentence_path", ",", "\"rb\"", ")", ")", "\n", "\n", "# If no sentence has been \"processed\", i.e., entity phrases", "\n", "# belonging to a sentence have not yet been translated, initialize", "\n", "# the phrase lists by an empty list.", "\n", "if", "self", ".", "args", ".", "phrase_iter", "<", "0", ":", "\n", "                ", "src_phrase_list", "=", "list", "(", ")", "\n", "tgt_phrase_list", "=", "list", "(", ")", "\n", "# Else, simply load the phrase lists.", "\n", "", "else", ":", "\n", "                ", "src_phrase_list", "=", "pickle", ".", "load", "(", "open", "(", "src_phrase_path", ",", "'rb'", ")", ")", "\n", "tgt_phrase_list", "=", "pickle", ".", "load", "(", "open", "(", "tgt_phrase_path", ",", "'rb'", ")", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "verbosity", ">=", "1", ":", "\n", "                ", "print", "(", "\"Length of source phrase list: \"", ",", "len", "(", "src_phrase_list", ")", ")", "\n", "print", "(", "\"Length of target phrase list: \"", ",", "len", "(", "tgt_phrase_list", ")", ")", "\n", "\n", "", "for", "i", ",", "sid", "in", "enumerate", "(", "sentence_ids", ")", ":", "\n", "                ", "if", "i", "<", "self", ".", "args", ".", "phrase_iter", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "args", ".", "verbosity", ">=", "1", ":", "\n", "                    ", "print", "(", "\"####################################################\"", "\n", "\"##################\"", ")", "\n", "print", "(", "\"Sentence-%d\"", "%", "i", ")", "\n", "\n", "", "a", "=", "self", ".", "src_annotated_list", "[", "sid", "]", "\n", "span_list", "=", "a", ".", "span_list", "\n", "\n", "if", "self", ".", "args", ".", "verbosity", "==", "2", ":", "\n", "                    ", "print", "(", "\"Source tokens: \"", ",", "a", ".", "tokens", ")", "\n", "\n", "", "src_phrase_list", ".", "append", "(", "list", "(", ")", ")", "\n", "# Construct a list of entity phrases for the sentence Id", "\n", "# indicated by sid.", "\n", "for", "span", "in", "span_list", ":", "\n", "                    ", "phrase", "=", "\" \"", ".", "join", "(", "a", ".", "tokens", "[", "span", ".", "beg", ":", "span", ".", "end", "]", ")", "\n", "src_phrase_list", "[", "i", "]", ".", "append", "(", "phrase", ")", "\n", "if", "self", ".", "args", ".", "verbosity", "==", "2", ":", "\n", "                        ", "print", "(", "\"Beginning: %d, End: %d, Tag: %s\"", "%", "\n", "(", "span", ".", "beg", ",", "span", ".", "end", ",", "span", ".", "tag_type", ")", ",", "end", "=", "''", ")", "\n", "print", "(", "\"; Phrase: \"", ",", "phrase", ")", "\n", "\n", "# The complete list of all entity phrases in sentence sid.", "\n", "", "", "if", "self", ".", "args", ".", "verbosity", "==", "2", ":", "\n", "                    ", "print", "(", "\"Source phrase list: \"", ",", "src_phrase_list", "[", "i", "]", ")", "\n", "\n", "# Send the phrase list for translation if it is not empty.", "\n", "", "if", "src_phrase_list", "[", "i", "]", ":", "\n", "                    ", "tgt_phrase_list", ".", "append", "(", "get_google_translations", "(", "\n", "src_phrase_list", "[", "i", "]", ",", "src_lang_code", ",", "\n", "tgt_lang_code", ",", "self", ".", "args", ".", "api_key", ")", ")", "\n", "", "else", ":", "\n", "                    ", "tgt_phrase_list", ".", "append", "(", "list", "(", ")", ")", "\n", "\n", "# Store the source and target phrase lists obtained so far.", "\n", "", "with", "open", "(", "src_phrase_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                    ", "pickle", ".", "dump", "(", "src_phrase_list", ",", "f", ")", "\n", "", "with", "open", "(", "tgt_phrase_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                    ", "pickle", ".", "dump", "(", "tgt_phrase_list", ",", "f", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "verbosity", "==", "2", ":", "\n", "                    ", "print", "(", "\"Target phrase list: \"", ",", "tgt_phrase_list", "[", "i", "]", ")", "\n", "\n", "", "", "if", "self", ".", "args", ".", "verbosity", ">=", "1", ":", "\n", "                ", "print", "(", "\"########################################################\"", "\n", "\"##############\"", ")", "\n", "print", "(", "\"Source entities (phrases) translated.\"", ")", "\n", "\n", "# Store the final source and target entity phrase lists.", "\n", "", "with", "open", "(", "tgt_phrase_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "tgt_phrase_list", ",", "f", ")", "\n", "", "with", "open", "(", "src_phrase_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "src_phrase_list", ",", "f", ")", "\n", "\n", "# If trans_sent == 0, then simply load the target sentence list,", "\n", "# source phrase list and the target phrase list.", "\n", "", "", "else", ":", "\n", "            ", "tgt_sentence_list", "=", "pickle", ".", "load", "(", "open", "(", "tgt_sentence_path", ",", "'rb'", ")", ")", "\n", "tgt_phrase_list", "=", "pickle", ".", "load", "(", "open", "(", "tgt_phrase_path", ",", "'rb'", ")", ")", "\n", "src_phrase_list", "=", "pickle", ".", "load", "(", "open", "(", "src_phrase_path", ",", "'rb'", ")", ")", "\n", "\n", "", "self", ".", "sentence_ids", "=", "sentence_ids", "\n", "self", ".", "tgt_sentence_list", "=", "tgt_sentence_list", "\n", "self", ".", "tgt_phrase_list", "=", "tgt_phrase_list", "\n", "self", ".", "src_phrase_list", "=", "src_phrase_list", "\n", "\n", "for", "i", ",", "src_phrase", "in", "enumerate", "(", "self", ".", "src_phrase_list", ")", ":", "\n", "# The number of entity phrases should be the same in any given", "\n", "# source-target sentence pair.", "\n", "            ", "assert", "len", "(", "self", ".", "src_phrase_list", "[", "i", "]", ")", "==", "len", "(", "self", ".", "tgt_phrase_list", "[", "i", "]", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "verbosity", ">=", "1", ":", "\n", "            ", "print", "(", "\"############################################################\"", "\n", "\"##########\"", ")", "\n", "print", "(", "\"Target sentence list loaded. Length: %d\"", "%", "\n", "len", "(", "tgt_sentence_list", ")", ")", "\n", "print", "(", "\"Source entity (phrase) list loaded. Length: %d\"", "%", "\n", "len", "(", "src_phrase_list", ")", ")", "\n", "print", "(", "\"Target entity (phrase) list loaded. Length: %d\"", "%", "\n", "len", "(", "tgt_phrase_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.prepare_tgt_candidate_phrase_list": [[297, 391], ["list", "list", "os.path.join", "os.path.exists", "tmp.TMP._test_tgt_candidate_phrase_list", "pickle.load", "enumerate", "open", "pickle.load", "list.append", "list.append", "enumerate", "enumerate", "open", "pickle.dump", "open", "list", "tmp.TgtPhrases", "tmp.TgtPhrases.add_tgt_phrase", "tmp.TgtPhrases.add_tgt_phrase", "tgt_candidate_phrase_list[].append", "enumerate", "os.path.join", "dict", "list", "src_phrase.split.split.split", "enumerate", "[].add_tgt_phrase", "[].replace", "list", "src_token.lower", "temp[].append", "list", "temp[].append", "src_token.lower", "tgt_token.capitalize"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP._test_tgt_candidate_phrase_list", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TgtPhrases.add_tgt_phrase", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TgtPhrases.add_tgt_phrase", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TgtPhrases.add_tgt_phrase"], ["", "", "def", "prepare_tgt_candidate_phrase_list", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Prepare a set of candidate translations for every source entity phrase.\n        :return: Nothing.\n        \"\"\"", "\n", "lexicons", "=", "list", "(", ")", "\n", "tgt_candidate_phrase_list", "=", "list", "(", ")", "\n", "\n", "tgt_candidate_phrase_list_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_tgt_candidate_phrase_list.pkl\"", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "tgt_candidate_phrase_list_path", ")", ":", "\n", "            ", "self", ".", "tgt_candidate_phrase_list", "=", "pickle", ".", "load", "(", "\n", "open", "(", "tgt_candidate_phrase_list_path", ",", "'rb'", ")", ")", "\n", "\n", "", "else", ":", "\n", "            ", "for", "lexicon_file_name", "in", "config", ".", "LEXICON_FILE_NAMES", ":", "\n", "                ", "lexicon", "=", "pickle", ".", "load", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\n", "lexicon_file_name", "+", "\".pkl\"", ")", ",", "\"rb\"", ")", ")", "\n", "lexicons", ".", "append", "(", "lexicon", ")", "\n", "\n", "# For every sentence", "\n", "", "for", "i", ",", "src_phrase_sent", "in", "enumerate", "(", "self", ".", "src_phrase_list", ")", ":", "\n", "                ", "tgt_candidate_phrase_list", ".", "append", "(", "list", "(", ")", ")", "\n", "# For every entity phrase in the i'th sentence", "\n", "for", "j", ",", "src_phrase", "in", "enumerate", "(", "src_phrase_sent", ")", ":", "\n", "                    ", "tgt_phrase_obj", "=", "TgtPhrases", "(", "src_phrase", ")", "\n", "# Translation from Google", "\n", "tgt_phrase_obj", ".", "add_tgt_phrase", "(", "\n", "[", "self", ".", "tgt_phrase_list", "[", "i", "]", "[", "j", "]", ".", "replace", "(", "\"&#39;\"", ",", "\"\\'\"", ")", "]", ")", "\n", "# Copy", "\n", "tgt_phrase_obj", ".", "add_tgt_phrase", "(", "[", "src_phrase", "]", ")", "\n", "tgt_candidate_phrase_list", "[", "i", "]", ".", "append", "(", "tgt_phrase_obj", ")", "\n", "\n", "# Translations from each lexicon", "\n", "", "for", "k", ",", "lexicon", "in", "enumerate", "(", "lexicons", ")", ":", "\n", "# For each source entity phrase", "\n", "                    ", "for", "l", ",", "src_phrase", "in", "enumerate", "(", "src_phrase_sent", ")", ":", "\n", "                        ", "temp", "=", "dict", "(", ")", "\n", "tgt_phrase", "=", "list", "(", ")", "\n", "\n", "# src_phrase expressed as a list of tokens", "\n", "#", "\n", "# Get a list of all possible translations of the", "\n", "# source entity phrase from this lexicon. This lexicon", "\n", "# can, in general, be a one-to-many map from tokens", "\n", "# to one or many tokens or phrases.", "\n", "#", "\n", "# E.g., A B is the source entity phrase,", "\n", "# where A and B are tokens. Suppose there is an entry", "\n", "# A -> [a1, a2, a3] in the lexicon and another entry", "\n", "# B -> [b1, b2]. Then, we add the mapping", "\n", "# A B -> [a1b1, a1b2, a2b1, ..., a3b2] to our", "\n", "# list of candidate translations of the source entity", "\n", "# phrase \"A B\" in the target language.", "\n", "src_phrase", "=", "src_phrase", ".", "split", "(", "\" \"", ")", "\n", "for", "m", ",", "src_token", "in", "enumerate", "(", "src_phrase", ")", ":", "\n", "                            ", "temp", "[", "m", "]", "=", "list", "(", ")", "\n", "if", "src_token", ".", "lower", "(", ")", "in", "lexicon", ":", "\n", "                                ", "for", "tgt_token", "in", "lexicon", "[", "src_token", ".", "lower", "(", ")", "]", ":", "\n", "                                    ", "temp", "[", "m", "]", ".", "append", "(", "tgt_token", ".", "capitalize", "(", ")", ")", "\n", "", "", "else", ":", "\n", "# If A -> [], but B -> [b1, b2], we construct", "\n", "# the map A B -> [Ab1, Ab2], rather than", "\n", "# A B -> [], because the second step of MATCH (", "\n", "# first is what we are doing here: constructing", "\n", "# a list of candidate target translations)", "\n", "# involves token-level matching and if we find", "\n", "# a match for B, we improve our chances of", "\n", "# finding a match for A B, despite A not", "\n", "# having an entry in the lexicon.", "\n", "                                ", "temp", "[", "m", "]", ".", "append", "(", "src_token", ")", "\n", "\n", "", "", "for", "key", "in", "temp", ":", "\n", "                            ", "if", "tgt_phrase", "==", "list", "(", ")", ":", "\n", "                                ", "tgt_phrase", "=", "temp", "[", "key", "]", "\n", "", "else", ":", "\n", "# Successively add token-level translations", "\n", "# to populate the tgt_phrase list.", "\n", "                                ", "tgt_phrase", "=", "[", "phrase_1", "+", "\" \"", "+", "phrase_2", "for", "\n", "phrase_1", "in", "tgt_phrase", "\n", "for", "phrase_2", "in", "temp", "[", "key", "]", "]", "\n", "\n", "", "", "tgt_candidate_phrase_list", "[", "i", "]", "[", "l", "]", ".", "add_tgt_phrase", "(", "\n", "tgt_phrase", ")", "\n", "\n", "", "", "", "with", "open", "(", "tgt_candidate_phrase_list_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "tgt_candidate_phrase_list", ",", "f", ")", "\n", "\n", "", "self", ".", "tgt_candidate_phrase_list", "=", "tgt_candidate_phrase_list", "\n", "\n", "", "self", ".", "_test_tgt_candidate_phrase_list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP._test_tgt_candidate_phrase_list": [[392, 404], ["len", "len"], "methods", ["None"], ["", "def", "_test_tgt_candidate_phrase_list", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        A very basic test to ensure that every element of the\n        tgt_candidate_phrase_list (i.e., a list of TgtPhrases objects) has\n        objects whose tgt_phrase_list field as 2 (1 Google Translate, 1 copy)\n        + #lexicon items.\n        :return: Nothing.\n        \"\"\"", "\n", "for", "tgt_phrase_object_list", "in", "self", ".", "tgt_candidate_phrase_list", ":", "\n", "            ", "for", "tpobj", "in", "tgt_phrase_object_list", ":", "\n", "                ", "assert", "len", "(", "tpobj", ".", "tgt_phrase_list", ")", "==", "len", "(", "\n", "config", ".", "LEXICON_FILE_NAMES", ")", "+", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_tgt_annotations_new": [[405, 478], ["os.path.join", "os.path.exists", "os.path.join", "os.path.exists", "os.path.join", "os.path.exists", "pickle.load", "enumerate", "pickle.load", "tmp.TMP._get_candidates_list", "tmp.TMP.get_potential_matches", "tmp.TMP.get_partial_tags", "pickle.load", "print", "tmp.TMP.get_final_tags", "open", "sentence.replace.replace.replace", "sentence.replace.replace.replace", "src.util.data_processing.Annotation", "tmp.get_clean_tokens", "tmp.TMP.tgt_annotated_list.append", "open", "pickle.dump", "open", "open", "pickle.dump", "open", "open", "pickle.dump", "enumerate", "print", "print", "print", "token.endswith", "token.rstrip", "src.util.data_processing.Annotation.tokens.insert", "str", "str"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP._get_candidates_list", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_potential_matches", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_partial_tags", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_final_tags", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_clean_tokens"], ["", "", "", "def", "get_tgt_annotations_new", "(", "self", ")", ":", "\n", "# Target language tokens (obtained through Google Translate + cleaned)", "\n", "# These tokens do not have tags yet, which is what would be done in", "\n", "# this function.", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_tgt_annotated_list_\"", "+", "self", ".", "suffix", "+", "\n", "\".pkl\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "self", ".", "tgt_annotated_list", "=", "pickle", ".", "load", "(", "open", "(", "path", ",", "\"rb\"", ")", ")", "\n", "\n", "", "else", ":", "\n", "            ", "for", "i", ",", "sentence", "in", "enumerate", "(", "self", ".", "tgt_sentence_list", ")", ":", "\n", "                ", "sentence", "=", "sentence", ".", "replace", "(", "\"&#39;\"", ",", "\"\\'\"", ")", "\n", "sentence", "=", "sentence", ".", "replace", "(", "\" &amp; \"", ",", "\"&\"", ")", "\n", "tgt_annotation", "=", "data_processing", ".", "Annotation", "(", ")", "\n", "tgt_annotation", ".", "tokens", "=", "get_clean_tokens", "(", "sentence", ",", "\n", "self", ".", "use_corenlp", ")", "\n", "\n", "if", "self", ".", "tgt_lang", "==", "\"hi\"", ":", "\n", "# Modifying the end of sentence punctuation mark for", "\n", "# Hindi, to ensure that the train and test files match.", "\n", "                    ", "for", "j", ",", "token", "in", "enumerate", "(", "tgt_annotation", ".", "tokens", ")", ":", "\n", "                        ", "if", "token", ".", "endswith", "(", "\"\u0964\"", ")", ":", "\n", "                            ", "tgt_annotation", ".", "tokens", "[", "j", "]", "=", "token", ".", "rstrip", "(", "\"\u0964\"", ")", "\n", "if", "tgt_annotation", ".", "tokens", "[", "j", "]", "==", "\"\"", ":", "\n", "                                ", "tgt_annotation", ".", "tokens", "[", "j", "]", "=", "\".\"", "\n", "", "else", ":", "\n", "                                ", "tgt_annotation", ".", "tokens", ".", "insert", "(", "j", "+", "1", ",", "\".\"", ")", "\n", "", "", "", "", "if", "self", ".", "args", ".", "verbosity", ">=", "1", ":", "\n", "                    ", "print", "(", "\"####################################################\"", "\n", "\"##################\"", ")", "\n", "print", "(", "\"Sentence-%d: %s\"", "%", "(", "i", ",", "sentence", ")", ")", "\n", "print", "(", "\"Tokens: \"", ",", "tgt_annotation", ".", "tokens", ")", "\n", "\n", "", "self", ".", "tgt_annotated_list", ".", "append", "(", "tgt_annotation", ")", "\n", "\n", "", "with", "open", "(", "path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "tgt_annotated_list", ",", "f", ")", "\n", "\n", "", "", "temp_suffix", "=", "self", ".", "args", ".", "ablation_string", "\n", "\n", "# Get partial NER tags first.", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_annotated_list_\"", "+", "\n", "str", "(", "self", ".", "args", ".", "matching_score_threshold", ")", "+", "\n", "temp_suffix", "+", "self", ".", "suffix", "\n", "+", "self", ".", "date_today", "+", "\"_partial\"", "+", "\".pkl\"", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "self", ".", "tgt_annotated_list", "=", "pickle", ".", "load", "(", "open", "(", "path", ",", "\"rb\"", ")", ")", "\n", "\n", "", "else", ":", "\n", "            ", "candidates_list", "=", "self", ".", "_get_candidates_list", "(", ")", "\n", "potential_matches", "=", "self", ".", "get_potential_matches", "(", "candidates_list", ")", "\n", "self", ".", "get_partial_tags", "(", "potential_matches", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "tgt_annotated_list", ",", "f", ")", "\n", "\n", "", "", "temp_suffix", "=", "self", ".", "args", ".", "ablation_string", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "self", ".", "args", ".", "translate_fname", "+", "\n", "\"_annotated_list_\"", "+", "\n", "str", "(", "self", ".", "args", ".", "matching_score_threshold", ")", "+", "\n", "temp_suffix", "+", "self", ".", "suffix", "+", "self", ".", "date_today", "+", "\n", "\".pkl\"", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "self", ".", "tgt_annotated_list", "=", "pickle", ".", "load", "(", "open", "(", "path", ",", "\"rb\"", ")", ")", "\n", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Final tags unavailable. Getting them...\"", ")", "\n", "self", ".", "get_final_tags", "(", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "tgt_annotated_list", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP._get_candidates_list": [[479, 556], ["list", "print", "print", "print", "print", "print", "enumerate", "print", "print", "print", "list.append", "list", "enumerate", "print", "print", "print", "candidates_list[].append", "enumerate", "list", "[].append", "enumerate", "list", "print", "print", "print", "tgt_phrase.split", "[].append", "enumerate", "print", "len", "list", "print", "[].append"], "methods", ["None"], ["", "", "", "def", "_get_candidates_list", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        :return: List of candidate token-level translations for all source entity\n        # phrases in all source sentences.\n        \"\"\"", "\n", "# candidates_list = [c1, c2, ...., cn], where n is the no. of sentences.", "\n", "# c_i = [t_i1, t_i2, ...], where t_ij, j=1, 2, 3, ..., is the list", "\n", "# of all candidate token-level translations corresponding to the", "\n", "# source entity phrase j in sentence i. Finally, t_ij = [tok_ij1,", "\n", "# tok_ij2, ...], where tok_ijk, k=1, 2, 3, ..., is the k'th token-level", "\n", "# translation corresponding to entity phrase j in sentence i.", "\n", "candidates_list", "=", "list", "(", ")", "\n", "\n", "print", "(", "\"###############################################################\"", ")", "\n", "print", "(", "\"###############################################################\"", ")", "\n", "print", "(", "\"Step-1: Getting candidate tags.\"", ")", "\n", "print", "(", "\"###############################################################\"", ")", "\n", "print", "(", "\"###############################################################\"", ")", "\n", "\n", "for", "i", ",", "sid", "in", "enumerate", "(", "self", ".", "sentence_ids", ")", ":", "\n", "            ", "src_a", "=", "self", ".", "src_annotated_list", "[", "sid", "]", "\n", "src_phrases", "=", "self", ".", "src_phrase_list", "[", "sid", "]", "\n", "\n", "print", "(", "\"###########################################################\"", ")", "\n", "print", "(", "\"Sentence ID: \"", ",", "sid", ")", "\n", "print", "(", "\"###########################################################\"", ")", "\n", "\n", "candidates_list", ".", "append", "(", "list", "(", ")", ")", "\n", "\n", "if", "src_phrases", ":", "\n", "                ", "for", "j", ",", "src_phrase", "in", "enumerate", "(", "src_phrases", ")", ":", "\n", "                    ", "print", "(", "\"###################################################\"", ")", "\n", "print", "(", "\"Source phrase: \"", ",", "src_phrase", ")", "\n", "print", "(", "\"###################################################\"", ")", "\n", "\n", "candidates_list", "[", "i", "]", ".", "append", "(", "list", "(", ")", ")", "\n", "\n", "all_tgt_phrases", "=", "self", ".", "tgt_candidate_phrase_list", "[", "sid", "]", "[", "j", "]", ".", "tgt_phrase_list", "\n", "if", "self", ".", "args", ".", "ablation_string", "==", "\"_no_idc_\"", ":", "\n", "# Assuming idc lexicon is the last one in the", "\n", "# tgt_candidate_phrase_list.", "\n", "                        ", "all_tgt_phrases", "=", "all_tgt_phrases", "[", ":", "-", "1", "]", "\n", "", "elif", "self", ".", "args", ".", "ablation_string", "==", "\"_no_gold_\"", ":", "\n", "# Assuming gold lexicon is second last.", "\n", "                        ", "all_tgt_phrases", "=", "all_tgt_phrases", "[", ":", "-", "2", "]", "\n", "", "elif", "self", ".", "args", ".", "ablation_string", "==", "\"_no_copy_\"", "or", "self", ".", "args", ".", "ablation_string", "==", "\"_no_phonetic_\"", ":", "\n", "# Assuming", "\n", "                        ", "all_tgt_phrases", "=", "all_tgt_phrases", "[", ":", "-", "3", "]", "\n", "", "elif", "self", ".", "args", ".", "ablation_string", "==", "\"_no_google_\"", "or", "self", ".", "args", ".", "ablation_string", "==", "\"_no_google_v2_\"", ":", "\n", "                        ", "all_tgt_phrases", "=", "all_tgt_phrases", "[", "1", ":", "]", "\n", "\n", "", "for", "k", ",", "tgt_phrases", "in", "enumerate", "(", "all_tgt_phrases", ")", ":", "\n", "                        ", "candidates_list", "[", "i", "]", "[", "j", "]", ".", "append", "(", "list", "(", ")", ")", "\n", "for", "l", ",", "tgt_phrase", "in", "enumerate", "(", "tgt_phrases", ")", ":", "\n", "                            ", "if", "l", "==", "MAX_ENTITY_PHRASE_TRANSLATIONS", ":", "\n", "                                ", "break", "\n", "", "print", "(", "src_a", ".", "tokens", ")", "\n", "\n", "for", "span", "in", "src_a", ".", "span_list", ":", "\n", "                                ", "print", "(", "span", ".", "beg", ",", "span", ".", "end", ")", "\n", "\n", "", "print", "(", "len", "(", "src_a", ".", "span_list", ")", ")", "\n", "print", "(", "\"Index: \"", ",", "j", ")", "\n", "tag_type", "=", "src_a", ".", "span_list", "[", "j", "]", ".", "tag_type", "\n", "tgt_tokens", "=", "tgt_phrase", ".", "split", "(", "\" \"", ")", "\n", "\n", "candidates_list", "[", "i", "]", "[", "j", "]", "[", "k", "]", ".", "append", "(", "list", "(", ")", ")", "\n", "for", "m", ",", "tgt_token", "in", "enumerate", "(", "tgt_tokens", ")", ":", "\n", "                                ", "if", "m", "==", "0", ":", "\n", "                                    ", "ner_tag", "=", "\"B-\"", "+", "tag_type", "\n", "", "else", ":", "\n", "                                    ", "ner_tag", "=", "\"I-\"", "+", "tag_type", "\n", "", "print", "(", "\"Target token: \"", ",", "tgt_token", ")", "\n", "candidates_list", "[", "i", "]", "[", "j", "]", "[", "k", "]", "[", "l", "]", ".", "append", "(", "(", "tgt_token", ",", "ner_tag", ")", ")", "\n", "", "", "", "", "", "", "return", "candidates_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_potential_matches": [[557, 615], ["print", "print", "print", "print", "print", "list", "enumerate", "dict", "list.append", "list", "print", "enumerate", "enumerate", "enumerate", "enumerate", "print", "print", "dict", "enumerate", "enumerate", "enumerate", "list", "enumerate", "list", "set", "tmp._find_match", "temp_matches[].append", "map", "max", "zip", "enumerate", "tgt_matches[].add"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._find_match"], ["", "def", "get_potential_matches", "(", "self", ",", "candidates_list", ")", ":", "\n", "        ", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Step-2: Getting all possible potential matches.\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "potential_matches", "=", "list", "(", ")", "\n", "for", "i", ",", "sid", "in", "enumerate", "(", "self", ".", "sentence_ids", ")", ":", "\n", "            ", "tgt_matches", "=", "dict", "(", ")", "\n", "sent_candidates", "=", "candidates_list", "[", "i", "]", "\n", "if", "sent_candidates", "==", "list", "(", ")", ":", "\n", "                ", "print", "(", "\"%d: \"", "%", "i", ",", "sent_candidates", ")", "\n", "", "else", ":", "\n", "                ", "for", "j", ",", "phrase_candidates", "in", "enumerate", "(", "sent_candidates", ")", ":", "\n", "                    ", "for", "k", ",", "diff_phrase_candidates", "in", "enumerate", "(", "phrase_candidates", ")", ":", "\n", "                        ", "for", "l", ",", "all_candidates", "in", "enumerate", "(", "diff_phrase_candidates", ")", ":", "\n", "                            ", "for", "m", ",", "candidate", "in", "enumerate", "(", "all_candidates", ")", ":", "\n", "                                ", "print", "(", "\"%d.%d.%d.%d.%d Candidate token: %s\"", "%", "(", "i", ",", "j", ",", "k", ",", "l", ",", "m", ",", "candidate", "[", "0", "]", ")", ",", "end", "=", "''", ")", "\n", "print", "(", "\", Candidate tag: \"", ",", "candidate", "[", "1", "]", ")", "\n", "\n", "tgt_a", "=", "self", ".", "tgt_annotated_list", "[", "sid", "]", "\n", "\n", "temp_matches", "=", "dict", "(", ")", "\n", "for", "o", ",", "matching_algo", "in", "enumerate", "(", "config", ".", "MATCHING_ALGOS", ")", ":", "\n", "                                    ", "temp_matches", "[", "matching_algo", "]", "=", "list", "(", ")", "\n", "\n", "", "for", "n", ",", "reference_token", "in", "enumerate", "(", "tgt_a", ".", "tokens", ")", ":", "\n", "                                    ", "if", "n", "not", "in", "tgt_matches", ":", "\n", "                                        ", "tgt_matches", "[", "n", "]", "=", "set", "(", ")", "\n", "", "for", "o", ",", "matching_algo", "in", "enumerate", "(", "config", ".", "MATCHING_ALGOS", ")", ":", "\n", "\n", "                                        ", "if", "o", "==", "0", ":", "\n", "                                            ", "transliterate", "=", "False", "\n", "", "else", ":", "\n", "                                            ", "transliterate", "=", "True", "\n", "\n", "", "if", "self", ".", "args", ".", "ablation_string", "==", "\"_no_phonetic_\"", "and", "o", "==", "1", ":", "\n", "                                            ", "transliterate", "=", "False", "\n", "\n", "", "epi", "=", "self", ".", "epi", "\n", "\n", "x", "=", "_find_match", "(", "reference_token", ",", "candidate", "[", "0", "]", ",", "epi", ",", "\n", "self", ".", "stop_word_list", ",", "transliterate", "=", "transliterate", ")", "\n", "temp_matches", "[", "matching_algo", "]", ".", "append", "(", "x", ")", "\n", "\n", "", "", "for", "o", ",", "matching_algo", "in", "enumerate", "(", "config", ".", "MATCHING_ALGOS", ")", ":", "\n", "                                    ", "temp_match", "=", "temp_matches", "[", "matching_algo", "]", "\n", "temp_match", "=", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "temp_match", ")", ")", ")", "\n", "if", "True", "in", "temp_match", "[", "1", "]", ":", "\n", "                                        ", "max_score", "=", "max", "(", "temp_match", "[", "0", "]", ")", "\n", "max_indices", "=", "[", "ind", "for", "ind", ",", "score", "in", "enumerate", "(", "temp_match", "[", "0", "]", ")", "\n", "if", "score", "==", "max_score", "]", "\n", "for", "index", "in", "max_indices", ":", "\n", "                                            ", "if", "max_score", ">=", "self", ".", "args", ".", "matching_score_threshold", ":", "\n", "                                                ", "tgt_matches", "[", "index", "]", ".", "add", "(", "(", "j", ",", "k", ",", "o", ",", "max_score", ",", "candidate", "[", "1", "]", ")", ")", "\n", "\n", "", "", "", "", "", "", "", "", "", "potential_matches", ".", "append", "(", "tgt_matches", ")", "\n", "", "return", "potential_matches", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_partial_tags": [[616, 664], ["print", "print", "print", "print", "print", "enumerate", "print", "print", "print", "potential_match_sent.items", "dict", "print", "list", "list", "map", "list", "tgt_a.ner_tags.append", "list.sort", "set", "set", "list", "list", "list", "list", "enumerate", "tgt_a.ner_tags.append", "print", "zip", "set", "set", "list.append", "list.append", "list.append", "list.append", "list.append", "list.append"], "methods", ["None"], ["", "def", "get_partial_tags", "(", "self", ",", "potential_matches", ")", ":", "\n", "        ", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Step-3: Getting a minimal set of potential matches.\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "for", "i", ",", "sid", "in", "enumerate", "(", "self", ".", "sentence_ids", ")", ":", "\n", "            ", "potential_match_sent", "=", "potential_matches", "[", "i", "]", "\n", "tgt_a", "=", "self", ".", "tgt_annotated_list", "[", "sid", "]", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Sentence ID: \"", ",", "sid", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "\n", "if", "potential_match_sent", "==", "dict", "(", ")", ":", "\n", "                ", "tgt_a", ".", "ner_tags", "=", "[", "config", ".", "OUTSIDE_TAG", "for", "_", "in", "tgt_a", ".", "tokens", "]", "\n", "\n", "", "for", "key", ",", "val", "in", "potential_match_sent", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "\"Key: \"", ",", "tgt_a", ".", "tokens", "[", "key", "]", ",", "\" Value: \"", ",", "val", ")", "\n", "val", "=", "list", "(", "val", ")", "\n", "all_vals", "=", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "val", ")", ")", ")", "\n", "if", "all_vals", "==", "list", "(", ")", ":", "\n", "                    ", "tgt_a", ".", "ner_tags", ".", "append", "(", "config", ".", "OUTSIDE_TAG", ")", "\n", "", "else", ":", "\n", "                    ", "val", ".", "sort", "(", "key", "=", "lambda", "a", ":", "(", "-", "a", "[", "3", "]", ",", "a", "[", "1", "]", ",", "a", "[", "2", "]", ")", ")", "\n", "\n", "phrases", "=", "set", "(", "all_vals", "[", "0", "]", ")", "\n", "tags", "=", "set", "(", "all_vals", "[", "-", "1", "]", ")", "\n", "\n", "new_vals", "=", "list", "(", ")", "\n", "phrase_added", "=", "list", "(", ")", "\n", "tag_added", "=", "list", "(", ")", "\n", "ind_added", "=", "list", "(", ")", "\n", "\n", "for", "t", ",", "v", "in", "enumerate", "(", "val", ")", ":", "\n", "                        ", "for", "phrase", "in", "set", "(", "phrases", ")", ":", "\n", "                            ", "if", "(", "phrase", "not", "in", "phrase_added", ")", "and", "(", "v", "[", "0", "]", "==", "phrase", ")", ":", "\n", "                                ", "new_vals", ".", "append", "(", "v", ")", "\n", "phrase_added", ".", "append", "(", "phrase", ")", "\n", "ind_added", ".", "append", "(", "t", ")", "\n", "\n", "", "", "for", "tag", "in", "set", "(", "tags", ")", ":", "\n", "                            ", "if", "(", "tag", "not", "in", "tag_added", ")", "and", "(", "t", "not", "in", "ind_added", ")", "and", "(", "v", "[", "-", "1", "]", "==", "tag", ")", ":", "\n", "                                ", "new_vals", ".", "append", "(", "v", ")", "\n", "tag_added", ".", "append", "(", "tag", ")", "\n", "ind_added", ".", "append", "(", "t", ")", "\n", "\n", "", "", "", "tgt_a", ".", "ner_tags", ".", "append", "(", "new_vals", ")", "\n", "print", "(", "\"Final tag: \"", ",", "new_vals", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_final_tags": [[665, 887], ["print", "print", "print", "print", "print", "collections.Counter", "dict", "dict", "dict", "enumerate", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "print", "print", "print", "tmp.find_entity_spans", "dict", "print", "print", "print", "enumerate", "print", "len", "logging.info", "logging.info", "list", "dict.items", "logging.info", "logging.info", "os.path.join", "os.path.join", "os.path.join", "tmp.TMP.get_refined_candidates", "tmp.TMP.tag_problematic_sentences", "logging.info", "logging.info", "copy.deepcopy", "tmp.get_spans", "set", "str", "str", "str", "str", "open", "pickle.dump", "open", "pickle.dump", "open", "pickle.dump", "range", "problematic_sentences[].append", "collections.Counter.update", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "print", "print", "print", "set", "enumerate", "copy.deepcopy", "print", "list", "enumerate", "float", "enumerate", "dict", "min", "list", "enumerate", "len", "str", "problem_children[].items", "logging.info", "logging.info", "list.append", "len", "len", "str", "len", "tok.lower", "list", "len", "list", "collections.Counter", "problem_children[].update", "set.update", "len", "print", "float", "print", "min", "range", "src_phrases[].split", "str", "str", "str", "str", "len", "copy.deepcopy", "start_list.append", "end_list.append", "len", "print", "print", "nltk.edit_distance", "min", "score_match[].keys", "k.lower", "math.log", "str", "len", "itertools.permutations", "copy.deepcopy.update", "list.append", "tgt_phrase.lower", "possible_translation.lower", "dict", "score_match[].pop", "score_match[].values", "float", "problem_children[].most_common", "tgt_phrase.split", "k.lower"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.find_entity_spans", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_refined_candidates", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.tag_problematic_sentences", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_spans"], ["", "", "", "", "def", "get_final_tags", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Step-4: Getting final annotations.\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "problematic_entities", "=", "Counter", "(", ")", "\n", "src_entities", "=", "0", "\n", "occur_problem_entities", "=", "0", "\n", "problematic_sentences", "=", "dict", "(", ")", "\n", "problem_children", "=", "dict", "(", ")", "\n", "idf", "=", "dict", "(", ")", "\n", "\n", "for", "i", ",", "sid", "in", "enumerate", "(", "self", ".", "sentence_ids", ")", ":", "\n", "            ", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Sentence ID: \"", ",", "sid", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "tgt_a", "=", "self", ".", "tgt_annotated_list", "[", "sid", "]", "\n", "src_a", "=", "self", ".", "src_annotated_list", "[", "sid", "]", "\n", "src_phrases", "=", "self", ".", "src_phrase_list", "[", "sid", "]", "\n", "ner_tag_list", "=", "tgt_a", ".", "ner_tags", "\n", "\n", "dummy_list", "=", "[", "-", "1", "for", "_", "in", "ner_tag_list", "]", "\n", "\n", "entity_candidates", "=", "{", "j", ":", "copy", ".", "deepcopy", "(", "dummy_list", ")", "for", "j", "in", "range", "(", "len", "(", "src_phrases", ")", ")", "}", "\n", "find_entity_spans", "(", "entity_candidates", ",", "ner_tag_list", ")", "\n", "\n", "score_match", "=", "dict", "(", ")", "\n", "for", "j", "in", "entity_candidates", ":", "\n", "                ", "src_entities", "+=", "1", "\n", "span_list", "=", "get_spans", "(", "j", ",", "entity_candidates", "[", "j", "]", ")", "\n", "\n", "tok_set", "=", "set", "(", "[", "tok", ".", "lower", "(", ")", "for", "tok", "in", "tgt_a", ".", "tokens", "]", ")", "\n", "for", "tok", "in", "tok_set", ":", "\n", "                    ", "if", "tok", "not", "in", "idf", ":", "\n", "                        ", "idf", "[", "tok", "]", "=", "0", "\n", "", "idf", "[", "tok", "]", "+=", "1", "\n", "\n", "", "if", "span_list", "==", "list", "(", ")", "or", "len", "(", "src_phrases", "[", "j", "]", ".", "split", "(", "\" \"", ")", ")", ">=", "10", ":", "\n", "                    ", "occur_problem_entities", "+=", "1", "\n", "if", "i", "not", "in", "problematic_sentences", ":", "\n", "                        ", "problematic_sentences", "[", "i", "]", "=", "list", "(", ")", "\n", "", "problematic_sentences", "[", "i", "]", ".", "append", "(", "(", "j", ",", "src_phrases", "[", "j", "]", ")", ")", "\n", "\n", "problematic_entities", ".", "update", "(", "{", "src_phrases", "[", "j", "]", ":", "1", "}", ")", "\n", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"No correspondence found in sentence: \"", "+", "str", "(", "sid", ")", ")", "\n", "logging", ".", "info", "(", "\"Source tokens: \"", "+", "str", "(", "src_a", ".", "tokens", ")", ")", "\n", "logging", ".", "info", "(", "\"Target tokens: \"", "+", "str", "(", "tgt_a", ".", "tokens", ")", ")", "\n", "logging", ".", "info", "(", "\"Source phrase for which no match found: \"", "+", "str", "(", "src_phrases", "[", "j", "]", ")", ")", "\n", "\n", "if", "src_phrases", "[", "j", "]", "not", "in", "problem_children", ":", "\n", "                        ", "problem_children", "[", "src_phrases", "[", "j", "]", "]", "=", "Counter", "(", ")", "\n", "\n", "", "for", "tok", "in", "tgt_a", ".", "tokens", ":", "\n", "                        ", "problem_children", "[", "src_phrases", "[", "j", "]", "]", ".", "update", "(", "{", "tok", ":", "1", "}", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Source phrase: \"", ",", "src_phrases", "[", "j", "]", ")", "\n", "print", "(", "\"######################################################################\"", ")", "\n", "temp_tgt_phrases", "=", "set", "(", ")", "\n", "all_tgt_phrases", "=", "self", ".", "tgt_candidate_phrase_list", "[", "sid", "]", "[", "j", "]", ".", "tgt_phrase_list", "\n", "for", "k", ",", "candidate_phrases", "in", "enumerate", "(", "all_tgt_phrases", ")", ":", "\n", "                        ", "temp_tgt_phrases", ".", "update", "(", "candidate_phrases", ")", "\n", "\n", "", "final_tgt_phrases", "=", "copy", ".", "deepcopy", "(", "temp_tgt_phrases", ")", "\n", "\n", "if", "self", ".", "args", ".", "ablation_string", "!=", "\"_no_google_v2_\"", ":", "\n", "                        ", "for", "tgt_phrase", "in", "temp_tgt_phrases", ":", "\n", "                            ", "if", "len", "(", "final_tgt_phrases", ")", ">", "self", ".", "args", ".", "max_set_size", ":", "\n", "                                ", "break", "\n", "", "if", "tgt_phrase", "!=", "all_tgt_phrases", "[", "0", "]", "[", "0", "]", ":", "\n", "                                ", "permutations", "=", "itertools", ".", "permutations", "(", "tgt_phrase", ".", "split", "(", "\" \"", ")", ")", "\n", "final_tgt_phrases", ".", "update", "(", "[", "\" \"", ".", "join", "(", "x", ")", "for", "x", "in", "permutations", "]", ")", "\n", "\n", "", "", "if", "len", "(", "final_tgt_phrases", ")", ">", "2", "*", "self", ".", "args", ".", "max_set_size", ":", "\n", "                            ", "final_tgt_phrases", "=", "copy", ".", "deepcopy", "(", "temp_tgt_phrases", ")", "\n", "\n", "", "", "print", "(", "\"All source candidate phrases: \"", ",", "len", "(", "final_tgt_phrases", ")", ")", "\n", "\n", "new_span_list", "=", "list", "(", ")", "\n", "for", "l", ",", "span", "in", "enumerate", "(", "span_list", ")", ":", "\n", "                        ", "start", ",", "end", "=", "span", "\n", "orig_start", "=", "start", "\n", "orig_end", "=", "end", "\n", "\n", "start_list", "=", "[", "orig_start", "]", "\n", "while", "tgt_a", ".", "tokens", "[", "start", "]", "in", "self", ".", "stop_word_list", ":", "\n", "                            ", "start", "+=", "1", "\n", "if", "start", "==", "orig_end", ":", "\n", "                                ", "break", "\n", "", "start_list", ".", "append", "(", "start", ")", "\n", "\n", "", "end_list", "=", "[", "orig_end", "]", "\n", "while", "tgt_a", ".", "tokens", "[", "end", "-", "1", "]", "in", "self", ".", "stop_word_list", ":", "\n", "                            ", "end", "-=", "1", "\n", "if", "end", "==", "orig_start", ":", "\n", "                                ", "break", "\n", "", "end_list", ".", "append", "(", "end", ")", "\n", "\n", "", "for", "s", "in", "start_list", ":", "\n", "                            ", "for", "e", "in", "end_list", ":", "\n", "                                ", "new_span_list", ".", "append", "(", "(", "s", ",", "e", ")", ")", "\n", "\n", "", "", "", "best_score", "=", "float", "(", "\"inf\"", ")", "\n", "\n", "for", "l", ",", "span", "in", "enumerate", "(", "new_span_list", ")", ":", "\n", "                        ", "tgt_phrase", "=", "\" \"", ".", "join", "(", "tgt_a", ".", "tokens", "[", "span", "[", "0", "]", ":", "span", "[", "1", "]", "]", ")", "\n", "\n", "print", "(", "\"Target candidate phrase: \"", ",", "tgt_phrase", ")", "\n", "\n", "score", "=", "float", "(", "\"inf\"", ")", "\n", "best_possible_translation", "=", "\"\"", "\n", "\n", "if", "len", "(", "final_tgt_phrases", ")", "<", "100", ":", "\n", "                            ", "print", "(", "\"Final target phrases: \"", ",", "final_tgt_phrases", ")", "\n", "", "else", ":", "\n", "                            ", "print", "(", "\"Size of target phrase set more than 100.\"", ")", "\n", "\n", "", "for", "possible_translation", "in", "final_tgt_phrases", ":", "\n", "                            ", "edit_dist", "=", "nltk", ".", "edit_distance", "(", "tgt_phrase", ".", "lower", "(", ")", ",", "possible_translation", ".", "lower", "(", ")", ")", "\n", "score", "=", "min", "(", "score", ",", "edit_dist", ")", "\n", "if", "score", "==", "edit_dist", ":", "\n", "                                ", "best_possible_translation", "=", "possible_translation", "\n", "\n", "", "", "print", "(", "\"Score: \"", ",", "score", ")", "\n", "\n", "best_score", "=", "min", "(", "best_score", ",", "score", ")", "\n", "if", "best_score", "==", "score", ":", "\n", "                            ", "if", "span", "not", "in", "score_match", ":", "\n", "                                ", "score_match", "[", "span", "]", "=", "dict", "(", ")", "\n", "", "score_match", "[", "span", "]", "[", "j", "]", "=", "(", "best_score", ",", "best_possible_translation", ")", "\n", "\n", "", "", "for", "span", "in", "score_match", ":", "\n", "                        ", "if", "j", "in", "score_match", "[", "span", "]", ":", "\n", "                            ", "if", "score_match", "[", "span", "]", "[", "j", "]", "[", "0", "]", ">", "best_score", ":", "\n", "                                ", "score_match", "[", "span", "]", ".", "pop", "(", "j", ")", "\n", "\n", "", "", "", "", "", "print", "(", "\"Final score match: \"", ",", "score_match", ")", "\n", "for", "span", "in", "score_match", ":", "\n", "                ", "if", "score_match", "[", "span", "]", "!=", "dict", "(", ")", ":", "\n", "# max_score = max([v[0] for v in score_match[span].values()])", "\n", "                    ", "max_score", "=", "min", "(", "[", "v", "[", "0", "]", "for", "v", "in", "score_match", "[", "span", "]", ".", "values", "(", ")", "]", ")", "\n", "max_indices", "=", "[", "i", "for", "i", "in", "score_match", "[", "span", "]", ".", "keys", "(", ")", "\n", "if", "score_match", "[", "span", "]", "[", "i", "]", "[", "0", "]", "==", "max_score", "]", "\n", "\n", "# Pick the first one.", "\n", "tag_type", "=", "src_a", ".", "span_list", "[", "max_indices", "[", "0", "]", "]", ".", "tag_type", "\n", "start", ",", "end", "=", "span", "\n", "best_span", "=", "list", "(", "range", "(", "start", ",", "end", ")", ")", "\n", "for", "ind", ",", "sp", "in", "enumerate", "(", "best_span", ")", ":", "\n", "                        ", "if", "ind", "==", "0", ":", "\n", "                            ", "prefix", "=", "\"B\"", "\n", "", "else", ":", "\n", "                            ", "prefix", "=", "\"I\"", "\n", "", "tgt_a", ".", "ner_tags", "[", "sp", "]", "=", "prefix", "+", "\"-\"", "+", "tag_type", "\n", "\n", "", "", "", "print", "(", "\"######################################################################\"", ")", "\n", "print", "(", "\"Target tokens: \"", ",", "tgt_a", ".", "tokens", ")", "\n", "for", "ind", ",", "tag", "in", "enumerate", "(", "tgt_a", ".", "ner_tags", ")", ":", "\n", "                ", "if", "tag", "not", "in", "config", ".", "allowed_tags", ":", "\n", "                    ", "tgt_a", ".", "ner_tags", "[", "ind", "]", "=", "config", ".", "OUTSIDE_TAG", "\n", "", "", "print", "(", "\"Target NER tags: \"", ",", "tgt_a", ".", "ner_tags", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"Entities with no match: \"", "+", "str", "(", "problematic_entities", ")", ")", "\n", "logging", ".", "info", "(", "\"Number of unique entities with no match: \"", "+", "str", "(", "len", "(", "problematic_entities", ")", ")", ")", "\n", "logging", ".", "info", "(", "\"Number of occurrences of entities with no match: \"", "+", "str", "(", "occur_problem_entities", ")", ")", "\n", "logging", ".", "info", "(", "\"Number of all source entities: \"", "+", "str", "(", "src_entities", ")", ")", "\n", "\n", "if", "self", ".", "args", ".", "ablation_string", "==", "\"original\"", "or", "self", ".", "args", ".", "ablation_string", "==", "\"_no_google_\"", "or", "self", ".", "args", ".", "ablation_string", "==", "\"_no_google_v2_\"", ":", "\n", "            ", "N", "=", "len", "(", "self", ".", "sentence_ids", ")", "\n", "logging", ".", "info", "(", "\"Total documents: \"", "+", "str", "(", "N", ")", ")", "\n", "logging", ".", "info", "(", "\"Potential candidates for problematic entities:\"", ")", "\n", "count_pc", "=", "0", "\n", "for", "pc", "in", "problem_children", ":", "\n", "                ", "if", "problematic_entities", "[", "pc", "]", ">", "self", ".", "args", ".", "min_occur", ":", "\n", "                    ", "count_pc", "+=", "problematic_entities", "[", "pc", "]", "\n", "", "if", "problematic_entities", "[", "pc", "]", ">", "0", ":", "\n", "                    ", "for", "k", ",", "v", "in", "problem_children", "[", "pc", "]", ".", "items", "(", ")", ":", "\n", "                        ", "assert", "k", ".", "lower", "(", ")", "in", "idf", "\n", "problem_children", "[", "pc", "]", "[", "k", "]", "=", "v", "*", "math", ".", "log", "(", "float", "(", "N", "/", "idf", "[", "k", ".", "lower", "(", ")", "]", ")", ")", "\n", "", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "pc", "+", "\": \"", "+", "str", "(", "problem_children", "[", "pc", "]", ".", "most_common", "(", "100", ")", ")", ")", "\n", "\n", "", "", "sentences_to_drop", "=", "list", "(", ")", "\n", "for", "k", ",", "val", "in", "problematic_sentences", ".", "items", "(", ")", ":", "\n", "                ", "to_be_dropped", "=", "False", "\n", "for", "v", "in", "val", ":", "\n", "                    ", "if", "problematic_entities", "[", "v", "[", "1", "]", "]", ">", "0", ":", "\n", "                        ", "to_be_dropped", "=", "True", "\n", "", "", "if", "to_be_dropped", ":", "\n", "                    ", "sentences_to_drop", ".", "append", "(", "k", ")", "\n", "\n", "", "", "logging", ".", "info", "(", "\"Number of problematic entities %d\"", "%", "len", "(", "problematic_entities", ")", ")", "\n", "logging", ".", "info", "(", "\"Number of sentences with at least one problematic entity: %d\"", "%", "\n", "len", "(", "sentences_to_drop", ")", ")", "\n", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\"problematic_entities_\"", "+", "self", ".", "date_today", "+", "\".pkl\"", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "problematic_entities", ",", "f", ")", "\n", "\n", "", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\"problem_children_\"", "+", "self", ".", "date_today", "+", "\".pkl\"", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "problem_children", ",", "f", ")", "\n", "\n", "", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\"problematic_sentences_\"", "+", "self", ".", "date_today", "+", "\".pkl\"", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "problematic_sentences", ",", "f", ")", "\n", "\n", "", "final_candidate_dict", "=", "self", ".", "get_refined_candidates", "(", "problem_children", ",", "problematic_entities", ")", "\n", "checklist", "=", "[", "[", "False", "for", "_", "in", "problematic_sentences", "[", "sent", "]", "]", "for", "sent", "in", "problematic_sentences", "]", "\n", "num_annotation", "=", "0", "\n", "num_annotation", "+=", "self", ".", "tag_problematic_sentences", "(", "problematic_sentences", ",", "final_candidate_dict", ",", "\n", "problematic_entities", ",", "checklist", ")", "\n", "logging", ".", "info", "(", "\"Number of occurrences of problematic entities with count > %d (PC-%d): %d\"", "%", "\n", "(", "self", ".", "args", ".", "min_occur", ",", "self", ".", "args", ".", "min_occur", ",", "\n", "count_pc", ")", ")", "\n", "logging", ".", "info", "(", "\"Total number of annotations: \"", "+", "str", "(", "num_annotation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.get_refined_candidates": [[888, 909], ["dict", "logging.info", "logging.info", "problem_children[].most_common", "list", "enumerate", "logging.info", "logging.info", "str", "list.append", "str", "len", "re.findall", "str"], "methods", ["None"], ["", "", "def", "get_refined_candidates", "(", "self", ",", "problem_children", ",", "problematic_entities", ")", ":", "\n", "        ", "final_candidate_dict", "=", "dict", "(", ")", "\n", "for", "token", "in", "problem_children", ":", "\n", "            ", "if", "problematic_entities", "[", "token", "]", ">", "self", ".", "args", ".", "min_occur", ":", "\n", "                ", "most_common_candidates", "=", "problem_children", "[", "token", "]", ".", "most_common", "(", "10", "*", "self", ".", "args", ".", "topk", ")", "\n", "\n", "new_most_common_candidates", "=", "list", "(", ")", "\n", "\n", "for", "i", ",", "candidates", "in", "enumerate", "(", "most_common_candidates", ")", ":", "\n", "                    ", "if", "candidates", "[", "0", "]", "not", "in", "self", ".", "stop_word_list", "and", "candidates", "[", "0", "]", "not", "in", "string", ".", "punctuation", "and", "len", "(", "re", ".", "findall", "(", "\"\\d+\"", ",", "candidates", "[", "0", "]", ")", ")", "==", "0", ":", "\n", "                        ", "new_most_common_candidates", ".", "append", "(", "candidates", ")", "\n", "\n", "", "", "final_candidates", "=", "new_most_common_candidates", "[", ":", "self", ".", "args", ".", "topk", "]", "\n", "\n", "final_candidate_dict", "[", "token", "]", "=", "final_candidates", "\n", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"Token: \"", "+", "str", "(", "token", ")", "+", "\" Final candidates: \"", "+", "str", "(", "final_candidates", ")", ")", "\n", "", "", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"Final candidates dict: \"", "+", "str", "(", "final_candidate_dict", ")", ")", "\n", "return", "final_candidate_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.tag_problematic_sentences": [[910, 963], ["logging.info", "enumerate", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "enumerate", "str", "str", "str", "str", "str", "enumerate", "logging.info", "logging.info", "tmp.get_ordered_spans", "logging.info", "tmp.TMP.can_tag", "str", "str", "logging.info", "logging.info", "logging.info", "list", "enumerate", "range"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_ordered_spans", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.can_tag"], ["", "def", "tag_problematic_sentences", "(", "self", ",", "problematic_sentences", ",", "final_candidate_dict", ",", "problematic_entities", ",", "checklist", ")", ":", "\n", "        ", "total_entities", "=", "0", "\n", "total_annotations_done", "=", "0", "\n", "\n", "logging", ".", "info", "(", "\"Tagging problematic sentences.\"", ")", "\n", "for", "i_sent", ",", "sent", "in", "enumerate", "(", "problematic_sentences", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"Sentence id: \"", "+", "str", "(", "sent", ")", ")", "\n", "src_a", "=", "self", ".", "src_annotated_list", "[", "sent", "]", "\n", "tgt_a", "=", "self", ".", "tgt_annotated_list", "[", "sent", "]", "\n", "logging", ".", "info", "(", "\"Source tokens: \"", "+", "str", "(", "src_a", ".", "tokens", ")", ")", "\n", "logging", ".", "info", "(", "\"Target tokens: \"", "+", "str", "(", "tgt_a", ".", "tokens", ")", ")", "\n", "\n", "for", "i_ent", ",", "entities", "in", "enumerate", "(", "problematic_sentences", "[", "sent", "]", ")", ":", "\n", "                ", "if", "not", "checklist", "[", "i_sent", "]", "[", "i_ent", "]", ":", "\n", "                    ", "if", "problematic_entities", "[", "entities", "[", "1", "]", "]", ">", "self", ".", "args", ".", "min_occur", ":", "\n", "                        ", "total_entities", "+=", "1", "\n", "tag_type", "=", "src_a", ".", "span_list", "[", "entities", "[", "0", "]", "]", ".", "tag_type", "\n", "entity_str", "=", "entities", "[", "1", "]", "\n", "candidate_list", "=", "final_candidate_dict", "[", "entity_str", "]", "\n", "\n", "token_list", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "candidate_list", "}", "\n", "temp_ner_list", "=", "[", "(", "-", "1", ",", "0", ")", "for", "_", "in", "tgt_a", ".", "tokens", "]", "\n", "\n", "for", "i", ",", "tgt_token", "in", "enumerate", "(", "tgt_a", ".", "tokens", ")", ":", "\n", "                            ", "if", "tgt_token", "in", "token_list", ":", "\n", "                                ", "temp_ner_list", "[", "i", "]", "=", "(", "tag_type", ",", "token_list", "[", "tgt_token", "]", ")", "\n", "\n", "", "", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"Entity: \"", "+", "str", "(", "entity_str", ")", ")", "\n", "ordered_span_list", "=", "get_ordered_spans", "(", "tag_type", ",", "temp_ner_list", ")", "\n", "logging", ".", "info", "(", "\"Ordered span list: \"", "+", "str", "(", "ordered_span_list", ")", ")", "\n", "\n", "for", "span", "in", "ordered_span_list", ":", "\n", "                            ", "if", "self", ".", "can_tag", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", ",", "tgt_a", ")", ":", "\n", "                                ", "checklist", "[", "i_sent", "]", "[", "i_ent", "]", "=", "True", "\n", "logging", ".", "info", "(", "\"Tagging possible!\"", ")", "\n", "logging", ".", "info", "(", "\"Target phrase: \"", "+", "\" \"", ".", "join", "(", "tgt_a", ".", "tokens", "[", "span", "[", "0", "]", ":", "span", "[", "1", "]", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"Tag: \"", "+", "tag_type", ")", "\n", "total_annotations_done", "+=", "1", "\n", "best_span", "=", "list", "(", "range", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", ")", ")", "\n", "for", "ind", ",", "sp", "in", "enumerate", "(", "best_span", ")", ":", "\n", "                                    ", "if", "ind", "==", "0", ":", "\n", "                                        ", "prefix", "=", "\"B\"", "\n", "", "else", ":", "\n", "                                        ", "prefix", "=", "\"I\"", "\n", "", "tgt_a", ".", "ner_tags", "[", "sp", "]", "=", "prefix", "+", "\"-\"", "+", "tag_type", "\n", "", "break", "\n", "\n", "", "", "", "", "", "", "logging", ".", "info", "(", "\"######################################################################\"", ")", "\n", "logging", ".", "info", "(", "\"Number of remaining problematic entities: \"", "+", "str", "(", "total_entities", ")", ")", "\n", "logging", ".", "info", "(", "\"Number of annotated entities: \"", "+", "str", "(", "total_annotations_done", ")", ")", "\n", "return", "total_annotations_done", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.can_tag": [[964, 970], ["range", "type"], "methods", ["None"], ["", "def", "can_tag", "(", "self", ",", "span_beg", ",", "span_end", ",", "tgt_a", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "span_beg", ",", "span_end", ")", ":", "\n", "            ", "if", "tgt_a", ".", "ner_tags", "[", "i", "]", "!=", "\"O\"", ":", "\n", "                ", "if", "type", "(", "tgt_a", ".", "ner_tags", "[", "i", "]", ")", "!=", "list", ":", "\n", "                    ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.prepare_train_file": [[971, 986], ["tmp.post_process_annotations", "os.path.join", "src.util.data_processing.prepare_train_file", "datetime.datetime.datetime.today().strftime", "str", "datetime.datetime.datetime.today"], "methods", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.post_process_annotations", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TMP.prepare_train_file"], ["", "def", "prepare_train_file", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "tgt_lang", "in", "[", "\"es\"", ",", "\"nl\"", ",", "\"uz\"", "]", ":", "\n", "            ", "capitalize", "=", "True", "\n", "", "else", ":", "\n", "            ", "capitalize", "=", "False", "\n", "", "self", ".", "tgt_annotated_list", "=", "post_process_annotations", "(", "self", ".", "tgt_annotated_list", ",", "self", ".", "stop_word_list", ",", "\n", "capitalize", "=", "capitalize", ")", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "self", ".", "args", ".", "translate_fname", "+", "\"_affix-match_blah_\"", "+", "\n", "str", "(", "self", ".", "args", ".", "matching_score_threshold", ")", "+", "self", ".", "args", ".", "ablation_string", "+", "datetime", ".", "today", "(", ")", ".", "strftime", "(", "\"%d-%m-%Y\"", ")", ")", "\n", "if", "self", ".", "tgt_lang", "in", "[", "\"hi\"", ",", "\"ta\"", "]", ":", "\n", "            ", "remove_misc", "=", "True", "\n", "", "else", ":", "\n", "            ", "remove_misc", "=", "False", "\n", "", "data_processing", ".", "prepare_train_file", "(", "self", ".", "tgt_annotated_list", ",", "self", ".", "drop_list", ",", "file_path", ",", "\n", "remove_misc", "=", "remove_misc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TgtPhrases.__init__": [[989, 992], ["list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "src_phrase", ")", ":", "\n", "        ", "self", ".", "src_phrase", "=", "src_phrase", "\n", "self", ".", "tgt_phrase_list", "=", "list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.TgtPhrases.add_tgt_phrase": [[993, 995], ["tmp.TgtPhrases.tgt_phrase_list.append"], "methods", ["None"], ["", "def", "add_tgt_phrase", "(", "self", ",", "tgt_phrase", ")", ":", "\n", "        ", "self", ".", "tgt_phrase_list", ".", "append", "(", "tgt_phrase", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.Match.__init__": [[998, 1002], ["list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token", ",", "ner_tag", ")", ":", "\n", "        ", "self", ".", "token", "=", "token", "\n", "self", ".", "ner_tag", "=", "ner_tag", "\n", "self", ".", "match_list", "=", "list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.Match.add_match": [[1003, 1005], ["tmp.Match.match_list.append"], "methods", ["None"], ["", "def", "add_match", "(", "self", ",", "ref_id", ",", "score", ")", ":", "\n", "        ", "self", ".", "match_list", ".", "append", "(", "(", "ref_id", ",", "score", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_google_translations": [[1007, 1012], ["googleapiclient.discovery.build", "googleapiclient.discovery.build.translations().list().execute", "googleapiclient.discovery.build.translations().list", "googleapiclient.discovery.build.translations"], "function", ["None"], ["", "", "def", "get_google_translations", "(", "src_list", ",", "src_lang_code", ",", "tgt_lang_code", ",", "api_key", ")", ":", "\n", "    ", "service", "=", "build", "(", "'translate'", ",", "'v2'", ",", "developerKey", "=", "api_key", ")", "\n", "tgt_dict", "=", "service", ".", "translations", "(", ")", ".", "list", "(", "\n", "source", "=", "src_lang_code", ",", "target", "=", "tgt_lang_code", ",", "q", "=", "src_list", ")", ".", "execute", "(", ")", "\n", "return", "[", "t", "[", "'translatedText'", "]", "for", "t", "in", "tgt_dict", "[", "'translations'", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.tokenize_using_corenlp": [[1014, 1027], ["nltk.parse.corenlp.CoreNLPParser", "nltk.parse.corenlp.CoreNLPParser.api_call"], "function", ["None"], ["", "def", "tokenize_using_corenlp", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    :param text: String that needs to be tokenized.\n    :return: Tokens\n    This requires a CoreNLP server to be running on port 9001. Please follow\n    the steps listed here:\n    https://stanfordnlp.github.io/CoreNLP/corenlp-server.html.\n    \"\"\"", "\n", "corenlp_parser", "=", "CoreNLPParser", "(", "'http://localhost:9001'", ",", "encoding", "=", "'utf8'", ")", "\n", "result", "=", "corenlp_parser", ".", "api_call", "(", "text", ",", "{", "'annotators'", ":", "'tokenize,ssplit'", "}", ")", "\n", "tokens", "=", "[", "token", "[", "'originalText'", "]", "or", "token", "[", "'word'", "]", "for", "sentence", "in", "\n", "result", "[", "'sentences'", "]", "for", "token", "in", "sentence", "[", "'tokens'", "]", "]", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_clean_tokens": [[1029, 1074], ["list", "enumerate", "sentence.replace.replace", "tmp.tokenize_using_corenlp", "nltk.word_tokenize", "float", "len", "list.pop", "list.append", "len", "list.pop", "list.append", "list.append", "list.append", "len", "len", "list.pop", "float", "len", "list.pop", "list.append"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.tokenize_using_corenlp"], ["", "def", "get_clean_tokens", "(", "sentence", ",", "use_corenlp", "=", "True", ")", ":", "\n", "    ", "for", "ent", "in", "html", ".", "entities", ".", "html5", ":", "\n", "        ", "sentence", "=", "sentence", ".", "replace", "(", "\"&\"", "+", "ent", "+", "\";\"", ",", "\n", "html", ".", "entities", ".", "html5", "[", "ent", "]", ")", "\n", "\n", "", "if", "use_corenlp", ":", "\n", "        ", "tokens", "=", "tokenize_using_corenlp", "(", "sentence", ")", "\n", "", "else", ":", "\n", "        ", "tokens", "=", "nltk", ".", "word_tokenize", "(", "sentence", ")", "\n", "\n", "", "final_tokens", "=", "list", "(", ")", "\n", "ampersand_found", "=", "False", "\n", "prior_period_index", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "# Combine the tokens in \"S & P\" to obtain a single entity \"S&P\" and \"U. S.\"", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "ampersand_found", ":", "\n", "            ", "ampersand_found", "=", "False", "\n", "continue", "\n", "\n", "", "elif", "token", "==", "\"&\"", "and", "i", "!=", "0", "and", "i", "!=", "len", "(", "tokens", ")", "-", "1", ":", "\n", "            ", "if", "len", "(", "final_tokens", ")", ">", "0", ":", "\n", "                ", "final_tokens", ".", "pop", "(", ")", "\n", "final_tokens", ".", "append", "(", "tokens", "[", "i", "-", "1", "]", "+", "\"&\"", "+", "tokens", "[", "i", "+", "1", "]", ")", "\n", "ampersand_found", "=", "True", "\n", "\n", "", "", "elif", "token", "==", "\".\"", "and", "i", "!=", "0", "and", "i", "!=", "len", "(", "tokens", ")", "-", "1", ":", "\n", "            ", "if", "prior_period_index", "==", "-", "float", "(", "\"inf\"", ")", "or", "i", "-", "prior_period_index", ">", "2", ":", "\n", "                ", "prior_period_index", "=", "i", "\n", "final_tokens", ".", "pop", "(", ")", "\n", "final_tokens", ".", "append", "(", "tokens", "[", "i", "-", "1", "]", "+", "\".\"", ")", "\n", "", "else", ":", "\n", "                ", "if", "len", "(", "final_tokens", ")", ">", "0", ":", "\n", "                    ", "final_tokens", ".", "pop", "(", ")", "\n", "if", "len", "(", "final_tokens", ")", ">", "0", ":", "\n", "                        ", "final_tokens", ".", "pop", "(", ")", "\n", "final_tokens", ".", "append", "(", "tokens", "[", "i", "-", "3", "]", "+", "\".\"", "+", "tokens", "[", "i", "-", "1", "]", "+", "\".\"", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "            ", "if", "token", "==", "\"``\"", "or", "token", "==", "\"''\"", ":", "\n", "                ", "final_tokens", ".", "append", "(", "\"\\\"\"", ")", "\n", "", "else", ":", "\n", "                ", "final_tokens", ".", "append", "(", "token", ")", "\n", "", "", "", "return", "final_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._find_match": [[1076, 1096], ["copy.deepcopy().lower", "copy.deepcopy().lower", "set", "set.add", "set", "set.add", "copy.deepcopy().lower.split", "copy.deepcopy().lower.split", "list", "sorted", "tmp._find_match_helper", "copy.deepcopy", "copy.deepcopy", "len", "len", "sorted.append", "tmp._find_match_helper"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._find_match_helper", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._find_match_helper"], ["", "def", "_find_match", "(", "reference", ",", "hypothesis", ",", "epi", ",", "stop_word_list", ",", "transliterate", "=", "False", ")", ":", "\n", "    ", "reference", "=", "copy", ".", "deepcopy", "(", "reference", ")", ".", "lower", "(", ")", "\n", "hypothesis", "=", "copy", ".", "deepcopy", "(", "hypothesis", ")", ".", "lower", "(", ")", "\n", "\n", "reference_set", "=", "set", "(", "reference", ".", "split", "(", "\"-\"", ")", ")", "\n", "reference_set", ".", "add", "(", "reference", ")", "\n", "hypothesis_set", "=", "set", "(", "hypothesis", ".", "split", "(", "\"-\"", ")", ")", "\n", "hypothesis_set", ".", "add", "(", "hypothesis", ")", "\n", "\n", "if", "len", "(", "reference_set", ")", ">", "1", "or", "len", "(", "hypothesis_set", ")", ">", "1", ":", "\n", "        ", "list_of_all_scores", "=", "list", "(", ")", "\n", "for", "r", "in", "reference_set", ":", "\n", "            ", "for", "h", "in", "hypothesis_set", ":", "\n", "                ", "list_of_all_scores", ".", "append", "(", "_find_match_helper", "(", "r", ",", "h", ",", "epi", ",", "stop_word_list", ",", "\n", "transliterate", "=", "transliterate", ")", ")", "\n", "", "", "list_of_all_scores", "=", "sorted", "(", "list_of_all_scores", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "return", "list_of_all_scores", "[", "0", "]", "\n", "\n", "", "else", ":", "\n", "        ", "return", "_find_match_helper", "(", "reference", ",", "hypothesis", ",", "epi", ",", "stop_word_list", ",", "transliterate", "=", "transliterate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._find_match_helper": [[1098, 1128], ["len", "epi.transliterate", "epi.transliterate", "range", "min", "epi.transliterate.startswith", "float", "float", "epi.transliterate.endswith", "len"], "function", ["None"], ["", "", "def", "_find_match_helper", "(", "reference", ",", "hypothesis", ",", "epi", ",", "stop_word_list", ",", "transliterate", "=", "False", ")", ":", "\n", "    ", "is_stop_word", "=", "False", "\n", "if", "(", "hypothesis", "in", "stop_word_list", ")", "or", "(", "reference", "in", "stop_word_list", ")", ":", "\n", "        ", "is_stop_word", "=", "True", "\n", "\n", "", "if", "transliterate", ":", "\n", "        ", "hypothesis", "=", "epi", ".", "transliterate", "(", "hypothesis", ")", "\n", "reference", "=", "epi", ".", "transliterate", "(", "reference", ")", "\n", "\n", "", "L", "=", "len", "(", "hypothesis", ")", "\n", "score", "=", "L", "\n", "ret_val", "=", "False", "\n", "if", "L", "==", "0", "or", "reference", "==", "''", ":", "\n", "        ", "return", "0", ",", "False", "\n", "", "if", "reference", "==", "hypothesis", ":", "\n", "        ", "ret_val", "=", "True", "\n", "", "else", ":", "\n", "        ", "for", "j", "in", "range", "(", "L", ",", "0", ",", "-", "1", ")", ":", "\n", "            ", "sub_str", "=", "hypothesis", "[", ":", "j", "]", "\n", "if", "reference", ".", "startswith", "(", "sub_str", ")", ":", "\n", "                ", "if", "not", "is_stop_word", ":", "\n", "                    ", "ret_val", "=", "True", "\n", "break", "\n", "", "", "elif", "reference", ".", "endswith", "(", "sub_str", ")", ":", "\n", "                ", "if", "not", "is_stop_word", ":", "\n", "                    ", "ret_val", "=", "True", "\n", "break", "\n", "", "", "score", "-=", "1", "\n", "\n", "", "", "return", "min", "(", "float", "(", "score", "/", "len", "(", "reference", ")", ")", ",", "float", "(", "score", "/", "L", ")", ")", ",", "ret_val", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.generate_fast_align_data": [[1130, 1134], ["open", "enumerate", "f.writelines"], "function", ["None"], ["", "def", "generate_fast_align_data", "(", "src_sent_list", ",", "tgt_sent_list", ",", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "src_sent", "in", "enumerate", "(", "src_sent_list", ")", ":", "\n", "            ", "f", ".", "writelines", "(", "src_sent", "+", "config", ".", "FAST_ALIGN_SEP", "+", "tgt_sent_list", "[", "i", "]", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_readable_annotations": [[1136, 1150], ["list", "map", "enumerate", "zip", "full_annotations.append", "full_annotations.append", "flat[].index"], "function", ["None"], ["", "", "", "def", "get_readable_annotations", "(", "tokens", ",", "annotations", ")", ":", "\n", "# Annotation is a tuple containing score, index, tag", "\n", "    ", "flat", "=", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "annotations", ")", ")", ")", "\n", "if", "flat", "==", "[", "]", ":", "\n", "        ", "full_annotations", "=", "[", "config", ".", "OUTSIDE_TAG", "for", "_", "in", "tokens", "]", "\n", "\n", "", "else", ":", "\n", "        ", "full_annotations", "=", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "i", "in", "flat", "[", "1", "]", ":", "\n", "                ", "full_annotations", ".", "append", "(", "flat", "[", "2", "]", "[", "flat", "[", "1", "]", ".", "index", "(", "i", ")", "]", ")", "\n", "", "else", ":", "\n", "                ", "full_annotations", ".", "append", "(", "config", ".", "OUTSIDE_TAG", ")", "\n", "", "", "", "return", "full_annotations", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.read_lexicon": [[1152, 1170], ["open", "f.readlines", "dict", "open", "pickle.dump", "row.rstrip().split.rstrip().split", "lexicon[].append", "list", "row.rstrip().split.rstrip"], "function", ["None"], ["", "def", "read_lexicon", "(", "txt_path", ",", "pkl_path", ")", ":", "\n", "    ", "\"\"\"\n    :param txt_path: Path of the input lexicon text file\n    :param pkl_path: Path of the output lexicon pickle file\n    :return: \n    \"\"\"", "\n", "with", "open", "(", "txt_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "rows", "=", "f", ".", "readlines", "(", ")", "\n", "lexicon", "=", "dict", "(", ")", "\n", "for", "row", "in", "rows", ":", "\n", "            ", "row", "=", "row", ".", "rstrip", "(", "\"\\n\"", ")", ".", "split", "(", "\" \"", ")", "\n", "src_word", "=", "row", "[", "0", "]", "\n", "tgt_word", "=", "row", "[", "1", "]", "\n", "if", "src_word", "not", "in", "lexicon", ":", "\n", "                ", "lexicon", "[", "src_word", "]", "=", "list", "(", ")", "\n", "", "lexicon", "[", "src_word", "]", ".", "append", "(", "tgt_word", ")", "\n", "", "", "with", "open", "(", "pkl_path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "lexicon", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._get_saved_list": [[1172, 1178], ["os.path.exists", "pickle.load", "list", "open"], "function", ["None"], ["", "", "def", "_get_saved_list", "(", "path", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "saved_list", "=", "pickle", ".", "load", "(", "open", "(", "path", ",", "'rb'", ")", ")", "\n", "", "else", ":", "\n", "        ", "saved_list", "=", "list", "(", ")", "\n", "", "return", "saved_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._get_tag_type": [[1180, 1186], ["tag.split"], "function", ["None"], ["", "def", "_get_tag_type", "(", "tag", ")", ":", "\n", "    ", "if", "tag", "==", "config", ".", "OUTSIDE_TAG", ":", "\n", "        ", "tag_type", "=", "None", "\n", "", "else", ":", "\n", "        ", "tag_type", "=", "tag", ".", "split", "(", "\"-\"", ")", "[", "1", "]", "\n", "", "return", "tag_type", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.find_entity_spans": [[1188, 1195], ["enumerate", "list", "map", "zip"], "function", ["None"], ["", "def", "find_entity_spans", "(", "entity_candidates", ",", "ner_tag_list", ")", ":", "\n", "    ", "for", "i", ",", "ner_tags", "in", "enumerate", "(", "ner_tag_list", ")", ":", "\n", "        ", "all_ids", "=", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "ner_tags", ")", ")", ")", "\n", "\n", "if", "all_ids", "[", "0", "]", "!=", "[", "\"O\"", "]", ":", "\n", "            ", "for", "j", "in", "all_ids", "[", "0", "]", ":", "\n", "                ", "entity_candidates", "[", "j", "]", "[", "i", "]", "=", "j", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_spans": [[1197, 1211], ["list", "tag_list.append", "enumerate", "list.append"], "function", ["None"], ["", "", "", "", "def", "get_spans", "(", "tag_id", ",", "tag_list", ")", ":", "\n", "    ", "prev_tag", "=", "-", "1", "\n", "span_list", "=", "list", "(", ")", "\n", "tag_list", ".", "append", "(", "-", "1", ")", "\n", "\n", "beg", "=", "0", "\n", "for", "i", ",", "curr_tag", "in", "enumerate", "(", "tag_list", ")", ":", "\n", "        ", "if", "(", "prev_tag", "==", "-", "1", ")", "and", "(", "curr_tag", "==", "tag_id", ")", ":", "\n", "            ", "beg", "=", "i", "\n", "", "elif", "(", "prev_tag", "==", "tag_id", ")", "and", "(", "curr_tag", "==", "-", "1", ")", ":", "\n", "            ", "span_list", ".", "append", "(", "(", "beg", ",", "i", ")", ")", "\n", "", "prev_tag", "=", "curr_tag", "\n", "\n", "", "return", "span_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.get_ordered_spans": [[1213, 1231], ["list", "tag_score_list.append", "enumerate", "sorted", "sorted.append"], "function", ["None"], ["", "def", "get_ordered_spans", "(", "tag_id", ",", "tag_score_list", ")", ":", "\n", "    ", "prev_tag", "=", "(", "-", "1", ",", "0.0", ")", "\n", "span_list", "=", "list", "(", ")", "\n", "tag_score_list", ".", "append", "(", "(", "-", "1", ",", "0.0", ")", ")", "\n", "\n", "beg", "=", "0", "\n", "score", "=", "0", "\n", "for", "i", ",", "curr_tag", "in", "enumerate", "(", "tag_score_list", ")", ":", "\n", "        ", "if", "(", "prev_tag", "[", "0", "]", "==", "-", "1", ")", "and", "(", "curr_tag", "[", "0", "]", "==", "tag_id", ")", ":", "\n", "            ", "beg", "=", "i", "\n", "", "elif", "(", "prev_tag", "[", "0", "]", "!=", "-", "1", ")", "and", "(", "curr_tag", "[", "0", "]", "==", "-", "1", ")", ":", "\n", "            ", "span_list", ".", "append", "(", "(", "beg", ",", "i", ",", "score", ")", ")", "\n", "score", "=", "0", "\n", "", "score", "+=", "curr_tag", "[", "1", "]", "\n", "prev_tag", "=", "curr_tag", "\n", "\n", "", "span_list", "=", "sorted", "(", "span_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "2", "]", ",", "reverse", "=", "True", ")", "\n", "return", "span_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp._google_translate_lang_code": [[1234, 1239], ["None"], "function", ["None"], ["", "def", "_google_translate_lang_code", "(", "lang", ")", ":", "\n", "    ", "if", "lang", "==", "\"zh\"", ":", "\n", "        ", "return", "\"zh-CN\"", "\n", "", "else", ":", "\n", "        ", "return", "lang", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.post_process_annotations": [[1241, 1267], ["enumerate", "enumerate", "curr_tag.startswith", "curr_tag.startswith", "curr_tag.split", "tmp.capitalize_conditionally", "curr_tag.split", "tmp.capitalize_conditionally"], "function", ["home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.capitalize_conditionally", "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.capitalize_conditionally"], ["", "", "def", "post_process_annotations", "(", "tgt_annotated_list", ",", "stop_word_list", ",", "capitalize", "=", "False", ")", ":", "\n", "    ", "for", "i", ",", "tgt_a", "in", "enumerate", "(", "tgt_annotated_list", ")", ":", "\n", "        ", "prev_type", "=", "None", "\n", "for", "j", ",", "curr_tag", "in", "enumerate", "(", "tgt_a", ".", "ner_tags", ")", ":", "\n", "            ", "if", "curr_tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "                ", "curr_type", "=", "curr_tag", ".", "split", "(", "\"-\"", ")", "[", "1", "]", "\n", "if", "capitalize", ":", "\n", "                    ", "if", "curr_type", "==", "\"PER\"", ":", "\n", "                        ", "is_per", "=", "True", "\n", "", "else", ":", "\n", "                        ", "is_per", "=", "False", "\n", "", "tgt_a", ".", "tokens", "[", "j", "]", "=", "capitalize_conditionally", "(", "tgt_a", ".", "tokens", "[", "j", "]", ",", "stop_word_list", ",", "is_per", ")", "\n", "", "if", "curr_type", "!=", "prev_type", ":", "\n", "                    ", "tgt_a", ".", "ner_tags", "[", "j", "]", "=", "\"B-\"", "+", "curr_type", "\n", "", "", "elif", "curr_tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "curr_type", "=", "curr_tag", ".", "split", "(", "\"-\"", ")", "[", "1", "]", "\n", "if", "capitalize", ":", "\n", "                    ", "if", "curr_type", "==", "\"PER\"", ":", "\n", "                        ", "is_per", "=", "True", "\n", "", "else", ":", "\n", "                        ", "is_per", "=", "False", "\n", "", "tgt_a", ".", "tokens", "[", "j", "]", "=", "capitalize_conditionally", "(", "tgt_a", ".", "tokens", "[", "j", "]", ",", "stop_word_list", ",", "is_per", ")", "\n", "", "", "else", ":", "\n", "                ", "curr_type", "=", "None", "\n", "", "prev_type", "=", "curr_type", "\n", "", "", "return", "tgt_annotated_list", "\n", "\n"]], "home.repos.pwc.inspect_result.alankarj_cross_lingual_ner.util.tmp.capitalize_conditionally": [[1269, 1277], ["token[].isupper", "token.capitalize.capitalize", "token[].isupper", "token.capitalize.capitalize"], "function", ["None"], ["", "def", "capitalize_conditionally", "(", "token", ",", "stop_word_list", ",", "is_per", "=", "False", ")", ":", "\n", "    ", "if", "is_per", ":", "\n", "        ", "if", "not", "token", "[", "0", "]", ".", "isupper", "(", ")", ":", "\n", "            ", "token", "=", "token", ".", "capitalize", "(", ")", "\n", "", "", "elif", "token", "not", "in", "stop_word_list", ":", "\n", "        ", "if", "not", "token", "[", "0", "]", ".", "isupper", "(", ")", ":", "\n", "            ", "token", "=", "token", ".", "capitalize", "(", ")", "\n", "", "", "return", "token", "\n", "", ""]]}