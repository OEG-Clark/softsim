{"home.repos.pwc.inspect_result.allenai_specter.specter.predictor.SpecterPredictor.predict_json": [[19, 46], ["predictor.SpecterPredictor._dataset_reader.text_to_instance", "predictor.SpecterPredictor._dataset_reader.text_to_instance", "predictor.SpecterPredictor._model.forward_on_instance", "outputs[].tolist", "hasattr"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.text_to_instance", "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.text_to_instance"], ["    ", "@", "overrides", "\n", "def", "predict_json", "(", "self", ",", "paper", ":", "JsonDict", ")", "->", "Dict", ":", "\n", "        ", "ret", "=", "{", "}", "\n", "for", "key", "in", "[", "'paper_id'", ",", "'title'", ",", "'abstract'", ",", "'authors'", ",", "'venue'", "]", ":", "\n", "            ", "try", ":", "\n", "                ", "ret", "[", "key", "]", "=", "paper", "[", "key", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "pass", "\n", "", "", "ret", "[", "'embedding'", "]", "=", "[", "]", "\n", "try", ":", "\n", "            ", "if", "hasattr", "(", "self", ".", "_model", ",", "'bert_finetune'", ")", "and", "self", ".", "_model", ".", "bert_finetune", ":", "\n", "                ", "if", "not", "paper", "[", "'title'", "]", "and", "not", "paper", "[", "'abstract'", "]", ":", "\n", "                    ", "return", "ret", "\n", "", "", "else", ":", "\n", "                ", "if", "not", "paper", "[", "'title'", "]", "or", "not", "paper", "[", "'abstract'", "]", ":", "\n", "                    ", "return", "ret", "\n", "", "", "", "except", "KeyError", ":", "\n", "            ", "return", "ret", "\n", "\n", "", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "paper", ")", "\n", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "paper", ")", "\n", "\n", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "\n", "ret", "[", "'embedding'", "]", "=", "outputs", "[", "'embedding'", "]", ".", "tolist", "(", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predictor.SpecterPredictor.predict_batch_json": [[48, 101], ["enumerate", "range", "predictor.SpecterPredictor._model.forward_on_instances", "len", "skipped_idx.append", "instances.append", "skipped_idx.append", "[].tolist", "results.append", "results.append", "hasattr", "predictor.SpecterPredictor._dataset_reader.text_to_instance", "len"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "predict_batch_json", "(", "self", ",", "inputs", ":", "List", "[", "JsonDict", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "instances", "=", "[", "]", "\n", "skipped_idx", "=", "[", "]", "\n", "for", "idx", ",", "json_dict", "in", "enumerate", "(", "inputs", ")", ":", "\n", "            ", "paper", "=", "{", "}", "\n", "if", "'title'", "not", "in", "json_dict", ":", "\n", "                ", "skipped_idx", ".", "append", "(", "idx", ")", "\n", "continue", "\n", "", "skip", "=", "False", "\n", "for", "key", "in", "[", "'paper_id'", ",", "'title'", ",", "'abstract'", ",", "'authors'", ",", "'venue'", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "paper", "[", "key", "]", "=", "json_dict", "[", "key", "]", "\n", "", "except", "KeyError", ":", "\n", "                    ", "pass", "\n", "", "", "paper", "[", "'embedding'", "]", "=", "[", "]", "\n", "try", ":", "\n", "                ", "if", "hasattr", "(", "self", ".", "_model", ",", "'bert_finetune'", ")", "and", "self", ".", "_model", ".", "bert_finetune", ":", "\n", "# this model concatenates title/abstract", "\n", "                    ", "if", "not", "json_dict", "[", "'title'", "]", ":", "\n", "                        ", "skip", "=", "True", "\n", "", "", "else", ":", "\n", "# both title and abstract must be present", "\n", "                    ", "if", "not", "json_dict", "[", "'title'", "]", "or", "not", "json_dict", "[", "'abstract'", "]", ":", "\n", "                        ", "skip", "=", "True", "\n", "", "", "", "except", "KeyError", ":", "\n", "                ", "skip", "=", "True", "\n", "", "if", "not", "skip", ":", "\n", "                ", "instances", ".", "append", "(", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "json_dict", ")", ")", "\n", "", "else", ":", "\n", "                ", "skipped_idx", ".", "append", "(", "idx", ")", "\n", "", "", "if", "instances", ":", "\n", "            ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "[", "]", "\n", "", "k", "=", "0", "\n", "results", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "paper", "=", "{", "}", "\n", "for", "key", "in", "[", "'paper_id'", ",", "'title'", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "paper", "[", "key", "]", "=", "inputs", "[", "j", "]", "[", "key", "]", "\n", "", "except", "KeyError", ":", "\n", "                    ", "pass", "\n", "", "", "paper", "[", "'embedding'", "]", "=", "[", "]", "\n", "if", "not", "skipped_idx", "or", "k", ">=", "len", "(", "skipped_idx", ")", "or", "skipped_idx", "[", "k", "]", "!=", "j", ":", "\n", "                ", "paper", "[", "'embedding'", "]", "=", "outputs", "[", "j", "-", "k", "]", "[", "'embedding'", "]", ".", "tolist", "(", ")", "\n", "results", ".", "append", "(", "paper", ")", "\n", "", "else", ":", "\n", "                ", "paper", "[", "'embedding'", "]", "=", "[", "]", "\n", "results", ".", "append", "(", "paper", ")", "\n", "k", "+=", "1", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predictor.SpecterPredictor.dump_line": [[102, 105], ["json.dumps"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "\n", "        ", "return", "json", ".", "dumps", "(", "outputs", ",", "cls", "=", "NumpyEncoder", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predictor.SpecterPredictor.load_line": [[106, 116], ["line.strip", "line.strip", "line.strip"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "load_line", "(", "self", ",", "line", ":", "str", ")", "->", "JsonDict", ":", "# pylint: disable=no-self-use", "\n", "        ", "\"\"\"\n        If your inputs are not in JSON-lines format (e.g. you have a CSV)\n        you can override this function to parse them correctly.\n        \"\"\"", "\n", "if", "line", ".", "strip", "(", ")", "not", "in", "self", ".", "_dataset_reader", ".", "papers", ":", "\n", "            ", "return", "{", "'paper_id'", ":", "line", ".", "strip", "(", ")", "}", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_dataset_reader", ".", "papers", "[", "line", ".", "strip", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predictor.NumpyEncoder.default": [[120, 131], ["isinstance", "json.JSONEncoder.default", "int", "isinstance", "float", "isinstance", "obj.tolist"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.specter.predictor.NumpyEncoder.default"], ["def", "default", "(", "self", ",", "obj", ")", ":", "\n", "        ", "if", "isinstance", "(", "obj", ",", "(", "np", ".", "int_", ",", "np", ".", "intc", ",", "np", ".", "intp", ",", "np", ".", "int8", ",", "\n", "np", ".", "int16", ",", "np", ".", "int32", ",", "np", ".", "int64", ",", "np", ".", "uint8", ",", "\n", "np", ".", "uint16", ",", "np", ".", "uint32", ",", "np", ".", "uint64", ")", ")", ":", "\n", "            ", "return", "int", "(", "obj", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "(", "np", ".", "float_", ",", "np", ".", "float16", ",", "np", ".", "float32", ",", "\n", "np", ".", "float64", ")", ")", ":", "\n", "            ", "return", "float", "(", "obj", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "(", "np", ".", "ndarray", ",", ")", ")", ":", "\n", "            ", "return", "obj", ".", "tolist", "(", ")", "\n", "", "return", "json", ".", "JSONEncoder", ".", "default", "(", "self", ",", "obj", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.Predict.add_subparser": [[30, 68], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["None"], ["    ", "def", "add_subparser", "(", "self", ",", "name", ":", "str", ",", "parser", ":", "argparse", ".", "_SubParsersAction", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "# pylint: disable=protected-access", "\n", "        ", "description", "=", "'''Run the specified model against a JSON-lines input file.'''", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "name", ",", "description", "=", "description", ",", "help", "=", "'Use a trained model to make predictions.'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'archive_file'", ",", "type", "=", "str", ",", "help", "=", "'the archived model to make predictions with'", ")", "\n", "subparser", ".", "add_argument", "(", "'input_file'", ",", "type", "=", "str", ",", "help", "=", "'path to input file'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--output-file'", ",", "type", "=", "str", ",", "help", "=", "'path to output file'", ")", "\n", "subparser", ".", "add_argument", "(", "'--weights-file'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'a path that overrides which weights file to use'", ")", "\n", "\n", "batch_size", "=", "subparser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "batch_size", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'The batch size to use for processing'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--silent'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not print output to stdout'", ")", "\n", "\n", "cuda_device", "=", "subparser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "cuda_device", ".", "add_argument", "(", "'--cuda-device'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "'id of GPU to use (if any)'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--use-dataset-reader'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to use the dataset reader of the original model to load Instances'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'-o'", ",", "'--overrides'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "'a JSON structure used to override the experiment configuration'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--predictor'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'optionally specify a specific predictor to use'", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "_predict", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command._PredictManagerCustom.__init__": [[76, 86], ["allennlp.commands.predict._PredictManager.__init__", "int", "sum", "open"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["def", "__init__", "(", "self", ",", "\n", "predictor", ":", "Predictor", ",", "\n", "input_file", ":", "str", ",", "\n", "output_file", ":", "Optional", "[", "str", "]", ",", "\n", "batch_size", ":", "int", ",", "\n", "print_to_console", ":", "bool", ",", "\n", "has_dataset_reader", ":", "bool", ")", "->", "None", ":", "\n", "        ", "super", "(", "_PredictManagerCustom", ",", "self", ")", ".", "__init__", "(", "predictor", ",", "input_file", ",", "output_file", ",", "batch_size", ",", "print_to_console", ",", "\n", "has_dataset_reader", ")", "\n", "self", ".", "total_size", "=", "int", "(", "sum", "(", "[", "1", "for", "_", "in", "open", "(", "self", ".", "_input_file", ")", "]", ")", "/", "self", ".", "_batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command._PredictManagerCustom.run": [[87, 104], ["tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "predict_command._PredictManagerCustom._output_file.close", "allennlp.common.util.lazy_groups_of", "zip", "allennlp.common.util.lazy_groups_of", "zip", "predict_command._PredictManagerCustom._get_instance_data", "predict_command._PredictManagerCustom._predict_instances", "predict_command._PredictManagerCustom._maybe_print_to_console_and_file", "predict_command._PredictManagerCustom._get_json_data", "predict_command._PredictManagerCustom._predict_json", "predict_command._PredictManagerCustom._maybe_print_to_console_and_file", "str", "json.dumps"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "run", "(", "self", ")", "->", "None", ":", "\n", "        ", "has_reader", "=", "self", ".", "_dataset_reader", "is", "not", "None", "\n", "index", "=", "0", "\n", "if", "has_reader", ":", "\n", "            ", "for", "batch", "in", "tqdm", "(", "lazy_groups_of", "(", "self", ".", "_get_instance_data", "(", ")", ",", "self", ".", "_batch_size", ")", ",", "total", "=", "self", ".", "total_size", ",", "unit", "=", "\"batches\"", ")", ":", "\n", "                ", "for", "model_input_instance", ",", "result", "in", "zip", "(", "batch", ",", "self", ".", "_predict_instances", "(", "batch", ")", ")", ":", "\n", "                    ", "self", ".", "_maybe_print_to_console_and_file", "(", "index", ",", "result", ",", "str", "(", "model_input_instance", ")", ")", "\n", "index", "=", "index", "+", "1", "\n", "", "", "", "else", ":", "\n", "            ", "for", "batch_json", "in", "tqdm", "(", "lazy_groups_of", "(", "self", ".", "_get_json_data", "(", ")", ",", "self", ".", "_batch_size", ")", ",", "total", "=", "self", ".", "total_size", ",", "unit", "=", "\"batches\"", ")", ":", "\n", "                ", "for", "model_input_json", ",", "result", "in", "zip", "(", "batch_json", ",", "self", ".", "_predict_json", "(", "batch_json", ")", ")", ":", "\n", "                    ", "self", ".", "_maybe_print_to_console_and_file", "(", "index", ",", "result", ",", "json", ".", "dumps", "(", "model_input_json", ")", ")", "\n", "index", "=", "index", "+", "1", "\n", "\n", "", "", "", "if", "self", ".", "_output_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "_output_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.predictor_from_archive": [[107, 141], ["archive.config.duplicate", "config[].as_dict", "allennlp.data.DatasetReader.from_params", "model.eval", "archive.config.duplicate.get().get", "allennlp.common.Params", "allennlp.predictors.predictor.Predictor.by_name", "allennlp.common.checks.ConfigurationError", "archive.config.duplicate.get"], "function", ["None"], ["", "", "", "def", "predictor_from_archive", "(", "archive", ":", "Archive", ",", "predictor_name", ":", "str", "=", "None", ",", "\n", "paper_features_path", ":", "str", "=", "None", ")", "->", "'Predictor'", ":", "\n", "    ", "\"\"\"\n    Extends allennlp.predictors.predictor.from_archive to allow processing multiprocess reader\n\n    paper_features_path is passed to replace the correct one if the dataset_reader is multiprocess\n    \"\"\"", "\n", "\n", "# Duplicate the config so that the config inside the archive doesn't get consumed", "\n", "config", "=", "archive", ".", "config", ".", "duplicate", "(", ")", "\n", "\n", "if", "not", "predictor_name", ":", "\n", "        ", "model_type", "=", "config", ".", "get", "(", "\"model\"", ")", ".", "get", "(", "\"type\"", ")", "\n", "if", "not", "model_type", "in", "DEFAULT_PREDICTORS", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"No default predictor for model type {model_type}.\\n\"", "f\"Please specify a predictor explicitly.\"", ")", "\n", "", "predictor_name", "=", "DEFAULT_PREDICTORS", "[", "model_type", "]", "\n", "\n", "", "dataset_config", "=", "config", "[", "\"dataset_reader\"", "]", ".", "as_dict", "(", ")", "\n", "if", "dataset_config", "[", "'type'", "]", "==", "'multiprocess'", ":", "\n", "        ", "dataset_config", "=", "dataset_config", "[", "'base_reader'", "]", "\n", "if", "paper_features_path", ":", "\n", "            ", "dataset_config", "[", "'paper_features_path'", "]", "=", "paper_features_path", "\n", "", "dataset_reader_params", "=", "Params", "(", "dataset_config", ")", "\n", "\n", "", "else", ":", "\n", "        ", "dataset_reader_params", "=", "config", "[", "\"dataset_reader\"", "]", "\n", "\n", "", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "dataset_reader_params", ")", "\n", "\n", "model", "=", "archive", ".", "model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "return", "Predictor", ".", "by_name", "(", "predictor_name", ")", "(", "model", ",", "dataset_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command._get_predictor": [[143, 156], ["allennlp.common.checks.check_for_gpu", "allennlp.models.archival.load_archive", "allennlp.common.params.parse_overrides", "predict_command.predictor_from_archive"], "function", ["home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.predictor_from_archive"], ["", "def", "_get_predictor", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "Predictor", ":", "\n", "    ", "check_for_gpu", "(", "args", ".", "cuda_device", ")", "\n", "archive", "=", "load_archive", "(", "args", ".", "archive_file", ",", "\n", "weights_file", "=", "args", ".", "weights_file", ",", "\n", "cuda_device", "=", "args", ".", "cuda_device", ",", "\n", "overrides", "=", "args", ".", "overrides", ")", "\n", "ov", "=", "parse_overrides", "(", "args", ".", "overrides", ")", "\n", "paper_features_path", "=", "None", "\n", "try", ":", "\n", "        ", "paper_features_path", "=", "ov", "[", "'dataset_reader'", "]", "[", "'paper_features_path'", "]", "\n", "", "except", "KeyError", ":", "\n", "        ", "pass", "\n", "", "return", "predictor_from_archive", "(", "archive", ",", "args", ".", "predictor", ",", "paper_features_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command._predict": [[158, 173], ["predict_command._get_predictor", "predict_command._PredictManagerCustom", "predict_command._PredictManagerCustom.run", "print", "print", "sys.exit"], "function", ["home.repos.pwc.inspect_result.allenai_specter.specter.predict_command._get_predictor", "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.run"], ["", "def", "_predict", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "None", ":", "\n", "    ", "predictor", "=", "_get_predictor", "(", "args", ")", "\n", "\n", "if", "args", ".", "silent", "and", "not", "args", ".", "output_file", ":", "\n", "        ", "print", "(", "\"--silent specified without --output-file.\"", ")", "\n", "print", "(", "\"Exiting early because no output will be created.\"", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "", "manager", "=", "_PredictManagerCustom", "(", "predictor", ",", "\n", "args", ".", "input_file", ",", "\n", "args", ".", "output_file", ",", "\n", "args", ".", "batch_size", ",", "\n", "not", "args", ".", "silent", ",", "\n", "args", ".", "use_dataset_reader", ")", "\n", "manager", ".", "run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.main": [[175, 218], ["allennlp.commands.ArgumentParserWithDefaults", "allennlp.commands.ArgumentParserWithDefaults.add_argument", "allennlp.commands.ArgumentParserWithDefaults.add_subparsers", "subcommands.items", "allennlp.commands.ArgumentParserWithDefaults.parse_args", "predict_command.Predict", "subcommand.add_subparser", "dir", "getattr", "parser.parse_args.func", "allennlp.commands.ArgumentParserWithDefaults.print_help", "subcommand.add_subparser.add_argument", "allennlp.common.util.import_submodules"], "function", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.parse_args", "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.Predict.add_subparser"], ["", "def", "main", "(", "prog", ":", "str", "=", "None", ",", "\n", "subcommand_overrides", ":", "Dict", "[", "str", ",", "Subcommand", "]", "=", "{", "}", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    The :mod:`~allennlp.run` command only knows about the registered classes in the ``allennlp``\n    codebase. In particular, once you start creating your own ``Model`` s and so forth, it won't\n    work for them, unless you use the ``--include-package`` flag.\n    \"\"\"", "\n", "# pylint: disable=dangerous-default-value", "\n", "parser", "=", "ArgumentParserWithDefaults", "(", "description", "=", "\"Run AllenNLP\"", ",", "usage", "=", "'%(prog)s'", ",", "prog", "=", "prog", ")", "\n", "parser", ".", "add_argument", "(", "'--version'", ",", "action", "=", "'version'", ",", "version", "=", "'%(prog)s '", "+", "__version__", ")", "\n", "\n", "subparsers", "=", "parser", ".", "add_subparsers", "(", "title", "=", "'Commands'", ",", "metavar", "=", "''", ")", "\n", "\n", "subcommands", "=", "{", "\n", "# Default commands", "\n", "\"predict\"", ":", "Predict", "(", ")", ",", "\n", "# Superseded by overrides", "\n", "**", "subcommand_overrides", "\n", "}", "\n", "\n", "for", "name", ",", "subcommand", "in", "subcommands", ".", "items", "(", ")", ":", "\n", "        ", "subparser", "=", "subcommand", ".", "add_subparser", "(", "name", ",", "subparsers", ")", "\n", "# configure doesn't need include-package because it imports", "\n", "# whatever classes it needs.", "\n", "if", "name", "!=", "\"configure\"", ":", "\n", "            ", "subparser", ".", "add_argument", "(", "'--include-package'", ",", "\n", "type", "=", "str", ",", "\n", "action", "=", "'append'", ",", "\n", "default", "=", "[", "]", ",", "\n", "help", "=", "'additional packages to include'", ")", "\n", "\n", "", "", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# If a subparser is triggered, it adds its work as `args.func`.", "\n", "# So if no such attribute has been added, no subparser was triggered,", "\n", "# so give the user some help.", "\n", "if", "'func'", "in", "dir", "(", "args", ")", ":", "\n", "# Import any additional modules needed (to register custom classes).", "\n", "        ", "for", "package_name", "in", "getattr", "(", "args", ",", "'include_package'", ",", "(", ")", ")", ":", "\n", "            ", "import_submodules", "(", "package_name", ")", "\n", "", "args", ".", "func", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "parser", ".", "print_help", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.run": [[220, 222], ["predict_command.main"], "function", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.main"], ["", "", "def", "run", "(", ")", ":", "\n", "    ", "main", "(", "prog", "=", "\"allennlp\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReaderFromPickled.__init__": [[40, 59], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "word_splitter", ":", "WordSplitter", "=", "None", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "max_sequence_length", ":", "int", "=", "256", ",", "\n", "concat_title_abstract", ":", "bool", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Dataset reader that uses pickled preprocessed instances\n        Consumes the output resulting from data_utils/create_training_files.py\n\n        the additional arguments are not used here and are for compatibility with\n        the other data reader at prediction time\n        \"\"\"", "\n", "self", ".", "max_sequence_length", "=", "max_sequence_length", "\n", "self", ".", "token_indexers", "=", "token_indexers", "\n", "self", ".", "_concat_title_abstract", "=", "concat_title_abstract", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReaderFromPickled._read": [[60, 110], ["open", "pickle.Unpickler", "pickle.Unpickler.load", "pickle.Unpickler.load.fields.get", "pickle.Unpickler.load.fields.get", "pickle.Unpickler.load.fields.get", "tokens.extend", "tokens.extend", "tokens.extend", "pickle.Unpickler.load.fields.pop", "allennlp.data.tokenizers.token.Token"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            file_path: path to the pickled instances\n        \"\"\"", "\n", "with", "open", "(", "file_path", ",", "'rb'", ")", "as", "f_in", ":", "\n", "            ", "unpickler", "=", "pickle", ".", "Unpickler", "(", "f_in", ")", "\n", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "instance", "=", "unpickler", ".", "load", "(", ")", "\n", "# compatibility with old models:", "\n", "# for field in instance.fields:", "\n", "#     if hasattr(instance.fields[field], '_token_indexers') and 'bert' in instance.fields[field]._token_indexers:", "\n", "#         if not hasattr(instance.fields['source_title']._token_indexers['bert'], '_truncate_long_sequences'):", "\n", "#             instance.fields[field]._token_indexers['bert']._truncate_long_sequences = True", "\n", "#             instance.fields[field]._token_indexers['bert']._token_min_padding_length = 0", "\n", "if", "self", ".", "max_sequence_length", ":", "\n", "                        ", "for", "paper_type", "in", "[", "'source'", ",", "'pos'", ",", "'neg'", "]", ":", "\n", "                            ", "if", "self", ".", "_concat_title_abstract", ":", "\n", "                                ", "tokens", "=", "[", "]", "\n", "title_field", "=", "instance", ".", "fields", ".", "get", "(", "f'{paper_type}_title'", ")", "\n", "abst_field", "=", "instance", ".", "fields", ".", "get", "(", "f'{paper_type}_abstract'", ")", "\n", "if", "title_field", ":", "\n", "                                    ", "tokens", ".", "extend", "(", "title_field", ".", "tokens", ")", "\n", "", "if", "tokens", ":", "\n", "                                    ", "tokens", ".", "extend", "(", "[", "Token", "(", "'[SEP]'", ")", "]", ")", "\n", "", "if", "abst_field", ":", "\n", "                                    ", "tokens", ".", "extend", "(", "abst_field", ".", "tokens", ")", "\n", "", "if", "title_field", ":", "\n", "                                    ", "title_field", ".", "tokens", "=", "tokens", "\n", "instance", ".", "fields", "[", "f'{paper_type}_title'", "]", "=", "title_field", "\n", "", "elif", "abst_field", ":", "\n", "                                    ", "abst_field", ".", "tokens", "=", "tokens", "\n", "instance", ".", "fields", "[", "f'{paper_type}_title'", "]", "=", "abst_field", "\n", "", "else", ":", "\n", "                                    ", "yield", "None", "\n", "# title_tokens = get_text_tokens(query_title_tokens, query_abstract_tokens, abstract_delimiter)", "\n", "# pos_title_tokens = get_text_tokens(pos_title_tokens, pos_abstract_tokens, abstract_delimiter)", "\n", "# neg_title_tokens = get_text_tokens(neg_title_tokens, neg_abstract_tokens, abstract_delimiter)", "\n", "# query_abstract_tokens = pos_abstract_tokens = neg_abstract_tokens = []", "\n", "", "", "for", "field_type", "in", "[", "'title'", ",", "'abstract'", ",", "'authors'", ",", "'author_positions'", "]", ":", "\n", "                                ", "field", "=", "paper_type", "+", "'_'", "+", "field_type", "\n", "if", "instance", ".", "fields", ".", "get", "(", "field", ")", ":", "\n", "                                    ", "instance", ".", "fields", "[", "field", "]", ".", "tokens", "=", "instance", ".", "fields", "[", "field", "]", ".", "tokens", "[", ":", "self", ".", "max_sequence_length", "]", "\n", "", "if", "field_type", "==", "'abstract'", "and", "self", ".", "_concat_title_abstract", ":", "\n", "                                    ", "instance", ".", "fields", ".", "pop", "(", "field", ",", "None", ")", "\n", "", "", "", "", "yield", "instance", "\n", "", "except", "EOFError", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.__init__": [[117, 229], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "specter.data_utils.triplet_sampling.TripletGenerator", "set", "allennlp.data.tokenizers.word_splitter.SimpleWordSplitter", "allennlp.data.tokenizers.WordTokenizer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "open", "json.load", "os.path.splitext", "float", "included_text_fields.split", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "allennlp.data.token_indexers.SingleIdTokenIndexer", "open", "json.load", "root_path.split", "list", "data.DataReader.papers.keys"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "paper_features_path", ":", "str", "=", "None", ",", "\n", "word_splitter", ":", "WordSplitter", "=", "None", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "data_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "samples_per_query", ":", "int", "=", "5", ",", "\n", "margin_fraction", ":", "float", "=", "0.5", ",", "\n", "ratio_hard_negatives", ":", "float", "=", "0.5", ",", "\n", "predict_mode", ":", "bool", "=", "False", ",", "\n", "max_num_authors", ":", "Optional", "[", "int", "]", "=", "5", ",", "\n", "ratio_training_samples", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "max_sequence_length", ":", "Optional", "[", "int", "]", "=", "-", "1", ",", "\n", "cache_path", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "overwrite_cache", ":", "Optional", "[", "bool", "]", "=", "False", ",", "\n", "use_cls_token", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "concat_title_abstract", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "coviews_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "included_text_fields", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "use_paper_feature_cache", ":", "bool", "=", "True", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Args:\n            lazy: if false returns a list\n            paper_features_path: path to the paper features json file (result of scripts.generate_paper_features.py\n            candidates_path: path to the candidate papers\n            tokenizer: tokenizer to be used for tokenizing strings\n            token_indexers: token indexer for indexing vocab\n            data_file: path to the data file (e.g, citations)\n            samples_per_query: number of triplets to generate for each query\n            margin_fraction: minimum margin of co-views between positive and negative samples\n            ratio_hard_negatives: ratio of training data that is selected from hard negatives\n                remaining is allocated to easy negatives. should be set to 1.0 in case of similar click data\n            predict_mode: if `True` the model only considers the current paper and returns an embedding\n                otherwise the model uses the triplet format to train the embedder\n            author_id_embedder: Embedder for author ids\n            s2_id_embedder: Embedder for respresenting s2 ids\n            other_id_embedder: Embedder for representing other ids (e.g., id assigned by metadata)\n            max_num_authors: maximum number of authors,\n            ratio_training_samples: Limits training to proportion of all training instances\n            max_sequence_length: Longer sequences would be truncated (if -1 then there would be no truncation)\n            cache_path: Path to file to cache instances, if None, instances won't be cached.\n                If specified, instances are cached after being created so next time they are not created\n                again from scratch\n            overwrite_cache: If true, it overwrites the cached files. Each file corresponds to\n                all instances created from the train, dev or test set.\n            use_cls_token: Like bert, use an additional CLS token in the begginning (for transoformer)\n            concat_title_abstract: Whether to consider title and abstract as a single field.\n            coviews_file: Only for backward compatibility to work with older models (renamed to \n                `data_file` in newer models), leave this empty as it won't have any effect\n            included_text_fields: space delimited fields to concat to the title: e.g., `title abstract authors`\n            use_paper_feature_cache: set to False to disable the in-memory cache of paper features\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_word_splitter", "=", "word_splitter", "or", "SimpleWordSplitter", "(", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "or", "WordTokenizer", "(", "self", ".", "_word_splitter", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_token_indexer_author_id", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'author'", ")", "}", "\n", "self", ".", "_token_indexer_author_position", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'author_positions'", ")", "}", "\n", "\n", "self", ".", "_token_indexer_venue", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'venue'", ")", "}", "\n", "self", ".", "_token_indexer_id", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'id'", ")", "}", "\n", "\n", "with", "open", "(", "paper_features_path", ")", "as", "f_in", ":", "\n", "            ", "self", ".", "papers", "=", "json", ".", "load", "(", "f_in", ")", "\n", "", "self", ".", "samples_per_query", "=", "samples_per_query", "\n", "self", ".", "margin_fraction", "=", "margin_fraction", "\n", "self", ".", "ratio_hard_negatives", "=", "ratio_hard_negatives", "\n", "\n", "self", ".", "predict_mode", "=", "predict_mode", "\n", "self", ".", "max_sequence_length", "=", "max_sequence_length", "\n", "self", ".", "use_cls_token", "=", "use_cls_token", "\n", "\n", "if", "data_file", "and", "not", "predict_mode", ":", "\n", "# logger.info(f'reading contents of the file at: {coviews_file}')", "\n", "            ", "with", "open", "(", "data_file", ")", "as", "f_in", ":", "\n", "                ", "self", ".", "dataset", "=", "json", ".", "load", "(", "f_in", ")", "\n", "# logger.info(f'reading complete. Total {len(self.dataset)} records found.')", "\n", "", "root_path", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "data_file", ")", "\n", "# for multitask interleaving reader, track which dataset the instance is coming from", "\n", "self", ".", "data_source", "=", "root_path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "dataset", "=", "None", "\n", "self", ".", "data_source", "=", "None", "\n", "\n", "", "self", ".", "max_num_authors", "=", "max_num_authors", "\n", "\n", "self", ".", "triplet_generator", "=", "TripletGenerator", "(", "\n", "paper_ids", "=", "list", "(", "self", ".", "papers", ".", "keys", "(", ")", ")", ",", "\n", "coviews", "=", "self", ".", "dataset", ",", "\n", "margin_fraction", "=", "margin_fraction", ",", "\n", "samples_per_query", "=", "samples_per_query", ",", "\n", "ratio_hard_negatives", "=", "ratio_hard_negatives", "\n", ")", "\n", "self", ".", "paper_feature_cache", "=", "{", "}", "# paper_id -> paper features. Serves as a cache for the _get_paper_features function", "\n", "\n", "self", ".", "ratio_training_samples", "=", "float", "(", "ratio_training_samples", ")", "if", "ratio_training_samples", "else", "None", "\n", "\n", "self", ".", "cache_path", "=", "cache_path", "\n", "self", ".", "overwrite_cache", "=", "overwrite_cache", "\n", "self", ".", "data_file", "=", "data_file", "\n", "self", ".", "paper_features_path", "=", "paper_features_path", "\n", "self", ".", "ratio_training_samples", "=", "ratio_training_samples", "\n", "\n", "self", ".", "concat_title_abstract", "=", "concat_title_abstract", "\n", "self", ".", "included_text_fields", "=", "set", "(", "included_text_fields", ".", "split", "(", ")", ")", "\n", "self", ".", "use_paper_feature_cache", "=", "use_paper_feature_cache", "\n", "\n", "self", ".", "abstract_delimiter", "=", "[", "Token", "(", "'[SEP]'", ")", "]", "\n", "self", ".", "author_delimiter", "=", "[", "Token", "(", "'[unused0]'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader._get_paper_features": [[231, 272], ["paper.get", "data.DataReader._tokenizer.tokenize", "data.DataReader._tokenizer.tokenize", "data.DataReader._tokenizer.tokenize", "data.DataReader._tokenizer.tokenize", "data.DataReader._tokenizer.tokenize", "paper.get", "data.DataReader._tokenizer.tokenize", "specter.data_utils.create_training_files.get_text_tokens", "data.DataReader._tokenizer.tokenize", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_text_tokens"], ["", "def", "_get_paper_features", "(", "self", ",", "paper", ":", "Optional", "[", "dict", "]", "=", "None", ")", "->", "Tuple", "[", "List", "[", "Token", "]", ",", "List", "[", "Token", "]", ",", "List", "[", "Token", "]", ",", "int", ",", "List", "[", "Token", "]", "]", ":", "\n", "        ", "\"\"\" Given a paper, extract and tokenize abstract, title, venue and year\"\"\"", "\n", "if", "paper", ":", "\n", "            ", "paper_id", "=", "paper", ".", "get", "(", "'paper_id'", ")", "\n", "\n", "# This function is being called by the same paper multiple times.", "\n", "# Cache the result to avoid wasted compute", "\n", "if", "self", ".", "use_paper_feature_cache", "and", "paper_id", "in", "self", ".", "paper_feature_cache", ":", "\n", "                ", "return", "self", ".", "paper_feature_cache", "[", "paper_id", "]", "\n", "\n", "", "if", "not", "self", ".", "concat_title_abstract", ":", "\n", "                ", "abstract_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'abstract'", ")", "or", "''", ")", "\n", "title_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'title'", ")", "or", "''", ")", "\n", "if", "self", ".", "max_sequence_length", ">", "0", ":", "\n", "                    ", "title_tokens", "=", "title_tokens", "[", ":", "self", ".", "max_sequence_length", "]", "\n", "abstract_tokens", "=", "abstract_tokens", "[", ":", "self", ".", "max_sequence_length", "]", "\n", "", "", "else", ":", "\n", "                ", "abstract_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"abstract\"", ")", "or", "\"\"", ")", "\n", "title_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"title\"", ")", "or", "\"\"", ")", "\n", "if", "'abstract'", "in", "self", ".", "included_text_fields", ":", "\n", "                    ", "title_tokens", "=", "get_text_tokens", "(", "title_tokens", ",", "abstract_tokens", ",", "self", ".", "abstract_delimiter", ")", "\n", "", "if", "'authors'", "in", "self", ".", "included_text_fields", ":", "\n", "                    ", "author_text", "=", "' '", ".", "join", "(", "paper", ".", "get", "(", "\"author-names\"", ")", "or", "[", "]", ")", "\n", "author_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "author_text", ")", "\n", "max_seq_len_title", "=", "self", ".", "max_sequence_length", "-", "15", "# reserve max 15 tokens for author names", "\n", "title_tokens", "=", "title_tokens", "[", ":", "max_seq_len_title", "]", "+", "self", ".", "author_delimiter", "+", "author_tokens", "\n", "", "title_tokens", "=", "title_tokens", "[", ":", "self", ".", "max_sequence_length", "]", "\n", "# abstract and title are identical (abstract won't be used in this case)", "\n", "abstract_tokens", "=", "title_tokens", "\n", "\n", "", "venue", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'venue'", ")", "or", "NO_VENUE_TEXT", ")", "\n", "year", "=", "paper", ".", "get", "(", "'year'", ")", "or", "0", "\n", "body_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'body'", ")", ")", "if", "'body'", "in", "paper", "else", "None", "\n", "features", "=", "abstract_tokens", ",", "title_tokens", ",", "venue", ",", "year", ",", "body_tokens", "\n", "\n", "if", "self", ".", "use_paper_feature_cache", ":", "\n", "                ", "self", ".", "paper_feature_cache", "[", "paper_id", "]", "=", "features", "\n", "\n", "", "return", "features", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader._get_author_field": [[273, 302], ["allennlp.data.fields.ListField", "enumerate", "allennlp.data.fields.ListField", "data.DataReader._tokenizer.tokenize", "len", "allennlp.data.fields.TextField", "author_positions.append", "allennlp.data.fields.TextField", "author_positions.append", "author_positions.append", "data.DataReader._tokenizer.tokenize", "len", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "data.DataReader._tokenizer.tokenize", "data.DataReader._tokenizer.tokenize"], "methods", ["None"], ["", "", "def", "_get_author_field", "(", "self", ",", "authors", ":", "List", "[", "str", "]", ")", "->", "Tuple", "[", "ListField", ",", "ListField", "]", ":", "\n", "        ", "\"\"\"\n        Get a Label field associated with authors along with their position\n        Args:\n            authors: list of authors\n\n        Returns:\n            authors and their positions\n        \"\"\"", "\n", "if", "authors", "==", "[", "]", ":", "\n", "            ", "authors", "=", "[", "'##'", "]", "\n", "", "authors", "=", "[", "self", ".", "_tokenizer", ".", "tokenize", "(", "author", ")", "for", "author", "in", "authors", "]", "\n", "if", "len", "(", "authors", ")", ">", "self", ".", "max_num_authors", ":", "\n", "            ", "authors", "=", "authors", "[", ":", "self", ".", "max_num_authors", "-", "1", "]", "+", "[", "authors", "[", "-", "1", "]", "]", "\n", "", "author_field", "=", "ListField", "(", "[", "TextField", "(", "author", ",", "token_indexers", "=", "self", ".", "_token_indexer_author_id", ")", "for", "author", "in", "authors", "]", ")", "\n", "\n", "author_positions", "=", "[", "]", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "authors", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "author_positions", ".", "append", "(", "TextField", "(", "\n", "self", ".", "_tokenizer", ".", "tokenize", "(", "'00'", ")", ",", "token_indexers", "=", "self", ".", "_token_indexer_author_position", ")", ")", "\n", "", "elif", "i", "<", "len", "(", "authors", ")", "-", "1", ":", "\n", "                ", "author_positions", ".", "append", "(", "TextField", "(", "\n", "self", ".", "_tokenizer", ".", "tokenize", "(", "'01'", ")", ",", "token_indexers", "=", "self", ".", "_token_indexer_author_position", ")", ")", "\n", "", "else", ":", "\n", "                ", "author_positions", ".", "append", "(", "TextField", "(", "\n", "self", ".", "_tokenizer", ".", "tokenize", "(", "'02'", ")", ",", "token_indexers", "=", "self", ".", "_token_indexer_author_position", ")", ")", "\n", "", "", "position_field", "=", "ListField", "(", "author_positions", ")", "\n", "return", "author_field", ",", "position_field", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.get_hash": [[303, 314], ["hashlib.md5().hexdigest", "str", "hashlib.md5", "key.encode"], "methods", ["None"], ["", "def", "get_hash", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "\"\"\"\n        Get hashname for the current dataset reader config\n        \"\"\"", "\n", "key", "=", "f\"{file_path},{self.data_file},{self.paper_features_path},\"", "f\"{self.ratio_training_samples},{str(self._word_splitter.__class__)},{self.samples_per_query},\"", "f\"{self.margin_fraction},{self.ratio_hard_negatives},{self.use_cls_token},\"", "f\"{self.max_sequence_length}\"", "\n", "if", "self", ".", "concat_title_abstract", ":", "\n", "            ", "key", "+=", "'concat-title'", "\n", "", "return", "hashlib", ".", "md5", "(", "key", ".", "encode", "(", "'utf-8'", ")", ")", ".", "hexdigest", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.read": [[316, 377], ["getattr", "logger.warning", "allennlp.data.dataset_readers.dataset_reader._LazyInstances", "data.DataReader.get_hash", "pathlib.Path().mkdir", "os.path.join", "data.DataReader._read", "iter", "data.DataReader._read", "logger.info", "logger.info", "isinstance", "allennlp.common.checks.ConfigurationError", "data.DataReader._read", "pathlib.Path", "os.path.exists", "isinstance", "allennlp.common.checks.ConfigurationError", "open", "dill.dump", "open", "dill.load", "allennlp.common.Tqdm.tqdm", "allennlp.common.Tqdm.tqdm"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.get_hash", "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled._read", "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled._read", "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled._read"], ["", "@", "overrides", "\n", "def", "read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Returns an ``Iterable`` containing all the instances\n        in the specified dataset.\n\n        If ``self.lazy`` is False, this calls ``self._read()``,\n        ensures that the result is a list, then returns the resulting list.\n\n        If ``self.lazy`` is True, this returns an object whose\n        ``__iter__`` method calls ``self._read()`` each iteration.\n        In this case your implementation of ``_read()`` must also be lazy\n        (that is, not load all instances into memory at once), otherwise\n        you will get a ``ConfigurationError``.\n\n        In either case, the returned ``Iterable`` can be iterated\n        over multiple times. It's unlikely you want to override this function,\n        but if you do your result should likewise be repeatedly iterable.\n        \"\"\"", "\n", "lazy", "=", "getattr", "(", "self", ",", "'lazy'", ",", "None", ")", "\n", "if", "lazy", "is", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\"DatasetReader.lazy is not set, \"", "\n", "\"did you forget to call the superclass constructor?\"", ")", "\n", "", "if", "lazy", ":", "\n", "            ", "return", "_LazyInstances", "(", "lambda", ":", "iter", "(", "self", ".", "_read", "(", "file_path", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "cache_path", "is", "not", "None", ":", "\n", "# create a key for the file based on the reader config", "\n", "                ", "hash_", "=", "self", ".", "get_hash", "(", "file_path", ")", "\n", "pathlib", ".", "Path", "(", "self", ".", "cache_path", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "cache_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "cache_path", ",", "(", "hash_", "+", "'.cache'", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_file", ")", "or", "self", ".", "overwrite_cache", ":", "\n", "                    ", "instances", "=", "self", ".", "_read", "(", "file_path", ")", "\n", "if", "not", "isinstance", "(", "instances", ",", "list", ")", ":", "\n", "                        ", "instances", "=", "[", "instance", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", "]", "\n", "", "if", "not", "instances", ":", "\n", "                        ", "raise", "ConfigurationError", "(", "\"No instances were read from the given filepath {}. \"", "\n", "\"Is the path correct?\"", ".", "format", "(", "file_path", ")", ")", "\n", "", "logger", ".", "info", "(", "f'caching instances to file: {cache_file}'", ")", "\n", "\n", "with", "open", "(", "cache_file", ",", "'wb'", ")", "as", "cache", ":", "\n", "                        ", "dill", ".", "dump", "(", "instances", ",", "cache", ")", "\n", "", "", "else", ":", "\n", "                    ", "logger", ".", "info", "(", "f'Reading instances from cache file: {cache_file}'", ")", "\n", "# instances = []", "\n", "# with open(cache_file, 'rb') as cache:", "\n", "#     start   = time.time()", "\n", "#     instances = []", "\n", "#     for line in Tqdm.tqdm(cache):", "\n", "#         instances.append(self.deserialize_instance(line.strip()))", "\n", "#     print(time.time()-start)", "\n", "with", "open", "(", "cache_file", ",", "'rb'", ")", "as", "f_in", ":", "\n", "                        ", "instances", "=", "dill", ".", "load", "(", "f_in", ")", "\n", "", "", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "_read", "(", "file_path", ")", "\n", "if", "not", "isinstance", "(", "instances", ",", "list", ")", ":", "\n", "                    ", "instances", "=", "[", "instance", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", "]", "\n", "", "if", "not", "instances", ":", "\n", "                    ", "raise", "ConfigurationError", "(", "\"No instances were read from the given filepath {}. \"", "\n", "\"Is the path correct?\"", ".", "format", "(", "file_path", ")", ")", "\n", "", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader._read": [[378, 421], ["logger.info", "data.DataReader.triplet_generator.generate_triplets", "logger.info", "line.strip", "len", "logger.info", "open", "int", "data.DataReader.text_to_instance", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator.generate_triplets", "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.text_to_instance"], ["", "", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "query_file", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            query_file: path to the list of query paper ids, can be train/dev/test\n        \"\"\"", "\n", "query_ids", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "open", "(", "query_file", ")", "]", "\n", "\n", "# logger.info('reading triplets ...')", "\n", "if", "self", ".", "ratio_training_samples", "is", "not", "None", ":", "\n", "            ", "total_len", "=", "len", "(", "query_ids", ")", "\n", "query_ids", "=", "query_ids", "[", ":", "int", "(", "len", "(", "query_ids", ")", "*", "self", ".", "ratio_training_samples", ")", "]", "\n", "logger", ".", "info", "(", "f'using {len(query_ids)} (={len(query_ids)*100/total_len:.4f}%) '", "\n", "f'out of total {total_len} available query ids'", ")", "\n", "\n", "", "count_success", ",", "count_fail", "=", "0", ",", "0", "\n", "\n", "logger", ".", "info", "(", "'reading triplets ...'", ")", "\n", "\n", "# triplets are in format (p0, (p1, count1), (p2, count2))", "\n", "for", "triplet", "in", "self", ".", "triplet_generator", ".", "generate_triplets", "(", "query_ids", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "source_paper", "=", "self", ".", "papers", "[", "triplet", "[", "0", "]", "]", "\n", "pos_paper", "=", "self", ".", "papers", "[", "triplet", "[", "1", "]", "[", "0", "]", "]", "\n", "neg_paper", "=", "self", ".", "papers", "[", "triplet", "[", "2", "]", "[", "0", "]", "]", "\n", "count_success", "+=", "1", "\n", "\n", "# check if all papers have title and abstract", "\n", "failed", "=", "False", "\n", "for", "paper", "in", "(", "source_paper", ",", "pos_paper", ",", "neg_paper", ")", ":", "\n", "                    ", "if", "not", "paper", "[", "'abstract'", "]", "or", "not", "paper", "[", "'title'", "]", ":", "\n", "                        ", "failed", "=", "True", "\n", "break", "\n", "", "", "if", "failed", ":", "\n", "                    ", "count_fail", "+=", "1", "\n", "continue", "\n", "\n", "", "yield", "(", "self", ".", "text_to_instance", "(", "\n", "source_paper", ",", "pos_paper", ",", "neg_paper", ",", "self", ".", "data_source", ")", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "count_fail", "+=", "1", "\n", "pass", "\n", "", "", "logger", ".", "info", "(", "f'done reading triplets success: {count_success}, failed: {count_fail}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.data.DataReader.text_to_instance": [[422, 483], ["data.DataReader._get_paper_features", "data.DataReader._get_paper_features", "data.DataReader._get_paper_features", "data.DataReader._get_author_field", "allennlp.data.instance.Instance", "source_paper.get", "data.DataReader._get_author_field", "data.DataReader._get_author_field", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.LabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.LabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.LabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.ArrayField", "positive_paper.get", "negative_paper.get"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "\n", "source_paper", ":", "dict", ",", "\n", "positive_paper", ":", "Optional", "[", "dict", "]", "=", "None", ",", "\n", "negative_paper", ":", "Optional", "[", "dict", "]", "=", "None", ",", "\n", "data_source", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "mixing_ratio", ":", "Optional", "[", "float", "]", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "source_abstract_tokens", ",", "source_title_tokens", ",", "source_venue", ",", "source_year", ",", "source_body", "=", "self", ".", "_get_paper_features", "(", "source_paper", ")", "\n", "pos_abstract_tokens", ",", "pos_title_tokens", ",", "pos_venue", ",", "pos_year", ",", "pos_body", "=", "self", ".", "_get_paper_features", "(", "positive_paper", ")", "\n", "neg_abstract_tokens", ",", "neg_title_tokens", ",", "neg_venue", ",", "neg_year", ",", "neg_body", "=", "self", ".", "_get_paper_features", "(", "negative_paper", ")", "\n", "\n", "source_author_tokens", "=", "None", "\n", "pos_author_tokens", "=", "None", "\n", "neg_author_tokens", "=", "None", "\n", "\n", "source_author", ",", "source_author_position", "=", "self", ".", "_get_author_field", "(", "source_paper", ".", "get", "(", "'authors'", ",", "[", "]", ")", ")", "\n", "if", "positive_paper", "is", "not", "None", ":", "\n", "            ", "pos_author", ",", "pos_author_position", "=", "self", ".", "_get_author_field", "(", "positive_paper", ".", "get", "(", "'authors'", ",", "[", "]", ")", ")", "\n", "", "if", "negative_paper", "is", "not", "None", ":", "\n", "            ", "neg_author", ",", "neg_author_position", "=", "self", ".", "_get_author_field", "(", "negative_paper", ".", "get", "(", "'authors'", ",", "[", "]", ")", ")", "\n", "", "fields", "=", "{", "\n", "'source_abstract'", ":", "TextField", "(", "source_abstract_tokens", ",", "self", ".", "_token_indexers", ")", ",", "\n", "'source_title'", ":", "TextField", "(", "source_title_tokens", ",", "self", ".", "_token_indexers", ")", ",", "\n", "'source_authors'", ":", "source_author", ",", "\n", "'source_author_positions'", ":", "source_author_position", ",", "\n", "'source_year'", ":", "LabelField", "(", "source_year", ",", "skip_indexing", "=", "True", ",", "label_namespace", "=", "'year'", ")", ",", "\n", "'source_venue'", ":", "TextField", "(", "source_venue", ",", "self", ".", "_token_indexer_venue", ")", ",", "\n", "'source_paper_id'", ":", "MetadataField", "(", "source_paper", "[", "'paper_id'", "]", ")", ",", "\n", "}", "\n", "\n", "if", "source_author_tokens", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'source_author_text'", "]", "=", "TextField", "(", "source_author_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "", "if", "positive_paper", ":", "\n", "            ", "fields", "[", "'pos_abstract'", "]", "=", "TextField", "(", "pos_abstract_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "'pos_title'", "]", "=", "TextField", "(", "pos_title_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "'pos_authors'", "]", "=", "pos_author", "\n", "fields", "[", "'pos_author_positions'", "]", "=", "pos_author_position", "\n", "if", "pos_author_tokens", "is", "not", "None", ":", "\n", "                ", "fields", "[", "'pos_author_text'", "]", "=", "pos_author_tokens", "\n", "", "fields", "[", "'pos_year'", "]", "=", "LabelField", "(", "pos_year", ",", "skip_indexing", "=", "True", ",", "label_namespace", "=", "'year'", ")", "\n", "fields", "[", "'pos_venue'", "]", "=", "TextField", "(", "pos_venue", ",", "self", ".", "_token_indexer_venue", ")", "\n", "fields", "[", "'pos_paper_id'", "]", "=", "MetadataField", "(", "positive_paper", "[", "'paper_id'", "]", ")", "\n", "", "if", "negative_paper", ":", "\n", "            ", "fields", "[", "'neg_abstract'", "]", "=", "TextField", "(", "neg_abstract_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "'neg_title'", "]", "=", "TextField", "(", "neg_title_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "'neg_authors'", "]", "=", "neg_author", "\n", "fields", "[", "'neg_author_positions'", "]", "=", "neg_author_position", "\n", "if", "neg_author_tokens", "is", "not", "None", ":", "\n", "                ", "fields", "[", "'neg_author_text'", "]", "=", "neg_author_tokens", "\n", "", "fields", "[", "'neg_year'", "]", "=", "LabelField", "(", "neg_year", ",", "skip_indexing", "=", "True", ",", "label_namespace", "=", "'year'", ")", "\n", "fields", "[", "'neg_venue'", "]", "=", "TextField", "(", "neg_venue", ",", "self", ".", "_token_indexer_venue", ")", "\n", "fields", "[", "'neg_paper_id'", "]", "=", "MetadataField", "(", "negative_paper", "[", "'paper_id'", "]", ")", "\n", "", "if", "data_source", ":", "\n", "            ", "fields", "[", "'data_source'", "]", "=", "MetadataField", "(", "data_source", ")", "\n", "", "if", "mixing_ratio", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'mixing_ratio'", "]", "=", "ArrayField", "(", "mixing_ratio", ")", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.TripletLoss.__init__": [[35, 51], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["def", "__init__", "(", "self", ",", "margin", "=", "1.0", ",", "distance", "=", "'l2-norm'", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            margin: margin (float, optional): Default: `1`.\n            distance: can be `l2-norm` or `cosine`, or `dot`\n            reduction (string, optional): Specifies the reduction to apply to the output:\n                'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n                'mean': the sum of the output will be divided by the number of\n                elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n                specifying either of those two args will override :attr:`reduction`. Default: 'mean'\n        \"\"\"", "\n", "super", "(", "TripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "distance", "=", "distance", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.TripletLoss.forward": [[52, 84], ["torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.mean", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.sum", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "TypeError", "TypeError", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "query.view", "positive.view", "query.view", "negative.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "positive", ",", "negative", ")", ":", "\n", "        ", "if", "self", ".", "distance", "==", "'l2-norm'", ":", "\n", "            ", "distance_positive", "=", "F", ".", "pairwise_distance", "(", "query", ",", "positive", ")", "\n", "distance_negative", "=", "F", ".", "pairwise_distance", "(", "query", ",", "negative", ")", "\n", "losses", "=", "F", ".", "relu", "(", "distance_positive", "-", "distance_negative", "+", "self", ".", "margin", ")", "\n", "", "elif", "self", ".", "distance", "==", "'cosine'", ":", "# independent of length", "\n", "            ", "distance_positive", "=", "F", ".", "cosine_similarity", "(", "query", ",", "positive", ")", "\n", "distance_negative", "=", "F", ".", "cosine_similarity", "(", "query", ",", "negative", ")", "\n", "losses", "=", "F", ".", "relu", "(", "-", "distance_positive", "+", "distance_negative", "+", "self", ".", "margin", ")", "\n", "", "elif", "self", ".", "distance", "==", "'dot'", ":", "# takes into account the length of vectors", "\n", "            ", "shapes", "=", "query", ".", "shape", "\n", "# batch dot product", "\n", "distance_positive", "=", "torch", ".", "bmm", "(", "\n", "query", ".", "view", "(", "shapes", "[", "0", "]", ",", "1", ",", "shapes", "[", "1", "]", ")", ",", "\n", "positive", ".", "view", "(", "shapes", "[", "0", "]", ",", "shapes", "[", "1", "]", ",", "1", ")", "\n", ")", ".", "reshape", "(", "shapes", "[", "0", "]", ",", ")", "\n", "distance_negative", "=", "torch", ".", "bmm", "(", "\n", "query", ".", "view", "(", "shapes", "[", "0", "]", ",", "1", ",", "shapes", "[", "1", "]", ")", ",", "\n", "negative", ".", "view", "(", "shapes", "[", "0", "]", ",", "shapes", "[", "1", "]", ",", "1", ")", "\n", ")", ".", "reshape", "(", "shapes", "[", "0", "]", ",", ")", "\n", "losses", "=", "F", ".", "relu", "(", "-", "distance_positive", "+", "distance_negative", "+", "self", ".", "margin", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"Unrecognized option for `distance`:{self.distance}\"", ")", "\n", "\n", "", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "            ", "return", "losses", ".", "mean", "(", ")", "\n", "", "elif", "self", ".", "reduction", "==", "'sum'", ":", "\n", "            ", "return", "losses", ".", "sum", "(", ")", "\n", "", "elif", "self", ".", "reduction", "==", "'none'", ":", "\n", "            ", "return", "losses", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"Unrecognized option for `reduction`:{self.reduction}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.BinaryLoss.__init__": [[94, 101], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["def", "__init__", "(", "self", ",", "margin", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            margin: margin (float, optional): Default: `1.0`.\n        \"\"\"", "\n", "super", "(", "BinaryLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "margin", "=", "margin", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.BinaryLoss.forward": [[102, 108], ["torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "positive", ",", "negative", ")", ":", "\n", "        ", "distance_positive", "=", "F", ".", "pairwise_distance", "(", "query", ",", "positive", ")", "\n", "distance_negative", "=", "F", ".", "pairwise_distance", "(", "query", ",", "negative", ")", "\n", "\n", "# adding the two contrastive losses then simplify", "\n", "return", "torch", ".", "pow", "(", "distance_positive", ",", "2", ")", "+", "torch", ".", "pow", "(", "torch", ".", "clamp", "(", "self", ".", "margin", "-", "distance_negative", ",", "min", "=", "0.0", ")", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter.__init__": [[114, 203], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "range", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "initializer", "vocab.add_token_to_namespace", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "torch.nn.TripletMarginLoss", "allennlp.modules.LayerNorm", "allennlp.modules.LayerNorm", "allennlp.modules.LayerNorm", "allennlp.modules.TimeDistributed", "model.BinaryLoss", "model.TripletLoss", "model.Specter.feedforward.get_output_dim", "model.Specter.title_encoder.get_input_dim", "model.Specter.venue_encoder.get_input_dim", "str"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "title_encoder", ":", "Seq2VecEncoder", ",", "\n", "abstract_encoder", ":", "Seq2VecEncoder", ",", "\n", "venue_encoder", ":", "Seq2VecEncoder", ",", "\n", "body_encoder", ":", "Seq2VecEncoder", "=", "None", ",", "\n", "predict_mode", ":", "bool", "=", "False", ",", "\n", "author_text_embedder", ":", "TextFieldEmbedder", "=", "None", ",", "\n", "venue_field_embedder", ":", "TextFieldEmbedder", "=", "None", ",", "\n", "author_text_encoder", ":", "Seq2VecEncoder", "=", "None", ",", "\n", "# author_id_embedder: Optional[Embedding] = None,", "\n", "author_id_embedder", ":", "TextFieldEmbedder", "=", "None", ",", "\n", "# author_position_embedder: Optional[Embedding] = None,", "\n", "author_position_embedder", ":", "TextFieldEmbedder", "=", "None", ",", "\n", "feedforward", ":", "FeedForward", "=", "None", ",", "\n", "author_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ",", "\n", "max_num_authors", ":", "Optional", "[", "int", "]", "=", "5", ",", "\n", "dropout", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "ignore_authors", ":", "Optional", "[", "bool", "]", "=", "False", ",", "\n", "layer_norm", ":", "Optional", "[", "bool", "]", "=", "True", ",", "\n", "embedding_layer_norm", ":", "Optional", "[", "bool", "]", "=", "False", ",", "\n", "loss_distance", ":", "Optional", "[", "str", "]", "=", "'l2-norm'", ",", "\n", "loss_margin", ":", "Optional", "[", "float", "]", "=", "1", ",", "\n", "bert_finetune", ":", "Optional", "[", "bool", "]", "=", "False", ",", "\n", "include_venue", ":", "Optional", "[", "bool", "]", "=", "False", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "Specter", ",", "self", ")", ".", "__init__", "(", "vocab", ",", "regularizer", ")", "\n", "\n", "for", "lbl", "in", "range", "(", "max_num_authors", ")", ":", "\n", "            ", "vocab", ".", "add_token_to_namespace", "(", "token", "=", "str", "(", "lbl", ")", ",", "namespace", "=", "'author_positions'", ")", "\n", "\n", "", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "venue_field_embedder", "=", "venue_field_embedder", "\n", "self", ".", "title_encoder", "=", "title_encoder", "\n", "self", ".", "abstract_encoder", "=", "abstract_encoder", "\n", "self", ".", "body_encoder", "=", "body_encoder", "\n", "self", ".", "venue_encoder", "=", "venue_encoder", "\n", "\n", "self", ".", "predict_mode", "=", "predict_mode", "\n", "\n", "self", ".", "feedforward", "=", "feedforward", "\n", "\n", "if", "loss_distance", "==", "'l2-norm'", ":", "\n", "            ", "self", ".", "loss", "=", "torch", ".", "nn", ".", "TripletMarginLoss", "(", "margin", "=", "loss_margin", ",", "reduction", "=", "'none'", ")", "\n", "", "elif", "loss_distance", "==", "'binary'", ":", "\n", "            ", "self", ".", "loss", "=", "BinaryLoss", "(", "margin", "=", "loss_margin", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss", "=", "TripletLoss", "(", "margin", "=", "loss_margin", ",", "distance", "=", "loss_distance", ",", "reduction", "=", "'none'", ")", "\n", "\n", "", "if", "layer_norm", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "self", ".", "feedforward", ".", "get_output_dim", "(", ")", ")", "\n", "", "self", ".", "do_layer_norm", "=", "layer_norm", "\n", "\n", "# self.layer_norm_author_embedding = LayerNorm(author_feedforward.get_output_dim())", "\n", "\n", "if", "embedding_layer_norm", ":", "\n", "            ", "self", ".", "layer_norm_word_embedding", "=", "LayerNorm", "(", "self", ".", "title_encoder", ".", "get_input_dim", "(", ")", ")", "\n", "self", ".", "layer_norm_word_embedding_venue", "=", "LayerNorm", "(", "self", ".", "venue_encoder", ".", "get_input_dim", "(", ")", ")", "\n", "", "self", ".", "embedding_layer_norm", "=", "embedding_layer_norm", "\n", "\n", "self", ".", "dropout", "=", "Dropout", "(", ")", "\n", "\n", "self", ".", "ignore_authors", "=", "ignore_authors", "\n", "\n", "if", "not", "ignore_authors", ":", "\n", "            ", "self", ".", "author_id_embedder", "=", "author_id_embedder", "\n", "self", ".", "author_position_embedder", "=", "author_position_embedder", "\n", "self", ".", "author_text_embedder", "=", "author_text_embedder", "\n", "self", ".", "author_text_encoder", "=", "author_text_encoder", "\n", "# author representation would be a concatenation of author-id and author-position", "\n", "# [batch, num-authors, auth-dim + position-dim]", "\n", "# we apply timedistributed mlp on top to make this a:", "\n", "# [batch, num-authors, dim]", "\n", "self", ".", "author_time_dist_ff", "=", "TimeDistributed", "(", "author_feedforward", ")", "\n", "\n", "# internal variable showing that the title/abstract should be encoded with a transformer", "\n", "# do not change this as it should be by default `false` in this class", "\n", "# in the inheriting `PaperRepresentationTransoformer` class it is set to true in the constructor", "\n", "# to indicate that the title/abstract should be encoded with a transformer.", "\n", "", "self", ".", "tansformer_encoder", "=", "False", "\n", "\n", "self", ".", "bert_finetune", "=", "bert_finetune", "\n", "self", ".", "include_venue", "=", "include_venue", "\n", "\n", "self", ".", "include_venue", "=", "include_venue", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter.get_embedding_and_mask": [[204, 215], ["allennlp.nn.util.get_text_field_mask", "model.Specter.author_text_embedder", "model.Specter.venue_field_embedder", "model.Specter.text_field_embedder", "TypeError"], "methods", ["None"], ["", "def", "get_embedding_and_mask", "(", "self", ",", "text_field", ",", "embedder_type", "=", "'generic'", ")", ":", "\n", "        ", "if", "embedder_type", "==", "'author'", ":", "\n", "            ", "embedded_", "=", "self", ".", "author_text_embedder", "(", "text_field", ")", "\n", "", "elif", "embedder_type", "==", "'venue'", ":", "\n", "            ", "embedded_", "=", "self", ".", "venue_field_embedder", "(", "text_field", ")", "\n", "", "elif", "embedder_type", "==", "'generic'", ":", "\n", "            ", "embedded_", "=", "self", ".", "text_field_embedder", "(", "text_field", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"Unknown embedder type passed: {embedder_type}\"", ")", "\n", "", "mask_", "=", "util", ".", "get_text_field_mask", "(", "text_field", ")", "\n", "return", "embedded_", ",", "mask_", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter._embed_paper": [[216, 245], ["model.Specter.get_embedding_and_mask", "model.Specter.title_encoder", "model.Specter.dropout", "model.Specter.get_embedding_and_mask", "model.Specter.venue_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.Specter.layer_norm_word_embedding_venue", "model.Specter.dropout"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter.get_embedding_and_mask", "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter.get_embedding_and_mask"], ["", "def", "_embed_paper", "(", "self", ",", "\n", "title", ":", "torch", ".", "Tensor", ",", "\n", "abstract", ":", "torch", ".", "Tensor", ",", "\n", "year", ":", "torch", ".", "Tensor", ",", "\n", "venue", ":", "torch", ".", "Tensor", ",", "\n", "body", ":", "torch", ".", "Tensor", ",", "\n", "author_text", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\" Embed the paper\"\"\"", "\n", "\n", "# in finetuning mode, title and abstract are one long sequence.", "\n", "embedded_title", ",", "title_mask", "=", "self", ".", "get_embedding_and_mask", "(", "title", ")", "\n", "encoded_title", "=", "self", ".", "title_encoder", "(", "embedded_title", ",", "title_mask", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "encoded_title", "=", "self", ".", "dropout", "(", "encoded_title", ")", "\n", "\n", "", "if", "self", ".", "include_venue", ":", "\n", "            ", "embedded_venue", ",", "venue_mask", "=", "self", ".", "get_embedding_and_mask", "(", "venue", ",", "embedder_type", "=", "'venue'", ")", "\n", "if", "self", ".", "embedding_layer_norm", ":", "\n", "                ", "embedded_venue", "=", "self", ".", "layer_norm_word_embedding_venue", "(", "embedded_venue", ")", "\n", "", "encoded_venue", "=", "self", ".", "venue_encoder", "(", "embedded_venue", ",", "venue_mask", ")", "\n", "if", "self", ".", "dropout", ":", "\n", "                ", "encoded_venue", "=", "self", ".", "dropout", "(", "encoded_venue", ")", "\n", "\n", "", "paper_embedding", "=", "torch", ".", "cat", "(", "(", "encoded_title", ",", "encoded_venue", ")", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "paper_embedding", "=", "encoded_title", "\n", "\n", "", "return", "paper_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter.forward": [[247, 331], ["model.Specter._embed_paper", "allennlp.common.checks.ConfigurationError", "model.Specter._embed_paper", "model.Specter._embed_paper", "model.Specter.loss", "mixing_ratio.float", "loss.mean.mean.mean", "loss.mean.mean.mean"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter._embed_paper", "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter._embed_paper", "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter._embed_paper"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# type: ignore", "\n", "source_title", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "source_abstract", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_authors", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_author_positions", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_year", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_venue", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_body", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_author_text", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "source_paper_id", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_title", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_abstract", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_authors", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_author_positions", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_year", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_venue", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_body", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_author_text", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "pos_paper_id", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_title", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_abstract", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_authors", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_author_positions", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_year", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_venue", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_body", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_author_text", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "neg_paper_id", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "data_source", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "mixing_ratio", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "dataset", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Args:\n            Most args are self explanatory\n\n            data_source: in case of multitask data reader, this shows which dataset\n                the instance is coming from\n            mixing_ratio: in case of multitask data reader and when in multitask_data_reader the\n                `use_loss_weighting=True`, this specifies the corresponding loss weight\n                for this instance. It is the loss weight for the corresponding dataset and it\n                is proportional to the square root of the size of the dataset.\n                See `multitask_data_reader.py` for more details\n            dataset: AllenNLPs interleaving dataset reader adds a `dataset` metadatafield\n                So we need to have it here\n        \"\"\"", "\n", "src_paper", "=", "self", ".", "_embed_paper", "(", "source_title", ",", "\n", "source_abstract", ",", "\n", "source_year", ",", "\n", "source_venue", ",", "source_body", ",", "source_author_text", ")", "\n", "\n", "if", "(", "pos_title", "is", "None", "or", "neg_title", "is", "None", ")", "and", "not", "self", ".", "predict_mode", ":", "\n", "            ", "raise", "ConfigurationError", "(", "'Positive or negative paper title is None in training mode. This field is'", "\n", "' mandatory for training. If in prediction mode, '", "\n", "'set `predict_mode`=true in config'", ")", "\n", "\n", "# this will be training mode, embed the positive and negative papers", "\n", "", "if", "pos_title", "is", "not", "None", "and", "neg_title", "is", "not", "None", "and", "not", "self", ".", "predict_mode", ":", "\n", "# embed the positive paper", "\n", "            ", "pos_paper", "=", "self", ".", "_embed_paper", "(", "pos_title", ",", "\n", "pos_abstract", ",", "\n", "pos_year", ",", "\n", "pos_venue", ",", "pos_body", ",", "pos_author_text", ")", "\n", "\n", "# embed the negative paper", "\n", "neg_paper", "=", "self", ".", "_embed_paper", "(", "neg_title", ",", "\n", "neg_abstract", ",", "\n", "neg_year", ",", "\n", "neg_venue", ",", "neg_body", ",", "neg_author_text", ")", "\n", "\n", "loss", "=", "self", ".", "loss", "(", "src_paper", ",", "pos_paper", ",", "neg_paper", ")", "\n", "\n", "if", "mixing_ratio", "is", "not", "None", ":", "\n", "                ", "loss", "*=", "mixing_ratio", ".", "float", "(", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "output_dict", "=", "{", "\"loss\"", ":", "loss", "}", "\n", "\n", "", "else", ":", "# predict mode, we only care about the source paper", "\n", "            ", "output_dict", "=", "{", "\"embedding\"", ":", "src_paper", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.specter.model.Specter.decode": [[332, 335], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "return", "output_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator.__init__": [[246, 261], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "data", ",", "\n", "metadata", ",", "\n", "samples_per_query", ":", "int", "=", "5", ",", "\n", "margin_fraction", ":", "float", "=", "0.5", ",", "\n", "ratio_hard_negatives", ":", "float", "=", "0.3", ",", "\n", "data_source", ":", "str", "=", "None", ")", ":", "\n", "        ", "self", ".", "samples_per_query", "=", "samples_per_query", "\n", "self", ".", "margin_fraction", "=", "margin_fraction", "\n", "self", ".", "ratio_hard_negatives", "=", "ratio_hard_negatives", "\n", "self", ".", "paper_feature_cache", "=", "{", "}", "\n", "self", ".", "metadata", "=", "metadata", "\n", "self", ".", "data_source", "=", "data_source", "\n", "\n", "self", ".", "data", "=", "data", "\n", "# self.triplet_generator = TripletGenerator(", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features": [[269, 287], ["paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get"], "methods", ["None"], ["", "def", "_get_paper_features", "(", "self", ",", "paper", ":", "Optional", "[", "dict", "]", "=", "None", ")", "->", "Tuple", "[", "List", "[", "Token", "]", ",", "List", "[", "Token", "]", ",", "List", "[", "Token", "]", ",", "int", ",", "List", "[", "Token", "]", "]", ":", "\n", "        ", "if", "paper", ":", "\n", "            ", "paper_id", "=", "paper", ".", "get", "(", "'paper_id'", ")", "\n", "if", "paper_id", "in", "self", ".", "paper_feature_cache", ":", "# This function is being called by the same paper multiple times.", "\n", "                ", "return", "self", ".", "paper_feature_cache", "[", "paper_id", "]", "\n", "\n", "", "venue", "=", "paper", ".", "get", "(", "'venue'", ")", "or", "NO_VENUE", "\n", "year", "=", "paper", ".", "get", "(", "'year'", ")", "or", "0", "\n", "body", "=", "paper", ".", "get", "(", "'body'", ")", "\n", "authors", "=", "paper", ".", "get", "(", "'author-names'", ")", "\n", "author_ids", "=", "paper", ".", "get", "(", "'authors'", ")", "\n", "references", "=", "paper", ".", "get", "(", "'references'", ")", "\n", "features", "=", "paper", ".", "get", "(", "'abstract'", ")", ",", "paper", ".", "get", "(", "'title'", ")", ",", "venue", ",", "year", ",", "body", ",", "authors", ",", "references", "\n", "self", ".", "paper_feature_cache", "[", "paper_id", "]", "=", "features", "\n", "return", "features", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator.get_raw_instances": [[288, 361], ["logger.info", "specter.data_utils.triplet_sampling_parallel.generate_triplets", "logger.info", "list", "create_training_files.TrainingInstanceGenerator.metadata.keys", "create_training_files.TrainingInstanceGenerator._get_paper_features", "create_training_files.TrainingInstanceGenerator._get_paper_features", "create_training_files.TrainingInstanceGenerator._get_paper_features"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator.generate_triplets", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator._get_paper_features"], ["", "", "def", "get_raw_instances", "(", "self", ",", "query_ids", ",", "subset_name", "=", "None", ",", "n_jobs", "=", "10", ")", ":", "\n", "        ", "\"\"\"\n        Given query ids, it generates triplets and returns corresponding instances\n        These are raw instances (i.e., untensorized objects )\n        The output of this function is later used with DatasetConstructor.get_instance to convert raw fields to tensors\n\n        Args:\n            query_ids: list of query ids from which triplets are generated\n            subset_name: Optional name to specify the subset (train, test, or val)\n\n        Returns:\n            list of instances (dictionaries)\n            each instance is a dictionary containing fields corresponding to the `query`, `positive` and\n                `negative` papers in the triplet: e.g., query_abstract, query_title, pos_title, neg_title, etc\n        \"\"\"", "\n", "logger", ".", "info", "(", "'Generating triplets ...'", ")", "\n", "count_success", ",", "count_fail", "=", "0", ",", "0", "\n", "# instances = []", "\n", "for", "triplet", "in", "triplet_sampling_parallel", ".", "generate_triplets", "(", "list", "(", "self", ".", "metadata", ".", "keys", "(", ")", ")", ",", "self", ".", "data", ",", "\n", "self", ".", "margin_fraction", ",", "self", ".", "samples_per_query", ",", "\n", "self", ".", "ratio_hard_negatives", ",", "query_ids", ",", "\n", "data_subset", "=", "subset_name", ",", "n_jobs", "=", "n_jobs", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "query_paper", "=", "self", ".", "metadata", "[", "triplet", "[", "0", "]", "]", "\n", "pos_paper", "=", "self", ".", "metadata", "[", "triplet", "[", "1", "]", "[", "0", "]", "]", "\n", "neg_paper", "=", "self", ".", "metadata", "[", "triplet", "[", "2", "]", "[", "0", "]", "]", "\n", "count_success", "+=", "1", "\n", "\n", "# check if all papers have title and abstract (all must have title)", "\n", "failed", "=", "False", "\n", "for", "paper", "in", "(", "query_paper", ",", "pos_paper", ",", "neg_paper", ")", ":", "\n", "                    ", "if", "not", "paper", "[", "'title'", "]", "or", "(", "not", "paper", "[", "'title'", "]", "and", "not", "paper", "[", "'abstract'", "]", ")", ":", "\n", "                        ", "failed", "=", "True", "\n", "break", "\n", "", "", "if", "failed", ":", "\n", "                    ", "count_fail", "+=", "1", "\n", "continue", "\n", "\n", "", "query_abstract", ",", "query_title", ",", "query_venue", ",", "query_year", ",", "query_body", ",", "query_authors", ",", "query_refs", "=", "self", ".", "_get_paper_features", "(", "query_paper", ")", "\n", "pos_abstract", ",", "pos_title", ",", "pos_venue", ",", "pos_year", ",", "pos_body", ",", "pos_authors", ",", "pos_refs", "=", "self", ".", "_get_paper_features", "(", "pos_paper", ")", "\n", "neg_abstract", ",", "neg_title", ",", "neg_venue", ",", "neg_year", ",", "neg_body", ",", "neg_authors", ",", "neg_refs", "=", "self", ".", "_get_paper_features", "(", "neg_paper", ")", "\n", "\n", "instance", "=", "{", "\n", "\"query_abstract\"", ":", "query_abstract", ",", "\n", "\"query_title\"", ":", "query_title", ",", "\n", "\"query_venue\"", ":", "query_venue", ",", "\n", "\"query_year\"", ":", "query_year", ",", "\n", "\"query_body\"", ":", "query_body", ",", "\n", "\"query_authors\"", ":", "query_authors", ",", "\n", "\"query_paper_id\"", ":", "query_paper", "[", "\"paper_id\"", "]", ",", "\n", "\"pos_abstract\"", ":", "pos_abstract", ",", "\n", "\"pos_title\"", ":", "pos_title", ",", "\n", "\"pos_venue\"", ":", "pos_venue", ",", "\n", "\"pos_year\"", ":", "pos_year", ",", "\n", "\"pos_body\"", ":", "pos_body", ",", "\n", "\"pos_authors\"", ":", "pos_authors", ",", "\n", "\"pos_paper_id\"", ":", "pos_paper", "[", "\"paper_id\"", "]", ",", "\n", "\"neg_abstract\"", ":", "neg_abstract", ",", "\n", "\"neg_title\"", ":", "neg_title", ",", "\n", "\"neg_venue\"", ":", "neg_venue", ",", "\n", "\"neg_year\"", ":", "neg_year", ",", "\n", "\"neg_body\"", ":", "neg_body", ",", "\n", "\"neg_authors\"", ":", "neg_authors", ",", "\n", "\"neg_paper_id\"", ":", "neg_paper", "[", "\"paper_id\"", "]", ",", "\n", "\"data_source\"", ":", "self", ".", "data_source", "\n", "}", "\n", "yield", "instance", "\n", "", "except", "KeyError", ":", "\n", "# if there is no title and abstract skip this triplet", "\n", "                ", "count_fail", "+=", "1", "\n", "pass", "\n", "", "", "logger", ".", "info", "(", "f\"done getting triplets, success rate:{(count_success*100/(count_success+count_fail+0.001)):.2f}%,\"", "\n", "f\"total: {count_success+count_fail}\"", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.init_logger": [[28, 44], ["reload", "logging.basicConfig", "logging.info"], "function", ["None"], ["def", "init_logger", "(", "*", ",", "fn", "=", "None", ")", ":", "\n", "\n", "# !!! here", "\n", "    ", "from", "importlib", "import", "reload", "# python 2.x don't need to import reload, use it directly", "\n", "reload", "(", "logging", ")", "\n", "\n", "logging_params", "=", "{", "\n", "'level'", ":", "logging", ".", "INFO", ",", "\n", "'format'", ":", "'%(asctime)s,%(msecs)d %(levelname)-3s [%(filename)s:%(lineno)d] %(message)s'", "\n", "}", "\n", "\n", "if", "fn", "is", "not", "None", ":", "\n", "        ", "logging_params", "[", "'filename'", "]", "=", "fn", "\n", "\n", "", "logging", ".", "basicConfig", "(", "**", "logging_params", ")", "\n", "logging", ".", "info", "(", "'reloading logger'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field": [[76, 98], ["_tokenizer.tokenize", "allennlp.data.fields.TextField", "_tokenizer.tokenize", "allennlp.data.fields.TextField", "len", "range", "len"], "function", ["None"], ["def", "_get_author_field", "(", "authors", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"\n    Converts a list of author ids to their corresponding label and positions\n    Args:\n        authors: list of authors\n\n    Returns:\n        authors and their positions\n    \"\"\"", "\n", "global", "_token_indexer_author_id", "\n", "global", "_token_indexer_author_position", "\n", "global", "_tokenizer", "\n", "authors", "=", "' '", ".", "join", "(", "authors", ")", "\n", "authors", "=", "_tokenizer", ".", "tokenize", "(", "authors", ")", "\n", "if", "len", "(", "authors", ")", ">", "MAX_NUM_AUTHORS", ":", "\n", "        ", "authors", "=", "authors", "[", ":", "MAX_NUM_AUTHORS", "-", "1", "]", "+", "[", "authors", "[", "-", "1", "]", "]", "\n", "", "author_field", "=", "TextField", "(", "authors", ",", "token_indexers", "=", "_token_indexer_author_id", ")", "\n", "\n", "author_positions", "=", "' '", ".", "join", "(", "[", "f'{i:02}'", "for", "i", "in", "range", "(", "len", "(", "authors", ")", ")", "]", ")", "\n", "author_positions_tokens", "=", "_tokenizer", ".", "tokenize", "(", "author_positions", ")", "\n", "position_field", "=", "TextField", "(", "author_positions_tokens", ",", "token_indexers", "=", "_token_indexer_author_position", ")", "\n", "return", "author_field", ",", "position_field", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.set_values": [[100, 131], ["allennlp.data.tokenizers.WordTokenizer", "allennlp.data.token_indexers.PretrainedBertIndexer.from_params", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.tokenizers.word_splitter.BertBasicWordSplitter", "allennlp.common.Params"], "function", ["None"], ["", "def", "set_values", "(", "max_sequence_length", ":", "Optional", "[", "int", "]", "=", "-", "1", ",", "\n", "concat_title_abstract", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "data_source", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "included_text_fields", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "# set global values", "\n", "# note: a class with __init__ would have been a better design", "\n", "# we have this structure for efficiency reasons: to support multiprocessing", "\n", "# as multiprocessing with class methods is slower", "\n", "    ", "global", "_tokenizer", "\n", "global", "_token_indexers", "\n", "global", "_token_indexer_author_id", "\n", "global", "_token_indexer_author_position", "\n", "global", "_token_indexer_venue", "\n", "global", "_token_indexer_id", "\n", "global", "_max_sequence_length", "\n", "global", "_concat_title_abstract", "\n", "global", "_data_source", "\n", "global", "_included_text_fields", "\n", "\n", "if", "_tokenizer", "is", "None", ":", "# if not initialized, initialize the tokenizers and token indexers", "\n", "        ", "_tokenizer", "=", "WordTokenizer", "(", "word_splitter", "=", "BertBasicWordSplitter", "(", "do_lower_case", "=", "bert_params", "[", "\"do_lowercase\"", "]", ")", ")", "\n", "_token_indexers", "=", "{", "\"bert\"", ":", "PretrainedBertIndexer", ".", "from_params", "(", "Params", "(", "bert_params", ")", ")", "}", "\n", "_token_indexer_author_id", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'author'", ")", "}", "\n", "_token_indexer_author_position", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'author_positions'", ")", "}", "\n", "_token_indexer_venue", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'venue'", ")", "}", "\n", "_token_indexer_id", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'id'", ")", "}", "\n", "", "_max_sequence_length", "=", "max_sequence_length", "\n", "_concat_title_abstract", "=", "concat_title_abstract", "\n", "_data_source", "=", "data_source", "\n", "_included_text_fields", "=", "included_text_fields", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_text_tokens": [[133, 140], ["allennlp.data.Token", "allennlp.data.Token"], "function", ["None"], ["", "def", "get_text_tokens", "(", "title_tokens", ",", "abstract_tokens", ",", "abstract_delimiter", ")", ":", "\n", "    ", "\"\"\" concats title and abstract using a delimiter\"\"\"", "\n", "if", "title_tokens", "[", "-", "1", "]", "!=", "Token", "(", "'.'", ")", ":", "\n", "            ", "title_tokens", "+=", "[", "Token", "(", "'.'", ")", "]", "\n", "\n", "", "title_tokens", "=", "title_tokens", "+", "abstract_delimiter", "+", "abstract_tokens", "\n", "return", "title_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_instance": [[141, 243], ["set", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "create_training_files._get_author_field", "create_training_files._get_author_field", "create_training_files._get_author_field", "allennlp.data.Instance", "_included_text_fields.split", "create_training_files.get_text_tokens", "create_training_files.get_text_tokens", "create_training_files.get_text_tokens", "_tokenizer.tokenize", "_tokenizer.tokenize", "_tokenizer.tokenize", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "allennlp.data.Token", "allennlp.data.Token", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "paper.get", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "paper.get", "paper.get", "paper.get"], "function", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files._get_author_field", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_text_tokens", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_text_tokens", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_text_tokens"], ["", "def", "get_instance", "(", "paper", ")", ":", "\n", "    ", "global", "_tokenizer", "\n", "global", "_token_indexers", "\n", "global", "_token_indexer_author_id", "\n", "global", "_token_indexer_author_position", "\n", "global", "_token_indexer_venue", "\n", "global", "_token_indexer_id", "\n", "global", "_max_sequence_length", "\n", "global", "_concat_title_abstract", "\n", "global", "_data_source", "\n", "global", "_included_text_fields", "\n", "\n", "included_text_fields", "=", "set", "(", "_included_text_fields", ".", "split", "(", ")", ")", "\n", "\n", "query_abstract_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"query_abstract\"", ")", "or", "\"\"", ")", "\n", "query_title_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"query_title\"", ")", "or", "\"\"", ")", "\n", "\n", "pos_abstract_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"pos_abstract\"", ")", "or", "\"\"", ")", "\n", "pos_title_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"pos_title\"", ")", "or", "\"\"", ")", "\n", "\n", "neg_abstract_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"neg_abstract\"", ")", "or", "\"\"", ")", "\n", "neg_title_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "\"neg_title\"", ")", "or", "\"\"", ")", "\n", "\n", "if", "_concat_title_abstract", "and", "'abstract'", "in", "included_text_fields", ":", "\n", "        ", "abstract_delimiter", "=", "[", "Token", "(", "'[SEP]'", ")", "]", "\n", "query_title_tokens", "=", "get_text_tokens", "(", "query_title_tokens", ",", "query_abstract_tokens", ",", "abstract_delimiter", ")", "\n", "pos_title_tokens", "=", "get_text_tokens", "(", "pos_title_tokens", ",", "pos_abstract_tokens", ",", "abstract_delimiter", ")", "\n", "neg_title_tokens", "=", "get_text_tokens", "(", "neg_title_tokens", ",", "neg_abstract_tokens", ",", "abstract_delimiter", ")", "\n", "query_abstract_tokens", "=", "pos_abstract_tokens", "=", "neg_abstract_tokens", "=", "[", "]", "\n", "\n", "", "if", "'authors'", "in", "included_text_fields", "and", "_max_sequence_length", ">", "0", ":", "\n", "        ", "max_seq_len", "=", "_max_sequence_length", "-", "15", "# reserve max 15 tokens for author names", "\n", "", "else", ":", "\n", "        ", "max_seq_len", "=", "_max_sequence_length", "\n", "\n", "", "if", "_max_sequence_length", ">", "0", ":", "\n", "        ", "query_abstract_tokens", "=", "query_abstract_tokens", "[", ":", "max_seq_len", "]", "\n", "query_title_tokens", "=", "query_title_tokens", "[", ":", "max_seq_len", "]", "\n", "pos_abstract_tokens", "=", "pos_abstract_tokens", "[", ":", "max_seq_len", "]", "\n", "pos_title_tokens", "=", "pos_title_tokens", "[", ":", "max_seq_len", "]", "\n", "neg_abstract_tokens", "=", "neg_abstract_tokens", "[", ":", "max_seq_len", "]", "\n", "neg_title_tokens", "=", "neg_title_tokens", "[", ":", "max_seq_len", "]", "\n", "\n", "", "if", "'authors'", "in", "included_text_fields", ":", "\n", "        ", "source_author_text", "=", "' '", ".", "join", "(", "paper", ".", "get", "(", "\"query_authors\"", ")", "or", "[", "]", ")", "\n", "pos_author_text", "=", "' '", ".", "join", "(", "paper", ".", "get", "(", "\"pos_authors\"", ")", "or", "[", "]", ")", "\n", "neg_author_text", "=", "' '", ".", "join", "(", "paper", ".", "get", "(", "\"neg_authors\"", ")", "or", "[", "]", ")", "\n", "source_author_tokens", "=", "_tokenizer", ".", "tokenize", "(", "source_author_text", ")", "\n", "pos_author_tokens", "=", "_tokenizer", ".", "tokenize", "(", "pos_author_text", ")", "\n", "neg_author_tokens", "=", "_tokenizer", ".", "tokenize", "(", "neg_author_text", ")", "\n", "\n", "author_delimiter", "=", "[", "Token", "(", "'[unused0]'", ")", "]", "\n", "\n", "query_title_tokens", "=", "query_title_tokens", "+", "author_delimiter", "+", "source_author_tokens", "\n", "pos_title_tokens", "=", "pos_title_tokens", "+", "author_delimiter", "+", "pos_author_tokens", "\n", "neg_title_tokens", "=", "neg_title_tokens", "+", "author_delimiter", "+", "neg_author_tokens", "\n", "\n", "", "query_venue_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'query_venue'", ")", "or", "NO_VENUE", ")", "\n", "pos_venue_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'pos_venue'", ")", "or", "NO_VENUE", ")", "\n", "neg_venue_tokens", "=", "_tokenizer", ".", "tokenize", "(", "paper", ".", "get", "(", "'neg_venue'", ")", "or", "NO_VENUE", ")", "\n", "\n", "# pos_year_tokens = _tokenizer.tokenize(paper.get(\"pos_year\"))", "\n", "# pos_body_tokens = _tokenizer.tokenize(paper.get(\"pos_body\"))", "\n", "#", "\n", "# neg_year_tokens = _tokenizer.tokenize(paper.get(\"neg_year\"))", "\n", "# neg_body_tokens = _tokenizer.tokenize(paper.get(\"neg_body\"))", "\n", "\n", "fields", "=", "{", "\n", "\"source_title\"", ":", "TextField", "(", "query_title_tokens", ",", "token_indexers", "=", "_token_indexers", ")", ",", "\n", "\"pos_title\"", ":", "TextField", "(", "pos_title_tokens", ",", "token_indexers", "=", "_token_indexers", ")", ",", "\n", "\"neg_title\"", ":", "TextField", "(", "neg_title_tokens", ",", "token_indexers", "=", "_token_indexers", ")", ",", "\n", "\"source_venue\"", ":", "TextField", "(", "query_venue_tokens", ",", "token_indexers", "=", "_token_indexer_venue", ")", ",", "\n", "\"pos_venue\"", ":", "TextField", "(", "pos_venue_tokens", ",", "token_indexers", "=", "_token_indexer_venue", ")", ",", "\n", "\"neg_venue\"", ":", "TextField", "(", "neg_venue_tokens", ",", "token_indexers", "=", "_token_indexer_venue", ")", ",", "\n", "'source_paper_id'", ":", "MetadataField", "(", "paper", "[", "'query_paper_id'", "]", ")", ",", "\n", "\"pos_paper_id\"", ":", "MetadataField", "(", "paper", "[", "'pos_paper_id'", "]", ")", ",", "\n", "\"neg_paper_id\"", ":", "MetadataField", "(", "paper", "[", "'neg_paper_id'", "]", ")", ",", "\n", "}", "\n", "\n", "source_authors", ",", "source_author_positions", "=", "_get_author_field", "(", "paper", ".", "get", "(", "\"query_authors\"", ")", "or", "[", "]", ")", "\n", "pos_authors", ",", "pos_author_positions", "=", "_get_author_field", "(", "paper", ".", "get", "(", "\"pos_authors\"", ")", "or", "[", "]", ")", "\n", "neg_authors", ",", "neg_author_positions", "=", "_get_author_field", "(", "paper", ".", "get", "(", "\"neg_authors\"", ")", "or", "[", "]", ")", "\n", "\n", "fields", "[", "'source_authors'", "]", "=", "source_authors", "\n", "fields", "[", "'source_author_positions'", "]", "=", "source_author_positions", "\n", "fields", "[", "'pos_authors'", "]", "=", "pos_authors", "\n", "fields", "[", "'pos_author_positions'", "]", "=", "pos_author_positions", "\n", "fields", "[", "'neg_authors'", "]", "=", "neg_authors", "\n", "fields", "[", "'neg_author_positions'", "]", "=", "neg_author_positions", "\n", "\n", "if", "not", "_concat_title_abstract", ":", "\n", "        ", "if", "query_abstract_tokens", ":", "\n", "            ", "fields", "[", "\"source_abstract\"", "]", "=", "TextField", "(", "query_abstract_tokens", ",", "token_indexers", "=", "_token_indexers", ")", "\n", "", "if", "pos_abstract_tokens", ":", "\n", "            ", "fields", "[", "\"pos_abstract\"", "]", "=", "TextField", "(", "pos_abstract_tokens", ",", "token_indexers", "=", "_token_indexers", ")", "\n", "", "if", "neg_abstract_tokens", ":", "\n", "            ", "fields", "[", "\"neg_abstract\"", "]", "=", "TextField", "(", "neg_abstract_tokens", ",", "token_indexers", "=", "_token_indexers", ")", "\n", "\n", "", "", "if", "_data_source", ":", "\n", "        ", "fields", "[", "\"data_source\"", "]", "=", "MetadataField", "(", "_data_source", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_instances": [[364, 415], ["create_training_files.TrainingInstanceGenerator", "create_training_files.set_values", "RuntimeError", "line.strip", "logger.info", "tqdm.tqdm", "logger.info", "open", "create_training_files.TrainingInstanceGenerator.get_raw_instances", "multiprocessing.Pool", "list", "create_training_files.get_instance", "tqdm.tqdm", "p.imap", "query_ids_file.split"], "function", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.set_values", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.TrainingInstanceGenerator.get_raw_instances", "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_instance"], ["", "", "def", "get_instances", "(", "data", ",", "query_ids_file", ",", "metadata", ",", "data_source", "=", "None", ",", "n_jobs", "=", "1", ",", "n_jobs_raw", "=", "12", ",", "\n", "ratio_hard_negatives", "=", "0.3", ",", "margin_fraction", "=", "0.5", ",", "samples_per_query", "=", "5", ",", "\n", "concat_title_abstract", "=", "False", ",", "included_text_fields", "=", "'title abstract'", ")", ":", "\n", "    ", "\"\"\"\n    Gets allennlp instances from the data file\n    Args:\n        data: the data file (e.g., coviews, or cocites)\n        query_ids_file: train.csv file (one query paper id per line)\n        metadata: a json file containing mapping between paper ids and paper dictionaries\n        data_source: the source of the data (e.g., is it coviews, cite1hop, copdf?)\n        n_jobs: number of jobs to process allennlp instance conversion\n        n_jobs_raw: number of jobs to generate raw triplets\n        ratio_hard_negatives: how many hard nagatives\n        margin_fraction: margin fraction param of triplet generation\n        samples_per_query: how many smaples for each query paper\n\n    Returns:\n        List[Instance]\n    \"\"\"", "\n", "\n", "if", "n_jobs", "==", "0", ":", "\n", "        ", "raise", "RuntimeError", "(", "f\"argument `n_jobs`={n_jobs} is invalid, should be >0\"", ")", "\n", "\n", "", "generator", "=", "TrainingInstanceGenerator", "(", "data", "=", "data", ",", "metadata", "=", "metadata", ",", "data_source", "=", "data_source", ",", "\n", "margin_fraction", "=", "margin_fraction", ",", "ratio_hard_negatives", "=", "ratio_hard_negatives", ",", "\n", "samples_per_query", "=", "samples_per_query", ")", "\n", "\n", "set_values", "(", "max_sequence_length", "=", "512", ",", "\n", "concat_title_abstract", "=", "concat_title_abstract", ",", "\n", "data_source", "=", "data_source", ",", "\n", "included_text_fields", "=", "included_text_fields", ")", "\n", "\n", "query_ids", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "open", "(", "query_ids_file", ")", "]", "\n", "\n", "instances_raw", "=", "[", "e", "for", "e", "in", "generator", ".", "get_raw_instances", "(", "\n", "query_ids", ",", "subset_name", "=", "query_ids_file", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", ",", "n_jobs", "=", "n_jobs_raw", ")", "]", "\n", "\n", "if", "n_jobs", "==", "1", ":", "\n", "        ", "logger", ".", "info", "(", "f'converting raw instances to allennlp instances:'", ")", "\n", "for", "e", "in", "tqdm", ".", "tqdm", "(", "instances_raw", ")", ":", "\n", "            ", "yield", "get_instance", "(", "e", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "f'converting raw instances to allennlp instances ({n_jobs} parallel jobs)'", ")", "\n", "with", "Pool", "(", "n_jobs", ")", "as", "p", ":", "\n", "            ", "instances", "=", "list", "(", "tqdm", ".", "tqdm", "(", "p", ".", "imap", "(", "\n", "get_instance", ",", "instances_raw", ")", ")", ")", "\n", "\n", "# multiprocessing does not work as generator, needs to generate everything", "\n", "# see: https://stackoverflow.com/questions/5318936/python-multiprocessing-pool-lazy-iteration", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.main": [[417, 486], ["pathlib.Path().mkdir", "zip", "open", "logger.info", "json.load", "logger.info", "zip", "pathlib.Path", "open", "json.load", "logger.info", "logger.info", "open", "json.dump", "data_file.split", "open", "pickle.Pickler", "create_training_files.get_instances", "pickle.Pickler.dump", "pickle.Pickler.clear_memo"], "function", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.create_training_files.get_instances"], ["", "", "def", "main", "(", "data_files", ",", "train_ids", ",", "val_ids", ",", "test_ids", ",", "metadata_file", ",", "outdir", ",", "n_jobs", "=", "1", ",", "njobs_raw", "=", "1", ",", "\n", "margin_fraction", "=", "0.5", ",", "ratio_hard_negatives", "=", "0.3", ",", "samples_per_query", "=", "5", ",", "comment", "=", "''", ",", "bert_vocab", "=", "''", ",", "\n", "concat_title_abstract", "=", "False", ",", "included_text_fields", "=", "'title abstract'", ")", ":", "\n", "    ", "\"\"\"\n    Generates instances from a list of datafiles and stores them as a stream of objects\n    Args:\n        data_files: list of files indicating cooccurrence counts\n        train_ids: list of training paper ids corresponding to each data file\n        val_ids: list of validation paper ids\n        test_ids: list of test paper ids\n        metadata_file: path to the metadata file (should cover all data files)\n        outdir: path to an output directory\n        n_jobs: number of parallel jobs for converting instances\n            (in practice we did not find parallelization to help with this)\n        njobs_raw: number of parallel jobs for generating triplets\n            (in practice we found njobs_raw between 10 to 15 to perform fastests)\n        margin_fraction: parameter for triplet generation\n        ratio_hard_negatives: how many of triplets are hard negatives\n        samples_per_query: how many samples per query paper\n        comment: custom comment to add to the file name when saved\n\n\n    Returns:\n        Nothing\n            Creates files corresponding to each data file\n    \"\"\"", "\n", "global", "bert_params", "\n", "bert_params", "[", "'pretrained_model'", "]", "=", "bert_vocab", "\n", "\n", "pathlib", ".", "Path", "(", "outdir", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "metadata_file", ")", "as", "f_in", ":", "\n", "        ", "logger", ".", "info", "(", "f'loading metadata: {metadata_file}'", ")", "\n", "metadata", "=", "json", ".", "load", "(", "f_in", ")", "\n", "\n", "", "for", "data_file", ",", "train_set", ",", "val_set", ",", "test_set", "in", "zip", "(", "data_files", ",", "train_ids", ",", "val_ids", ",", "test_ids", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f'loading data file: {data_file}'", ")", "\n", "with", "open", "(", "data_file", ")", "as", "f_in", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "f_in", ")", "\n", "", "data_source", "=", "data_file", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "[", ":", "-", "5", "]", "# e.g., coviews_v2012", "\n", "if", "comment", ":", "\n", "            ", "data_source", "+=", "f'-{comment}'", "\n", "\n", "", "metrics", "=", "{", "}", "\n", "for", "ds_name", ",", "ds", "in", "zip", "(", "(", "'train'", ",", "'val'", ",", "'test'", ")", ",", "(", "train_set", ",", "val_set", ",", "test_set", ")", ")", ":", "\n", "            ", "logger", ".", "info", "(", "f'getting instances for `{data_source}` and `{ds_name}` set'", ")", "\n", "outfile", "=", "f'{outdir}/{data_source}-{ds_name}.p'", "\n", "logger", ".", "info", "(", "f'writing output {outfile}'", ")", "\n", "with", "open", "(", "outfile", ",", "'wb'", ")", "as", "f_in", ":", "\n", "                ", "pickler", "=", "pickle", ".", "Pickler", "(", "f_in", ")", "\n", "# pickler.fast = True", "\n", "idx", "=", "0", "\n", "for", "instance", "in", "get_instances", "(", "data", "=", "data", ",", "\n", "query_ids_file", "=", "ds", ",", "\n", "metadata", "=", "metadata", ",", "\n", "data_source", "=", "data_source", ",", "\n", "n_jobs", "=", "n_jobs", ",", "n_jobs_raw", "=", "njobs_raw", ",", "\n", "margin_fraction", "=", "margin_fraction", ",", "\n", "ratio_hard_negatives", "=", "ratio_hard_negatives", ",", "\n", "samples_per_query", "=", "samples_per_query", ",", "\n", "concat_title_abstract", "=", "concat_title_abstract", ",", "\n", "included_text_fields", "=", "included_text_fields", ")", ":", "\n", "                    ", "pickler", ".", "dump", "(", "instance", ")", "\n", "idx", "+=", "1", "\n", "# to prevent from memory blow", "\n", "if", "idx", "%", "2000", "==", "0", ":", "\n", "                        ", "pickler", ".", "clear_memo", "(", ")", "\n", "", "", "", "metrics", "[", "ds_name", "]", "=", "idx", "\n", "", "with", "open", "(", "f'{outdir}/{data_source}-metrics.json'", ",", "'w'", ")", "as", "f_out2", ":", "\n", "            ", "json", ".", "dump", "(", "metrics", ",", "f_out2", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling_parallel.is_int": [[32, 35], ["round"], "function", ["None"], ["def", "is_int", "(", "n", ")", ":", "\n", "    ", "\"\"\" checks if a number is float. 2.0 is True while 0.3 is False\"\"\"", "\n", "return", "round", "(", "n", ")", "==", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling_parallel._get_triplet": [[44, 138], ["sorted", "len", "triplet_sampling_parallel.is_int", "triplet_sampling_parallel.is_int", "numpy.ceil", "range", "len", "list", "range", "_coviews[].items", "operator.itemgetter", "len", "len", "len", "math.ceil", "range", "len", "_paper_ids_set.difference", "range", "results.extend", "range", "len", "len", "easy_samples.append", "results.append", "numpy.random.randint", "len", "numpy.random.randint", "numpy.random.randint", "len", "numpy.random.randint", "len", "len", "float", "len"], "function", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.is_int", "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.is_int"], ["def", "_get_triplet", "(", "query", ")", ":", "\n", "    ", "global", "_coviews", "\n", "global", "_margin_fraction", "\n", "global", "_paper_ids_set", "\n", "global", "_samples_per_query", "\n", "global", "_ratio_hard_negatives", "\n", "if", "query", "not", "in", "_coviews", ":", "\n", "        ", "return", "\n", "# self.coviews[query] is a dictionary of format {paper_id: {count: 1, frac: 1}}", "\n", "", "candidates", "=", "[", "(", "k", ",", "v", "[", "'count'", "]", ")", "for", "k", ",", "v", "in", "_coviews", "[", "query", "]", ".", "items", "(", ")", "]", "\n", "candidates", "=", "sorted", "(", "candidates", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "reverse", "=", "True", ")", "\n", "if", "len", "(", "candidates", ")", ">", "1", ":", "\n", "        ", "coview_spread", "=", "candidates", "[", "0", "]", "[", "1", "]", "-", "candidates", "[", "-", "1", "]", "[", "1", "]", "\n", "", "else", ":", "\n", "        ", "coview_spread", "=", "0", "\n", "", "margin", "=", "_margin_fraction", "*", "coview_spread", "# minimum margin of coviews between good and bad sample", "\n", "\n", "# If distance is 1 increase margin to 1 otherwise any margin_fraction will pass", "\n", "if", "is_int", "(", "candidates", "[", "0", "]", "[", "1", "]", ")", "and", "is_int", "(", "candidates", "[", "-", "1", "]", "[", "1", "]", ")", "and", "coview_spread", "==", "1", ":", "\n", "        ", "margin", "=", "np", ".", "ceil", "(", "margin", ")", "\n", "\n", "\n", "", "results", "=", "[", "]", "\n", "\n", "# -------- hard triplets", "\n", "if", "len", "(", "candidates", ")", ">", "2", "and", "margin", "!=", "0", ":", "\n", "\n", "# find valid candidates by going through sorted", "\n", "# list and finding index of first sample with max coviews - margin", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "candidates", ")", ")", ":", "\n", "            ", "if", "candidates", "[", "j", "]", "[", "1", "]", "<", "(", "candidates", "[", "0", "]", "[", "1", "]", "-", "margin", ")", ":", "\n", "                ", "candidates_hard_neg", "=", "candidates", "[", "j", ":", "]", "\n", "break", "\n", "", "else", ":", "\n", "                ", "candidates_hard_neg", "=", "[", "]", "\n", "\n", "", "", "neg_len", "=", "len", "(", "candidates_hard_neg", ")", "\n", "pos_len", "=", "len", "(", "candidates", ")", "-", "neg_len", "\n", "\n", "if", "neg_len", ">", "0", ":", "\n", "\n", "# generate hard candidates", "\n", "            ", "n_hard_samples", "=", "math", ".", "ceil", "(", "_ratio_hard_negatives", "*", "_samples_per_query", ")", "\n", "# if there aren't enough candidates to generate enough unique samples", "\n", "# reduce the number of samples to make it possible for them to be unique", "\n", "if", "(", "pos_len", "*", "neg_len", ")", "<", "n_hard_samples", ":", "\n", "                ", "n_hard_samples", "=", "pos_len", "*", "neg_len", "\n", "\n", "", "for", "i", "in", "range", "(", "n_hard_samples", ")", ":", "\n", "# find the negative sample first.", "\n", "                ", "neg", "=", "candidates_hard_neg", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_hard_neg", ")", ")", "]", "# random neg sample from candidates", "\n", "\n", "candidates_pos", "=", "[", "]", "\n", "# find the good sample. find valid candidates by going through sorted list", "\n", "# in reverse and finding index of first sample with bad sample + margin", "\n", "for", "j", "in", "range", "(", "len", "(", "candidates", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                    ", "if", "candidates", "[", "j", "]", "[", "1", "]", ">", "(", "neg", "[", "1", "]", "+", "margin", ")", ":", "\n", "                        ", "candidates_pos", "=", "candidates", "[", "0", ":", "j", "+", "1", "]", "\n", "break", "\n", "\n", "", "", "if", "candidates_pos", ":", "\n", "                    ", "pos", "=", "candidates_pos", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_pos", ")", ")", "]", "# random pos sample from candidates", "\n", "\n", "# append the good and bad samples with their coview number to output", "\n", "results", ".", "append", "(", "[", "query", ",", "pos", ",", "neg", "]", ")", "\n", "\n", "", "", "", "n_easy_samples", "=", "_samples_per_query", "-", "len", "(", "results", ")", "\n", "\n", "# ---------- easy triplets", "\n", "\n", "# valid candidates are those with zeros", "\n", "candidates_zero", "=", "list", "(", "_paper_ids_set", ".", "difference", "(", "[", "i", "[", "0", "]", "for", "i", "in", "candidates", "]", "+", "[", "query", "]", ")", ")", "\n", "\n", "# find valid candidates for good sample by going through sorted list", "\n", "# in reverse and finding index of first sample with at least margin coviews", "\n", "# note: this is another way to write candidates_pos = [i for i in candidates if i[1] > margin]", "\n", "# but is much faster for large candidate arrays", "\n", "for", "j", "in", "range", "(", "len", "(", "candidates", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "if", "candidates", "[", "j", "]", "[", "1", "]", ">", "margin", "+", "candidates", "[", "-", "1", "]", "[", "1", "]", ":", "\n", "                ", "candidates_pos", "=", "candidates", "[", "0", ":", "j", "+", "1", "]", "\n", "break", "\n", "", "else", ":", "\n", "                ", "candidates_pos", "=", "[", "]", "\n", "\n", "# if there are no valid candidates for good rec, return None to trigger query resample", "\n", "", "", "if", "candidates", "and", "len", "(", "candidates_pos", ")", ">", "0", ":", "\n", "            ", "easy_samples", ":", "List", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_easy_samples", ")", ":", "\n", "                ", "pos", "=", "candidates_pos", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_pos", ")", ")", "]", "# random good sample from candidates", "\n", "neg", "=", "candidates_zero", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_zero", ")", ")", "]", "# random zero", "\n", "easy_samples", ".", "append", "(", "[", "query", ",", "pos", ",", "(", "neg", ",", "float", "(", "\"-inf\"", ")", ")", "]", ")", "\n", "", "results", ".", "extend", "(", "easy_samples", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling_parallel.generate_triplets": [[140, 170], ["set", "logger.info", "triplet_sampling_parallel._get_triplet", "logger.info", "RuntimeError", "tqdm.tqdm", "multiprocessing.Pool", "list", "tqdm.tqdm", "p.imap", "len"], "function", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_triplet"], ["", "def", "generate_triplets", "(", "paper_ids", ",", "coviews", ",", "margin_fraction", ",", "samples_per_query", ",", "ratio_hard_negatives", ",", "query_ids", ",", "data_subset", "=", "None", ",", "n_jobs", "=", "1", ")", ":", "\n", "    ", "global", "_coviews", "\n", "global", "_margin_fraction", "\n", "global", "_samples_per_query", "\n", "global", "_ratio_hard_negatives", "\n", "global", "_query_ids", "\n", "global", "_paper_ids_set", "\n", "\n", "_coviews", "=", "coviews", "\n", "_margin_fraction", "=", "margin_fraction", "\n", "_samples_per_query", "=", "samples_per_query", "\n", "_ratio_hard_negatives", "=", "ratio_hard_negatives", "\n", "_query_ids", "=", "query_ids", "\n", "_paper_ids_set", "=", "set", "(", "paper_ids", ")", "\n", "\n", "logger", ".", "info", "(", "f'generating triplets with: samples_per_query:{_samples_per_query},'", "\n", "f'ratio_hard_negatives:{_ratio_hard_negatives}, margin_fraction:{_margin_fraction}'", ")", "\n", "if", "n_jobs", "==", "1", ":", "\n", "        ", "results", "=", "[", "_get_triplet", "(", "query", ")", "for", "query", "in", "tqdm", ".", "tqdm", "(", "query_ids", ")", "]", "\n", "", "elif", "n_jobs", ">", "0", ":", "\n", "        ", "logger", ".", "info", "(", "f'running {n_jobs} parallel jobs to get triplets for {data_subset or \"not-specified\"} set'", ")", "\n", "with", "Pool", "(", "n_jobs", ")", "as", "p", ":", "\n", "# results = p.imap(_get_triplet, query, coviews, margin_fraction, paper_ids_set, samples_per_query, ratio_hard_negatives)", "\n", "            ", "results", "=", "list", "(", "tqdm", ".", "tqdm", "(", "p", ".", "imap", "(", "_get_triplet", ",", "query_ids", ")", ",", "total", "=", "len", "(", "query_ids", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "f\"bad argument `n_jobs`={n_jobs}, `n_jobs` should be -1 or >0\"", ")", "\n", "", "for", "res", "in", "results", ":", "\n", "        ", "if", "res", ":", "\n", "            ", "for", "triplet", "in", "res", ":", "\n", "                ", "yield", "triplet", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator.__init__": [[34, 58], ["set"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "paper_ids", ":", "List", "[", "str", "]", ",", "\n", "coviews", ":", "Dict", "[", "str", ",", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "]", ",", "\n", "margin_fraction", ":", "float", ",", "\n", "samples_per_query", ":", "int", ",", "\n", "ratio_hard_negatives", ":", "Optional", "[", "float", "]", "=", "0.5", ")", "->", "NoReturn", ":", "\n", "        ", "\"\"\"\n        Args:\n            paper_ids: list of all paper ids\n            coviews: a dictionary where keys are paper ids and values are lists of [paper_id, count] pairs\n                showing the number of coviews for each paper\n            margin_fraction: minimum margin of co-views between positive and negative samples\n            samples_per_query: how many samples for each query\n            query_list: list of query ids. If None, it will return for all papers in the coviews file\n            ratio_hard_negatives: ratio of negative samples selected from difficult and easy negatives\n                respectively. Difficult negative samples are those that are also coviewed but less than\n                a positive sample. Easy negative samples are those with zero coviews\n        \"\"\"", "\n", "self", ".", "paper_ids", "=", "paper_ids", "\n", "self", ".", "paper_ids_set", "=", "set", "(", "paper_ids", ")", "\n", "self", ".", "coviews", "=", "coviews", "\n", "self", ".", "margin_fraction", "=", "margin_fraction", "\n", "self", ".", "samples_per_query", "=", "samples_per_query", "\n", "self", ".", "ratio_hard_negatives", "=", "ratio_hard_negatives", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_triplet": [[59, 79], ["sorted", "triplet_sampling.TripletGenerator._get_hard_negatives", "triplet_sampling.TripletGenerator._get_easy_negatives", "len", "triplet_sampling.is_int", "triplet_sampling.is_int", "numpy.ceil", "len", "triplet_sampling.TripletGenerator.coviews[].items", "operator.itemgetter"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_hard_negatives", "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_easy_negatives", "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.is_int", "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.is_int"], ["", "def", "_get_triplet", "(", "self", ",", "query", ")", ":", "\n", "        ", "if", "query", "not", "in", "self", ".", "coviews", ":", "\n", "            ", "return", "\n", "# self.coviews[query] is a dictionary of format {paper_id: {count: 1, frac: 1}}", "\n", "", "candidates", "=", "[", "(", "k", ",", "v", "[", "'count'", "]", ")", "for", "k", ",", "v", "in", "self", ".", "coviews", "[", "query", "]", ".", "items", "(", ")", "]", "\n", "candidates", "=", "sorted", "(", "candidates", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "reverse", "=", "True", ")", "\n", "if", "len", "(", "candidates", ")", ">", "1", ":", "\n", "            ", "coview_spread", "=", "candidates", "[", "0", "]", "[", "1", "]", "-", "candidates", "[", "-", "1", "]", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "coview_spread", "=", "0", "\n", "", "margin", "=", "self", ".", "margin_fraction", "*", "coview_spread", "# minimum margin of coviews between good and bad sample", "\n", "\n", "# If distance is 1 increase margin to 1 otherwise any margin_fraction will pass", "\n", "if", "is_int", "(", "candidates", "[", "0", "]", "[", "1", "]", ")", "and", "is_int", "(", "candidates", "[", "-", "1", "]", "[", "1", "]", ")", "and", "coview_spread", "==", "1", ":", "\n", "            ", "margin", "=", "np", ".", "ceil", "(", "margin", ")", "\n", "\n", "", "difficult_triplets", "=", "self", ".", "_get_hard_negatives", "(", "query", ",", "candidates", ",", "margin", ")", "\n", "n_easy_samples", "=", "self", ".", "samples_per_query", "-", "len", "(", "difficult_triplets", ")", "\n", "easy_triplets", "=", "self", ".", "_get_easy_negatives", "(", "query", ",", "candidates", ",", "margin", ",", "n_easy_samples", ")", "\n", "return", "difficult_triplets", "+", "easy_triplets", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator.generate_triplets": [[80, 107], ["logger.info", "triplet_sampling.TripletGenerator._get_triplet"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_triplet"], ["", "def", "generate_triplets", "(", "self", ",", "query_ids", ":", "List", "[", "str", "]", ")", "->", "Iterator", "[", "List", "[", "Tuple", "]", "]", ":", "\n", "        ", "\"\"\" Generate triplets from a list of query ids\n\n        This generates a list of triplets each query according to:\n            [(query_id, (positive_id, coviews), (negative_id, coviews)), ...]\n        The upperbound of the list length is according to self.samples_per_query\n\n        Args:\n            query_ids: a list of query paper ids\n\n        Returns:\n            Lists of tuples\n                The format of tuples is according to the triples\n        \"\"\"", "\n", "# logger.info('Generating triplets for queries')", "\n", "count_skipped", "=", "0", "# count how many of the queries are not in coveiws file", "\n", "count_success", "=", "0", "\n", "\n", "for", "query", "in", "query_ids", ":", "# tqdm.tqdm(query_ids):", "\n", "            ", "results", "=", "self", ".", "_get_triplet", "(", "query", ")", "\n", "if", "results", ":", "\n", "                ", "for", "triplet", "in", "results", ":", "\n", "                    ", "yield", "triplet", "\n", "", "count_success", "+=", "1", "\n", "", "else", ":", "\n", "                ", "count_skipped", "+=", "1", "\n", "", "", "logger", ".", "info", "(", "f'Done generating triplets, #successful queries: {count_success},'", "\n", "f'#skipped queries: {count_skipped}'", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_easy_negatives": [[109, 150], ["list", "range", "range", "len", "triplet_sampling.TripletGenerator.paper_ids_set.difference", "len", "easy_samples.append", "len", "numpy.random.randint", "numpy.random.randint", "len", "len", "float"], "methods", ["None"], ["", "def", "_get_easy_negatives", "(", "self", ",", "query_id", ":", "str", ",", "candidates", ":", "List", "[", "Tuple", "[", "str", ",", "float", "]", "]", ",", "margin", ":", "float", ",", "\n", "n_samples", ":", "int", ")", "->", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "str", ",", "int", "]", ",", "Tuple", "[", "str", ",", "int", "]", "]", "]", ":", "\n", "        ", "\"\"\" Given a query get easy negative samples\n        Easy samples are defined by those that are at with 0 coviews/copdfs/etc.\n\n        Args:\n            query_id: string specifying the id of the query paper\n            candidates: a list of candidates, i.e., papers with co-view information with the query paper\n            margin: minimum distance of coviews between positive and negative example\n        \"\"\"", "\n", "# If there are fewer than 2 candidates, return none", "\n", "# if len(candidates) < 2 or margin == 0:", "\n", "if", "len", "(", "candidates", ")", "<", "2", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "# valid candidates are those with zeros", "\n", "", "candidates_zero", "=", "list", "(", "self", ".", "paper_ids_set", ".", "difference", "(", "[", "i", "[", "0", "]", "for", "i", "in", "candidates", "]", "+", "[", "query_id", "]", ")", ")", "\n", "\n", "# find valid candidates for good sample by going through sorted list", "\n", "# in reverse and finding index of first sample with at least margin coviews", "\n", "# note: this is another way to write candidates_pos = [i for i in candidates if i[1] > margin]", "\n", "# but is much faster for large candidate arrays", "\n", "for", "j", "in", "range", "(", "len", "(", "candidates", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "if", "candidates", "[", "j", "]", "[", "1", "]", ">", "margin", "+", "candidates", "[", "-", "1", "]", "[", "1", "]", ":", "\n", "                ", "candidates_pos", "=", "candidates", "[", "0", ":", "j", "+", "1", "]", "\n", "break", "\n", "", "else", ":", "\n", "                ", "candidates_pos", "=", "[", "]", "\n", "\n", "# if there are no valid candidates for good rec, return None to trigger query resample", "\n", "", "", "if", "len", "(", "candidates_pos", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "", "easy_samples", ":", "List", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_samples", ")", ":", "\n", "            ", "pos", "=", "candidates_pos", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_pos", ")", ")", "]", "# random good sample from candidates", "\n", "neg", "=", "candidates_zero", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_zero", ")", ")", "]", "# random zero", "\n", "easy_samples", ".", "append", "(", "[", "query_id", ",", "pos", ",", "(", "neg", ",", "float", "(", "\"-inf\"", ")", ")", "]", ")", "\n", "\n", "", "return", "easy_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.TripletGenerator._get_hard_negatives": [[151, 204], ["range", "len", "math.ceil", "range", "len", "len", "range", "hard_samples.append", "len", "numpy.random.randint", "len", "numpy.random.randint", "len", "len"], "methods", ["None"], ["", "def", "_get_hard_negatives", "(", "self", ",", "query_id", ":", "str", ",", "candidates", ":", "List", "[", "Tuple", "[", "str", ",", "float", "]", "]", ",", "margin", ":", "float", ")", "->", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "str", ",", "int", "]", ",", "Tuple", "[", "str", ",", "int", "]", "]", "]", ":", "\n", "        ", "\"\"\" Given a query get difficult negative samples\n        hard/difficult samples are defined by those samples that have fewer coviews than the positive ones\n\n        Args:\n            query_id: string specifying the id of the query paper\n            candidates: a list of candidates, i.e., papers with co-view information with the query paper\n            margin: minimum distance of coviews between positive and negative example\n        \"\"\"", "\n", "# If there are fewer than 2 candidates, return none", "\n", "if", "len", "(", "candidates", ")", "<", "2", "or", "margin", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "# find valid candidates by going through sorted", "\n", "# list and finding index of first sample with max coviews - margin", "\n", "", "for", "j", "in", "range", "(", "len", "(", "candidates", ")", ")", ":", "\n", "            ", "if", "candidates", "[", "j", "]", "[", "1", "]", "<", "(", "candidates", "[", "0", "]", "[", "1", "]", "-", "margin", ")", ":", "\n", "                ", "candidates_hard_neg", "=", "candidates", "[", "j", ":", "]", "\n", "break", "\n", "", "else", ":", "\n", "                ", "candidates_hard_neg", "=", "[", "]", "\n", "\n", "", "", "neg_len", "=", "len", "(", "candidates_hard_neg", ")", "\n", "pos_len", "=", "len", "(", "candidates", ")", "-", "neg_len", "\n", "\n", "if", "neg_len", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "# generate hard candidates", "\n", "", "samples_per_query", "=", "math", ".", "ceil", "(", "self", ".", "ratio_hard_negatives", "*", "self", ".", "samples_per_query", ")", "\n", "# if there aren't enough candidates to generate enough unique samples", "\n", "# reduce the number of samples to make it possible for them to be unique", "\n", "if", "(", "pos_len", "*", "neg_len", ")", "<", "samples_per_query", ":", "\n", "            ", "samples_per_query", "=", "pos_len", "*", "neg_len", "\n", "\n", "", "hard_samples", ":", "List", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "samples_per_query", ")", ":", "\n", "# find the negative sample first.", "\n", "            ", "neg", "=", "candidates_hard_neg", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_hard_neg", ")", ")", "]", "# random neg sample from candidates", "\n", "\n", "# find the good sample. find valid candidates by going through sorted list", "\n", "# in reverse and finding index of first sample with bad sample + margin", "\n", "for", "j", "in", "range", "(", "len", "(", "candidates", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "if", "candidates", "[", "j", "]", "[", "1", "]", ">", "(", "neg", "[", "1", "]", "+", "margin", ")", ":", "\n", "                    ", "candidates_pos", "=", "candidates", "[", "0", ":", "j", "+", "1", "]", "\n", "break", "\n", "", "", "pos", "=", "candidates_pos", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "candidates_pos", ")", ")", "]", "# random pos sample from candidates", "\n", "\n", "# append the good and bad samples with their coview number to output", "\n", "hard_samples", ".", "append", "(", "[", "query_id", ",", "pos", ",", "neg", "]", ")", "\n", "\n", "", "return", "hard_samples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_specter.data_utils.triplet_sampling.is_int": [[26, 29], ["round"], "function", ["None"], ["def", "is_int", "(", "n", ")", ":", "\n", "    ", "\"\"\" checks if a number is float. 2.0 is True while 0.3 is False\"\"\"", "\n", "return", "round", "(", "n", ")", "==", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed.main": [[23, 66], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.info", "logging.info", "subprocess.run", "command.extend"], "function", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.parse_args", "home.repos.pwc.inspect_result.allenai_specter.specter.predict_command.run"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--ids'", ",", "help", "=", "'path to the paper ids file to embed'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "help", "=", "'path to the model'", ")", "\n", "parser", ".", "add_argument", "(", "'--metadata'", ",", "help", "=", "'path to the paper metadata'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-file'", ",", "help", "=", "'path to the output file'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda-device'", ",", "default", "=", "0", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "default", "=", "1", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--vocab-dir'", ",", "default", "=", "'data/vocab/'", ")", "\n", "parser", ".", "add_argument", "(", "'--included-text-fields'", ",", "default", "=", "'abstract title'", ")", "\n", "parser", ".", "add_argument", "(", "'--weights-file'", ",", "default", "=", "None", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "overrides", "=", "f\"{{'model':{{'predict_mode':'true','include_venue':'false'}},'dataset_reader':{{'type':'specter_data_reader','predict_mode':'true','paper_features_path':'{args.metadata}','included_text_fields': '{args.included_text_fields}'}},'vocabulary':{{'directory_path':'{args.vocab_dir}'}}}}\"", "\n", "\n", "command", "=", "[", "\n", "'python'", ",", "\n", "'specter/predict_command.py'", ",", "\n", "'predict'", ",", "\n", "args", ".", "model", ",", "\n", "args", ".", "ids", ",", "\n", "'--include-package'", ",", "\n", "'specter'", ",", "\n", "'--predictor'", ",", "\n", "'specter_predictor'", ",", "\n", "'--overrides'", ",", "\n", "f'\"{overrides}\"'", ",", "\n", "'--cuda-device'", ",", "\n", "args", ".", "cuda_device", ",", "\n", "'--output-file'", ",", "\n", "args", ".", "output_file", ",", "\n", "'--batch-size'", ",", "\n", "args", ".", "batch_size", ",", "\n", "'--silent'", "\n", "]", "\n", "if", "args", ".", "weights_file", "is", "not", "None", ":", "\n", "        ", "command", ".", "extend", "(", "[", "'--weights-file'", ",", "args", ".", "weights_file", "]", ")", "\n", "\n", "", "logging", ".", "info", "(", "'running command:'", ")", "\n", "logging", ".", "info", "(", "' '", ".", "join", "(", "command", ")", ")", "\n", "\n", "subprocess", ".", "run", "(", "' '", ".", "join", "(", "command", ")", ",", "shell", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.Dataset.__init__": [[11, 19], ["transformers.AutoTokenizer.from_pretrained", "open", "json.load"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_path", ",", "max_length", "=", "512", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "'allenai/specter'", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "# data is assumed to be a json file", "\n", "with", "open", "(", "data_path", ")", "as", "f", ":", "\n", "# key: 'paper_id', value: paper data (including 'title', 'abstract')", "\n", "            ", "self", ".", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.Dataset.__len__": [[20, 22], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.Dataset.batches": [[23, 45], ["embed_papers_hf.Dataset.data.items", "len", "embed_papers_hf.Dataset.tokenizer", "embed_papers_hf.Dataset.to", "batch_ids.append", "batch.append", "embed_papers_hf.Dataset.tokenizer", "embed_papers_hf.Dataset.to", "d.get", "d.get"], "methods", ["None"], ["", "def", "batches", "(", "self", ")", ":", "\n", "# create batches", "\n", "        ", "batch", "=", "[", "]", "\n", "batch_ids", "=", "[", "]", "\n", "batch_size", "=", "self", ".", "batch_size", "\n", "i", "=", "0", "\n", "for", "k", ",", "d", "in", "self", ".", "data", ".", "items", "(", ")", ":", "\n", "            ", "if", "(", "i", ")", "%", "batch_size", "!=", "0", "or", "i", "==", "0", ":", "\n", "                ", "batch_ids", ".", "append", "(", "k", ")", "\n", "batch", ".", "append", "(", "d", "[", "'title'", "]", "+", "' '", "+", "(", "d", ".", "get", "(", "'abstract'", ")", "or", "''", ")", ")", "\n", "", "else", ":", "\n", "                ", "input_ids", "=", "self", ".", "tokenizer", "(", "batch", ",", "padding", "=", "True", ",", "truncation", "=", "True", ",", "\n", "return_tensors", "=", "\"pt\"", ",", "max_length", "=", "self", ".", "max_length", ")", "\n", "yield", "input_ids", ".", "to", "(", "'cuda'", ")", ",", "batch_ids", "\n", "batch_ids", "=", "[", "k", "]", "\n", "batch", "=", "[", "d", "[", "'title'", "]", "+", "' '", "+", "(", "d", ".", "get", "(", "'abstract'", ")", "or", "''", ")", "]", "\n", "", "i", "+=", "1", "\n", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "            ", "input_ids", "=", "self", ".", "tokenizer", "(", "batch", ",", "padding", "=", "True", ",", "truncation", "=", "True", ",", "\n", "return_tensors", "=", "\"pt\"", ",", "max_length", "=", "self", ".", "max_length", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "'cuda'", ")", "\n", "yield", "input_ids", ",", "batch_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.Model.__init__": [[48, 52], ["transformers.AutoModel.from_pretrained", "embed_papers_hf.Model.model.to", "embed_papers_hf.Model.model.eval"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", "=", "AutoModel", ".", "from_pretrained", "(", "'allenai/specter'", ")", "\n", "self", ".", "model", ".", "to", "(", "'cuda'", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.Model.__call__": [[53, 56], ["embed_papers_hf.Model.model"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "output", "=", "self", ".", "model", "(", "**", "input_ids", ")", "\n", "return", "output", ".", "last_hidden_state", "[", ":", ",", "0", ",", ":", "]", "# cls token", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.main": [[57, 79], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "embed_papers_hf.Dataset", "embed_papers_hf.Model", "tqdm.auto.tqdm", "pathlib.Path().parent.mkdir", "embed_papers_hf.Dataset.batches", "batches.append", "Model.", "zip", "open", "results.values", "model.unbind", "fout.write", "len", "embedding.detach().cpu().numpy().tolist", "pathlib.Path", "json.dumps", "embedding.detach().cpu().numpy", "embedding.detach().cpu", "embedding.detach"], "function", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.parse_args", "home.repos.pwc.inspect_result.allenai_specter.scripts.embed_papers_hf.Dataset.batches"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data-path'", ",", "help", "=", "'path to a json file containing paper metadata'", ")", "\n", "parser", ".", "add_argument", "(", "'--output'", ",", "help", "=", "'path to write the output embeddings file. '", "\n", "'the output format is jsonlines where each line has \"paper_id\" and \"embedding\" keys'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'batch size for prediction'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "dataset", "=", "Dataset", "(", "data_path", "=", "args", ".", "data_path", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "model", "=", "Model", "(", ")", "\n", "results", "=", "{", "}", "\n", "batches", "=", "[", "]", "\n", "for", "batch", ",", "batch_ids", "in", "tqdm", "(", "dataset", ".", "batches", "(", ")", ",", "total", "=", "len", "(", "dataset", ")", "//", "args", ".", "batch_size", ")", ":", "\n", "        ", "batches", ".", "append", "(", "batch", ")", "\n", "emb", "=", "model", "(", "batch", ")", "\n", "for", "paper_id", ",", "embedding", "in", "zip", "(", "batch_ids", ",", "emb", ".", "unbind", "(", ")", ")", ":", "\n", "            ", "results", "[", "paper_id", "]", "=", "{", "\"paper_id\"", ":", "paper_id", ",", "\"embedding\"", ":", "embedding", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "}", "\n", "\n", "", "", "pathlib", ".", "Path", "(", "args", ".", "output", ")", ".", "parent", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "args", ".", "output", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "res", "in", "results", ".", "values", "(", ")", ":", "\n", "            ", "fout", ".", "write", "(", "json", ".", "dumps", "(", "res", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled.__init__": [[67, 86], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["def", "__init__", "(", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "word_splitter", ":", "WordSplitter", "=", "None", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "max_sequence_length", ":", "int", "=", "256", ",", "\n", "concat_title_abstract", ":", "bool", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Dataset reader that uses pickled preprocessed instances\n        Consumes the output resulting from data_utils/create_training_files.py\n\n        the additional arguments are not used here and are for compatibility with\n        the other data reader at prediction time\n        \"\"\"", "\n", "self", ".", "max_sequence_length", "=", "max_sequence_length", "\n", "self", ".", "token_indexers", "=", "token_indexers", "\n", "self", ".", "_concat_title_abstract", "=", "concat_title_abstract", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled._read": [[87, 137], ["open", "pickle.Unpickler", "pickle.Unpickler.load", "pickle.Unpickler.load.fields.get", "pickle.Unpickler.load.fields.get", "pickle.Unpickler.load.fields.get", "tokens.extend", "tokens.extend", "tokens.extend", "pickle.Unpickler.load.fields.pop", "allennlp.data.tokenizers.token.Token"], "methods", ["None"], ["", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            file_path: path to the pickled instances\n        \"\"\"", "\n", "with", "open", "(", "file_path", ",", "'rb'", ")", "as", "f_in", ":", "\n", "            ", "unpickler", "=", "pickle", ".", "Unpickler", "(", "f_in", ")", "\n", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "instance", "=", "unpickler", ".", "load", "(", ")", "\n", "# compatibility with old models:", "\n", "# for field in instance.fields:", "\n", "#     if hasattr(instance.fields[field], '_token_indexers') and 'bert' in instance.fields[field]._token_indexers:", "\n", "#         if not hasattr(instance.fields['source_title']._token_indexers['bert'], '_truncate_long_sequences'):", "\n", "#             instance.fields[field]._token_indexers['bert']._truncate_long_sequences = True", "\n", "#             instance.fields[field]._token_indexers['bert']._token_min_padding_length = 0", "\n", "if", "self", ".", "max_sequence_length", ":", "\n", "                        ", "for", "paper_type", "in", "[", "'source'", ",", "'pos'", ",", "'neg'", "]", ":", "\n", "                            ", "if", "self", ".", "_concat_title_abstract", ":", "\n", "                                ", "tokens", "=", "[", "]", "\n", "title_field", "=", "instance", ".", "fields", ".", "get", "(", "f'{paper_type}_title'", ")", "\n", "abst_field", "=", "instance", ".", "fields", ".", "get", "(", "f'{paper_type}_abstract'", ")", "\n", "if", "title_field", ":", "\n", "                                    ", "tokens", ".", "extend", "(", "title_field", ".", "tokens", ")", "\n", "", "if", "tokens", ":", "\n", "                                    ", "tokens", ".", "extend", "(", "[", "Token", "(", "'[SEP]'", ")", "]", ")", "\n", "", "if", "abst_field", ":", "\n", "                                    ", "tokens", ".", "extend", "(", "abst_field", ".", "tokens", ")", "\n", "", "if", "title_field", ":", "\n", "                                    ", "title_field", ".", "tokens", "=", "tokens", "\n", "instance", ".", "fields", "[", "f'{paper_type}_title'", "]", "=", "title_field", "\n", "", "elif", "abst_field", ":", "\n", "                                    ", "abst_field", ".", "tokens", "=", "tokens", "\n", "instance", ".", "fields", "[", "f'{paper_type}_title'", "]", "=", "abst_field", "\n", "", "else", ":", "\n", "                                    ", "yield", "None", "\n", "# title_tokens = get_text_tokens(query_title_tokens, query_abstract_tokens, abstract_delimiter)", "\n", "# pos_title_tokens = get_text_tokens(pos_title_tokens, pos_abstract_tokens, abstract_delimiter)", "\n", "# neg_title_tokens = get_text_tokens(neg_title_tokens, neg_abstract_tokens, abstract_delimiter)", "\n", "# query_abstract_tokens = pos_abstract_tokens = neg_abstract_tokens = []", "\n", "", "", "for", "field_type", "in", "[", "'title'", ",", "'abstract'", ",", "'authors'", ",", "'author_positions'", "]", ":", "\n", "                                ", "field", "=", "paper_type", "+", "'_'", "+", "field_type", "\n", "if", "instance", ".", "fields", ".", "get", "(", "field", ")", ":", "\n", "                                    ", "instance", ".", "fields", "[", "field", "]", ".", "tokens", "=", "instance", ".", "fields", "[", "field", "]", ".", "tokens", "[", "\n", ":", "self", ".", "max_sequence_length", "]", "\n", "", "if", "field_type", "==", "'abstract'", "and", "self", ".", "_concat_title_abstract", ":", "\n", "                                    ", "instance", ".", "fields", ".", "pop", "(", "field", ",", "None", ")", "\n", "", "", "", "", "yield", "instance", "\n", "", "except", "EOFError", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorker.__init__": [[140, 146], ["train.DataReaderFromPickled", "train.IterableDataSetMultiWorker.datareaderfp._read"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled._read"], ["    ", "def", "__init__", "(", "self", ",", "file_path", ",", "tokenizer", ",", "size", ",", "block_size", "=", "100", ")", ":", "\n", "        ", "self", ".", "datareaderfp", "=", "DataReaderFromPickled", "(", "max_sequence_length", "=", "512", ")", "\n", "self", ".", "data_instances", "=", "self", ".", "datareaderfp", ".", "_read", "(", "file_path", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "block_size", "=", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorker.__iter__": [[147, 169], ["torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "itertools.islice", "itertools.islice", "train.IterableDataSetMultiWorker.ai2_to_transformers", "train.IterableDataSetMultiWorker.ai2_to_transformers", "int"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers", "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "if", "worker_info", "is", "None", ":", "\n", "            ", "iter_end", "=", "self", ".", "size", "\n", "for", "data_instance", "in", "itertools", ".", "islice", "(", "self", ".", "data_instances", ",", "iter_end", ")", ":", "\n", "                ", "data_input", "=", "self", ".", "ai2_to_transformers", "(", "data_instance", ",", "self", ".", "tokenizer", ")", "\n", "yield", "data_input", "\n", "\n", "", "", "else", ":", "\n", "# when num_worker is greater than 1. we implement multiple process data loading.", "\n", "            ", "iter_end", "=", "self", ".", "size", "\n", "worker_id", "=", "worker_info", ".", "id", "\n", "num_workers", "=", "worker_info", ".", "num_workers", "\n", "i", "=", "0", "\n", "for", "data_instance", "in", "itertools", ".", "islice", "(", "self", ".", "data_instances", ",", "iter_end", ")", ":", "\n", "                ", "if", "int", "(", "i", "/", "self", ".", "block_size", ")", "%", "num_workers", "!=", "worker_id", ":", "\n", "                    ", "i", "=", "i", "+", "1", "\n", "pass", "\n", "", "else", ":", "\n", "                    ", "i", "=", "i", "+", "1", "\n", "data_input", "=", "self", ".", "ai2_to_transformers", "(", "data_instance", ",", "self", ".", "tokenizer", ")", "\n", "yield", "data_input", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorker.ai2_to_transformers": [[170, 201], ["tokenizer", "tokenizer", "tokenizer", "str", "str", "str"], "methods", ["None"], ["", "", "", "", "def", "ai2_to_transformers", "(", "self", ",", "data_instance", ",", "tokenizer", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            data_instance: ai2 data instance\n            tokenizer: huggingface transformers tokenizer\n        \"\"\"", "\n", "source_tokens", "=", "data_instance", "[", "\"source_title\"", "]", ".", "tokens", "\n", "source_title", "=", "tokenizer", "(", "' '", ".", "join", "(", "[", "str", "(", "token", ")", "for", "token", "in", "source_tokens", "]", ")", ",", "\n", "truncation", "=", "True", ",", "padding", "=", "\"max_length\"", ",", "return_tensors", "=", "\"pt\"", ",", "\n", "max_length", "=", "512", ")", "\n", "source_input", "=", "{", "'input_ids'", ":", "source_title", "[", "'input_ids'", "]", "[", "0", "]", ",", "\n", "'token_type_ids'", ":", "source_title", "[", "'token_type_ids'", "]", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "source_title", "[", "'attention_mask'", "]", "[", "0", "]", "}", "\n", "\n", "pos_tokens", "=", "data_instance", "[", "\"pos_title\"", "]", ".", "tokens", "\n", "pos_title", "=", "tokenizer", "(", "' '", ".", "join", "(", "[", "str", "(", "token", ")", "for", "token", "in", "pos_tokens", "]", ")", ",", "\n", "truncation", "=", "True", ",", "padding", "=", "\"max_length\"", ",", "return_tensors", "=", "\"pt\"", ",", "max_length", "=", "512", ")", "\n", "\n", "pos_input", "=", "{", "'input_ids'", ":", "pos_title", "[", "'input_ids'", "]", "[", "0", "]", ",", "\n", "'token_type_ids'", ":", "pos_title", "[", "'token_type_ids'", "]", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "pos_title", "[", "'attention_mask'", "]", "[", "0", "]", "}", "\n", "\n", "neg_tokens", "=", "data_instance", "[", "\"neg_title\"", "]", ".", "tokens", "\n", "neg_title", "=", "tokenizer", "(", "' '", ".", "join", "(", "[", "str", "(", "token", ")", "for", "token", "in", "neg_tokens", "]", ")", ",", "\n", "truncation", "=", "True", ",", "padding", "=", "\"max_length\"", ",", "return_tensors", "=", "\"pt\"", ",", "max_length", "=", "512", ")", "\n", "\n", "neg_input", "=", "{", "'input_ids'", ":", "neg_title", "[", "'input_ids'", "]", "[", "0", "]", ",", "\n", "'token_type_ids'", ":", "neg_title", "[", "'token_type_ids'", "]", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "neg_title", "[", "'attention_mask'", "]", "[", "0", "]", "}", "\n", "\n", "return", "source_input", ",", "pos_input", ",", "neg_input", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.__init__": [[204, 210], ["train.DataReaderFromPickled", "train.IterableDataSetMultiWorkerTestStep.datareaderfp._read"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.DataReaderFromPickled._read"], ["    ", "def", "__init__", "(", "self", ",", "file_path", ",", "tokenizer", ",", "size", ",", "block_size", "=", "100", ")", ":", "\n", "        ", "self", ".", "datareaderfp", "=", "DataReaderFromPickled", "(", "max_sequence_length", "=", "512", ")", "\n", "self", ".", "data_instances", "=", "self", ".", "datareaderfp", ".", "_read", "(", "file_path", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "block_size", "=", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.__iter__": [[211, 233], ["torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "itertools.islice", "itertools.islice", "train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers", "train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers", "int"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers", "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "if", "worker_info", "is", "None", ":", "\n", "            ", "iter_end", "=", "self", ".", "size", "\n", "for", "data_instance", "in", "itertools", ".", "islice", "(", "self", ".", "data_instances", ",", "iter_end", ")", ":", "\n", "                ", "data_input", "=", "self", ".", "ai2_to_transformers", "(", "data_instance", ",", "self", ".", "tokenizer", ")", "\n", "yield", "data_input", "\n", "\n", "", "", "else", ":", "\n", "# when num_worker is greater than 1. we implement multiple process data loading.", "\n", "            ", "iter_end", "=", "self", ".", "size", "\n", "worker_id", "=", "worker_info", ".", "id", "\n", "num_workers", "=", "worker_info", ".", "num_workers", "\n", "i", "=", "0", "\n", "for", "data_instance", "in", "itertools", ".", "islice", "(", "self", ".", "data_instances", ",", "iter_end", ")", ":", "\n", "                ", "if", "int", "(", "i", "/", "self", ".", "block_size", ")", "%", "num_workers", "!=", "worker_id", ":", "\n", "                    ", "i", "=", "i", "+", "1", "\n", "pass", "\n", "", "else", ":", "\n", "                    ", "i", "=", "i", "+", "1", "\n", "data_input", "=", "self", ".", "ai2_to_transformers", "(", "data_instance", ",", "self", ".", "tokenizer", ")", "\n", "yield", "data_input", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.IterableDataSetMultiWorkerTestStep.ai2_to_transformers": [[234, 251], ["tokenizer", "str"], "methods", ["None"], ["", "", "", "", "def", "ai2_to_transformers", "(", "self", ",", "data_instance", ",", "tokenizer", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            data_instance: ai2 data instance\n            tokenizer: huggingface transformers tokenizer\n        \"\"\"", "\n", "source_tokens", "=", "data_instance", "[", "\"source_title\"", "]", ".", "tokens", "\n", "source_title", "=", "tokenizer", "(", "' '", ".", "join", "(", "[", "str", "(", "token", ")", "for", "token", "in", "source_tokens", "]", ")", ",", "\n", "truncation", "=", "True", ",", "padding", "=", "\"max_length\"", ",", "return_tensors", "=", "\"pt\"", ",", "\n", "max_length", "=", "512", ")", "\n", "source_input", "=", "{", "'input_ids'", ":", "source_title", "[", "'input_ids'", "]", "[", "0", "]", ",", "\n", "'token_type_ids'", ":", "source_title", "[", "'token_type_ids'", "]", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "source_title", "[", "'attention_mask'", "]", "[", "0", "]", "}", "\n", "\n", "source_paper_id", "=", "data_instance", "[", "'source_paper_id'", "]", ".", "metadata", "\n", "\n", "return", "source_input", ",", "source_paper_id", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.TripletLoss.__init__": [[257, 273], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["def", "__init__", "(", "self", ",", "margin", "=", "1.0", ",", "distance", "=", "'l2-norm'", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            margin: margin (float, optional): Default: `1`.\n            distance: can be `l2-norm` or `cosine`, or `dot`\n            reduction (string, optional): Specifies the reduction to apply to the output:\n                'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n                'mean': the sum of the output will be divided by the number of\n                elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n                specifying either of those two args will override :attr:`reduction`. Default: 'mean'\n        \"\"\"", "\n", "super", "(", "TripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "distance", "=", "distance", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.TripletLoss.forward": [[274, 306], ["torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.pairwise_distance", "torch.relu", "torch.relu", "torch.relu", "torch.relu.mean", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "torch.relu", "torch.relu", "torch.relu", "torch.relu.sum", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.bmm().reshape", "torch.relu", "torch.relu", "torch.relu", "TypeError", "TypeError", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "query.view", "positive.view", "query.view", "negative.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "positive", ",", "negative", ")", ":", "\n", "        ", "if", "self", ".", "distance", "==", "'l2-norm'", ":", "\n", "            ", "distance_positive", "=", "F", ".", "pairwise_distance", "(", "query", ",", "positive", ")", "\n", "distance_negative", "=", "F", ".", "pairwise_distance", "(", "query", ",", "negative", ")", "\n", "losses", "=", "F", ".", "relu", "(", "distance_positive", "-", "distance_negative", "+", "self", ".", "margin", ")", "\n", "", "elif", "self", ".", "distance", "==", "'cosine'", ":", "# independent of length", "\n", "            ", "distance_positive", "=", "F", ".", "cosine_similarity", "(", "query", ",", "positive", ")", "\n", "distance_negative", "=", "F", ".", "cosine_similarity", "(", "query", ",", "negative", ")", "\n", "losses", "=", "F", ".", "relu", "(", "-", "distance_positive", "+", "distance_negative", "+", "self", ".", "margin", ")", "\n", "", "elif", "self", ".", "distance", "==", "'dot'", ":", "# takes into account the length of vectors", "\n", "            ", "shapes", "=", "query", ".", "shape", "\n", "# batch dot product", "\n", "distance_positive", "=", "torch", ".", "bmm", "(", "\n", "query", ".", "view", "(", "shapes", "[", "0", "]", ",", "1", ",", "shapes", "[", "1", "]", ")", ",", "\n", "positive", ".", "view", "(", "shapes", "[", "0", "]", ",", "shapes", "[", "1", "]", ",", "1", ")", "\n", ")", ".", "reshape", "(", "shapes", "[", "0", "]", ",", ")", "\n", "distance_negative", "=", "torch", ".", "bmm", "(", "\n", "query", ".", "view", "(", "shapes", "[", "0", "]", ",", "1", ",", "shapes", "[", "1", "]", ")", ",", "\n", "negative", ".", "view", "(", "shapes", "[", "0", "]", ",", "shapes", "[", "1", "]", ",", "1", ")", "\n", ")", ".", "reshape", "(", "shapes", "[", "0", "]", ",", ")", "\n", "losses", "=", "F", ".", "relu", "(", "-", "distance_positive", "+", "distance_negative", "+", "self", ".", "margin", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"Unrecognized option for `distance`:{self.distance}\"", ")", "\n", "\n", "", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "            ", "return", "losses", ".", "mean", "(", ")", "\n", "", "elif", "self", ".", "reduction", "==", "'sum'", ":", "\n", "            ", "return", "losses", ".", "sum", "(", ")", "\n", "", "elif", "self", ".", "reduction", "==", "'none'", ":", "\n", "            ", "return", "losses", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"Unrecognized option for `reduction`:{self.reduction}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__": [[309, 331], ["pytorch_lightning.LightningModule.__init__", "isinstance", "logger.info", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "train.TripletLoss", "argparse.Namespace"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "init_args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "init_args", ",", "dict", ")", ":", "\n", "# for loading the checkpoint, pl passes a dict (hparams are saved as dict)", "\n", "            ", "init_args", "=", "Namespace", "(", "**", "init_args", ")", "\n", "", "checkpoint_path", "=", "init_args", ".", "checkpoint_path", "\n", "logger", ".", "info", "(", "f'loading model from checkpoint: {checkpoint_path}'", ")", "\n", "\n", "self", ".", "hparams", "=", "init_args", "\n", "self", ".", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"allenai/scibert_scivocab_cased\"", ")", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"allenai/scibert_scivocab_cased\"", ")", "\n", "self", ".", "tokenizer", ".", "model_max_length", "=", "self", ".", "model", ".", "config", ".", "max_position_embeddings", "\n", "self", ".", "hparams", ".", "seqlen", "=", "self", ".", "model", ".", "config", ".", "max_position_embeddings", "\n", "self", ".", "triple_loss", "=", "TripletLoss", "(", ")", "\n", "# number of training instances", "\n", "self", ".", "training_size", "=", "None", "\n", "# number of testing instances", "\n", "self", ".", "validation_size", "=", "None", "\n", "# number of test instances", "\n", "self", ".", "test_size", "=", "None", "\n", "# This is a dictionary to save the embeddings for source papers in test step.", "\n", "self", ".", "embedding_output", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.forward": [[332, 336], ["train.Specter.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "# in lightning, forward defines the prediction/inference actions", "\n", "        ", "source_embedding", "=", "self", ".", "model", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "return", "source_embedding", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter._get_loader": [[337, 359], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "train.IterableDataSetMultiWorkerTestStep", "train.IterableDataSetMultiWorker"], "methods", ["None"], ["", "def", "_get_loader", "(", "self", ",", "split", ")", ":", "\n", "        ", "if", "split", "==", "'train'", ":", "\n", "            ", "fname", "=", "self", ".", "hparams", ".", "train_file", "\n", "size", "=", "self", ".", "training_size", "\n", "", "elif", "split", "==", "'dev'", ":", "\n", "            ", "fname", "=", "self", ".", "hparams", ".", "dev_file", "\n", "size", "=", "self", ".", "validation_size", "\n", "", "elif", "split", "==", "'test'", ":", "\n", "            ", "fname", "=", "self", ".", "hparams", ".", "test_file", "\n", "size", "=", "self", ".", "test_size", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "\n", "", "if", "split", "==", "'test'", ":", "\n", "            ", "dataset", "=", "IterableDataSetMultiWorkerTestStep", "(", "file_path", "=", "fname", ",", "tokenizer", "=", "self", ".", "tokenizer", ",", "size", "=", "size", ")", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "IterableDataSetMultiWorker", "(", "file_path", "=", "fname", ",", "tokenizer", "=", "self", ".", "tokenizer", ",", "size", "=", "size", ")", "\n", "\n", "# pin_memory enables faster data transfer to CUDA-enabled GPU.", "\n", "", "loader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "self", ".", "hparams", ".", "batch_size", ",", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "shuffle", "=", "False", ",", "pin_memory", "=", "False", ")", "\n", "return", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.setup": [[360, 362], ["train.Specter._get_loader"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter._get_loader"], ["", "def", "setup", "(", "self", ",", "mode", ")", ":", "\n", "        ", "self", ".", "train_loader", "=", "self", ".", "_get_loader", "(", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.train_dataloader": [[363, 365], ["None"], "methods", ["None"], ["", "def", "train_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "train_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.val_dataloader": [[366, 369], ["train.Specter._get_loader"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter._get_loader"], ["", "def", "val_dataloader", "(", "self", ")", ":", "\n", "        ", "self", ".", "val_dataloader_obj", "=", "self", ".", "_get_loader", "(", "'dev'", ")", "\n", "return", "self", ".", "val_dataloader_obj", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.test_dataloader": [[370, 372], ["train.Specter._get_loader"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter._get_loader"], ["", "def", "test_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_get_loader", "(", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.total_steps": [[373, 382], ["max"], "methods", ["None"], ["", "@", "property", "\n", "def", "total_steps", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"The number of total training steps that will be run. Used for lr scheduler purposes.\"\"\"", "\n", "num_devices", "=", "max", "(", "1", ",", "self", ".", "hparams", ".", "total_gpus", ")", "# TODO: consider num_tpu_cores", "\n", "effective_batch_size", "=", "self", ".", "hparams", ".", "batch_size", "*", "self", ".", "hparams", ".", "grad_accum", "*", "num_devices", "\n", "# dataset_size = len(self.train_loader.dataset)", "\n", "\"\"\"The size of the training data need to be coded with more accurate number\"\"\"", "\n", "dataset_size", "=", "training_size", "\n", "return", "(", "dataset_size", "/", "effective_batch_size", ")", "*", "self", ".", "hparams", ".", "num_epochs", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.get_lr_scheduler": [[383, 390], ["get_schedule_func"], "methods", ["None"], ["", "def", "get_lr_scheduler", "(", "self", ")", ":", "\n", "        ", "get_schedule_func", "=", "arg_to_scheduler", "[", "self", ".", "hparams", ".", "lr_scheduler", "]", "\n", "scheduler", "=", "get_schedule_func", "(", "\n", "self", ".", "opt", ",", "num_warmup_steps", "=", "self", ".", "hparams", ".", "warmup_steps", ",", "num_training_steps", "=", "self", ".", "total_steps", "\n", ")", "\n", "scheduler", "=", "{", "\"scheduler\"", ":", "scheduler", ",", "\"interval\"", ":", "\"step\"", ",", "\"frequency\"", ":", "1", "}", "\n", "return", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.configure_optimizers": [[391, 419], ["train.Specter.get_lr_scheduler", "transformers.optimization.Adafactor", "transformers.AdamW", "model.named_parameters", "model.named_parameters", "any", "any"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.get_lr_scheduler"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"", "\n", "model", "=", "self", ".", "model", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "self", ".", "hparams", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "if", "self", ".", "hparams", ".", "adafactor", ":", "\n", "            ", "optimizer", "=", "Adafactor", "(", "\n", "optimizer_grouped_parameters", ",", "lr", "=", "self", ".", "hparams", ".", "lr", ",", "scale_parameter", "=", "False", ",", "relative_step", "=", "False", "\n", ")", "\n", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "AdamW", "(", "\n", "optimizer_grouped_parameters", ",", "lr", "=", "self", ".", "hparams", ".", "lr", ",", "eps", "=", "self", ".", "hparams", ".", "adam_epsilon", "\n", ")", "\n", "", "self", ".", "opt", "=", "optimizer", "\n", "\n", "scheduler", "=", "self", ".", "get_lr_scheduler", "(", ")", "\n", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.training_step": [[420, 432], ["train.Specter.triple_loss", "train.Specter.log", "train.Specter.log", "train.Specter.model", "train.Specter.model", "train.Specter.model", "lr_scheduler.get_last_lr"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "source_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "0", "]", ")", "[", "1", "]", "\n", "pos_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "1", "]", ")", "[", "1", "]", "\n", "neg_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "2", "]", ")", "[", "1", "]", "\n", "\n", "loss", "=", "self", ".", "triple_loss", "(", "source_embedding", ",", "pos_embedding", ",", "neg_embedding", ")", "\n", "\n", "lr_scheduler", "=", "self", ".", "trainer", ".", "lr_schedulers", "[", "0", "]", "[", "\"scheduler\"", "]", "\n", "\n", "self", ".", "log", "(", "'train_loss'", ",", "loss", ",", "on_step", "=", "True", ",", "on_epoch", "=", "False", ",", "prog_bar", "=", "True", ",", "logger", "=", "True", ")", "\n", "self", ".", "log", "(", "'rate'", ",", "lr_scheduler", ".", "get_last_lr", "(", ")", "[", "-", "1", "]", ",", "on_step", "=", "True", ",", "on_epoch", "=", "False", ",", "prog_bar", "=", "True", ",", "logger", "=", "True", ")", "\n", "return", "{", "\"loss\"", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.validation_step": [[433, 441], ["train.Specter.triple_loss", "train.Specter.log", "train.Specter.model", "train.Specter.model", "train.Specter.model"], "methods", ["None"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "source_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "0", "]", ")", "[", "1", "]", "\n", "pos_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "1", "]", ")", "[", "1", "]", "\n", "neg_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "2", "]", ")", "[", "1", "]", "\n", "\n", "loss", "=", "self", ".", "triple_loss", "(", "source_embedding", ",", "pos_embedding", ",", "neg_embedding", ")", "\n", "self", ".", "log", "(", "'val_loss'", ",", "loss", ",", "on_step", "=", "True", ",", "on_epoch", "=", "False", ",", "prog_bar", "=", "True", ")", "\n", "return", "{", "'val_loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter._eval_end": [[442, 452], ["torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "results.items", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "isinstance", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "v.detach().cpu().item", "v.detach().cpu", "v.detach"], "methods", ["None"], ["", "def", "_eval_end", "(", "self", ",", "outputs", ")", "->", "tuple", ":", "\n", "        ", "avg_loss", "=", "torch", ".", "stack", "(", "[", "x", "[", "'val_loss'", "]", "for", "x", "in", "outputs", "]", ")", ".", "mean", "(", ")", "\n", "if", "self", ".", "trainer", ".", "use_ddp", ":", "\n", "            ", "torch", ".", "distributed", ".", "all_reduce", "(", "avg_loss", ",", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "avg_loss", "/=", "self", ".", "trainer", ".", "world_size", "\n", "", "results", "=", "{", "\"avg_val_loss\"", ":", "avg_loss", "}", "\n", "for", "k", ",", "v", "in", "results", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "results", "[", "k", "]", "=", "v", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.validation_epoch_end": [[453, 457], ["train.Specter._eval_end", "train.Specter.log"], "methods", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter._eval_end"], ["", "def", "validation_epoch_end", "(", "self", ",", "outputs", ":", "list", ")", "->", "dict", ":", "\n", "        ", "ret", "=", "self", ".", "_eval_end", "(", "outputs", ")", "\n", "\n", "self", ".", "log", "(", "'avg_val_loss'", ",", "ret", "[", "\"avg_val_loss\"", "]", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.test_epoch_end": [[458, 466], ["open", "fp.write", "value.detach().cpu().numpy().tolist", "train.Specter.embedding_output.items", "value.detach().cpu().numpy", "json.dumps", "value.detach().cpu", "value.detach"], "methods", ["None"], ["", "def", "test_epoch_end", "(", "self", ",", "outputs", ":", "list", ")", ":", "\n", "# convert the dictionary of {id1:embedding1, id2:embedding2, ...} to a", "\n", "# list of dictionaries [{'id':'id1', 'embedding': 'embedding1'},{'id':'id2', 'embedding': 'embedding2'}, ...]", "\n", "        ", "embedding_output_list", "=", "[", "{", "'id'", ":", "key", ",", "'embedding'", ":", "value", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "self", ".", "embedding_output", ".", "items", "(", ")", "]", "\n", "\n", "with", "open", "(", "self", ".", "hparams", ".", "save_dir", "+", "'/embedding_result.jsonl'", ",", "'w'", ")", "as", "fp", ":", "\n", "            ", "fp", ".", "write", "(", "'\\n'", ".", "join", "(", "json", ".", "dumps", "(", "i", ")", "for", "i", "in", "embedding_output_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.Specter.test_step": [[467, 475], ["dict", "train.Specter.embedding_output.update", "train.Specter.model", "zip"], "methods", ["None"], ["", "", "def", "test_step", "(", "self", ",", "batch", ",", "batch_nb", ")", ":", "\n", "        ", "source_embedding", "=", "self", ".", "model", "(", "**", "batch", "[", "0", "]", ")", "[", "1", "]", "\n", "source_paper_id", "=", "batch", "[", "1", "]", "\n", "\n", "batch_embedding_output", "=", "dict", "(", "zip", "(", "source_paper_id", ",", "source_embedding", ")", ")", "\n", "\n", "# .update() will automatically remove duplicates.", "\n", "self", ".", "embedding_output", ".", "update", "(", "batch_embedding_output", ")", "\n", "# return self.validation_step(batch, batch_nb)", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.parse_args": [[477, 523], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "glob.glob", "f.split"], "function", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.parse_args"], ["", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint_path'", ",", "default", "=", "None", ",", "help", "=", "'path to the model (if not setting checkpoint)'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_file'", ")", "\n", "parser", ".", "add_argument", "(", "'--dev_file'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_file'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_dir'", ",", "default", "=", "None", ",", "help", "=", "'optionally provide a directory of the data and train/test/dev files will be automatically detected'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "default", "=", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_accum'", ",", "default", "=", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--gpus'", ",", "default", "=", "'1'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "1918", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_only'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_checkpoint'", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--limit_test_batches'", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "'--limit_val_batches'", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "'--val_check_interval'", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "'--num_epochs'", ",", "default", "=", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "2e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "\"kwarg passed to DataLoader\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adafactor\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--num_samples'", ",", "default", "=", "None", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr_scheduler\"", ",", "\n", "default", "=", "\"linear\"", ",", "\n", "choices", "=", "arg_to_scheduler_choices", ",", "\n", "metavar", "=", "arg_to_scheduler_metavar", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Learning rate scheduler\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "input_dir", "is", "not", "None", ":", "\n", "        ", "files", "=", "glob", ".", "glob", "(", "args", ".", "input_dir", "+", "'/*'", ")", "\n", "for", "f", "in", "files", ":", "\n", "            ", "fname", "=", "f", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "if", "'train'", "in", "fname", ":", "\n", "                ", "args", ".", "train_file", "=", "f", "\n", "", "elif", "'dev'", "in", "fname", "or", "'val'", "in", "fname", ":", "\n", "                ", "args", ".", "dev_file", "=", "f", "\n", "", "elif", "'test'", "in", "fname", ":", "\n", "                ", "args", ".", "test_file", "=", "f", "\n", "", "", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.get_train_params": [[525, 540], ["isinstance", "isinstance", "len"], "function", ["None"], ["", "def", "get_train_params", "(", "args", ")", ":", "\n", "    ", "train_params", "=", "{", "}", "\n", "train_params", "[", "\"precision\"", "]", "=", "16", "if", "args", ".", "fp16", "else", "32", "\n", "if", "(", "isinstance", "(", "args", ".", "gpus", ",", "int", ")", "and", "args", ".", "gpus", ">", "1", ")", "or", "(", "isinstance", "(", "args", ".", "gpus", ",", "list", ")", "and", "len", "(", "args", ".", "gpus", ")", ">", "1", ")", ":", "\n", "        ", "train_params", "[", "\"distributed_backend\"", "]", "=", "\"ddp\"", "\n", "", "else", ":", "\n", "        ", "train_params", "[", "\"distributed_backend\"", "]", "=", "None", "\n", "", "train_params", "[", "\"accumulate_grad_batches\"", "]", "=", "args", ".", "grad_accum", "\n", "train_params", "[", "'track_grad_norm'", "]", "=", "-", "1", "\n", "train_params", "[", "'limit_val_batches'", "]", "=", "args", ".", "limit_val_batches", "\n", "train_params", "[", "'val_check_interval'", "]", "=", "args", ".", "val_check_interval", "\n", "train_params", "[", "'gpus'", "]", "=", "args", ".", "gpus", "\n", "train_params", "[", "'max_epochs'", "]", "=", "args", ".", "num_epochs", "\n", "train_params", "[", "'log_every_n_steps'", "]", "=", "log_every_n_steps", "\n", "return", "train_params", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.main": [[542, 595], ["train.parse_args", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "print", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "list", "len", "int", "print", "Specter.load_from_checkpoint", "pytorch_lightning.Trainer", "pl.Trainer.test", "train.Specter", "pytorch_lightning.loggers.TensorBoardLogger", "pytorch_lightning.callbacks.ModelCheckpoint", "train.get_train_params", "pytorch_lightning.Trainer", "pl.Trainer.fit", "map", "parse_args.gpus.split"], "function", ["home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.parse_args", "home.repos.pwc.inspect_result.allenai_specter.pytorch_lightning_training_script.train.get_train_params"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "num_workers", "==", "0", ":", "\n", "        ", "print", "(", "\"num_workers cannot be less than 1\"", ")", "\n", "return", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "", "if", "','", "in", "args", ".", "gpus", ":", "\n", "        ", "args", ".", "gpus", "=", "list", "(", "map", "(", "int", ",", "args", ".", "gpus", ".", "split", "(", "','", ")", ")", ")", "\n", "args", ".", "total_gpus", "=", "len", "(", "args", ".", "gpus", ")", "\n", "", "else", ":", "\n", "        ", "args", ".", "gpus", "=", "int", "(", "args", ".", "gpus", ")", "\n", "args", ".", "total_gpus", "=", "args", ".", "gpus", "\n", "\n", "", "if", "args", ".", "test_only", ":", "\n", "        ", "print", "(", "'loading model...'", ")", "\n", "model", "=", "Specter", ".", "load_from_checkpoint", "(", "args", ".", "test_checkpoint", ")", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "gpus", "=", "args", ".", "gpus", ",", "limit_val_batches", "=", "args", ".", "limit_val_batches", ")", "\n", "trainer", ".", "test", "(", "model", ")", "\n", "\n", "", "else", ":", "\n", "\n", "        ", "model", "=", "Specter", "(", "args", ")", "\n", "\n", "# default logger used by trainer", "\n", "logger", "=", "TensorBoardLogger", "(", "\n", "save_dir", "=", "args", ".", "save_dir", ",", "\n", "version", "=", "0", ",", "\n", "name", "=", "'pl-logs'", "\n", ")", "\n", "\n", "# second part of the path shouldn't be f-string", "\n", "filepath", "=", "f'{args.save_dir}/version_{logger.version}/checkpoints/'", "+", "'ep-{epoch}_avg_val_loss-{avg_val_loss:.3f}'", "\n", "checkpoint_callback", "=", "ModelCheckpoint", "(", "\n", "filepath", "=", "filepath", ",", "\n", "save_top_k", "=", "1", ",", "\n", "verbose", "=", "True", ",", "\n", "monitor", "=", "'avg_val_loss'", ",", "# monitors metrics logged by self.log.", "\n", "mode", "=", "'min'", ",", "\n", "prefix", "=", "''", "\n", ")", "\n", "\n", "extra_train_params", "=", "get_train_params", "(", "args", ")", "\n", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "logger", "=", "logger", ",", "\n", "checkpoint_callback", "=", "checkpoint_callback", ",", "\n", "**", "extra_train_params", ")", "\n", "\n", "trainer", ".", "fit", "(", "model", ")", "\n", "\n"]]}