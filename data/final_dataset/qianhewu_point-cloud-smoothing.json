{"home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.Accuracy.at_radii": [[15, 17], ["NotImplementedError"], "methods", ["None"], ["    ", "def", "at_radii", "(", "self", ",", "radii", ":", "np", ".", "ndarray", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.ApproximateAccuracy.__init__": [[20, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_file_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "data_file_path", "=", "data_file_path", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.ApproximateAccuracy.at_radii": [[23, 26], ["pandas.read_csv", "numpy.array", "analyze.ApproximateAccuracy.at_radius"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radius"], ["", "def", "at_radii", "(", "self", ",", "radii", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "self", ".", "data_file_path", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "return", "np", ".", "array", "(", "[", "self", ".", "at_radius", "(", "df", ",", "radius", ")", "for", "radius", "in", "radii", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.ApproximateAccuracy.at_radius": [[27, 29], ["None"], "methods", ["None"], ["", "def", "at_radius", "(", "self", ",", "df", ":", "pd", ".", "DataFrame", ",", "radius", ":", "float", ")", ":", "\n", "        ", "return", "(", "df", "[", "\"correct\"", "]", "&", "(", "df", "[", "\"radius\"", "]", ">=", "radius", ")", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.__init__": [[32, 36], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_file_path", ":", "str", ",", "alpha", ":", "float", ",", "rho", ":", "float", ")", ":", "\n", "        ", "self", ".", "data_file_path", "=", "data_file_path", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "rho", "=", "rho", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radii": [[37, 40], ["pandas.read_csv", "numpy.array", "analyze.HighProbAccuracy.at_radius"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radius"], ["", "def", "at_radii", "(", "self", ",", "radii", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "self", ".", "data_file_path", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "return", "np", ".", "array", "(", "[", "self", ".", "at_radius", "(", "df", ",", "radius", ")", "for", "radius", "in", "radii", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radius": [[41, 46], ["len", "math.sqrt", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.log", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.log"], ["", "def", "at_radius", "(", "self", ",", "df", ":", "pd", ".", "DataFrame", ",", "radius", ":", "float", ")", ":", "\n", "        ", "mean", "=", "(", "df", "[", "\"correct\"", "]", "&", "(", "df", "[", "\"radius\"", "]", ">=", "radius", ")", ")", ".", "mean", "(", ")", "\n", "num_examples", "=", "len", "(", "df", ")", "\n", "return", "(", "mean", "-", "self", ".", "alpha", "-", "math", ".", "sqrt", "(", "self", ".", "alpha", "*", "(", "1", "-", "self", ".", "alpha", ")", "*", "math", ".", "log", "(", "1", "/", "self", ".", "rho", ")", "/", "num_examples", ")", "\n", "-", "math", ".", "log", "(", "1", "/", "self", ".", "rho", ")", "/", "(", "3", "*", "num_examples", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.Line.__init__": [[49, 54], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "quantity", ":", "Accuracy", ",", "legend", ":", "str", ",", "plot_fmt", ":", "str", "=", "\"\"", ",", "scale_x", ":", "float", "=", "1", ")", ":", "\n", "        ", "self", ".", "quantity", "=", "quantity", "\n", "self", ".", "legend", "=", "legend", "\n", "self", ".", "plot_fmt", "=", "plot_fmt", "\n", "self", ".", "scale_x", "=", "scale_x", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.plot_certified_accuracy": [[56, 75], ["numpy.arange", "matplotlib.figure", "matplotlib.ylim", "matplotlib.xlim", "matplotlib.tick_params", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.legend", "matplotlib.savefig", "matplotlib.tight_layout", "matplotlib.title", "matplotlib.tight_layout", "matplotlib.savefig", "matplotlib.close", "matplotlib.plot", "line.quantity.at_radii"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radii"], ["", "", "def", "plot_certified_accuracy", "(", "outfile", ":", "str", ",", "title", ":", "str", ",", "max_radius", ":", "float", ",", "\n", "lines", ":", "List", "[", "Line", "]", ",", "radius_step", ":", "float", "=", "0.01", ")", "->", "None", ":", "\n", "    ", "radii", "=", "np", ".", "arange", "(", "0", ",", "max_radius", "+", "radius_step", ",", "radius_step", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "plt", ".", "plot", "(", "radii", "*", "line", ".", "scale_x", ",", "line", ".", "quantity", ".", "at_radii", "(", "radii", ")", ",", "line", ".", "plot_fmt", ")", "\n", "\n", "", "plt", ".", "ylim", "(", "(", "0", ",", "1", ")", ")", "\n", "plt", ".", "xlim", "(", "(", "0", ",", "max_radius", ")", ")", "\n", "plt", ".", "tick_params", "(", "labelsize", "=", "14", ")", "\n", "plt", ".", "xlabel", "(", "\"radius\"", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "ylabel", "(", "\"certified accuracy\"", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "legend", "(", "[", "method", ".", "legend", "for", "method", "in", "lines", "]", ",", "loc", "=", "'upper right'", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "savefig", "(", "outfile", "+", "\".pdf\"", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "title", "(", "title", ",", "fontsize", "=", "20", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "savefig", "(", "outfile", "+", "\".png\"", ",", "dpi", "=", "300", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.smallplot_certified_accuracy": [[77, 94], ["numpy.arange", "matplotlib.figure", "matplotlib.ylim", "matplotlib.xlim", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.tick_params", "matplotlib.gca().xaxis.set_major_locator", "matplotlib.legend", "matplotlib.tight_layout", "matplotlib.savefig", "matplotlib.close", "matplotlib.plot", "matplotlib.MultipleLocator", "method.quantity.at_radii", "matplotlib.gca"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radii"], ["", "def", "smallplot_certified_accuracy", "(", "outfile", ":", "str", ",", "title", ":", "str", ",", "max_radius", ":", "float", ",", "\n", "methods", ":", "List", "[", "Line", "]", ",", "radius_step", ":", "float", "=", "0.01", ",", "xticks", "=", "0.5", ")", "->", "None", ":", "\n", "    ", "radii", "=", "np", ".", "arange", "(", "0", ",", "max_radius", "+", "radius_step", ",", "radius_step", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "for", "method", "in", "methods", ":", "\n", "        ", "plt", ".", "plot", "(", "radii", ",", "method", ".", "quantity", ".", "at_radii", "(", "radii", ")", ",", "method", ".", "plot_fmt", ")", "\n", "\n", "", "plt", ".", "ylim", "(", "(", "0", ",", "1", ")", ")", "\n", "plt", ".", "xlim", "(", "(", "0", ",", "max_radius", ")", ")", "\n", "plt", ".", "xlabel", "(", "\"radius\"", ",", "fontsize", "=", "22", ")", "\n", "plt", ".", "ylabel", "(", "\"certified accuracy\"", ",", "fontsize", "=", "22", ")", "\n", "plt", ".", "tick_params", "(", "labelsize", "=", "20", ")", "\n", "plt", ".", "gca", "(", ")", ".", "xaxis", ".", "set_major_locator", "(", "plt", ".", "MultipleLocator", "(", "xticks", ")", ")", "\n", "plt", ".", "legend", "(", "[", "method", ".", "legend", "for", "method", "in", "methods", "]", ",", "loc", "=", "'upper right'", ",", "fontsize", "=", "20", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "savefig", "(", "outfile", "+", "\".pdf\"", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.latex_table_certified_accuracy": [[96, 121], ["numpy.arange", "numpy.zeros", "enumerate", "open", "open.write", "open.write", "enumerate", "open.close", "method.quantity.at_radii", "open.write", "open.write", "enumerate", "open.write", "len", "len", "open.write", "accuracies[].argmax"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radii"], ["", "def", "latex_table_certified_accuracy", "(", "outfile", ":", "str", ",", "radius_start", ":", "float", ",", "radius_stop", ":", "float", ",", "radius_step", ":", "float", ",", "\n", "methods", ":", "List", "[", "Line", "]", ")", ":", "\n", "    ", "radii", "=", "np", ".", "arange", "(", "radius_start", ",", "radius_stop", "+", "radius_step", ",", "radius_step", ")", "\n", "accuracies", "=", "np", ".", "zeros", "(", "(", "len", "(", "methods", ")", ",", "len", "(", "radii", ")", ")", ")", "\n", "for", "i", ",", "method", "in", "enumerate", "(", "methods", ")", ":", "\n", "        ", "accuracies", "[", "i", ",", ":", "]", "=", "method", ".", "quantity", ".", "at_radii", "(", "radii", ")", "\n", "\n", "", "f", "=", "open", "(", "outfile", ",", "'w'", ")", "\n", "\n", "for", "radius", "in", "radii", ":", "\n", "        ", "f", ".", "write", "(", "\"& $r = {:.3}$\"", ".", "format", "(", "radius", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\\\\\\\\\n\"", ")", "\n", "\n", "f", ".", "write", "(", "\"\\midrule\\n\"", ")", "\n", "\n", "for", "i", ",", "method", "in", "enumerate", "(", "methods", ")", ":", "\n", "        ", "f", ".", "write", "(", "method", ".", "legend", ")", "\n", "for", "j", ",", "radius", "in", "enumerate", "(", "radii", ")", ":", "\n", "            ", "if", "i", "==", "accuracies", "[", ":", ",", "j", "]", ".", "argmax", "(", ")", ":", "\n", "                ", "txt", "=", "r\" & \\textbf{\"", "+", "\"{:.2f}\"", ".", "format", "(", "accuracies", "[", "i", ",", "j", "]", ")", "+", "\"}\"", "\n", "", "else", ":", "\n", "                ", "txt", "=", "\" & {:.2f}\"", ".", "format", "(", "accuracies", "[", "i", ",", "j", "]", ")", "\n", "", "f", ".", "write", "(", "txt", ")", "\n", "", "f", ".", "write", "(", "\"\\\\\\\\\\n\"", ")", "\n", "", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.markdown_table_certified_accuracy": [[123, 151], ["numpy.arange", "numpy.zeros", "enumerate", "open", "open.write", "open.write", "open.write", "range", "open.write", "enumerate", "open.close", "method.quantity.at_radii", "open.write", "len", "open.write", "open.write", "enumerate", "open.write", "len", "len", "open.write", "accuracies[].argmax"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.analyze.HighProbAccuracy.at_radii"], ["", "def", "markdown_table_certified_accuracy", "(", "outfile", ":", "str", ",", "radius_start", ":", "float", ",", "radius_stop", ":", "float", ",", "radius_step", ":", "float", ",", "\n", "methods", ":", "List", "[", "Line", "]", ")", ":", "\n", "    ", "radii", "=", "np", ".", "arange", "(", "radius_start", ",", "radius_stop", "+", "radius_step", ",", "radius_step", ")", "\n", "accuracies", "=", "np", ".", "zeros", "(", "(", "len", "(", "methods", ")", ",", "len", "(", "radii", ")", ")", ")", "\n", "for", "i", ",", "method", "in", "enumerate", "(", "methods", ")", ":", "\n", "        ", "accuracies", "[", "i", ",", ":", "]", "=", "method", ".", "quantity", ".", "at_radii", "(", "radii", ")", "\n", "\n", "", "f", "=", "open", "(", "outfile", ",", "'w'", ")", "\n", "f", ".", "write", "(", "\"|  | \"", ")", "\n", "for", "radius", "in", "radii", ":", "\n", "        ", "f", ".", "write", "(", "\"r = {:.3} |\"", ".", "format", "(", "radius", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "f", ".", "write", "(", "\"| --- | \"", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "radii", ")", ")", ":", "\n", "        ", "f", ".", "write", "(", "\" --- |\"", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "for", "i", ",", "method", "in", "enumerate", "(", "methods", ")", ":", "\n", "        ", "f", ".", "write", "(", "\"<b> {} </b>| \"", ".", "format", "(", "method", ".", "legend", ")", ")", "\n", "for", "j", ",", "radius", "in", "enumerate", "(", "radii", ")", ":", "\n", "            ", "if", "i", "==", "accuracies", "[", ":", ",", "j", "]", ".", "argmax", "(", ")", ":", "\n", "                ", "txt", "=", "\"{:.2f}<b>*</b> |\"", ".", "format", "(", "accuracies", "[", "i", ",", "j", "]", ")", "\n", "", "else", ":", "\n", "                ", "txt", "=", "\"{:.2f} |\"", ".", "format", "(", "accuracies", "[", "i", ",", "j", "]", ")", "\n", "", "f", ".", "write", "(", "txt", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.ConvertFromGeometric.__call__": [[11, 13], ["data.pos.numpy", "data.y.numpy", "data.face.numpy", "hasattr"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "return", "data", ".", "pos", ".", "numpy", "(", ")", ",", "None", "if", "not", "hasattr", "(", "data", ",", "'face'", ")", "else", "data", ".", "face", ".", "numpy", "(", ")", ",", "data", ".", "y", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.ConvertFromGeometric.__repr__": [[14, 16], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.NormalizeUnitSphere.__call__": [[20, 38], ["numpy.mean", "numpy.expand_dims", "numpy.max", "numpy.linalg.norm", "numpy.max", "numpy.reshape", "numpy.expand_dims", "numpy.reshape", "numpy.linalg.norm"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "points", ",", "faces", ",", "label", "=", "data", "\n", "# center shape", "\n", "offset", "=", "np", ".", "mean", "(", "points", ",", "axis", "=", "0", ")", "\n", "points", "-=", "np", ".", "expand_dims", "(", "offset", ",", "axis", "=", "0", ")", "\n", "\n", "# scale to unit sphere", "\n", "distance", "=", "np", ".", "max", "(", "np", ".", "linalg", ".", "norm", "(", "points", ",", "ord", "=", "2", ",", "axis", "=", "1", ")", ")", "\n", "points", "=", "points", "/", "distance", "\n", "assert", "np", ".", "max", "(", "np", ".", "linalg", ".", "norm", "(", "points", ",", "ord", "=", "2", ",", "axis", "=", "1", ")", ")", "<=", "1.0001", "\n", "\n", "if", "faces", "is", "not", "None", ":", "\n", "            ", "faces", "=", "np", ".", "reshape", "(", "faces", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "faces", "-=", "np", ".", "expand_dims", "(", "offset", ",", "axis", "=", "0", ")", "\n", "faces", "=", "faces", "/", "distance", "\n", "faces", "=", "np", ".", "reshape", "(", "faces", ",", "(", "-", "1", ",", "3", ",", "3", ")", ")", "\n", "\n", "", "return", "points", ",", "faces", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.NormalizeUnitSphere.__repr__": [[39, 41], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.SelectPoints.__init__": [[45, 47], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_points", ")", ":", "\n", "        ", "self", ".", "num_points", "=", "num_points", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.SelectPoints.__call__": [[48, 52], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "points", ",", "faces", ",", "label", "=", "data", "\n", "assert", "points", ".", "shape", "[", "0", "]", ">=", "self", ".", "num_points", ",", "\"Cannot select more points than given in input data\"", "\n", "return", "points", "[", ":", "self", ".", "num_points", "]", ",", "None", "if", "faces", "is", "None", "else", "faces", "[", ":", "self", ".", "num_points", "]", ",", "label", "[", ":", "self", ".", "num_points", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.SelectPoints.__repr__": [[53, 55], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.Identity.__call__": [[61, 63], ["None"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.Identity.__repr__": [[64, 66], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.GaussianNoise.__call__": [[70, 74], ["numpy.random.normal"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "points", ",", "faces", ",", "label", "=", "data", "\n", "points", "+=", "np", ".", "random", ".", "normal", "(", "0", ",", "0.02", ",", "size", "=", "points", ".", "shape", ")", "\n", "return", "points", ",", "faces", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.GaussianNoise.__repr__": [[75, 77], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.RemoveNones.__call__": [[81, 87], ["None"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "points", ",", "faces", ",", "label", "=", "data", "\n", "if", "faces", "is", "None", ":", "\n", "            ", "return", "points", ",", "label", "\n", "", "else", ":", "\n", "            ", "return", "points", ",", "faces", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.RemoveNones.__repr__": [[88, 90], ["None"], "methods", ["None"], ["", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.__init__": [[94, 96], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_points", ")", ":", "\n", "        ", "self", ".", "num_points", "=", "num_points", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.euclidean_distance_matrix": [[98, 104], ["numpy.sum", "r.reshape.reshape.reshape", "numpy.dot"], "methods", ["None"], ["", "def", "euclidean_distance_matrix", "(", "self", ",", "x", ")", ":", "\n", "        ", "r", "=", "np", ".", "sum", "(", "x", "*", "x", ",", "1", ")", "\n", "r", "=", "r", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "distance_mat", "=", "r", "-", "2", "*", "np", ".", "dot", "(", "x", ",", "x", ".", "T", ")", "+", "r", ".", "T", "\n", "# return np.sqrt(distance_mat)", "\n", "return", "distance_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.update_farthest_distance": [[106, 110], ["range", "numpy.argmax"], "methods", ["None"], ["", "def", "update_farthest_distance", "(", "self", ",", "far_mat", ",", "dist_mat", ",", "s", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "far_mat", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "far_mat", "[", "i", "]", "=", "dist_mat", "[", "i", ",", "s", "]", "if", "far_mat", "[", "i", "]", ">", "dist_mat", "[", "i", ",", "s", "]", "else", "far_mat", "[", "i", "]", "\n", "", "return", "far_mat", ",", "np", ".", "argmax", "(", "far_mat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.init_farthest_distance": [[112, 116], ["range"], "methods", ["None"], ["", "def", "init_farthest_distance", "(", "self", ",", "far_mat", ",", "dist_mat", ",", "s", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "far_mat", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "far_mat", "[", "i", "]", "=", "dist_mat", "[", "i", ",", "s", "]", "\n", "", "return", "far_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.__call__": [[117, 153], ["data.pos.numpy", "process_data.FarthestPoints.euclidean_distance_matrix", "numpy.random.randint", "process_data.FarthestPoints.init_farthest_distance", "range", "torch.from_numpy", "hasattr", "numpy.concatenate", "numpy.zeros", "torch.from_numpy.append", "hasattr", "process_data.FarthestPoints.update_farthest_distance", "numpy.array", "torch.stack", "torch.tensor", "len", "len", "numpy.concatenate", "torch.stack.append", "torch.tensor.append"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.euclidean_distance_matrix", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.init_farthest_distance", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.update_farthest_distance"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "pos", "=", "data", ".", "pos", ".", "numpy", "(", ")", "\n", "y", "=", "data", ".", "y", "if", "len", "(", "data", ".", "y", ")", "==", "len", "(", "pos", ")", "else", "None", "\n", "while", "pos", ".", "shape", "[", "0", "]", "<", "self", ".", "num_points", ":", "\n", "            ", "pos", "=", "np", ".", "concatenate", "(", "[", "pos", ",", "pos", "]", ",", "axis", "=", "0", ")", "\n", "y", "=", "np", ".", "concatenate", "(", "[", "y", ",", "y", "]", ",", "axis", "=", "0", ")", "if", "y", "is", "not", "None", "else", "None", "\n", "\n", "", "assert", "pos", ".", "shape", "[", "1", "]", "==", "3", "and", "pos", ".", "shape", "[", "0", "]", ">=", "self", ".", "num_points", "\n", "\n", "distance_matrix", "=", "self", ".", "euclidean_distance_matrix", "(", "pos", ")", "\n", "\n", "selected_points", "=", "[", "]", "\n", "selected_faces", "=", "[", "]", "\n", "selected_y", "=", "[", "]", "\n", "s", "=", "np", ".", "random", ".", "randint", "(", "pos", ".", "shape", "[", "0", "]", ")", "\n", "far_mat", "=", "self", ".", "init_farthest_distance", "(", "np", ".", "zeros", "(", "(", "pos", ".", "shape", "[", "0", "]", ")", ")", ",", "distance_matrix", ",", "s", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_points", ")", ":", "\n", "            ", "selected_points", ".", "append", "(", "pos", "[", "s", "]", ")", "\n", "# if data.face is not None:", "\n", "if", "hasattr", "(", "data", ",", "'face'", ")", ":", "\n", "                ", "selected_faces", ".", "append", "(", "data", ".", "face", "[", "s", "]", ")", "\n", "", "if", "y", "is", "not", "None", ":", "\n", "                ", "selected_y", ".", "append", "(", "y", "[", "s", "]", ")", "\n", "", "far_mat", ",", "s", "=", "self", ".", "update_farthest_distance", "(", "far_mat", ",", "distance_matrix", ",", "s", ")", "\n", "\n", "", "selected_points", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "selected_points", ")", ")", "\n", "data", ".", "pos", "=", "selected_points", "\n", "# if data.face is not None:", "\n", "if", "hasattr", "(", "data", ",", "'face'", ")", ":", "\n", "            ", "selected_faces", "=", "torch", ".", "stack", "(", "selected_faces", ")", "\n", "data", ".", "face", "=", "selected_faces", "\n", "", "if", "y", "is", "not", "None", ":", "\n", "            ", "selected_y", "=", "torch", ".", "tensor", "(", "selected_y", ")", "\n", "data", ".", "y", "=", "selected_y", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.FarthestPoints.__repr__": [[154, 156], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{}({})'", ".", "format", "(", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "num_points", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.SamplePoints.__init__": [[171, 173], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num", ")", ":", "\n", "        ", "self", ".", "num", "=", "num", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.SamplePoints.__call__": [[174, 211], ["print", "pos.max", "torch.multinomial", "torch.rand", "torch.zeros", "area.norm().abs", "area.sum", "torch.rand.sum", "pos.size", "face.size", "area.norm"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "print", "(", "data", ")", "\n", "pos", ",", "face", "=", "data", ".", "pos", ",", "data", ".", "face", "\n", "assert", "pos", ".", "size", "(", "1", ")", "==", "3", "and", "face", ".", "size", "(", "0", ")", "==", "3", "\n", "\n", "pos_max", "=", "pos", ".", "max", "(", ")", "\n", "pos", "=", "pos", "/", "pos_max", "\n", "\n", "area", "=", "(", "pos", "[", "face", "[", "1", "]", "]", "-", "pos", "[", "face", "[", "0", "]", "]", ")", ".", "cross", "(", "pos", "[", "face", "[", "2", "]", "]", "-", "pos", "[", "face", "[", "0", "]", "]", ")", "\n", "area", "=", "area", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "1", ")", ".", "abs", "(", ")", "/", "2", "\n", "\n", "prob", "=", "area", "/", "area", ".", "sum", "(", ")", "\n", "sample", "=", "torch", ".", "multinomial", "(", "prob", ",", "self", ".", "num", ",", "replacement", "=", "True", ")", "\n", "face", "=", "face", "[", ":", ",", "sample", "]", "\n", "\n", "frac", "=", "torch", ".", "rand", "(", "self", ".", "num", ",", "2", ",", "device", "=", "pos", ".", "device", ")", "\n", "mask", "=", "frac", ".", "sum", "(", "dim", "=", "-", "1", ")", ">", "1", "\n", "frac", "[", "mask", "]", "=", "1", "-", "frac", "[", "mask", "]", "\n", "\n", "vec1", "=", "pos", "[", "face", "[", "1", "]", "]", "-", "pos", "[", "face", "[", "0", "]", "]", "\n", "vec2", "=", "pos", "[", "face", "[", "2", "]", "]", "-", "pos", "[", "face", "[", "0", "]", "]", "\n", "\n", "pos_sampled", "=", "pos", "[", "face", "[", "0", "]", "]", "\n", "pos_sampled", "+=", "frac", "[", ":", ",", ":", "1", "]", "*", "vec1", "\n", "pos_sampled", "+=", "frac", "[", ":", ",", "1", ":", "]", "*", "vec2", "\n", "\n", "pos_sampled", "=", "pos_sampled", "*", "pos_max", "\n", "data", ".", "pos", "=", "pos_sampled", "\n", "sampled_faces", "=", "torch", ".", "zeros", "(", "self", ".", "num", ",", "3", ",", "3", ")", "\n", "sampled_faces", "[", ":", ",", "0", ",", ":", "]", "=", "pos", "[", "face", "[", "0", "]", "]", "\n", "sampled_faces", "[", ":", ",", "1", ",", ":", "]", "=", "pos", "[", "face", "[", "1", "]", "]", "\n", "sampled_faces", "[", ":", ",", "2", ",", ":", "]", "=", "pos", "[", "face", "[", "2", "]", "]", "\n", "\n", "sampled_faces", "=", "sampled_faces", "*", "pos_max", "\n", "data", ".", "face", "=", "sampled_faces", "\n", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.SamplePoints.__repr__": [[212, 214], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{}({})'", ".", "format", "(", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.TestDummy.__call__": [[218, 221], ["print"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "print", "(", "data", ")", "\n", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.process_data.TestDummy.__repr__": [[222, 224], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{}()'", ".", "format", "(", "self", ".", "__class__", ".", "__name__", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.__init__": [[5, 7], ["train_utils.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.reset": [[8, 13], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update": [[14, 19], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.accuracy": [[21, 36], ["torch.no_grad", "max", "target.size", "output.topk", "pred.t.t", "pred.t.eq().contiguous", "correct[].view().float().sum", "res.append", "pred.t.eq", "correct[].view().float().sum.mul_", "target.view().expand_as", "correct[].view().float", "target.view", "correct[].view"], "function", ["None"], ["", "", "def", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "maxk", "=", "max", "(", "topk", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", ".", "contiguous", "(", ")", "\n", "\n", "res", "=", "[", "]", "\n", "for", "k", "in", "topk", ":", "\n", "            ", "correct_k", "=", "correct", "[", ":", "k", "]", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "res", ".", "append", "(", "correct_k", ".", "mul_", "(", "100.0", "/", "batch_size", ")", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.init_logfile": [[37, 41], ["open", "open.write", "open.close"], "function", ["None"], ["", "", "def", "init_logfile", "(", "filename", ":", "str", ",", "text", ":", "str", ")", ":", "\n", "    ", "f", "=", "open", "(", "filename", ",", "'w'", ")", "\n", "f", ".", "write", "(", "text", "+", "\"\\n\"", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.log": [[42, 46], ["open", "open.write", "open.close"], "function", ["None"], ["", "def", "log", "(", "filename", ":", "str", ",", "text", ":", "str", ")", ":", "\n", "    ", "f", "=", "open", "(", "filename", ",", "'a'", ")", "\n", "f", ".", "write", "(", "text", "+", "\"\\n\"", ")", "\n", "f", ".", "close", "(", ")", "", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.architectures.get_architecture": [[10, 41], ["archs.pointnet.PointNet", "archs.pointnet.PointNet", "archs.pointnet.PointNet", "archs.pointnet.PointNet", "archs.pointnet.PointNet", "archs.pointnet.PointNet", "archs.pointnet.PointNet", "archs.pointnet.PointNetSegmentation"], "function", ["None"], ["def", "get_architecture", "(", "arch", ":", "str", ",", "dataset", ":", "str", ")", "->", "torch", ".", "nn", ".", "Module", ":", "\n", "    ", "\"\"\" Return a neural network (with random weights)\n\n    :param arch: the architecture - should be in the ARCHITECTURES list above\n    :param dataset: the dataset - should be in the datasets.DATASETS list\n    :return: a Pytorch module\n    \"\"\"", "\n", "if", "arch", "==", "\"pointnet\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "64", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet128\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "128", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet256\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "256", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet512\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "512", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet1024\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "1024", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet32\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "32", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet16\"", ":", "\n", "        ", "model", "=", "PointNet", "(", "number_points", "=", "16", ",", "num_classes", "=", "40", ")", "\n", "model", "=", "model", "\n", "", "elif", "arch", "==", "\"pointnet_segmentation\"", ":", "\n", "        ", "model", "=", "PointNetSegmentation", "(", "number_points", "=", "64", ",", "num_seg_classes", "=", "50", ")", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.get_dataset": [[14, 32], ["datasets.modelnet40", "datasets.modelnet40", "datasets.modelnet40", "datasets.modelnet40", "datasets.modelnet40", "datasets.modelnet40", "datasets.modelnet40", "datasets.shapenet"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.shapenet"], ["def", "get_dataset", "(", "dataset", ":", "str", ",", "split", ":", "str", ")", "->", "Dataset", ":", "\n", "    ", "\"\"\"Return the dataset as a PyTorch Dataset object\"\"\"", "\n", "if", "dataset", "==", "\"modelnet40\"", ":", "\n", "        ", "return", "modelnet40", "(", "64", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"modelnet40_128\"", ":", "\n", "        ", "return", "modelnet40", "(", "128", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"modelnet40_256\"", ":", "\n", "        ", "return", "modelnet40", "(", "256", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"modelnet40_512\"", ":", "\n", "        ", "return", "modelnet40", "(", "512", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"modelnet40_1024\"", ":", "\n", "        ", "return", "modelnet40", "(", "1024", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"modelnet40_32\"", ":", "\n", "        ", "return", "modelnet40", "(", "32", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"modelnet40_16\"", ":", "\n", "        ", "return", "modelnet40", "(", "16", ",", "split", ")", "\n", "", "elif", "dataset", "==", "\"shapenet\"", ":", "\n", "        ", "return", "shapenet", "(", "64", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.get_num_classes": [[34, 40], ["None"], "function", ["None"], ["", "", "def", "get_num_classes", "(", "dataset", ":", "str", ")", ":", "\n", "    ", "\"\"\"Return the number of classes in the dataset. \"\"\"", "\n", "if", "\"modelnet40\"", "in", "dataset", ":", "\n", "        ", "return", "40", "\n", "", "elif", "\"shapenet\"", "in", "dataset", ":", "\n", "        ", "return", "50", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.modelnet40": [[42, 74], ["torchvision.transforms.Compose", "torchvision.datasets.modelnet.ModelNet", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "process_data.SamplePoints", "process_data.FarthestPoints", "process_data.ConvertFromGeometric", "process_data.NormalizeUnitSphere", "process_data.SelectPoints", "process_data.RemoveNones", "process_data.ConvertFromGeometric", "process_data.NormalizeUnitSphere", "process_data.SelectPoints", "process_data.RemoveNones"], "function", ["None"], ["", "", "def", "modelnet40", "(", "num_points", ":", "int", "=", "1024", ",", "split", ":", "str", "=", "'train'", ")", "->", "datasets", ".", "modelnet", ".", "ModelNet", ":", "\n", "    ", "dataset_root", "=", "\"./dataset_cache/modelnet40fp\"", "\n", "assert", "1", "<=", "num_points", "<=", "1024", ",", "\"num_points must be between 1 and 1024\"", "\n", "train", "=", "split", "==", "'train'", "\n", "\n", "pre_transforms", "=", "transforms", ".", "Compose", "(", "[", "\n", "process_data", ".", "SamplePoints", "(", "num", "=", "4096", ")", ",", "\n", "process_data", ".", "FarthestPoints", "(", "num_points", "=", "1024", ")", "\n", "]", ")", "\n", "\n", "# if add_noise is None:", "\n", "#     add_noise = split == 'train'", "\n", "\n", "if", "train", ":", "\n", "        ", "post_transforms", "=", "transforms", ".", "Compose", "(", "[", "\n", "process_data", ".", "ConvertFromGeometric", "(", ")", ",", "\n", "process_data", ".", "NormalizeUnitSphere", "(", ")", ",", "\n", "process_data", ".", "SelectPoints", "(", "num_points", ")", ",", "\n", "process_data", ".", "RemoveNones", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "        ", "post_transforms", "=", "transforms", ".", "Compose", "(", "[", "\n", "process_data", ".", "ConvertFromGeometric", "(", ")", ",", "\n", "process_data", ".", "NormalizeUnitSphere", "(", ")", ",", "\n", "process_data", ".", "SelectPoints", "(", "num_points", ")", ",", "\n", "process_data", ".", "RemoveNones", "(", ")", "\n", "]", ")", "\n", "# if add_noise:", "\n", "#     post_transforms.transforms.append(transformers.GaussianNoise())", "\n", "\n", "", "return", "datasets", ".", "modelnet", ".", "ModelNet", "(", "root", "=", "dataset_root", ",", "name", "=", "'40'", ",", "train", "=", "train", ",", "\n", "pre_transform", "=", "pre_transforms", ",", "transform", "=", "post_transforms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.shapenet": [[75, 103], ["torchvision.transforms.Compose", "torchvision.datasets.shapenet.ShapeNet", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "process_data.FarthestPoints", "process_data.ConvertFromGeometric", "process_data.NormalizeUnitSphere", "process_data.SelectPoints", "process_data.RemoveNones", "process_data.ConvertFromGeometric", "process_data.NormalizeUnitSphere", "process_data.SelectPoints", "process_data.RemoveNones"], "function", ["None"], ["", "def", "shapenet", "(", "num_points", ":", "int", "=", "1024", ",", "split", ":", "str", "=", "'train'", ")", "->", "datasets", ".", "shapenet", ".", "ShapeNet", ":", "\n", "    ", "dataset_root", "=", "\"./dataset_cache/shapenet\"", "\n", "assert", "1", "<=", "num_points", "<=", "1024", ",", "\"num_points must be between 1 and 1024\"", "\n", "assert", "split", "in", "[", "'train'", ",", "'test'", "]", ",", "\"split must either be 'train' or 'test'\"", "\n", "train", "=", "split", "==", "'train'", "\n", "\n", "pre_transforms", "=", "transforms", ".", "Compose", "(", "[", "\n", "process_data", ".", "FarthestPoints", "(", "num_points", "=", "1024", ")", "\n", "]", ")", "\n", "\n", "if", "train", ":", "\n", "        ", "split", "=", "'trainval'", "\n", "post_transforms", "=", "transforms", ".", "Compose", "(", "[", "\n", "process_data", ".", "ConvertFromGeometric", "(", ")", ",", "\n", "process_data", ".", "NormalizeUnitSphere", "(", ")", ",", "\n", "process_data", ".", "SelectPoints", "(", "num_points", ")", ",", "\n", "process_data", ".", "RemoveNones", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "        ", "split", "=", "'test'", "\n", "post_transforms", "=", "transforms", ".", "Compose", "(", "[", "\n", "process_data", ".", "ConvertFromGeometric", "(", ")", ",", "\n", "process_data", ".", "NormalizeUnitSphere", "(", ")", ",", "\n", "process_data", ".", "SelectPoints", "(", "num_points", ")", ",", "\n", "process_data", ".", "RemoveNones", "(", ")", "\n", "]", ")", "\n", "", "return", "datasets", ".", "shapenet", ".", "ShapeNet", "(", "root", "=", "dataset_root", ",", "include_normals", "=", "False", ",", "split", "=", "split", ",", "\n", "pre_transform", "=", "pre_transforms", ",", "transform", "=", "post_transforms", ")", "", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmooth.__init__": [[17, 26], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "base_classifier", ":", "torch", ".", "nn", ".", "Module", ",", "num_classes", ":", "int", ",", "transformer", ":", "AbstractTransformer", ")", ":", "\n", "        ", "\"\"\"\n        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n        :param num_classes:\n        :param sigma: the noise level hyperparameter\n        \"\"\"", "\n", "self", ".", "base_classifier", "=", "base_classifier", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "transformer", "=", "transformer", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmooth.certify": [[27, 69], ["core.SemanticSmooth.base_classifier.eval", "core.SemanticSmooth._sample_noise", "core.SemanticSmooth.argmax().item", "min", "core.SemanticSmooth._sample_noise", "counts_estimation[].item", "core.SemanticSmooth._lower_confidence_bound", "core.SemanticSmooth.transformer.calc_radius", "core.SemanticSmooth.argmax", "math.sqrt", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._lower_confidence_bound", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistTaperRotationNoise.calc_radius"], ["", "def", "certify", "(", "self", ",", "x", ":", "torch", ".", "tensor", ",", "n0", ":", "int", ",", "maxn", ":", "int", ",", "alpha", ":", "float", ",", "batch_size", ":", "int", ",", "cAHat", "=", "None", ",", "margin_sq", "=", "None", ")", "->", "Union", "[", "int", ",", "float", "]", ":", "\n", "        ", "\"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n        robust within a L2 ball of radius R around x.\n\n        :param x: the input [channel x height x width]\n        :param n0: the number of Monte Carlo samples to use for selection\n        :param n: the number of Monte Carlo samples to use for estimation\n        :param alpha: the failure probability\n        :param batch_size: batch size to use when evaluating the base classifier\n        :return: (predicted class, certified radius)\n                 in the case of abstention, the class will be ABSTAIN and the radius 0.\n        \"\"\"", "\n", "\n", "self", ".", "base_classifier", ".", "eval", "(", ")", "\n", "if", "cAHat", "is", "None", ":", "\n", "# draw samples of f(x+ epsilon)", "\n", "            ", "counts_selection", "=", "self", ".", "_sample_noise", "(", "x", ",", "n0", ")", "\n", "# use these samples to take a guess at the top class", "\n", "cAHat", "=", "counts_selection", ".", "argmax", "(", ")", ".", "item", "(", ")", "\n", "", "nA", ",", "n", "=", "0", ",", "0", "\n", "pABar", "=", "0.0", "\n", "while", "n", "<", "maxn", ":", "\n", "            ", "now_batch", "=", "min", "(", "batch_size", ",", "maxn", "-", "n", ")", "\n", "# draw more samples of f(x + epsilon)", "\n", "counts_estimation", "=", "self", ".", "_sample_noise", "(", "x", ",", "now_batch", ")", "\n", "# print(counts_estimation)", "\n", "n", "+=", "now_batch", "\n", "# use these samples to estimate a lower bound on pA", "\n", "nA", "+=", "counts_estimation", "[", "cAHat", "]", ".", "item", "(", ")", "\n", "pABar", "=", "self", ".", "_lower_confidence_bound", "(", "nA", ",", "n", ",", "alpha", ")", "\n", "r", "=", "self", ".", "transformer", ".", "calc_radius", "(", "pABar", ")", "\n", "# early stop if margin_sq is specified", "\n", "if", "margin_sq", "is", "not", "None", "and", "r", ">=", "sqrt", "(", "margin_sq", ")", ":", "\n", "                ", "return", "cAHat", ",", "r", "-", "sqrt", "(", "margin_sq", ")", "\n", "", "", "if", "margin_sq", "is", "None", ":", "\n", "            ", "if", "r", "<=", "EPS", ":", "\n", "                ", "return", "SemanticSmooth", ".", "ABSTAIN", ",", "0.0", "\n", "", "else", ":", "\n", "                ", "return", "cAHat", ",", "r", "\n", "", "", "else", ":", "\n", "            ", "return", "(", "SemanticSmooth", ".", "ABSTAIN", "if", "r", "<=", "EPS", "else", "cAHat", ")", ",", "r", "-", "sqrt", "(", "margin_sq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmooth.predict": [[72, 106], ["core.SemanticSmooth.base_classifier.eval", "min", "core.SemanticSmooth._sample_noise", "scipy.stats.binom_test", "counts.argsort"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise"], ["", "", "def", "predict", "(", "self", ",", "x", ":", "torch", ".", "tensor", ",", "n0", ":", "int", ",", "alpha", ":", "float", ",", "batch_size", ":", "int", ")", "->", "int", ":", "\n", "        ", "\"\"\" Monte Carlo algorithm for evaluating the prediction of g at x.  With probability at least 1 - alpha, the\n        class returned by this method will equal g(x).\n\n        This function uses the hypothesis test described in https://arxiv.org/abs/1610.03944\n        for identifying the top category of a multinomial distribution.\n\n        :param x: the input [channel x height x width]\n        :param n: the number of Monte Carlo samples to use\n        :param alpha: the failure probability\n        :param batch_size: batch size to use when evaluating the base classifier\n        :return: the predicted class, or ABSTAIN\n        \"\"\"", "\n", "self", ".", "base_classifier", ".", "eval", "(", ")", "\n", "\n", "n", "=", "0", "\n", "counts", "=", "None", "\n", "while", "n", "<", "n0", ":", "\n", "            ", "now_batch", "=", "min", "(", "batch_size", ",", "n0", "-", "n", ")", "\n", "# draw more samples of f(x + epsilon)", "\n", "counts_estimation", "=", "self", ".", "_sample_noise", "(", "x", ",", "now_batch", ")", "\n", "# print(counts_estimation)", "\n", "n", "+=", "now_batch", "\n", "if", "counts", "is", "None", ":", "\n", "                ", "counts", "=", "counts_estimation", "\n", "", "else", ":", "\n", "                ", "counts", "+=", "counts_estimation", "\n", "", "", "top2", "=", "counts", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "2", "]", "\n", "count1", "=", "counts", "[", "top2", "[", "0", "]", "]", "\n", "count2", "=", "counts", "[", "top2", "[", "1", "]", "]", "\n", "if", "binom_test", "(", "count1", ",", "count1", "+", "count2", ",", "p", "=", "0.5", ")", ">", "alpha", ":", "\n", "            ", "return", "SemanticSmooth", ".", "ABSTAIN", "\n", "", "else", ":", "\n", "            ", "return", "top2", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmooth._sample_noise": [[107, 124], ["torch.no_grad", "numpy.zeros", "x.repeat", "core.SemanticSmooth.transformer.process().to", "core.SemanticSmooth.base_classifier().argmax", "core.SemanticSmooth._count_arr", "core.SemanticSmooth.cpu().numpy", "core.SemanticSmooth.transformer.process", "core.SemanticSmooth.base_classifier", "core.SemanticSmooth.cpu"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._count_arr", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process"], ["", "", "def", "_sample_noise", "(", "self", ",", "x", ":", "torch", ".", "tensor", ",", "num", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n\n        :param x: the input [channel x width x height]\n        :param num: number of samples to collect\n        :param batch_size:\n        :return: an ndarray[int] of length num_classes containing the per-class counts\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "counts", "=", "np", ".", "zeros", "(", "self", ".", "num_classes", ",", "dtype", "=", "int", ")", "\n", "# batch = x.repeat((num, 1, 1, 1))", "\n", "# print(x.shape, num)", "\n", "batch", "=", "x", ".", "repeat", "(", "(", "num", ",", "1", ",", "1", ")", ")", "\n", "batch_noised", "=", "self", ".", "transformer", ".", "process", "(", "batch", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "predictions", "=", "self", ".", "base_classifier", "(", "batch_noised", ")", ".", "argmax", "(", "1", ")", "\n", "counts", "+=", "self", ".", "_count_arr", "(", "predictions", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "self", ".", "num_classes", ")", "\n", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmooth._count_arr": [[125, 130], ["numpy.zeros"], "methods", ["None"], ["", "", "def", "_count_arr", "(", "self", ",", "arr", ":", "np", ".", "ndarray", ",", "length", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "counts", "=", "np", ".", "zeros", "(", "length", ",", "dtype", "=", "int", ")", "\n", "for", "idx", "in", "arr", ":", "\n", "            ", "counts", "[", "idx", "]", "+=", "1", "\n", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmooth._lower_confidence_bound": [[131, 142], ["statsmodels.stats.proportion.proportion_confint"], "methods", ["None"], ["", "def", "_lower_confidence_bound", "(", "self", ",", "NA", ":", "int", ",", "N", ":", "int", ",", "alpha", ":", "float", ")", "->", "float", ":", "\n", "        ", "\"\"\" Returns a (1 - alpha) lower confidence bound on a bernoulli proportion.\n\n        This function uses the Clopper-Pearson method.\n\n        :param NA: the number of \"successes\"\n        :param N: the number of total draws\n        :param alpha: the confidence level\n        :return: a lower bound on the binomial proportion which holds true w.p at least (1 - alpha) over the samples\n        \"\"\"", "\n", "return", "proportion_confint", "(", "NA", ",", "N", ",", "alpha", "=", "2", "*", "alpha", ",", "method", "=", "\"beta\"", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation.__init__": [[150, 160], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "base_classifier", ":", "torch", ".", "nn", ".", "Module", ",", "num_classes", ":", "int", ",", "transformer", ":", "AbstractTransformer", ",", "num_points", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n        :param num_classes:\n        :param sigma: the noise level hyperparameter\n        \"\"\"", "\n", "self", ".", "base_classifier", "=", "base_classifier", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_points", "=", "num_points", "\n", "self", ".", "transformer", "=", "transformer", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation.certify": [[161, 197], ["numpy.zeros", "numpy.zeros", "core.SemanticSmoothSegmentation.base_classifier.eval", "core.SemanticSmoothSegmentation._sample_noise", "core.SemanticSmoothSegmentation.argmax", "core.SemanticSmoothSegmentation._sample_noise", "range", "[].item", "core.SemanticSmoothSegmentation._lower_confidence_bound", "core.SemanticSmoothSegmentation.transformer.calc_radius"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._lower_confidence_bound", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistTaperRotationNoise.calc_radius"], ["", "def", "certify", "(", "self", ",", "x", ":", "torch", ".", "tensor", ",", "n0", ":", "int", ",", "maxn", ":", "int", ",", "alpha", ":", "float", ",", "batch_size", ":", "int", ",", "cAHat", "=", "None", ",", "margin_sq", "=", "None", ")", "->", "Union", "[", "int", ",", "float", "]", ":", "\n", "        ", "\"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n        robust within a L2 ball of radius R around x.\n\n        :param x: the input [channel x height x width]\n        :param n0: the number of Monte Carlo samples to use for selection\n        :param n: the number of Monte Carlo samples to use for estimation\n        :param alpha: the failure probability\n        :param batch_size: batch size to use when evaluating the base classifier\n        :return: (predicted class, certified radius)\n                 in the case of abstention, the class will be ABSTAIN and the radius 0.\n        \"\"\"", "\n", "\n", "predictions", "=", "np", ".", "zeros", "(", "self", ".", "num_points", ")", "\n", "radius", "=", "np", ".", "zeros", "(", "self", ".", "num_points", ")", "\n", "self", ".", "base_classifier", ".", "eval", "(", ")", "\n", "# draw samples of f(x+ epsilon)", "\n", "counts_selection", "=", "self", ".", "_sample_noise", "(", "x", ",", "n0", ")", "\n", "# use these samples to take a guess at the top class", "\n", "cAHat", "=", "counts_selection", ".", "argmax", "(", "1", ")", "\n", "# print(cAHat)", "\n", "# draw more samples of f(x + epsilon)", "\n", "counts_estimation", "=", "self", ".", "_sample_noise", "(", "x", ",", "maxn", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_points", ")", ":", "\n", "# use these samples to estimate a lower bound on pA", "\n", "            ", "nA", "=", "counts_estimation", "[", "i", "]", "[", "cAHat", "[", "i", "]", "]", ".", "item", "(", ")", "\n", "pABar", "=", "self", ".", "_lower_confidence_bound", "(", "nA", ",", "maxn", ",", "alpha", ")", "\n", "r", "=", "self", ".", "transformer", ".", "calc_radius", "(", "pABar", ")", "\n", "if", "r", "<=", "EPS", ":", "\n", "                ", "predictions", "[", "i", "]", "=", "SemanticSmoothSegmentation", ".", "ABSTAIN", "\n", "radius", "[", "i", "]", "=", "0", "\n", "", "else", ":", "\n", "                ", "predictions", "[", "i", "]", "=", "cAHat", "[", "i", "]", "\n", "radius", "[", "i", "]", "=", "r", "\n", "", "", "return", "predictions", ",", "radius", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation.predict": [[198, 232], ["core.SemanticSmoothSegmentation.base_classifier.eval", "min", "core.SemanticSmoothSegmentation._sample_noise", "scipy.stats.binom_test", "counts.argsort"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise"], ["", "def", "predict", "(", "self", ",", "x", ":", "torch", ".", "tensor", ",", "n0", ":", "int", ",", "alpha", ":", "float", ",", "batch_size", ":", "int", ")", "->", "int", ":", "\n", "        ", "\"\"\" Monte Carlo algorithm for evaluating the prediction of g at x.  With probability at least 1 - alpha, the\n        class returned by this method will equal g(x).\n\n        This function uses the hypothesis test described in https://arxiv.org/abs/1610.03944\n        for identifying the top category of a multinomial distribution.\n\n        :param x: the input [channel x height x width]\n        :param n: the number of Monte Carlo samples to use\n        :param alpha: the failure probability\n        :param batch_size: batch size to use when evaluating the base classifier\n        :return: the predicted class, or ABSTAIN\n        \"\"\"", "\n", "self", ".", "base_classifier", ".", "eval", "(", ")", "\n", "\n", "n", "=", "0", "\n", "counts", "=", "None", "\n", "while", "n", "<", "n0", ":", "\n", "            ", "now_batch", "=", "min", "(", "batch_size", ",", "n0", "-", "n", ")", "\n", "# draw more samples of f(x + epsilon)", "\n", "counts_estimation", "=", "self", ".", "_sample_noise", "(", "x", ",", "now_batch", ")", "\n", "# print(counts_estimation)", "\n", "n", "+=", "now_batch", "\n", "if", "counts", "is", "None", ":", "\n", "                ", "counts", "=", "counts_estimation", "\n", "", "else", ":", "\n", "                ", "counts", "+=", "counts_estimation", "\n", "", "", "top2", "=", "counts", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "2", "]", "\n", "count1", "=", "counts", "[", "top2", "[", "0", "]", "]", "\n", "count2", "=", "counts", "[", "top2", "[", "1", "]", "]", "\n", "if", "binom_test", "(", "count1", ",", "count1", "+", "count2", ",", "p", "=", "0.5", ")", ">", "alpha", ":", "\n", "            ", "return", "SemanticSmooth", ".", "ABSTAIN", "\n", "", "else", ":", "\n", "            ", "return", "top2", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._sample_noise": [[233, 253], ["torch.no_grad", "numpy.zeros", "x.repeat", "core.SemanticSmoothSegmentation.transformer.process().to", "core.SemanticSmoothSegmentation.base_classifier", "core.SemanticSmoothSegmentation._count_arr", "core.SemanticSmoothSegmentation.max", "prediction.cpu().numpy", "core.SemanticSmoothSegmentation.transformer.process", "prediction.cpu"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._count_arr", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process"], ["", "", "def", "_sample_noise", "(", "self", ",", "x", ":", "torch", ".", "tensor", ",", "num", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n\n        :param x: the input [channel x width x height]\n        :param num: number of samples to collect\n        :param batch_size:\n        :return: an ndarray[int] of length num_classes containing the per-class counts\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "counts", "=", "np", ".", "zeros", "(", "[", "self", ".", "num_points", ",", "self", ".", "num_classes", "]", ",", "dtype", "=", "int", ")", "\n", "# batch = x.repeat((num, 1, 1, 1))", "\n", "# print(x.shape, num)", "\n", "batch", "=", "x", ".", "repeat", "(", "(", "num", ",", "1", ",", "1", ")", ")", "\n", "batch_noised", "=", "self", ".", "transformer", ".", "process", "(", "batch", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "predictions", "=", "self", ".", "base_classifier", "(", "batch_noised", ")", "\n", "# print(predictions.shape)", "\n", "prediction", "=", "predictions", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "# print(prediction.shape)", "\n", "counts", "+=", "self", ".", "_count_arr", "(", "prediction", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "self", ".", "num_points", ",", "self", ".", "num_classes", ")", "\n", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._count_arr": [[254, 260], ["numpy.zeros", "enumerate"], "methods", ["None"], ["", "", "def", "_count_arr", "(", "self", ",", "arr", ":", "np", ".", "ndarray", ",", "num_points", ":", "int", ",", "length", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "counts", "=", "np", ".", "zeros", "(", "(", "num_points", ",", "length", ")", ",", "dtype", "=", "int", ")", "\n", "for", "batch", "in", "arr", ":", "\n", "            ", "for", "i", ",", "idx", "in", "enumerate", "(", "batch", ")", ":", "\n", "                ", "counts", "[", "i", "]", "[", "idx", "]", "+=", "1", "\n", "", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.core.SemanticSmoothSegmentation._lower_confidence_bound": [[261, 272], ["statsmodels.stats.proportion.proportion_confint"], "methods", ["None"], ["", "def", "_lower_confidence_bound", "(", "self", ",", "NA", ":", "int", ",", "N", ":", "int", ",", "alpha", ":", "float", ")", "->", "float", ":", "\n", "        ", "\"\"\" Returns a (1 - alpha) lower confidence bound on a bernoulli proportion.\n\n        This function uses the Clopper-Pearson method.\n\n        :param NA: the number of \"successes\"\n        :param N: the number of total draws\n        :param alpha: the confidence level\n        :return: a lower bound on the binomial proportion which holds true w.p at least (1 - alpha) over the samples\n        \"\"\"", "\n", "return", "proportion_confint", "(", "NA", ",", "N", ",", "alpha", "=", "2", "*", "alpha", ",", "method", "=", "\"beta\"", ")", "[", "0", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Noise.__init__": [[18, 20], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Noise.proc": [[21, 24], ["torch.randn_like"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ")", ":", "\n", "        ", "noise", "=", "torch", ".", "randn_like", "(", "input", ")", "*", "self", ".", "sigma", "\n", "return", "input", "+", "noise", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Noise.batch_proc": [[25, 28], ["torch.randn_like"], "methods", ["None"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "noise", "=", "torch", ".", "randn_like", "(", "inputs", ")", "*", "self", ".", "sigma", "\n", "return", "inputs", "+", "noise", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Rotation3D.__init__": [[32, 36], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "sigma", ",", "axis", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "num_points", "=", "canopy", ".", "shape", "[", "0", "]", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Rotation3D.gen_param": [[37, 40], ["torch.randn"], "methods", ["None"], ["", "def", "gen_param", "(", "self", ")", ":", "\n", "        ", "theta", "=", "torch", ".", "randn", "(", "1", ")", "\n", "return", "theta", "*", "self", ".", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Rotation3D.gen_param_uniform": [[41, 44], ["random.uniform"], "methods", ["None"], ["", "def", "gen_param_uniform", "(", "self", ")", ":", "\n", "        ", "theta", "=", "random", ".", "uniform", "(", "-", "self", ".", "sigma", ",", "self", ".", "sigma", ")", "\n", "return", "theta", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Rotation3D.proc": [[45, 60], ["math.cos", "math.sin", "torch.matmul().squeeze", "torch.tensor().to", "torch.tensor().to", "torch.matmul", "torch.tensor", "torch.tensor().to", "input.unsqueeze", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ",", "angle", ")", ":", "\n", "        ", "angle", "=", "angle", "*", "math", ".", "pi", "/", "180", "\n", "# assume input: num_points x 3", "\n", "cost", "=", "math", ".", "cos", "(", "angle", ")", "\n", "sint", "=", "math", ".", "sin", "(", "angle", ")", "\n", "if", "self", ".", "axis", "==", "'z'", ":", "\n", "            ", "rotation_matrix", "=", "torch", ".", "tensor", "(", "[", "[", "cost", ",", "-", "sint", ",", "0", "]", ",", "[", "sint", ",", "cost", ",", "0", "]", ",", "[", "0", ",", "0", ",", "1", "]", "]", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "", "elif", "self", ".", "axis", "==", "'x'", ":", "\n", "            ", "rotation_matrix", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "0", ",", "0", "]", ",", "[", "0", ",", "cost", ",", "-", "sint", "]", ",", "[", "0", ",", "sint", ",", "cost", "]", "]", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "", "elif", "self", ".", "axis", "==", "'y'", ":", "\n", "            ", "rotation_matrix", "=", "torch", ".", "tensor", "(", "[", "[", "cost", ",", "0", ",", "sint", "]", ",", "[", "0", ",", "1", ",", "0", "]", ",", "[", "-", "sint", ",", "0", ",", "cost", "]", "]", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "# print(input.shape)", "\n", "", "return", "torch", ".", "matmul", "(", "rotation_matrix", ",", "input", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Rotation3D.batch_proc": [[61, 68], ["torch.zeros_like", "range", "len", "transforms.Rotation3D.proc", "transforms.Rotation3D.gen_param"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "# print(inputs.shape)", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "# print(outs[i].shape, inputs[i].shape)", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "*", "self", ".", "gen_param", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Rotation3D.batch_proc_uniform": [[69, 74], ["torch.zeros_like", "range", "len", "transforms.Rotation3D.proc", "transforms.Rotation3D.gen_param_uniform"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.gen_param_uniform"], ["", "def", "batch_proc_uniform", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "self", ".", "gen_param_uniform", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.GeneralRotation3D.__init__": [[77, 80], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "sigma", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "num_points", "=", "canopy", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.GeneralRotation3D.gen_param": [[81, 87], ["numpy.random.randn", "numpy.linalg.norm", "random.uniform", "numpy.append"], "methods", ["None"], ["", "def", "gen_param", "(", "self", ")", ":", "\n", "        ", "k", "=", "np", ".", "random", ".", "randn", "(", "3", ")", "\n", "norm", "=", "np", ".", "linalg", ".", "norm", "(", "k", ")", "\n", "k", "=", "k", "/", "norm", "\n", "alpha", "=", "random", ".", "uniform", "(", "-", "self", ".", "sigma", ",", "self", ".", "sigma", ")", "\n", "return", "np", ".", "append", "(", "k", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.GeneralRotation3D.proc": [[88, 97], ["numpy.array", "torch.from_numpy().to().float", "torch.matmul().squeeze", "numpy.eye", "numpy.dot", "torch.from_numpy().to", "torch.matmul", "numpy.sin", "numpy.cos", "input.unsqueeze", "torch.from_numpy"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ",", "vec", ")", ":", "\n", "        ", "angle", "=", "vec", "[", "3", "]", "*", "math", ".", "pi", "/", "180", "\n", "mat", "=", "np", ".", "array", "(", "[", "[", "0", ",", "-", "vec", "[", "2", "]", ",", "vec", "[", "1", "]", "]", ",", "\n", "[", "vec", "[", "2", "]", ",", "0", ",", "-", "vec", "[", "0", "]", "]", ",", "\n", "[", "-", "vec", "[", "1", "]", ",", "vec", "[", "0", "]", ",", "0", "]", "]", ")", "\n", "rotation_matrix", "=", "np", ".", "eye", "(", "3", ")", "+", "np", ".", "sin", "(", "angle", ")", "*", "mat", "+", "(", "1", "-", "np", ".", "cos", "(", "angle", ")", ")", "*", "np", ".", "dot", "(", "mat", ",", "mat", ")", "\n", "rotation_matrix", "=", "torch", ".", "from_numpy", "(", "rotation_matrix", ")", ".", "to", "(", "input", ".", "device", ")", ".", "float", "(", ")", "\n", "# assume input: num_points x 3", "\n", "return", "torch", ".", "matmul", "(", "rotation_matrix", ",", "input", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.GeneralRotation3D.raw_proc": [[102, 104], ["torch.matmul().squeeze", "torch.matmul", "input.unsqueeze"], "methods", ["None"], ["", "def", "raw_proc", "(", "self", ",", "inputs", ",", "mat", ")", ":", "\n", "        ", "return", "torch", ".", "matmul", "(", "mat", ",", "input", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.GeneralRotation3D.batch_proc": [[105, 111], ["torch.zeros_like", "range", "len", "transforms.GeneralRotation3D.proc", "transforms.GeneralRotation3D.gen_param"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "# print(inputs.shape)", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "self", ".", "gen_param", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Shear.__init__": [[114, 118], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "sigma", ",", "axis", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "num_points", "=", "canopy", ".", "shape", "[", "0", "]", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Shear.gen_param": [[119, 122], ["torch.randn"], "methods", ["None"], ["", "def", "gen_param", "(", "self", ")", ":", "\n", "        ", "theta", "=", "torch", ".", "randn", "(", "2", ")", "\n", "return", "theta", "*", "self", ".", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Shear.proc": [[123, 133], ["torch.matmul().squeeze", "torch.tensor().to", "torch.tensor().to", "torch.matmul", "torch.tensor", "torch.tensor().to", "input.unsqueeze", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ",", "t0", ",", "t1", ")", ":", "\n", "        ", "if", "self", ".", "axis", "==", "'z'", ":", "\n", "            ", "shear_matrix", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "0", ",", "t0", "]", ",", "[", "0", ",", "1", ",", "t1", "]", ",", "[", "0", ",", "0", ",", "1", "]", "]", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "", "elif", "self", ".", "axis", "==", "'x'", ":", "\n", "            ", "shear_matrix", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "0", ",", "0", "]", ",", "[", "t0", ",", "1", ",", "0", "]", ",", "[", "t1", ",", "0", ",", "1", "]", "]", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "", "elif", "self", ".", "axis", "==", "'y'", ":", "\n", "            ", "shear_matrix", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "t0", ",", "0", "]", ",", "[", "0", ",", "1", ",", "0", "]", ",", "[", "0", ",", "t1", ",", "1", "]", "]", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "torch", ".", "matmul", "(", "shear_matrix", ",", "input", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Shear.batch_proc": [[134, 141], ["torch.zeros_like", "range", "len", "transforms.Shear.proc", "transforms.Shear.gen_param"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "# print(inputs.shape)", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "# print(outs[i].shape, inputs[i].shape)", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "*", "self", ".", "gen_param", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.__init__": [[145, 149], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "sigma", ",", "axis", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "num_points", "=", "canopy", ".", "shape", "[", "0", "]", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.gen_param": [[150, 153], ["torch.randn"], "methods", ["None"], ["", "def", "gen_param", "(", "self", ")", ":", "\n", "        ", "theta", "=", "torch", ".", "randn", "(", "1", ")", "\n", "return", "theta", "*", "self", ".", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.proc": [[154, 165], ["torch.cos().to", "torch.sin().to", "torch.zeros().to", "torch.cos", "torch.sin", "torch.zeros"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ",", "t", ")", ":", "\n", "# assume input: num_points x 3", "\n", "        ", "t", "=", "t", "*", "math", ".", "pi", "/", "180", "\n", "costz", "=", "torch", ".", "cos", "(", "input", "[", ":", ",", "2", "]", "*", "t", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "sintz", "=", "torch", ".", "sin", "(", "input", "[", ":", ",", "2", "]", "*", "t", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "outs", "=", "torch", ".", "zeros", "(", "input", ".", "shape", ")", ".", "to", "(", "input", ".", "device", ")", "\n", "outs", "[", ":", ",", "0", "]", "=", "input", "[", ":", ",", "0", "]", "*", "costz", "-", "input", "[", ":", ",", "1", "]", "*", "sintz", "\n", "outs", "[", ":", ",", "1", "]", "=", "input", "[", ":", ",", "0", "]", "*", "sintz", "+", "input", "[", ":", ",", "1", "]", "*", "costz", "\n", "outs", "[", ":", ",", "2", "]", "=", "input", "[", ":", ",", "2", "]", "\n", "# print(input.shape)", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.batch_proc": [[166, 173], ["torch.zeros_like", "range", "len", "transforms.Twist.proc", "transforms.Twist.gen_param"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "# print(inputs.shape)", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "# print(outs[i].shape, inputs[i].shape)", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "*", "self", ".", "gen_param", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.raw_proc": [[174, 180], ["torch.zeros_like", "range", "len", "transforms.Twist.proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc"], ["", "def", "raw_proc", "(", "self", ",", "inputs", ",", "theta", ":", "float", ")", ":", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "# print(outs[i].shape, inputs[i].shape)", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "theta", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.gen_param_uniform": [[181, 184], ["random.uniform"], "methods", ["None"], ["", "def", "gen_param_uniform", "(", "self", ")", ":", "\n", "        ", "theta", "=", "random", ".", "uniform", "(", "-", "self", ".", "sigma", ",", "self", ".", "sigma", ")", "\n", "return", "theta", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.batch_proc_uniform": [[185, 190], ["torch.zeros_like", "range", "len", "transforms.Twist.proc", "transforms.Twist.gen_param_uniform"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.gen_param_uniform"], ["", "def", "batch_proc_uniform", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "self", ".", "gen_param_uniform", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Taper.__init__": [[194, 198], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "theta", ",", "axis", "=", "'z'", ")", ":", "\n", "        ", "self", ".", "theta", "=", "theta", "\n", "self", ".", "num_points", "=", "canopy", ".", "shape", "[", "0", "]", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Taper.gen_param": [[199, 204], ["random.uniform"], "methods", ["None"], ["", "def", "gen_param", "(", "self", ")", ":", "\n", "# theta = torch.randn(2)", "\n", "# theta[0] *= self.sigma", "\n", "# theta [1] *= self.tau", "\n", "        ", "return", "random", ".", "uniform", "(", "-", "self", ".", "theta", ",", "self", ".", "theta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Taper.proc": [[205, 214], ["torch.zeros_like"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ",", "theta", ":", "float", ")", ":", "\n", "# taperz = 1 + theta1 * input[:,2] + theta2 * input[:,2]*input[:,2]*self.t**2/(1- self.t*input[:,2])", "\n", "# taperz = torch.exp(theta * input[:,2])", "\n", "        ", "taperz", "=", "1", "+", "theta", "*", "input", "[", ":", ",", "2", "]", "\n", "out", "=", "torch", ".", "zeros_like", "(", "input", ")", "\n", "out", "[", ":", ",", "0", "]", "=", "taperz", "*", "input", "[", ":", ",", "0", "]", "\n", "out", "[", ":", ",", "1", "]", "=", "taperz", "*", "input", "[", ":", ",", "1", "]", "\n", "out", "[", ":", ",", "2", "]", "=", "input", "[", ":", ",", "2", "]", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Taper.raw_proc": [[215, 220], ["torch.zeros_like", "range", "len", "transforms.Taper.proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc"], ["", "def", "raw_proc", "(", "self", ",", "inputs", ",", "theta", ":", "float", ")", ":", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "theta", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Taper.batch_proc": [[221, 228], ["torch.zeros_like", "range", "len", "transforms.Taper.proc", "transforms.Taper.gen_param"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "# print(inputs.shape)", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "# print(outs[i].shape, inputs[i].shape)", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "self", ".", "gen_param", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.__init__": [[230, 233], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "theta", ")", ":", "\n", "        ", "self", ".", "num_points", "=", "canopy", ".", "shape", "[", "0", "]", "\n", "self", ".", "theta", "=", "theta", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param": [[234, 236], ["torch.randn"], "methods", ["None"], ["", "def", "gen_param", "(", "self", ")", ":", "\n", "        ", "return", "torch", ".", "randn", "(", "3", ",", "3", ")", "*", "self", ".", "theta", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc": [[237, 241], ["transformation_matrix.to.to.to", "torch.matmul().squeeze", "torch.eye", "torch.matmul", "input.unsqueeze"], "methods", ["None"], ["", "def", "proc", "(", "self", ",", "input", ",", "t", ")", ":", "\n", "        ", "transformation_matrix", "=", "torch", ".", "eye", "(", "3", ")", "+", "t", "\n", "transformation_matrix", "=", "transformation_matrix", ".", "to", "(", "input", ".", "device", ")", "\n", "return", "torch", ".", "matmul", "(", "transformation_matrix", ",", "input", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc": [[242, 247], ["torch.zeros_like", "range", "len", "transforms.Linear.proc", "transforms.Linear.gen_param"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.gen_param"], ["", "def", "batch_proc", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "torch", ".", "zeros_like", "(", "inputs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "            ", "outs", "[", "i", "]", "=", "self", ".", "proc", "(", "inputs", "[", "i", "]", ",", "self", ".", "gen_param", "(", ")", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train.kl_div": [[75, 78], ["torch.kl_div", "torch.log_softmax"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.kl_div"], ["", "def", "kl_div", "(", "input", ",", "targets", ",", "reduction", "=", "'batchmean'", ")", ":", "\n", "    ", "return", "F", ".", "kl_div", "(", "F", ".", "log_softmax", "(", "input", ",", "dim", "=", "1", ")", ",", "targets", ",", "\n", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train._cross_entropy": [[80, 91], ["torch.softmax", "xent.sum", "xent.mean", "torch.log_softmax", "NotImplementedError"], "function", ["None"], ["", "def", "_cross_entropy", "(", "input", ",", "targets", ",", "reduction", "=", "'mean'", ")", ":", "\n", "    ", "targets_prob", "=", "F", ".", "softmax", "(", "targets", ",", "dim", "=", "1", ")", "\n", "xent", "=", "(", "-", "targets_prob", "*", "F", ".", "log_softmax", "(", "input", ",", "dim", "=", "1", ")", ")", ".", "sum", "(", "1", ")", "\n", "if", "reduction", "==", "'sum'", ":", "\n", "        ", "return", "xent", ".", "sum", "(", ")", "\n", "", "elif", "reduction", "==", "'mean'", ":", "\n", "        ", "return", "xent", ".", "mean", "(", ")", "\n", "", "elif", "reduction", "==", "'none'", ":", "\n", "        ", "return", "xent", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train._entropy": [[93, 95], ["train._cross_entropy"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._cross_entropy"], ["", "", "def", "_entropy", "(", "input", ",", "reduction", "=", "'mean'", ")", ":", "\n", "    ", "return", "_cross_entropy", "(", "input", ",", "input", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train.main": [[96, 158], ["datasets.get_dataset", "datasets.get_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "architectures.get_architecture().to", "os.path.join", "train_utils.init_logfile", "tensorboardX.SummaryWriter", "semantic.transformers.gen_transformer", "torch.nn.CrossEntropyLoss().to", "torch.optim.SGD", "torch.optim.lr_scheduler.StepLR", "range", "os.path.exists", "os.makedirs", "get_architecture().to.parameters", "time.time", "train.train", "train.test", "time.time", "torch.optim.lr_scheduler.StepLR.step", "train_utils.log", "torch.save", "torch.save", "torch.save", "architectures.get_architecture", "get_architecture().to.load_state_dict", "print", "torch.nn.CrossEntropyLoss", "os.path.join", "torch.load", "torch.load", "torch.load", "get_architecture().to.load_state_dict", "print", "torch.load", "torch.load", "torch.load", "get_architecture().to.load_state_dict", "print", "str", "get_architecture().to.state_dict", "torch.optim.SGD.state_dict", "torch.load", "torch.load", "torch.load", "datetime.timedelta", "torch.optim.lr_scheduler.StepLR.get_lr"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.get_dataset", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.get_dataset", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.init_logfile", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.gen_transformer", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.train", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.test", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.log", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.architectures.get_architecture"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "outdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "outdir", ")", "\n", "\n", "", "train_dataset", "=", "get_dataset", "(", "args", ".", "dataset", ",", "'train'", ")", "\n", "test_dataset", "=", "get_dataset", "(", "args", ".", "dataset", ",", "'test'", ")", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "shuffle", "=", "True", ",", "batch_size", "=", "args", ".", "batch", ",", "\n", "num_workers", "=", "args", ".", "workers", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "shuffle", "=", "False", ",", "batch_size", "=", "args", ".", "batch", ",", "\n", "num_workers", "=", "args", ".", "workers", ")", "\n", "\n", "model", "=", "get_architecture", "(", "args", ".", "arch", ",", "args", ".", "dataset", ")", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "pretrain", "is", "not", "None", ":", "\n", "        ", "if", "args", ".", "pretrain", "==", "'pointnet'", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "\"models/64p_natural.pth\"", ")", ")", "\n", "print", "(", "'loaded from models/64p_natural'", ")", "\n", "", "elif", "args", ".", "pretrain", "==", "'pointnet1024'", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "\"models/1024p_natural.pth\"", ")", ")", "\n", "print", "(", "'loaded from models/1024p_natural'", ")", "\n", "", "else", ":", "\n", "# load the base classifier", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "pretrain", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "print", "(", "f'loaded from {args.pretrain}'", ")", "\n", "\n", "", "", "logfilename", "=", "os", ".", "path", ".", "join", "(", "args", ".", "outdir", ",", "'log.txt'", ")", "\n", "init_logfile", "(", "logfilename", ",", "\"epoch\\ttime\\tlr\\ttrain loss\\ttrain acc\\ttestloss\\ttest acc\"", ")", "\n", "writer", "=", "SummaryWriter", "(", "args", ".", "outdir", ")", "\n", "\n", "canopy", "=", "None", "\n", "for", "data", "in", "train_loader", ":", "\n", "        ", "points", ",", "faces", ",", "label", "=", "data", "\n", "canopy", "=", "points", "[", "0", "]", "\n", "# print(points.shape,label.shape)", "\n", "break", "\n", "", "transformer", "=", "gen_transformer", "(", "args", ",", "canopy", ")", "\n", "\n", "criterion", "=", "CrossEntropyLoss", "(", ")", ".", "to", "(", "device", ")", "\n", "optimizer", "=", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "scheduler", "=", "StepLR", "(", "optimizer", ",", "step_size", "=", "args", ".", "lr_step_size", ",", "gamma", "=", "args", ".", "gamma", ")", "\n", "# optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999))", "\n", "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)", "\n", "\n", "for", "epoch", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "        ", "before", "=", "time", ".", "time", "(", ")", "\n", "train_loss", ",", "train_acc", "=", "train", "(", "train_loader", ",", "model", ",", "criterion", ",", "optimizer", ",", "epoch", ",", "transformer", ",", "writer", ")", "\n", "test_loss", ",", "test_acc", "=", "test", "(", "test_loader", ",", "model", ",", "criterion", ",", "epoch", ",", "transformer", ",", "writer", ")", "\n", "after", "=", "time", ".", "time", "(", ")", "\n", "\n", "scheduler", ".", "step", "(", "epoch", ")", "\n", "\n", "log", "(", "logfilename", ",", "\"{}\\t{:.3}\\t{:.3}\\t{:.3}\\t{:.3}\\t{:.3}\\t{:.3}\"", ".", "format", "(", "\n", "epoch", ",", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "(", "after", "-", "before", ")", ")", ")", ",", "\n", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "train_loss", ",", "train_acc", ",", "test_loss", ",", "test_acc", ")", ")", "\n", "\n", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "epoch", "+", "1", ",", "\n", "'arch'", ":", "args", ".", "arch", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "outdir", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train._chunk_minibatch": [[159, 164], ["range"], "function", ["None"], ["", "", "def", "_chunk_minibatch", "(", "batch", ",", "num_batches", ")", ":", "\n", "    ", "points", ",", "_", ",", "_", "=", "batch", "\n", "batch_size", "=", "points", ".", "shape", "[", "0", "]", "//", "num_batches", "\n", "for", "i", "in", "range", "(", "num_batches", ")", ":", "\n", "        ", "yield", "batch", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train.train": [[167, 261], ["train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "time.time", "model.train", "enumerate", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "train_utils.AverageMeter.update", "train._chunk_minibatch", "train_utils.AverageMeter.update", "time.time", "torch.squeeze().to", "torch.squeeze().to", "torch.squeeze().to", "points.float().to.float().to", "faces.float().to.float().to", "torch.cat", "torch.cat", "torch.cat", "torch.squeeze().to.repeat", "model", "criterion", "torch.chunk", "torch.chunk", "torch.chunk", "consistency.mean.mean", "train_utils.accuracy", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "optimizer.zero_grad", "loss.backward", "optimizer.step", "print", "time.time", "transformer.process().to", "torch.softmax", "sum", "sum", "torch.nll_loss", "criterion.item", "consistency.mean.item", "avg_confidence.item", "acc1.item", "acc5.item", "time.time", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "torch.squeeze", "torch.squeeze", "torch.squeeze", "points.float().to.float", "faces.float().to.float", "range", "kl_div().sum", "train._entropy", "len", "transformer.process", "train.kl_div"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.train", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._chunk_minibatch", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.accuracy", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._entropy", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.kl_div"], ["", "", "def", "train", "(", "loader", ":", "DataLoader", ",", "model", ":", "torch", ".", "nn", ".", "Module", ",", "criterion", ",", "optimizer", ":", "Optimizer", ",", "epoch", ":", "int", ",", "\n", "transformer", ":", "AbstractTransformer", ",", "writer", "=", "None", ")", ":", "\n", "    ", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "losses_reg", "=", "AverageMeter", "(", ")", "\n", "confidence", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "top5", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "# switch to train mode", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "        ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "\n", "mini_batches", "=", "_chunk_minibatch", "(", "batch", ",", "args", ".", "num_noise_vec", ")", "\n", "for", "data", "in", "mini_batches", ":", "\n", "            ", "points", ",", "faces", ",", "targets", "=", "data", "\n", "targets", "=", "torch", ".", "squeeze", "(", "targets", ")", ".", "to", "(", "device", ")", "\n", "points", "=", "points", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "faces", "=", "faces", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "# targets = targets.to(device)", "\n", "batch_size", "=", "points", ".", "shape", "[", "0", "]", "\n", "\n", "noised_inputs", "=", "[", "transformer", ".", "process", "(", "points", ")", ".", "to", "(", "device", ")", "for", "_", "in", "range", "(", "args", ".", "num_noise_vec", ")", "]", "\n", "\n", "# augment inputs with noise", "\n", "inputs_c", "=", "torch", ".", "cat", "(", "noised_inputs", ",", "dim", "=", "0", ")", "\n", "targets_c", "=", "targets", ".", "repeat", "(", "args", ".", "num_noise_vec", ")", "\n", "\n", "logits", "=", "model", "(", "inputs_c", ")", "\n", "\n", "loss_xent", "=", "criterion", "(", "logits", ",", "targets_c", ")", "\n", "\n", "logits_chunk", "=", "torch", ".", "chunk", "(", "logits", ",", "args", ".", "num_noise_vec", ",", "dim", "=", "0", ")", "\n", "softmax", "=", "[", "F", ".", "softmax", "(", "logit", ",", "dim", "=", "1", ")", "for", "logit", "in", "logits_chunk", "]", "\n", "avg_softmax", "=", "sum", "(", "softmax", ")", "/", "args", ".", "num_noise_vec", "\n", "\n", "consistency", "=", "[", "kl_div", "(", "logit", ",", "avg_softmax", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", "1", ")", "\n", "+", "_entropy", "(", "avg_softmax", ",", "reduction", "=", "'none'", ")", "\n", "for", "logit", "in", "logits_chunk", "]", "\n", "consistency", "=", "sum", "(", "consistency", ")", "/", "args", ".", "num_noise_vec", "\n", "consistency", "=", "consistency", ".", "mean", "(", ")", "\n", "\n", "loss", "=", "loss_xent", "+", "args", ".", "lbd", "*", "consistency", "\n", "# loss = loss_xent", "\n", "\n", "avg_confidence", "=", "-", "F", ".", "nll_loss", "(", "avg_softmax", ",", "targets", ")", "\n", "\n", "acc1", ",", "acc5", "=", "accuracy", "(", "logits", ",", "targets_c", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "losses", ".", "update", "(", "loss_xent", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "losses_reg", ".", "update", "(", "consistency", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "confidence", ".", "update", "(", "avg_confidence", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "top1", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "top5", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "\n", "# compute gradient and do SGD step", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# measure elapsed time", "\n", "", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "i", "%", "args", ".", "print_freq", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: [{0}][{1}/{2}]\\t'", "\n", "'Time {batch_time.avg:.3f}\\t'", "\n", "'Data {data_time.avg:.3f}\\t'", "\n", "'Loss {loss.avg:.4f}\\t'", "\n", "'Acc@1 {top1.avg:.3f}\\t'", "\n", "'Acc@5 {top5.avg:.3f}'", ".", "format", "(", "\n", "epoch", ",", "i", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "data_time", "=", "data_time", ",", "loss", "=", "losses", ",", "top1", "=", "top1", ",", "top5", "=", "top5", ")", ")", "\n", "\n", "if", "args", ".", "print_step", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/loss/train'", ",", "losses", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/loss/consistency'", ",", "losses_reg", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/loss/avg_confidence'", ",", "confidence", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/batch_time'", ",", "batch_time", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/accuracy/train@1'", ",", "top1", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/accuracy/train@5'", ",", "top5", ".", "avg", ",", "i", ")", "\n", "\n", "", "", "", "writer", ".", "add_scalar", "(", "'loss/train'", ",", "losses", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'loss/consistency'", ",", "losses_reg", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'loss/avg_confidence'", ",", "confidence", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'batch_time'", ",", "batch_time", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'accuracy/train@1'", ",", "top1", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'accuracy/train@5'", ",", "top5", ".", "avg", ",", "epoch", ")", "\n", "\n", "return", "(", "losses", ".", "avg", ",", "top1", ".", "avg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train.test": [[263, 318], ["train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "time.time", "model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "train_utils.AverageMeter.update", "torch.squeeze().to", "torch.squeeze().to", "torch.squeeze().to", "points.float().to.float().to", "faces.float().to.float().to", "transformer.process().to", "model", "criterion", "train_utils.accuracy", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "time.time", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "criterion.item", "transformer.process().to.size", "acc1.item", "transformer.process().to.size", "acc5.item", "transformer.process().to.size", "print", "time.time", "torch.squeeze", "torch.squeeze", "torch.squeeze", "points.float().to.float", "faces.float().to.float", "transformer.process", "time.time", "len"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.accuracy", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process"], ["", "def", "test", "(", "loader", ",", "model", ",", "criterion", ",", "epoch", ",", "transformer", ":", "AbstractTransformer", ",", "writer", "=", "None", ",", "print_freq", "=", "25", ")", ":", "\n", "    ", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "top5", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "# switch to eval mode", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "data", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "            ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "points", ",", "faces", ",", "targets", "=", "data", "\n", "targets", "=", "torch", ".", "squeeze", "(", "targets", ")", ".", "to", "(", "device", ")", "\n", "points", "=", "points", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "faces", "=", "faces", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "# inputs = inputs", "\n", "# targets = targets.to(device)()", "\n", "\n", "# augment inputs with noise", "\n", "inputs", "=", "transformer", ".", "process", "(", "points", ")", ".", "to", "(", "device", ")", "\n", "\n", "# compute output", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "targets", ")", "\n", "\n", "# measure accuracy and record loss", "\n", "acc1", ",", "acc5", "=", "accuracy", "(", "outputs", ",", "targets", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "losses", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "inputs", ".", "size", "(", "0", ")", ")", "\n", "top1", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "inputs", ".", "size", "(", "0", ")", ")", "\n", "top5", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "inputs", ".", "size", "(", "0", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "print_freq", "==", "0", ":", "\n", "                ", "print", "(", "'Test: [{0}/{1}]\\t'", "\n", "'Time {batch_time.avg:.3f}\\t'", "\n", "'Data {data_time.avg:.3f}\\t'", "\n", "'Loss {loss.avg:.4f}\\t'", "\n", "'Acc@1 {top1.avg:.3f}\\t'", "\n", "'Acc@5 {top5.avg:.3f}'", ".", "format", "(", "\n", "i", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "data_time", "=", "data_time", ",", "\n", "loss", "=", "losses", ",", "top1", "=", "top1", ",", "top5", "=", "top5", ")", ")", "\n", "\n", "", "", "if", "writer", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'loss/test'", ",", "losses", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'accuracy/test@1'", ",", "top1", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'accuracy/test@5'", ",", "top5", ".", "avg", ",", "epoch", ")", "\n", "\n", "", "return", "(", "losses", ".", "avg", ",", "top1", ".", "avg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.AbstractTransformer.process": [[14, 16], ["None"], "methods", ["None"], ["    ", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.AbstractTransformer.calc_radius": [[17, 19], ["None"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", "->", "float", ":", "\n", "        ", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.NoiseTransformer.__init__": [[23, 27], ["super().__init__", "semantic.Noise"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ")", ":", "\n", "        ", "super", "(", "NoiseTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "noise_adder", "=", "transforms", ".", "Noise", "(", "self", ".", "sigma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.NoiseTransformer.process": [[28, 31], ["transformers.NoiseTransformer.noise_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "self", ".", "noise_adder", ".", "batch_proc", "(", "inputs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.NoiseTransformer.calc_radius": [[32, 35], ["scipy.stats.norm.ppf"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "radius", "=", "self", ".", "sigma", "*", "norm", ".", "ppf", "(", "pABar", ")", "\n", "return", "radius", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudRotation.__init__": [[38, 43], ["super().__init__", "semantic.Rotation3D"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ",", "canopy", ",", "axis", "=", "'z'", ")", ":", "\n", "        ", "super", "(", "PointCloudRotation", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "rotation_adder3d", "=", "transforms", ".", "Rotation3D", "(", "canopy", ",", "sigma", ",", "axis", ")", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudRotation.process": [[44, 47], ["transformers.PointCloudRotation.rotation_adder3d.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "self", ".", "rotation_adder3d", ".", "batch_proc", "(", "inputs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudRotation.calc_radius": [[48, 50], ["scipy.stats.norm.ppf"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "self", ".", "sigma", "*", "norm", ".", "ppf", "(", "pABar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudShear.__init__": [[53, 58], ["super().__init__", "semantic.Shear"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ",", "canopy", ",", "axis", "=", "'z'", ")", ":", "\n", "        ", "super", "(", "PointCloudShear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "shear_adder", "=", "transforms", ".", "Shear", "(", "canopy", ",", "sigma", ",", "axis", ")", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudShear.process": [[59, 62], ["transformers.PointCloudShear.shear_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "self", ".", "shear_adder", ".", "batch_proc", "(", "inputs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudShear.calc_radius": [[63, 65], ["scipy.stats.norm.ppf"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "self", ".", "sigma", "*", "norm", ".", "ppf", "(", "pABar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudTwist.__init__": [[68, 73], ["super().__init__", "semantic.Twist"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ",", "canopy", ",", "axis", "=", "'z'", ")", ":", "\n", "        ", "super", "(", "PointCloudTwist", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "twist_adder", "=", "transforms", ".", "Twist", "(", "canopy", ",", "sigma", ",", "axis", ")", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudTwist.process": [[74, 77], ["transformers.PointCloudTwist.twist_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "self", ".", "twist_adder", ".", "batch_proc", "(", "inputs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudTwist.calc_radius": [[78, 80], ["scipy.stats.norm.ppf"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "self", ".", "sigma", "*", "norm", ".", "ppf", "(", "pABar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudTaper.__init__": [[82, 86], ["super().__init__", "semantic.Taper"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "canopy", ",", "angle", "=", "0", ",", "axis", "=", "'z'", ")", ":", "\n", "        ", "super", "(", "PointCloudTaper", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "taper_adder", "=", "transforms", ".", "Taper", "(", "canopy", ",", "angle", ",", "axis", ")", "\n", "self", ".", "axis", "=", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudTaper.process": [[87, 90], ["transformers.PointCloudTaper.taper_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "input", ")", ":", "\n", "        ", "outs", "=", "self", ".", "taper_adder", ".", "batch_proc", "(", "input", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudTaper.calc_radius": [[91, 93], ["None"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "1e+99", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudNoise.__init__": [[96, 100], ["super().__init__", "semantic.Noise"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ")", ":", "\n", "        ", "super", "(", "PointCloudNoise", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "noise_adder", "=", "transforms", ".", "Noise", "(", "sigma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudNoise.process": [[101, 104], ["transformers.PointCloudNoise.noise_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "self", ".", "noise_adder", ".", "batch_proc", "(", "inputs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudNoise.calc_radius": [[105, 107], ["scipy.stats.norm.ppf"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "self", ".", "sigma", "*", "norm", ".", "ppf", "(", "pABar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.TaperNoiseTransformer.__init__": [[110, 115], ["super().__init__", "semantic.Noise", "semantic.Taper"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ",", "canopy", ",", "angle", ")", ":", "\n", "        ", "super", "(", "TaperNoiseTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "noise_adder", "=", "transforms", ".", "Noise", "(", "self", ".", "sigma", ")", "\n", "self", ".", "taper_adder", "=", "transforms", ".", "Taper", "(", "canopy", ",", "angle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.TaperNoiseTransformer.process": [[116, 121], ["transformers.TaperNoiseTransformer.taper_adder.batch_proc", "transformers.TaperNoiseTransformer.noise_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "inputs", "\n", "outs", "=", "self", ".", "taper_adder", ".", "batch_proc", "(", "outs", ")", "\n", "outs", "=", "self", ".", "noise_adder", ".", "batch_proc", "(", "outs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.TaperNoiseTransformer.calc_radius": [[122, 124], ["None"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "1e+99", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointRotationNoiseTransformer.__init__": [[127, 132], ["super().__init__", "semantic.Noise", "semantic.GeneralRotation3D"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ",", "canopy", ",", "angle", ")", ":", "\n", "        ", "super", "(", "PointRotationNoiseTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "noise_adder", "=", "transforms", ".", "Noise", "(", "self", ".", "sigma", ")", "\n", "self", ".", "rotation_adder", "=", "transforms", ".", "GeneralRotation3D", "(", "canopy", ",", "angle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointRotationNoiseTransformer.process": [[133, 138], ["transformers.PointRotationNoiseTransformer.rotation_adder.batch_proc", "transformers.PointRotationNoiseTransformer.noise_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "inputs", "\n", "outs", "=", "self", ".", "rotation_adder", ".", "batch_proc", "(", "outs", ")", "\n", "outs", "=", "self", ".", "noise_adder", ".", "batch_proc", "(", "outs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointRotationNoiseTransformer.calc_radius": [[139, 141], ["None"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "1e+99", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistRotationZ.__init__": [[143, 149], ["super().__init__", "semantic.Twist", "semantic.Rotation3D"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma_t", ",", "sigma_r", ",", "canopy", ")", ":", "\n", "        ", "super", "(", "PointTwistRotationZ", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma_t", "=", "sigma_t", "\n", "self", ".", "sigma_r", "=", "sigma_r", "\n", "self", ".", "twist_adder", "=", "transforms", ".", "Twist", "(", "canopy", ",", "self", ".", "sigma_t", ",", "'z'", ")", "\n", "self", ".", "rotation_adder", "=", "transforms", ".", "Rotation3D", "(", "canopy", ",", "self", ".", "sigma_r", ",", "'z'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistRotationZ.process": [[150, 155], ["transformers.PointTwistRotationZ.twist_adder.batch_proc", "transformers.PointTwistRotationZ.rotation_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "inputs", "\n", "outs", "=", "self", ".", "twist_adder", ".", "batch_proc", "(", "outs", ")", "\n", "outs", "=", "self", ".", "rotation_adder", ".", "batch_proc", "(", "outs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistRotationZ.calc_radius": [[156, 159], ["scipy.stats.norm.ppf"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "# Return a relative radius of an ellipse", "\n", "        ", "return", "norm", ".", "ppf", "(", "pABar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTaperRotationNoise.__init__": [[161, 169], ["super().__init__", "semantic.Taper", "semantic.Rotation3D", "semantic.Noise"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "taper_scale", ",", "rotation_angle", ",", "noise_sd", ",", "canopy", ")", ":", "\n", "        ", "super", "(", "PointTaperRotationNoise", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "taper_scale", "=", "taper_scale", "\n", "self", ".", "rotation_angle", "=", "rotation_angle", "\n", "self", ".", "noise_sd", "=", "noise_sd", "\n", "self", ".", "taper_adder", "=", "transforms", ".", "Taper", "(", "canopy", ",", "taper_scale", ")", "\n", "self", ".", "rotation_adder", "=", "transforms", ".", "Rotation3D", "(", "canopy", ",", "rotation_angle", ",", "'z'", ")", "\n", "self", ".", "noise_adder", "=", "transforms", ".", "Noise", "(", "self", ".", "noise_sd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTaperRotationNoise.process": [[170, 176], ["transformers.PointTaperRotationNoise.rotation_adder.batch_proc_uniform", "transformers.PointTaperRotationNoise.taper_adder.batch_proc", "transformers.PointTaperRotationNoise.noise_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.batch_proc_uniform", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "inputs", "\n", "outs", "=", "self", ".", "rotation_adder", ".", "batch_proc_uniform", "(", "outs", ")", "\n", "outs", "=", "self", ".", "taper_adder", ".", "batch_proc", "(", "outs", ")", "\n", "outs", "=", "self", ".", "noise_adder", ".", "batch_proc", "(", "outs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTaperRotationNoise.calc_radius": [[177, 179], ["None"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "1e+99", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistTaperRotationNoise.__init__": [[183, 192], ["super().__init__", "semantic.Taper", "semantic.Twist", "semantic.Rotation3D", "semantic.Noise"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "taper_scale", ",", "rotation_angle", ",", "twist_angle", ",", "noise_sd", ",", "canopy", ")", ":", "\n", "        ", "super", "(", "PointTwistTaperRotationNoise", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "taper_scale", "=", "taper_scale", "\n", "self", ".", "rotation_angle", "=", "rotation_angle", "\n", "self", ".", "noise_sd", "=", "noise_sd", "\n", "self", ".", "taper_adder", "=", "transforms", ".", "Taper", "(", "canopy", ",", "taper_scale", ")", "\n", "self", ".", "twist_adder", "=", "transforms", ".", "Twist", "(", "canopy", ",", "twist_angle", ",", "'z'", ")", "\n", "self", ".", "rotation_adder", "=", "transforms", ".", "Rotation3D", "(", "canopy", ",", "rotation_angle", ",", "'z'", ")", "\n", "self", ".", "noise_adder", "=", "transforms", ".", "Noise", "(", "self", ".", "noise_sd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistTaperRotationNoise.process": [[193, 200], ["transformers.PointTwistTaperRotationNoise.rotation_adder.batch_proc_uniform", "transformers.PointTwistTaperRotationNoise.taper_adder.batch_proc", "transformers.PointTwistTaperRotationNoise.twist_adder.batch_proc_uniform", "transformers.PointTwistTaperRotationNoise.noise_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.batch_proc_uniform", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Twist.batch_proc_uniform", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "inputs", "\n", "outs", "=", "self", ".", "rotation_adder", ".", "batch_proc_uniform", "(", "outs", ")", "\n", "outs", "=", "self", ".", "taper_adder", ".", "batch_proc", "(", "outs", ")", "\n", "outs", "=", "self", ".", "twist_adder", ".", "batch_proc_uniform", "(", "outs", ")", "\n", "outs", "=", "self", ".", "noise_adder", ".", "batch_proc", "(", "outs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointTwistTaperRotationNoise.calc_radius": [[201, 203], ["None"], "methods", ["None"], ["", "def", "calc_radius", "(", "self", ",", "pABar", ":", "float", ")", ":", "\n", "        ", "return", "1e+99", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.__init__": [[206, 211], ["super().__init__", "semantic.Linear"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sigma", ",", "canopy", ")", ":", "\n", "        ", "super", "(", "PointCloudLinear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "linear_adder", "=", "transforms", ".", "Linear", "(", "canopy", ",", "sigma", ")", "\n", "self", ".", "canopy", "=", "canopy", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process": [[212, 216], ["transformers.PointCloudLinear.linear_adder.batch_proc"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transforms.Linear.batch_proc"], ["", "def", "process", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "outs", "=", "inputs", "\n", "outs", "=", "self", ".", "linear_adder", ".", "batch_proc", "(", "outs", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.monte_carlo": [[217, 238], ["numpy.zeros", "range", "numpy.sqrt().flatten", "torch.from_numpy().cuda", "torch.randn().cuda", "range", "range", "numpy.sqrt", "torch.from_numpy", "torch.randn", "statsmodels.stats.proportion.proportion_confint", "statsmodels.stats.proportion.proportion_confint", "numpy.linalg.norm"], "methods", ["None"], ["", "def", "monte_carlo", "(", "self", ",", "pA", ",", "A", ")", ":", "\n", "        ", "k", "=", "np", ".", "zeros", "(", "[", "3", ",", "3", "]", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "3", ")", ":", "\n", "                ", "k", "[", "i", ",", "j", "]", "=", "(", "1", "+", "A", "[", "j", ",", "j", "]", ")", "**", "2", "+", "np", ".", "linalg", ".", "norm", "(", "A", "[", ":", ",", "j", "]", ")", "**", "2", "-", "A", "[", "j", ",", "j", "]", "**", "2", "\n", "", "", "k", "=", "np", ".", "sqrt", "(", "k", ")", ".", "flatten", "(", ")", "\n", "k", "=", "torch", ".", "from_numpy", "(", "k", ")", ".", "cuda", "(", ")", "\n", "num", "=", "200000", "\n", "sample_points", "=", "torch", ".", "randn", "(", "num", ",", "9", ")", ".", "cuda", "(", ")", "\n", "radi", "=", "(", "sample_points", "**", "2", "*", "(", "1", "-", "1", "/", "k", "**", "2", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "radi2", "=", "(", "sample_points", "**", "2", "*", "(", "1", "-", "1", "/", "k", "**", "2", ")", "*", "(", "k", "**", "2", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "for", "t1", "in", "range", "(", "0", ",", "2000", ",", "1", ")", ":", "\n", "            ", "t", "=", "(", "1000", "-", "t1", ")", "/", "50", "\n", "count", "=", "(", "(", "radi", "-", "t", ")", "<", "0", ")", ".", "sum", "(", ")", ".", "cpu", "(", ")", "\n", "pA_high", "=", "proportion_confint", "(", "count", ",", "num", ",", "alpha", "=", "0.01", "/", "2", ",", "method", "=", "'normal'", ")", "[", "1", "]", "\n", "if", "(", "pA_high", ">=", "pA", ")", ":", "\n", "                ", "continue", "\n", "", "count_tilde", "=", "(", "(", "radi2", "-", "t", ")", "<", "0", ")", ".", "sum", "(", ")", ".", "cpu", "(", ")", "\n", "p_tilde_low", "=", "proportion_confint", "(", "count_tilde", ",", "num", ",", "alpha", "=", "0.01", "/", "2", ",", "method", "=", "'normal'", ")", "[", "0", "]", "\n", "return", "p_tilde_low", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.test_certify": [[239, 252], ["A.append", "A.append", "A.append", "A.append", "numpy.diag", "numpy.diag", "min", "numpy.eye", "math.sqrt", "math.sqrt", "numpy.array", "numpy.array", "transformers.PointCloudLinear.monte_carlo", "numpy.eye", "scipy.stats.norm.ppf"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.monte_carlo"], ["", "def", "test_certify", "(", "self", ",", "pA", ",", "alpha", ")", ":", "\n", "# print(alpha)", "\n", "        ", "A", "=", "[", "]", "\n", "pA_tilde", "=", "1", "\n", "A", ".", "append", "(", "np", ".", "eye", "(", "3", ")", "*", "math", ".", "sqrt", "(", "alpha", "**", "2", "/", "3", ")", ")", "\n", "A", ".", "append", "(", "-", "np", ".", "eye", "(", "3", ")", "*", "math", ".", "sqrt", "(", "alpha", "**", "2", "/", "3", ")", ")", "\n", "A", ".", "append", "(", "np", ".", "diag", "(", "np", ".", "array", "(", "[", "alpha", ",", "0", ",", "0", "]", ")", ")", ")", "\n", "A", ".", "append", "(", "np", ".", "diag", "(", "np", ".", "array", "(", "[", "-", "alpha", ",", "0", ",", "0", "]", ")", ")", ")", "\n", "for", "i", "in", "A", ":", "\n", "            ", "pA_tilde", "=", "min", "(", "pA_tilde", ",", "self", ".", "monte_carlo", "(", "pA", ",", "i", ")", ")", "\n", "", "if", "alpha", "<", "self", ".", "sigma", "*", "norm", ".", "ppf", "(", "pA_tilde", ")", "*", "(", "1", "-", "alpha", ")", ":", "\n", "           ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.gen_transformer": [[255, 299], ["print", "print", "transformers.PointCloudRotation", "print", "print", "transformers.PointCloudShear", "print", "print", "transformers.PointCloudTwist", "print", "print", "transformers.PointCloudTaper", "print", "transformers.TaperNoiseTransformer", "print", "transformers.PointCloudNoise", "print", "transformers.PointRotationNoiseTransformer", "print", "transformers.PointTwistRotationZ", "print", "transformers.PointTaperRotationNoise", "print", "transformers.PointTwistTaperRotationNoise", "print", "transformers.PointCloudLinear", "print", "transformers.NoiseTransformer"], "function", ["None"], ["", "", "def", "gen_transformer", "(", "args", ",", "canopy", ")", "->", "AbstractTransformer", ":", "\n", "    ", "if", "args", ".", "transtype", "==", "'points-rotation'", ":", "\n", "        ", "print", "(", "f'rotation point clouds {args.noise_sd}'", ")", "\n", "print", "(", "f'axis {args.axis}'", ")", "\n", "return", "PointCloudRotation", "(", "args", ".", "noise_sd", ",", "canopy", ",", "args", ".", "axis", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-shear'", ":", "\n", "        ", "print", "(", "f'shear point clouds {args.noise_sd}'", ")", "\n", "print", "(", "f'axis {args.axis}'", ")", "\n", "return", "PointCloudShear", "(", "args", ".", "noise_sd", ",", "canopy", ",", "args", ".", "axis", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-twist'", ":", "\n", "        ", "print", "(", "f'twist point clouds {args.noise_sd}'", ")", "\n", "print", "(", "f'axis {args.axis}'", ")", "\n", "return", "PointCloudTwist", "(", "args", ".", "noise_sd", ",", "canopy", ",", "args", ".", "axis", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-taper'", ":", "\n", "        ", "print", "(", "f'taper point clouds {args.noise_sd}'", ")", "\n", "print", "(", "f'axis {args.axis}'", ")", "\n", "return", "PointCloudTaper", "(", "args", ".", "noise_sd", ",", "canopy", ",", "args", ".", "axis", ")", "\n", "# return PointCloudTaper(args.noise_sd, 0.8*args.noise_sd, canopy, args.axis)", "\n", "", "elif", "args", ".", "transtype", "==", "'points-taper-noise'", ":", "\n", "        ", "print", "(", "f'taper angle in +- {args.taper_angle} and noise in {args.noise_sd}'", ")", "\n", "return", "TaperNoiseTransformer", "(", "args", ".", "noise_sd", ",", "canopy", ",", "args", ".", "taper_angle", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-noise'", ":", "\n", "        ", "print", "(", "f'Gaussian noise {args.noise_sd}'", ")", "\n", "return", "PointCloudNoise", "(", "args", ".", "noise_sd", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-rotation-noise'", ":", "\n", "        ", "print", "(", "f'rotation angle in +- {args.rotation_angle} and noise in {args.noise_sd}'", ")", "\n", "return", "PointRotationNoiseTransformer", "(", "args", ".", "noise_sd", ",", "canopy", ",", "args", ".", "rotation_angle", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-twist-rotationz'", ":", "\n", "        ", "print", "(", "f'rotation angle in +- {args.rotation_angle} and twist in {args.noise_sd}'", ")", "\n", "return", "PointTwistRotationZ", "(", "args", ".", "noise_sd", ",", "args", ".", "rotation_angle", ",", "canopy", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-taper-rotationz'", ":", "\n", "        ", "print", "(", "f'rotation angle in +- {args.rotation_angle}, taper in +- {args.taper_angle} and noise in {args.noise_sd}'", ")", "\n", "return", "PointTaperRotationNoise", "(", "args", ".", "taper_angle", ",", "args", ".", "rotation_angle", ",", "args", ".", "noise_sd", ",", "canopy", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-twist-taper-rotationz'", ":", "\n", "        ", "print", "(", "f'rotation angle in +- {args.rotation_angle}, taper in +- {args.taper_angle}, twist in +- {args.twist_angle} and noise in {args.noise_sd}'", ")", "\n", "return", "PointTwistTaperRotationNoise", "(", "args", ".", "taper_angle", ",", "args", ".", "rotation_angle", ",", "args", ".", "twist_angle", ",", "args", ".", "noise_sd", ",", "canopy", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'points-linear'", ":", "\n", "        ", "print", "(", "f'linear transformation in += {args.noise_sd}'", ")", "\n", "return", "PointCloudLinear", "(", "args", ".", "noise_sd", ",", "canopy", ")", "\n", "", "elif", "args", ".", "transtype", "==", "'noise'", ":", "\n", "        ", "print", "(", "f'noise {args.noise_sd}'", ")", "\n", "return", "NoiseTransformer", "(", "args", ".", "noise_sd", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.kl_div": [[78, 81], ["torch.kl_div", "torch.log_softmax"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.kl_div"], ["", "def", "kl_div", "(", "input", ",", "targets", ",", "reduction", "=", "'batchmean'", ")", ":", "\n", "    ", "return", "F", ".", "kl_div", "(", "F", ".", "log_softmax", "(", "input", ",", "dim", "=", "1", ")", ",", "targets", ",", "\n", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._cross_entropy": [[83, 94], ["torch.softmax", "xent.sum", "xent.mean", "torch.log_softmax", "NotImplementedError"], "function", ["None"], ["", "def", "_cross_entropy", "(", "input", ",", "targets", ",", "reduction", "=", "'mean'", ")", ":", "\n", "    ", "targets_prob", "=", "F", ".", "softmax", "(", "targets", ",", "dim", "=", "1", ")", "\n", "xent", "=", "(", "-", "targets_prob", "*", "F", ".", "log_softmax", "(", "input", ",", "dim", "=", "1", ")", ")", ".", "sum", "(", "1", ")", "\n", "if", "reduction", "==", "'sum'", ":", "\n", "        ", "return", "xent", ".", "sum", "(", ")", "\n", "", "elif", "reduction", "==", "'mean'", ":", "\n", "        ", "return", "xent", ".", "mean", "(", ")", "\n", "", "elif", "reduction", "==", "'none'", ":", "\n", "        ", "return", "xent", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._entropy": [[96, 98], ["train_segmentation._cross_entropy"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._cross_entropy"], ["", "", "def", "_entropy", "(", "input", ",", "reduction", "=", "'mean'", ")", ":", "\n", "    ", "return", "_cross_entropy", "(", "input", ",", "input", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.main": [[99, 161], ["datasets.get_dataset", "datasets.get_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "architectures.get_architecture().to", "os.path.join", "train_utils.init_logfile", "tensorboardX.SummaryWriter", "semantic.transformers.gen_transformer", "torch.nn.CrossEntropyLoss().to", "torch.optim.SGD", "torch.optim.lr_scheduler.StepLR", "range", "os.path.exists", "os.makedirs", "get_architecture().to.parameters", "time.time", "train_segmentation.train", "train_segmentation.test", "time.time", "torch.optim.lr_scheduler.StepLR.step", "train_utils.log", "torch.save", "torch.save", "torch.save", "architectures.get_architecture", "get_architecture().to.load_state_dict", "print", "torch.nn.CrossEntropyLoss", "os.path.join", "torch.load", "torch.load", "torch.load", "get_architecture().to.load_state_dict", "print", "torch.load", "torch.load", "torch.load", "get_architecture().to.load_state_dict", "print", "str", "get_architecture().to.state_dict", "torch.optim.SGD.state_dict", "torch.load", "torch.load", "torch.load", "datetime.timedelta", "torch.optim.lr_scheduler.StepLR.get_lr"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.get_dataset", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.datasets.get_dataset", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.init_logfile", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.gen_transformer", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.train", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.test", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.log", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.architectures.get_architecture"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "outdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "outdir", ")", "\n", "\n", "", "train_dataset", "=", "get_dataset", "(", "args", ".", "dataset", ",", "'train'", ")", "\n", "test_dataset", "=", "get_dataset", "(", "args", ".", "dataset", ",", "'test'", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "shuffle", "=", "True", ",", "batch_size", "=", "args", ".", "batch", ",", "\n", "num_workers", "=", "args", ".", "workers", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "shuffle", "=", "False", ",", "batch_size", "=", "args", ".", "batch", ",", "\n", "num_workers", "=", "args", ".", "workers", ")", "\n", "\n", "model", "=", "get_architecture", "(", "args", ".", "arch", ",", "args", ".", "dataset", ")", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "pretrain", "is", "not", "None", ":", "\n", "        ", "if", "args", ".", "pretrain", "==", "'pointnet'", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "\"models/64p_natural.pth\"", ")", ")", "\n", "print", "(", "'loaded from models/64p_natural'", ")", "\n", "", "elif", "args", ".", "pretrain", "==", "'pointnet1024'", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "\"models/1024p_natural.pth\"", ")", ")", "\n", "print", "(", "'loaded from models/1024p_natural'", ")", "\n", "", "else", ":", "\n", "# load the base classifier", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "pretrain", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "print", "(", "f'loaded from {args.pretrain}'", ")", "\n", "\n", "", "", "logfilename", "=", "os", ".", "path", ".", "join", "(", "args", ".", "outdir", ",", "'log.txt'", ")", "\n", "init_logfile", "(", "logfilename", ",", "\"epoch\\ttime\\tlr\\ttrain loss\\ttrain acc\\ttestloss\\ttest acc\"", ")", "\n", "writer", "=", "SummaryWriter", "(", "args", ".", "outdir", ")", "\n", "\n", "canopy", "=", "None", "\n", "for", "data", "in", "train_loader", ":", "\n", "        ", "points", ",", "label", "=", "data", "\n", "canopy", "=", "points", "[", "0", "]", "\n", "break", "\n", "", "transformer", "=", "gen_transformer", "(", "args", ",", "canopy", ")", "\n", "\n", "criterion", "=", "CrossEntropyLoss", "(", ")", ".", "to", "(", "device", ")", "\n", "optimizer", "=", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "scheduler", "=", "StepLR", "(", "optimizer", ",", "step_size", "=", "args", ".", "lr_step_size", ",", "gamma", "=", "args", ".", "gamma", ")", "\n", "# optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999))", "\n", "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)", "\n", "\n", "for", "epoch", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "        ", "before", "=", "time", ".", "time", "(", ")", "\n", "train_loss", ",", "train_acc", "=", "train", "(", "train_loader", ",", "model", ",", "criterion", ",", "optimizer", ",", "epoch", ",", "transformer", ",", "writer", ")", "\n", "test_loss", ",", "test_acc", "=", "test", "(", "test_loader", ",", "model", ",", "criterion", ",", "epoch", ",", "transformer", ",", "writer", ")", "\n", "after", "=", "time", ".", "time", "(", ")", "\n", "\n", "scheduler", ".", "step", "(", "epoch", ")", "\n", "\n", "log", "(", "logfilename", ",", "\"{}\\t{:.3}\\t{:.3}\\t{:.3}\\t{:.3}%\\t{:.3}\\t{:.3}%\"", ".", "format", "(", "\n", "epoch", ",", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "(", "after", "-", "before", ")", ")", ")", ",", "\n", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "train_loss", ",", "train_acc", "*", "100", ",", "test_loss", ",", "test_acc", "*", "100", ")", ")", "\n", "\n", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "epoch", "+", "1", ",", "\n", "'arch'", ":", "args", ".", "arch", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "outdir", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._chunk_minibatch": [[162, 167], ["range"], "function", ["None"], ["", "", "def", "_chunk_minibatch", "(", "batch", ",", "num_batches", ")", ":", "\n", "    ", "points", ",", "_", "=", "batch", "\n", "batch_size", "=", "points", ".", "shape", "[", "0", "]", "//", "num_batches", "\n", "for", "i", "in", "range", "(", "num_batches", ")", ":", "\n", "        ", "yield", "batch", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.train": [[170, 270], ["train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "time.time", "model.train", "enumerate", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "train_utils.AverageMeter.update", "train_segmentation._chunk_minibatch", "train_utils.AverageMeter.update", "time.time", "torch.squeeze().to", "torch.squeeze().to", "torch.squeeze().to", "points.float().to.float().to", "torch.cat", "torch.cat", "torch.cat", "model", "criterion", "torch.chunk", "torch.chunk", "torch.chunk", "consistency.mean.mean", "max_predict.eq().float().mean().cpu().sum", "max_predict.eq().float().mean().cpu().sum.item", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "optimizer.zero_grad", "loss.backward", "optimizer.step", "print", "time.time", "transformer.process().to", "torch.softmax", "sum", "sum", "torch.nll_loss", "model.data.max", "points.float().to.size", "criterion.item", "consistency.mean.item", "avg_confidence.item", "time.time", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "torch.squeeze", "torch.squeeze", "torch.squeeze", "points.float().to.float", "range", "kl_div().sum", "train_segmentation._entropy", "max_predict.eq().float().mean().cpu", "len", "transformer.process", "train_segmentation.kl_div", "max_predict.eq().float().mean", "max_predict.eq().float", "max_predict.eq"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.train", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._chunk_minibatch", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation._entropy", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.kl_div"], ["", "", "def", "train", "(", "loader", ":", "DataLoader", ",", "model", ":", "torch", ".", "nn", ".", "Module", ",", "criterion", ",", "optimizer", ":", "Optimizer", ",", "epoch", ":", "int", ",", "\n", "transformer", ":", "AbstractTransformer", ",", "writer", "=", "None", ")", ":", "\n", "    ", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "losses_reg", "=", "AverageMeter", "(", ")", "\n", "confidence", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "top5", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "train_correct", "=", "0.0", "\n", "train_amount", "=", "0.0", "\n", "# switch to train mode", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "        ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "\n", "mini_batches", "=", "_chunk_minibatch", "(", "batch", ",", "args", ".", "num_noise_vec", ")", "\n", "for", "data", "in", "mini_batches", ":", "\n", "            ", "points", ",", "targets", "=", "data", "\n", "targets", "=", "torch", ".", "squeeze", "(", "targets", ")", ".", "to", "(", "device", ")", "\n", "points", "=", "points", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "# targets = targets.to(device)", "\n", "batch_size", "=", "points", ".", "shape", "[", "0", "]", "\n", "\n", "noised_inputs", "=", "[", "transformer", ".", "process", "(", "points", ")", ".", "to", "(", "device", ")", "for", "_", "in", "range", "(", "args", ".", "num_noise_vec", ")", "]", "\n", "\n", "# augment inputs with noise", "\n", "inputs_c", "=", "torch", ".", "cat", "(", "noised_inputs", ",", "dim", "=", "0", ")", "\n", "# print(inputs_c.shape, targets.shape)", "\n", "targets_c", "=", "targets", "\n", "\n", "logits", "=", "model", "(", "inputs_c", ")", "\n", "\n", "loss_xent", "=", "criterion", "(", "logits", ",", "targets_c", ")", "\n", "# print(logits.shape)", "\n", "logits_chunk", "=", "torch", ".", "chunk", "(", "logits", ",", "args", ".", "num_noise_vec", ",", "dim", "=", "0", ")", "\n", "softmax", "=", "[", "F", ".", "softmax", "(", "logit", ",", "dim", "=", "1", ")", "for", "logit", "in", "logits_chunk", "]", "\n", "avg_softmax", "=", "sum", "(", "softmax", ")", "/", "args", ".", "num_noise_vec", "\n", "\n", "consistency", "=", "[", "kl_div", "(", "logit", ",", "avg_softmax", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", "1", ")", "\n", "+", "_entropy", "(", "avg_softmax", ",", "reduction", "=", "'none'", ")", "\n", "for", "logit", "in", "logits_chunk", "]", "\n", "consistency", "=", "sum", "(", "consistency", ")", "/", "args", ".", "num_noise_vec", "\n", "consistency", "=", "consistency", ".", "mean", "(", ")", "\n", "\n", "loss", "=", "loss_xent", "+", "args", ".", "lbd", "*", "consistency", "\n", "# loss = loss_xent", "\n", "\n", "avg_confidence", "=", "-", "F", ".", "nll_loss", "(", "avg_softmax", ",", "targets", ")", "\n", "\n", "max_predict", "=", "logits", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "# print(targets_c)", "\n", "correct", "=", "max_predict", ".", "eq", "(", "targets_c", ".", "data", ")", ".", "float", "(", ")", ".", "mean", "(", "1", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", "\n", "train_correct", "+=", "correct", ".", "item", "(", ")", "\n", "train_amount", "+=", "points", ".", "size", "(", ")", "[", "0", "]", "\n", "# acc1, acc5 = accuracy(logits, targets_c, topk=(1, 5))", "\n", "losses", ".", "update", "(", "loss_xent", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "losses_reg", ".", "update", "(", "consistency", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "confidence", ".", "update", "(", "avg_confidence", ".", "item", "(", ")", ",", "batch_size", ")", "\n", "# top1.update(acc1.item(), batch_size)", "\n", "# top5.update(acc5.item(), batch_size)", "\n", "\n", "# compute gradient and do SGD step", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# measure elapsed time", "\n", "", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "i", "%", "args", ".", "print_freq", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: [{0}][{1}/{2}]\\t'", "\n", "'Time {batch_time.avg:.3f}\\t'", "\n", "'Data {data_time.avg:.3f}\\t'", "\n", "'Loss {loss.avg:.4f}\\t'", "\n", "'Accuracy {acc:.3f}%\\t'", ".", "format", "(", "\n", "epoch", ",", "i", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "data_time", "=", "data_time", ",", "loss", "=", "losses", ",", "acc", "=", "100", "*", "train_correct", "/", "train_amount", ")", ")", "\n", "\n", "if", "args", ".", "print_step", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/loss/train'", ",", "losses", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/loss/consistency'", ",", "losses_reg", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/loss/avg_confidence'", ",", "confidence", ".", "avg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "f'epoch/{epoch}/batch_time'", ",", "batch_time", ".", "avg", ",", "i", ")", "\n", "# writer.add_scalar(f'epoch/{epoch}/accuracy/train@1', top1.avg, i)", "\n", "# writer.add_scalar(f'epoch/{epoch}/accuracy/train@5', top5.avg, i)", "\n", "\n", "", "", "", "writer", ".", "add_scalar", "(", "'loss/train'", ",", "losses", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'loss/consistency'", ",", "losses_reg", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'loss/avg_confidence'", ",", "confidence", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'batch_time'", ",", "batch_time", ".", "avg", ",", "epoch", ")", "\n", "# writer.add_scalar('accuracy/train@1', top1.avg, epoch)", "\n", "# writer.add_scalar('accuracy/train@5', top5.avg, epoch)", "\n", "\n", "return", "(", "losses", ".", "avg", ",", "train_correct", "/", "train_amount", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.train_segmentation.test": [[272, 330], ["train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "train_utils.AverageMeter", "time.time", "model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "train_utils.AverageMeter.update", "torch.squeeze().to", "torch.squeeze().to", "torch.squeeze().to", "points.float().to.float().to", "transformer.process().to", "model", "criterion", "max_predict.eq().float().mean().cpu().sum", "max_predict.eq().float().mean().cpu().sum.item", "train_utils.AverageMeter.update", "train_utils.AverageMeter.update", "time.time", "writer.add_scalar", "model.data.max", "points.float().to.size", "criterion.item", "transformer.process().to.size", "print", "time.time", "torch.squeeze", "torch.squeeze", "torch.squeeze", "points.float().to.float", "transformer.process", "max_predict.eq().float().mean().cpu", "time.time", "len", "max_predict.eq().float().mean", "max_predict.eq().float", "max_predict.eq"], "function", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.None.train_utils.AverageMeter.update", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.semantic.transformers.PointCloudLinear.process"], ["", "def", "test", "(", "loader", ",", "model", ",", "criterion", ",", "epoch", ",", "transformer", ":", "AbstractTransformer", ",", "writer", "=", "None", ",", "print_freq", "=", "25", ")", ":", "\n", "    ", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "top5", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "test_correct", "=", "0.0", "\n", "test_amount", "=", "0.0", "\n", "# switch to eval mode", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "data", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "            ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "points", ",", "targets", "=", "data", "\n", "targets", "=", "torch", ".", "squeeze", "(", "targets", ")", ".", "to", "(", "device", ")", "\n", "points", "=", "points", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "# inputs = inputs", "\n", "# targets = targets.to(device)()", "\n", "\n", "# augment inputs with noise", "\n", "inputs", "=", "transformer", ".", "process", "(", "points", ")", ".", "to", "(", "device", ")", "\n", "\n", "# compute output", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "targets", ")", "\n", "\n", "# measure accuracy and record loss", "\n", "max_predict", "=", "outputs", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "correct", "=", "max_predict", ".", "eq", "(", "targets", ".", "data", ")", ".", "float", "(", ")", ".", "mean", "(", "1", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", "\n", "test_correct", "+=", "correct", ".", "item", "(", ")", "\n", "test_amount", "+=", "points", ".", "size", "(", ")", "[", "0", "]", "\n", "# acc1, acc5 = accuracy(outputs, targets, topk=(1, 5))", "\n", "losses", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "inputs", ".", "size", "(", "0", ")", ")", "\n", "# top1.update(acc1.item(), inputs.size(0))", "\n", "# top5.update(acc5.item(), inputs.size(0))", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "print_freq", "==", "0", ":", "\n", "                ", "print", "(", "'Test: [{0}/{1}]\\t'", "\n", "'Time {batch_time.avg:.3f}\\t'", "\n", "'Data {data_time.avg:.3f}\\t'", "\n", "'Loss {loss.avg:.4f}\\t'", "\n", "'Accuracy {acc:.3f}%\\t'", ".", "format", "(", "\n", "i", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "data_time", "=", "data_time", ",", "\n", "loss", "=", "losses", ",", "acc", "=", "100", "*", "test_correct", "/", "test_amount", ")", ")", "\n", "\n", "", "", "if", "writer", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'loss/test'", ",", "losses", ".", "avg", ",", "epoch", ")", "\n", "# writer.add_scalar('accuracy/test@1', top1.avg, epoch)", "\n", "# writer.add_scalar('accuracy/test@5', top5.avg, epoch)", "\n", "\n", "", "return", "(", "losses", ".", "avg", ",", "test_correct", "/", "test_amount", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.TNet.__init__": [[7, 36], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "number_points", ",", "num_features", ")", ":", "\n", "        ", "super", "(", "TNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_features", "=", "num_features", "\n", "\n", "self", ".", "mlp1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "num_features", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "64", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "128", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "128", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "128", ",", "out_channels", "=", "1024", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "pooling", "=", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "number_points", ")", "\n", "\n", "self", ".", "mlp2", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "in_features", "=", "1024", ",", "out_features", "=", "512", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "in_features", "=", "512", ",", "out_features", "=", "256", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "256", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "regress", "=", "nn", ".", "Linear", "(", "in_features", "=", "256", ",", "out_features", "=", "num_features", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.TNet.forward": [[37, 51], ["pointnet.TNet.mlp1", "pointnet.TNet.pooling", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "pointnet.TNet.mlp2", "pointnet.TNet.regress", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "identity.expand.expand.expand", "x.view.view.view", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "x", ".", "size", "(", ")", "[", "0", "]", "\n", "\n", "x", "=", "self", ".", "mlp1", "(", "x", ")", "\n", "x", "=", "self", ".", "pooling", "(", "x", ")", "\n", "x", "=", "torch", ".", "squeeze", "(", "x", ")", "\n", "x", "=", "self", ".", "mlp2", "(", "x", ")", "\n", "x", "=", "self", ".", "regress", "(", "x", ")", "\n", "\n", "identity", "=", "torch", ".", "eye", "(", "n", "=", "self", ".", "num_features", ",", "device", "=", "x", ".", "device", ")", "\n", "identity", "=", "identity", ".", "expand", "(", "batch_size", ",", "self", ".", "num_features", ",", "self", ".", "num_features", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "num_features", ",", "self", ".", "num_features", ")", "\n", "x", "=", "identity", "+", "x", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNet.__init__": [[55, 127], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "pooling.append", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "pooling.append", "torch.MaxPool1d", "torch.MaxPool1d", "pooling.append", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "pooling.append", "torch.AvgPool1d", "torch.AvgPool1d"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "number_points", ":", "int", ",", "\n", "num_classes", ":", "int", ",", "\n", "max_features", ":", "int", "=", "1024", ",", "\n", "pool_function", ":", "str", "=", "'max'", ",", "\n", "disable_assertions", ":", "bool", "=", "False", ",", "\n", "transposed_input", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", "PointNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "number_points", "%", "8", "==", "0", ",", "f\"Number of points must be divisible by 8: {number_points}\"", "\n", "self", ".", "number_points", "=", "number_points", "\n", "self", ".", "max_features", "=", "max_features", "\n", "self", ".", "disable_assertions", "=", "disable_assertions", "\n", "self", ".", "transposed_input", "=", "transposed_input", "\n", "\n", "# input-dimension: (batch_size, features (coordinates), number_points)", "\n", "# First MLP with weight sharing, implemented as 1d convolution", "\n", "self", ".", "mlp1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "3", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "64", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "64", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "# Second MLP with weight sharing, implemented as 1d convolution", "\n", "# dimension: (batch_size, 64, number_points)", "\n", "self", ".", "mlp2", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "64", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "128", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "128", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "128", ",", "out_channels", "=", "self", ".", "max_features", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "self", ".", "max_features", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "# dimension: (batch_size, 1024, number_points)", "\n", "\n", "# global pooling of features across points", "\n", "pooling", "=", "[", "]", "\n", "if", "pool_function", "==", "'improved_max'", ":", "\n", "            ", "remaining_dim", "=", "self", ".", "number_points", "\n", "while", "remaining_dim", ">", "8", ":", "\n", "                ", "assert", "remaining_dim", "%", "8", "==", "0", ",", "\"number_points must be recursively divisible by 8\"", "\n", "pooling", ".", "append", "(", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "8", ",", "stride", "=", "8", ")", ")", "\n", "remaining_dim", "=", "remaining_dim", "//", "8", "\n", "", "pooling", ".", "append", "(", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "remaining_dim", ",", "stride", "=", "remaining_dim", ")", ")", "\n", "", "elif", "pool_function", "==", "'max'", ":", "\n", "            ", "pooling", ".", "append", "(", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "number_points", ")", ")", "\n", "", "elif", "pool_function", "==", "'avg'", ":", "\n", "            ", "pooling", ".", "append", "(", "nn", ".", "AvgPool1d", "(", "kernel_size", "=", "number_points", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"Invalid pooling operation {pool_function}\"", "\n", "", "self", ".", "pooling", "=", "nn", ".", "Sequential", "(", "*", "pooling", ")", "\n", "# print(self.pooling)", "\n", "# dimension: (batch_size, 1024, 1)", "\n", "\n", "# global fully connected layers", "\n", "# dimension: (batch_size, 1024)", "\n", "self", ".", "mlp3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "in_features", "=", "self", ".", "max_features", ",", "out_features", "=", "512", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "in_features", "=", "512", ",", "out_features", "=", "256", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "0.3", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "256", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "in_features", "=", "256", ",", "out_features", "=", "num_classes", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNet.forward": [[130, 144], ["pointnet.PointNet.mlp1", "pointnet.PointNet.mlp2", "pointnet.PointNet.pooling", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "pointnet.PointNet.mlp3", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose.dim", "torch.transpose.dim", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size", "torch.transpose.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "not", "self", ".", "disable_assertions", "and", "not", "self", ".", "transposed_input", ":", "\n", "            ", "assert", "x", ".", "dim", "(", ")", "==", "3", ",", "f\"Expect input with 3 dimensions: (batch x num_points x features), got {x.size()}.\"", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "self", ".", "number_points", ",", "f\"Expect input of size (N x num_points x features), got {x.size()}.\"", "\n", "assert", "x", ".", "size", "(", "2", ")", "==", "3", ",", "f\"Expect input of size (N x num_points x features), got {x.size()}.\"", "\n", "# input is in (batch x num_points x features), but we need (batch x features x num_points) for conv layers", "\n", "", "if", "not", "self", ".", "transposed_input", ":", "\n", "            ", "x", "=", "torch", ".", "transpose", "(", "x", ",", "2", ",", "1", ")", "\n", "", "x", "=", "self", ".", "mlp1", "(", "x", ")", "\n", "x", "=", "self", ".", "mlp2", "(", "x", ")", "\n", "x", "=", "self", ".", "pooling", "(", "x", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "shape", "=", "(", "-", "1", ",", "self", ".", "max_features", ")", ")", "\n", "x", "=", "self", ".", "mlp3", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.MiniPointNet.__init__": [[148, 189], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "pointnet.MiniPointNet.layers.append", "pointnet.MiniPointNet.layers.append", "pointnet.MiniPointNet.layers.append", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Sequential", "torch.Sequential", "torch.AvgPool1d", "torch.AvgPool1d"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "number_points", ",", "num_classes", ",", "pool_function", "=", "'max'", ")", ":", "\n", "        ", "super", "(", "MiniPointNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "number_points", "%", "8", "==", "0", ",", "f\"Number of points must be divisible by 8: {number_points}\"", "\n", "self", ".", "number_points", "=", "number_points", "\n", "\n", "# input-dimension: (batch_size, features (coordinates), number_points)", "\n", "\n", "# First MLP with weight sharing, implemented as 1d convolution", "\n", "self", ".", "mlp1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "3", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "256", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "# max-pooling across samples", "\n", "if", "pool_function", "==", "'max'", ":", "\n", "            ", "self", ".", "pooling", "=", "nn", ".", "Sequential", "(", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "self", ".", "number_points", ",", "stride", "=", "self", ".", "number_points", ")", ")", "\n", "", "elif", "pool_function", "==", "'avg'", ":", "\n", "            ", "self", ".", "pooling", "=", "nn", ".", "Sequential", "(", "nn", ".", "AvgPool1d", "(", "kernel_size", "=", "self", ".", "number_points", ",", "stride", "=", "self", ".", "number_points", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"Invalid pooling function {pool_function}\"", "\n", "# dimension: (batch_size, 1024, 1)", "\n", "\n", "# global fully connected layers", "\n", "# dimension: (batch_size, 1024)", "\n", "", "self", ".", "mlp3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "in_features", "=", "256", ",", "out_features", "=", "128", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "0.3", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "in_features", "=", "128", ",", "out_features", "=", "num_classes", ")", ",", "\n", ")", "\n", "# dimension: (batch_size, num_classes)", "\n", "\n", "self", ".", "layers", "=", "[", "]", "\n", "for", "l", "in", "self", ".", "mlp1", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "l", ")", "\n", "", "for", "l", "in", "self", ".", "pooling", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "l", ")", "\n", "", "for", "l", "in", "self", ".", "mlp3", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.MiniPointNet.forward": [[190, 196], ["pointnet.MiniPointNet.mlp1", "pointnet.MiniPointNet.pooling", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "pointnet.MiniPointNet.mlp3"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "mlp1", "(", "x", ")", "\n", "x", "=", "self", ".", "pooling", "(", "x", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "shape", "=", "(", "-", "1", ",", "256", ")", ")", "\n", "x", "=", "self", ".", "mlp3", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__": [[208, 245], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "pooling.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "pointnet.mlp_block", "pointnet.mlp_block", "pointnet.mlp_block", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "pooling.append", "torch.MaxPool1d", "torch.MaxPool1d", "pointnet.mlp_block", "pointnet.mlp_block", "torch.Conv1d", "torch.Conv1d", "torch.MaxPool1d", "torch.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.__init__", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.mlp_block", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.mlp_block", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.mlp_block", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.mlp_block", "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.mlp_block"], ["    ", "def", "__init__", "(", "self", ",", "number_points", ":", "int", ",", "num_seg_classes", ":", "int", ",", "encode_onnx", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "PointNetSegmentation", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "number_points", "%", "8", "==", "0", ",", "f\"Number of points must be divisible by 8: {number_points}\"", "\n", "self", ".", "number_points", "=", "number_points", "\n", "self", ".", "num_seg_classes", "=", "num_seg_classes", "\n", "self", ".", "encode_onnx", "=", "encode_onnx", "\n", "\n", "# input-dimension: (batch_size, features (coordinates), number_points)", "\n", "\n", "# First MLP with weight sharing, implemented as 1d convolution", "\n", "self", ".", "point_features", "=", "nn", ".", "Sequential", "(", "\n", "mlp_block", "(", "in_channels", "=", "3", ",", "out_channels", "=", "64", ")", ",", "\n", "mlp_block", "(", "in_channels", "=", "64", ",", "out_channels", "=", "128", ")", ",", "\n", "#        mlp_block(in_channels=128, out_channels=128),", "\n", "mlp_block", "(", "in_channels", "=", "128", ",", "out_channels", "=", "256", ")", "\n", ")", "\n", "\n", "self", ".", "global_features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "128", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "128", ")", "\n", ")", "\n", "\n", "# max-pooling across samples", "\n", "pooling", "=", "[", "]", "\n", "remaining_dim", "=", "self", ".", "number_points", "\n", "while", "remaining_dim", ">", "8", ":", "\n", "            ", "assert", "remaining_dim", "%", "8", "==", "0", ",", "\"number_points must be recursively divisible by 8\"", "\n", "pooling", ".", "append", "(", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "8", ",", "stride", "=", "8", ")", ")", "\n", "remaining_dim", "=", "remaining_dim", "//", "8", "\n", "", "pooling", ".", "append", "(", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "remaining_dim", ",", "stride", "=", "remaining_dim", ")", ")", "\n", "self", ".", "pooling", "=", "nn", ".", "Sequential", "(", "*", "pooling", ")", "\n", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "mlp_block", "(", "576", ",", "256", ")", ",", "\n", "#        mlp_block(256, 256),", "\n", "mlp_block", "(", "256", ",", "128", ")", ",", "\n", "nn", ".", "Conv1d", "(", "128", ",", "self", ".", "num_seg_classes", ",", "kernel_size", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.PointNetSegmentation.forward": [[247, 270], ["pointnet.PointNetSegmentation.global_features", "pointnet.PointNetSegmentation.pooling", "torch.unsqueeze.repeat", "torch.unsqueeze.repeat", "features.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pointnet.PointNetSegmentation.classifier", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "mlp", "features.append", "len", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size", "torch.unsqueeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# input is in (batch x num_points x features), but we need (batch x features x num_points) for conv layers", "\n", "        ", "if", "not", "self", ".", "encode_onnx", ":", "\n", "            ", "if", "len", "(", "x", ".", "size", "(", ")", ")", "==", "2", ":", "\n", "                ", "x", "=", "torch", ".", "unsqueeze", "(", "x", ",", "0", ")", "\n", "", "assert", "x", ".", "size", "(", "1", ")", "==", "self", ".", "number_points", ",", "f\"Expect input of size (N x num_points x features), got {x.size()}\"", "\n", "assert", "x", ".", "size", "(", "2", ")", "==", "3", ",", "f\"Expect input of size (N x num_points x features), got {x.size()}\"", "\n", "x", "=", "torch", ".", "transpose", "(", "x", ",", "2", ",", "1", ")", "\n", "\n", "", "features", "=", "[", "]", "\n", "for", "mlp", "in", "self", ".", "point_features", ":", "\n", "            ", "x", "=", "mlp", "(", "x", ")", "\n", "features", ".", "append", "(", "x", ")", "\n", "\n", "", "x", "=", "self", ".", "global_features", "(", "x", ")", "\n", "x", "=", "self", ".", "pooling", "(", "x", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "number_points", ")", "\n", "features", ".", "append", "(", "x", ")", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "features", ",", "dim", "=", "1", ")", "\n", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.qianhewu_point-cloud-smoothing.archs.pointnet.mlp_block": [[198, 203], ["torch.Sequential", "torch.Conv1d", "torch.BatchNorm1d", "torch.ReLU"], "function", ["None"], ["", "", "def", "mlp_block", "(", "in_channels", ":", "int", ",", "out_channels", ":", "int", ")", "->", "nn", ".", "Module", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "out_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n"]]}