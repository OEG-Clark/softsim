{"home.repos.pwc.inspect_result.robert-giaquinto_dapper.None.setup.Install.run": [[13, 15], ["setuptools.command.install.install.do_egg_install"], "methods", ["None"], ["    ", "def", "run", "(", "self", ")", ":", "\n", "        ", "_install", ".", "do_egg_install", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.None.setup.readfile": [[7, 10], ["os.path.join", "io.open().read", "os.path.dirname", "io.open"], "function", ["None"], ["def", "readfile", "(", "fname", ")", ":", "\n", "    ", "path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "fname", ")", "\n", "return", "io", ".", "open", "(", "path", ",", "encoding", "=", "'utf8'", ")", ".", "read", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.__init__": [[12, 26], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_topics", ",", "vocab_size", ",", "num_authors", ",", "num_personas", ",", "num_times", ")", ":", "\n", "        ", "self", ".", "num_topics", "=", "num_topics", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "num_authors", "=", "num_authors", "\n", "self", ".", "num_personas", "=", "num_personas", "\n", "self", ".", "num_times", "=", "num_times", "\n", "\n", "# initialize the sufficient statistics", "\n", "self", ".", "beta", "=", "np", ".", "zeros", "(", "(", "num_topics", ",", "vocab_size", ")", ")", "\n", "self", ".", "kappa", "=", "np", ".", "zeros", "(", "(", "num_authors", ",", "num_personas", ")", ")", "\n", "self", ".", "x", "=", "np", ".", "zeros", "(", "(", "num_times", ",", "num_personas", ")", ")", "\n", "self", ".", "x2", "=", "np", ".", "zeros", "(", "(", "num_times", ",", "num_personas", ")", ")", "\n", "self", ".", "alpha", "=", "np", ".", "zeros", "(", "(", "num_times", ",", "num_topics", ",", "num_personas", ")", ")", "\n", "self", ".", "batch_size", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.reset": [[27, 38], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        reset the sufficient statistics (done after each e step)\n        :return:\n        \"\"\"", "\n", "self", ".", "beta", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_topics", ",", "self", ".", "vocab_size", ")", ")", "\n", "self", ".", "kappa", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_authors", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "x", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "x2", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "alpha", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "batch_size", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.update": [[39, 54], ["numpy.exp", "range", "numpy.log"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "doc", ",", "doc_m", ",", "doc_tau", ",", "log_phi", ")", ":", "\n", "        ", "\"\"\"\n        update the ss given some document and variational parameters\n        :param doc:\n        :param vp:\n        :return:\n        \"\"\"", "\n", "t", "=", "doc", ".", "time_id", "\n", "self", ".", "batch_size", "+=", "1", "\n", "self", ".", "kappa", "[", "doc", ".", "author_id", ",", ":", "]", "+=", "doc_tau", "\n", "self", ".", "x", "[", "t", ",", ":", "]", "+=", "doc_tau", "\n", "self", ".", "x2", "[", "t", ",", ":", "]", "+=", "doc_tau", "**", "2", "\n", "self", ".", "beta", "[", ":", ",", "doc", ".", "words", "]", "+=", "np", ".", "exp", "(", "log_phi", "+", "np", ".", "log", "(", "doc", ".", "counts", ")", ")", "\n", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "            ", "self", ".", "alpha", "[", "t", ",", ":", ",", "p", "]", "+=", "doc_m", "*", "doc_tau", "[", "p", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.merge": [[55, 69], ["None"], "methods", ["None"], ["", "", "def", "merge", "(", "self", ",", "other", ")", ":", "\n", "        ", "\"\"\"\n        merge in sufficient statistics given a batch of other sufficient statistics\n        this is needed to run e-step in parallel\n        :param stats:\n        :return:\n        \"\"\"", "\n", "if", "other", "is", "not", "None", ":", "\n", "            ", "self", ".", "x", "+=", "other", ".", "x", "\n", "self", ".", "x2", "+=", "other", ".", "x2", "\n", "self", ".", "beta", "+=", "other", ".", "beta", "\n", "self", ".", "alpha", "+=", "other", ".", "alpha", "\n", "self", ".", "kappa", "+=", "other", ".", "kappa", "\n", "self", ".", "batch_size", "+=", "other", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.__str__": [[70, 73], ["None"], "methods", ["None"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "rval", "=", "\"SufficientStatistics derived from batch_size = {}\"", ".", "format", "(", "self", ".", "batch_size", ")", "\n", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.sample_batches": [[6, 20], ["numpy.arange", "numpy.random.shuffle", "int", "math.ceil", "arr.tolist", "numpy.array_split", "range"], "function", ["None"], ["def", "sample_batches", "(", "n", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"\n\n    :param n: total number of things to draw from (e.g. number of documents in corpus)\n    :param batch_size: how many documents should appear in each batch\n    :return: list of lists of batches.\n    \"\"\"", "\n", "if", "n", "==", "batch_size", ":", "\n", "        ", "return", "[", "[", "i", "for", "i", "in", "range", "(", "n", ")", "]", "]", "\n", "", "indices", "=", "np", ".", "arange", "(", "n", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "indices", ")", "\n", "num_splits", "=", "int", "(", "ceil", "(", "1.0", "*", "n", "/", "batch_size", ")", ")", "\n", "rval", "=", "[", "arr", ".", "tolist", "(", ")", "for", "arr", "in", "np", ".", "array_split", "(", "indices", ",", "num_splits", ")", "]", "\n", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.softmax": [[22, 31], ["numpy.exp", "numpy.sum", "numpy.max"], "function", ["None"], ["", "def", "softmax", "(", "x", ",", "axis", ")", ":", "\n", "    ", "\"\"\"\n    Softmax for normalizing a matrix along an axis\n    Use max substraction approach for numerical stability\n    :param x:\n    :return:\n    \"\"\"", "\n", "e_x", "=", "np", ".", "exp", "(", "x", "-", "np", ".", "max", "(", "x", ")", ")", "\n", "return", "e_x", "/", "np", ".", "sum", "(", "e_x", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.sum_normalize": [[33, 43], ["numpy.sum", "numpy.min"], "function", ["None"], ["", "def", "sum_normalize", "(", "mat", ",", "axis", ")", ":", "\n", "    ", "\"\"\"\n    normalize to a probability distribution, first convert any\n    negative numbers if they exist\n    :param mat:\n    :param axis:\n    :return:\n    \"\"\"", "\n", "pos_mat", "=", "mat", "-", "np", ".", "min", "(", "mat", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "+", "0.001", "\n", "return", "pos_mat", "/", "np", ".", "sum", "(", "pos_mat", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.dirichlet_expectation": [[45, 54], ["len", "scipy.special.psi", "scipy.special.psi", "scipy.special.psi", "scipy.special.psi", "numpy.sum", "numpy.sum"], "function", ["None"], ["", "def", "dirichlet_expectation", "(", "dirichlet_parameter", ")", ":", "\n", "    ", "\"\"\"\n    compute dirichlet expectation\n    :param dirichlet_parameter:\n    :return:\n    \"\"\"", "\n", "if", "len", "(", "dirichlet_parameter", ".", "shape", ")", "==", "1", ":", "\n", "        ", "return", "psi", "(", "dirichlet_parameter", ")", "-", "psi", "(", "np", ".", "sum", "(", "dirichlet_parameter", ")", ")", "\n", "", "return", "psi", "(", "dirichlet_parameter", ")", "-", "psi", "(", "np", ".", "sum", "(", "dirichlet_parameter", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.matrix2str": [[56, 72], ["str", "round", "len", "str", "zip", "numpy.abs", "s.format", "round", "round"], "function", ["None"], ["", "def", "matrix2str", "(", "mat", ",", "num_digits", "=", "2", ")", ":", "\n", "    ", "\"\"\"\n    take a matrix (either list of lists of numpy array) and put it in a\n    pretty printable format.\n    :param mat: matrix to print\n    :param num_digits: how many significant digits to show\n    :return:\n    \"\"\"", "\n", "rval", "=", "''", "\n", "for", "row", "in", "mat", ":", "\n", "        ", "s", "=", "'{:.'", "+", "str", "(", "num_digits", ")", "+", "'}'", "\n", "# rval += '\\t'.join([s.format(round(elt, num_digits)) for elt in row]) + '\\n'", "\n", "fpad", "=", "[", "''", "if", "round", "(", "elt", ",", "num_digits", ")", "<", "0", "else", "' '", "for", "elt", "in", "row", "]", "\n", "bpad", "=", "[", "' '", "*", "(", "7", "-", "len", "(", "str", "(", "np", ".", "abs", "(", "round", "(", "elt", ",", "num_digits", ")", ")", ")", ")", ")", "for", "elt", "in", "row", "]", "\n", "rval", "+=", "''", ".", "join", "(", "[", "f", "+", "s", ".", "format", "(", "round", "(", "elt", ",", "num_digits", ")", ")", "+", "b", "for", "elt", ",", "f", ",", "b", "in", "zip", "(", "row", ",", "fpad", ",", "bpad", ")", "]", ")", "+", "'\\n'", "\n", "", "return", "rval", "", "", ""]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.__init__": [[18, 54], ["corpus.Corpus.read_file", "corpus.Corpus.read_vocab", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logging.basicConfig", "len", "str", "str", "str", "str", "str", "str", "str", "str", "enumerate", "str"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.read_file", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.read_vocab"], ["def", "__init__", "(", "self", ",", "input_file", "=", "\"cb_small.txt\"", ",", "vocab_file", "=", "\"cb_small_vocab.txt\"", ",", "author2id", "=", "None", ",", "log", "=", "True", ")", ":", "\n", "        ", "if", "log", ":", "\n", "            ", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "\n", "", "self", ".", "docs", "=", "[", "]", "\n", "self", ".", "total_documents", "=", "0", "\n", "self", ".", "times", "=", "[", "]", "\n", "self", ".", "num_times", "=", "0", "\n", "self", ".", "vocab_size", "=", "0", "\n", "if", "author2id", "is", "None", ":", "\n", "            ", "self", ".", "author2id", "=", "{", "}", "\n", "self", ".", "num_authors", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "author2id", "=", "author2id", "\n", "self", ".", "num_authors", "=", "len", "(", "author2id", ")", "\n", "", "self", ".", "vocab", "=", "[", "]", "\n", "self", ".", "input_file", "=", "input_file", "\n", "self", ".", "num_docs_per_time", "=", "[", "]", "\n", "self", ".", "total_words", "=", "0", "\n", "\n", "# read and process the input file", "\n", "skipped_docs", ",", "skipped_times", "=", "self", ".", "read_file", "(", ")", "\n", "\n", "# read and process the vocabulary file", "\n", "self", ".", "read_vocab", "(", "vocab_file", ")", "\n", "\n", "logger", ".", "info", "(", "\"PROCESSED CORPUS\"", ")", "\n", "logger", ".", "info", "(", "\"Number of time points: \"", "+", "str", "(", "self", ".", "num_times", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of authors: \"", "+", "str", "(", "self", ".", "num_authors", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of documents: \"", "+", "str", "(", "self", ".", "total_documents", ")", ")", "\n", "logger", ".", "info", "(", "\"Total number of words: \"", "+", "str", "(", "self", ".", "total_words", ")", ")", "\n", "logger", ".", "info", "(", "\"Found ids for \"", "+", "str", "(", "self", ".", "vocab_size", ")", "+", "\" terms in vocabulary\"", ")", "\n", "logger", ".", "info", "(", "\"Number of documents skipped (no words): \"", "+", "str", "(", "skipped_docs", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of times skipped (no documents): \"", "+", "str", "(", "skipped_times", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of documents per time-step: {}\"", ".", "format", "(", "\n", "\", \"", ".", "join", "(", "[", "str", "(", "i", ")", "+", "\":\"", "+", "str", "(", "n", ")", "for", "i", ",", "n", "in", "enumerate", "(", "self", ".", "num_docs_per_time", ")", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.read_file": [[55, 133], ["len", "logger.info", "logger.info", "open", "int", "range", "f.readline().replace", "f.readline().replace", "int", "int", "corpus.Corpus.times.append", "range", "float", "f.readline().replace", "src.doc.Doc", "f.readline().replace().split", "int", "numpy.array", "numpy.array", "corpus.Corpus.docs.append", "numpy.sum", "f.readline", "f.readline", "len", "max", "f.readline", "f.readline().replace", "int", "max", "wc.split", "f.readline"], "methods", ["None"], ["", "def", "read_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Main processing method for reading and parsing information in the file\n        \"\"\"", "\n", "skipped_docs", "=", "0", "\n", "skipped_times", "=", "0", "\n", "doc_id", "=", "0", "\n", "for_training", "=", "len", "(", "self", ".", "author2id", ")", "==", "0", "# don't include new authors in a test set", "\n", "if", "for_training", ":", "\n", "            ", "logger", ".", "info", "(", "\"Building training corpus.\"", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"Building testing corpus. Will skip any new authors.\"", ")", "\n", "\n", "", "with", "open", "(", "self", ".", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "self", ".", "num_times", "=", "int", "(", "f", ".", "readline", "(", ")", ".", "replace", "(", "'\\n'", ",", "''", ")", ")", "\n", "self", ".", "num_docs_per_time", "=", "[", "0", "]", "*", "self", ".", "num_times", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", ")", ":", "\n", "# catch newlines at the end of the file", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "if", "line", "==", "''", ":", "\n", "                    ", "break", "\n", "\n", "", "time_stamp", "=", "int", "(", "float", "(", "line", ")", ")", "\n", "num_docs", "=", "int", "(", "f", ".", "readline", "(", ")", ".", "replace", "(", "'\\n'", ",", "''", ")", ")", "\n", "if", "num_docs", "==", "0", ":", "\n", "                    ", "skipped_times", "+=", "1", "\n", "continue", "\n", "\n", "", "self", ".", "times", ".", "append", "(", "time_stamp", ")", "\n", "self", ".", "total_documents", "+=", "num_docs", "\n", "\n", "for", "d", "in", "range", "(", "num_docs", ")", ":", "\n", "                    ", "doc", "=", "Doc", "(", ")", "\n", "doc", ".", "time", "=", "time_stamp", "\n", "doc", ".", "time_id", "=", "t", "\n", "\n", "# read one line = one document", "\n", "fields", "=", "f", ".", "readline", "(", ")", ".", "replace", "(", "'\\n'", ",", "''", ")", ".", "split", "(", ")", "\n", "\n", "# extract author", "\n", "doc", ".", "author", "=", "fields", "[", "0", "]", "\n", "\n", "# convert author to a unique author id", "\n", "if", "doc", ".", "author", "not", "in", "self", ".", "author2id", ":", "\n", "                        ", "if", "for_training", ":", "\n", "                            ", "self", ".", "author2id", "[", "doc", ".", "author", "]", "=", "self", ".", "num_authors", "\n", "self", ".", "num_authors", "+=", "1", "\n", "", "else", ":", "\n", "# skip this doc, don't want to add new authors to a test set", "\n", "                            ", "skipped_docs", "+=", "1", "\n", "continue", "\n", "\n", "# save author id by looking up author name", "\n", "", "", "doc", ".", "author_id", "=", "self", ".", "author2id", "[", "doc", ".", "author", "]", "\n", "\n", "# length of document (i.e. number of unique terms", "\n", "doc", ".", "num_terms", "=", "int", "(", "fields", "[", "1", "]", ")", "\n", "\n", "# extract words and corresponding counts in this document", "\n", "word_counts", "=", "[", "[", "int", "(", "elt", ")", "for", "elt", "in", "wc", ".", "split", "(", "\":\"", ")", "]", "for", "wc", "in", "fields", "[", "2", ":", "]", "]", "\n", "if", "len", "(", "word_counts", ")", "==", "0", ":", "\n", "                        ", "self", ".", "total_documents", "-=", "1", "\n", "skipped_docs", "+=", "1", "\n", "continue", "\n", "\n", "", "doc", ".", "doc_id", "=", "doc_id", "\n", "doc_id", "+=", "1", "\n", "self", ".", "num_docs_per_time", "[", "t", "]", "+=", "1", "\n", "\n", "doc", ".", "words", "=", "np", ".", "array", "(", "[", "w", "for", "w", ",", "c", "in", "word_counts", "]", ")", "\n", "doc", ".", "counts", "=", "np", ".", "array", "(", "[", "c", "for", "w", ",", "c", "in", "word_counts", "]", ")", "\n", "self", ".", "docs", ".", "append", "(", "doc", ")", "\n", "self", ".", "total_words", "+=", "np", ".", "sum", "(", "doc", ".", "counts", ")", "\n", "\n", "if", "max", "(", "doc", ".", "words", ")", ">", "self", ".", "vocab_size", ":", "\n", "                        ", "self", ".", "vocab_size", "=", "max", "(", "doc", ".", "words", ")", "+", "1", "\n", "\n", "", "", "", "", "return", "skipped_docs", ",", "skipped_times", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.read_vocab": [[134, 139], ["open", "tuple", "len", "logger.info", "v.replace", "str", "f.readlines", "len"], "methods", ["None"], ["", "def", "read_vocab", "(", "self", ",", "vocab_file", ")", ":", "\n", "        ", "with", "open", "(", "vocab_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "self", ".", "vocab", "=", "tuple", "(", "[", "v", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", "for", "v", "in", "f", ".", "readlines", "(", ")", "]", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "logger", ".", "info", "(", "\"Number of words in vocabulary: \"", "+", "str", "(", "len", "(", "self", ".", "vocab", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.__iter__": [[140, 147], ["None"], "methods", ["None"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Iterator returning each doc\n        :return:\n        \"\"\"", "\n", "for", "doc", "in", "self", ".", "docs", ":", "\n", "            ", "yield", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.__len__": [[148, 150], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "docs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.corpus.Corpus.__str__": [[151, 153], ["str", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"Corpus with \"", "+", "str", "(", "self", ".", "num_times", ")", "+", "\" time periods, and \"", "+", "str", "(", "self", ".", "total_documents", ")", "+", "\" total documents.\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.__init__": [[23, 67], ["dapper.DAPPER._check_params"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER._check_params"], ["def", "__init__", "(", "self", ",", "num_topics", ",", "num_personas", ",", "\n", "process_noise", "=", "0.1", ",", "measurement_noise", "=", "0.8", ",", "regularization", "=", "0.2", ",", "\n", "max_epochs", "=", "5", ",", "max_training_minutes", "=", "0", ",", "\n", "local_convergence", "=", "1e-03", ",", "step_size", "=", "0.7", ",", "queue_size", "=", "1", ",", "\n", "max_local_iters", "=", "30", ",", "batch_size", "=", "-", "1", ",", "learning_offset", "=", "10", ",", "learning_decay", "=", "0.7", ",", "\n", "num_workers", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        :param num_topics: number of topics to find\n        :param num_personas: number of personas (latent author clusters)\n        :param process_noise: 1st parameter controlling smoothness of topics, between 0.1 and 0.3 ideal.\n        :param measurement_noise: 2nd parameter controlling smoothness of topics, leave fixed at 0.8\n        :param regularization: penalty for similar personas, good results typically with 0.1 to 0.3\n        :param max_epochs: maximum passes over the dataset for training\n        :param max_training_minutes: maximum number of minutes of training\n        :param local_convergence: local variational convergence criteria\n        :param step_size: step size of CVI updates\n        :param queue_size: leave at 1, smoothing gradients doesn't seem to help\n        :param max_local_iters: max number of iteretions to run for location variables (assuming no convergence)\n        :param batch_size: mini-batch size (number of documents)\n        :param learning_offset: SVI parameter (see online LDA paper)\n        :param learning_decay: SVI parameter (see online LDA paper)\n        :param num_workers: set to >1 for parallel implementation\n        \"\"\"", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "learning_offset", "=", "learning_offset", "\n", "self", ".", "learning_decay", "=", "learning_decay", "\n", "self", ".", "step_size", "=", "step_size", "\n", "\n", "self", ".", "process_noise", "=", "process_noise", "\n", "self", ".", "measurement_noise", "=", "measurement_noise", "\n", "self", ".", "regularization", "=", "regularization", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "\n", "self", ".", "num_topics", "=", "num_topics", "\n", "self", ".", "num_personas", "=", "num_personas", "\n", "\n", "self", ".", "current_em_iter", "=", "0", "\n", "self", ".", "total_epochs", "=", "0", "\n", "self", ".", "max_epochs", "=", "max_epochs", "if", "max_epochs", "is", "not", "None", "else", "0", "\n", "self", ".", "max_training_minutes", "=", "max_training_minutes", "if", "max_training_minutes", "is", "not", "None", "else", "0", "\n", "self", ".", "local_convergence", "=", "local_convergence", "\n", "self", ".", "max_local_iters", "=", "max_local_iters", "\n", "self", ".", "queue_size", "=", "queue_size", "\n", "self", ".", "_check_params", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER._check_params": [[68, 95], ["ValueError", "ValueError", "multiprocessing.cpu_count", "logger.info", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "multiprocessing.cpu_count", "multiprocessing.cpu_count"], "methods", ["None"], ["", "def", "_check_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Check that given parameters to the model are legitimate.\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "num_personas", "is", "None", "or", "self", ".", "num_personas", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"number of personas >= 0\"", ")", "\n", "", "if", "self", ".", "num_topics", "is", "None", "or", "self", ".", "num_topics", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"number of topics > 0\"", ")", "\n", "", "if", "self", ".", "num_workers", ">", "cpu_count", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Cannot have more workers than available CPUs, setting number workers to {}\"", ".", "format", "(", "cpu_count", "(", ")", "-", "1", ")", ")", "\n", "self", ".", "num_workers", "=", "cpu_count", "(", ")", "-", "1", "\n", "", "if", "self", ".", "learning_decay", "<", "0.5", "or", "self", ".", "learning_decay", ">", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"learning decay must by in [0.5, 1.0] to ensure convergence.\"", ")", "\n", "", "if", "self", ".", "step_size", "<", "0", "or", "self", ".", "step_size", ">", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"step size must be in [0, 1].\"", ")", "\n", "", "if", "self", ".", "process_noise", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"process noise must be positive (recommended to be between 0.5 to 0.1 times measurement_noise).\"", ")", "\n", "", "if", "self", ".", "measurement_noise", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"process noise must be positive (recommended to be between 2 to 10 times process_noise).\"", ")", "\n", "", "if", "self", ".", "regularization", "<", "0.0", "or", "self", ".", "regularization", ">", "0.5", ":", "\n", "            ", "raise", "ValueError", "(", "\"regularization parameter is recommended to be between [0, 0.5]\"", ")", "\n", "", "if", "self", ".", "max_training_minutes", "<=", "0", "and", "self", ".", "max_epochs", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Both max training minutes and max epochs cannot be None.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER._check_corpora": [[96, 116], ["ValueError", "ValueError", "train_corpus.author2id.items", "test_corpus.author2id.items", "len", "len", "ValueError", "ValueError"], "methods", ["None"], ["", "", "def", "_check_corpora", "(", "self", ",", "train_corpus", ",", "test_corpus", ")", ":", "\n", "        ", "\"\"\"\n        Check that corpus being tested on has the necessary same meta data as the\n        training corpus\n        :param train_corpus:\n        :param test_corpus:\n        :return:\n        \"\"\"", "\n", "if", "train_corpus", ".", "vocab", "!=", "test_corpus", ".", "vocab", ":", "\n", "            ", "raise", "ValueError", "(", "\"vocabularies for training and test sets must be equal\"", ")", "\n", "", "if", "train_corpus", ".", "num_authors", "!=", "test_corpus", ".", "num_authors", ":", "\n", "            ", "raise", "ValueError", "(", "\"Training and test sets must have the same number of authors\"", ")", "\n", "\n", "", "common_authors", "=", "train_corpus", ".", "author2id", ".", "items", "(", ")", "&", "test_corpus", ".", "author2id", ".", "items", "(", ")", "\n", "if", "len", "(", "common_authors", ")", "!=", "len", "(", "train_corpus", ".", "author2id", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Training and test sets must have the same authors (and author2id lookup tables)\"", ")", "\n", "\n", "# TODO should setup test corpus capable of having *fewer* time steps than training set", "\n", "", "if", "train_corpus", ".", "num_times", "!=", "test_corpus", ".", "num_times", ":", "\n", "            ", "raise", "ValueError", "(", "\"Training and test sets must have the same number of time steps\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_from_corpus": [[117, 135], ["numpy.array", "corpus.author2id.items"], "methods", ["None"], ["", "", "def", "init_from_corpus", "(", "self", ",", "corpus", ")", ":", "\n", "        ", "\"\"\"\n        initialize model based on the corpus that will be fit\n        :param corpus:\n        :return:\n        \"\"\"", "\n", "# initializations from corpus", "\n", "self", ".", "vocab_size", "=", "corpus", ".", "vocab_size", "\n", "self", ".", "num_times", "=", "corpus", ".", "num_times", "\n", "self", ".", "times", "=", "corpus", ".", "times", "\n", "\n", "self", ".", "total_documents", "=", "corpus", ".", "total_documents", "\n", "self", ".", "total_words", "=", "corpus", ".", "total_words", "\n", "self", ".", "num_authors", "=", "corpus", ".", "num_authors", "\n", "self", ".", "vocab", "=", "np", ".", "array", "(", "corpus", ".", "vocab", ")", "\n", "self", ".", "id2author", "=", "{", "y", ":", "x", "for", "x", ",", "y", "in", "corpus", ".", "author2id", ".", "items", "(", ")", "}", "\n", "if", "self", ".", "num_personas", "==", "0", ":", "\n", "            ", "self", ".", "num_personas", "=", "corpus", ".", "num_authors", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.parameter_init": [[136, 177], ["numpy.random.gamma", "src.utilities.dirichlet_expectation", "numpy.random.gamma", "src.utilities.dirichlet_expectation", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "range", "numpy.copy", "src.sufficient_statistics.SufficientStatistics", "dapper.DAPPER.ss.reset", "collections.deque", "numpy.ones", "numpy.ones", "numpy.random.uniform", "numpy.linalg.inv", "src.utilities.softmax", "numpy.eye", "numpy.eye"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.dirichlet_expectation", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.dirichlet_expectation", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.reset", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.softmax"], ["", "", "def", "parameter_init", "(", "self", ")", ":", "\n", "# initialize matrices", "\n", "        ", "self", ".", "omega", "=", "np", ".", "ones", "(", "self", ".", "num_personas", ")", "*", "(", "1.0", "/", "self", ".", "num_personas", ")", "\n", "self", ".", "_delta", "=", "np", ".", "random", ".", "gamma", "(", "500.", ",", "0.1", ",", "(", "self", ".", "num_authors", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "E_log_kappa", "=", "dirichlet_expectation", "(", "self", ".", "_delta", ")", "\n", "\n", "self", ".", "eta", "=", "1.0", "/", "self", ".", "num_topics", "\n", "self", ".", "_lambda", "=", "np", ".", "random", ".", "gamma", "(", "500.", ",", "0.1", ",", "(", "self", ".", "num_topics", ",", "self", ".", "vocab_size", ")", ")", "\n", "self", ".", "E_log_beta", "=", "dirichlet_expectation", "(", "self", ".", "_lambda", ")", "\n", "self", ".", "log_beta", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_topics", ",", "self", ".", "vocab_size", ")", ")", "\n", "\n", "self", ".", "mu0", "=", "np", ".", "ones", "(", "self", ".", "num_topics", ")", "*", "(", "1.0", "/", "self", ".", "num_topics", ")", "\n", "self", ".", "alpha", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "sigma", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ",", "self", ".", "num_topics", ")", ")", "\n", "self", ".", "sigma_inv", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ",", "self", ".", "num_topics", ")", ")", "\n", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", ")", ":", "\n", "            ", "alpha_init", "=", "np", ".", "random", ".", "uniform", "(", "0.01", ",", "5.0", ",", "(", "self", ".", "num_topics", ",", "self", ".", "num_personas", ")", ")", "\n", "self", ".", "alpha", "[", "t", ",", ":", ",", ":", "]", "=", "alpha_init", "\n", "\n", "if", "t", "==", "0", ":", "\n", "                ", "self", ".", "sigma", "[", "t", ",", ":", ",", ":", "]", "=", "np", ".", "eye", "(", "self", ".", "num_topics", ")", "*", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "1", "]", "-", "self", ".", "times", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "sigma", "[", "t", ",", ":", ",", ":", "]", "=", "np", ".", "eye", "(", "self", ".", "num_topics", ")", "*", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "t", "]", "-", "self", ".", "times", "[", "t", "-", "1", "]", ")", "\n", "\n", "", "self", ".", "sigma_inv", "[", "t", ",", ":", ",", ":", "]", "=", "np", ".", "linalg", ".", "inv", "(", "self", ".", "sigma", "[", "t", ",", ":", ",", ":", "]", ")", "\n", "\n", "# normalize alpha", "\n", "", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "            ", "self", ".", "alpha", "[", ":", ",", ":", ",", "p", "]", "=", "softmax", "(", "self", ".", "alpha", "[", ":", ",", ":", ",", "p", "]", ",", "axis", "=", "1", ")", "\n", "\n", "", "self", ".", "alpha_hat", "=", "np", ".", "copy", "(", "self", ".", "alpha", ")", "\n", "\n", "self", ".", "ss", "=", "SufficientStatistics", "(", "num_topics", "=", "self", ".", "num_topics", ",", "\n", "vocab_size", "=", "self", ".", "vocab_size", ",", "\n", "num_authors", "=", "self", ".", "num_authors", ",", "\n", "num_personas", "=", "self", ".", "num_personas", ",", "\n", "num_times", "=", "self", ".", "num_times", ")", "\n", "self", ".", "ss", ".", "reset", "(", ")", "\n", "self", ".", "ss_queue", "=", "deque", "(", "maxlen", "=", "self", ".", "queue_size", ")", "\n", "self", ".", "beta_ss", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.cvi_gamma_update": [[178, 195], ["sigma_inv.dot", "alpha.dot", "numpy.diag", "numpy.exp", "numpy.sum", "numpy.exp"], "methods", ["None"], ["", "def", "cvi_gamma_update", "(", "self", ",", "doc_topic_param_1", ",", "doc_topic_param_2", ",", "doc_zeta_factor", ",", "doc_tau", ",", "sum_phi", ",", "doc_word_count", ",", "t", ")", ":", "\n", "        ", "sigma_inv", "=", "self", ".", "sigma_inv", "[", "t", ",", ":", ",", ":", "]", "\n", "alpha", "=", "self", ".", "alpha", "[", "t", ",", ":", ",", ":", "]", "\n", "\n", "nat_param_1", "=", "sigma_inv", ".", "dot", "(", "alpha", ".", "dot", "(", "doc_tau", ")", ")", "\n", "nat_param_1", "+=", "sum_phi", "\n", "\n", "nat_param_2", "=", "-", "0.5", "*", "np", ".", "diag", "(", "sigma_inv", ")", "\n", "nat_param_2", "-=", "(", "doc_word_count", "/", "(", "2", "*", "np", ".", "sum", "(", "np", ".", "exp", "(", "doc_zeta_factor", ")", ")", ")", ")", "*", "np", ".", "exp", "(", "doc_zeta_factor", ")", "\n", "\n", "new_doc_topic_param_1", "=", "self", ".", "step_size", "*", "nat_param_1", "+", "(", "1", "-", "self", ".", "step_size", ")", "*", "doc_topic_param_1", "\n", "new_doc_topic_param_2", "=", "self", ".", "step_size", "*", "nat_param_2", "+", "(", "1", "-", "self", ".", "step_size", ")", "*", "doc_topic_param_2", "\n", "\n", "new_doc_m", "=", "-", "1", "*", "(", "new_doc_topic_param_1", "/", "(", "2", "*", "new_doc_topic_param_2", ")", ")", "\n", "new_doc_vsq", "=", "-", "1", "/", "(", "2", "*", "new_doc_topic_param_2", ")", "\n", "\n", "return", "new_doc_m", ",", "new_doc_vsq", ",", "new_doc_topic_param_1", ",", "new_doc_topic_param_2", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.cvi_tau_update": [[196, 210], ["alpha.T.dot().dot", "range", "numpy.sum", "numpy.exp", "alpha.T.dot", "alpha.dot", "numpy.eye", "numpy.trace", "numpy.diag", "sigma_inv.dot"], "methods", ["None"], ["", "def", "cvi_tau_update", "(", "self", ",", "doc_tau", ",", "doc_persona_param", ",", "doc_m", ",", "t", ",", "a", ")", ":", "\n", "        ", "alpha", "=", "self", ".", "alpha", "[", "t", ",", ":", ",", ":", "]", "\n", "sigma_inv", "=", "self", ".", "sigma_inv", "[", "t", ",", ":", ",", ":", "]", "\n", "sigma", "=", "self", ".", "sigma", "[", "t", ",", ":", ",", ":", "]", "\n", "gradient", "=", "alpha", ".", "T", ".", "dot", "(", "sigma_inv", ")", ".", "dot", "(", "doc_m", "-", "alpha", ".", "dot", "(", "doc_tau", ")", ")", "\n", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "            ", "S", "=", "np", ".", "eye", "(", "self", ".", "num_topics", ")", "*", "(", "np", ".", "diag", "(", "alpha", "[", ":", ",", "p", "]", "**", "2", ")", "+", "sigma", ")", "\n", "gradient", "[", "p", "]", "-=", "0.5", "*", "np", ".", "trace", "(", "sigma_inv", ".", "dot", "(", "S", ")", ")", "\n", "\n", "", "new_doc_persona_param", "=", "self", ".", "step_size", "*", "(", "self", ".", "E_log_kappa", "[", "a", ",", ":", "]", "+", "gradient", ")", "+", "(", "1", "-", "self", ".", "step_size", ")", "*", "doc_persona_param", "\n", "new_doc_tau", "=", "doc_tau", "*", "np", ".", "exp", "(", "new_doc_persona_param", ")", "\n", "new_doc_tau", "/=", "np", ".", "sum", "(", "new_doc_tau", ")", "\n", "\n", "return", "new_doc_tau", ",", "new_doc_persona_param", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.m_step": [[211, 263], ["src.utilities.dirichlet_expectation", "numpy.where", "numpy.sum", "numpy.log", "src.utilities.dirichlet_expectation", "dapper.DAPPER.estimate_alpha", "range", "dapper.DAPPER.smooth_alpha", "range", "numpy.power", "dapper.DAPPER.ss_queue.append", "src.utilities.sum_normalize", "len", "numpy.sum", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.dirichlet_expectation", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.dirichlet_expectation", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.estimate_alpha", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.smooth_alpha", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.sum_normalize"], ["", "def", "m_step", "(", "self", ",", "batch_size", ",", "num_docs_per_time", ")", ":", "\n", "        ", "\"\"\"\n        Stochastic M-step: update the variational parameter for topics using a mini-batch of documents\n        \"\"\"", "\n", "if", "batch_size", "==", "self", ".", "total_documents", ":", "\n", "            ", "rhot", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "rhot", "=", "np", ".", "power", "(", "self", ".", "learning_offset", "+", "self", ".", "current_em_iter", "/", "2", ",", "-", "self", ".", "learning_decay", ")", "\n", "\n", "# topic maximization", "\n", "", "if", "self", ".", "queue_size", ">", "1", ":", "\n", "            ", "self", ".", "ss_queue", ".", "append", "(", "self", ".", "total_documents", "*", "self", ".", "ss", ".", "beta", "/", "batch_size", ")", "\n", "if", "len", "(", "self", ".", "ss_queue", ")", "<", "self", ".", "queue_size", ":", "\n", "                ", "self", ".", "beta_ss", "=", "sum", "(", "self", ".", "ss_queue", ")", "/", "len", "(", "self", ".", "ss_queue", ")", "\n", "", "else", ":", "\n", "# update gradient averages with new info", "\n", "                ", "self", ".", "beta_ss", "+=", "(", "self", ".", "ss_queue", "[", "-", "1", "]", "-", "self", ".", "ss_queue", "[", "0", "]", ")", "/", "self", ".", "queue_size", "\n", "\n", "# perform gradient update", "\n", "", "lambda_gradient", "=", "(", "(", "self", ".", "eta", "-", "self", ".", "_lambda", ")", "+", "self", ".", "beta_ss", ")", "\n", "self", ".", "_lambda", "=", "self", ".", "_lambda", "+", "0.001", "*", "lambda_gradient", "\n", "", "else", ":", "\n", "            ", "new_lambda", "=", "self", ".", "eta", "+", "self", ".", "total_documents", "*", "self", ".", "ss", ".", "beta", "/", "batch_size", "\n", "self", ".", "_lambda", "=", "(", "1", "-", "rhot", ")", "*", "self", ".", "_lambda", "+", "rhot", "*", "new_lambda", "\n", "\n", "", "self", ".", "E_log_beta", "=", "dirichlet_expectation", "(", "self", ".", "_lambda", ")", "\n", "# normalize", "\n", "beta", "=", "np", ".", "where", "(", "self", ".", "ss", ".", "beta", "<=", "0", ",", "1e-30", ",", "self", ".", "_lambda", ")", "\n", "beta", "/=", "np", ".", "sum", "(", "beta", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "self", ".", "log_beta", "=", "np", ".", "log", "(", "beta", ")", "\n", "\n", "# update the kappa terms", "\n", "new_delta", "=", "self", ".", "omega", "+", "(", "self", ".", "total_documents", "*", "self", ".", "ss", ".", "kappa", "/", "batch_size", ")", "\n", "self", ".", "_delta", "=", "(", "1", "-", "rhot", ")", "*", "self", ".", "_delta", "+", "rhot", "*", "new_delta", "\n", "self", ".", "E_log_kappa", "=", "dirichlet_expectation", "(", "self", ".", "_delta", ")", "\n", "\n", "# estimate a new noisy estimate of alpha", "\n", "new_alpha_hat", "=", "self", ".", "estimate_alpha", "(", "num_docs_per_time", ")", "\n", "\n", "# update alpha hat using svi update rule", "\n", "self", ".", "alpha_hat", "=", "(", "1", "-", "rhot", ")", "*", "self", ".", "alpha_hat", "+", "rhot", "*", "new_alpha_hat", "\n", "\n", "# normalize alpha hat before applying kalman filter smoother", "\n", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "            ", "self", ".", "alpha_hat", "[", ":", ",", ":", ",", "p", "]", "=", "sum_normalize", "(", "self", ".", "alpha_hat", "[", ":", ",", ":", ",", "p", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# kalman smoothing", "\n", "", "self", ".", "alpha", "=", "self", ".", "smooth_alpha", "(", ")", "\n", "\n", "# update priors mu0", "\n", "for", "k", "in", "range", "(", "self", ".", "num_topics", ")", ":", "\n", "            ", "self", ".", "mu0", "[", "k", "]", "=", "np", ".", "sum", "(", "self", ".", "alpha", "[", "0", ",", "k", ",", ":", "]", ")", "*", "(", "1.0", "/", "self", ".", "num_personas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.estimate_alpha": [[264, 300], ["numpy.zeros", "range", "numpy.ones", "numpy.diag_indices_from", "numpy.linalg.solve", "logger.warning", "logger.warning", "src.utilities.matrix2str", "src.utilities.matrix2str"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.matrix2str", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.matrix2str"], ["", "", "def", "estimate_alpha", "(", "self", ",", "num_docs_per_time", ")", ":", "\n", "        ", "\"\"\"\n        Estimation of alpha in the estep\n        Solved using system of linear equations: Ax = b where unknown x is alpha_hat\n        See Equation (6) of paper\n        :return:\n        \"\"\"", "\n", "# compute noisy alpha hat at each time step", "\n", "# solve: A (alpha) = b", "\n", "alpha_hat", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ",", "self", ".", "num_personas", ")", ")", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", ")", ":", "\n", "            ", "if", "num_docs_per_time", "[", "t", "]", "==", "0", ":", "\n", "                ", "if", "t", ">", "0", ":", "\n", "                    ", "alpha_hat", "[", "t", ",", ":", ",", ":", "]", "=", "alpha_hat", "[", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "", "continue", "\n", "\n", "", "if", "t", "==", "0", ":", "\n", "                ", "b", "=", "self", ".", "mu0", "[", ":", ",", "np", ".", "newaxis", "]", "+", "self", ".", "ss", ".", "alpha", "[", "t", ",", ":", ",", ":", "]", "-", "self", ".", "ss", ".", "x", "[", "t", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "b", "=", "alpha_hat", "[", "t", "-", "1", ",", ":", ",", ":", "]", "+", "self", ".", "ss", ".", "alpha", "[", "t", ",", ":", ",", ":", "]", "-", "self", ".", "ss", ".", "x", "[", "t", ",", ":", "]", "\n", "\n", "", "denom", "=", "self", ".", "ss", ".", "x2", "[", "t", ",", ":", "]", "+", "1.0", "\n", "if", "self", ".", "regularization", ">", "0", ":", "\n", "                ", "A", "=", "np", ".", "ones", "(", "(", "self", ".", "num_personas", ",", "self", ".", "num_personas", ")", ")", "\n", "A", "*=", "(", "self", ".", "regularization", "/", "self", ".", "num_personas", ")", "*", "num_docs_per_time", "[", "t", "]", "\n", "A", "[", "np", ".", "diag_indices_from", "(", "A", ")", "]", "=", "denom", "\n", "try", ":", "\n", "                    ", "alpha_hat", "[", "t", ",", ":", ",", ":", "]", "=", "np", ".", "linalg", ".", "solve", "(", "A", ",", "b", ".", "T", ")", ".", "T", "\n", "", "except", "np", ".", "linalg", ".", "linalg", ".", "LinAlgError", ":", "\n", "                    ", "logger", ".", "warning", "(", "\"Singular matrix in solving for alpha hat. A:\\n\"", "+", "matrix2str", "(", "A", ",", "2", ")", "+", "\"\\n\"", ")", "\n", "logger", ".", "warning", "(", "\"Singular matrix in solving for alpha hat. b^T:\\n\"", "+", "matrix2str", "(", "b", ".", "T", ",", "2", ")", "+", "\"\\n\"", ")", "\n", "alpha_hat", "[", "t", ",", ":", ",", ":", "]", "=", "1.0", "*", "b", "/", "denom", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "", "", "else", ":", "\n", "                ", "alpha_hat", "[", "t", ",", ":", ",", ":", "]", "=", "1.0", "*", "b", "/", "denom", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "\n", "", "", "return", "alpha_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.smooth_alpha": [[301, 331], ["dapper.DAPPER.variance_dynamics", "numpy.zeros", "range", "numpy.zeros", "range", "range"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.variance_dynamics"], ["", "def", "smooth_alpha", "(", "self", ")", ":", "\n", "# compute forward and backward variances", "\n", "        ", "forward_var", ",", "backward_var", ",", "P", "=", "self", ".", "variance_dynamics", "(", ")", "\n", "\n", "# smooth noisy alpha using forward and backwards equations", "\n", "backwards_alpha", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ",", "self", ".", "num_personas", ")", ")", "\n", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "# forward equations", "\n", "# initialize forward_alpha[p]", "\n", "            ", "forwards_alpha", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ")", ")", "\n", "forwards_alpha", "[", "0", ",", ":", "]", "=", "self", ".", "alpha_hat", "[", "0", ",", ":", ",", "p", "]", "\n", "for", "t", "in", "range", "(", "1", ",", "self", ".", "num_times", ")", ":", "\n", "                ", "step", "=", "P", "[", "t", ",", ":", "]", "/", "(", "self", ".", "measurement_noise", "+", "P", "[", "t", ",", ":", "]", ")", "\n", "forwards_alpha", "[", "t", ",", ":", "]", "=", "step", "*", "self", ".", "alpha_hat", "[", "t", ",", ":", ",", "p", "]", "+", "(", "1", "-", "step", ")", "*", "forwards_alpha", "[", "t", "-", "1", ",", ":", "]", "\n", "\n", "# backward equations", "\n", "", "for", "t", "in", "range", "(", "self", ".", "num_times", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "if", "t", "==", "(", "self", ".", "num_times", "-", "1", ")", ":", "\n", "                    ", "backwards_alpha", "[", "self", ".", "num_times", "-", "1", ",", ":", ",", "p", "]", "=", "forwards_alpha", "[", "self", ".", "num_times", "-", "1", ",", ":", "]", "\n", "continue", "\n", "\n", "", "if", "t", "==", "0", ":", "\n", "                    ", "delta", "=", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "1", "]", "-", "self", ".", "times", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                    ", "delta", "=", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "t", "]", "-", "self", ".", "times", "[", "t", "-", "1", "]", ")", "\n", "\n", "", "step", "=", "delta", "/", "(", "forward_var", "[", "t", ",", ":", "]", "+", "delta", ")", "\n", "backwards_alpha", "[", "t", ",", ":", ",", "p", "]", "=", "step", "*", "forwards_alpha", "[", "t", ",", ":", "]", "+", "(", "1", "-", "step", ")", "*", "backwards_alpha", "[", "t", "+", "1", ",", ":", ",", "p", "]", "\n", "\n", "", "", "return", "backwards_alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.variance_dynamics": [[332, 353], ["numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "range", "numpy.diag"], "methods", ["None"], ["", "def", "variance_dynamics", "(", "self", ")", ":", "\n", "# compute forward variance", "\n", "        ", "forward_var", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ")", ")", "\n", "P", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ")", ")", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", ")", ":", "\n", "            ", "if", "t", "==", "0", ":", "\n", "                ", "P", "[", "t", ",", ":", "]", "=", "np", ".", "diag", "(", "self", ".", "sigma", "[", "0", ",", ":", ",", ":", "]", ")", "+", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "1", "]", "-", "self", ".", "times", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "P", "[", "t", ",", ":", "]", "=", "forward_var", "[", "t", "-", "1", ",", ":", "]", "+", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "t", "]", "-", "self", ".", "times", "[", "t", "-", "1", "]", ")", "\n", "\n", "# use a fixed estimate of the measurement noise", "\n", "", "forward_var", "[", "t", ",", ":", "]", "=", "self", ".", "measurement_noise", "*", "P", "[", "t", ",", ":", "]", "/", "(", "P", "[", "t", ",", ":", "]", "+", "self", ".", "measurement_noise", ")", "\n", "\n", "# compute backward variance for persona p", "\n", "", "backward_var", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_times", ",", "self", ".", "num_topics", ")", ")", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "backward_var", "[", "t", ",", ":", "]", "=", "forward_var", "[", "t", ",", ":", "]", "\n", "if", "t", "!=", "(", "self", ".", "num_times", "-", "1", ")", ":", "\n", "                ", "backward_var", "[", "t", ",", ":", "]", "+=", "(", "forward_var", "[", "t", ",", ":", "]", "**", "2", "/", "P", "[", "t", "+", "1", ",", ":", "]", "**", "2", ")", "*", "(", "backward_var", "[", "t", "+", "1", ",", ":", "]", "-", "P", "[", "t", "+", "1", ",", ":", "]", ")", "\n", "\n", "", "", "return", "forward_var", ",", "backward_var", ",", "P", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_topic_lhood": [[354, 359], ["numpy.sum", "scipy.special.gammaln", "numpy.sum", "numpy.sum", "scipy.special.gammaln", "numpy.sum", "scipy.special.gammaln", "scipy.special.gammaln", "numpy.sum"], "methods", ["None"], ["", "def", "compute_topic_lhood", "(", "self", ")", ":", "\n", "# compute the beta terms lhood", "\n", "        ", "rval", "=", "self", ".", "num_topics", "*", "(", "gammaln", "(", "np", ".", "sum", "(", "self", ".", "eta", ")", ")", "-", "np", ".", "sum", "(", "gammaln", "(", "self", ".", "eta", ")", ")", ")", "\n", "rval", "+=", "np", ".", "sum", "(", "np", ".", "sum", "(", "gammaln", "(", "self", ".", "_lambda", ")", ",", "axis", "=", "1", ")", "-", "gammaln", "(", "np", ".", "sum", "(", "self", ".", "_lambda", ",", "axis", "=", "1", ")", ")", ")", "\n", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_author_lhood": [[360, 370], ["numpy.sum", "numpy.sum", "scipy.special.gammaln", "scipy.special.gammaln().sum", "scipy.special.gammaln", "scipy.special.gammaln", "dapper.DAPPER.omega.sum", "dapper.DAPPER._delta.sum", "scipy.special.gammaln"], "methods", ["None"], ["", "def", "compute_author_lhood", "(", "self", ")", ":", "\n", "# compute the kappa terms lhood", "\n", "        ", "rval", "=", "self", ".", "num_authors", "*", "(", "gammaln", "(", "self", ".", "omega", ".", "sum", "(", ")", ")", "-", "gammaln", "(", "self", ".", "omega", ")", ".", "sum", "(", ")", ")", "\n", "\n", "# note: cancellation of omega between model and entropy term", "\n", "rval", "+=", "np", ".", "sum", "(", "(", "self", ".", "_delta", "-", "1.0", ")", "*", "self", ".", "E_log_kappa", ")", "\n", "\n", "# entropy term", "\n", "rval", "+=", "np", ".", "sum", "(", "gammaln", "(", "self", ".", "_delta", ")", "-", "gammaln", "(", "self", ".", "_delta", ".", "sum", "(", "axis", "=", "1", ",", "keepdims", "=", "True", ")", ")", ")", "\n", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_persona_lhood": [[371, 398], ["range", "range", "numpy.log", "numpy.log", "numpy.tile().reshape", "numpy.linalg.det", "alpha_dif[].T.dot().dot", "numpy.tile", "numpy.trace", "alpha_dif[].T.dot"], "methods", ["None"], ["", "def", "compute_persona_lhood", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The bound on just the alpha terms is usually small since it primarly depends on\n        the different between alpha_t and alpha_[t-1] which is small if the model\n        is properly smoothed.\n        :return:\n        \"\"\"", "\n", "rval", "=", "0.0", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", ")", ":", "\n", "            ", "if", "t", "==", "0", ":", "\n", "                ", "alpha_prev", "=", "np", ".", "tile", "(", "self", ".", "mu0", ",", "self", ".", "num_personas", ")", ".", "reshape", "(", "(", "self", ".", "num_personas", ",", "self", ".", "num_topics", ")", ")", ".", "T", "\n", "", "else", ":", "\n", "                ", "alpha_prev", "=", "self", ".", "alpha", "[", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "", "alpha", "=", "self", ".", "alpha", "[", "t", ",", ":", ",", ":", "]", "\n", "alpha_dif", "=", "alpha", "-", "alpha_prev", "\n", "sigma_inv", "=", "self", ".", "sigma_inv", "[", "t", ",", ":", ",", ":", "]", "\n", "sigma", "=", "self", ".", "sigma", "[", "t", ",", ":", ",", ":", "]", "\n", "\n", "# 0.5 * log | Sigma^-1| - log 2 pi", "\n", "rval", "-=", "self", ".", "num_personas", "*", "0.5", "*", "np", ".", "log", "(", "np", ".", "linalg", ".", "det", "(", "sigma", ")", ")", "\n", "rval", "-=", "self", ".", "num_topics", "*", "self", ".", "num_personas", "*", "0.5", "*", "np", ".", "log", "(", "2", "*", "3.1415", ")", "\n", "# quadratic term", "\n", "delta", "=", "self", ".", "process_noise", "*", "(", "self", ".", "times", "[", "t", "]", "-", "self", ".", "times", "[", "t", "-", "1", "]", "if", "t", ">", "0", "else", "self", ".", "times", "[", "1", "]", "-", "self", ".", "times", "[", "0", "]", ")", "\n", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "                ", "rval", "-=", "0.5", "*", "(", "alpha_dif", "[", ":", ",", "p", "]", ".", "T", ".", "dot", "(", "sigma_inv", ")", ".", "dot", "(", "alpha_dif", "[", ":", ",", "p", "]", ")", "+", "(", "1.0", "/", "delta", ")", "*", "np", ".", "trace", "(", "sigma", ")", ")", "\n", "\n", "", "", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_doc_lhood": [[399, 438], ["dapper.DAPPER.E_log_kappa[].dot", "alpha.dot", "numpy.zeros", "range", "numpy.sum", "doc_tau.dot", "numpy.sum", "numpy.log", "numpy.sum", "numpy.trace", "numpy.sum", "numpy.log", "numpy.sum", "sigma_inv.dot", "scipy.misc.logsumexp", "numpy.sum", "numpy.log", "numpy.linalg.det", "numpy.diag", "numpy.eye", "numpy.diag", "numpy.exp", "numpy.exp"], "methods", ["None"], ["", "def", "compute_doc_lhood", "(", "self", ",", "doc", ",", "doc_tau", ",", "doc_m", ",", "doc_vsq", ",", "log_phi", ")", ":", "\n", "        ", "sigma", "=", "self", ".", "sigma", "[", "doc", ".", "time_id", ",", ":", ",", ":", "]", "\n", "sigma_inv", "=", "self", ".", "sigma_inv", "[", "doc", ".", "time_id", ",", ":", ",", ":", "]", "\n", "alpha", "=", "self", ".", "alpha", "[", "doc", ".", "time_id", ",", ":", ",", ":", "]", "\n", "\n", "# term1: sum_P [ tau * (digamma(delta) - digamma(sum(delta))) ]", "\n", "lhood", "=", "self", ".", "E_log_kappa", "[", "doc", ".", "author_id", ",", ":", "]", ".", "dot", "(", "doc_tau", ")", "\n", "\n", "# term 2: log det inv Sigma - k/2 log 2 pi +", "\n", "# term 2: -0.5 * (gamma - alpha*tau) sigma_inv (gamma - alpha_tau) +", "\n", "#         -0.5 * tr(sigma_inv vhat) +", "\n", "#         -0.5 * tr(sigma_inv diag(tau alpha + sigma_hat)", "\n", "# note - K/2 log 2 pi cancels with term 6", "\n", "lhood", "-=", "0.5", "*", "np", ".", "log", "(", "np", ".", "linalg", ".", "det", "(", "sigma", ")", "+", "1e-30", ")", "\n", "alpha_tau", "=", "alpha", ".", "dot", "(", "doc_tau", ")", "\n", "lhood", "-=", "0.5", "*", "(", "doc_m", "-", "alpha_tau", ")", ".", "T", ".", "dot", "(", "sigma_inv", ")", ".", "dot", "(", "doc_m", "-", "alpha_tau", ")", "\n", "lhood", "-=", "0.5", "*", "np", ".", "sum", "(", "doc_vsq", "*", "np", ".", "diag", "(", "sigma_inv", ")", ")", "\n", "S", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_topics", ",", "self", ".", "num_topics", ")", ")", "\n", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "            ", "S", "+=", "np", ".", "eye", "(", "self", ".", "num_topics", ")", "*", "doc_tau", "[", "p", "]", "*", "(", "np", ".", "diag", "(", "alpha", "[", ":", ",", "p", "]", "**", "2", ")", "+", "sigma", ")", "\n", "", "lhood", "-=", "0.5", "*", "np", ".", "trace", "(", "sigma_inv", ".", "dot", "(", "S", ")", ")", "\n", "\n", "# term 3: - zeta_inv Sum( exp(gamma + vhat) ) + 1 - log(zeta)", "\n", "# use the fact that doc_zeta = np.sum(np.exp(doc_m+0.5*doc_v2)), to cancel the factors", "\n", "lhood", "+=", "-", "1.0", "*", "logsumexp", "(", "doc_m", "+", "0.5", "*", "doc_vsq", ")", "*", "np", ".", "sum", "(", "doc", ".", "counts", ")", "\n", "\n", "# term 4: Sum(gamma * phi)", "\n", "lhood", "+=", "np", ".", "sum", "(", "np", ".", "sum", "(", "np", ".", "exp", "(", "log_phi", ")", "*", "doc", ".", "counts", ",", "axis", "=", "1", ")", "*", "doc_m", ")", "\n", "\n", "# term 5: -tau log tau", "\n", "lhood", "-=", "doc_tau", ".", "dot", "(", "np", ".", "log", "(", "doc_tau", "+", "1e-30", ")", ")", "\n", "\n", "# term 6: v_hat in entropy", "\n", "# Note K/2 log 2 pi cancels with term 2", "\n", "lhood", "+=", "0.5", "*", "np", ".", "sum", "(", "np", ".", "log", "(", "doc_vsq", ")", ")", "\n", "\n", "# term 7: phi log phi", "\n", "lhood", "-=", "np", ".", "sum", "(", "np", ".", "exp", "(", "log_phi", ")", "*", "log_phi", "*", "doc", ".", "counts", ")", "\n", "return", "lhood", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_word_lhood": [[439, 442], ["numpy.sum", "numpy.exp", "numpy.log"], "methods", ["None"], ["", "def", "compute_word_lhood", "(", "self", ",", "doc", ",", "log_phi", ")", ":", "\n", "        ", "lhood", "=", "np", ".", "sum", "(", "np", ".", "exp", "(", "log_phi", "+", "np", ".", "log", "(", "doc", ".", "counts", ")", ")", "*", "self", ".", "E_log_beta", "[", ":", ",", "doc", ".", "words", "]", ")", "\n", "return", "lhood", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.em_step": [[443, 480], ["len", "dapper.DAPPER.ss.reset", "time.process_time", "time.process_time", "dapper.DAPPER.m_step", "dapper.DAPPER.compute_topic_lhood", "dapper.DAPPER.compute_author_lhood", "dapper.DAPPER.compute_persona_lhood", "dapper.DAPPER.e_step_parallel", "dapper.DAPPER.e_step", "time.process_time", "time.process_time"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.reset", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.m_step", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_topic_lhood", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_author_lhood", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_persona_lhood", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.e_step_parallel", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.e_step"], ["", "def", "em_step", "(", "self", ",", "docs", ",", "total_docs", ",", "check_model_lhood", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Performs stochastic EM-update for one iteration using a mini-batch of documents\n        and compute the training log-likelihood\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "docs", ")", "\n", "self", ".", "current_em_iter", "+=", "1", "\n", "self", ".", "ss", ".", "reset", "(", ")", "\n", "\n", "# e-step", "\n", "clock_e_step", "=", "time", ".", "process_time", "(", ")", "\n", "if", "self", ".", "num_workers", ">", "1", ":", "\n", "# run e-step in parallel", "\n", "            ", "batch_lhood", ",", "words_lhood", "=", "self", ".", "e_step_parallel", "(", "docs", "=", "docs", ",", "save_ss", "=", "True", ",", "check_model_lhood", "=", "check_model_lhood", ")", "\n", "", "else", ":", "\n", "            ", "batch_lhood", ",", "words_lhood", "=", "self", ".", "e_step", "(", "docs", "=", "docs", ",", "save_ss", "=", "True", ",", "check_model_lhood", "=", "check_model_lhood", ")", "\n", "", "clock_e_step", "=", "time", ".", "process_time", "(", ")", "-", "clock_e_step", "\n", "doc_lhood", "=", "batch_lhood", "*", "total_docs", "/", "batch_size", "\n", "\n", "# count docs per timestep in this batch for adjusting the regularization", "\n", "num_docs_per_time", "=", "[", "0", "]", "*", "self", ".", "num_times", "\n", "for", "d", "in", "docs", ":", "\n", "            ", "time_id", "=", "d", ".", "time_id", "\n", "num_docs_per_time", "[", "time_id", "]", "+=", "1", "\n", "\n", "# m-step", "\n", "", "clock_m_step", "=", "time", ".", "process_time", "(", ")", "\n", "self", ".", "m_step", "(", "batch_size", "=", "batch_size", ",", "num_docs_per_time", "=", "num_docs_per_time", ")", "\n", "clock_m_step", "=", "time", ".", "process_time", "(", ")", "-", "clock_m_step", "\n", "\n", "topic_lhood", "=", "self", ".", "compute_topic_lhood", "(", ")", "\n", "persona_lhood", "=", "self", ".", "compute_author_lhood", "(", ")", "\n", "alpha_lhood", "=", "self", ".", "compute_persona_lhood", "(", ")", "\n", "\n", "model_lhood", "=", "doc_lhood", "+", "topic_lhood", "+", "persona_lhood", "+", "alpha_lhood", "\n", "total_time", "=", "clock_e_step", "+", "clock_m_step", "\n", "return", "model_lhood", ",", "words_lhood", ",", "total_time", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.e_step_parallel": [[481, 539], ["len", "int", "multiprocessing.Queue", "multiprocessing.Queue", "math.ceil", "sum", "docs_per_worker.append", "multiprocessing.Pool", "enumerate", "multiprocessing.Queue.empty", "multiprocessing.Queue.get", "dapper.DAPPER.ss.merge", "len", "dapper.DAPPER.e_step_parallel.process_result_queue"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.merge"], ["", "def", "e_step_parallel", "(", "self", ",", "docs", ",", "save_ss", ",", "check_model_lhood", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "docs", ")", "\n", "\n", "# setup stream that returns chunks of documents at each iteration", "\n", "# determine size of partitions of documents", "\n", "max_docs_per_worker", "=", "int", "(", "ceil", "(", "1.0", "*", "batch_size", "/", "self", ".", "num_workers", ")", ")", "\n", "docs_per_worker", "=", "[", "]", "\n", "while", "sum", "(", "docs_per_worker", ")", "<", "batch_size", ":", "\n", "            ", "if", "sum", "(", "docs_per_worker", ")", "+", "max_docs_per_worker", "<", "batch_size", ":", "\n", "                ", "n", "=", "max_docs_per_worker", "\n", "", "else", ":", "\n", "                ", "n", "=", "batch_size", "-", "sum", "(", "docs_per_worker", ")", "\n", "", "docs_per_worker", ".", "append", "(", "n", ")", "\n", "\n", "# set up pool of workers and arguments passed to each worker", "\n", "", "job_queue", "=", "Queue", "(", "maxsize", "=", "self", ".", "num_workers", ")", "\n", "result_queue", "=", "Queue", "(", ")", "\n", "\n", "queue_size", ",", "reallen", "=", "[", "0", "]", ",", "0", "\n", "batch_lhood", ",", "words_lhood", "=", "[", "0.0", "]", ",", "[", "0.0", "]", "\n", "\n", "def", "process_result_queue", "(", ")", ":", "\n", "            ", "\"\"\"\n            clear result queue, merge intermediate SS\n            :return:\n            \"\"\"", "\n", "while", "not", "result_queue", ".", "empty", "(", ")", ":", "\n", "# collect summary statistics and merge all sufficient statistics", "\n", "                ", "bl", ",", "wl", ",", "partial_ss", "=", "result_queue", ".", "get", "(", ")", "\n", "batch_lhood", "[", "0", "]", "+=", "bl", "\n", "words_lhood", "[", "0", "]", "+=", "wl", "\n", "self", ".", "ss", ".", "merge", "(", "partial_ss", ")", "\n", "queue_size", "[", "0", "]", "-=", "1", "\n", "\n", "", "", "with", "Pool", "(", "self", ".", "num_workers", ",", "_doc_e_step_worker", ",", "(", "job_queue", ",", "result_queue", ",", ")", ")", "as", "pool", ":", "\n", "# loop through chunks of documents placing chunks on the queue", "\n", "            ", "for", "chunk_id", ",", "chunk_size", "in", "enumerate", "(", "docs_per_worker", ")", ":", "\n", "                ", "doc_mini_batch", "=", "docs", "[", "reallen", ":", "(", "reallen", "+", "chunk_size", ")", "]", "\n", "reallen", "+=", "len", "(", "doc_mini_batch", ")", "# track how many documents have been seen", "\n", "chunk_put", "=", "False", "\n", "while", "not", "chunk_put", ":", "\n", "                    ", "try", ":", "\n", "                        ", "args", "=", "(", "self", ",", "doc_mini_batch", ",", "save_ss", ",", "check_model_lhood", ")", "\n", "job_queue", ".", "put", "(", "args", ",", "block", "=", "False", ",", "timeout", "=", "0.1", ")", "\n", "chunk_put", "=", "True", "\n", "queue_size", "[", "0", "]", "+=", "1", "\n", "", "except", "job_queue", ".", "full", "(", ")", ":", "\n", "                        ", "process_result_queue", "(", ")", "\n", "\n", "", "", "process_result_queue", "(", ")", "\n", "\n", "", "while", "queue_size", "[", "0", "]", ">", "0", ":", "\n", "                ", "process_result_queue", "(", ")", "\n", "\n", "", "if", "reallen", "!=", "batch_size", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"input corpus size changed during training (don't use generators as input)\"", ")", "\n", "\n", "", "", "return", "batch_lhood", "[", "0", "]", ",", "words_lhood", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.e_step": [[540, 612], ["numpy.ones", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.sum", "dapper.DAPPER.compute_word_lhood", "numpy.ones", "numpy.zeros", "scipy.misc.logsumexp", "numpy.sum", "dapper.DAPPER.cvi_gamma_update", "numpy.mean", "dapper.DAPPER.ss.update", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "numpy.ones", "dapper.DAPPER.cvi_tau_update", "abs", "dapper.DAPPER.compute_doc_lhood", "numpy.exp", "str", "str", "str", "round", "round", "round", "str", "round"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_word_lhood", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.cvi_gamma_update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.cvi_tau_update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_doc_lhood"], ["", "def", "e_step", "(", "self", ",", "docs", ",", "save_ss", "=", "True", ",", "check_model_lhood", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        E-step: update the variational parameters for topic proportions and topic assignments.\n        \"\"\"", "\n", "# iterate over all documents", "\n", "batch_lhood", ",", "batch_lhood_d", "=", "0", ",", "0", "\n", "words_lhood", "=", "0", "\n", "for", "doc", "in", "docs", ":", "\n", "# initialize gamma for this document", "\n", "            ", "doc_m", "=", "np", ".", "ones", "(", "self", ".", "num_topics", ")", "*", "(", "1.0", "/", "self", ".", "num_topics", ")", "\n", "doc_vsq", "=", "np", ".", "ones", "(", "self", ".", "num_topics", ")", "\n", "doc_topic_param_1", "=", "np", ".", "zeros", "(", "self", ".", "num_topics", ")", "\n", "doc_topic_param_2", "=", "np", ".", "zeros", "(", "self", ".", "num_topics", ")", "\n", "doc_persona_param", "=", "np", ".", "zeros", "(", "self", ".", "num_personas", ")", "\n", "if", "self", ".", "num_personas", "==", "self", ".", "num_authors", ":", "\n", "                ", "doc_tau", "=", "np", ".", "zeros", "(", "self", ".", "num_personas", ")", "\n", "doc_tau", "[", "doc", ".", "author_id", "]", "=", "1.0", "\n", "", "else", ":", "\n", "                ", "doc_tau", "=", "np", ".", "ones", "(", "self", ".", "num_personas", ")", "*", "(", "1.0", "/", "self", ".", "num_personas", ")", "\n", "", "doc_word_count", "=", "np", ".", "sum", "(", "doc", ".", "counts", ")", "\n", "\n", "# update zeta in close form", "\n", "doc_zeta_factor", "=", "doc_m", "+", "0.5", "*", "doc_vsq", "\n", "\n", "iters", "=", "0", "\n", "while", "iters", "<", "self", ".", "max_local_iters", ":", "\n", "                ", "prev_doc_m", "=", "doc_m", "\n", "iters", "+=", "1", "\n", "# update phi in closed form", "\n", "log_phi", "=", "doc_m", "[", ":", ",", "np", ".", "newaxis", "]", "+", "self", ".", "log_beta", "[", ":", ",", "doc", ".", "words", "]", "\n", "log_phi", "-=", "logsumexp", "(", "log_phi", ",", "axis", "=", "0", ",", "keepdims", "=", "True", ")", "\n", "\n", "# CVI update to m and v", "\n", "sum_phi", "=", "np", ".", "sum", "(", "np", ".", "exp", "(", "log_phi", ")", "*", "doc", ".", "counts", "[", "np", ".", "newaxis", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "doc_m", ",", "doc_vsq", ",", "doc_topic_param_1", ",", "doc_topic_param_2", "=", "self", ".", "cvi_gamma_update", "(", "\n", "doc_topic_param_1", ",", "doc_topic_param_2", ",", "doc_zeta_factor", ",", "doc_tau", ",", "sum_phi", ",", "doc_word_count", ",", "doc", ".", "time_id", ")", "\n", "\n", "# CVI update to tau", "\n", "if", "self", ".", "num_personas", "!=", "self", ".", "num_authors", ":", "\n", "                    ", "doc_tau", ",", "doc_persona_param", "=", "self", ".", "cvi_tau_update", "(", "doc_tau", ",", "doc_persona_param", ",", "doc_m", ",", "\n", "doc", ".", "time_id", ",", "doc", ".", "author_id", ")", "\n", "\n", "# update zeta in closed form", "\n", "", "doc_zeta_factor", "=", "doc_m", "+", "0.5", "*", "doc_vsq", "\n", "\n", "mean_change", "=", "np", ".", "mean", "(", "abs", "(", "doc_m", "-", "prev_doc_m", ")", ")", "\n", "if", "mean_change", "<", "self", ".", "local_convergence", ":", "\n", "                    ", "break", "\n", "\n", "# compute word likelihoods", "\n", "", "", "words_lhood_d", "=", "self", ".", "compute_word_lhood", "(", "doc", ",", "log_phi", ")", "\n", "words_lhood", "+=", "words_lhood_d", "\n", "\n", "# collect sufficient statistics", "\n", "if", "save_ss", ":", "\n", "                ", "self", ".", "ss", ".", "update", "(", "doc", ",", "doc_m", ",", "doc_tau", ",", "log_phi", ")", "\n", "\n", "if", "check_model_lhood", ":", "\n", "# if updating the model (save_ss) then also compute variational likelihoods", "\n", "                    ", "batch_lhood_d", "=", "self", ".", "compute_doc_lhood", "(", "doc", ",", "doc_tau", ",", "doc_m", ",", "doc_vsq", ",", "log_phi", ")", "\n", "batch_lhood", "+=", "batch_lhood_d", "\n", "\n", "", "", "if", "SHOW_EVERY", ">", "0", "and", "doc", ".", "doc_id", "%", "SHOW_EVERY", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Variational parameters for document: {}, converged in {} steps\"", ".", "format", "(", "doc", ".", "doc_id", ",", "iters", ")", ")", "\n", "logger", ".", "info", "(", "\"Per-word likelihood for doc[{}]: ({} + {}) / {} = {:.2f}\"", ".", "format", "(", "\n", "doc", ".", "doc_id", ",", "words_lhood_d", ",", "batch_lhood_d", ",", "doc_word_count", ",", "(", "words_lhood_d", "+", "batch_lhood_d", ")", "/", "doc_word_count", ")", ")", "\n", "logger", ".", "info", "(", "\"new zeta: \"", "+", "' '", ".", "join", "(", "[", "str", "(", "round", "(", "g", ",", "3", ")", ")", "for", "g", "in", "doc_zeta_factor", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"new doc_m[{}]: \"", ".", "format", "(", "doc", ".", "doc_id", ")", "+", "' '", ".", "join", "(", "[", "str", "(", "round", "(", "g", ",", "3", ")", ")", "for", "g", "in", "doc_m", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"new vhat: \"", "+", "' '", ".", "join", "(", "[", "str", "(", "round", "(", "g", ",", "3", ")", ")", "for", "g", "in", "doc_vsq", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"new tau: \"", "+", "' '", ".", "join", "(", "[", "str", "(", "round", "(", "g", ",", "3", ")", ")", "for", "g", "in", "doc_tau", "]", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "return", "batch_lhood", ",", "words_lhood", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.fit": [[613, 701], ["dapper.DAPPER.init_from_corpus", "dapper.DAPPER.parameter_init", "time.process_time", "dapper.DAPPER.print_topics_over_time", "dapper.DAPPER.print_author_personas", "dapper.DAPPER.print_topics", "logger.info", "dapper.DAPPER.print_convergence", "dapper.DAPPER.init_beta_random", "dapper.DAPPER.init_beta_from_corpus", "src.utilities.sample_batches", "enumerate", "time.process_time", "log_str.format", "numpy.finfo", "numpy.finfo", "dapper.DAPPER.em_step", "len", "numpy.abs", "train_results.append", "logger.info", "dapper.DAPPER.print_topics", "logger.info", "logger.info", "numpy.sum", "log_str.format", "log_str.format", "len", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_from_corpus", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.parameter_init", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics_over_time", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_author_personas", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_convergence", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_beta_random", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_beta_from_corpus", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.sample_batches", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.em_step", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics"], ["", "def", "fit", "(", "self", ",", "corpus", ",", "random_beta", "=", "False", ",", "check_model_lhood", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Performs EM-update until reaching target average change in the log-likelihood\n        \"\"\"", "\n", "# init from corpus", "\n", "self", ".", "init_from_corpus", "(", "corpus", ")", "\n", "self", ".", "parameter_init", "(", ")", "\n", "if", "random_beta", ":", "\n", "            ", "self", ".", "init_beta_random", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "init_beta_from_corpus", "(", "corpus", "=", "corpus", ")", "\n", "\n", "", "if", "self", ".", "batch_size", "==", "0", ":", "\n", "            ", "batch_size", "=", "corpus", ".", "total_documents", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "prev_train_model_lhood", ",", "train_model_lhood", "=", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "min", ",", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "min", "\n", "train_results", "=", "[", "]", "\n", "elapsed_time", "=", "0.0", "\n", "total_training_docs", "=", "0", "\n", "\n", "total_time", "=", "time", ".", "process_time", "(", ")", "\n", "while", "self", ".", "max_epochs", "<=", "0", "or", "self", ".", "total_epochs", "<", "self", ".", "max_epochs", ":", "\n", "            ", "batches", "=", "sample_batches", "(", "corpus", ".", "total_documents", ",", "batch_size", ")", "\n", "epoch_time", "=", "0.0", "\n", "finished_epoch", "=", "True", "\n", "for", "batch_id", ",", "doc_ids", "in", "enumerate", "(", "batches", ")", ":", "\n", "                ", "batch", "=", "[", "corpus", ".", "docs", "[", "d", "]", "for", "d", "in", "doc_ids", "]", "\n", "train_model_lhood", ",", "train_words_lhood", ",", "batch_time", "=", "self", ".", "em_step", "(", "docs", "=", "batch", ",", "\n", "total_docs", "=", "corpus", ".", "total_documents", ",", "\n", "check_model_lhood", "=", "check_model_lhood", ")", "\n", "\n", "train_model_pwll", "=", "train_model_lhood", "/", "corpus", ".", "total_words", "\n", "train_words_pwll", "=", "train_words_lhood", "/", "np", ".", "sum", "(", "[", "np", ".", "sum", "(", "d", ".", "counts", ")", "for", "d", "in", "batch", "]", ")", "\n", "total_training_docs", "+=", "len", "(", "batch", ")", "\n", "epoch_time", "+=", "batch_time", "\n", "elapsed_time", "+=", "batch_time", "\n", "convergence", "=", "np", ".", "abs", "(", "(", "train_model_lhood", "-", "prev_train_model_lhood", ")", "/", "prev_train_model_lhood", ")", "\n", "train_results", ".", "append", "(", "[", "self", ".", "total_epochs", ",", "batch_id", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "\n", "convergence", ",", "batch_time", ",", "elapsed_time", ",", "total_training_docs", "]", ")", "\n", "# report current stats", "\n", "log_str", "=", "\"epoch: {}, batch: {}, model ll: {:.1f}, model pwll: {:.2f}, words pwll: {:.2f}, convergence: {:.3f}, time: {:.3f}\"", "\n", "logger", ".", "info", "(", "log_str", ".", "format", "(", "self", ".", "total_epochs", ",", "batch_id", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "\n", "convergence", ",", "batch_time", ")", ")", "\n", "\n", "prev_train_model_lhood", "=", "train_model_lhood", "\n", "if", "(", "elapsed_time", "/", "60.0", ")", ">", "self", ".", "max_training_minutes", ">", "0", ":", "\n", "                    ", "finished_epoch", "=", "(", "batch_id", "+", "1", ")", "==", "len", "(", "batches", ")", "# finished if we're already through the last batch", "\n", "break", "\n", "\n", "", "", "if", "finished_epoch", ":", "\n", "# report stats after each full epoch", "\n", "                ", "self", ".", "total_epochs", "+=", "1", "\n", "#self.print_topics_over_time(5)", "\n", "#self.print_author_personas()", "\n", "self", ".", "print_topics", "(", "topn", "=", "8", ")", "\n", "docs_per_hour", "=", "total_training_docs", "/", "(", "epoch_time", "/", "60.0", "/", "60.0", ")", "\n", "log_str", "=", "\"\"\"{} Epochs Completed\n                    train model lhood: {:.1f}, model per-word log-lhood: {:.2f}, words per-word log-lhood: {:.2f}, convergence: {:.3f},\n                    total minutes training: {:.2f}, previous epoch minutes training: {:.2f}, epoch's docs/hr {:.1f}\n                    \"\"\"", "\n", "logger", ".", "info", "(", "log_str", ".", "format", "(", "self", ".", "total_epochs", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "convergence", ",", "\n", "elapsed_time", "/", "60.0", ",", "epoch_time", "/", "60.0", ",", "docs_per_hour", ")", ")", "\n", "\n", "", "if", "(", "elapsed_time", "/", "60.0", ")", ">", "self", ".", "max_training_minutes", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Maxed training time has elapsed, stopping training.\"", ")", "\n", "break", "\n", "\n", "# print last stats", "\n", "", "", "total_time", "=", "time", ".", "process_time", "(", ")", "-", "total_time", "\n", "self", ".", "print_topics_over_time", "(", "5", ")", "\n", "self", ".", "print_author_personas", "(", "max_show", "=", "25", ")", "\n", "self", ".", "print_topics", "(", "topn", "=", "8", ")", "\n", "log_str", "=", "\"\"\"Finished after {} EM iterations ({} full epochs, {} total documents)\n                    train model lhood: {:.1f}, model per-word log-lhood: {:.2f}, words per-word log-lhood: {:.2f},\n                    total minutes elapsed: {:.1f}, total minutes training: {:.3f}, avg docs/hr {:.1f}\n                    \"\"\"", "\n", "docs_per_hour", "=", "total_training_docs", "/", "(", "elapsed_time", "/", "60.0", "/", "60.0", ")", "\n", "logger", ".", "info", "(", "log_str", ".", "format", "(", "self", ".", "current_em_iter", ",", "self", ".", "total_epochs", ",", "total_training_docs", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "\n", "total_time", "/", "60.0", ",", "elapsed_time", "/", "60.0", ",", "docs_per_hour", ")", ")", "\n", "\n", "self", ".", "print_convergence", "(", "train_results", ",", "show_batches", "=", "False", ")", "\n", "return", "train_results", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_beta_from_corpus": [[702, 727], ["logger.info", "range", "numpy.log", "min", "numpy.sort", "logger.debug", "int", "numpy.random.randint", "range", "numpy.sum", "round", "str"], "methods", ["None"], ["", "def", "init_beta_from_corpus", "(", "self", ",", "corpus", ",", "num_docs_init", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        initialize model for training using the corpus\n        :param corpus:\n        :param num_docs_init:\n        :return:\n        \"\"\"", "\n", "if", "num_docs_init", "is", "None", ":", "\n", "            ", "num_docs_init", "=", "min", "(", "500", ",", "int", "(", "round", "(", "corpus", ".", "total_documents", "*", "0.01", ")", ")", ")", "\n", "", "if", "num_docs_init", "==", "0", ":", "\n", "            ", "return", "\n", "\n", "", "logger", ".", "info", "(", "\"Initializing beta from {} random documents in the corpus for each topics\"", ".", "format", "(", "num_docs_init", ")", ")", "\n", "for", "k", "in", "range", "(", "self", ".", "num_topics", ")", ":", "\n", "            ", "doc_ids", "=", "np", ".", "sort", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "corpus", ".", "total_documents", ",", "num_docs_init", ")", ")", "\n", "logger", ".", "debug", "(", "\"Initializing topic {} from docs: {}\"", ".", "format", "(", "k", ",", "' '", ".", "join", "(", "[", "str", "(", "d", ")", "for", "d", "in", "doc_ids", "]", ")", ")", ")", "\n", "\n", "for", "i", "in", "doc_ids", ":", "\n", "                ", "doc", "=", "corpus", ".", "docs", "[", "i", "]", "\n", "for", "n", "in", "range", "(", "doc", ".", "num_terms", ")", ":", "\n", "                    ", "v", "=", "doc", ".", "words", "[", "n", "]", "\n", "self", ".", "_lambda", "[", "k", ",", "v", "]", "+=", "doc", ".", "counts", "[", "n", "]", "\n", "\n", "# save log of beta for VI computations", "\n", "", "", "", "self", ".", "log_beta", "=", "np", ".", "log", "(", "self", ".", "_lambda", "/", "np", ".", "sum", "(", "self", ".", "_lambda", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_beta_random": [[728, 736], ["numpy.random.uniform", "dapper.DAPPER.log_beta.sum", "numpy.log"], "methods", ["None"], ["", "def", "init_beta_random", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        random initializations before training\n        :return:\n        \"\"\"", "\n", "self", ".", "log_beta", "=", "np", ".", "random", ".", "uniform", "(", "0.01", ",", "0.99", ",", "(", "self", ".", "num_topics", ",", "self", ".", "vocab_size", ")", ")", "\n", "row_sums", "=", "self", ".", "log_beta", ".", "sum", "(", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "self", ".", "log_beta", "=", "np", ".", "log", "(", "self", ".", "log_beta", "/", "row_sums", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.predict": [[737, 751], ["logger.info", "dapper.DAPPER.e_step_parallel", "dapper.DAPPER.e_step"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.e_step_parallel", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.e_step"], ["", "def", "predict", "(", "self", ",", "test_corpus", ")", ":", "\n", "        ", "\"\"\"\n        Performs E-step on test corpus using stored topics obtained by training\n        Computes the average heldout log-likelihood\n        \"\"\"", "\n", "if", "self", ".", "num_workers", ">", "1", ":", "\n", "            ", "_", ",", "test_words_lhood", "=", "self", ".", "e_step_parallel", "(", "docs", "=", "test_corpus", ".", "docs", ",", "save_ss", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "_", ",", "test_words_lhood", "=", "self", ".", "e_step", "(", "docs", "=", "test_corpus", ".", "docs", ",", "save_ss", "=", "False", ")", "\n", "\n", "", "test_words_pwll", "=", "test_words_lhood", "/", "test_corpus", ".", "total_words", "\n", "logger", ".", "info", "(", "'Test words log-lhood: {:.1f}, words per-word log-lhood: {:.2f}'", ".", "format", "(", "\n", "test_words_lhood", ",", "test_words_pwll", ")", ")", "\n", "return", "test_words_lhood", ",", "test_words_pwll", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.fit_predict": [[752, 862], ["dapper.DAPPER.init_from_corpus", "dapper.DAPPER.parameter_init", "dapper.DAPPER._check_corpora", "time.process_time", "dapper.DAPPER.print_topics_over_time", "dapper.DAPPER.print_author_personas", "dapper.DAPPER.print_topics", "logger.info", "dapper.DAPPER.print_convergence", "dapper.DAPPER.print_convergence", "dapper.DAPPER.init_beta_random", "dapper.DAPPER.init_beta_from_corpus", "src.utilities.sample_batches", "enumerate", "time.process_time", "log_str.format", "numpy.finfo", "numpy.finfo", "dapper.DAPPER.em_step", "len", "numpy.abs", "train_results.append", "logger.info", "dapper.DAPPER.predict", "test_results.append", "dapper.DAPPER.print_topics", "logger.info", "logger.info", "numpy.sum", "log_str.format", "dapper.DAPPER.predict", "test_results.append", "log_str.format", "len", "len", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_from_corpus", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.parameter_init", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER._check_corpora", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics_over_time", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_author_personas", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_convergence", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_convergence", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_beta_random", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.init_beta_from_corpus", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.sample_batches", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.em_step", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.predict", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.predict"], ["", "def", "fit_predict", "(", "self", ",", "train_corpus", ",", "test_corpus", ",", "evaluate_every", "=", "None", ",", "random_beta", "=", "False", ",", "check_model_lhood", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Computes the heldout-log likelihood on the test corpus after \"evaluate_every\" iterations\n        (mini-batches) of training.\n        \"\"\"", "\n", "# initializarions from the training corpus", "\n", "self", ".", "init_from_corpus", "(", "train_corpus", ")", "\n", "self", ".", "parameter_init", "(", ")", "\n", "if", "random_beta", ":", "\n", "            ", "self", ".", "init_beta_random", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "init_beta_from_corpus", "(", "corpus", "=", "train_corpus", ")", "\n", "\n", "# verify that the training and test corpora have same metadata", "\n", "", "self", ".", "_check_corpora", "(", "train_corpus", "=", "train_corpus", ",", "test_corpus", "=", "test_corpus", ")", "\n", "\n", "if", "self", ".", "batch_size", "<=", "0", ":", "\n", "            ", "batch_size", "=", "train_corpus", ".", "total_documents", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "prev_train_model_lhood", ",", "train_model_lhood", "=", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "min", ",", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "min", "\n", "train_results", "=", "[", "]", "\n", "test_results", "=", "[", "]", "\n", "elapsed_time", "=", "0.0", "\n", "total_training_docs", "=", "0", "\n", "\n", "total_time", "=", "time", ".", "process_time", "(", ")", "\n", "while", "self", ".", "max_epochs", "<=", "0", "or", "self", ".", "total_epochs", "<", "self", ".", "max_epochs", ":", "\n", "            ", "batches", "=", "sample_batches", "(", "train_corpus", ".", "total_documents", ",", "batch_size", ")", "\n", "epoch_time", "=", "0.0", "\n", "finished_epoch", "=", "True", "\n", "for", "batch_id", ",", "doc_ids", "in", "enumerate", "(", "batches", ")", ":", "\n", "# do an EM iteration on this batch", "\n", "                ", "batch", "=", "[", "train_corpus", ".", "docs", "[", "d", "]", "for", "d", "in", "doc_ids", "]", "\n", "train_model_lhood", ",", "train_words_lhood", ",", "batch_time", "=", "self", ".", "em_step", "(", "docs", "=", "batch", ",", "\n", "total_docs", "=", "train_corpus", ".", "total_documents", ",", "\n", "check_model_lhood", "=", "check_model_lhood", ")", "\n", "\n", "# collect stats from EM iteration", "\n", "train_model_pwll", "=", "train_model_lhood", "/", "train_corpus", ".", "total_words", "\n", "train_words_pwll", "=", "train_words_lhood", "/", "np", ".", "sum", "(", "[", "np", ".", "sum", "(", "d", ".", "counts", ")", "for", "d", "in", "batch", "]", ")", "\n", "elapsed_time", "+=", "batch_time", "\n", "epoch_time", "+=", "batch_time", "\n", "total_training_docs", "+=", "len", "(", "batch", ")", "\n", "convergence", "=", "np", ".", "abs", "(", "(", "train_model_lhood", "-", "prev_train_model_lhood", ")", "/", "prev_train_model_lhood", ")", "\n", "train_results", ".", "append", "(", "[", "self", ".", "total_epochs", ",", "batch_id", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "\n", "convergence", ",", "batch_time", ",", "elapsed_time", ",", "total_training_docs", "]", ")", "\n", "\n", "# report current stats", "\n", "log_str", "=", "\"epoch: {}, batch: {}, model ll: {:.1f}, model pwll: {:.2f}, words pwll: {:.2f}, convergence: {:.3f}, batch time: {:.3f}, total training minutes: {:.3f}\"", "\n", "logger", ".", "info", "(", "log_str", ".", "format", "(", "self", ".", "total_epochs", ",", "batch_id", ",", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "convergence", ",", "batch_time", ",", "elapsed_time", "/", "60.", ")", ")", "\n", "\n", "# test set evaluation:", "\n", "if", "evaluate_every", "is", "not", "None", "and", "(", "batch_id", "+", "1", ")", "%", "evaluate_every", "==", "0", "and", "(", "batch_id", "+", "1", ")", "!=", "len", "(", "batches", ")", ":", "\n", "                    ", "test_words_lhood", ",", "test_words_pwll", "=", "self", ".", "predict", "(", "test_corpus", "=", "test_corpus", ")", "\n", "test_results", ".", "append", "(", "[", "self", ".", "total_epochs", ",", "batch_id", ",", "\n", "0.0", ",", "0.0", ",", "test_words_pwll", ",", "convergence", ",", "\n", "0.0", ",", "elapsed_time", ",", "total_training_docs", "]", ")", "\n", "\n", "", "prev_train_model_lhood", "=", "train_model_lhood", "\n", "if", "(", "elapsed_time", "/", "60.0", ")", ">", "self", ".", "max_training_minutes", ">", "0", ":", "\n", "                    ", "finished_epoch", "=", "(", "batch_id", "+", "1", ")", "==", "len", "(", "batches", ")", "# finished if we're already through the last batch", "\n", "break", "\n", "\n", "", "", "if", "finished_epoch", ":", "\n", "                ", "self", ".", "total_epochs", "+=", "1", "\n", "# evaluate the log-likelihood at the end of each full epoch", "\n", "test_words_lhood", ",", "test_words_pwll", "=", "self", ".", "predict", "(", "test_corpus", "=", "test_corpus", ")", "\n", "test_results", ".", "append", "(", "[", "self", ".", "total_epochs", ",", "-", "1", ",", "\n", "0.0", ",", "0.0", ",", "test_words_pwll", ",", "convergence", ",", "\n", "epoch_time", ",", "elapsed_time", ",", "total_training_docs", "]", ")", "\n", "#self.print_topics_over_time(5)", "\n", "#self.print_author_personas()", "\n", "self", ".", "print_topics", "(", "topn", "=", "8", ")", "\n", "# report stats on this epoch", "\n", "docs_per_hour", "=", "total_training_docs", "/", "(", "epoch_time", "/", "60.0", "/", "60.0", ")", "\n", "log_str", "=", "\"\"\"{} Epochs Completed\n                    train model lhood: {:.1f}, model per-word log-lhood: {:.2f}, words per-word log-lhood: {:.2f},\n                    test words lhood:  {:.1f}, words per-word log-lhood: {:.2f}, convergence: {:.3f},\n                    total minutes training: {:.2f}, previous epoch minutes training: {:.2f}, epoch's docs/hr {:.1f}\n                    \"\"\"", "\n", "logger", ".", "info", "(", "log_str", ".", "format", "(", "self", ".", "total_epochs", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "\n", "test_words_lhood", ",", "test_words_pwll", ",", "convergence", ",", "\n", "elapsed_time", "/", "60.0", ",", "epoch_time", "/", "60.0", ",", "docs_per_hour", ")", ")", "\n", "\n", "", "if", "(", "elapsed_time", "/", "60.0", ")", ">", "self", ".", "max_training_minutes", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Maxed training time has elapsed, stopping training.\"", ")", "\n", "break", "\n", "\n", "# print last stats", "\n", "", "", "total_time", "=", "time", ".", "process_time", "(", ")", "-", "total_time", "\n", "self", ".", "print_topics_over_time", "(", "5", ")", "\n", "self", ".", "print_author_personas", "(", ")", "\n", "self", ".", "print_topics", "(", "topn", "=", "8", ")", "\n", "log_str", "=", "\"\"\"Finished after {} EM iterations ({} full epochs, {} total documents)\n            train model lhood: {:.1f}, model per-word log-lhood: {:.2f}, words per-word log-lhood: {:.2f},\n            test words lhood:  {:.1f}, words per-word log-lhood: {:.2f},\n            total minutes elapsed: {:.1f}, total minutes training: {:.3f}, avg docs/hr {:.1f}\n            \"\"\"", "\n", "docs_per_hour", "=", "total_training_docs", "/", "(", "elapsed_time", "/", "60.0", "/", "60.0", ")", "\n", "logger", ".", "info", "(", "log_str", ".", "format", "(", "self", ".", "current_em_iter", ",", "self", ".", "total_epochs", ",", "total_training_docs", ",", "\n", "train_model_lhood", ",", "train_model_pwll", ",", "train_words_pwll", ",", "\n", "test_words_lhood", ",", "test_words_pwll", ",", "\n", "total_time", "/", "60.0", ",", "elapsed_time", "/", "60.0", ",", "docs_per_hour", ")", ")", "\n", "self", ".", "print_convergence", "(", "train_results", ",", "show_batches", "=", "False", ")", "\n", "self", ".", "print_convergence", "(", "test_results", ",", "show_batches", "=", "True", ")", "\n", "return", "train_results", ",", "test_results", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics_over_time": [[863, 874], ["range", "numpy.sum", "top_topic_ids.sort", "logger.info", "logger.info", "logger.info", "numpy.argsort", "src.utilities.matrix2str", "src.utilities.matrix2str", "str"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.matrix2str", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.matrix2str"], ["", "def", "print_topics_over_time", "(", "self", ",", "top_n_topics", "=", "None", ")", ":", "\n", "        ", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "            ", "if", "top_n_topics", "is", "not", "None", ":", "\n", "                ", "topic_totals", "=", "np", ".", "sum", "(", "self", ".", "alpha", "[", ":", ",", ":", ",", "p", "]", ",", "axis", "=", "0", ")", "\n", "top_topic_ids", "=", "np", ".", "argsort", "(", "topic_totals", ")", "[", "-", "top_n_topics", ":", "]", "\n", "top_topic_ids", ".", "sort", "(", ")", "\n", "alpha", "=", "self", ".", "alpha", "[", ":", ",", "top_topic_ids", ",", "p", "]", "\n", "logger", ".", "info", "(", "'alpha[p={}] top topic ids: '", "+", "'\\t'", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "top_topic_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "'alpha[p={}]\\n'", ".", "format", "(", "p", ")", "+", "matrix2str", "(", "alpha", ",", "3", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "'alpha[p={}]\\n'", ".", "format", "(", "p", ")", "+", "matrix2str", "(", "self", ".", "alpha", "[", ":", ",", ":", ",", "p", "]", ",", "3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_convergence": [[875, 886], ["logger.info", "logger.info", "log_str.format"], "methods", ["None"], ["", "", "", "def", "print_convergence", "(", "self", ",", "results", ",", "show_batches", "=", "False", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"EM Iteration\\tMini-batch\\tModel LL\\tModel PWLL\\tWords PWLL\\tConvergence\\tSeconds per Batch\\tSeconds Training\\tDocs Trained\"", ")", "\n", "for", "stats", "in", "results", ":", "\n", "            ", "em_iter", ",", "batch_id", ",", "model_lhood", ",", "model_pwll", ",", "words_pwll", ",", "convergence", ",", "batch_time", ",", "training_time", ",", "docs_trained", "=", "stats", "\n", "if", "batch_id", "==", "0", "or", "show_batches", ":", "\n", "                ", "if", "model_lhood", "==", "0.0", ":", "\n", "                    ", "log_str", "=", "\"{}\\t\\t{}\\t\\t{:.1f}\\t\\t{:.2f}\\t\\t{:.2f}\\t\\t{:.4f}\\t\\t{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{}\"", "\n", "", "else", ":", "\n", "                    ", "log_str", "=", "\"{}\\t\\t{}\\t\\t{:.1f}\\t{:.2f}\\t\\t{:.2f}\\t\\t{:.4f}\\t\\t{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{}\"", "\n", "", "logger", ".", "info", "(", "log_str", ".", "format", "(", "\n", "em_iter", ",", "batch_id", ",", "model_lhood", ",", "model_pwll", ",", "words_pwll", ",", "convergence", ",", "batch_time", ",", "training_time", ",", "docs_trained", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_author_personas": [[887, 906], ["max", "logger.info", "logger.info", "logger.info", "range", "numpy.sum", "len", "logger.info", "logger.info", "logger.info", "dapper.DAPPER.id2author.values", "len", "str", "range", "str", "str", "round", "round"], "methods", ["None"], ["", "", "", "def", "print_author_personas", "(", "self", ",", "max_show", "=", "10", ")", ":", "\n", "        ", "max_key_len", "=", "max", "(", "[", "len", "(", "k", ")", "for", "k", "in", "self", ".", "id2author", ".", "values", "(", ")", "]", ")", "\n", "kappa", "=", "self", ".", "_delta", "/", "np", ".", "sum", "(", "self", ".", "_delta", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "\n", "logger", ".", "info", "(", "\"Kappa:\"", ")", "\n", "spaces", "=", "' '", "*", "(", "max_key_len", "-", "6", ")", "\n", "logger", ".", "info", "(", "\"Author{} \\t{}\"", ".", "format", "(", "spaces", ",", "'\\t'", ".", "join", "(", "[", "'p'", "+", "str", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_personas", ")", "]", ")", ")", ")", "\n", "logger", ".", "info", "(", "'-'", "*", "(", "max_key_len", "+", "10", "*", "self", ".", "num_personas", ")", ")", "\n", "for", "author_id", "in", "range", "(", "self", ".", "num_authors", ")", ":", "\n", "            ", "author", "=", "self", ".", "id2author", "[", "author_id", "]", "\n", "pad", "=", "' '", "*", "(", "max_key_len", "-", "len", "(", "author", ")", ")", "\n", "if", "author_id", "==", "self", ".", "num_authors", "-", "1", ":", "\n", "                ", "logger", ".", "info", "(", "author", "+", "pad", "+", "\"\\t\"", "+", "\"\\t\"", ".", "join", "(", "[", "str", "(", "round", "(", "k", ",", "2", ")", ")", "for", "k", "in", "kappa", "[", "author_id", ",", ":", "]", "]", ")", "+", "'\\n'", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "author", "+", "pad", "+", "\"\\t\"", "+", "\"\\t\"", ".", "join", "(", "[", "str", "(", "round", "(", "k", ",", "2", ")", ")", "for", "k", "in", "kappa", "[", "author_id", ",", ":", "]", "]", ")", ")", "\n", "\n", "", "if", "author_id", ">", "max_show", ":", "\n", "                ", "logger", ".", "info", "(", "\"...\\n\"", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.print_topics": [[907, 912], ["dapper.DAPPER.get_topics", "range", "logger.info"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.get_topics"], ["", "", "", "def", "print_topics", "(", "self", ",", "topn", "=", "10", ",", "tfidf", "=", "True", ")", ":", "\n", "        ", "beta", "=", "self", ".", "get_topics", "(", "topn", ",", "tfidf", ")", "\n", "for", "k", "in", "range", "(", "self", ".", "num_topics", ")", ":", "\n", "            ", "topic_", "=", "' + '", ".", "join", "(", "[", "'%.3f*\"%s\"'", "%", "(", "p", ",", "w", ")", "for", "w", ",", "p", "in", "beta", "[", "k", "]", "]", ")", "\n", "logger", ".", "info", "(", "\"topic #%i: %s\"", ",", "k", ",", "topic_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.get_topics": [[913, 932], ["src.utilities.dirichlet_expectation", "numpy.exp", "numpy.sum", "dapper.DAPPER._term_scores", "range", "range", "numpy.argsort", "rval.append", "rval.append", "numpy.array", "list", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.utilities.dirichlet_expectation", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER._term_scores"], ["", "", "def", "get_topics", "(", "self", ",", "topn", "=", "10", ",", "tfidf", "=", "True", ")", ":", "\n", "        ", "E_log_beta", "=", "dirichlet_expectation", "(", "self", ".", "_lambda", ")", "\n", "beta", "=", "np", ".", "exp", "(", "E_log_beta", ")", "\n", "beta", "/=", "np", ".", "sum", "(", "beta", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "\n", "if", "tfidf", ":", "\n", "            ", "beta", "=", "self", ".", "_term_scores", "(", "beta", ")", "\n", "\n", "", "rval", "=", "[", "]", "\n", "if", "topn", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "range", "(", "self", ".", "num_topics", ")", ":", "\n", "                ", "word_rank", "=", "np", ".", "argsort", "(", "beta", "[", "k", ",", ":", "]", ")", "\n", "sorted_probs", "=", "beta", "[", "k", ",", "word_rank", "]", "\n", "sorted_words", "=", "np", ".", "array", "(", "list", "(", "self", ".", "vocab", ")", ")", "[", "word_rank", "]", "\n", "rval", ".", "append", "(", "[", "(", "w", ",", "p", ")", "for", "w", ",", "p", "in", "zip", "(", "sorted_words", "[", "-", "topn", ":", "]", "[", ":", ":", "-", "1", "]", ",", "sorted_probs", "[", "-", "topn", ":", "]", "[", ":", ":", "-", "1", "]", ")", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "k", "in", "range", "(", "self", ".", "num_topics", ")", ":", "\n", "                ", "rval", ".", "append", "(", "[", "(", "w", ",", "p", ")", "for", "w", ",", "p", "in", "zip", "(", "self", ".", "vocab", ",", "beta", "[", "k", ",", ":", "]", ")", "]", ")", "\n", "", "", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER._term_scores": [[933, 945], ["numpy.power", "numpy.any", "numpy.log", "numpy.multiply", "numpy.prod", "numpy.divide"], "methods", ["None"], ["", "def", "_term_scores", "(", "self", ",", "beta", ")", ":", "\n", "        ", "\"\"\"\n        TF-IDF type calculation for determining top topic terms\n        from \"Topic Models\" by Blei and Lafferty 2009, equation 3\n        :param beta:\n        :return:\n        \"\"\"", "\n", "denom", "=", "np", ".", "power", "(", "np", ".", "prod", "(", "beta", ",", "axis", "=", "0", ")", ",", "1.0", "/", "self", ".", "num_topics", ")", "\n", "if", "np", ".", "any", "(", "denom", "==", "0", ")", ":", "\n", "            ", "denom", "+=", "0.000001", "\n", "", "term2", "=", "np", ".", "log", "(", "np", ".", "divide", "(", "beta", ",", "denom", ")", ")", "\n", "return", "np", ".", "multiply", "(", "beta", ",", "term2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_topics": [[946, 952], ["dapper.DAPPER.get_topics", "open", "range", "f.write"], "methods", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.get_topics"], ["", "def", "save_topics", "(", "self", ",", "filename", ",", "topn", "=", "10", ",", "tfidf", "=", "True", ")", ":", "\n", "        ", "beta", "=", "self", ".", "get_topics", "(", "topn", ",", "tfidf", ")", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "k", "in", "range", "(", "self", ".", "num_topics", ")", ":", "\n", "                ", "topic_", "=", "', '", ".", "join", "(", "[", "w", "for", "w", ",", "p", "in", "beta", "[", "k", "]", "]", ")", "\n", "f", ".", "write", "(", "\"topic #{} ({:.2f}): {}\\n\"", ".", "format", "(", "k", ",", "self", ".", "mu0", "[", "k", "]", ",", "topic_", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_author_personas": [[953, 960], ["numpy.sum", "open", "f.write", "range", "f.write", "str", "range", "str", "round"], "methods", ["None"], ["", "", "", "def", "save_author_personas", "(", "self", ",", "filename", ")", ":", "\n", "        ", "kappa", "=", "self", ".", "_delta", "/", "np", ".", "sum", "(", "self", ".", "_delta", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"author\\t\"", "+", "\"\\t\"", ".", "join", "(", "[", "\"persona\"", "+", "str", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_personas", ")", "]", ")", "+", "\"\\n\"", ")", "\n", "for", "author_id", "in", "range", "(", "self", ".", "num_authors", ")", ":", "\n", "                ", "author", "=", "self", ".", "id2author", "[", "author_id", "]", "\n", "f", ".", "write", "(", "author", "+", "\"\\t\"", "+", "\"\\t\"", ".", "join", "(", "[", "str", "(", "round", "(", "k", ",", "7", ")", ")", "for", "k", "in", "kappa", "[", "author_id", "]", "]", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_persona_topics": [[961, 968], ["open", "f.write", "range", "range", "f.write", "str", "range", "str"], "methods", ["None"], ["", "", "", "def", "save_persona_topics", "(", "self", ",", "filename", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"time_id\\ttime\\tpersona\\t\"", "+", "\"\\t\"", ".", "join", "(", "[", "\"topic\"", "+", "str", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_topics", ")", "]", ")", "+", "\"\\n\"", ")", "\n", "for", "t", "in", "range", "(", "self", ".", "num_times", ")", ":", "\n", "                ", "for", "p", "in", "range", "(", "self", ".", "num_personas", ")", ":", "\n", "                    ", "time_val", "=", "self", ".", "times", "[", "t", "]", "\n", "f", ".", "write", "(", "\"{}\\t{}\\t{}\\t{}\\n\"", ".", "format", "(", "t", ",", "time_val", ",", "p", ",", "'\\t'", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "self", ".", "alpha", "[", "t", ",", ":", ",", "p", "]", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_convergences": [[969, 974], ["open", "f.write", "f.write", "str"], "methods", ["None"], ["", "", "", "", "def", "save_convergences", "(", "self", ",", "filename", ",", "results", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"em_iter\\tbatch_iter\\tmodel_log_lhood\\tmodel_pwll\\twords_pwll\\tconvergence\\tbatch_time\\ttraining_time\\tdocs_trained\\n\"", ")", "\n", "for", "stats", "in", "results", ":", "\n", "                ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "[", "str", "(", "stat", ")", "for", "stat", "in", "stats", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.__str__": [[975, 1002], ["None"], "methods", ["None"], ["", "", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "rval", "=", "\"\"\"\\nDAP Model:\n            training iterations: {} ({} epochs)\n            trained on corpus with {} time points, {} authors, {} documents, {} total words\n            using {} processors\n            \"\"\"", ".", "format", "(", "self", ".", "current_em_iter", ",", "self", ".", "total_epochs", ",", "self", ".", "num_times", ",", "\n", "self", ".", "num_authors", ",", "self", ".", "total_documents", ",", "self", ".", "total_words", ",", "self", ".", "num_workers", ")", "\n", "rval", "+=", "\"\"\"\\nModel Settings:\n            regularization {:.2f}\n            number of topics: {}\n            number of personas: {}\n            measurement noise: {}\n            process noise: {}\n            smoothing last {} gradients\n            batch size: {}\n            learning offset: {}\n            learning decay: {:.2f}\n            step size: {:.2f}\n            max epochs: {}\n            convergence criteria for local parameters: {:.2f}\n            local parameter max iterations: {}\n            \"\"\"", ".", "format", "(", "self", ".", "regularization", ",", "self", ".", "num_topics", ",", "self", ".", "num_personas", ",", "\n", "self", ".", "measurement_noise", ",", "self", ".", "process_noise", ",", "\n", "self", ".", "queue_size", ",", "\n", "self", ".", "batch_size", ",", "self", ".", "learning_offset", ",", "self", ".", "learning_decay", ",", "self", ".", "step_size", ",", "\n", "self", ".", "max_epochs", ",", "self", ".", "local_convergence", ",", "self", ".", "max_local_iters", ")", "\n", "return", "rval", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper._doc_e_step_worker": [[1004, 1080], ["input_queue.get", "src.sufficient_statistics.SufficientStatistics", "numpy.ones", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.sum", "dap.compute_word_lhood", "result_queue.put", "result_queue.put", "numpy.ones", "numpy.zeros", "scipy.misc.logsumexp", "numpy.sum", "dap.cvi_gamma_update", "numpy.mean", "src.sufficient_statistics.SufficientStatistics.update", "numpy.ones", "dap.cvi_tau_update", "abs", "dap.compute_doc_lhood", "numpy.exp"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_word_lhood", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.cvi_gamma_update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.cvi_tau_update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.compute_doc_lhood"], ["", "", "def", "_doc_e_step_worker", "(", "input_queue", ",", "result_queue", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "dap", ",", "docs", ",", "save_ss", ",", "check_model_lhood", "=", "input_queue", ".", "get", "(", ")", "\n", "\n", "if", "save_ss", ":", "\n", "# initialize sufficient statistics to gather information learned from each doc", "\n", "            ", "ss", "=", "SufficientStatistics", "(", "num_topics", "=", "dap", ".", "num_topics", ",", "\n", "vocab_size", "=", "dap", ".", "vocab_size", ",", "\n", "num_authors", "=", "dap", ".", "num_authors", ",", "\n", "num_personas", "=", "dap", ".", "num_personas", ",", "\n", "num_times", "=", "dap", ".", "num_times", ")", "\n", "\n", "# iterate over all documents", "\n", "", "batch_lhood", "=", "0", "\n", "words_lhood", "=", "0", "\n", "for", "doc", "in", "docs", ":", "\n", "# initialize gamma for this document", "\n", "            ", "doc_m", "=", "np", ".", "ones", "(", "dap", ".", "num_topics", ")", "*", "(", "1.0", "/", "dap", ".", "num_topics", ")", "\n", "doc_vsq", "=", "np", ".", "ones", "(", "dap", ".", "num_topics", ")", "\n", "doc_topic_param_1", "=", "np", ".", "zeros", "(", "dap", ".", "num_topics", ")", "\n", "doc_topic_param_2", "=", "np", ".", "zeros", "(", "dap", ".", "num_topics", ")", "\n", "doc_persona_param", "=", "np", ".", "zeros", "(", "dap", ".", "num_personas", ")", "\n", "if", "dap", ".", "num_personas", "==", "dap", ".", "num_authors", ":", "\n", "                ", "doc_tau", "=", "np", ".", "zeros", "(", "dap", ".", "num_personas", ")", "\n", "doc_tau", "[", "doc", ".", "author_id", "]", "=", "1.0", "\n", "", "else", ":", "\n", "                ", "doc_tau", "=", "np", ".", "ones", "(", "dap", ".", "num_personas", ")", "*", "(", "1.0", "/", "dap", ".", "num_personas", ")", "\n", "", "doc_word_count", "=", "np", ".", "sum", "(", "doc", ".", "counts", ")", "\n", "\n", "# update zeta in close form", "\n", "doc_zeta_factor", "=", "doc_m", "+", "0.5", "*", "doc_vsq", "\n", "\n", "iters", "=", "0", "\n", "while", "iters", "<", "dap", ".", "max_local_iters", ":", "\n", "                ", "prev_doc_m", "=", "doc_m", "\n", "iters", "+=", "1", "\n", "# update phi in closed form", "\n", "log_phi", "=", "doc_m", "[", ":", ",", "np", ".", "newaxis", "]", "+", "dap", ".", "log_beta", "[", ":", ",", "doc", ".", "words", "]", "\n", "log_phi", "-=", "logsumexp", "(", "log_phi", ",", "axis", "=", "0", ",", "keepdims", "=", "True", ")", "\n", "\n", "# CVI update to m and v", "\n", "sum_phi", "=", "np", ".", "sum", "(", "np", ".", "exp", "(", "log_phi", ")", "*", "doc", ".", "counts", "[", "np", ".", "newaxis", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "doc_m", ",", "doc_vsq", ",", "doc_topic_param_1", ",", "doc_topic_param_2", "=", "dap", ".", "cvi_gamma_update", "(", "\n", "doc_topic_param_1", ",", "doc_topic_param_2", ",", "doc_zeta_factor", ",", "doc_tau", ",", "sum_phi", ",", "\n", "doc_word_count", ",", "doc", ".", "time_id", ")", "\n", "\n", "# CVI update to tau", "\n", "if", "dap", ".", "num_personas", "!=", "dap", ".", "num_authors", ":", "\n", "                    ", "doc_tau", ",", "doc_persona_param", "=", "dap", ".", "cvi_tau_update", "(", "doc_tau", ",", "doc_persona_param", ",", "doc_m", ",", "\n", "doc", ".", "time_id", ",", "doc", ".", "author_id", ")", "\n", "\n", "# update zeta in closed form", "\n", "", "doc_zeta_factor", "=", "doc_m", "+", "0.5", "*", "doc_vsq", "\n", "\n", "mean_change", "=", "np", ".", "mean", "(", "abs", "(", "doc_m", "-", "prev_doc_m", ")", ")", "\n", "if", "mean_change", "<", "dap", ".", "local_convergence", ":", "\n", "                    ", "break", "\n", "\n", "", "", "words_lhood_d", "=", "dap", ".", "compute_word_lhood", "(", "doc", ",", "log_phi", ")", "\n", "words_lhood", "+=", "words_lhood_d", "\n", "if", "save_ss", ":", "\n", "                ", "ss", ".", "update", "(", "doc", ",", "doc_m", ",", "doc_tau", ",", "log_phi", ")", "\n", "if", "check_model_lhood", ":", "\n", "# compute likelihoods", "\n", "                    ", "batch_lhood_d", "=", "dap", ".", "compute_doc_lhood", "(", "doc", ",", "doc_tau", ",", "doc_m", ",", "doc_vsq", ",", "log_phi", ")", "\n", "batch_lhood", "+=", "batch_lhood_d", "\n", "\n", "# clean up and save results", "\n", "", "", "", "del", "docs", "\n", "del", "dap", "\n", "\n", "if", "save_ss", ":", "\n", "            ", "result_queue", ".", "put", "(", "[", "batch_lhood", ",", "words_lhood", ",", "ss", "]", ")", "\n", "del", "ss", "\n", "", "else", ":", "\n", "            ", "result_queue", ".", "put", "(", "[", "batch_lhood", ",", "words_lhood", ",", "None", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.doc.Doc.__init__": [[6, 15], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "num_terms", "=", "0", "\n", "self", ".", "words", "=", "None", "# converted to np array after running corpus", "\n", "self", ".", "counts", "=", "None", "# converted to np array after running corpus", "\n", "self", ".", "author", "=", "None", "\n", "self", ".", "author_id", "=", "None", "\n", "self", ".", "time", "=", "None", "\n", "self", ".", "time_id", "=", "None", "\n", "self", ".", "doc_id", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.doc.Doc.__str__": [[16, 19], ["len"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"num_terms: {}\\nlen words: {}\\nauthor: {}\\nauthor id: {}\\ntime: {}\\ntime id: {}\\ndoc id: {}\\n\"", ".", "format", "(", "\n", "self", ".", "num_terms", ",", "len", "(", "self", ".", "words", ")", ",", "self", ".", "author", ",", "self", ".", "author_id", ",", "self", ".", "time", ",", "self", ".", "time_id", ",", "self", ".", "doc_id", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_sotu.get_wordnet_pos": [[69, 85], ["treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith"], "function", ["None"], ["def", "get_wordnet_pos", "(", "treebank_tag", ")", ":", "\n", "    ", "\"\"\"\n    Function to help in lemmatizing words\n    :param treebank_tag:\n    :return:\n    \"\"\"", "\n", "if", "treebank_tag", ".", "startswith", "(", "'J'", ")", ":", "\n", "        ", "return", "wordnet", ".", "ADJ", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'V'", ")", ":", "\n", "        ", "return", "wordnet", ".", "VERB", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'N'", ")", ":", "\n", "        ", "return", "wordnet", ".", "NOUN", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'R'", ")", ":", "\n", "        ", "return", "wordnet", ".", "ADV", "\n", "", "else", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_sotu.clean_text": [[87, 153], ["re.sub", "re.sub", "iam.sub", "ive.sub", "hes.sub", "shes.sub", "weve.sub", "youve.sub", "willnot.sub", "cannot.sub", "itis.sub", "letus.sub", "heis.sub", "sheis.sub", "howis.sub", "thatis.sub", "thereis.sub", "whatis.sub", "whereis.sub", "whenis.sub", "whois.sub", "whyis.sub", "youall.sub", "youare.sub", "would.sub", "will.sub", "s_apostrophe.sub", "has.sub", "nt.sub", "have.sub", "united_states_of_america.sub", "united_states.sub", "us.sub", "usa.sub", "usp.sub", "usap.sub", "years.sub", "dollars.sub", "percent.sub", "percents.sub", "times.sub", "dates.sub", "punct.sub", "punct.sub.split", "lemmatizer.lemmatize().lower", "lemmatizer.lemmatize().lower", "nltk.pos_tag", "w.lower", "preprocess_sotu.get_wordnet_pos", "lemmatizer.lemmatize", "lemmatizer.lemmatize", "preprocess_sotu.get_wordnet_pos"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.get_wordnet_pos", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.get_wordnet_pos"], ["", "", "def", "clean_text", "(", "sotu", ")", ":", "\n", "    ", "\"\"\"\n    Defines how to each the text portion of each speech paragraph\n    :param sotu:\n    :return:\n    \"\"\"", "\n", "# treat hyphens between words as a single word", "\n", "sotu", "=", "re", ".", "sub", "(", "r\"([a-zA-Z])\\-([a-zA-Z])\"", ",", "r\"\\1_\\2\"", ",", "sotu", ")", "\n", "\n", "# but remove hyphens between words", "\n", "sotu", "=", "re", ".", "sub", "(", "r\"([0-9])\\-([0-9])\"", ",", "r\"\\1 \\2\"", ",", "sotu", ")", "\n", "\n", "# expand contractions", "\n", "sotu", "=", "iam", ".", "sub", "(", "\"i am\"", ",", "sotu", ")", "\n", "sotu", "=", "ive", ".", "sub", "(", "\"i have\"", ",", "sotu", ")", "\n", "sotu", "=", "hes", ".", "sub", "(", "\"he is\"", ",", "sotu", ")", "\n", "sotu", "=", "shes", ".", "sub", "(", "\"she is\"", ",", "sotu", ")", "\n", "sotu", "=", "weve", ".", "sub", "(", "\"we have\"", ",", "sotu", ")", "\n", "sotu", "=", "youve", ".", "sub", "(", "\"you have\"", ",", "sotu", ")", "\n", "sotu", "=", "willnot", ".", "sub", "(", "\"will not\"", ",", "sotu", ")", "\n", "sotu", "=", "cannot", ".", "sub", "(", "\"can not\"", ",", "sotu", ")", "\n", "sotu", "=", "itis", ".", "sub", "(", "\"it is\"", ",", "sotu", ")", "\n", "sotu", "=", "letus", ".", "sub", "(", "\"let us\"", ",", "sotu", ")", "\n", "sotu", "=", "heis", ".", "sub", "(", "\"he is\"", ",", "sotu", ")", "\n", "sotu", "=", "sheis", ".", "sub", "(", "\"she is\"", ",", "sotu", ")", "\n", "sotu", "=", "howis", ".", "sub", "(", "\"how is\"", ",", "sotu", ")", "\n", "sotu", "=", "thatis", ".", "sub", "(", "\"that is\"", ",", "sotu", ")", "\n", "sotu", "=", "thereis", ".", "sub", "(", "\"there is\"", ",", "sotu", ")", "\n", "sotu", "=", "whatis", ".", "sub", "(", "\"what is\"", ",", "sotu", ")", "\n", "sotu", "=", "whereis", ".", "sub", "(", "\"where is\"", ",", "sotu", ")", "\n", "sotu", "=", "whenis", ".", "sub", "(", "\"when is\"", ",", "sotu", ")", "\n", "sotu", "=", "whois", ".", "sub", "(", "\"who is\"", ",", "sotu", ")", "\n", "sotu", "=", "whyis", ".", "sub", "(", "\"why is\"", ",", "sotu", ")", "\n", "sotu", "=", "youall", ".", "sub", "(", "\"you all\"", ",", "sotu", ")", "\n", "sotu", "=", "youare", ".", "sub", "(", "\"you are\"", ",", "sotu", ")", "\n", "sotu", "=", "would", ".", "sub", "(", "\" would\"", ",", "sotu", ")", "\n", "sotu", "=", "will", ".", "sub", "(", "\" will\"", ",", "sotu", ")", "\n", "sotu", "=", "s_apostrophe", ".", "sub", "(", "\"s has \"", ",", "sotu", ")", "\n", "sotu", "=", "has", ".", "sub", "(", "\" has\"", ",", "sotu", ")", "\n", "sotu", "=", "nt", ".", "sub", "(", "\" not\"", ",", "sotu", ")", "\n", "sotu", "=", "have", ".", "sub", "(", "\" have\"", ",", "sotu", ")", "\n", "\n", "# capture common patterns", "\n", "sotu", "=", "united_states_of_america", ".", "sub", "(", "\" _usa_ \"", ",", "sotu", ")", "\n", "sotu", "=", "united_states", ".", "sub", "(", "\" _usa_ \"", ",", "sotu", ")", "\n", "sotu", "=", "us", ".", "sub", "(", "\" _usa_ \"", ",", "sotu", ")", "\n", "sotu", "=", "usa", ".", "sub", "(", "\" _usa_ \"", ",", "sotu", ")", "\n", "sotu", "=", "usp", ".", "sub", "(", "\" _usa_ \"", ",", "sotu", ")", "\n", "sotu", "=", "usap", ".", "sub", "(", "\" _usa_ \"", ",", "sotu", ")", "\n", "sotu", "=", "years", ".", "sub", "(", "\" _year_ \"", ",", "sotu", ")", "\n", "sotu", "=", "dollars", ".", "sub", "(", "\" _dollars_ \"", ",", "sotu", ")", "\n", "sotu", "=", "percent", ".", "sub", "(", "\" percent \"", ",", "sotu", ")", "\n", "sotu", "=", "percents", ".", "sub", "(", "\" percent \"", ",", "sotu", ")", "\n", "sotu", "=", "times", ".", "sub", "(", "\" _time_ \"", ",", "sotu", ")", "\n", "sotu", "=", "dates", ".", "sub", "(", "\" _date_ \"", ",", "sotu", ")", "\n", "sotu", "=", "punct", ".", "sub", "(", "\" \"", ",", "sotu", ")", "\n", "\n", "# tokenize and remove stopwords", "\n", "sotu", "=", "[", "w", "for", "w", "in", "sotu", ".", "split", "(", ")", "if", "w", ".", "lower", "(", ")", "not", "in", "stopword_set", "]", "\n", "\n", "# lemmatize and lowercase", "\n", "sotu", "=", "[", "lemmatizer", ".", "lemmatize", "(", "w", ",", "pos", "=", "get_wordnet_pos", "(", "p", ")", ")", ".", "lower", "(", ")", "if", "get_wordnet_pos", "(", "p", ")", "!=", "''", "else", "lemmatizer", ".", "lemmatize", "(", "w", ")", ".", "lower", "(", ")", "for", "w", ",", "p", "in", "pos_tag", "(", "sotu", ")", "]", "\n", "sotu", "=", "[", "'us'", "if", "w", "==", "'u'", "else", "w", "for", "w", "in", "sotu", "]", "\n", "return", "sotu", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_sotu.doc_generator": [[155, 161], ["open", "line.replace().split", "doc.split", "line.replace"], "function", ["None"], ["", "def", "doc_generator", "(", "fname", ")", ":", "\n", "    ", "with", "open", "(", "fname", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "doc", "=", "fields", "[", "-", "1", "]", "\n", "yield", "doc", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_sotu.keys_generator": [[163, 171], ["open", "line.replace().split", "line.replace", "int"], "function", ["None"], ["", "", "", "def", "keys_generator", "(", "fname", ")", ":", "\n", "    ", "\"\"\"\n    returns generator of (timestep, author) pairs\n    \"\"\"", "\n", "with", "open", "(", "fname", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "yield", "int", "(", "fields", "[", "0", "]", ")", ",", "fields", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_sotu.main": [[174, 337], ["print", "words_per_speech.items", "numpy.array", "numpy.array", "numpy.array", "numpy.argsort", "open", "enumerate", "print", "norm_keys.append", "int", "open", "zip", "print", "subprocess.call", "print", "preprocess_sotu.doc_generator", "gensim.corpora.Dictionary", "corpora.Dictionary.filter_extremes", "corpora.Dictionary.compactify", "gensim.corpora.MmCorpus.serialize", "gensim.corpora.MmCorpus", "gensim.corpora.BleiCorpus.save_corpus", "os.remove", "os.remove", "os.rename", "len", "open", "open.close", "os.remove", "print", "line.replace.replace", "preprocess_sotu.clean_text", "doc_generator.append", "np.array.append", "len", "outfile.write", "corpora.Dictionary.doc2bow", "open", "dap_file.write", "preprocess_sotu.keys_generator", "open", "open", "open", "int", "train.write", "test.write", "range", "line.replace.split", "fields[].replace().replace().replace", "int", "int", "str().zfill", "zip", "preprocess_sotu.doc_generator", "open.readline", "dap_file.write", "dap_file.readline().replace", "int", "int", "print", "numpy.random.choice", "numpy.delete", "print", "train.write", "test.write", "train.write", "test.write", "range", "round", "str", "dap_file.write", "dap_file.write", "str", "str", "dap_file.readline().replace", "dap_file.readline().replace", "numpy.arange", "len", "len", "dap_file.readline", "fields[].replace().replace", "str", "str", "dap_file.readline", "int", "str", "str", "str", "str", "test.write", "train.write", "str", "str", "dap_file.readline", "dap_file.readline", "numpy.ceil", "len", "len", "fields[].replace", "str", "str"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_asoiaf.clean_text", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.keys_generator", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator"], ["", "", "", "def", "main", "(", ")", ":", "\n", "# loop through each paragraph in each speech, collect keys and preprocess each text", "\n", "    ", "data_dir", "=", "\"../../data/sotu/\"", "\n", "# infile = \"sotu_doc_per_paragraph.txt\"", "\n", "# infile = \"sotu_1901_2016.txt\"", "\n", "infile", "=", "\"sotu_raw.txt\"", "\n", "min_year", "=", "1901", "\n", "\n", "words_per_speech", "=", "{", "}", "# total number of lines per speech", "\n", "docs", "=", "[", "]", "\n", "keys", "=", "[", "]", "\n", "year", "=", "None", "\n", "word_count", "=", "0", "\n", "with", "open", "(", "data_dir", "+", "infile", ",", "\"r\"", ")", "as", "ifile", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "ifile", ")", ":", "\n", "            ", "line", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", "\n", "\n", "if", "line", "[", "0", ":", "3", "]", "==", "\"***\"", ":", "\n", "# this line is the beginning of a new speech", "\n", "                ", "fields", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "author", "=", "fields", "[", "1", "]", ".", "replace", "(", "' '", ",", "'_'", ")", ".", "replace", "(", "'.'", ",", "'_'", ")", ".", "replace", "(", "\",\"", ",", "\"_\"", ")", "\n", "year", "=", "int", "(", "fields", "[", "2", "]", "[", "-", "4", ":", "]", ")", "\n", "speech", "=", "str", "(", "year", ")", "+", "\"_\"", "+", "author", "\n", "word_count", "=", "0", "\n", "text", "=", "fields", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "# another line from the same speech", "\n", "                ", "text", "=", "line", "\n", "\n", "", "if", "year", "<", "min_year", ":", "\n", "                ", "continue", "\n", "\n", "", "doc", "=", "clean_text", "(", "text", ")", "\n", "docs", ".", "append", "(", "doc", ")", "\n", "keys", ".", "append", "(", "(", "speech", ",", "word_count", ")", ")", "# save word count at the start of this document", "\n", "words_per_speech", "[", "speech", "]", "=", "word_count", "\n", "word_count", "+=", "len", "(", "doc", ")", "\n", "\n", "\n", "", "", "print", "(", "\"Number of words in each speech:\"", ")", "\n", "for", "a", ",", "n", "in", "words_per_speech", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "a", ",", "n", ")", "\n", "\n", "\n", "\n", "# normalize timesteps so each document's \"time step\" indicates position in speech [0, 100]", "\n", "", "norm_keys", "=", "[", "]", "\n", "for", "speech", ",", "word_count", "in", "keys", ":", "\n", "        ", "norm_keys", ".", "append", "(", "(", "speech", ",", "int", "(", "round", "(", "100.0", "*", "word_count", "/", "words_per_speech", "[", "speech", "]", ")", ")", ")", ")", "\n", "\n", "", "keys", "=", "norm_keys", "\n", "# sort documents by time then speech", "\n", "docs", "=", "np", ".", "array", "(", "docs", ")", "\n", "keys", "=", "np", ".", "array", "(", "keys", ")", "\n", "sorting_values", "=", "np", ".", "array", "(", "[", "str", "(", "n", ")", ".", "zfill", "(", "4", ")", "+", "a", "for", "a", ",", "n", "in", "zip", "(", "keys", "[", ":", ",", "0", "]", ",", "keys", "[", ":", ",", "1", "]", ")", "]", ")", "\n", "sort_order", "=", "np", ".", "argsort", "(", "sorting_values", ")", "\n", "docs", "=", "docs", "[", "sort_order", "]", "\n", "keys", "=", "keys", "[", "sort_order", "]", "\n", "\n", "\n", "# how many docs per time step", "\n", "timestep_counts", "=", "{", "}", "\n", "for", "_", ",", "timestep", "in", "keys", ":", "\n", "        ", "ts", "=", "int", "(", "timestep", ")", "\n", "if", "ts", "not", "in", "timestep_counts", ":", "\n", "            ", "timestep_counts", "[", "ts", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "timestep_counts", "[", "ts", "]", "+=", "1", "\n", "\n", "# write out the keys and cleaned text to seperate file, possibly useful later", "\n", "", "", "with", "open", "(", "data_dir", "+", "\"sotu_bow.txt\"", ",", "\"w\"", ")", "as", "outfile", ":", "\n", "        ", "for", "(", "speech", ",", "pct_in_speech", ")", ",", "doc", "in", "zip", "(", "keys", ",", "docs", ")", ":", "\n", "            ", "outfile", ".", "write", "(", "str", "(", "pct_in_speech", ")", "+", "\"\\t\"", "+", "str", "(", "speech", ")", "+", "\"\\t\"", "+", "' '", ".", "join", "(", "doc", ")", "+", "\"\\n\"", ")", "\n", "\n", "\n", "", "", "resort", "=", "False", "\n", "if", "resort", ":", "\n", "        ", "print", "(", "\"sorting keys and docs\"", ")", "\n", "cmd", "=", "\"\"\"/bin/bash -c \"sort %s -n -t $'\\t' -k1,1 -k2,2 -o %s -S %s\" \"\"\"", "%", "(", "\"sotu_bow.txt\"", ",", "\"sotu_bow.txt\"", ",", "\"75%\"", ")", "\n", "subprocess", ".", "call", "(", "cmd", ",", "shell", "=", "True", ")", "\n", "\n", "\n", "", "build_corpus", "=", "True", "\n", "if", "build_corpus", ":", "\n", "        ", "print", "(", "\"creating vocab\"", ")", "\n", "docs", "=", "doc_generator", "(", "data_dir", "+", "\"sotu_bow.txt\"", ")", "\n", "vocab", "=", "corpora", ".", "Dictionary", "(", "docs", ")", "\n", "vocab", ".", "filter_extremes", "(", "no_below", "=", "3", ",", "no_above", "=", "0.95", ",", "keep_n", "=", "10000", ")", "\n", "vocab", ".", "compactify", "(", ")", "\n", "\n", "corpus", "=", "(", "vocab", ".", "doc2bow", "(", "tokens", ")", "for", "tokens", "in", "doc_generator", "(", "data_dir", "+", "\"sotu_bow.txt\"", ")", ")", "\n", "corpora", ".", "MmCorpus", ".", "serialize", "(", "data_dir", "+", "'sotu.mm'", ",", "corpus", ")", "\n", "corpus", "=", "corpora", ".", "MmCorpus", "(", "data_dir", "+", "'sotu.mm'", ")", "\n", "corpora", ".", "BleiCorpus", ".", "save_corpus", "(", "data_dir", "+", "'sotu.ldac'", ",", "corpus", ",", "id2word", "=", "vocab", ")", "\n", "os", ".", "remove", "(", "data_dir", "+", "\"sotu.mm\"", ")", "\n", "os", ".", "remove", "(", "data_dir", "+", "\"sotu.mm.index\"", ")", "\n", "os", ".", "rename", "(", "data_dir", "+", "\"sotu.ldac.vocab\"", ",", "data_dir", "+", "\"sotu_vocab.txt\"", ")", "\n", "del", "docs", "\n", "del", "corpus", "\n", "del", "vocab", "\n", "\n", "# write out data in format for DAP model:", "\n", "# total_timesteps", "\n", "# timestep[0]", "\n", "# num_docs[t=0]", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# ...", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# ...", "\n", "# timestep[T]", "\n", "# num_docs[t=T]", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "", "dap", "=", "True", "\n", "if", "dap", ":", "\n", "        ", "num_timesteps", "=", "len", "(", "timestep_counts", ")", "\n", "lda_file", "=", "open", "(", "data_dir", "+", "\"sotu.ldac\"", ",", "\"r\"", ")", "\n", "prev_ts", "=", "-", "1", "\n", "with", "open", "(", "data_dir", "+", "\"sotu_full.txt\"", ",", "\"w\"", ")", "as", "dap_file", ":", "\n", "            ", "dap_file", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "for", "ts", ",", "speech", "in", "keys_generator", "(", "data_dir", "+", "\"sotu_bow.txt\"", ")", ":", "\n", "                ", "if", "ts", "!=", "prev_ts", ":", "\n", "# new time step", "\n", "                    ", "dap_file", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "dap_file", ".", "write", "(", "str", "(", "timestep_counts", "[", "ts", "]", ")", "+", "\"\\n\"", ")", "\n", "\n", "# otherwise, write out next speech + doc", "\n", "", "ldac", "=", "lda_file", ".", "readline", "(", ")", "\n", "dap_file", ".", "write", "(", "speech", "+", "\" \"", "+", "ldac", ")", "\n", "prev_ts", "=", "ts", "\n", "\n", "", "", "lda_file", ".", "close", "(", ")", "\n", "os", ".", "remove", "(", "data_dir", "+", "'sotu.ldac'", ")", "\n", "\n", "", "split", "=", "True", "\n", "if", "split", ":", "\n", "# split DAP file into training and test sets", "\n", "        ", "print", "(", "\"Split into training and test sets\"", ")", "\n", "pct_train", "=", "0.1", "\n", "with", "open", "(", "data_dir", "+", "\"sotu_full.txt\"", ",", "\"r\"", ")", "as", "dap_file", ",", "open", "(", "data_dir", "+", "\"sotu_train.txt\"", ",", "\"w\"", ")", "as", "train", ",", "open", "(", "data_dir", "+", "\"sotu_test.txt\"", ",", "\"w\"", ")", "as", "test", ":", "\n", "            ", "num_timesteps", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "train", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "for", "t", "in", "range", "(", "num_timesteps", ")", ":", "\n", "                ", "ts", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "num_docs_t", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "print", "(", "\"t:\"", ",", "t", ",", "\"total:\"", ",", "num_docs_t", ")", "\n", "test_ids", "=", "np", ".", "random", ".", "choice", "(", "num_docs_t", ",", "size", "=", "int", "(", "np", ".", "ceil", "(", "num_docs_t", "*", "pct_train", ")", ")", ",", "replace", "=", "False", ")", "\n", "train_ids", "=", "np", ".", "delete", "(", "np", ".", "arange", "(", "num_docs_t", ")", ",", "test_ids", ")", "\n", "print", "(", "\"\\ttrain:\"", ",", "len", "(", "train_ids", ")", ",", "\"test:\"", ",", "len", "(", "test_ids", ")", ")", "\n", "train", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "train", ".", "write", "(", "str", "(", "len", "(", "train_ids", ")", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "len", "(", "test_ids", ")", ")", "+", "\"\\n\"", ")", "\n", "for", "i", "in", "range", "(", "num_docs_t", ")", ":", "\n", "                    ", "doc", "=", "dap_file", ".", "readline", "(", ")", "\n", "if", "i", "in", "test_ids", ":", "\n", "                        ", "test", ".", "write", "(", "doc", ")", "\n", "", "else", ":", "\n", "                        ", "train", ".", "write", "(", "doc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.stopwords.__init__": [[20, 47], ["set", "type", "list", "list"], "methods", ["None"], ["\n", "\n", "", "def", "softmax", "(", "x", ",", "axis", ")", ":", "\n", "    ", "\"\"\"\n    Softmax for normalizing a matrix along an axis\n    Use max substraction approach for numerical stability\n    :param x:\n    :return:\n    \"\"\"", "\n", "e_x", "=", "np", ".", "exp", "(", "x", "-", "np", ".", "max", "(", "x", ")", ")", "\n", "return", "e_x", "/", "np", ".", "sum", "(", "e_x", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n", "\n", "", "def", "sum_normalize", "(", "mat", ",", "axis", ")", ":", "\n", "    ", "\"\"\"\n    normalize to a probability distribution, first convert any\n    negative numbers if they exist\n    :param mat:\n    :param axis:\n    :return:\n    \"\"\"", "\n", "pos_mat", "=", "mat", "-", "np", ".", "min", "(", "mat", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "+", "0.001", "\n", "return", "pos_mat", "/", "np", ".", "sum", "(", "pos_mat", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n", "\n", "", "def", "dirichlet_expectation", "(", "dirichlet_parameter", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.preprocessing_regex.__init__": [[53, 93], ["re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile"], "methods", ["None"], ["", "return", "psi", "(", "dirichlet_parameter", ")", "-", "psi", "(", "np", ".", "sum", "(", "dirichlet_parameter", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "\n", "\n", "", "def", "matrix2str", "(", "mat", ",", "num_digits", "=", "2", ")", ":", "\n", "    ", "\"\"\"\n    take a matrix (either list of lists of numpy array) and put it in a\n    pretty printable format.\n    :param mat: matrix to print\n    :param num_digits: how many significant digits to show\n    :return:\n    \"\"\"", "\n", "rval", "=", "''", "\n", "for", "row", "in", "mat", ":", "\n", "        ", "s", "=", "'{:.'", "+", "str", "(", "num_digits", ")", "+", "'}'", "\n", "# rval += '\\t'.join([s.format(round(elt, num_digits)) for elt in row]) + '\\n'", "\n", "fpad", "=", "[", "''", "if", "round", "(", "elt", ",", "num_digits", ")", "<", "0", "else", "' '", "for", "elt", "in", "row", "]", "\n", "bpad", "=", "[", "' '", "*", "(", "7", "-", "len", "(", "str", "(", "np", ".", "abs", "(", "round", "(", "elt", ",", "num_digits", ")", ")", ")", ")", ")", "for", "elt", "in", "row", "]", "\n", "rval", "+=", "''", ".", "join", "(", "[", "f", "+", "s", ".", "format", "(", "round", "(", "elt", ",", "num_digits", ")", ")", "+", "b", "for", "elt", ",", "f", ",", "b", "in", "zip", "(", "row", ",", "fpad", ",", "bpad", ")", "]", ")", "+", "'\\n'", "\n", "", "return", "rval", "", "", ""]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.get_wordnet_pos": [[95, 113], ["treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_asoiaf.clean_text": [[41, 89], ["punct.sub.lower", "re.sub", "iam.sub", "ive.sub", "hes.sub", "shes.sub", "weve.sub", "youve.sub", "willnot.sub", "cannot.sub", "itis.sub", "letus.sub", "heis.sub", "sheis.sub", "howis.sub", "thatis.sub", "thereis.sub", "whatis.sub", "whereis.sub", "whenis.sub", "whois.sub", "whyis.sub", "youall.sub", "youare.sub", "would.sub", "will.sub", "s_apostrophe.sub", "has.sub", "nt.sub", "have.sub", "punct.sub", "punct.sub.split"], "function", ["None"], ["def", "clean_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Defines how to clean each of the texts\n    :param text:\n    :return:\n    \"\"\"", "\n", "# all to lowercase", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "\n", "# treat hyphens between words as a single word", "\n", "text", "=", "re", ".", "sub", "(", "r\"([a-zA-Z])\\-([a-zA-Z])\"", ",", "r\"\\1_\\2\"", ",", "text", ")", "\n", "\n", "# expand contractions", "\n", "text", "=", "iam", ".", "sub", "(", "\"i am\"", ",", "text", ")", "\n", "text", "=", "ive", ".", "sub", "(", "\"i have\"", ",", "text", ")", "\n", "text", "=", "hes", ".", "sub", "(", "\"he is\"", ",", "text", ")", "\n", "text", "=", "shes", ".", "sub", "(", "\"she is\"", ",", "text", ")", "\n", "text", "=", "weve", ".", "sub", "(", "\"we have\"", ",", "text", ")", "\n", "text", "=", "youve", ".", "sub", "(", "\"you have\"", ",", "text", ")", "\n", "text", "=", "willnot", ".", "sub", "(", "\"will not\"", ",", "text", ")", "\n", "text", "=", "cannot", ".", "sub", "(", "\"can not\"", ",", "text", ")", "\n", "text", "=", "itis", ".", "sub", "(", "\"it is\"", ",", "text", ")", "\n", "text", "=", "letus", ".", "sub", "(", "\"let us\"", ",", "text", ")", "\n", "text", "=", "heis", ".", "sub", "(", "\"he is\"", ",", "text", ")", "\n", "text", "=", "sheis", ".", "sub", "(", "\"she is\"", ",", "text", ")", "\n", "text", "=", "howis", ".", "sub", "(", "\"how is\"", ",", "text", ")", "\n", "text", "=", "thatis", ".", "sub", "(", "\"that is\"", ",", "text", ")", "\n", "text", "=", "thereis", ".", "sub", "(", "\"there is\"", ",", "text", ")", "\n", "text", "=", "whatis", ".", "sub", "(", "\"what is\"", ",", "text", ")", "\n", "text", "=", "whereis", ".", "sub", "(", "\"where is\"", ",", "text", ")", "\n", "text", "=", "whenis", ".", "sub", "(", "\"when is\"", ",", "text", ")", "\n", "text", "=", "whois", ".", "sub", "(", "\"who is\"", ",", "text", ")", "\n", "text", "=", "whyis", ".", "sub", "(", "\"why is\"", ",", "text", ")", "\n", "text", "=", "youall", ".", "sub", "(", "\"you all\"", ",", "text", ")", "\n", "text", "=", "youare", ".", "sub", "(", "\"you are\"", ",", "text", ")", "\n", "text", "=", "would", ".", "sub", "(", "\" would\"", ",", "text", ")", "\n", "text", "=", "will", ".", "sub", "(", "\" will\"", ",", "text", ")", "\n", "text", "=", "s_apostrophe", ".", "sub", "(", "\"s has \"", ",", "text", ")", "\n", "text", "=", "has", ".", "sub", "(", "\" has\"", ",", "text", ")", "\n", "text", "=", "nt", ".", "sub", "(", "\" not\"", ",", "text", ")", "\n", "text", "=", "have", ".", "sub", "(", "\" have\"", ",", "text", ")", "\n", "\n", "# remove punctuation", "\n", "text", "=", "punct", ".", "sub", "(", "\" \"", ",", "text", ")", "\n", "\n", "# tokenize and remove stopwords", "\n", "text", "=", "[", "w", "for", "w", "in", "text", ".", "split", "(", ")", "if", "w", "not", "in", "stopword_set", "]", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_asoiaf.doc_generator": [[91, 97], ["open", "line.replace().split", "doc.split", "line.replace"], "function", ["None"], ["", "def", "doc_generator", "(", "fname", ")", ":", "\n", "    ", "with", "open", "(", "fname", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "doc", "=", "fields", "[", "-", "1", "]", "\n", "yield", "doc", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_asoiaf.keys_generator": [[99, 107], ["open", "line.replace().split", "line.replace", "int"], "function", ["None"], ["", "", "", "def", "keys_generator", "(", "fname", ")", ":", "\n", "    ", "\"\"\"\n    returns generator of (timestep, author) pairs\n    \"\"\"", "\n", "with", "open", "(", "fname", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "yield", "int", "(", "fields", "[", "0", "]", ")", ",", "fields", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_asoiaf.main": [[109, 283], ["print", "print", "print", "wpc.items", "numpy.array", "numpy.array", "numpy.array", "numpy.argsort", "print", "print", "open", "print", "norm_keys.append", "int", "open", "zip", "print", "preprocess_asoiaf.doc_generator", "gensim.corpora.Dictionary", "corpora.Dictionary.filter_extremes", "corpora.Dictionary.compactify", "print", "gensim.corpora.MmCorpus.serialize", "gensim.corpora.MmCorpus", "gensim.corpora.BleiCorpus.save_corpus", "os.remove", "os.remove", "os.rename", "print", "dict", "numpy.array", "numpy.sort", "print", "print", "len", "open", "open.close", "os.remove", "print", "line.replace.replace", "outfile.write", "corpora.Dictionary.doc2bow", "list", "open", "dap_file.write", "preprocess_asoiaf.keys_generator", "open", "open", "open", "int", "train.write", "test.write", "range", "preprocess_asoiaf.clean_text", "len", "len", "int", "str().zfill", "zip", "preprocess_asoiaf.doc_generator", "corpora.Dictionary.values", "np.sort.items", "open.readline", "dap_file.write", "dap_file.readline().replace", "int", "int", "numpy.random.choice", "numpy.delete", "train.write", "test.write", "train.write", "test.write", "range", "len", "np.array.append", "np.array.append", "round", "str", "dap_file.write", "dap_file.write", "str", "str", "dap_file.readline().replace", "dap_file.readline().replace", "numpy.arange", "dap_file.readline", "str", "dap_file.readline", "int", "str", "str", "str", "str", "test.write", "train.write", "str", "str", "dap_file.readline", "dap_file.readline", "numpy.ceil", "len", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.keys_generator", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_asoiaf.clean_text", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "print", "(", "stopword_set", ")", "\n", "\n", "data_dir", "=", "\"../../data/asoiaf/\"", "\n", "docs", "=", "[", "]", "\n", "keys", "=", "[", "]", "\n", "characters", "=", "[", "\"AERON\"", ",", "\"AREO\"", ",", "\"ARIANNE\"", ",", "\"ARYA\"", ",", "\"ARYS\"", ",", "\"ASHA\"", ",", "\"BARRISTAN\"", ",", "\"BRAN\"", ",", "\"BRIENNE\"", ",", "\"CATELYN\"", ",", "\"CERSEI\"", ",", "\"CONNINGTON\"", ",", "\"DAENERYS\"", ",", "\"DAVOS\"", ",", "\"EDDARD\"", ",", "\"JAIME\"", ",", "\"JON\"", ",", "\"MELISANDRE\"", ",", "\"QUENTYN\"", ",", "\"SAMWELL\"", ",", "\"SANSA\"", ",", "\"THEON\"", ",", "\"TYRION\"", ",", "\"VICTARION\"", "]", "\n", "chapter_counts", "=", "{", "name", ":", "0", "for", "name", "in", "characters", "}", "\n", "wpc", "=", "{", "name", ":", "0", "for", "name", "in", "characters", "}", "# words per character", "\n", "n", "=", "0", "\n", "total_wc", "=", "0", "\n", "\n", "with", "open", "(", "data_dir", "+", "\"asoiaf.txt\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", "\n", "if", "line", "in", "characters", ":", "\n", "# new chapter", "\n", "                ", "chapter_counts", "[", "line", "]", "+=", "1", "\n", "current_char", "=", "line", "\n", "", "else", ":", "\n", "# continuation of previous chapter", "\n", "                ", "text", "=", "clean_text", "(", "line", ")", "\n", "if", "len", "(", "text", ")", "<", "10", ":", "\n", "# super short line, just add to previous line", "\n", "                    ", "docs", "[", "-", "1", "]", "+=", "text", "\n", "", "else", ":", "\n", "                    ", "keys", ".", "append", "(", "[", "current_char", ",", "wpc", "[", "current_char", "]", "]", ")", "\n", "docs", ".", "append", "(", "text", ")", "\n", "\n", "", "wpc", "[", "current_char", "]", "+=", "len", "(", "text", ")", "\n", "n", "+=", "1", "\n", "total_wc", "+=", "len", "(", "text", ")", "\n", "\n", "", "", "", "print", "(", "\"total_wc / n = {}\"", ".", "format", "(", "1.0", "*", "total_wc", "/", "n", ")", ")", "\n", "\n", "print", "(", "\"Number of words for each character:\"", ")", "\n", "for", "c", ",", "wc", "in", "wpc", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "c", ",", "wc", ")", "\n", "\n", "# normalize timesteps so each document's \"time step\" indicates position in plot [0, 100]", "\n", "", "norm_keys", "=", "[", "]", "\n", "for", "c", ",", "wc", "in", "keys", ":", "\n", "        ", "norm_keys", ".", "append", "(", "(", "c", ",", "int", "(", "round", "(", "999.0", "*", "wc", "/", "wpc", "[", "c", "]", ")", ")", ")", ")", "\n", "\n", "", "keys", "=", "norm_keys", "\n", "# sort documents by time then speech", "\n", "docs", "=", "np", ".", "array", "(", "docs", ")", "\n", "keys", "=", "np", ".", "array", "(", "keys", ")", "\n", "sorting_values", "=", "np", ".", "array", "(", "[", "str", "(", "n", ")", ".", "zfill", "(", "4", ")", "+", "a", "for", "a", ",", "n", "in", "zip", "(", "keys", "[", ":", ",", "0", "]", ",", "keys", "[", ":", ",", "1", "]", ")", "]", ")", "\n", "sort_order", "=", "np", ".", "argsort", "(", "sorting_values", ")", "\n", "docs", "=", "docs", "[", "sort_order", "]", "\n", "keys", "=", "keys", "[", "sort_order", "]", "\n", "\n", "\n", "# how many docs per time step", "\n", "timestep_counts", "=", "{", "}", "\n", "for", "_", ",", "ts", "in", "keys", ":", "\n", "        ", "ts", "=", "int", "(", "ts", ")", "\n", "if", "ts", "not", "in", "timestep_counts", ":", "\n", "            ", "timestep_counts", "[", "ts", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "timestep_counts", "[", "ts", "]", "+=", "1", "\n", "\n", "", "", "print", "(", "\"docs per timestep:\"", ",", "timestep_counts", ")", "\n", "\n", "# write out the keys and cleaned text to seperate file, possibly useful later", "\n", "with", "open", "(", "data_dir", "+", "\"asoiaf_bow.txt\"", ",", "\"w\"", ")", "as", "outfile", ":", "\n", "        ", "for", "(", "character", ",", "pct_in_arc", ")", ",", "doc", "in", "zip", "(", "keys", ",", "docs", ")", ":", "\n", "            ", "outfile", ".", "write", "(", "str", "(", "pct_in_arc", ")", "+", "\"\\t\"", "+", "str", "(", "character", ")", "+", "\"\\t\"", "+", "' '", ".", "join", "(", "doc", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "build_corpus", "=", "True", "\n", "if", "build_corpus", ":", "\n", "        ", "print", "(", "\"creating vocab\"", ")", "\n", "bow", "=", "doc_generator", "(", "data_dir", "+", "\"asoiaf_bow.txt\"", ")", "\n", "vocab", "=", "corpora", ".", "Dictionary", "(", "bow", ")", "\n", "vocab", ".", "filter_extremes", "(", "no_above", "=", "0.5", ",", "keep_n", "=", "10000", ")", "\n", "vocab", ".", "compactify", "(", ")", "\n", "\n", "print", "(", "\"creating corpus\"", ")", "\n", "corpus", "=", "(", "vocab", ".", "doc2bow", "(", "tokens", ")", "for", "tokens", "in", "doc_generator", "(", "data_dir", "+", "\"asoiaf_bow.txt\"", ")", ")", "\n", "corpora", ".", "MmCorpus", ".", "serialize", "(", "data_dir", "+", "'asoiaf.mm'", ",", "corpus", ")", "\n", "corpus", "=", "corpora", ".", "MmCorpus", "(", "data_dir", "+", "'asoiaf.mm'", ")", "\n", "corpora", ".", "BleiCorpus", ".", "save_corpus", "(", "data_dir", "+", "'asoiaf.ldac'", ",", "corpus", ",", "id2word", "=", "vocab", ")", "\n", "os", ".", "remove", "(", "data_dir", "+", "\"asoiaf.mm\"", ")", "\n", "os", ".", "remove", "(", "data_dir", "+", "\"asoiaf.mm.index\"", ")", "\n", "os", ".", "rename", "(", "data_dir", "+", "\"asoiaf.ldac.vocab\"", ",", "data_dir", "+", "\"asoiaf_vocab.txt\"", ")", "\n", "\n", "# print term frequencies", "\n", "print", "(", "\"word freq\"", ")", "\n", "word_counts", "=", "{", "v", ":", "0", "for", "v", "in", "vocab", ".", "values", "(", ")", "}", "\n", "\n", "for", "doc", "in", "docs", ":", "\n", "            ", "for", "word", "in", "doc", ":", "\n", "                ", "if", "word", "in", "word_counts", ":", "\n", "                    ", "word_counts", "[", "word", "]", "+=", "1", "\n", "\n", "\n", "", "", "", "names", "=", "[", "'word'", ",", "'count'", "]", "\n", "formats", "=", "[", "'S100'", ",", "'int'", "]", "\n", "dtype", "=", "dict", "(", "names", "=", "names", ",", "formats", "=", "formats", ")", "\n", "word_counts", "=", "np", ".", "array", "(", "list", "(", "word_counts", ".", "items", "(", ")", ")", ",", "dtype", "=", "dtype", ")", "\n", "word_counts", "=", "np", ".", "sort", "(", "word_counts", ",", "order", "=", "[", "'count'", "]", ")", "\n", "print", "(", "word_counts", "[", "-", "50", ":", "]", ")", "\n", "print", "(", "word_counts", "[", "0", ":", "50", "]", ")", "\n", "\n", "del", "docs", "\n", "del", "corpus", "\n", "del", "vocab", "\n", "\n", "\n", "\n", "# write out data in format for DAP model:", "\n", "# total_timesteps", "\n", "# timestep[0]", "\n", "# num_docs[t=0]", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# ...", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# ...", "\n", "# timestep[T]", "\n", "# num_docs[t=T]", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "# author num_terms term_0:count ... term_n:count", "\n", "", "print", "(", "\"dap format\"", ")", "\n", "dap", "=", "True", "\n", "if", "dap", ":", "\n", "        ", "num_timesteps", "=", "len", "(", "timestep_counts", ")", "\n", "lda_file", "=", "open", "(", "data_dir", "+", "\"asoiaf.ldac\"", ",", "\"r\"", ")", "\n", "prev_ts", "=", "-", "1", "\n", "with", "open", "(", "data_dir", "+", "\"asoiaf_full.txt\"", ",", "\"w\"", ")", "as", "dap_file", ":", "\n", "            ", "dap_file", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "for", "ts", ",", "character", "in", "keys_generator", "(", "data_dir", "+", "\"asoiaf_bow.txt\"", ")", ":", "\n", "                ", "if", "ts", "!=", "prev_ts", ":", "\n", "# new time step", "\n", "                    ", "dap_file", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "dap_file", ".", "write", "(", "str", "(", "timestep_counts", "[", "ts", "]", ")", "+", "\"\\n\"", ")", "\n", "\n", "# otherwise, write out next character + doc", "\n", "", "ldac", "=", "lda_file", ".", "readline", "(", ")", "\n", "dap_file", ".", "write", "(", "character", "+", "\" \"", "+", "ldac", ")", "\n", "prev_ts", "=", "ts", "\n", "\n", "", "", "lda_file", ".", "close", "(", ")", "\n", "os", ".", "remove", "(", "data_dir", "+", "'asoiaf.ldac'", ")", "\n", "\n", "", "split", "=", "True", "\n", "if", "split", ":", "\n", "# split DAP file into training and test sets", "\n", "        ", "print", "(", "\"Split into training and test sets\"", ")", "\n", "pct_train", "=", "0.1", "\n", "with", "open", "(", "data_dir", "+", "\"asoiaf_full.txt\"", ",", "\"r\"", ")", "as", "dap_file", ",", "open", "(", "data_dir", "+", "\"asoiaf_train.txt\"", ",", "\"w\"", ")", "as", "train", ",", "open", "(", "data_dir", "+", "\"asoiaf_test.txt\"", ",", "\"w\"", ")", "as", "test", ":", "\n", "            ", "num_timesteps", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "train", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "for", "t", "in", "range", "(", "num_timesteps", ")", ":", "\n", "                ", "ts", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "num_docs_t", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "# print(\"t:\", t, \"total:\", num_docs_t)", "\n", "test_ids", "=", "np", ".", "random", ".", "choice", "(", "num_docs_t", ",", "size", "=", "int", "(", "np", ".", "ceil", "(", "num_docs_t", "*", "pct_train", ")", ")", ",", "replace", "=", "False", ")", "\n", "train_ids", "=", "np", ".", "delete", "(", "np", ".", "arange", "(", "num_docs_t", ")", ",", "test_ids", ")", "\n", "# print(\"\\ttrain:\", len(train_ids), \"test:\", len(test_ids))", "\n", "train", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "train", ".", "write", "(", "str", "(", "len", "(", "train_ids", ")", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "len", "(", "test_ids", ")", ")", "+", "\"\\n\"", ")", "\n", "for", "i", "in", "range", "(", "num_docs_t", ")", ":", "\n", "                    ", "doc", "=", "dap_file", ".", "readline", "(", ")", "\n", "if", "i", "in", "test_ids", ":", "\n", "                        ", "test", ".", "write", "(", "doc", ")", "\n", "", "else", ":", "\n", "                        ", "train", ".", "write", "(", "doc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.keys_generator": [[25, 37], ["open", "line.replace().split", "int", "datetime.datetime.strptime", "line.replace"], "function", ["None"], ["def", "keys_generator", "(", "fname", ",", "rel_date", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    returns generator of (source, date) pairs\n    \"\"\"", "\n", "with", "open", "(", "fname", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "rel_date", ":", "\n", "                ", "t", "=", "int", "(", "fields", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "t", "=", "dt", ".", "datetime", ".", "strptime", "(", "fields", "[", "1", "]", ",", "\"%Y-%m-%d %H:%M:%S\"", ")", "\n", "", "yield", "fields", "[", "0", "]", ",", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator": [[39, 45], ["open", "line.replace().split", "doc.split", "line.replace"], "function", ["None"], ["", "", "", "def", "doc_generator", "(", "fname", ")", ":", "\n", "    ", "with", "open", "(", "fname", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "doc", "=", "fields", "[", "-", "1", "]", "\n", "yield", "doc", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.text2corpus": [[47, 57], ["preprocess_signalmedia.doc_generator", "gensim.corpora.Dictionary", "corpora.Dictionary.filter_extremes", "corpora.Dictionary.compactify", "gensim.corpora.BleiCorpus.save_corpus", "corpora.Dictionary.doc2bow", "preprocess_signalmedia.doc_generator"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.doc_generator"], ["", "", "", "def", "text2corpus", "(", "input_fname", ",", "corpus_fname", ",", "keep_n", "=", "25000", ")", ":", "\n", "# define the the vocabulary", "\n", "    ", "docs", "=", "doc_generator", "(", "input_fname", ")", "\n", "vocab", "=", "corpora", ".", "Dictionary", "(", "docs", ")", "\n", "vocab", ".", "filter_extremes", "(", "no_above", "=", "0.5", ",", "keep_n", "=", "keep_n", ")", "\n", "vocab", ".", "compactify", "(", ")", "\n", "\n", "# create BOW corpus", "\n", "corpus", "=", "(", "vocab", ".", "doc2bow", "(", "tokens", ")", "for", "tokens", "in", "doc_generator", "(", "input_fname", ")", ")", "\n", "corpora", ".", "BleiCorpus", ".", "save_corpus", "(", "corpus_fname", ",", "corpus", ",", "id2word", "=", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.compute_relative_dates": [[59, 93], ["list", "datetime.datetime.today", "collections.Counter", "os.rename", "os.remove", "preprocess_signalmedia.keys_generator", "int", "rel_dts.append", "open", "open", "zip", "round", "line.replace().split", "fout.write", "line.replace", "str"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.keys_generator"], ["", "def", "compute_relative_dates", "(", "fname", ")", ":", "\n", "    ", "keys", "=", "list", "(", "keys_generator", "(", "fname", ")", ")", "\n", "# identify first date for each src", "\n", "src_min", "=", "{", "}", "\n", "overall_min", "=", "dt", ".", "datetime", ".", "today", "(", ")", "\n", "for", "src", ",", "d", "in", "keys", ":", "\n", "        ", "if", "d", "<", "overall_min", ":", "\n", "            ", "overall_min", "=", "d", "\n", "\n", "", "if", "src", "not", "in", "src_min", ":", "\n", "            ", "src_min", "[", "src", "]", "=", "d", "\n", "", "else", ":", "\n", "            ", "if", "src_min", "[", "src", "]", ">", "d", ":", "\n", "                ", "src_min", "[", "src", "]", "=", "d", "\n", "\n", "# compute relative dates", "\n", "", "", "", "rel_dts", "=", "[", "]", "\n", "for", "src", ",", "d", "in", "keys", ":", "\n", "        ", "days_dif", "=", "int", "(", "round", "(", "(", "d", "-", "overall_min", ")", ".", "days", "+", "(", "(", "d", "-", "overall_min", ")", ".", "seconds", "/", "60.", "/", "60.", "/", "24.", ")", ")", ")", "\n", "rel_dts", ".", "append", "(", "days_dif", ")", "\n", "\n", "# compute counts per time step", "\n", "", "time_counts", "=", "Counter", "(", "rel_dts", ")", "\n", "\n", "# replace the current date in the data with the relative date.", "\n", "tmp_file", "=", "\"tmp.txt\"", "\n", "os", ".", "rename", "(", "fname", ",", "tmp_file", ")", "\n", "with", "open", "(", "tmp_file", ",", "\"r\"", ")", "as", "fin", ",", "open", "(", "fname", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "for", "line", ",", "rel_dt", "in", "zip", "(", "fin", ",", "rel_dts", ")", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "arr", "=", "[", "fields", "[", "0", "]", ",", "rel_dt", ",", "fields", "[", "2", "]", "]", "\n", "fout", ".", "write", "(", "'\\t'", ".", "join", "(", "[", "str", "(", "s", ")", "for", "s", "in", "arr", "]", ")", "+", "\"\\n\"", ")", "\n", "", "", "os", ".", "remove", "(", "tmp_file", ")", "\n", "return", "time_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.dappify": [[95, 113], ["len", "open", "open.close", "open", "fout.write", "preprocess_signalmedia.keys_generator", "open.readline", "fout.write", "str", "fout.write", "fout.write", "str", "str"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.keys_generator"], ["", "def", "dappify", "(", "time_counts", ",", "keys_fname", ",", "corpus_fname", ",", "full_fname", ")", ":", "\n", "    ", "num_timesteps", "=", "len", "(", "time_counts", ")", "\n", "lda_file", "=", "open", "(", "corpus_fname", ",", "\"r\"", ")", "\n", "prev_ts", "=", "-", "1", "\n", "with", "open", "(", "full_fname", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "fout", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "for", "src", ",", "ts", "in", "keys_generator", "(", "keys_fname", ",", "rel_date", "=", "True", ")", ":", "\n", "            ", "if", "ts", "!=", "prev_ts", ":", "\n", "# new time step", "\n", "                ", "fout", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "fout", ".", "write", "(", "str", "(", "time_counts", "[", "ts", "]", ")", "+", "\"\\n\"", ")", "\n", "\n", "# otherwise, write out next source + doc", "\n", "", "ldac", "=", "lda_file", ".", "readline", "(", ")", "\n", "fout", ".", "write", "(", "src", "+", "\" \"", "+", "ldac", ")", "\n", "prev_ts", "=", "ts", "\n", "\n", "", "", "lda_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.split_train_test": [[115, 144], ["open", "open", "open", "int", "train.write", "test.write", "range", "dap_file.readline().replace", "int", "int", "print", "numpy.random.choice", "numpy.delete", "train.write", "test.write", "train.write", "test.write", "range", "str", "str", "dap_file.readline().replace", "dap_file.readline().replace", "numpy.arange", "dap_file.readline", "dap_file.readline", "int", "str", "str", "str", "str", "test.write", "train.write", "dap_file.readline", "dap_file.readline", "numpy.ceil", "len", "len"], "function", ["None"], ["", "def", "split_train_test", "(", "full_fname", ",", "train_fname", ",", "test_fname", ",", "test_ratio", "=", "0.0", ")", ":", "\n", "# split DAP file into training and test sets", "\n", "    ", "with", "open", "(", "full_fname", ",", "\"r\"", ")", "as", "dap_file", ",", "open", "(", "train_fname", ",", "\"w\"", ")", "as", "train", ",", "open", "(", "test_fname", ",", "\"w\"", ")", "as", "test", ":", "\n", "\n", "        ", "num_timesteps", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "train", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "num_timesteps", ")", "+", "\"\\n\"", ")", "\n", "\n", "for", "t", "in", "range", "(", "num_timesteps", ")", ":", "\n", "            ", "ts", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "num_docs_t", "=", "int", "(", "dap_file", ".", "readline", "(", ")", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", "\n", "print", "(", "\"t:\"", ",", "t", ",", "\"total:\"", ",", "num_docs_t", ")", "\n", "\n", "test_ids", "=", "np", ".", "random", ".", "choice", "(", "num_docs_t", ",", "size", "=", "int", "(", "np", ".", "ceil", "(", "num_docs_t", "*", "test_ratio", ")", ")", ",", "replace", "=", "False", ")", "\n", "train_ids", "=", "np", ".", "delete", "(", "np", ".", "arange", "(", "num_docs_t", ")", ",", "test_ids", ")", "\n", "\n", "train", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "ts", ")", "+", "\"\\n\"", ")", "\n", "train", ".", "write", "(", "str", "(", "len", "(", "train_ids", ")", ")", "+", "\"\\n\"", ")", "\n", "test", ".", "write", "(", "str", "(", "len", "(", "test_ids", ")", ")", "+", "\"\\n\"", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_docs_t", ")", ":", "\n", "                ", "doc", "=", "dap_file", ".", "readline", "(", ")", "\n", "if", "i", "in", "test_ids", ":", "\n", "                    ", "test", ".", "write", "(", "doc", ")", "\n", "", "else", ":", "\n", "                    ", "train", ".", "write", "(", "doc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.filter_sources": [[146, 157], ["datetime.datetime", "open", "open", "line.replace().split", "fout.write", "line.replace", "datetime.datetime.strptime"], "function", ["None"], ["", "", "", "", "", "def", "filter_sources", "(", "keep_sources", ",", "input_fname", ",", "filtered_fname", ",", "min_date", "=", "dt", ".", "datetime", "(", "year", "=", "2015", ",", "month", "=", "9", ",", "day", "=", "1", ")", ")", ":", "\n", "# replace the current date in the data with the relative date.", "\n", "    ", "num_docs", "=", "0", "\n", "with", "open", "(", "input_fname", ",", "\"r\"", ")", "as", "fin", ",", "open", "(", "filtered_fname", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "fields", "[", "0", "]", "in", "keep_sources", "and", "dt", ".", "datetime", ".", "strptime", "(", "fields", "[", "1", "]", ",", "\"%Y-%m-%d %H:%M:%S\"", ")", ">=", "min_date", ":", "\n", "                ", "fout", ".", "write", "(", "line", ")", "\n", "num_docs", "+=", "1", "\n", "\n", "", "", "", "return", "num_docs", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.check_for_news": [[159, 166], ["None"], "function", ["None"], ["", "def", "check_for_news", "(", "json_dict", ")", ":", "\n", "    ", "if", "json_dict", "[", "\"media-type\"", "]", "==", "\"News\"", ":", "\n", "        ", "is_news", "=", "True", "\n", "", "else", ":", "\n", "        ", "is_news", "=", "False", "\n", "\n", "", "return", "is_news", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.extract_keys": [[168, 182], ["re.sub", "re.sub", "re.sub", "re.sub", "datetime.datetime.strptime"], "function", ["None"], ["", "def", "extract_keys", "(", "json_dict", ")", ":", "\n", "    ", "\"\"\"\n    extract keys from the json data\n    :param json_dict:\n    :return:\n    \"\"\"", "\n", "src", "=", "re", ".", "sub", "(", "r\"\\s+\"", ",", "\"_\"", ",", "json_dict", "[", "'source'", "]", ")", "\n", "src", "=", "re", ".", "sub", "(", "r\"[^a-zA-Z0-9_]\"", ",", "\"_\"", ",", "src", ")", "\n", "src", "=", "re", ".", "sub", "(", "r\"[_]+\"", ",", "\"_\"", ",", "src", ")", "\n", "src", "=", "re", ".", "sub", "(", "r'^([0-9])'", ",", "r'_\\1'", ",", "src", ")", "\n", "published_date", "=", "dt", ".", "datetime", ".", "strptime", "(", "json_dict", "[", "'published'", "]", ",", "\"%Y-%m-%dT%H:%M:%SZ\"", ")", "\n", "# keys = [src, published_date, json_dict[\"media-type\"]]", "\n", "keys", "=", "[", "src", ",", "published_date", "]", "\n", "return", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.extract_text": [[184, 193], ["re.sub"], "function", ["None"], ["", "def", "extract_text", "(", "json_dict", ")", ":", "\n", "    ", "\"\"\"\n    Pull out the raw text from the dictionary and remove newlines\n    :param json_dict:\n    :return:\n    \"\"\"", "\n", "text", "=", "' '", ".", "join", "(", "[", "json_dict", "[", "'title'", "]", ",", "json_dict", "[", "'content'", "]", "]", ")", ".", "strip", "(", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"\\s+\"", ",", "' '", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.scrub_text": [[195, 259], ["regex.punct.sub.lower", "regex.html.sub", "regex.years.sub", "regex.dollars.sub", "regex.percent.sub", "regex.times.sub", "regex.urls.sub", "regex.dates.sub", "regex.special_chars.sub", "regex.emails.sub", "re.sub", "regex.iam.sub", "regex.ive.sub", "regex.hes.sub", "regex.shes.sub", "regex.weve.sub", "regex.youve.sub", "regex.willnot.sub", "regex.cannot.sub", "regex.itis.sub", "regex.letus.sub", "regex.heis.sub", "regex.sheis.sub", "regex.howis.sub", "regex.thatis.sub", "regex.thereis.sub", "regex.whatis.sub", "regex.whereis.sub", "regex.whenis.sub", "regex.whois.sub", "regex.whyis.sub", "regex.youall.sub", "regex.youare.sub", "regex.would.sub", "regex.will.sub", "regex.s_apostrophe.sub", "regex.has.sub", "regex.nt.sub", "regex.have.sub", "regex.punct.sub", "lemmatizer.lemmatize().lower", "lemmatizer.lemmatize().lower", "nltk.pos_tag", "src.preprocessing.utilities.get_wordnet_pos", "regex.punct.sub.split", "lemmatizer.lemmatize", "lemmatizer.lemmatize", "src.preprocessing.utilities.get_wordnet_pos"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.get_wordnet_pos", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.utilities.get_wordnet_pos"], ["", "def", "scrub_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n        Defines how to clean each of the texts\n        :param text:\n        :return:\n        \"\"\"", "\n", "# all to lowercase", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "\n", "text", "=", "regex", ".", "html", ".", "sub", "(", "\" \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "years", ".", "sub", "(", "\" _year_ \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "dollars", ".", "sub", "(", "\" _dollars_ \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "percent", ".", "sub", "(", "\" _percent_ \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "times", ".", "sub", "(", "\" _time_ \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "urls", ".", "sub", "(", "\" _url_ \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "dates", ".", "sub", "(", "\" _date_ \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "special_chars", ".", "sub", "(", "\" \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "emails", ".", "sub", "(", "\" _email_ \"", ",", "text", ")", "\n", "\n", "# treat hyphens between words as a single word", "\n", "text", "=", "re", ".", "sub", "(", "r\"([a-zA-Z])\\-([a-zA-Z])\"", ",", "r\"\\1_\\2\"", ",", "text", ")", "\n", "\n", "# expand contractions", "\n", "text", "=", "regex", ".", "iam", ".", "sub", "(", "\"i am\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "ive", ".", "sub", "(", "\"i have\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "hes", ".", "sub", "(", "\"he is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "shes", ".", "sub", "(", "\"she is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "weve", ".", "sub", "(", "\"we have\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "youve", ".", "sub", "(", "\"you have\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "willnot", ".", "sub", "(", "\"will not\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "cannot", ".", "sub", "(", "\"can not\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "itis", ".", "sub", "(", "\"it is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "letus", ".", "sub", "(", "\"let us\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "heis", ".", "sub", "(", "\"he is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "sheis", ".", "sub", "(", "\"she is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "howis", ".", "sub", "(", "\"how is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "thatis", ".", "sub", "(", "\"that is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "thereis", ".", "sub", "(", "\"there is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "whatis", ".", "sub", "(", "\"what is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "whereis", ".", "sub", "(", "\"where is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "whenis", ".", "sub", "(", "\"when is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "whois", ".", "sub", "(", "\"who is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "whyis", ".", "sub", "(", "\"why is\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "youall", ".", "sub", "(", "\"you all\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "youare", ".", "sub", "(", "\"you are\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "would", ".", "sub", "(", "\" would\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "will", ".", "sub", "(", "\" will\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "s_apostrophe", ".", "sub", "(", "\"s has \"", ",", "text", ")", "\n", "text", "=", "regex", ".", "has", ".", "sub", "(", "\" has\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "nt", ".", "sub", "(", "\" not\"", ",", "text", ")", "\n", "text", "=", "regex", ".", "have", ".", "sub", "(", "\" have\"", ",", "text", ")", "\n", "\n", "# remove punctuation", "\n", "text", "=", "regex", ".", "punct", ".", "sub", "(", "\" \"", ",", "text", ")", "\n", "\n", "# tokenize and lemmatize", "\n", "text", "=", "[", "lemmatizer", ".", "lemmatize", "(", "w", ",", "pos", "=", "get_wordnet_pos", "(", "p", ")", ")", ".", "lower", "(", ")", "if", "get_wordnet_pos", "(", "p", ")", "!=", "''", "else", "lemmatizer", ".", "lemmatize", "(", "w", ")", ".", "lower", "(", ")", "for", "w", ",", "p", "in", "pos_tag", "(", "text", ".", "split", "(", ")", ")", "]", "\n", "\n", "# remove stopwords", "\n", "text", "=", "[", "w", "for", "w", "in", "text", "if", "w", "not", "in", "stopword_set", "]", "\n", "return", "' '", ".", "join", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.parse_json": [[261, 285], ["collections.Counter", "open", "open", "json.loads", "preprocess_signalmedia.check_for_news", "preprocess_signalmedia.extract_keys", "preprocess_signalmedia.extract_text", "collections.Counter.update", "preprocess_signalmedia.scrub_text", "fout.write", "str"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.check_for_news", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.extract_keys", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.extract_text", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.update", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.scrub_text"], ["", "def", "parse_json", "(", "input", ",", "output", ")", ":", "\n", "    ", "sources", "=", "Counter", "(", ")", "\n", "i", "=", "0", "\n", "with", "open", "(", "input", ",", "'r'", ")", "as", "fin", ",", "open", "(", "output", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "# parse the json into a dictionary", "\n", "            ", "json_dict", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "is_news", "=", "check_for_news", "(", "json_dict", ")", "\n", "if", "is_news", ":", "\n", "                ", "continue", "\n", "\n", "", "i", "+=", "1", "\n", "# pull out the data we need from the text", "\n", "a_key", "=", "extract_keys", "(", "json_dict", ")", "\n", "a_text", "=", "extract_text", "(", "json_dict", ")", "\n", "sources", ".", "update", "(", "[", "a_key", "[", "0", "]", "]", ")", "\n", "\n", "# clean text", "\n", "clean_text", "=", "scrub_text", "(", "a_text", ")", "\n", "\n", "fout", ".", "write", "(", "'\\t'", ".", "join", "(", "[", "str", "(", "s", ")", "for", "s", "in", "a_key", "]", ")", "+", "\"\\t\"", "+", "clean_text", "+", "\"\\n\"", ")", "\n", "\n", "", "", "return", "sources", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.reload_source_keys": [[287, 299], ["collections.Counter", "open", "line.replace().split", "collections.Counter.update", "line.replace"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.sufficient_statistics.SufficientStatistics.update"], ["", "def", "reload_source_keys", "(", "fname", ")", ":", "\n", "    ", "\"\"\"\n    Loop through the already preprocessed text and pull out the keys\n    associated with each news source\n    :param fname: name of the keys_and_text file\n    \"\"\"", "\n", "sources", "=", "Counter", "(", ")", "\n", "with", "open", "(", "fname", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "fields", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "sources", ".", "update", "(", "[", "fields", "[", "0", "]", "]", ")", "\n", "", "", "return", "sources", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.main": [[301, 367], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.abspath", "set", "preprocess_signalmedia.filter_sources", "print", "preprocess_signalmedia.compute_relative_dates", "print", "subprocess.call", "preprocess_signalmedia.text2corpus", "preprocess_signalmedia.dappify", "os.remove", "os.remove", "preprocess_signalmedia.split_train_test", "os.remove", "os.path.dirname", "os.path.join", "os.path.isfile", "FileNotFoundError", "os.path.isfile", "preprocess_signalmedia.reload_source_keys", "preprocess_signalmedia.parse_json", "datetime.datetime", "len"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.filter_sources", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.compute_relative_dates", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.text2corpus", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.dappify", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.split_train_test", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.reload_source_keys", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.preprocessing.preprocess_signalmedia.parse_json"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Preprocess signalmedia data.'", ")", "\n", "parser", ".", "add_argument", "(", "'--min_threshold'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--max_threshold'", ",", "type", "=", "int", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"Directory of where to save DAP data.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--input_file'", ",", "type", "=", "str", ",", "help", "=", "\"full file path to the signalmedia-m.jsonl file.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--keys_and_text_file'", ",", "type", "=", "str", ",", "default", "=", "\"signalmedia_keys_and_text.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "'--filtered_file'", ",", "type", "=", "str", ",", "default", "=", "\"filtered.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "'--corpus_file'", ",", "type", "=", "str", ",", "default", "=", "\"signalmedia.bow\"", ")", "\n", "parser", ".", "add_argument", "(", "'--dap_file'", ",", "type", "=", "str", ",", "default", "=", "\"signalmedia.dap\"", ")", "\n", "parser", ".", "add_argument", "(", "'--test_ratio'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--vocab_size'", ",", "type", "=", "int", ",", "default", "=", "25000", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "keys_and_text_fname", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "keys_and_text_file", ")", "\n", "filtered_fname", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "filtered_file", ")", "\n", "corpus_fname", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "corpus_file", ")", "\n", "dap_all_fname", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "args", ".", "dap_file", ")", "\n", "train_fname", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"train_\"", "+", "args", ".", "dap_file", ")", "\n", "test_fname", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"test_\"", "+", "args", ".", "dap_file", ")", "\n", "\n", "path_to_current_file", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ")", "\n", "if", "args", ".", "input_file", "is", "None", ":", "\n", "        ", "input_file", "=", "os", ".", "path", ".", "join", "(", "path_to_current_file", ",", "\"../../data/signalmedia/signalmedia-1m.jsonl\"", ")", "\n", "", "else", ":", "\n", "        ", "input_file", "=", "args", ".", "input_file", "\n", "\n", "# check if the raw data exists", "\n", "", "if", "not", "os", ".", "path", ".", "isfile", "(", "input_file", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"Raw Signal Media 1M file not found. See: http://research.signalmedia.co/newsir16/signal-dataset.html to download the raw data.\"", ")", "\n", "\n", "# parse out the keys and save cleaned up text to keys_and_text", "\n", "", "rerun", "=", "False", "\n", "if", "os", ".", "path", ".", "isfile", "(", "keys_and_text_fname", ")", "and", "rerun", "==", "False", ":", "\n", "# don't reprocess all the text, just use previous version and reload the list of new sources", "\n", "        ", "sources", "=", "reload_source_keys", "(", "keys_and_text_fname", ")", "\n", "", "else", ":", "\n", "# reprocess the whole text file, save text to keys_and_text", "\n", "        ", "sources", "=", "parse_json", "(", "input_file", ",", "keys_and_text_fname", ")", "\n", "\n", "# save only the news sources with at least min_threshold articles, but no more than max_threshold", "\n", "", "keep_sources", "=", "set", "(", "[", "src", "for", "src", "in", "sources", "if", "args", ".", "min_threshold", "<=", "sources", "[", "src", "]", "<=", "args", ".", "max_threshold", "]", ")", "\n", "num_docs", "=", "filter_sources", "(", "keep_sources", ",", "keys_and_text_fname", ",", "filtered_fname", ",", "\n", "min_date", "=", "dt", ".", "datetime", "(", "year", "=", "2015", ",", "month", "=", "9", ",", "day", "=", "1", ")", ")", "\n", "print", "(", "\"Keeping {} documents in the corpus and {} sources\"", ".", "format", "(", "num_docs", ",", "len", "(", "keep_sources", ")", ")", ")", "\n", "\n", "# transform the dates into relative dates", "\n", "time_counts", "=", "compute_relative_dates", "(", "filtered_fname", ")", "\n", "print", "(", "\"time counts\"", ",", "time_counts", ")", "\n", "\n", "# sort the filtered data by source and date", "\n", "# TODO: use python to sort data in filtered_fname instead of bash", "\n", "cmd", "=", "\"\"\"/bin/bash -c \"sort %s -n -t $'\\t' -k1,1 -k2,2 -o %s -S %s\" \"\"\"", "%", "(", "filtered_fname", ",", "filtered_fname", ",", "\"75%\"", ")", "\n", "subprocess", ".", "call", "(", "cmd", ",", "shell", "=", "True", ")", "\n", "\n", "# convert texts to bag-of-words format", "\n", "text2corpus", "(", "filtered_fname", ",", "corpus_fname", ",", "keep_n", "=", "args", ".", "vocab_size", ")", "\n", "\n", "# arrange data into DAP format", "\n", "dappify", "(", "time_counts", ",", "filtered_fname", ",", "corpus_fname", ",", "dap_all_fname", ")", "\n", "os", ".", "remove", "(", "filtered_fname", ")", "\n", "os", ".", "remove", "(", "corpus_fname", ")", "\n", "\n", "# split train test", "\n", "split_train_test", "(", "dap_all_fname", ",", "train_fname", ",", "test_fname", ",", "test_ratio", "=", "args", ".", "test_ratio", ")", "\n", "os", ".", "remove", "(", "dap_all_fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.experiments.sotu_wc.main": [[9, 107], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.parse_args", "os.path.abspath", "numpy.random.seed", "src.dapper.DAPPER", "os.path.join", "src.corpus.Corpus", "src.corpus.Corpus", "src.dapper.DAPPER.fit_predict", "print", "os.path.join", "src.dapper.DAPPER.save_topics", "src.dapper.DAPPER.save_author_personas", "src.dapper.DAPPER.save_persona_topics", "src.dapper.DAPPER.save_convergnces", "src.dapper.DAPPER.save_convergnces", "os.path.dirname", "logging.disable", "os.path.join", "int", "int", "int", "int", "time.strftime", "logging.basicConfig", "logging.basicConfig", "time.strftime", "int", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.fit_predict", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_topics", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_author_personas", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_persona_topics"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n    Example of call main program\n    :return:\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Run dap model on state of the union data.'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "type", "=", "int", ",", "\n", "help", "=", "\"If given a test file, number of EM iterations between evaluations of test set. Default of 0 = evaluate after each epoch.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_training_minutes'", ",", "type", "=", "float", ",", "\n", "help", "=", "\"If given this will stop training once the specified number of minutes have elapsed.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--normalization'", ",", "type", "=", "str", ",", "default", "=", "\"sum\"", ",", "\n", "help", "=", "'Method for normalizing alpha values. Can be sum, none, or softmax.'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_epochs'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--max_local_iters'", ",", "type", "=", "int", ",", "default", "=", "30", ",", "help", "=", "\"max iterations to run on local parameters.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--local_convergence'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ",", "\n", "help", "=", "\"Convergence threshold for e-step.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--process_noise'", ",", "type", "=", "float", ",", "default", "=", "0.2", ")", "\n", "parser", ".", "add_argument", "(", "'--measurement_noise'", ",", "type", "=", "float", ",", "default", "=", "0.8", ")", "\n", "parser", ".", "add_argument", "(", "'--num_topics'", ",", "type", "=", "int", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'--num_personas'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--regularization'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"How much to penalize similar personas. Recommend [0, 0.5].\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Batch size. Set to -1 for full gradient updates, else stochastic minibatches used.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_offset'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "\"Learning offset used to control rate of convergence of gradient updates.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_decay'", ",", "type", "=", "float", ",", "default", "=", "0.7", ",", "\n", "help", "=", "\"Learning decay rate used to control rate of convergence of gradient updates.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--step_size'", ",", "type", "=", "float", ",", "default", "=", "0.7", ",", "\n", "help", "=", "\"Learning rate for CVI updates.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--queue_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of previous gradient to average over for smoothed gradient updates. Default 1 = no smoothing.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_workers'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--print_log'", ",", "dest", "=", "\"log\"", ",", "action", "=", "\"store_false\"", ",", "\n", "help", "=", "'Add this flag to print log to console instead of saving it to a file.'", ")", "\n", "parser", ".", "set_defaults", "(", "log", "=", "True", ")", "\n", "parser", ".", "set_defaults", "(", "corpus_in_memory", "=", "False", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "path_to_current_file", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ")", "\n", "\n", "disable_log", "=", "True", "\n", "if", "disable_log", ":", "\n", "        ", "logging", ".", "disable", "(", "logging", ".", "INFO", ")", "\n", "", "else", ":", "\n", "        ", "log_format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "path_to_current_file", ",", "\"../../scripts/log/sotu/\"", ")", "\n", "if", "args", ".", "log", ":", "\n", "           ", "filename", "=", "log_dir", "+", "time", ".", "strftime", "(", "'%m_%d_%Y_%H%M'", ")", "+", "'_K{}_P{}_bs{}_q{}_lo{}_ld{}_pn{}_mn{}_reg{}_{}_cpu{}.log'", ".", "format", "(", "\n", "args", ".", "num_topics", ",", "args", ".", "num_personas", ",", "\n", "args", ".", "batch_size", ",", "args", ".", "queue_size", ",", "\n", "args", ".", "learning_offset", ",", "int", "(", "100", "*", "args", ".", "learning_decay", ")", ",", "\n", "int", "(", "100", "*", "args", ".", "process_noise", ")", ",", "int", "(", "100", "*", "args", ".", "measurement_noise", ")", ",", "\n", "int", "(", "100", "*", "args", ".", "regularization", ")", ",", "args", ".", "normalization", ",", "args", ".", "num_workers", ")", "\n", "logging", ".", "basicConfig", "(", "filename", "=", "filename", ",", "format", "=", "log_format", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "", "else", ":", "\n", "           ", "logging", ".", "basicConfig", "(", "format", "=", "log_format", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "\n", "", "", "np", ".", "random", ".", "seed", "(", "2018", ")", "\n", "\n", "# initialize model", "\n", "dap", "=", "DAPPER", "(", "num_topics", "=", "args", ".", "num_topics", ",", "num_personas", "=", "args", ".", "num_personas", ",", "\n", "process_noise", "=", "args", ".", "process_noise", ",", "measurement_noise", "=", "args", ".", "measurement_noise", ",", "\n", "regularization", "=", "args", ".", "regularization", ",", "\n", "normalization", "=", "args", ".", "normalization", ",", "\n", "max_epochs", "=", "args", ".", "max_epochs", ",", "max_training_minutes", "=", "args", ".", "max_training_minutes", ",", "\n", "local_convergence", "=", "args", ".", "local_convergence", ",", "max_local_iters", "=", "args", ".", "max_local_iters", ",", "\n", "step_size", "=", "args", ".", "step_size", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "queue_size", "=", "args", ".", "queue_size", ",", "\n", "learning_offset", "=", "args", ".", "learning_offset", ",", "learning_decay", "=", "args", ".", "learning_decay", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ")", "\n", "\n", "data_dir", "=", "os", ".", "path", ".", "join", "(", "path_to_current_file", ",", "\"../../data/sotu_wc/\"", ")", "\n", "\n", "train", "=", "Corpus", "(", "input_file", "=", "data_dir", "+", "\"sotu_train.txt\"", ",", "vocab_file", "=", "data_dir", "+", "\"sotu_vocab.txt\"", ")", "\n", "test", "=", "Corpus", "(", "input_file", "=", "data_dir", "+", "\"sotu_test.txt\"", ",", "\n", "vocab_file", "=", "data_dir", "+", "\"sotu_vocab.txt\"", ",", "author2id", "=", "train", ".", "author2id", ")", "\n", "\n", "train_results", ",", "test_results", "=", "dap", ".", "fit_predict", "(", "train_corpus", "=", "train", ",", "test_corpus", "=", "test", ",", "\n", "evaluate_every", "=", "args", ".", "evaluate_every", ")", "\n", "print", "(", "dap", ")", "\n", "\n", "# save model output", "\n", "results_dir", "=", "os", ".", "path", ".", "join", "(", "path_to_current_file", ",", "\"../../results/sotu_wc/\"", ")", "\n", "model_sig", "=", "\"K{}_P{}_bs{}_q{}_lo{}_ld{}_pn{}_mn{}_reg{}_{}_epochs{}_cpu{}_{}.txt\"", ".", "format", "(", "\n", "args", ".", "num_topics", ",", "args", ".", "num_personas", ",", "\n", "args", ".", "batch_size", ",", "args", ".", "queue_size", ",", "\n", "args", ".", "learning_offset", ",", "int", "(", "100", "*", "args", ".", "learning_decay", ")", ",", "\n", "int", "(", "100", "*", "args", ".", "process_noise", ")", ",", "int", "(", "100", "*", "args", ".", "measurement_noise", ")", ",", "\n", "int", "(", "100", "*", "args", ".", "regularization", ")", ",", "args", ".", "normalization", ",", "dap", ".", "total_epochs", ",", "\n", "args", ".", "num_workers", ",", "time", ".", "strftime", "(", "'%m_%d_%Y_%H%M'", ")", ")", "\n", "dap", ".", "save_topics", "(", "filename", "=", "results_dir", "+", "\"topics_\"", "+", "model_sig", ",", "topn", "=", "10", ")", "\n", "dap", ".", "save_author_personas", "(", "filename", "=", "results_dir", "+", "\"personas_\"", "+", "model_sig", ")", "\n", "dap", ".", "save_persona_topics", "(", "filename", "=", "results_dir", "+", "\"alpha_\"", "+", "model_sig", ")", "\n", "dap", ".", "save_convergnces", "(", "filename", "=", "results_dir", "+", "\"train_convergence_\"", "+", "model_sig", ",", "results", "=", "train_results", ")", "\n", "dap", ".", "save_convergnces", "(", "filename", "=", "results_dir", "+", "\"test_convergence_\"", "+", "model_sig", ",", "results", "=", "test_results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.robert-giaquinto_dapper.experiments.signalmedia_blogs.main": [[9, 93], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.abspath", "numpy.random.seed", "src.dapper.DAPPER", "src.corpus.Corpus", "src.corpus.Corpus", "src.dapper.DAPPER.fit_predict", "print", "os.path.join.replace", "src.dapper.DAPPER.save_topics", "src.dapper.DAPPER.save_author_personas", "src.dapper.DAPPER.save_persona_topics", "src.dapper.DAPPER.save_convergences", "src.dapper.DAPPER.save_convergences", "os.path.dirname", "os.path.join", "logging.disable", "logging.basicConfig", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "int", "int", "int", "time.strftime"], "function", ["home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.fit_predict", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_topics", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_author_personas", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_persona_topics", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_convergences", "home.repos.pwc.inspect_result.robert-giaquinto_dapper.src.dapper.DAPPER.save_convergences"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n    Example of call main program\n    :return:\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Run dap model on signalmedia data.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_file'", ",", "type", "=", "str", ",", "help", "=", "'Path to training data file.'", ",", "\n", "default", "=", "\"train_signalmedia.dap\"", ")", "\n", "parser", ".", "add_argument", "(", "'--test_file'", ",", "type", "=", "str", ",", "help", "=", "'Path to testing data file. If None, no prediction is run'", ",", "\n", "default", "=", "\"test_signalmedia.dap\"", ")", "\n", "parser", ".", "add_argument", "(", "'--vocab_file'", ",", "type", "=", "str", ",", "help", "=", "'Path to vocabulary file.'", ",", "\n", "default", "=", "\"signalmedia.bow.vocab\"", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "help", "=", "'directory where all data files reside.'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "type", "=", "int", ",", "\n", "help", "=", "\"If given a test file, number of EM iterations between evaluations of test set. Default of 0 = evaluate after each epoch.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_training_minutes'", ",", "type", "=", "float", ",", "\n", "help", "=", "\"If given this will stop training once the specified number of minutes have elapsed.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_epochs'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--process_noise'", ",", "type", "=", "float", ",", "default", "=", "0.2", ")", "\n", "parser", ".", "add_argument", "(", "'--measurement_noise'", ",", "type", "=", "float", ",", "default", "=", "0.8", ")", "\n", "parser", ".", "add_argument", "(", "'--num_topics'", ",", "type", "=", "int", ",", "default", "=", "75", ")", "\n", "parser", ".", "add_argument", "(", "'--num_personas'", ",", "type", "=", "int", ",", "default", "=", "25", ")", "\n", "parser", ".", "add_argument", "(", "'--regularization'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"How much to penalize similar personas. Recommend [0, 0.5].\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Batch size. Set to -1 for full gradient updates, else stochastic mini-batches used.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_workers'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "path_to_current_file", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ")", "\n", "if", "args", ".", "data_dir", "is", "None", ":", "\n", "        ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "path_to_current_file", ",", "\"../../data/signalmedia/blogs_aligned_3_30/\"", ")", "\n", "", "else", ":", "\n", "        ", "data_dir", "=", "args", ".", "data_dir", "\n", "\n", "", "np", ".", "random", ".", "seed", "(", "2018", ")", "\n", "\n", "disable_log", "=", "False", "\n", "if", "disable_log", ":", "\n", "        ", "logging", ".", "disable", "(", "logging", ".", "INFO", ")", "\n", "", "else", ":", "\n", "        ", "log_format", "=", "'%(asctime)s : %(levelname)s : %(message)s'", "\n", "logging", ".", "basicConfig", "(", "format", "=", "log_format", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "\n", "# initialize model", "\n", "", "dap", "=", "DAPPER", "(", "num_topics", "=", "args", ".", "num_topics", ",", "num_personas", "=", "args", ".", "num_personas", ",", "\n", "process_noise", "=", "args", ".", "process_noise", ",", "measurement_noise", "=", "args", ".", "measurement_noise", ",", "\n", "regularization", "=", "args", ".", "regularization", ",", "\n", "max_epochs", "=", "args", ".", "max_epochs", ",", "max_training_minutes", "=", "args", ".", "max_training_minutes", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "step_size", "=", "0.7", ",", "learning_offset", "=", "10", ",", "learning_decay", "=", "0.7", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "data_dir", ")", "\n", "\n", "", "train", "=", "Corpus", "(", "input_file", "=", "data_dir", "+", "args", ".", "train_file", ",", "vocab_file", "=", "data_dir", "+", "args", ".", "vocab_file", ")", "\n", "test", "=", "Corpus", "(", "input_file", "=", "data_dir", "+", "args", ".", "test_file", ",", "\n", "vocab_file", "=", "data_dir", "+", "args", ".", "vocab_file", ",", "\n", "author2id", "=", "train", ".", "author2id", ")", "\n", "\n", "train_results", ",", "test_results", "=", "dap", ".", "fit_predict", "(", "train_corpus", "=", "train", ",", "test_corpus", "=", "test", ",", "\n", "evaluate_every", "=", "args", ".", "evaluate_every", ",", "\n", "random_beta", "=", "False", ",", "\n", "check_model_lhood", "=", "True", ")", "\n", "# train_results = dap.fit(corpus=train, random_beta=False, check_model_lhood=False)", "\n", "print", "(", "dap", ")", "\n", "\n", "# save model output", "\n", "results_dir", "=", "data_dir", ".", "replace", "(", "\"/data/\"", ",", "\"/results/\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "results_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "results_dir", ")", "\n", "\n", "", "model_sig", "=", "\"K{}_P{}_bs{}_pn{}_mn{}_reg{}_epochs{}_cpu{}_{}.txt\"", ".", "format", "(", "\n", "args", ".", "num_topics", ",", "args", ".", "num_personas", ",", "\n", "args", ".", "batch_size", ",", "\n", "int", "(", "100", "*", "args", ".", "process_noise", ")", ",", "int", "(", "100", "*", "args", ".", "measurement_noise", ")", ",", "\n", "int", "(", "100", "*", "args", ".", "regularization", ")", ",", "dap", ".", "total_epochs", ",", "\n", "args", ".", "num_workers", ",", "time", ".", "strftime", "(", "'%m_%d_%Y_%H%M'", ")", ")", "\n", "dap", ".", "save_topics", "(", "filename", "=", "results_dir", "+", "\"topics_\"", "+", "model_sig", ",", "topn", "=", "10", ")", "\n", "dap", ".", "save_author_personas", "(", "filename", "=", "results_dir", "+", "\"personas_\"", "+", "model_sig", ")", "\n", "dap", ".", "save_persona_topics", "(", "filename", "=", "results_dir", "+", "\"alpha_\"", "+", "model_sig", ")", "\n", "dap", ".", "save_convergences", "(", "filename", "=", "results_dir", "+", "\"train_convergence_\"", "+", "model_sig", ",", "results", "=", "train_results", ")", "\n", "dap", ".", "save_convergences", "(", "filename", "=", "results_dir", "+", "\"test_convergence_\"", "+", "model_sig", ",", "results", "=", "test_results", ")", "\n", "\n"]]}