{"home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.None.main.setup": [[17, 23], ["torch.manual_seed", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed", "torch.set_num_threads"], "function", ["None"], ["def", "setup", "(", "cfg", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "cfg", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "cfg", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "cfg", ".", "seed", ")", "\n", "random", ".", "seed", "(", "cfg", ".", "seed", ")", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.None.main.main": [[24, 83], ["hydra.main", "print", "main.setup", "IRCR.misc.env_wrappers.MuJoCoEnv", "IRCR.buffers.fifo.FIFOBuffer", "IRCR.buffers.minheap.MinHeapBuffer", "IRCR.misc.rollout_storage.RolloutStorage", "hydra.utils.instantiate", "time.time", "collections.deque", "range", "print", "cfg.pretty", "float", "float", "int", "IRCR.misc.rollout_storage.RolloutStorage.collect_rollout", "IRCR.buffers.fifo.FIFOBuffer.add_paths", "IRCR.buffers.minheap.MinHeapBuffer.add_paths", "int", "paths.append", "range", "sum", "collections.deque.extend", "IRCR.buffers.fifo.FIFOBuffer.add_paths", "IRCR.buffers.minheap.MinHeapBuffer.add_paths", "IRCR.misc.env_wrappers.MuJoCoEnv.action_space.low.min", "IRCR.misc.env_wrappers.MuJoCoEnv.action_space.high.max", "IRCR.misc.rollout_storage.RolloutStorage.collect_rollout", "paths.append", "str", "time.time", "print", "sys.stdout.flush", "IRCR.misc.rollout_storage.RolloutStorage.collect_rollout", "path[].sum", "datetime.timedelta", "len", "numpy.average", "int", "time.time"], "function", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.None.main.main", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.None.main.setup", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.rollout_storage.RolloutStorage.collect_rollout", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.add_paths", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.add_paths", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.add_paths", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.add_paths", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.rollout_storage.RolloutStorage.collect_rollout", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.rollout_storage.RolloutStorage.collect_rollout"], ["", "@", "hydra", ".", "main", "(", "config_path", "=", "'config/mujoco.yaml'", ",", "strict", "=", "True", ")", "\n", "def", "main", "(", "cfg", ")", ":", "\n", "\n", "    ", "print", "(", "cfg", ".", "pretty", "(", ")", ")", "\n", "setup", "(", "cfg", ")", "\n", "\n", "wrappers", "=", "[", "'episodic_rewards'", "]", "\n", "env", "=", "MuJoCoEnv", "(", "cfg", ".", "env_name", ",", "wrappers", ",", "cfg", ".", "seed", ")", "\n", "\n", "cfg", ".", "algo", ".", "params", ".", "obs_dim", "=", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "cfg", ".", "algo", ".", "params", ".", "action_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "cfg", ".", "algo", ".", "params", ".", "action_range", "=", "[", "\n", "float", "(", "env", ".", "action_space", ".", "low", ".", "min", "(", ")", ")", ",", "\n", "float", "(", "env", ".", "action_space", ".", "high", ".", "max", "(", ")", ")", "]", "\n", "\n", "fifo_buffer", "=", "FIFOBuffer", "(", "env", ".", "observation_space", ".", "shape", ",", "env", ".", "action_space", ".", "shape", ",", "int", "(", "cfg", ".", "fifo_buffer_capacity", ")", ")", "\n", "mh_buffer", "=", "MinHeapBuffer", "(", "env", ".", "observation_space", ".", "shape", ",", "env", ".", "action_space", ".", "shape", ",", "cfg", ".", "mh_buffer_capacity", ")", "\n", "\n", "rollout_storage", "=", "RolloutStorage", "(", "env", ")", "\n", "actor_critic", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "algo", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "total_timesteps", "=", "0", "\n", "moving_returns", "=", "deque", "(", "maxlen", "=", "30", ")", "\n", "eval_marker", "=", "1", "\n", "\n", "# some initial exploration to fill up the buffers", "\n", "for", "_", "in", "range", "(", "cfg", ".", "exploration", ".", "num_init_explr", ")", ":", "\n", "        ", "path", "=", "rollout_storage", ".", "collect_rollout", "(", "actor_critic", ",", "fifo_buffer", ",", "mh_buffer", ",", "stochastic", "=", "True", ",", "update_agent", "=", "False", ")", "\n", "fifo_buffer", ".", "add_paths", "(", "[", "path", "]", ")", "\n", "mh_buffer", ".", "add_paths", "(", "[", "path", "]", ")", "\n", "", "assert", "fifo_buffer", ".", "min_credit_val", "<", "fifo_buffer", ".", "max_credit_val", ",", "\"Need more initial data for min-max normalization. Consider increasing cfg.exploration.num_init_explr!\"", "\n", "\n", "print", "(", "'Starting with the main training loop... Printing performance after every {} timesteps...'", ".", "format", "(", "cfg", ".", "eval_granularity", ")", ")", "\n", "while", "total_timesteps", "<", "int", "(", "cfg", ".", "num_train_steps", ")", ":", "\n", "\n", "        ", "paths", "=", "[", "]", "\n", "paths", ".", "append", "(", "rollout_storage", ".", "collect_rollout", "(", "actor_critic", ",", "fifo_buffer", ",", "mh_buffer", ",", "stochastic", "=", "True", ",", "update_agent", "=", "True", ")", ")", "\n", "\n", "# if desired, generate additional experience data", "\n", "for", "_", "in", "range", "(", "cfg", ".", "exploration", ".", "num_periodic_explr", ")", ":", "\n", "            ", "paths", ".", "append", "(", "rollout_storage", ".", "collect_rollout", "(", "actor_critic", ",", "fifo_buffer", ",", "mh_buffer", ",", "stochastic", "=", "True", ",", "update_agent", "=", "False", ")", ")", "\n", "\n", "", "total_timesteps", "+=", "sum", "(", "[", "path", "[", "'rewards'", "]", ".", "shape", "[", "0", "]", "for", "path", "in", "paths", "]", ")", "\n", "moving_returns", ".", "extend", "(", "[", "path", "[", "'rewards'", "]", ".", "sum", "(", ")", "for", "path", "in", "paths", "]", ")", "\n", "\n", "# add the generated paths to the buffers", "\n", "fifo_buffer", ".", "add_paths", "(", "paths", ")", "\n", "mh_buffer", ".", "add_paths", "(", "paths", ")", "\n", "\n", "# print performance", "\n", "if", "total_timesteps", ">=", "eval_marker", "*", "cfg", ".", "eval_granularity", ":", "\n", "            ", "duration", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "\"Duration={}, Total-timesteps:{}, Average returns for last {} episodes={:.2f}\"", ".", "format", "(", "\n", "duration", ",", "total_timesteps", ",", "len", "(", "moving_returns", ")", ",", "np", ".", "average", "(", "moving_returns", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "eval_marker", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.__init__": [[13, 46], ["IRCR.algorithm.Agent.__init__", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "hydra.utils.instantiate().to", "hydra.utils.instantiate().to", "sac.SACAgent.critic_target.load_state_dict", "hydra.utils.instantiate().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "sac.SACAgent.train", "sac.SACAgent.critic_target.train", "sac.SACAgent.critic.state_dict", "sac.SACAgent.actor.parameters", "sac.SACAgent.critic.parameters", "hydra.utils.instantiate", "hydra.utils.instantiate", "hydra.utils.instantiate", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.__init__", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.train", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.train"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "action_dim", ",", "action_range", ",", "device", ",", "\n", "critic_cfg", ",", "actor_cfg", ",", "discount", ",", "init_temperature", ",", "alpha_lr", ",", "\n", "alpha_betas", ",", "actor_lr", ",", "actor_betas", ",", "actor_update_frequency", ",", "\n", "critic_lr", ",", "critic_betas", ",", "critic_tau", ",", "\n", "critic_target_update_frequency", ",", "batch_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "action_range", "=", "action_range", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "device", ")", "\n", "self", ".", "discount", "=", "discount", "\n", "self", ".", "critic_tau", "=", "critic_tau", "\n", "self", ".", "actor_update_frequency", "=", "actor_update_frequency", "\n", "self", ".", "critic_target_update_frequency", "=", "critic_target_update_frequency", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "self", ".", "critic", "=", "hydra", ".", "utils", ".", "instantiate", "(", "critic_cfg", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target", "=", "hydra", ".", "utils", ".", "instantiate", "(", "critic_cfg", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target", ".", "load_state_dict", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "actor", "=", "hydra", ".", "utils", ".", "instantiate", "(", "actor_cfg", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "log_alpha", "=", "torch", ".", "tensor", "(", "np", ".", "log", "(", "init_temperature", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "log_alpha", ".", "requires_grad", "=", "True", "\n", "# set target entropy to -|A|", "\n", "self", ".", "target_entropy", "=", "-", "action_dim", "\n", "\n", "# optimizers", "\n", "self", ".", "actor_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "actor", ".", "parameters", "(", ")", ",", "lr", "=", "actor_lr", ",", "betas", "=", "actor_betas", ")", "\n", "self", ".", "critic_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "lr", "=", "critic_lr", ",", "betas", "=", "critic_betas", ")", "\n", "self", ".", "log_alpha_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "alpha_lr", ",", "betas", "=", "alpha_betas", ")", "\n", "\n", "self", ".", "train", "(", ")", "\n", "self", ".", "critic_target", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.train": [[47, 51], ["sac.SACAgent.actor.train", "sac.SACAgent.critic.train"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.train", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.train"], ["", "def", "train", "(", "self", ",", "training", "=", "True", ")", ":", "\n", "        ", "self", ".", "training", "=", "training", "\n", "self", ".", "actor", ".", "train", "(", "training", ")", "\n", "self", ".", "critic", ".", "train", "(", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.alpha": [[52, 55], ["sac.SACAgent.log_alpha.exp"], "methods", ["None"], ["", "@", "property", "\n", "def", "alpha", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_alpha", ".", "exp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.act": [[56, 64], ["torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "obs.unsqueeze.unsqueeze.unsqueeze", "sac.SACAgent.actor.sample", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "IRCR.misc.utils.to_np", "action.clamp", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "action.ndimension"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.to_np"], ["", "def", "act", "(", "self", ",", "obs", ",", "sample", "=", "False", ")", ":", "\n", "        ", "obs", "=", "torch", ".", "FloatTensor", "(", "obs", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "obs", "=", "obs", ".", "unsqueeze", "(", "0", ")", "\n", "stochastic_action", ",", "_", ",", "mean", "=", "self", ".", "actor", ".", "sample", "(", "obs", ",", "reparameterized", "=", "False", ")", "\n", "action", "=", "stochastic_action", "if", "sample", "else", "mean", "\n", "assert", "torch", ".", "equal", "(", "action", ",", "action", ".", "clamp", "(", "*", "self", ".", "action_range", ")", ")", "\n", "assert", "action", ".", "ndimension", "(", ")", "==", "2", "and", "action", ".", "shape", "[", "0", "]", "==", "1", "\n", "return", "to_np", "(", "action", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.update_critic": [[65, 85], ["sac.SACAgent.critic", "critic_loss.item", "sac.SACAgent.critic_optimizer.zero_grad", "critic_loss.backward", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "sac.SACAgent.critic_optimizer.step", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sac.SACAgent.actor.sample", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "sac.SACAgent.critic_target", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "sac.SACAgent.critic.parameters", "next_action.clamp", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample"], ["", "def", "update_critic", "(", "self", ",", "obs", ",", "action", ",", "reward", ",", "next_obs", ",", "not_done", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_action", ",", "log_prob", ",", "_", "=", "self", ".", "actor", ".", "sample", "(", "next_obs", ",", "reparameterized", "=", "False", ")", "\n", "assert", "torch", ".", "equal", "(", "next_action", ",", "next_action", ".", "clamp", "(", "*", "self", ".", "action_range", ")", ")", "\n", "target_Q1", ",", "target_Q2", "=", "self", ".", "critic_target", "(", "next_obs", ",", "next_action", ")", "\n", "target_V", "=", "torch", ".", "min", "(", "target_Q1", ",", "target_Q2", ")", "-", "self", ".", "alpha", "*", "log_prob", "\n", "target_Q", "=", "reward", "+", "(", "not_done", "*", "self", ".", "discount", "*", "target_V", ")", "\n", "\n", "# get current Q estimates", "\n", "", "current_Q1", ",", "current_Q2", "=", "self", ".", "critic", "(", "obs", ",", "action", ")", "\n", "critic_loss", "=", "F", ".", "smooth_l1_loss", "(", "current_Q1", ",", "target_Q", ")", "+", "F", ".", "smooth_l1_loss", "(", "current_Q2", ",", "target_Q", ")", "\n", "loss_val", "=", "critic_loss", ".", "item", "(", ")", "\n", "\n", "# Optimize the critic", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "critic_loss", ".", "backward", "(", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "max_norm", "=", "1.", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss_val", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.update_actor_and_alpha": [[86, 108], ["sac.SACAgent.actor.sample", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "sac.SACAgent.critic", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "actor_loss.item", "sac.SACAgent.actor_optimizer.zero_grad", "actor_loss.backward", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "sac.SACAgent.actor_optimizer.step", "sac.SACAgent.log_alpha_optimizer.zero_grad", "alpha_loss.backward", "sac.SACAgent.log_alpha_optimizer.step", "action.clamp", "sac.SACAgent.actor.parameters", "sac.SACAgent.alpha.detach"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step"], ["", "def", "update_actor_and_alpha", "(", "self", ",", "obs", ")", ":", "\n", "        ", "action", ",", "log_prob", ",", "_", "=", "self", ".", "actor", ".", "sample", "(", "obs", ",", "reparameterized", "=", "True", ")", "\n", "assert", "torch", ".", "equal", "(", "action", ",", "action", ".", "clamp", "(", "*", "self", ".", "action_range", ")", ")", "\n", "actor_Q1", ",", "actor_Q2", "=", "self", ".", "critic", "(", "obs", ",", "action", ")", "\n", "\n", "actor_Q", "=", "torch", ".", "min", "(", "actor_Q1", ",", "actor_Q2", ")", "\n", "actor_loss", "=", "(", "self", ".", "alpha", ".", "detach", "(", ")", "*", "log_prob", "-", "actor_Q", ")", ".", "mean", "(", ")", "\n", "loss_val", "=", "actor_loss", ".", "item", "(", ")", "\n", "\n", "# optimize the actor", "\n", "self", ".", "actor_optimizer", ".", "zero_grad", "(", ")", "\n", "actor_loss", ".", "backward", "(", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "actor", ".", "parameters", "(", ")", ",", "max_norm", "=", "1.", ")", "\n", "self", ".", "actor_optimizer", ".", "step", "(", ")", "\n", "\n", "alpha_loss", "=", "-", "(", "self", ".", "alpha", "*", "(", "log_prob", "+", "self", ".", "target_entropy", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "\n", "self", ".", "log_alpha_optimizer", ".", "zero_grad", "(", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "log_alpha_optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss_val", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt": [[109, 111], ["torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to"], ["", "def", "_np2pt", "(", "self", ",", "arr", ")", ":", "\n", "        ", "return", "torch", ".", "from_numpy", "(", "arr", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.compute_rg": [[112, 117], ["max", "min"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_rg", "(", "credits", ",", "fifo_buffer", ",", "mh_buffer", ")", ":", "\n", "        ", "r_max", "=", "max", "(", "mh_buffer", ".", "max_credit_val", ",", "fifo_buffer", ".", "max_credit_val", ")", "\n", "r_min", "=", "min", "(", "mh_buffer", ".", "min_credit_val", ",", "fifo_buffer", ".", "min_credit_val", ")", "\n", "return", "(", "credits", "-", "r_min", ")", "/", "(", "r_max", "-", "r_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.update": [[118, 153], ["fifo_buffer.sample", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "mh_buffer.sample", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sac.SACAgent.compute_rg", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "torch.equal", "sac.SACAgent.update_critic", "sac.SACAgent._np2pt().float", "sac.SACAgent.compute_rg", "sac.SACAgent.update_actor_and_alpha", "IRCR.misc.utils.soft_update", "print", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt", "sac.SACAgent._np2pt().float", "sac.SACAgent._np2pt"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.compute_rg", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.update_critic", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.compute_rg", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent.update_actor_and_alpha", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.soft_update", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.sac.SACAgent._np2pt"], ["", "def", "update", "(", "self", ",", "fifo_buffer", ",", "mh_buffer", ",", "step", ")", ":", "\n", "\n", "# return if the buffer has insufficient number of transitions", "\n", "        ", "samples", "=", "fifo_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "if", "samples", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "obs", "=", "self", ".", "_np2pt", "(", "samples", "[", "'observations'", "]", ")", "\n", "action", "=", "self", ".", "_np2pt", "(", "samples", "[", "'actions'", "]", ")", "\n", "next_obs", "=", "self", ".", "_np2pt", "(", "samples", "[", "'next_observations'", "]", ")", "\n", "not_done", "=", "1", "-", "self", ".", "_np2pt", "(", "samples", "[", "'dones'", "]", ")", ".", "float", "(", ")", "\n", "credits", "=", "self", ".", "_np2pt", "(", "samples", "[", "'credits'", "]", ")", "\n", "\n", "# gather some transitions from the MinHeapBuffer as well", "\n", "samples", "=", "mh_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "obs", "=", "torch", ".", "cat", "(", "[", "obs", ",", "self", ".", "_np2pt", "(", "samples", "[", "'observations'", "]", ")", "]", ",", "dim", "=", "0", ")", "\n", "action", "=", "torch", ".", "cat", "(", "[", "action", ",", "self", ".", "_np2pt", "(", "samples", "[", "'actions'", "]", ")", "]", ",", "dim", "=", "0", ")", "\n", "next_obs", "=", "torch", ".", "cat", "(", "[", "next_obs", ",", "self", ".", "_np2pt", "(", "samples", "[", "'next_observations'", "]", ")", "]", ",", "dim", "=", "0", ")", "\n", "not_done", "=", "torch", ".", "cat", "(", "[", "not_done", ",", "1", "-", "self", ".", "_np2pt", "(", "samples", "[", "'dones'", "]", ")", ".", "float", "(", ")", "]", ",", "dim", "=", "0", ")", "\n", "credits", "=", "torch", ".", "cat", "(", "[", "credits", ",", "self", ".", "_np2pt", "(", "samples", "[", "'credits'", "]", ")", "]", ",", "dim", "=", "0", ")", "\n", "\n", "# calculate guidance reward by normalizing return (line 18, Algorithm 2 in the paper)", "\n", "guidance_reward", "=", "SACAgent", ".", "compute_rg", "(", "credits", ",", "fifo_buffer", ",", "mh_buffer", ")", "\n", "assert", "torch", ".", "equal", "(", "guidance_reward", ",", "guidance_reward", ".", "clamp", "(", "min", "=", "0.", ",", "max", "=", "1.", ")", ")", "\n", "\n", "critic_loss", "=", "self", ".", "update_critic", "(", "obs", ",", "action", ",", "guidance_reward", ",", "next_obs", ",", "not_done", ")", "\n", "\n", "if", "step", "%", "self", ".", "actor_update_frequency", "==", "0", ":", "\n", "            ", "actor_loss", "=", "self", ".", "update_actor_and_alpha", "(", "obs", ")", "\n", "\n", "", "if", "step", "%", "self", ".", "critic_target_update_frequency", "==", "0", ":", "\n", "            ", "soft_update", "(", "self", ".", "critic_target", ",", "self", ".", "critic", ",", "self", ".", "critic_tau", ")", "\n", "\n", "", "if", "DEBUG", ":", "\n", "            ", "print", "(", "\"Training statistics: Actor-loss = {:.3f}, Critic-loss = {:.3f}\"", ".", "format", "(", "actor_loss", ",", "critic_loss", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.reset": [[4, 6], ["None"], "methods", ["None"], ["    ", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"For state-full agents this function performs reseting at the beginning of each episode.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.train": [[7, 10], ["None"], "methods", ["None"], ["", "@", "abc", ".", "abstractmethod", "\n", "def", "train", "(", "self", ",", "training", "=", "True", ")", ":", "\n", "        ", "\"\"\"Sets the agent in either training or evaluation mode.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.update": [[11, 14], ["None"], "methods", ["None"], ["", "@", "abc", ".", "abstractmethod", "\n", "def", "update", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Main function of the agent that performs learning.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.act": [[15, 18], ["None"], "methods", ["None"], ["", "@", "abc", ".", "abstractmethod", "\n", "def", "act", "(", "self", ",", "obs", ",", "sample", "=", "False", ")", ":", "\n", "        ", "\"\"\"Issues an action given an observation.\"\"\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.critic.DoubleQCritic.__init__": [[8, 16], ["torch.nn.Module.__init__", "IRCR.misc.utils.mlp", "IRCR.misc.utils.mlp", "dict", "critic.DoubleQCritic.apply"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.__init__", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.mlp", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.mlp"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "action_dim", ",", "hidden_dim", ",", "hidden_depth", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "Q1", "=", "mlp", "(", "obs_dim", "+", "action_dim", ",", "hidden_dim", ",", "1", ",", "hidden_depth", ")", "\n", "self", ".", "Q2", "=", "mlp", "(", "obs_dim", "+", "action_dim", ",", "hidden_dim", ",", "1", ",", "hidden_depth", ")", "\n", "\n", "self", ".", "outputs", "=", "dict", "(", ")", "\n", "self", ".", "apply", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.critic.DoubleQCritic.forward": [[17, 28], ["torch.cat", "critic.DoubleQCritic.Q1", "critic.DoubleQCritic.Q2", "obs.size", "action.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "action", ")", ":", "\n", "        ", "assert", "obs", ".", "size", "(", "0", ")", "==", "action", ".", "size", "(", "0", ")", "\n", "\n", "obs_action", "=", "torch", ".", "cat", "(", "[", "obs", ",", "action", "]", ",", "dim", "=", "-", "1", ")", "\n", "q1", "=", "self", ".", "Q1", "(", "obs_action", ")", "\n", "q2", "=", "self", ".", "Q2", "(", "obs_action", ")", "\n", "\n", "self", ".", "outputs", "[", "'q1'", "]", "=", "q1", "\n", "self", ".", "outputs", "[", "'q2'", "]", "=", "q2", "\n", "\n", "return", "q1", ",", "q2", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.__init__": [[9, 20], ["torch.nn.Module.__init__", "IRCR.misc.utils.mlp", "actor.DiagGaussianActor.apply", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.__init__", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "action_dim", ",", "action_range", ",", "hidden_dim", ",", "hidden_depth", ",", "log_std_bounds", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "log_std_bounds", "=", "log_std_bounds", "\n", "self", ".", "trunk", "=", "mlp", "(", "obs_dim", ",", "hidden_dim", ",", "2", "*", "action_dim", ",", "hidden_depth", ")", "\n", "\n", "self", ".", "apply", "(", "weight_init", ")", "\n", "\n", "self", ".", "action_scale", "=", "torch", ".", "tensor", "(", "(", "action_range", "[", "1", "]", "-", "action_range", "[", "0", "]", ")", "/", "2.", ")", "\n", "self", ".", "action_shift", "=", "torch", ".", "tensor", "(", "(", "action_range", "[", "1", "]", "+", "action_range", "[", "0", "]", ")", "/", "2.", ")", "\n", "self", ".", "use_stable_lp", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to": [[21, 25], ["actor.DiagGaussianActor.action_scale.to", "actor.DiagGaussianActor.action_shift.to", "super().to"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "        ", "self", ".", "action_scale", "=", "self", ".", "action_scale", ".", "to", "(", "device", ")", "\n", "self", ".", "action_shift", "=", "self", ".", "action_shift", ".", "to", "(", "device", ")", "\n", "return", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.forward": [[26, 32], ["actor.DiagGaussianActor.trunk().chunk", "torch.clamp", "actor.DiagGaussianActor.trunk"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ")", ":", "\n", "        ", "mean", ",", "log_std", "=", "self", ".", "trunk", "(", "obs", ")", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "log_std_min", ",", "log_std_max", "=", "self", ".", "log_std_bounds", "\n", "log_std", "=", "torch", ".", "clamp", "(", "log_std", ",", "min", "=", "log_std_min", ",", "max", "=", "log_std_max", ")", "\n", "\n", "return", "mean", ",", "log_std", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.sample": [[33, 56], ["actor.DiagGaussianActor.forward", "log_std.exp", "torch.distributions.Normal", "torch.tanh", "torch.distributions.Normal.log_prob", "log_prob.sum.sum.sum", "torch.distributions.Normal.rsample", "torch.distributions.Normal.sample", "torch.log", "torch.tanh", "torch.nn.functional.softplus", "torch.log", "torch.tanh.pow"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.actor.DiagGaussianActor.forward", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample"], ["", "def", "sample", "(", "self", ",", "obs", ",", "reparameterized", ")", ":", "\n", "        ", "mean", ",", "log_std", "=", "self", ".", "forward", "(", "obs", ")", "\n", "std", "=", "log_std", ".", "exp", "(", ")", "\n", "normal", "=", "Normal", "(", "mean", ",", "std", ")", "\n", "arctanh_action", "=", "normal", ".", "rsample", "(", ")", "if", "reparameterized", "else", "normal", ".", "sample", "(", ")", "\n", "action", "=", "torch", ".", "tanh", "(", "arctanh_action", ")", "\n", "action_scaled_shifted", "=", "action", "*", "self", ".", "action_scale", "+", "self", ".", "action_shift", "\n", "mean", "=", "torch", ".", "tanh", "(", "mean", ")", "*", "self", ".", "action_scale", "+", "self", ".", "action_shift", "\n", "\n", "# log_prob with tanh `flow` and scale-shift `flow`", "\n", "log_prob", "=", "normal", ".", "log_prob", "(", "arctanh_action", ")", "\n", "\n", "if", "self", ".", "use_stable_lp", ":", "\n", "# use a numerically stable formula for log(1 - tanh(x)^2), adapted from", "\n", "# https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/tanh.py#L73", "\n", "# formula => log(1 - tanh(x)^2) = 2 * (log(2) - x - softplus(-2x))", "\n", "            ", "f", "=", "-", "arctanh_action", "-", "torch", ".", "nn", ".", "functional", ".", "softplus", "(", "-", "2.", "*", "arctanh_action", ")", "\n", "log_prob", "-=", "(", "2", "*", "f", "+", "torch", ".", "log", "(", "4.", "*", "self", ".", "action_scale", ")", ")", "\n", "", "else", ":", "\n", "            ", "log_prob", "-=", "torch", ".", "log", "(", "self", ".", "action_scale", "*", "(", "1", "-", "action", ".", "pow", "(", "2", ")", ")", "+", "EPS", ")", "\n", "\n", "", "log_prob", "=", "log_prob", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "action_scaled_shifted", ",", "log_prob", ",", "mean", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.rollout_storage.RolloutStorage.__init__": [[6, 14], ["numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "\n", "obs_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "acs_shape", "=", "env", ".", "action_space", ".", "shape", "\n", "self", ".", "observations", "=", "np", ".", "zeros", "(", "(", "self", ".", "env", ".", "max_episode_steps", ",", "*", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "actions", "=", "np", ".", "zeros", "(", "(", "self", ".", "env", ".", "max_episode_steps", ",", "*", "acs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rewards", "=", "np", ".", "zeros", "(", "(", "self", ".", "env", ".", "max_episode_steps", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.rollout_storage.RolloutStorage.collect_rollout": [[15, 51], ["rollout_storage.RolloutStorage.env.reset", "numpy.zeros", "dict", "rollout_storage.RolloutStorage.env.step", "actor_critic.update", "torch.no_grad", "actor_critic.act", "copy.deepcopy", "numpy.array", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.reset", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.update", "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.algorithm.__init__.Agent.act"], ["", "def", "collect_rollout", "(", "self", ",", "actor_critic", ",", "fifo_buff", ",", "mh_buffer", ",", "stochastic", ",", "update_agent", ")", ":", "\n", "        ", "\"\"\"\n        Generate a single episode of experience. This function also updates the policy and\n        the critic depending on the input parameters\n        \"\"\"", "\n", "\n", "ob", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "step", "=", "0", "\n", "\n", "while", "not", "done", ":", "\n", "\n", "            ", "if", "update_agent", ":", "\n", "                ", "actor_critic", ".", "update", "(", "fifo_buff", ",", "mh_buffer", ",", "step", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "ac", "=", "actor_critic", ".", "act", "(", "ob", ",", "sample", "=", "stochastic", ")", "\n", "\n", "", "self", ".", "observations", "[", "step", "]", "=", "ob", "\n", "self", ".", "actions", "[", "step", "]", "=", "ac", "\n", "ob", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "ac", ")", "\n", "self", ".", "rewards", "[", "step", "]", "=", "reward", "\n", "step", "+=", "1", "\n", "\n", "", "dones", "=", "np", ".", "zeros", "(", "(", "step", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "# if the episode ended due to time constraints, we don't mark \"done\" as True", "\n", "if", "step", "!=", "self", ".", "env", ".", "max_episode_steps", ":", "\n", "            ", "dones", "[", "-", "1", "]", "=", "1", "\n", "\n", "", "return", "dict", "(", "\n", "observations", "=", "deepcopy", "(", "self", ".", "observations", "[", ":", "step", "]", ")", ",", "\n", "final_observation", "=", "np", ".", "array", "(", "[", "ob", "]", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", "actions", "=", "deepcopy", "(", "self", ".", "actions", "[", ":", "step", "]", ")", ",", "\n", "rewards", "=", "deepcopy", "(", "self", ".", "rewards", "[", ":", "step", "]", ")", ",", "\n", "dones", "=", "dones", ",", "\n", "info", "=", "info", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.EpisodicEnvWrapper.__init__": [[4, 8], ["print", "gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "print", "(", "'== Episodic Rewards Wrapper =='", ")", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "total_episode_reward", "=", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.EpisodicEnvWrapper._step": [[9, 20], ["env_wrappers.EpisodicEnvWrapper.env.step"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step"], ["", "def", "_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "observation", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "total_episode_reward", "+=", "reward", "\n", "\n", "if", "done", ":", "\n", "            ", "reward", "=", "self", ".", "total_episode_reward", "\n", "self", ".", "total_episode_reward", "=", "0.", "# reset", "\n", "", "else", ":", "\n", "            ", "reward", "=", "0.", "\n", "\n", "", "return", "observation", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.__init__": [[22, 31], ["gym.make", "env_wrappers.MuJoCoEnv.env.seed", "env_wrappers.EpisodicEnvWrapper", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env_name", ",", "wrappers", ",", "seed", ")", ":", "\n", "        ", "self", ".", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "self", ".", "max_episode_steps", "=", "self", ".", "env", ".", "_max_episode_steps", "\n", "self", ".", "observation_space", "=", "self", ".", "env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "self", ".", "env", ".", "action_space", "\n", "if", "'episodic_rewards'", "in", "wrappers", ":", "\n", "            ", "self", ".", "env", "=", "EpisodicEnvWrapper", "(", "self", ".", "env", ")", "\n", "", "else", ":", "assert", "len", "(", "wrappers", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.reset": [[32, 34], ["env_wrappers.MuJoCoEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step": [[35, 37], ["env_wrappers.MuJoCoEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.render": [[38, 40], ["env_wrappers.MuJoCoEnv.env.render"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.close": [[41, 43], ["env_wrappers.MuJoCoEnv.env.close"], "methods", ["home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.env_wrappers.MuJoCoEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.soft_update": [[4, 7], ["zip", "target.parameters", "source.parameters", "target_param.data.copy_"], "function", ["None"], ["def", "soft_update", "(", "target", ",", "source", ",", "tau", ")", ":", "\n", "    ", "for", "target_param", ",", "param", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "target_param", ".", "data", ".", "copy_", "(", "target_param", ".", "data", "*", "(", "1.0", "-", "tau", ")", "+", "param", ".", "data", "*", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.hard_update": [[8, 11], ["zip", "target.parameters", "source.parameters", "target_param.data.copy_"], "function", ["None"], ["", "", "def", "hard_update", "(", "target", ",", "source", ")", ":", "\n", "    ", "for", "target_param", ",", "param", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "target_param", ".", "data", ".", "copy_", "(", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.to_np": [[12, 18], ["t.cpu().detach().numpy", "t.nelement", "numpy.array", "t.cpu().detach", "t.cpu"], "function", ["None"], ["", "", "def", "to_np", "(", "t", ")", ":", "\n", "    ", "if", "t", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "t", ".", "nelement", "(", ")", "==", "0", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "]", ")", "\n", "", "return", "t", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.mlp": [[19, 38], ["torch.Sequential", "range", "mods.append", "mods.append", "ValueError", "torch.Linear", "torch.Linear", "actv", "torch.Linear", "torch.Linear", "actv"], "function", ["None"], ["", "def", "mlp", "(", "input_dim", ",", "hidden_dim", ",", "output_dim", ",", "hidden_depth", ",", "activation", "=", "\"relu\"", ",", "output_mod", "=", "None", ")", ":", "\n", "    ", "if", "activation", "==", "\"relu\"", ":", "\n", "        ", "actv", "=", "nn", ".", "ReLU", "\n", "", "elif", "activation", "==", "\"tanh\"", ":", "\n", "        ", "actv", "=", "nn", ".", "Tanh", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unsupported MLP activation\"", ")", "\n", "\n", "", "if", "hidden_depth", "==", "0", ":", "\n", "        ", "mods", "=", "[", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "]", "\n", "", "else", ":", "\n", "        ", "mods", "=", "[", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", ",", "actv", "(", ")", "]", "\n", "for", "_", "in", "range", "(", "hidden_depth", "-", "1", ")", ":", "\n", "            ", "mods", "+=", "[", "nn", ".", "Linear", "(", "hidden_dim", ",", "hidden_dim", ")", ",", "actv", "(", ")", "]", "\n", "", "mods", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_dim", ",", "output_dim", ")", ")", "\n", "", "if", "output_mod", "is", "not", "None", ":", "\n", "        ", "mods", ".", "append", "(", "output_mod", ")", "\n", "", "trunk", "=", "nn", ".", "Sequential", "(", "*", "mods", ")", "\n", "return", "trunk", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.misc.utils.weight_init": [[39, 45], ["isinstance", "torch.init.orthogonal_", "hasattr", "m.bias.data.fill_"], "function", ["None"], ["", "def", "weight_init", "(", "m", ")", ":", "\n", "    ", "\"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "nn", ".", "init", ".", "orthogonal_", "(", "m", ".", "weight", ".", "data", ")", "\n", "if", "hasattr", "(", "m", ".", "bias", ",", "'data'", ")", ":", "\n", "            ", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.minheap.MinHeapBuffer.__init__": [[6, 25], ["dict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "obs_shape", ",", "acs_shape", ",", "max_trajs", ")", ":", "\n", "\n", "        ", "self", ".", "obs_shape", "=", "obs_shape", "\n", "self", ".", "acs_shape", "=", "acs_shape", "\n", "self", ".", "heap", "=", "[", "]", "\n", "self", ".", "max_trajs", "=", "max_trajs", "\n", "self", ".", "num_trajs", "=", "0", "\n", "self", ".", "traj_data", "=", "dict", "(", ")", "\n", "\n", "# the {min, max} credit value assigned to a transition in the MinHeapBuffer", "\n", "self", ".", "max_credit_val", "=", "None", "\n", "self", ".", "min_credit_val", "=", "None", "\n", "\n", "self", ".", "obs", "=", "None", "\n", "self", ".", "next_obs", "=", "None", "\n", "self", ".", "acs", "=", "None", "\n", "self", ".", "dones", "=", "None", "\n", "self", ".", "rewards", "=", "None", "\n", "self", ".", "credits", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.minheap.MinHeapBuffer.__len__": [[26, 28], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs", ".", "shape", "[", "0", "]", "if", "self", ".", "obs", "is", "not", "None", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.minheap.MinHeapBuffer.add_paths": [[29, 75], ["numpy.asscalar", "copy.deepcopy", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "minheap.MinHeapBuffer.traj_data.values", "minheap.MinHeapBuffer.credits.min().item", "minheap.MinHeapBuffer.credits.max().item", "sum", "heapq.heappush", "heapq.heappushpop", "minheap.MinHeapBuffer.traj_data.keys", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "numpy.concatenate", "path[].mean().repeat", "minheap.MinHeapBuffer.credits.min", "minheap.MinHeapBuffer.credits.max", "path[].mean"], "methods", ["None"], ["", "def", "add_paths", "(", "self", ",", "paths", ")", ":", "\n", "        ", "updated", "=", "False", "\n", "\n", "for", "path", "in", "paths", ":", "\n", "            ", "priority", "=", "np", ".", "asscalar", "(", "sum", "(", "path", "[", "'rewards'", "]", ")", ")", "\n", "\n", "if", "self", ".", "num_trajs", "<", "self", ".", "max_trajs", ":", "\n", "                ", "heappush", "(", "self", ".", "heap", ",", "[", "priority", ",", "self", ".", "num_trajs", "]", ")", "\n", "loc", "=", "self", ".", "num_trajs", "\n", "self", ".", "num_trajs", "+=", "1", "\n", "", "else", ":", "\n", "                ", "min_priority", ",", "loc", "=", "self", ".", "heap", "[", "0", "]", "\n", "if", "priority", "<", "min_priority", ":", "\n", "                    ", "continue", "\n", "\n", "# replace min-priority entry", "\n", "", "heappushpop", "(", "self", ".", "heap", ",", "[", "priority", ",", "loc", "]", ")", "\n", "\n", "# free memory", "\n", "", "if", "loc", "in", "self", ".", "traj_data", ".", "keys", "(", ")", ":", "\n", "                ", "del", "self", ".", "traj_data", "[", "loc", "]", "\n", "\n", "# update data", "\n", "", "self", ".", "traj_data", "[", "loc", "]", "=", "deepcopy", "(", "path", ")", "\n", "updated", "=", "True", "\n", "\n", "", "if", "updated", ":", "\n", "            ", "self", ".", "obs", "=", "np", ".", "empty", "(", "(", "0", ",", "*", "self", ".", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "next_obs", "=", "np", ".", "empty", "(", "(", "0", ",", "*", "self", ".", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acs", "=", "np", ".", "empty", "(", "(", "0", ",", "*", "self", ".", "acs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "dones", "=", "np", ".", "empty", "(", "(", "0", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "self", ".", "rewards", "=", "np", ".", "empty", "(", "(", "0", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "credits", "=", "np", ".", "empty", "(", "(", "0", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "path", "in", "self", ".", "traj_data", ".", "values", "(", ")", ":", "\n", "                ", "l", "=", "path", "[", "'observations'", "]", ".", "shape", "[", "0", "]", "\n", "self", ".", "obs", "=", "np", ".", "append", "(", "self", ".", "obs", ",", "path", "[", "'observations'", "]", ",", "axis", "=", "0", ")", "\n", "self", ".", "next_obs", "=", "np", ".", "append", "(", "self", ".", "next_obs", ",", "np", ".", "concatenate", "(", "[", "path", "[", "'observations'", "]", "[", "1", ":", "]", ",", "path", "[", "'final_observation'", "]", "]", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", "\n", "self", ".", "acs", "=", "np", ".", "append", "(", "self", ".", "acs", ",", "path", "[", "'actions'", "]", ",", "axis", "=", "0", ")", "\n", "self", ".", "dones", "=", "np", ".", "append", "(", "self", ".", "dones", ",", "path", "[", "'dones'", "]", ",", "axis", "=", "0", ")", "\n", "self", ".", "rewards", "=", "np", ".", "append", "(", "self", ".", "rewards", ",", "path", "[", "'rewards'", "]", ",", "axis", "=", "0", ")", "\n", "self", ".", "credits", "=", "np", ".", "append", "(", "self", ".", "credits", ",", "path", "[", "'rewards'", "]", ".", "mean", "(", "axis", "=", "0", ",", "keepdims", "=", "True", ")", ".", "repeat", "(", "l", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# update credit values", "\n", "", "self", ".", "min_credit_val", "=", "self", ".", "credits", ".", "min", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "max_credit_val", "=", "self", ".", "credits", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.minheap.MinHeapBuffer.sample": [[76, 101], ["dict", "len", "len", "numpy.random.choice", "numpy.random.choice", "numpy.arange", "numpy.arange", "len", "numpy.sum", "len", "len"], "methods", ["None"], ["", "", "def", "sample", "(", "self", ",", "batch_size", ",", "repeat", "=", "True", ",", "weighted", "=", "False", ")", ":", "\n", "        ", "if", "len", "(", "self", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "len", "(", "self", ")", "<", "batch_size", ":", "\n", "            ", "if", "not", "repeat", ":", "\n", "                ", "return", "None", "\n", "", "inds", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "self", ")", ")", ",", "size", "=", "batch_size", ",", "replace", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "p", "=", "None", "\n", "if", "weighted", ":", "\n", "                ", "p", "=", "self", ".", "credits", "[", ":", "len", "(", "self", ")", "]", "\n", "p", "=", "p", "/", "np", ".", "sum", "(", "p", ")", "\n", "", "inds", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "self", ")", ")", ",", "size", "=", "batch_size", ",", "replace", "=", "False", ",", "p", "=", "p", ")", "\n", "\n", "", "data", "=", "dict", "(", "\n", "observations", "=", "self", ".", "obs", "[", "inds", "]", ",", "\n", "next_observations", "=", "self", ".", "next_obs", "[", "inds", "]", ",", "\n", "actions", "=", "self", ".", "acs", "[", "inds", "]", ",", "\n", "rewards", "=", "self", ".", "rewards", "[", "inds", "]", ",", "\n", "credits", "=", "self", ".", "credits", "[", "inds", "]", ",", "\n", "dones", "=", "self", ".", "dones", "[", "inds", "]", "\n", ")", "\n", "\n", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.__init__": [[4, 24], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "obs_shape", ",", "acs_shape", ",", "max_steps", ")", ":", "\n", "\n", "        ", "self", ".", "obs_shape", "=", "obs_shape", "\n", "self", ".", "acs_shape", "=", "acs_shape", "\n", "self", ".", "max_steps", "=", "max_steps", "\n", "\n", "self", ".", "obs", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_steps", ",", "*", "self", ".", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "next_obs", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_steps", ",", "*", "self", ".", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acs", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_steps", ",", "*", "self", ".", "acs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "self", ".", "dones", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_steps", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "self", ".", "rewards", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_steps", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "credits", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_steps", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# the {min, max} credit value assigned to a transition in the FIFOBuffer", "\n", "self", ".", "min_credit_val", "=", "None", "\n", "self", ".", "max_credit_val", "=", "None", "\n", "\n", "self", ".", "filled_i", "=", "0", "# index of first empty location in buffer (last index when full)", "\n", "self", ".", "curr_i", "=", "0", "# current index to write to (ovewrite oldest data)", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.__len__": [[25, 27], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "filled_i", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.add_paths": [[28, 73], ["numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "fifo.FIFOBuffer.credits[].min().item", "fifo.FIFOBuffer.credits[].max().item", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.concatenate", "path[].mean().repeat", "fifo.FIFOBuffer.credits[].min", "fifo.FIFOBuffer.credits[].max", "path[].mean", "len", "len"], "methods", ["None"], ["", "def", "add_paths", "(", "self", ",", "paths", ")", ":", "\n", "        ", "all_obs", "=", "np", ".", "empty", "(", "(", "0", ",", "*", "self", ".", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "all_next_obs", "=", "np", ".", "empty", "(", "(", "0", ",", "*", "self", ".", "obs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "all_acs", "=", "np", ".", "empty", "(", "(", "0", ",", "*", "self", ".", "acs_shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "all_dones", "=", "np", ".", "empty", "(", "(", "0", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "all_rews", "=", "np", ".", "empty", "(", "(", "0", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "all_cr", "=", "np", ".", "empty", "(", "(", "0", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "path", "in", "paths", ":", "\n", "            ", "l", "=", "path", "[", "'observations'", "]", ".", "shape", "[", "0", "]", "\n", "all_obs", "=", "np", ".", "append", "(", "all_obs", ",", "path", "[", "'observations'", "]", ",", "axis", "=", "0", ")", "\n", "all_next_obs", "=", "np", ".", "append", "(", "all_next_obs", ",", "np", ".", "concatenate", "(", "[", "path", "[", "'observations'", "]", "[", "1", ":", "]", ",", "path", "[", "'final_observation'", "]", "]", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", "\n", "all_acs", "=", "np", ".", "append", "(", "all_acs", ",", "path", "[", "'actions'", "]", ",", "axis", "=", "0", ")", "\n", "all_dones", "=", "np", ".", "append", "(", "all_dones", ",", "path", "[", "'dones'", "]", ",", "axis", "=", "0", ")", "\n", "all_rews", "=", "np", ".", "append", "(", "all_rews", ",", "path", "[", "'rewards'", "]", ",", "axis", "=", "0", ")", "\n", "all_cr", "=", "np", ".", "append", "(", "all_cr", ",", "path", "[", "'rewards'", "]", ".", "mean", "(", "axis", "=", "0", ",", "keepdims", "=", "True", ")", ".", "repeat", "(", "l", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "nentries", "=", "all_obs", ".", "shape", "[", "0", "]", "\n", "if", "self", ".", "curr_i", "+", "nentries", ">", "self", ".", "max_steps", ":", "\n", "            ", "rollover", "=", "self", ".", "max_steps", "-", "self", ".", "curr_i", "# num of indices to roll over", "\n", "self", ".", "obs", "=", "np", ".", "roll", "(", "self", ".", "obs", ",", "rollover", ",", "axis", "=", "0", ")", "\n", "self", ".", "next_obs", "=", "np", ".", "roll", "(", "self", ".", "next_obs", ",", "rollover", ",", "axis", "=", "0", ")", "\n", "self", ".", "acs", "=", "np", ".", "roll", "(", "self", ".", "acs", ",", "rollover", ",", "axis", "=", "0", ")", "\n", "self", ".", "dones", "=", "np", ".", "roll", "(", "self", ".", "dones", ",", "rollover", ",", "axis", "=", "0", ")", "\n", "self", ".", "rewards", "=", "np", ".", "roll", "(", "self", ".", "rewards", ",", "rollover", ",", "axis", "=", "0", ")", "\n", "self", ".", "credits", "=", "np", ".", "roll", "(", "self", ".", "credits", ",", "rollover", ",", "axis", "=", "0", ")", "\n", "self", ".", "curr_i", "=", "0", "\n", "self", ".", "filled_i", "=", "self", ".", "max_steps", "\n", "\n", "", "self", ".", "obs", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "all_obs", "\n", "self", ".", "next_obs", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "all_next_obs", "\n", "self", ".", "acs", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "all_acs", "\n", "self", ".", "dones", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "all_dones", "\n", "self", ".", "rewards", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "all_rews", "\n", "self", ".", "credits", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "all_cr", "\n", "\n", "self", ".", "curr_i", "+=", "nentries", "\n", "if", "self", ".", "filled_i", "<", "self", ".", "max_steps", ":", "\n", "            ", "self", ".", "filled_i", "+=", "nentries", "\n", "", "if", "self", ".", "curr_i", "==", "self", ".", "max_steps", ":", "\n", "            ", "self", ".", "curr_i", "=", "0", "\n", "\n", "# update credit values", "\n", "", "self", ".", "min_credit_val", "=", "self", ".", "credits", "[", ":", "len", "(", "self", ")", "]", ".", "min", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "max_credit_val", "=", "self", ".", "credits", "[", ":", "len", "(", "self", ")", "]", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.tgangwani_GuidanceRewards.buffers.fifo.FIFOBuffer.sample": [[74, 89], ["numpy.random.choice", "dict", "len", "numpy.arange", "len"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "if", "len", "(", "self", ")", "<", "batch_size", ":", "\n", "            ", "return", "None", "\n", "\n", "", "inds", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "self", ")", ")", ",", "size", "=", "batch_size", ",", "replace", "=", "False", ")", "\n", "data", "=", "dict", "(", "\n", "observations", "=", "self", ".", "obs", "[", "inds", "]", ",", "\n", "next_observations", "=", "self", ".", "next_obs", "[", "inds", "]", ",", "\n", "actions", "=", "self", ".", "acs", "[", "inds", "]", ",", "\n", "rewards", "=", "self", ".", "rewards", "[", "inds", "]", ",", "\n", "credits", "=", "self", ".", "credits", "[", "inds", "]", ",", "\n", "dones", "=", "self", ".", "dones", "[", "inds", "]", "\n", ")", "\n", "\n", "return", "data", "\n", "", "", ""]]}